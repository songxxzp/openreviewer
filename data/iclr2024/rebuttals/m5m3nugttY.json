[
    {
        "title": "UniVis: A Universal Framework for Computer Vision Tasks"
    },
    {
        "review": {
            "id": "vEjmr1JYlH",
            "forum": "m5m3nugttY",
            "replyto": "m5m3nugttY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5574/Reviewer_zYeV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5574/Reviewer_zYeV"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework called UniVis, with the aim of handling various categories and granularities of computer vision tasks. The framework is built upon a pre-trained stable diffusion model, thus enabling the transformation of some computer vision tasks into image generation (completion) tasks. The authors tested the framework on multiple benchmarks, and the experimental results showcased competitive performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors unify some computer vision tasks in terms of training format, viewing them as image generation tasks based on stable diffusion model (SD). With SD, this approach achieves better or competitive results on some benchmarks."
                },
                "weaknesses": {
                    "value": "1. Constrained by the pretraining model (i.e., SD), UniVis performs better in image generation (Table 3) while average in visual understanding and low-level image processing (Table 1&2). This indicates that the framework is coupled with the training format of the pretraining model. Furthermore, the authors claim that there is no need for high training costs, but this method requires more parameters.\n\n2. When using multiple datasets for joint training, there is no significant gain, even a slight decrease, for the three different categories of tasks (Table 4). This seems to suggest that this work is limited when using only one model to unify computer vision tasks (joint training doesn't yield gains). If it's just about unifying the training format, it doesn't seem to be very meaningful, as it doesn't outperform task-specific models on specific benchmarks.\n\n3. Fig4 is significant as I personally believe one of the benefits of unifying computer vision tasks should be the ability to generalize to other unseen tasks (analogous to LLM), but unfortunately, only one figure is provided to demonstrate this. How to further enhance generalizability is an interesting point.\n\n4. The testing scheme in the paper is similar to one-shot, how can it be extended to few-shot (>1)?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5574/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763203294,
            "cdate": 1698763203294,
            "tmdate": 1699636573771,
            "mdate": 1699636573771,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dg4ZYeWEJA",
                "forum": "m5m3nugttY",
                "replyto": "vEjmr1JYlH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5574/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5574/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zYeV (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable comments. We address your concerns below:\n\n**Q1: Average performance on visual understanding/low-level image processing and high training costs.**\n> Constrained by the pretraining model (i.e., SD), UniVis performs better in image generation (Table 3) while average in visual understanding and low-level image processing (Table 1&2). This indicates that the framework is coupled with the training format of the pretraining model. Furthermore, the authors claim that there is no need for high training costs, but this method requires more parameters.\n\n**A1:** (1) We would like to clarify that one major contribution of this paper is to build a  unified framework for vision tasks through **generative modeling** rather than commonly adopted contrastive learning or masked image modeling. Different from the well-known dictum by Richard Feynman, \"What I cannot create, I do not understand\", we find it very challenging for the generative models (like PromptDiffusion) to perform well on other vision tasks, especially the visual understanding tasks. Hence, the main aim of this work is making the generative model simultaneously work well on three distinct categories of vision tasks. To this end, we resort to a SOTA text-to-image model, which is one of the very few trained on web-scale data, and we carefully devise an instruction tuning pipeline to adapt it to downstream tasks. As you stated, we achieve competitive results on visual understanding and low-level image processing and exhibit SOTA performance on image generation (**see Figure 5 in the revised paper**). In contrast, our competing methods (Painter and PromptDiffusion) experience a clear collapse or near breakdown on one of the three categories of vision tasks (**see first table in general response #1**). Please refer to our general response #1 for more details.\n\n(2) We are a bit confused about the comment saying \"requires more parameters\". If you mean UniVis-st requires distinct model parameters to tackle different tasks (please let us know if you did not mean this), we would like to clarify that we also present models (UniVis-sc and UniVis-mc) that are jointly trained on multiple tasks (see Tables 2, 3, and 4), which use shared model parameters. The main contribution of our paper is a unified framework for visual task learning, and it can be employed to produce three types of models under different practice scenarios (e.g., the computing power at hand).\n\n**Q2: Joint training does not yield significant gains.**\n> When using multiple datasets for joint training, there is no significant gain, even a slight decrease, for the three different categories of tasks (Table 4). This seems to suggest that this work is limited when using only one model to unify computer vision tasks (joint training doesn't yield gains). If it's just about unifying the training format, it doesn't seem to be very meaningful, as it doesn't outperform task-specific models on specific benchmarks.\n\n**A2:** We would like to emphasize that our primary focus is to investigate how to induce a profound understanding of vision tasks (which involve very disparate visual signals and data domains) through a shared scheme of **generative modeling** instead of seeking gains with multi-task training. Please see our general response #2 for more details.\n\n**Q3: More generalization results.**\n> Fig4 is significant as I personally believe one of the benefits of unifying computer vision tasks should be the ability to generalize to other unseen tasks (analogous to LLM), but unfortunately, only one figure is provided to demonstrate this. How to further enhance generalizability is an interesting point.\n\n**A3:** We have added more results in the updated paper (**please refer to Figure 17**). They verify the generalization capability of the proposed framework. We totally agree that performing out-of-distribution inference is one of the intriguing properties of unified models. Gathering more diverse data for training UniVis could be promising to enhance generalizability and we plan to investigate this in future work."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5574/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497812796,
                "cdate": 1700497812796,
                "tmdate": 1700497812796,
                "mdate": 1700497812796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qKDShfhVtU",
                "forum": "m5m3nugttY",
                "replyto": "sI7tyvCKWo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5574/Reviewer_zYeV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5574/Reviewer_zYeV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2019 efforts in rebuttal.\nHowever, after reviewing the responses and considering the feedback from other reviewers, I have chosen to maintain my original score. My primary concern is that the proposed unified framework does not display significant improvements across the various tasks, nor does it exhibit any emergent properties. While the authors emphasize their focus on exploring methods to induce a deeper understanding of vision tasks, the lack of noticeable improvements or new capabilities (just some figures are shown) makes the unification less meaningful, as I previously mentioned. Additionally, although the authors highlight that the framework does not rely on SD (and masked data modeling would be sufficient), all conducted experiments are exclusively based on SD."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5574/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659882525,
                "cdate": 1700659882525,
                "tmdate": 1700659882525,
                "mdate": 1700659882525,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xuoen4qoHD",
            "forum": "m5m3nugttY",
            "replyto": "m5m3nugttY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5574/Reviewer_ZgiD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5574/Reviewer_ZgiD"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces UniVis, a universal learning framework designed for various computer vision tasks. Drawing inspiration from the success of large language models (LLMs) in natural language processing (NLP), UniVis seeks to offer a unified solution for visual tasks. Based on the text-to-image diffusion model, Stable Diffusion (SD), the framework leverages instruction tuning to adapt pre-trained knowledge to diverse downstream vision tasks. This approach employs an image completion framework, where input comprises a query image paired with another input-output image related to the target task. Through this, the model discerns the desired output for the query. The central tenets include:  1. Vision tasks can be represented as unique input-output pairs. 2. Vision tasks can benefit from optional text prompts. 3. The reasoning ability of SD can be harnessed for diverse vision tasks. \nThe authors undertook comprehensive experiments across ten vision tasks and three distinct training methodologies, aiming to ignite further exploration into fostering a deeper comprehension of vision tasks via a unified generative modeling approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "[Task] The undertaking of employing SD as an interface for diverse downstream tasks presented in this study is intriguing. It enables the pre-trained knowledge to be adaptable and applicable across various downstream vision tasks.\n\n[Experimental Results] The authors carried out a thorough evaluation across many downstream tasks.\n\n[Paper Writing] The manuscript is well written, effectively conveying the primary concepts."
                },
                "weaknesses": {
                    "value": "[Model Performance] While the authors claim reduced computational resource usage, the model's performance significantly lags behind the open-sourced baseline model, Painter [1], as evident in Table 2.\n\n[Universality with Text Instructions] The framework's universality is some kind of diminished by its reliance on task-specific text instructions. A more compelling setup would operate without any task prompts. Text instructions should be supplemental, enriching tasks like text-to-image generation with finer details, rather than being a mandatory prerequisite for all tasks.\n\n[Task Prompt Limitation] The inclusion of task prompts detracts from the intriguing properties of in-context instruction tuning. I think, could be wrong, the essence of in-context learning lies in the model's emergent properties, deciphering the logic and connections within paired VISUAL inputs to undertake related tasks. Ideally, we'd want to furnish only in-context visual cues, enabling the model to manage a myriad of downstream tasks. Yet, the current design seems to veer away from this ideal.\n\n[1] Wang, Xinlong, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. \"Images speak in images: A generalist painter for in-context visual learning.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6830-6839. 2023."
                },
                "questions": {
                    "value": "Most of my questions are in the weakness section. In addition, I'm curious how the model performs without relying on task prompts. Have you conducted any ablation studies to shed light on this aspect?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5574/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698906902736,
            "cdate": 1698906902736,
            "tmdate": 1699636573669,
            "mdate": 1699636573669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OQJojT19cm",
                "forum": "m5m3nugttY",
                "replyto": "xuoen4qoHD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5574/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5574/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZgiD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments. We are willing to address all the concerns raised in your review:\n\n**Q1: Model performance.**\n> While the authors claim reduced computational resource usage, the model's performance significantly lags behind the open-sourced baseline model, Painter, as evident in Table 2.\n\n**A1:** Please see our general response #1.\n\n**Q2: Universality with text instructions.**\n> The framework's universality is some kind of diminished by its reliance on task-specific text instructions. A more compelling setup would operate without any task prompts. Text instructions should be supplemental, enriching tasks like text-to-image generation with finer details, rather than being a mandatory prerequisite for all tasks.\n\n**A2:** (1) Text instructions are indeed supplemental in our design. An ablation result regarding different types of text instructions is given in Table 5. To further explore this, we conducted more experiments during rebuttal and provided the results below (added to Tables 9 and 10 in the revised paper as well). \n\n|               |   |              | Depth estimation |                   |   |              |    Denoising   |             |   |  Mask-to-image  |\n|----------------------------|---|:----------------:|:----------------:|:----------------------:|:-:|:--------------:|:--------------:|:-----------------:|:-:|:---------------:|\n| **Method**                 |   | RMSE$\\downarrow$ |  REL$\\downarrow$ | $\\delta_{1}$$\\uparrow$ |   | PSNR$\\uparrow$ | SSIM$\\uparrow$ | LPIPS$\\downarrow$ |   | FID$\\downarrow$ |\n| UniVis-mc w/ task prompts  |   |       0.421      |       0.131      |          0.863         |   |      34.58     |      0.909     |       0.095       |   |       30.4      |\n| UniVis-mc w/o task prompts |   |       0.466      |       0.154      |          0.826         |   |      34.36     |      0.907     |       0.101       |   |       31.5      |\n\n|                            |   |                |    Deraining   |                   |\n|----------------------------|---|:--------------:|:--------------:|:-----------------:|\n| **Method**                 |   | PSNR$\\uparrow$ | SSIM$\\uparrow$ | LPIPS$\\downarrow$ |\n| UniVis-st w/ task prompts  |   |      22.62     |      0.598     |       0.302       |\n| UniVis-st w/o task prompts |   |      22.60     |      0.595     |       0.306       |\n\nAs can be seen, text instructions are beneficial for some tasks (e.g., semantic segmentation, depth estimation, and mask-to-image), but the gain by applying task prompts is very marginal for other tasks (e.g., deraining and denoising). Therefore, one can optionally apply text instructions during inference to strike a balance between better performance and extra human efforts. (2) We do not think incorporating task-specific text instructions undermines the universality of the proposed framework. From the perspective of a user, one is aware of the task name when constructing the image instruction (input-output pair), thus the text instruction can be given by 1) an empty string, or 2) a task-level prompt (e.g., \"depth map\"), or 3) an instance-level prompt (could use the help from some off-the-shelf tools such as image captioning methods). Our claimed \"universality\" lies in the shared framework for various computer vision tasks.\n\n**Q3: Task prompt limitation.**\n> The inclusion of task prompts detracts from the intriguing properties of in-context instruction tuning. I think, could be wrong, the essence of in-context learning lies in the model's emergent properties, deciphering the logic and connections within paired VISUAL inputs to undertake related tasks. Ideally, we'd want to furnish only in-context visual cues, enabling the model to manage a myriad of downstream tasks. Yet, the current design seems to veer away from this ideal.\n\n**A3:** We agree that the essence of in-context learning may lie in the model's emergent properties. The model we utilize is Stable Diffusion (SD) which translates textual inputs into realistic images, and a bunch of prior works has demonstrated that the knowledge learned in SD contains a rich understanding of both visual and linguistic signals. In light of this, we opt to exploit the inclusion of task prompts for UniVis, which we believe could be fruitful for unlocking the unifying capability of a pre-trained SD model.\n\n**Q4: Ablation studies regarding task prompts.**\n> I'm curious how the model performs without relying on task prompts. Have you conducted any ablation studies to shed light on this aspect?\n\n**A4:** An ablation result regarding different types of text instructions is given in Table 5. We conducted two more experiments during rebuttal and provided the results in Tables 9 and 10 in the revised paper. As can be seen, text instructions are beneficial for some tasks (e.g., semantic segmentation, depth estimation, and mask-to-image), but the gain by applying task prompts is very marginal for other tasks (e.g., deraining and denoising)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5574/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497494257,
                "cdate": 1700497494257,
                "tmdate": 1700497494257,
                "mdate": 1700497494257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i081KQBdhx",
                "forum": "m5m3nugttY",
                "replyto": "vdnuE3Nmbs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5574/Reviewer_ZgiD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5574/Reviewer_ZgiD"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for your time in addressing my comments. However, my key concern is still on the [Task Prompt Limitation]: The inclusion of task prompts detracts from the intriguing properties of in-context instruction tuning. I think, could be wrong, the essence of in-context learning lies in the model's emergent properties, deciphering the logic and connections within paired VISUAL inputs to undertake related tasks. Ideally, we'd want to furnish only in-context visual cues, enabling the model to manage a myriad of downstream tasks. Yet, the current design seems to veer away from this ideal.\n\nTherefore, I will keep my rating as borderline reject. I think this direction is interesting, but the absence of emerging properties makes this work less exciting. I encourage the authors to keep exploring this direction!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5574/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693797282,
                "cdate": 1700693797282,
                "tmdate": 1700693797282,
                "mdate": 1700693797282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MXWOX2YzoT",
            "forum": "m5m3nugttY",
            "replyto": "m5m3nugttY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5574/Reviewer_YRyF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5574/Reviewer_YRyF"
            ],
            "content": {
                "summary": {
                    "value": "The authors present and evaluate \"UniVis,\" which is an approach to training and obtaining inferences from a Stable Diffusion model across a variety of tasks from three distinct categories of tasks. The model is trained using an instruction image pair that demonstrates the task to be perform (e.g., for the depth estimation task, the \"instruction\" or \"example\" pair would be an RGB query image of a scene and a corresponding depth image as example output). At training time, the model is also presented an input query image of the kind expected for the task (e.g. a RGB scene) and trained to produce the ground-truth output for that particular example. This can be supplemented with a textual prompt to further condition the denoising U-Net within the Stable Diffusion model (e.g. the text instruction for the depth estimation task would be \"depth map\").\n\nThis is essentially an instruction-tuning framework for Stable Diffusion, where the instruction is an image pair that demonstrates the task, optionally supplemented with a text description of the task.\n\nMost of the evaluations are conducted by retraining UniVis for a specific task (e.g. depth estimation, or denoising, or pose-to-image image generation). Because of this, this paper is largely a demonstration of the generalizability of the training _process_, rather than the generalizability of a single trained model across the multiple kinds of tasks.\n\nHowever, one experiment is conducted to show that a single trained model can also generalize across tasks from the three task categories (from image understanding: depth estimation; from low-level image processing: denoising; and from conditional image generation: mask-to-image). One experiment also demonstrates that a single trained model can generalize across tasks from within one category: a model was trained to be able to perform inference for four conditional image generation tasks (mask-to-image, depth-to-image, pose-to-image, and edge-to-image).\n\nThe results appear to show performance from the single-task UniVis on par with the \"Painter\" model of Wang et al. (https://arxiv.org/abs/2212.02499), when Painter's training is constrained to use the same amount of computing power as UniVis's authors used for UniVis.\n\nResults did not appear to significantly deteriorate when UniVis was trained as a multi-task (but still single-category) model for the four conditional image generation tasks (see Table 3). Nor did results appear to significantly deteriorate when UniVis was trained on multiple tasks that spanned the three categories of tasks (see Table 4)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The ability to produce a single trained model that can generalize across diverse computer vision tasks by simple altering the \"instructions\" would be very useful. While instruction tuning is not original, the specific construction of the \"instructions\" for UniVis _is_ original. It has much in common with the instruction pairs from \"Painter\" (https://arxiv.org/abs/2212.02499), but instead of masking random subregions of the target images, the authors here train UniVis by masking the entire target image and train UniVis to generate the complete output. It did not seem to me that this would be significant, so I was quite surprised that it appears this change in masking strategy is crucial to unlocking the unifying capability of a pre-trained Stable Diffusion model (see Table 5)."
                },
                "weaknesses": {
                    "value": "**I note that these concerns have largely been addressed in response and revision, but I leave these comments here for context. I have updated my scoring, however.**\n\nI will write this in the first person, to directly address the authors, on the hope that these comments may help improve the paper.\n\nMy biggest concern is that the results in the tables cannot be understood without knowing the variation that might be produced from one repetition of an experiment to the next. It is crucial that you state whether you only ran each experiment once, or whether the numbers you are reporting are the averages across several trials. If the latter, it is also essential to provide some measure of variation (standard deviation, confidence interval). If you have no estimation of the experiment-to-experiment variation, how do you know that what you are observing as differences in the tables is not simply noise?\n\nIn two places, you make assertions / claims that could use further elaboration or specificity. They are also maybe unnecessary in light of what you are actually showing in this paper.\n\n1. At the bottom of p. 1, you say that LLMs exhibit \"superior linguistic understanding due to the highly organized and structured nature of language.\" But images also have significant structure. You go on to say that the \"disparity between low-level features and high-level semantics... is typically more pronounced in vision than in text.\" This is all very vague. Do you need to say it? If you do, could you be more precise about how you are assessing \"disparity\" or the degree of organization or structure?\n2. At p. 6, you say that \"patch-level inpainting that resembles word completion training schemes in NLP is not adequate for a holistic and profound understanding of vision tasks because images _have much lower semantic density than texts_.\" This is vague. I think I know what you mean: that there is a lot of redundancy and spatial correlation in images that is not present in text. Could you make this more precise?\n\nSome of the concepts are under-explained, or used without any explanation:\n\n1. DDIM is mentioned at p. 16 (B.2) without any explanation.\n2. I know U-Net is a well-understood term-of-art by now, but I think it could still use a brief explanation of its purpose, given that it is what is being trained to fit the distribution of latent codes.\n3. You use the phrase \"spatial-wise concatenation\" at p. 4 (first paragraph of 3.1). Can you describe this? I think you simply mean you can stitch the images together in a grid as you visualize in Figure 2, right?\n\nSome of the phrasing is unclear or awkward. I can provide some suggestions for improvement.\n\n1. At p. 3, you talk about \"three _implementations_\" of UniVis. But I would hesitate to call these different _implementations_. I think the contribution of your paper is that these are all the _same_ implementation, but simply trained in three different regimes (single-task, single-category, and multi-category). I prefer the phrase you use later: \"three types of model training.\"\n2. In the first sentence of the abstract: the word \"tam\" seems to be a typo.\n3. In the introduction, \"all-rounder\" is unclear.\n4. In the introduction, the sentence \"The Challenges are in three aspects, elaborated in the following\" is an awkward sentence. I suggest simply: \"There are three main challenges.\""
                },
                "questions": {
                    "value": "1. Why do you ignore the apparently better-performing comparator models in several of the tables when reporting the \"best\" and \"second best\"? For instance, in Table 1, why does OneFormer not get bolded as the \"best\"? If you are ignoring specialized models in your ranking of best and second best, you should include this caveat in your description of \"best.\"\n\n2. I see that you trained a single-category UniVis on the four conditional image generation tasks. Did you attempt training your multi-task/single-category UniVis on either of the two other categories? If not, why did you choose conditional image generation as the category to try as the single-category UniVis?\n\n3. I see that for multi-category UniVis, you selected depth estimation, denoising, and mask-to-image. Did you attempt training other combinations of tasks across the categories (e.g. semantic segmetation + deraining + pose-to-image)? If not, why did you choose the three you chose?\n\n4. Do you expect some combinations of tasks to be particularly difficult for UniVis to be trained for at the same time?\n\n5. If you have no estimation of the run-to-run variation, how do you know that what you are observing as differences in the tables is not simply noise?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5574/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5574/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5574/Reviewer_YRyF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5574/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699057881970,
            "cdate": 1699057881970,
            "tmdate": 1700756337064,
            "mdate": 1700756337064,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KT5yNcB0lO",
                "forum": "m5m3nugttY",
                "replyto": "MXWOX2YzoT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5574/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5574/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YRyF (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and constructive review. We are happy to follow your comments to further improve this paper:\n\n**Q1: Estimation of the experiment-to-experiment variation**\n> My biggest concern is that the results in the tables cannot be understood without knowing the variation that might be produced from one repetition of an experiment to the next. It is crucial that you state whether you only ran each experiment once, or whether the numbers you are reporting are the averages across several trials. If the latter, it is also essential to provide some measure of variation (standard deviation, confidence interval). If you have no estimation of the experiment-to-experiment variation, how do you know that what you are observing as differences in the tables is not simply noise?.\n\n**A1:** We have updated some tables in the paper to include the average scores and standard deviations across three trials. From the added results, we could find that the proposed UniVis is very stable with small stds. Due to limited time and computing resources, we are unable to run all experiments multiple times during rebuttal. We promise to update all tables in the final version if this paper is accepted.\n\n**Q2-1: Vague assertions/claims in the introduction.**\n> At the bottom of p. 1, you say that LLMs exhibit \"superior linguistic understanding due to the highly organized and structured nature of language.\" But images also have significant structure. You go on to say that the \"disparity between low-level features and high-level semantics... is typically more pronounced in vision than in text.\" This is all very vague. Do you need to say it? If you do, could you be more precise about how you are assessing \"disparity\" or the degree of organization or structure?\n\n**A2-1:** In the second paragraph of Section 1, we have changed \"...due to the highly organized and structured nature of language.\" into \"...due to the semantic-dense nature of language created by humans.\" We find that the description regarding \"disparity\" is indeed vague, and we thus remove it to avoid misunderstandings.\n\n**Q2-2: Vague assertions/claims in Section 3.2.**\n> At p. 6, you say that \"patch-level inpainting that resembles word completion training schemes in NLP is not adequate for a holistic and profound understanding of vision tasks because images have much lower semantic density than texts.\" This is vague. I think I know what you mean: that there is a lot of redundancy and spatial correlation in images that is not present in text. Could you make this more precise?\n\n**A2-2:** We have updated the description as follows (see last paragraph of Section 3.2): \"...due to the fact that the correlation between pixels is much stronger than that between words (e.g., this redundancy presented in images makes the model readily inpaint a patch with neighboring patches).\"\n\n**Q3-1: Under-explained concepts regarding DDIM.**\n> DDIM is mentioned at p. 16 (B.2) without any explanation.\n\n**A3-1:** We have added a brief introduction to DDIM in the revised paper as follows (see Appendix B.2): \"By setting the random noise in the reverse diffusion process to 0 (i.e., a  deterministic sampling), DDIM manages to generate an image with fewer sampling steps compared to DDPM.\"\n\n**Q3-2: Under-explained concepts regarding U-Net.**\n> I know U-Net is a well-understood term-of-art by now, but I think it could still use a brief explanation of its purpose, given that it is what is being trained to fit the distribution of latent codes.\n\n**A3-2:** We have added a brief explanation of the purpose of U-Net in the revised paper as follows (see first paragraph of Section 3.2): \"Specifically, a denoising U-Net is trained to fit the distribution of latent codes, which models the reverse diffusion process. Taking the noisy latent and the time step as input, this U-Net is further conditioned on the textual embeddings extracted through a text encoder CLIP via cross-attention to produce the output at the current time step.\"\n\n**Q3-3: Under-explained concepts regarding spatial-wise concatenation.**\n> You use the phrase \"spatial-wise concatenation\" at p. 4 (first paragraph of 3.1). Can you describe this? I think you simply mean you can stitch the images together in a grid as you visualize in Figure 2, right?\n\n**A3-3:** Yes it is, we have appended a brief explanation of \"spatial-wise concatenation\" in the revised paper as follows (see first paragraph of Section 3.1): \"we can implement spatial-wise concatenation of any input, out, and query samples (i.e., stitching all the images together into a grid as illustrated in Figure 2).\""
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5574/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497287094,
                "cdate": 1700497287094,
                "tmdate": 1700497287094,
                "mdate": 1700497287094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uC8uyxoFWD",
                "forum": "m5m3nugttY",
                "replyto": "l7hSuQzNyh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5574/Reviewer_YRyF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5574/Reviewer_YRyF"
                ],
                "content": {
                    "title": {
                        "value": "I have more confidence in the evaluations"
                    },
                    "comment": {
                        "value": "The newly reported standard deviations help provide necessary context for anyone trying to understand the significance (in a collquial sense even if not in a statistical sense) of the reported performance numbers. Thank you. I understand you have promised to add some measure of deviation to all tables for the final version. When you do, please explain somewhere (perhaps in the first table, or in the methodology section) how these +- figures are obtained (is it std dev, variance, a confidence interval, how many reps, etc.) *This raises my soundness score to 4*.\n\nYour edits also greatly clarified the points I thought could be better explained. *This raises my presentation score to 4*.\n\nI disagree that a novel approach to a generalizable multi-task/category model must be shown to outperform other single-task methods out of the gate. Reviewer bP4y asks: \"why not select the most effective method for each specific task and combine them to be a 'universal solver'\"? I understand the desire to continually seek strictly higher performance measures, and of course, that might be the appropriate pragmatic approach if one were deploying a live solution in a product today, but novel methods don't need to beat state-of-the-art to make useful contributions to the field. Nor do I think the method would have to exhibit \"emergent\" properties."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5574/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710206236,
                "cdate": 1700710206236,
                "tmdate": 1700710206236,
                "mdate": 1700710206236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "62kFp3xfn5",
            "forum": "m5m3nugttY",
            "replyto": "m5m3nugttY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5574/Reviewer_bP4y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5574/Reviewer_bP4y"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes UniVis, a framework that can deal with several visual tasks, including visual understanding (e.g., semantic segmentation), low-level image processing (e.g., denoising), and conditional image generation. The idea is to perform instruction tuning on a large-scale pre-trained text-to-image diffusion model. During training, the model is trained to fill in the missing query output based on the provided instructions, which consists of task-specific input-output pairs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper proposes a unified framework for multiple common visual tasks across different categories, including image understanding, image processing, and image generation. The idea of using instruction tuning on Stable Diffusion is novel and interesting, and the experimental results are reasonable. The presentation of the paper is also clear."
                },
                "weaknesses": {
                    "value": "1. The proposed method is limited to dense image tasks, where the output is at a high-dimensional image level. It does not demonstrate feasibility on a wide range of computer vision tasks where the output is low-dimensional, such as image classification, image captioning, VQA, etc.\n2. Even for dense image tasks, it misses multiple tasks. For example, for the task of conditional image generation, the paper misses two important tasks: class-conditional generation and text-conditional generation. Thus, I feel the claim that Univis is a \"Universal Framework for Computer Vision Tasks\" is exaggerated, making the contribution of the paper limited.\n3. Overall, the experiment results shown in the paper are weak. In two of the three categories, it is far behind the Painter baseline. It might be because of computation limitations, but it also might be because the proposed method does not scale with more computation resources. Therefore, it would be good to show at least one experiment that is trained with the same computation budget as the Painter baseline, otherwise, the results are not convincing enough to demonstrate any improvement over previous methods.\n4. The method does not show scaling ability with more tasks and data. As shown in Table 4, a model trained with multiple tasks is worse than a model trained on a single task. This raises concerns that this method may not scale up well with multi-task training, which is not desirable in a universal framework."
                },
                "questions": {
                    "value": "Please see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5574/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5574/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5574/Reviewer_bP4y"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5574/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699112298961,
            "cdate": 1699112298961,
            "tmdate": 1700694251687,
            "mdate": 1700694251687,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WuQhvfiPHt",
                "forum": "m5m3nugttY",
                "replyto": "62kFp3xfn5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5574/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5574/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bP4y"
                    },
                    "comment": {
                        "value": "Thank you for the insightful review. Below, we answer all the questions.\n\n**Q1: Limited to dense image tasks.**\n> The proposed method is limited to dense image tasks, where the output is at a high-dimensional image level. It does not demonstrate feasibility on a wide range of computer vision tasks where the output is low-dimensional, such as image classification, image captioning, VQA, etc.\n\n**A1:** (1) We have evaluated UniVis on keypoint detection, where the output is low-dimensional (please refer to Figures 3, 8, and 17). (2) Other computer vision tasks with sparse outputs could be tackled by our framework as long as one can transfer the output to RGB images. Taking image classification as an example, a class label can be turned into an RGB image filled with a unique color determined by its semantic class. Due to limited time during rebuttal, we cannot provide the results on image classification at this time, but we promise to include them in the final version if the paper gets accepted. (3) We would like to clarify that this paper focuses on standard computer vision tasks instead of vision-language ones including image captioning and VQA, which are also not explored by our competing methods (Painter and PromptDiffusion).\n\n**Q2: More dense image tasks.**\n> Even for dense image tasks, it misses multiple tasks. For example, for the task of conditional image generation, the paper misses two important tasks: class-conditional generation and text-conditional generation. Thus, I feel the claim that Univis is a \"Universal Framework for Computer Vision Tasks\" is exaggerated, making the contribution of the paper limited.\n\n**A2:** (1) Text-conditional generation can be fulfilled by directly applying UniVis-sc trained on four conditional image generation tasks where the query is set to a black image. Please see Figure 16 in the revised paper for visual results and more details in the updated Appendix (Sec. C). (2) Class-conditional generation could also be achieved by framing it as an inverse task of image classification (just like depth-to-image generation and depth estimation). We promise to provide the results on class-conditional generation in the final version if the paper is accepted. (3) We believe that a wide range of vision tasks from three **distinct** categories evaluated in our paper would adequately support our major claims.\n\n**Q3: Weak performance on visual understanding and low-level image processing tasks.**\n> Overall, the experiment results shown in the paper are weak. In two of the three categories, it is far behind the Painter baseline. It might be because of computation limitations, but it also might be because the proposed method does not scale with more computation resources. Therefore, it would be good to show at least one experiment that is trained with the same computation budget as the Painter baseline, otherwise, the results are not convincing enough to demonstrate any improvement over previous methods.\n\n**A3:** Please see our general response #1. The results provided in the second table in general response #1 verify that UniVis scales well with more computation resources.\n\n**Q4: Scaling ability with more tasks and data.**\n> The method does not show scaling ability with more tasks and data. As shown in Table 4, a model trained with multiple tasks is worse than a model trained on a single task. This raises concerns that this method may not scale up well with multi-task training, which is not desirable in a universal framework.\n\n**A4:** We would like to clarify that our primary focus is to investigate how to induce a profound understanding of vision tasks (which involve very disparate visual signals and data domains) through a shared scheme of **generative modeling** instead of seeking gains with multi-task training. Please see our general response #2 for more details."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5574/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497112505,
                "cdate": 1700497112505,
                "tmdate": 1700497112505,
                "mdate": 1700497112505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RGvnzQGk1z",
                "forum": "m5m3nugttY",
                "replyto": "62kFp3xfn5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5574/Reviewer_bP4y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5574/Reviewer_bP4y"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the authors' rebuttal and the clarification regarding the major aim of the paper. After reviewing the rebuttal, I have reassessed my original score, leading to its reduction. This decision stems from the authors' statement that the main goal of the paper is to \"reveal the potential of generative modeling in building a universal solver for vision tasks\", while not \"seeking gains with multi-task training\". This perspective notably diminishes the significance of a 'universal solver'. If UniVis does not have the ability to improve the performance with more tasks, a basic 'universal solver' might simply be a combination of Painter + PromptDiffusion, which shows better performance across all three evaluated tasks than UniVis.\n\nIn short, if a combined approach (UniVis-mc) does not elevate performance in any of the individual tasks (UniVis-st), then the benefit of this combination is quite constrained. It prompts the question: why not select the most effective method for each specific task and combine them to be a \"universal solver\"? One minor benefit of a universal solver with poor performance in all tasks could be a reduction in the total number of parameters required, but this aspect is not addressed in the paper."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5574/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694168830,
                "cdate": 1700694168830,
                "tmdate": 1700694168830,
                "mdate": 1700694168830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]