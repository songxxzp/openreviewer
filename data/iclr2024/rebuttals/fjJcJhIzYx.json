[
    {
        "title": "Neural Rankers for Code Generation via Inter-Cluster Modeling"
    },
    {
        "review": {
            "id": "RySS6nABqK",
            "forum": "fjJcJhIzYx",
            "replyto": "fjJcJhIzYx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8325/Reviewer_53rB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8325/Reviewer_53rB"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies ranking of generated code that are sampled from LLMs. The main idea is to exploit inter-cluster modeling that is overlooked in the literature. The idea is simple - use the overlap of answer with other clusters as the boost signal, assuming that highest overlap is most representative and likely to be the optimal solution. Experiments are done on HumanEval and MBPP-S datasets with 5 code models. Overall the proposed method can outperform 2 baselines published in the literature."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: the proposed method is simple. It is basically a statistical post-processing method that does not involve LLMs/models.\n\n\nS2: Experimenting on 6 code models looks comprehensive."
                },
                "weaknesses": {
                    "value": "W1: It is easy to come up with counterexamples to invalidate the method. For example, cluster 1 shares most solutions, but all solutions are wrong. Thus one can at least question the robustness of the proposed solution. The reviewer understands that this probably did not happen often on the datasets tested. But the reviewer can foresee possibilities: for example, when the tasks are difficult and the model may make wrong outputs. Also, the wrong answers match with each other may not be scarce, since the programs are sampled from the same LLMs.\n\n\nW2: the method is specific to code cluster ranking. The significance of the results are not very clear without a significance test. The reviewer is not sure if it is a good idea to discuss improvement of average performance across various models. With comparing with CodeT, on the 34b model, the improvement seems incremental, and it seems confusing that CodeT is even worse than Greedy."
                },
                "questions": {
                    "value": "Related to W1: any *factual* misunderstanding?\n\nHow to understand the significance of the results?\n\nPlease proofread the paper. Some sentences do not look complete."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698414673970,
            "cdate": 1698414673970,
            "tmdate": 1699637035675,
            "mdate": 1699637035675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BBYUqkqW4r",
                "forum": "fjJcJhIzYx",
                "replyto": "RySS6nABqK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 53rB,\n\nWe appreciate the time and effort you have invested in reviewing our manuscript. We try to address your concerns below.\n\n***Q1. \u201cIt is easy to come up with counterexamples to invalidate the method\u201d***\n\n***A1***:\n\nTo ensure the efficacy of our method, we operated under the assumption that incorrect solutions exhibited diversity with a low probability of functional agreement among them. This functional assumption also aligned with the practices of AlphaCode [1] and CodeT [2]. However, in certain cases, this assumption may be untenable as LLMs virtually generate incorrect solutions and erroneous test cases. The scarcity of correct solutions may be scarce compared to incorrect ones introduces  vulnerability to criteria based on sizes of clusters [1,2], especially when incorrect solutions may surpass correct ones in magnitude. In addition, the prevalence of erroneous test cases undermines the reliability of execution-based criteria [2]. These occurences hinder the effective re-ranking of clusters, causing both SRank and CodeT to falter in their re-ranking capabilities.\n\n\n***Q2. \u201cThe significance of the results are not very clear without a significance test.\u201d***\n\n***A2***:\n\nGiven a predetermined set of test cases and corresponding solutions, CodeT and our proposed method, SRank, are deterministic, signifying an absence of randomness that could impact their return values. In order to establish our method\u2019s superiority over CodeT, a comprehensive evaluation was undertaken.\n\nInitially, we assessed 6 models across 2 standard benchmarks, including HumanEval and MBPP, which indicated that SRank always obtained better results than CodeT. The extent of performance disparity between SRank\u2019s performance and that of CodeT varied depending on the model and the dataset. For instance, SRank could achieve 6.37\\% more than CodeT in pass@1 on CodeGen16 and HumanEval, marking a substantial 17.36\\% improvement while the improvement on WizardCoder34B and HumanEval was 2.95\\% in pass@1 (~4.08\\%). Therefore, the evaluation spanned multiple models and benchmarks, underscoring the robustness and effectiveness of our proposed method.\n\nSubsequently, we conducted an ablation study to analyze the impact of the number of test cases on the performance. Results in Figure 3 showed that when the number of test cases varied, SRank consistently performed better than CodeT.\n\n***Q3. \u201cCodeT is even worse than Greedy\u201d***\n\n***A3***:\n\nPlease allow us to explain that CodeT is worse than Greedy, an observation rooted in the distinct prompts employed for each. In the case of Greedy, our prompting instruction fed to LLMs includes test cases sourced from the datasets, functioning as soft constraints that guide LLMs behavior. Furthermore, the instruction-tuned models, like the WizardCoder family, exhibit a profound comprehension of natural language prompts. Consequently, they generate solutions cognizant of the test cases in the prompts, thereby enhancing the likelihood of satisfying the test cases. Conversely, when generating solutions for CodeT and SRank, the prompts do not contain test cases, rendering their outputs more susceptible to errors. To sustantiate this assertion, we present experimental results where prompts do not include test cases.\n\n|           | |WizardCoder34B | WizardCoder15B | \n|-----------|-----------|----------------|----------------|\n| HumanEval |Greedy| 68.90  | 50.61   |\n| HumanEval | CodeT|72.36| 58.64|\n| MBPP      | Greedy|60.42 | 51.29  | \n|MBPP|CodeT|63.39|58.18|\n\nThe above results illustrate that when employing identical prompts devoid of explicit test case consideration, CodeT surpasses Greedy in performance.\n\n**References:**\n1. *Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., ... & Vinyals, O. Competition-level code generation with alphacode. In Science, 378(6624), 1092-1097. 2022.*\n2. *Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J. G., & Chen, W. Codet: Code generation with generated tests. In ICLR. 2023.*"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592156405,
                "cdate": 1700592156405,
                "tmdate": 1700592795556,
                "mdate": 1700592795556,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gJEKfwzl1O",
            "forum": "fjJcJhIzYx",
            "replyto": "fjJcJhIzYx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8325/Reviewer_BPa9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8325/Reviewer_BPa9"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new rerank strategy which focuses on modeling inter-cluster relationship to select the best solution in code generation. Specifically, they introduce a new metric called functional overlap to quantify the similarity between clusters based on their execution outputs. Experimental results show that this approach achieves remarkable results on pass@1 score across several benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is clearly presented and easy to follow. \n2. The method is simple and experimental results show that this method is effective.\n3. Ablation study in this paper shows the proposed method can be applied to realistic scenarios with a limited number of test cases."
                },
                "weaknesses": {
                    "value": "1. **Reproducibility**: As this paper did not provide any codes in supplementary materials, we are not sure whether this work can be reproduced.\n2. Evaluation metric is limited in this paper since there is only pass@1 score. This paper can provide more results using metrics like pass@5, pass@100, exec@1 and ranked pass@1, as shown in [1]. \n3. Benchmarks are limited. There are only two benchmarks, i.e., HumanEval and MBPP-S, and these benchmarks contain limited examples. We are not sure whether this method works or not in some large-scale datasets such as APPS[1].\n\n[1] Inala et al., Fault-Aware Neural Code Rankers. NeurIPS 2022."
                },
                "questions": {
                    "value": "1. Please answer the questions in Weaknesses regarding the experiments. \n2. Several typos to be fixed:\n\n     a. However, selecting the best solutions from ~~among~~ all possible CodeLLM solutions remains a challenge. \n\n    b. a remarkable results -> remarkable results\n\n3. Could you please provide codes of this work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508115907,
            "cdate": 1698508115907,
            "tmdate": 1699637035541,
            "mdate": 1699637035541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oBwVSXmNI7",
                "forum": "fjJcJhIzYx",
                "replyto": "gJEKfwzl1O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer BPa9,\n\nThank you for your thoughtful review and invaluable feedback. Your comments are instrumental in refining our work. We appreciate your positive recognition of the paper presentation, our results, and effectiveness of our method. We fixed the typos in the revised manuscript according to your suggestion. We address your questions below.\n\n***Q1. Reproducibility***\n\n***A1***:\n\nPlease refer to our response in the authors\u2019 Overall Response comment.\n\n***Q2. \u201cEvaluation metric is limited in this paper since there is only pass@1 score\u201d***\n\n***A2***:\n\nIn the context of real-world usage, it is plausible that end-users typically opt for a singular suggested solution when soliciting the system to generate code. In alignment with this practical scenario, our evaluation protocol centers exclusively around the pass@1 score. This adherence to reporting pass@1 aligns with established conventions observed in numerous works employing HumanEval and MBPP methodologies. Exemplary instances include foundational code language models such as WizardCoder [1], StarCoder [2], and GPT-4 [3], alongside reranking algorithms like Coder-Reviewer [4], LEVER [5], MBR-exec [6], among others.\n\n***Q3. \u201cBenchmarks are limited.\u201d****\n\n***A3***:\n\nPlease refer to our response in the authors' Overall Response comment.\n\nReferences:\n1. *Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. 2023.*\n2. *Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, \u2026 StarCoder: May the source be with you! 2023.*\n3. *OpenAI. GPT-4 Technical Report. 2023.*\n4. *Zhang, T., Yu, T., Hashimoto, T., Lewis, M., Yih, W. T., Fried, D., & Wang, S Coder reviewer reranking for code generation. In ICML. 2023.*\n5. *Ni, A., Iyer, S., Radev, D., Stoyanov, V., Yih, W. T., Wang, S., & Lin, X. V. Lever: Learning to verify language-to-code generation with execution. In ICML. 2023.*\n6. *Shi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., & Wang, S. I. Natural language to code translation with execution. In EMNLP. 2022.*"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592055651,
                "cdate": 1700592055651,
                "tmdate": 1700592055651,
                "mdate": 1700592055651,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WfuWc845qP",
            "forum": "fjJcJhIzYx",
            "replyto": "fjJcJhIzYx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8325/Reviewer_6Abb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8325/Reviewer_6Abb"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel re-ranking approach, SRank, for solutions generated by CodeLLM. The authors recognize that previous re-ranking methods often overlooked complex function similarities and interactions among diverse solution clusters, leading to suboptimal outcomes. Consequently, they propose SRank, which focuses more on the interplay among clusters. By quantifying functional overlaps between clusters, SRank provides a more refined code solution ranking strategy. Empirical results demonstrate significant improvements in pass@1 scores on both the Human-Eval and MBPP benchmarks compared to traditional methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The approach is simple, innovative, and effectively considers relationships among solution clusters.\n- Experimental results are convincing, with ablation study findings further corroborating the method's efficacy."
                },
                "weaknesses": {
                    "value": "- Insufficient descriptions of task scenarios and research value. The authors only briefly reference two previous works when depicting solution clustering and re-ranking tasks, which may confuse the readers, e.g., why not just execute the test cases of the dataset instead of going to generate new test cases? Thus, it is advised to emphasize the generation of new test cases and the need for re-ranking due to the lack of human-designed test cases, as done in CodeT.    \n\n- Figure for SRank reveals that result of test case execution will determine the final rank. However, the process for generating suitable test cases is inadequately discussed. For example, are special means employed to ensure the quality of generated test cases? How were the test cases generation prompts designed?\n\n- As mentioned, test case quality can significantly impact re-ranking results. For more complex programming problems, the quality of CodeLLM-generated solutions and test cases may decline. In such cases, SRank's performance changes merit discussion. Therefore, it is suggested the authors validate SRank's effectiveness with more challenging datasets (e.g., Apps and CodeContests).\n\n- Table 2 results indicate Cluster sizes are a more effective feature than Pass rates, which seems counterintuitive as Pass rates more closely resemble the Pass@K metric. An in-depth explanation is recommended. Furthermore, no mention is made of how the authors ordered entries based on Cluster sizes feature\u2014ascending or descending?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754021115,
            "cdate": 1698754021115,
            "tmdate": 1699637035405,
            "mdate": 1699637035405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L4LsRSt9vz",
                "forum": "fjJcJhIzYx",
                "replyto": "WfuWc845qP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 6Abb,\n\nThank you for your helpful comments and valuable suggestions. Below we answer your concerns.\n\n***Q1. \u201c...why not just execute the test cases of the dataset instead of going to generate new test cases?...\u201d***\n\n***A1***:\n\nPlease refer to our response in the authors' Overall Response comment.\n\n***Q2. \u201c...the process for generating suitable test cases is inadequately discussed\u2026\u201d***\n\n***A2***:\n\nFor generating test cases, we followed the CodeT\u2019s pipeline to produce 100 sequences. At each time of sampling, we feed the context (function) and an instruction to LLMs to get a sequence that can contains multiple test cases. We further post-process generated test cases to filter out those with syntactic errors. Below is the template of our designed prompt.\n\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n    I have this function stub, please generate 50 test cases for this function. The function stub is as follow:\n    ```python\n    {input}    \n    pass\n    ```\n    - Each test case is in the form of assertion statement, for example: assert {entry_point}(...) == ...\n    - Each test case is in a single line\n    - The length of each test case should be too long, ideally less than or equal to 150 letters\n    - The test input should not be too long\n    - The inputs of test cases should be diverse and cover corner cases of the function\n    - Test cases should not be repeated\n\n### Response:\n   Here are 50 test cases for function `{entry_point}`:\\nassert {entry_point}(\"\n```\nwhere `input` is the function header and `entry_point` is the function name.\n\n***Q3. \u201c...it is suggested the authors validate SRank's effectiveness with more challenging datasets\u2026\u201d***\n\n***A3***:\n\nPlease refer to the overall response for more details.\n\n***Q4. \u201cTable 2 results indicate Cluster sizes are a more effective feature than Pass rates, which seems counterintuitive as Pass rates more closely resemble the Pass@K metric\u201d***\n\n***A4***:\n\nWe welcome the opportunity to clarify that results in Table 2 demonstrate that Cluster sizes appear to have a bigger impact than Pass rates, which seemingly looks counterintuitive first as Pass rates closely correlates with Pass@K metric. This problem is primarily attributed to the nature of generated test cases. We prompt LLMs to produce test cases and employ some simple heuristics to assess their syntactic validity. However, they may be prone to errors in which their expected outputs may be incorrect, thereby diminishing the Pass Rates. In contrast, the Cluster sizes are not dependent on expected outputs of the test cases. We operate under the assumption that there are many ways for solutions to be incorrect, whereas correct solutions share the same functionality. This assumption results in correct solutions forming larger clusters, with smaller clusters potentially containing incorrect solutions. Therefore, the Cluster sizes are likely to be more robust than Pass rates.\n\n***Q5. \u201cCluster sizes feature\u2014ascending or descending\u201d***\n\n***A5***:\n\nPlease allow us to elucidate that we re-rank cluster sizes feature in the descending order since we assume that there are many ways for solutions to be incorrect while correct solutions share the same functionality. Therefore, correct solutions tend to form larger clusters, and smaller clusters may contain incorrect solutions.\n\n**References:**\n\n1. *Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J. G., & Chen, W. Codet: Code generation with generated tests. In ICLR. 2023*\n2. *Ni, A., Iyer, S., Radev, D., Stoyanov, V., Yih, W. T., Wang, S., & Lin, X. V. Lever: Learning to verify language-to-code generation with execution. In ICML. 2023*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591654536,
                "cdate": 1700591654536,
                "tmdate": 1700741303015,
                "mdate": 1700741303015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WwQ7qMLqv3",
            "forum": "fjJcJhIzYx",
            "replyto": "fjJcJhIzYx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8325/Reviewer_c5Kt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8325/Reviewer_c5Kt"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a clustering-based solution for code re-ranking. Given a large language model (LLM) generates 100 solutions and 100 sequences of test cases, the proposed approach clusters the solutions by executing them against the test cases. The paper introduces a new metric called functional overlap to quantify the similarity between clusters based on their execution outputs, which allows for identifying the most representative cluster that exhibits maximal overlap with all other clusters. Then the inter-cluster relationships are incorporated into the ranking pipeline, the approach identifies the most promising solutions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed approach is effective on standard benchmarks.\n- Rigorous evaluation and ablation study."
                },
                "weaknesses": {
                    "value": "- Overall, I do not like the main idea of sampling a large number of solutions and then clustering them and identifying the best ones. This does not seem to be a practical solution.\n- The proposed solution is straightforward and there is not much scientific or technical contribution (I am not considering it as a weakness though mentioning it here). I am not sure if this paper passes the bar of ICLR. I would prefer to see the work as a short paper in some other venue.\n- Many statements written throughout the paper are not well-grounded. \n- It is not necessary to comment about CodeT or Code-Reviewer being not comprehensive about experiments to lift this work."
                },
                "questions": {
                    "value": "- I'm afraid I have to disagree with the last paragraph in section 2.2 regarding CodeT. Can you explain - how grouping solutions that pass the same subset of test cases potentially compromises cluster functionality?\n- How does modeling inter-cluster interactions better indicate cluster correctness? [section 2.3]\n- Why should we rely on the test cases generated by the model? Is there any relation between what model generates a solution to the input problem and the test cases? If yes, does executing the test cases for the generated solution add value? Can we directly compare the test cases sampled from the model?\n- Why the evaluation is confined to Python language only? There are many multilingual benchmarks available now.\n- \"For each problem, we sample 100 solutions and 100 sequences of test cases.\" - In a real setting (say in an industry setting), is this a good assumption that 100 solutions will be sampled? As a user, I would prefer to get one recommendation in my IDE, for that, why an LLM that has billions of parameters need to generate 100s of solutions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8325/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698898179566,
            "cdate": 1698898179566,
            "tmdate": 1699637035295,
            "mdate": 1699637035295,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D8aVG1T3PU",
                "forum": "fjJcJhIzYx",
                "replyto": "WwQ7qMLqv3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer c5Kt,\n\nWe are sincerely grateful for your insightful reviews. Your professional feedback not only guides us in crafting a more comprehensive and competitive paper but also reinforces our enthusiasm, especially regarding your positive acknowledgment of our results on code benchmarks, rigorous evaluation and ablation study. Below we address your questions.\n\n***Q1. \u201c...  Can you explain - how grouping solutions that pass the same subset of test cases potentially\u201d***\n\n***A1***:\n\nIn addressing your query, we provide a specific illustration. Consider a scenario utilizing the CodeT reranking method, where a cluster encompasses two solutions, denoted as `f_1` and `f_2`, both passing all prescribed test cases with the exception of one particular assertion, `assert f(i) == o`. In such instances, our assurance extends only to the divergence of `f_1(i)` and `f_2(i)` from `o`; yet, it remains plausible that `f_1(i) != f_2(i)`. This observation underscores the distinct functionality of  `f_1` and `f_2`, despite their classification within the same cluster.\n\nThe fundamental objective of clustering is to assemble solutions with identical functionality into sets, subsequently ranking these solution sets as opposed to directly assessing an extensive array of individual solutions. CodeT, unfortunately, deviates from this foundational principle. Notably, its inadequacy becomes pronounced, particularly when the quantity of test cases is limited. This leads to the aggregation of functionally disparate solutions into a shared cluster, resulting in suboptimal performance, as elucidated in Figure 3 within our submitted paper.\n\nOur proposed method systematically addresses the issue of functional inconsistency within clusters. If two solutions, `f_1` and `f_2`, coexist in the same cluster, and given the identical test case as described earlier, it is guaranteed that `f_1(i) == f_2(i)`, irrespective of the output value `o`. Essentially, our clustering algorithm yields finer-grained clusters than CodeT concerning functionality. Please refer to section 5.2 Case Study which shows some examples regarding functional inconsistency of CodeT. Consequently, our algorithm enhances the efficacy of the cluster ranking step. This distinction is illustrated in Figure 3, where we compare the *CodeT* line with the *Cluster size + Pass rate* line. Both lines utilize cluster size and pass rate as cluster features, with the disparity lying solely in the method of cluster formation.\n\n***Q2. \u201cHow does modeling inter-cluster interactions better indicate cluster correctness?\u201d***\n\n***A2***:\n\nAs articulated in the paper, \"The intuition is that clusters with high overlap exhibit greater shared functionality\". We elucidate why modeling inter-cluster interactions enhances the indication of cluster correctness through a concrete example. Consider solving a competitive programming problem where individuals propose solutions, which may be correct or incorrect. Here, clusters 1, 2, and 3 represent distinct solutions by three programmers addressing the same problem, respectively. In practice, incorrect solutions fail because they cover most cases but fail in specific instances. For two people with two incorrect solutions, each may fail for different reasons. In Figure 2, person 2, i.e cluster 2, may overlook the special case with input as 0, while person 3 forgets to cover input as 3. Consequently, when calculating the overlap in functionality, i.e., the match in outputs when executing the solutions on numerous test inputs, the solutions sharing the maximum output overlap are likely optimal.\n\nTo further validate that modeling inter-cluster interactions aids in ranking clusters, we present the results of reranking solely by matrix I without any cluster features V. In this scenario, V is a column vector with 1 at every entry. Consequently, the ith entry in R is the sum of functional overlap between the ith cluster and all other clusters.\n\n**HumanEval** \n||WizardCoder34B|WizardCoder15B|CodeGen2.5-Instruct|StarCoder|Codex002|CodeGen16B|\n|--|----|---|---|--|--|--|\n|Greedy    | **68.90** |**50.61**| 28.49   |  39.63 |   47.00   | 29.70 |\n|Random   | 59.88 |  45.20    |   26.68   |  32.55 |   37.06    | 22.78 |\n|Interaction only| 66.49   |49.79| **51.13** |**50.01**| **62.75** | **31.31**|\n\n**MBPP** \n||WizardCoder34B|WizardCoder15B|CodeGen2.5-Instruct|StarCoder|Codex002|CodeGen16B|\n|--|----|--|--|--|--|--|\n|Greedy  |**60.42**|51.29|42.86      |45.90|**58.10**|**42.40**|\n|Random  | 54.37 |   45.72 | 34.60    | 39.26 |47.50 | 31.54 |\n|Interaction only| 59.60|**53.94**|**47.70**|**48.85**| 57.49 | 41.98 |\n\nThe results demonstrate that incorporating interaction modeling consistently elevated performance beyond random. Furthermore, reranking with interaction alone significantly outperforms the performance of greedy decoding in some cases, such as 51.13 vs 28.49 for CodeGen2.5,  50.01 vs 39.63 for StarCoder, 62.75 vs 47.00 for CodeX002 in HumanEval."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591105735,
                "cdate": 1700591105735,
                "tmdate": 1700592847720,
                "mdate": 1700592847720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M76my48FDL",
                "forum": "fjJcJhIzYx",
                "replyto": "WwQ7qMLqv3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8325/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***Q3. \u201cWhy should we rely on the test cases generated by the model?\u201d***\n\n***A3***:\n\nFor clarification, we remind that in the context of our work, a test case can be decomposed into a test input and a test output. For example, a full test case `assert f(i) == o` has the test input `i` and test output `o`.\n\nOur reliance on the test cases generated by pre-trained language models stems from their capacity to produce syntactically and semantically accurate test cases. These expansive models are trained on extensive code datasets, thereby imbuing them with the capability to generate test cases that are, to a certain extent, syntactically and semantically sound. Syntactic correctness entails adherence to the format `assert function_name(*args, **kwargs) == output` or in specific cases, `assert function_name(*args, **kwargs) is None/True/False`. Semantically, correctness implies that the test input `*args` and `**kwargs` conform to valid argument types, order, and value ranges, while the test output aligns functionally with the input.\n\nWe measure the percentage of test cases with valid syntax and semantically correct test inputs.\nThis is done by checking the 1) format or structure of the test case by regular expression, 2) test input and 3) test output by compiling them.\n\n**HumanEval**\n||WizardCoder34B|WizardCoder15B|CodeGen2.5-Instruct|StarCoder|Codex002|CodeGen16B|\n|-|-|-|-|-|-|-|\n|No. Test cases|40,519|79,272|76,802|58,095|64,137|9124|\n|No. Invalid test cases|589|716|2757|3431|3306|1108|\n|Percentage of invalid test cases|1.45|0.90|3.59|5.91|5.15|12.14|\n\n**MBPP**\n||WizardCoder34B|WizardCoder15B|CodeGen2.5-Instruct|StarCoder|Codex002|CodeGen16B|\n|-|-|-|-|-|-|-|\n|No. Test cases|105,213|206,942|218,538|127,386|132,439|17,598|\n|No. Invalid test cases|1,960|2,170|9,886|10,118|10,624|1,561|\n|Percentage of invalid test cases|1.86|1.02|4.52|7.94|8.02|8.87|\n\nFrom the table, since the percentage of invalid test cases is very minor, we can confirm the ability of code language models in generating test cases with correct syntax and correct semantics to a certain extent. Note that the reported numbers above are not deduplicated. Although the total number of test cases seem to be large, for each sequence of test cases, we only choose the first 5 valid test cases. With 100 sequences of test cases, the possibly maximum number of test cases for SRank in our experiments is 500 test cases.\n\n***Q4. \u201cWhy the evaluation is confined to Python language only? There are many multilingual benchmarks available now.\u201d***\n\n***A4***:\n\nWe agree that there are multilingual benchmarks available and our method can be generalized to most of the programming languages. In this work, we choose Python as a demonstration language as it was also employed in previous work with the same research scope, e.g., CodeT [1], Coder-Reviewer [2], LEVER [3], MBR-exec [4], CodeRanker [5], and AlphaCode [6]. We leave the experiments for other programming languages for future work.\n\n***Q5. \u201cIn a real setting (say in an industry setting), is this a good assumption that 100 solutions will be sampled? \u2026, why an LLM that has billions of parameters need to generate 100s of solutions?\u201d***\n\n***A5***:\n\nAs we indicated in the introduction, maximum likelihood-based decoding often suffers from a degeneration problem, while the correct solutions of the problem may exist within the sampled solutions from the model distribution. Empirically, numbers in Table 1 (with updated greedy decoding results) showed that greedy decoding results can be much worse than SRank. This supports the argument that sampling a sufficiently enough number of solutions\ncan lead to higher chances that those sampled solutions would contain the correct one. Reranking algorithms such as SRank are specifically devised to discern and prioritize the correct solutions among the sampled set. Again, the ultimate goal of this work and many other works e.g CodeT, Coder-Reviewer, LEVER, MBR-exec, CodeRanker, AlphaCode, etc., is to improve performance for real world tasks, in this case, code generation, through reranking sampled solutions with the minimal trade-off for time and resources.\n\nBesides, the number 100 solutions is employed in the paper and can be seen as a hyperparameter. In application, a specific number can be adopted to balance the trade-off between accuracy for time and resources since performance tends to improve as more solutions are sampled."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591333212,
                "cdate": 1700591333212,
                "tmdate": 1700591885742,
                "mdate": 1700591885742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qfVZmibzOg",
                "forum": "fjJcJhIzYx",
                "replyto": "tIYMFpuhJg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8325/Reviewer_c5Kt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8325/Reviewer_c5Kt"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed response"
                    },
                    "comment": {
                        "value": "Thank you for addressing my questions. However, I am still not satisfied with responses on the following issues.\n\n1. I still didn't understand the point \u201cmodeling inter-cluster interactions better indicate cluster correctness\u201d. Basically, my concern is the assumption (section 2.3)- \"incorrect solutions are diverse and there is a low probability of having a functional agreement among incorrect solutions\" which I thought the authors will clarify. Given we sample solutions using LLMs, it is important to validate why two incorrect solutions would be diverse in terms of functionality?\n2. Why we need to sample 100s of solutions? If we sample only 5-10 solutions, do we still need the proposed approach?\n3. When prior works published, there might not be multilingual benchmarks. Given we have such benchmarks now, current works should start to use them. \n\nI am retaining my score as I do not convinced that the proposed approach will be useful for real scenario."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8325/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644010792,
                "cdate": 1700644010792,
                "tmdate": 1700644010792,
                "mdate": 1700644010792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]