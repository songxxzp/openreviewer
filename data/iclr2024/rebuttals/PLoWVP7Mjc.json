[
    {
        "title": "Embarrassingly Simple Dataset Distillation"
    },
    {
        "review": {
            "id": "T7s7DvlSeW",
            "forum": "PLoWVP7Mjc",
            "replyto": "PLoWVP7Mjc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4362/Reviewer_wwkm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4362/Reviewer_wwkm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes simple and effective method for dataset distillation. Since Back Propagating Through Time (BPTT) is computationally expensive, we truncate a trajectory of inner  optimization steps, called truncated BPTT. However, the performance of truncated BPTT is worse than BPTT due to the biased gradient. In order to get better trade-off between computational cost and performance, the paper proposes to sample random window of inner trajectory, which uses the same amount of computational cost as the truncated BPTT but with better performance. Moreover, it proposes \"Boosted Dataset Distillation\" to mitigate intercorrelation between distilled images, where it iteratively learns to distill a dataset into a small set while setting the learning rate of previously distilled instances to small value."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is simple and effective. It outperforms most of the baselines on CIFAR, CUB and Tiny-ImageNet datasets.\n\n- Ablation studies show the effective of proposed method.\n\n- The proposed method can be easily integrated with other bilevel optimization based dataset distillation methods."
                },
                "weaknesses": {
                    "value": "- Although the proposed method uses the same amount of computational cost as the truncated BPTT method, it still requires to store a trajectory of length $M$. If the dataset becomes large, we may need large $M$ for convergence at inner loop, which hinders scalability of the proposed algorithm.\n\n- It is hard to analyze why the proposed method helps improving performance compared to BPTT or truncated BPTT.\n\n- The paper requires more comprehensive empirical experiments to verify the effectiveness of the proposed method. First and foremost, the authors of the paper did not conduct experiments for architecture generalization. While Table 1 simply demonstrates the method's ability to generalize from a shallow convnet to a wide convnet, I believe this is not adequate. I am curious about how the proposed dataset distillation method can be applied to other architectures, such as VGG, EfficientNet, and ResNet. Secondly, I wonder the proposed method scales to ImageNet dataset which has 1000 classes. \n\n- There is no comparison of computational cost between the proposed method and the other baselines. I think the proposed method is way more expensive than other baselines (such as FrePo) which simplify the  inner loop."
                },
                "questions": {
                    "value": "- What happens if we scale the meta-gradient of BPTT with the norm of the gradient during training? The authors argue training instability of BPTT based on the larger gradient norm. Then the most straightforward way to mitigate the issue would be gradient clipping or scaling the gradient.\n\n- What happens if we gradually increase the total number of unrolling steps for BPTT instead of using fixed $T$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664014169,
            "cdate": 1698664014169,
            "tmdate": 1699636409011,
            "mdate": 1699636409011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3mhuxhuund",
                "forum": "PLoWVP7Mjc",
                "replyto": "T7s7DvlSeW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable suggestions, in particular to clarify computational aspects of our work. This has promted us to conduct an extensive series of additional experiments, as outlined below. We hope that the additional information below speaks for the merits of RaT-BPTT (and might help to determine whether you could raise your score).\n1. > Although the proposed method uses the same amount of computational cost as the truncated BPTT method, it still requires to store a trajectory of length $M$. If the dataset becomes large, we may need large $M$ for convergence at inner loop, which hinders scalability of the proposed algorithm.\n\nIn our experiments, both CIFAR10 with IPC50 (total 500 images) and Tiny-ImageNet with IPC10 (total 2000 images) are tested using the same value of $M$. A moderate $M$ is already large enough for yielding good performance and we observe that the benefits from increasing $M$ significantly diminish after $M=50$. This is evidenced in Figure 10 in the appendix, where we compare the effects of varying  $M$ for CIFAR10 IPC10. The figure demonstrates that increasing $M$ from 40 to 100 only yields small improvements of around 0.6%.\n\nAdditionally, we truly appreciate Reviewer dNLL's reference [1] for efficient meta-gradient calculation and reference [2] for algorithmic design for Adam with constant memory consumption. Integrating such methods into our framework presents an exciting avenue for developing a scalable algorithm. This integration could potentially enhance the computational efficiency and memory management of our method, an aspect we are keen to explore in future work.\n\n2. > It is hard to analyze why the proposed method helps improving performance compared to BPTT or truncated BPTT.\n\nSorry for the confusion. We have corrected Figure 3 as in the General Response 1. Please also see our discussion on theoretical groundings for RaT-BPTT with Reviewer 1 (Qtcx). \n\n\n3. > First and foremost, the authors of the paper did not conduct experiments for architecture generalization. While Table 1 simply demonstrates the method's ability to generalize from a shallow convnet to a wide convnet, I believe this is not adequate. I am curious about how the proposed dataset distillation method can be applied to other architectures, such as VGG, EfficientNet, and ResNet. \n\nIn our original Appendix C.3 (now C.5), we already include direct training and transfer results for ResNet-18. We further assessed our method across various architectures to demonstrate its universality. As suggested by the reviewer, we conduct experiments for VGG-11, AlexNet, and ResNet-18. The EfficientNet is designed for ImageNet dataset with input size 224 $\\times$ 224, and is only applied for finetuning on rescaled CIFAR datasets. Therefore, we choose to incorporate AlexNet instead. \n\nFor all these architectures, we conduct training it from scratch and transferring from the distilled dataset with 3-layer ConvNet. To our knowledge, we are the pioneers in applying direct distillation to a standard-sized network like ResNet-18 and VGG-11. Prior works never train directly on VGG11 and they only use small / modified ResNets like ResNet-10 [3,5], ResNet-12 [4] and ResNet-AP10 [5,6] in these settings. We hope these direct training results demonstrate the generality of our method.\n\n| Architecture | VGG-11 | AlexNet | ResNet-18 |\n|----------|----------|----------|----------|\n| Transfer from ConvNet | 46.6\\% | 60.1\\% | 49.2\\% |\n| Direct Training | 47.7\\% | 63.7\\% | 53.0\\% |\n\nBoth transfer and direct training achieve good performance. \nWe have added these results as Appendix C.5.\n\n\n4. > Secondly, I wonder the proposed method scales to ImageNet dataset which has 1000 classes.\n\nMost existing works in the dataset distillation field do not directly train on the full ImageNet dataset. However, there are notable exceptions, with two recent works [7, 8] scaling dataset distillation to ImageNet 1K. Given our method's strong performance on the CUB dataset and Tiny-ImageNet, both encompassing 200 classes, we are confident in its scalability to the 1,000 classes in ImageNet-1K. To substantiate this claim, we are happy to include preliminary results on ImageNet 1K in the final camera-ready version of our paper, demonstrating the potential of our approach in handling larger, more complex datasets."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700431431582,
                "cdate": 1700431431582,
                "tmdate": 1700431431582,
                "mdate": 1700431431582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vfk2IryXbv",
                "forum": "PLoWVP7Mjc",
                "replyto": "T7s7DvlSeW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Reviewer_wwkm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Reviewer_wwkm"
                ],
                "content": {
                    "title": {
                        "value": "Still not convincing about its scalability"
                    },
                    "comment": {
                        "value": "- I am not still convinced that the proposed method scales to large dataset because it still requires backpropagating through truncated inner trajectory. I am not sure the technique from [1] is applicable to the proposed method since [1] requires the inner optimization procedure \"invertible\". In general, Reverse Mode Differentiation requires large space complexity [2]. I am not familiar with [3], hard to say whether it does really save the memory. I highly recommend authors empirically show that some of those technique reduce the memory without performance degradation so that the proposed method scales to large datasets.\n\n- I do not think FRePO is applicable. Although we sample a feature extractor from a model pool, we need to optimize a linear classifier for inner optimization step.\n\n- Based on this, I keep my initial score as 5.\n# References\n\n[1] Maclaurin, Dougal, David Duvenaud, and Ryan Adams. \"Gradient-based hyperparameter optimization through reversible learning.\" International conference on machine learning. PMLR, 2015.\n\n[2] Franceschi, Luca, et al. \"Forward and reverse gradient-based hyperparameter optimization.\" International Conference on Machine Learning. PMLR, 2017.\n\n[3]  Sachdeva, Noveen, et al. \"Farzi Data: Autoregressive Data Distillation.\" arXiv preprint arXiv:2310.09983 (2023)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544195438,
                "cdate": 1700544195438,
                "tmdate": 1700544195438,
                "mdate": 1700544195438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U1JAGYzgV9",
                "forum": "PLoWVP7Mjc",
                "replyto": "T7s7DvlSeW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We wish to re-emphasize that within the Dataset Distillation community, Tiny-ImageNet is recognized as one of the largest datasets commonly utilized. Our review of over 20 papers reveals that only two recent studies have employed ImageNet-1K. While we are open to including results on ImageNet, the time constraints of the rebuttal phase preclude this addition. However, we would like to reinforce our confidence in our method's scalability with the following arguments, substantiated by relevant references.\n\n## Scaling of our method\n\nContrary to the concerns raised, we disagree with the notion that 'requiring backpropagation through a truncated inner trajectory' inherently leads to scalability issues with larger datasets. It is unrealistic to expect a method to perform equally across all datasets with identical resource allocation. A critical factor is the rate at which resource requirements increase with dataset size. When comparing ImageNet to smaller datasets such as CIFAR10, CUB-200, and TinyImageNet, we note three main areas of increase: the number of classes, the data volume per class, and the network size.\n\n\u2022 Our findings with CUB-200 and CIFAR-100 demonstrate that our method does not require additional unrolling for a larger number of classes.\n\n\u2022 An increase in data volume linearly increases training time, affecting only the number of steps per epoch. This is a natural consequence of fixed Image Per Class number, akin to the increase in epochs for standard training.\n\n\u2022 Our results with TinyImageNet (using ConvNet 4) and our generalization studies on architectures like ResNet-18, VGG-11, and AlexNet indicate that our method can adapt to these larger networks without extra unrolling steps. The memory increase is proportional to the network size, an expected and manageable outcome.\n\nBased on these observations, we conclude that our method's scaling to larger datasets is feasible and does not inherently suffer from the fact of using backpropagation through certain trajectories.\n\n## Reducing the memory consumption\n\nFurther, we propose strategies to reduce memory consumption beyond natural scaling expectations.\n\nWe agree that reversing the inner optimization \"requires the inner optimization procedure to be \"invertible\"\". We note that both SGD and Adam satisfy this criterion. Given the final state and velocity (momentums), we could revert to a previous state's weights. Recalculating the gradient and then subtracting it from the velocity or momentum reverts the previous velocity (momentums). With infinite precision, such inversion is exact, as detailed in Algorithm 2 [1]. We refer the reviewer to check out a blog post at [danielewworrall.github.io/blog/2020/12/reversible-optimisers/]. [2] implements similar reversing methods for dataset distillation. While numerical issues, such as loss of precision during backward propagation, are a potential concern, the methodologies outlined in lines 9 to 13 of Algorithm [1] are designed to mitigate this. Furthermore, [3] observes significant potential for gradient reuse, suggesting the accumulation of the overall gradient during the forward pass. This approach results in an equivalent, exact method that maintains constant memory usage, and has been successfully applied to MTT, which similarly involves backpropagation through inner trajectories. Notably, this technique has led to enhanced performance.\n\nRegarding the reviewer's query about [2] \"hard to say whether it really saves memory\", we direct attention to the fourth figure in [2], which clearly illustrates successful memory consumption reduction. Could the reviewer kindly clarify further concerns?\n\n## Using ideas from FRePO\n\n> Similar to FrePO, we may keep a pool of parameter checkpoints to further optimize our method. This strategy would reduce the need for inner training from new random initializations.\n\nLet us clarify again on how we could use the idea from FRePO. Instead of using random initializaions for every inner unrolling, we could choose one model randomly from the model pool and perform unroll and backward on it. This adjustment could save the forward pass time. This modification affects only lines 5 and 6 of our algorithm and the subsequent optimization and backward stages remain unchanged. This change does not conflict with \"Although we sample a feature extractor from a model pool, we need to optimize a linear classifier for inner optimization step\".\n\n## References\n\n[1] Maclaurin, Dougal, David Duvenaud, and Ryan Adams. \"Gradient-based hyperparameter optimization through reversible learning.\" International conference on machine learning. PMLR, 2015.\n\n[2] Sachdeva, Noveen, et al. \"Farzi Data: Autoregressive Data Distillation.\" arXiv preprint arXiv:2310.09983 (2023).\n\n[3] Cazenavette, George, et al. \"Dataset distillation by matching training trajectories.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676232661,
                "cdate": 1700676232661,
                "tmdate": 1700676414004,
                "mdate": 1700676414004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QfR9zycrnj",
                "forum": "PLoWVP7Mjc",
                "replyto": "U1JAGYzgV9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Reviewer_wwkm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Reviewer_wwkm"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thank you for further clarification. I could not go through the reply in detail. I will come back and read it carefully soon. Thanks"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728301481,
                "cdate": 1700728301481,
                "tmdate": 1700728301481,
                "mdate": 1700728301481,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yoOG5jWitO",
            "forum": "PLoWVP7Mjc",
            "replyto": "PLoWVP7Mjc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4362/Reviewer_dNLL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4362/Reviewer_dNLL"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes three key changes to the meta-matching framework of data distillation: (1) randomly truncated backpropagation through time (BPTT); (2) boosting in data distillation; and (3) using Adam optimizer in the inner-loop (although the authors don\u2019t state this as a primary difference). These changes lead to SoTA results on numerous image classification datasets, especially compared to the naive meta-matching algorithm [1].\n\n[1] Zhiwei Deng and Olga Russakovsky. Remember the past: Distilling datasets into addressable memories for neural networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Thorough empirical evaluation, and SoTA performance on multiple datasets and IPC settings.\n- Improved efficiency for meta-gradient computation compared to full BPTT [1] in terms of both memory & time.\n- I really like the brief hardness-stratified analysis in Figure 10."
                },
                "weaknesses": {
                    "value": "- Using meta-gradient norm as an indicator for stability (more in questions).\n- Boosting is optimization-agnostic procedure however it\u2019s only tested with RaT-BPTT (more in questions).\n- Section 5 (boosting) is not well-discussed with key details shifted to the appendix. I would suggest keeping a subset of results from Figures 7-10 (and moving others to the appendix), but with all details for those experiments complete in the main-text."
                },
                "questions": {
                    "value": "- (Figure 3) The change in gradient norm with more steps is not a good indicator of training stability. I would suggest looking at either the variance of these gradients [1], or the eigenvalues of the hessian matrix (a.k.a. sharpness) [2] as better aligned indicators.\n- Do you expect the boosting idea to work with techniques other than RaT-BPTT, e.g., DSA [3], MTT [4]?\n- One subtle change in RaT-BPTT is the usage of Adam optimizer in the inner-loop. Notably, all existing distillation techniques use SGD. Is this the main reason for improvement compared to other techniques? How does RaT-BPTT (SGD) compare with RaT-BPTT (Adam)? I would suggest referring to [5] for a better understanding of using Adam in data distillation.\n\nI'd be happy to consider increasing my overall rating if some of these questions are addressed in the rebuttal.\n\nOther comments and suggestions (not used in deciding my overall rating):\n- Please include full-data performance in Table 1.\n- The meta-gradient of SGD in the inner-loop can be efficiently computed [6]. Please mention it in the full-text.\n- It would be great to include an analysis of the effect of varying $\\beta$ in Boost-DD on downstream generalization.\n\n[1] Faghri, Fartash, et al. \"A study of gradient variance in deep learning.\" arXiv preprint arXiv:2007.04532 (2020).\n\n[2] Cohen, Jeremy M., et al. \"Gradient descent on neural networks typically occurs at the edge of stability.\" arXiv preprint arXiv:2103.00065 (2021).\n\n[3] Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In Proceedings of the International Conference on Machine Learning (ICML), pp. 12674\u201312685, 2021b.\n\n[4] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4750\u20134759, 2022.\n\n[5] Sachdeva, Noveen, et al. \"Farzi Data: Autoregressive Data Distillation.\" arXiv preprint arXiv:2310.09983 (2023).\n\n[6] Maclaurin, Dougal, David Duvenaud, and Ryan Adams. \"Gradient-based hyperparameter optimization through reversible learning.\" International conference on machine learning. PMLR, 2015."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4362/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4362/Reviewer_dNLL",
                        "ICLR.cc/2024/Conference/Submission4362/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698911632588,
            "cdate": 1698911632588,
            "tmdate": 1700733372589,
            "mdate": 1700733372589,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "quS8fSFUrI",
                "forum": "PLoWVP7Mjc",
                "replyto": "yoOG5jWitO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Requiring Clarifications"
                    },
                    "comment": {
                        "value": "Thank you for your valuable suggestions. We are currently in the process of conducting the experiments as recommended. \n\nIn the meantime, could you please provide further clarification on your suggestion '*It would be great to include an analysis of the effect of varying $\\beta$ in Boost-DD on downstream generalization*'? What does '*downstream generalization*' mean here? We want to ensure we fully understand your feedback to effectively address it."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029076976,
                "cdate": 1700029076976,
                "tmdate": 1700029155469,
                "mdate": 1700029155469,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Crty6zOfto",
                "forum": "PLoWVP7Mjc",
                "replyto": "quS8fSFUrI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Reviewer_dNLL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Reviewer_dNLL"
                ],
                "content": {
                    "title": {
                        "value": "Clarification"
                    },
                    "comment": {
                        "value": "By varying the effect of $\\beta$, I meant that how does the accuracy change when e.g. training IPC 50 on CIFAR-10 but with varying values of $\\beta$. So the final plot would have on x-axis as $\\beta$ and the y-axis would be the accuracy of models trained on data generated using that $\\beta$. Does that make sense?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700030737990,
                "cdate": 1700030737990,
                "tmdate": 1700030737990,
                "mdate": 1700030737990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "27DA0IPbPk",
                "forum": "PLoWVP7Mjc",
                "replyto": "yoOG5jWitO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for a host of very valuable comments and suggestions and for the generally positive reception of our paper. We have tried to address most of your requests, and hope to incite you to raise your score if you like our responses.\n\n1. > Using meta-gradient norm as an indicator for stability (more in questions). (Figure 3) The change in gradient norm with more steps is not a good indicator of training stability. I would suggest looking at either the variance of these gradients [1], or the eigenvalues of the hessian matrix (a.k.a. sharpness) [2] as better aligned indicators.\n\nThank you for giving this suggestion! In Figure 3, we investigated the stability of meta-gradients using gradient norms as a metric, predicated on the notion that stable and efficient learning should manifest as consistent and decreasing gradient norms throughout training. Following your suggestion, we now introduce another metric for evaluating gradient stability: the normalized gradient variance, in line with the methodology proposed in [1]. Each variance value reflects the instability across the batch samples, and the values across time steps reflects the instability across training steps.\n\nTo calculate this metric, we compute the average variance of all gradient entries using a set of 100 samples from the evaluation batch. Given the different scales in gradient norms across different methods, we normalize this variance against the square of the norm. This normalization yields a more consistent metric, termed the normalized variance. Employing the same experimental setup as in Figure 3, we present the results in Figure 11. It shows that RaT-BPTT not only maintains lower variance at each training step but also demonstrates more consistent variance trajectories over the course of training. These findings, in conjunction with the earlier results, collectively offer a comprehensive view of the argued training instability. We have added the discussion in a new section (C.3) in the Appendix, as we feel it further strengthens our analysis and thank this reviewer again for the very valuable suggestion!\n\nWe do not employ the Hessian analysis due to the prevailing uncertainty in the field regarding the learning dynamics at the edge of stability, particularly when using the Adam optimizer with moderate and small batch sizes. This aspect of optimization remains a topic of ongoing research, as current results, like those presented in [2, 7], are predominantly confined to scenarios involving full batch training. Given this limitation, it becomes challenging to establish a definitive standard for assessing gradient stability from the Hessian perspective. The variance of gradients instead stands as a more interpretable metric and our results again validate the improvements of RaT-BPTT.\n\n2. > Boosting is optimization-agnostic procedure however it\u2019s only tested with RaT-BPTT (more in questions). Do you expect the boosting idea to work with techniques other than RaT-BPTT, e.g., DSA [3], MTT [4]?\n\nYes, thank you for asking this question, prompting us to add what we hope is a valuable proof of concept. Indeed, we believe that the Boosting framework will work with other methods. As a proof of principle, and choosing a representative method, we implement strong boosting (with $\\beta = 0$) for MTT from IPC 10 to IPC 50 in steps of IPC 10. The final generalization accuracy is 71.4%. The accuracy improves rapidly throughout the boosting and the final performance is marginally lower (by 0.2%) than the direct distillation of IPC50 with MTT, which stands at 71.6%. We have added the discussion in a new section (C.6) in the Appendix."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700431298478,
                "cdate": 1700431298478,
                "tmdate": 1700431298478,
                "mdate": 1700431298478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vcfuPVrRh5",
                "forum": "PLoWVP7Mjc",
                "replyto": "kjZtxkgg8H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Reviewer_dNLL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Reviewer_dNLL"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for the comprehensive response to my questions, and including a variety of new experiments. Overall, I'm happy with the paper and have increased my rating to 8 accordingly.\n\nOne thing I leave for the Area Chair to decide is that the updated PDF now contains 10 pages which is more than the max page limit."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733500759,
                "cdate": 1700733500759,
                "tmdate": 1700733500759,
                "mdate": 1700733500759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DyMAEMsx6I",
            "forum": "PLoWVP7Mjc",
            "replyto": "PLoWVP7Mjc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4362/Reviewer_Qtcx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4362/Reviewer_Qtcx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Random Truncated BPTT (RaT-BPTT) for solving the bi-level optimization problem in dataset distillation. RaT-BPTT incorporates a truncation coupled with a random window, effectively stabilizing the gradients and speeding up the optimization while covering long dependencies. Empirical results show that such simple method outperforms existing methods for dataset distillation application."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea is simple and easily applicable in practice\n- The empirical results are positive"
                },
                "weaknesses": {
                    "value": "- No theoretical results supporting the empirical results. Analyzing the bi-level optimization problem might be difficult in general, but it is better to find some simple setting where theory can explain when/why the proposed method outperforms existing methods."
                },
                "questions": {
                    "value": "Is the proposed algorithm applicable for general bi-level optimization problems, not only for the dataset distillation application? If so, it would be great to add some discussions on what other applications using bi-level optimization (which currently use BPTT) one can use RaT-BPTT instead."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4362/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698985341969,
            "cdate": 1698985341969,
            "tmdate": 1699636408189,
            "mdate": 1699636408189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZsErWyD905",
                "forum": "PLoWVP7Mjc",
                "replyto": "DyMAEMsx6I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4362/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and valuable feedback. We hope the following responses address your questions. \n\n1. > No theoretical results supporting the empirical results. Analyzing the bi-level optimization problem might be difficult in general, but it is better to find some simple setting where theory can explain when/why the proposed method outperforms existing methods.\n\nIn numerous analyses of bi-level optimization, a common assumption is the convexity of the inner problem, with a focus on convergence to stationary points. However, in scenarios where the inner problem exhibits local strong convexity, along with other favorable conditions, it is unreasonable to expect that the convergence rate of RaT-BPTT would be faster in orders than traditional T-BPTT. On the other hand, in non-convex settings, which are typical of deep learning problems like ours and where RaT-BPTT demonstrates notable empirical performance, providing theoretical proofs of effectiveness becomes exceedingly challenging.\n\nNonetheless, in Section 3 we have aimed to provide a heuristic to justify the increased performance of RaT-BPTT, particularly from a gradient perspective. The inherent non-convexity in the inner loop necessitates long unrolling to adequately capture long term dependencies. This requirement, however, introduces significant computational challenges and leads to gradient instability due to the compounding effects of non-convex Hessian matrix products. RaT-BPTT is a natural solution to all three problems, by limiting the window to control stability and reduce computational burden, and by moving the window to cover the entire unrolling. Both gradient inspections and empirical performance support such heuristics.\n\nTo provide a different angle for theoretical justification, we lay out another hypothesis related to kernel approximations of neural nets (like the NTK). This hypothesis attributes the success of RaT-BPTT, in comparison to both BPTT and T-BPTT, to the separation between feature learning and kernel learning. Our inner loop is training a neural network from scratch. According to [1], there is a significant rotation in the empirical kernel at the onset of training, which then gradually stabilizes. This observation suggests a dominant role of feature learning initially, followed by kernel learning.\n\nFrom this perspective, for large unrolling $T$, T-BPTT is akin to optimizing kernel learning from $\\theta_{T-M}$. The initialization distribution $p_{\\theta_{T-M}}$ is only loosely related to the data. Therefore, T-BPTT only optimizes data that favors good kernel learning, without any assurance of effective feature learning optimization. In contrast, RaT-BPTT, with its sliding window, explicitly focuses on also optimizing the feature learning aspect, which is crucial, especially in the initial stages of training. \n\nPrevious works [2] have established provable benefits of feature learning with specifically tailored noises. We find it promising to adopt such constructions into bi-level optimziation problems to show different features captured by feature learning and kernel learning. However, we believe that such a theoretical guarantee is beyond the scope of our empirical study and likely requires an in-depth study of kernel evolution during unrolling, which likely will have empirical components as well.\n\n2. > Is the proposed algorithm applicable for general bi-level optimization problems, not only for the dataset distillation application? If so, it would be great to add some discussions on what other applications using bi-level optimization (which currently use BPTT) one can use RaT-BPTT instead.\n\nYes! Thanks for pointing it out. As suggested, we have added a new paragraph in Section 6. This addition discusses the broader implications of our work in bilevel optimizations, specifically highlighting its potential in meta-learning for hyperparameters, MAML-like problems, and Poisoning Attacks. \n\n[1] Fort, Stanislav, et al. \"Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel.\" Advances in Neural Information Processing Systems 33 (2020): 5850-5861.\n\n[2] Karp, Stefani, et al. \"Local signal adaptivity: Provable feature learning in neural networks beyond kernels.\" Advances in Neural Information Processing Systems 34 (2021): 24883-24897."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4362/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700431114496,
                "cdate": 1700431114496,
                "tmdate": 1700431114496,
                "mdate": 1700431114496,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]