[
    {
        "title": "SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings"
    },
    {
        "review": {
            "id": "n1D0GdSEER",
            "forum": "jBt8qp1iYK",
            "replyto": "jBt8qp1iYK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission363/Reviewer_q4FE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission363/Reviewer_q4FE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a family of submodular combinatorial objectives for representation learning tasks through the submodular combinatorial representation learning framework to overcome class imbalance in real-world vision tasks. The authors conduct experiments on two benchmark datasets to show the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to read.\n2. The performance seems good compared with other approaches."
                },
                "weaknesses": {
                    "value": "1. The novelty is unclear. The method part only lists some existing metric learning losses.\n2. The proposed framework is called the Submodular Combinatorial Representation learning framework. What does Combinatorial mean? It is unclear what the framework looks like since there are only some metric learning loss functions in the method part.\n3. The authors do not compare with the recent state-of-the-art method since the latest method in Table 2 is in 2020."
                },
                "questions": {
                    "value": "see the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667195455,
            "cdate": 1698667195455,
            "tmdate": 1699635963505,
            "mdate": 1699635963505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yzVuy5Q2vA",
                "forum": "jBt8qp1iYK",
                "replyto": "n1D0GdSEER",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission363/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1. The novelty is unclear. The method part only lists some existing metric learning losses.**\n\n**A1.** We thank the reviewer for raising this concern. Our paper introduces a novel family of objective functions based on set-based submodular information measures. **The paradigm shift in machine learning to adopt set-based information functions as learning objectives and exploiting their combinatorial properties to overcome inter-class bias and intra-class variance is the key motivation of SCoRe.**\n1. Unlike existing contrastive learning objectives which consider pairwise distance / similarity metrics to measures similarity / dissimilarity between object classes, we propose a novel paradigm in representation learning by considering each class in the training dataset $\\mathcal{T}$ as a set $A_k$, where $k \\in \\[1,C\\]$. \n\n2. To the best of our knowledge, we are the first to introduce set-based (combinatorial) information theoretic measures as representation learning objectives - Facility Location (FL), Graph-Cut (GC) and LogDet (LogDet) which demonstrates significantly superior performance over SoTA methods.\n3. Through theoretical proofs, we show that existing contrastive learners are either submodular in nature, or can be reformulated into submodular functions which demonstrate better performance on real-world, class-imbalanced datasets.\n\n4. The SCoRe framework encapsulates all novel combinatorial objectives proposed in SCoRe into a generalizable framework allowing researchers in this field to explore the capabilities of combinatorial objectives on various datasets and feature extractors (backbones).\n\nWe hope that the above points clarify the differences between our proposed approach and the existing metric / contrastive learners and illustrate the novelty of our proposed methodology.\n\n**W3. The authors do not compare with the recent state-of-the-art method since the latest method in Table 2 is in 2020.**\n\n**A3.** We thank the reviewer for pointing this out. Following the recommendations from reviewer zWRY we have included a literature survey of the existing approaches in longtail recognition alongside modern contrastive learners like MoCo, MoCo v2, BYOL etc. in Section 2 of the main paper. For contrastive learners, we observe a surge in modern architectures such as Vision Transformers (Dosovitskiy et al., 2021) , data augmentation based techniques and usage of auxiliary networks in recent years. Unfortunately, due to lack of compute we are unable to conduct experiments on models with such large parameter counts.\nAlso our method focuses mostly on the supervised setting and to the best of our knowledge Supervised Contrastive Learner (SupCon, Khosla et al., 2020) continues to be the SoTA objective function as it continues to be adopted in SoTA architectures like GPaCo (Cui et al., 2023), GLMC (Du et al., 2023), BCL (Zu et al., 2022)etc."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629749324,
                "cdate": 1700629749324,
                "tmdate": 1700629749324,
                "mdate": 1700629749324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BZ0HaKCf7q",
                "forum": "jBt8qp1iYK",
                "replyto": "n1D0GdSEER",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission363/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W2. The proposed framework is called the Submodular Combinatorial Representation learning framework. What does Combinatorial mean? It is unclear what the framework looks like since there are only some metric learning loss functions in the method part.**\n\n**A2.** We thank the reviewer for raising this question. By definition, combinatorics is a branch of mathematics which encapsulates functions that deal with operations such as counting, arranging and analyzing discrete structures. In the context of our paper, combinatorial functions refer to functions in information theory that learn feature information from a set of classes in the training dataset. \n\nObjective functions introduced in SCoRe such as Facility Location (FL), Graph-Cut (GC) and Log-Determinant (LogDet) have been shown to be combinatorial functions in Fujishige, (2005). For example, applying facility location on a feature set $A_k$ which is a subset of the ground set $\\mathcal{V}$ selects the most discriminative set of features (analogous to facilities in combinatorial optimization) thus uniquely identifying the class $A_k$ from classes in $\\mathcal{V} \\setminus A_k$. \n\nTraditionally combinatorial optimization is performed using discrete optimization techniques like greedy search, stochastic greedy search etc. (Iyer et al., 2021). By choosing the similarity metric $S_{ij}(\\theta)$ to be cosine similarity as shown in Section 3.2.1 we observe that the FL function is differentiable in continuous optimization space while exhibiting the combinatorial property described above. Note, that the \u2018max\u2019 in facility location has been smoothened in our paper using the approximation adopted in Song et al., (2017), making $L(\\theta, A_k) = \\sum_{i \\in \\mathcal{V} \\setminus A_k} log \\sum_{j \\in A_k} exp(S_{ij}(\\theta))$. This allows FL based objective function to be optimized using Stochastic Gradient Descent (SGD) for training neural networks.\n\nTo further demonstrate this we have included an experiment in section A.3 of the appendix by employing synthetic data and plotting the facility location information measure for varying levels of cluster overlap. The nature of the plot shows that submodular functions (FL, GC and LogDet) to be continuous and differentiable (Petersen and Pedersen, 2008). This shows the applicability of submodular functions as an objective in representation learning tasks to learn discriminative feature sets for each class (represented as $A_k$) in the training dataset $\\mathcal{T}$.\nUnlike existing contrastive learners / metric learners which learn similarity / dissimilarity between image pairs, objective functions in SCoRe minimize the total information over each set (class) in the dataset enforcing intra-class compactness while maximizing the information over the ground set to enforce inter-cluster separation.\n\nThe SCoRe framework provides the necessary tools to fellow researchers in this field to experiment with combinatorial objective functions and contrast their performance against existing objectives across multiple datasets and backbones. The learning framework has been adapted from Khosla et al., (2020) and has three major components:\n1. **Feature Extractor**, $F(I, \\theta)$ is a convolutional neural network which projects an input image $I$ into a $D_{f}$ dimensional feature space, $r = F(I, \\theta) \\in R^{D_{f}}$ given parameters $\\theta$. The modular design of SCoRe allows researchers to explore multiple network architectures like ResNet, AlexNet, VGGNet etc.\n2. **Classifier**, $Clf(r, \\theta)$ is a linear projection layer that projects the $D_{f}$ dimensional input features to a smaller dimensional vector $D_{p}$, $z = Clf(r, \\theta) \\in R^{D_{p}}$  such that a linear classifier can classify the input image $I$ to its corresponding class label $c_{i}$ for $i \\in [1, C]$. \n3. **Combinatorial Objective Function**, $L(\\theta)$ trains the feature extractor $F$ over all classes $C$ in $D$ to discriminate between classes in a multi-class classification setting. By varying the objective function we are able to study their behavior in learning discriminative feature sets for each class in $D$. SCoRe provides an unified platform to researchers to contrast existing objective functions against set-based combinatorial objectives.\n\nThese details have been updated in Section 3.2 of the main paper for better clarity. Since the training scheme has been majorly adapted from Khosla et al., (2020), we include the details of the learning objective in Section A.2 of the appendix."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629832013,
                "cdate": 1700629832013,
                "tmdate": 1700629933760,
                "mdate": 1700629933760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XmgbU5JJpY",
            "forum": "jBt8qp1iYK",
            "replyto": "jBt8qp1iYK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission363/Reviewer_KzrN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission363/Reviewer_KzrN"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on improving the way deep learning models handle imbalanced class scenarios in real-world applications. In such situations, where some classes are rare, conventional neural networks struggle to learn useful features. This leads to a significant imbalance between rare and abundant classes in the data. To address this, the paper introduces the SCoRe framework, which utilizes Submodular Combinatorial Loss functions. These functions can effectively model feature diversity and cooperation among classes. Experimental results on image classification tasks, including imbalanced datasets like CIFAR-10 and object detection tasks, show that the proposed approach outperforms existing metric learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a new approach to tackle the challenge of class-imbalanced data in deep learning, which is a critical problem in real-world applications.  \n\n- This paper is generally easy to follow."
                },
                "weaknesses": {
                    "value": "- Unclear Link Between Diversity and Robust Representations: While the paper's motivation to employ submodular functions as loss functions to promote diversity is evident, the direct connection between diversity and the creation of robust representations from imbalanced datasets remains somewhat ambiguous. The paper does not clearly elucidate how fostering diversity contributes to the development of robust representations in such scenarios.\n\n- Limited Experimental Evidence: The experimental results exhibit certain weaknesses:\na) The paper compares its approach with well-known metric learning methods but does not utilize popular metric learning datasets, which could potentially limit the generalizability of the findings.\nb) All experiments are conducted on relatively small datasets, as opposed to widely recognized datasets commonly used in imbalanced classification, such as ImageNet-LT. This choice of datasets might limit the broader applicability and relevance of the research."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675408948,
            "cdate": 1698675408948,
            "tmdate": 1699635963415,
            "mdate": 1699635963415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1t9Z0RHcXV",
                "forum": "jBt8qp1iYK",
                "replyto": "XmgbU5JJpY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission363/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1. Unclear Link Between Diversity and Robust Representations: While the paper's motivation to employ submodular functions as loss functions to promote diversity is evident, the direct connection between diversity and the creation of robust representations from imbalanced datasets remains somewhat ambiguous. The paper does not clearly elucidate how fostering diversity contributes to the development of robust representations in such scenarios.**\n\n**A1.** We thank the reviewer for the detailed comment. We do agree that diversity among class specific features alone cannot be a metric for robustness in learnt representations. \nOur paper points out (in Section 1) that inter-cluster separation alongside intra-class compactness has to be enforced during representation learning. \nTo verify this hypothesis in SCoRe, we introduce two variants of information measures as objectives based on (1) total information $L_{S_f}$ and (2) total correlation $L_{C_f}$ respectively. In summary, the $L_{S_f}$ maximizes the diversity within each object class $A_k$ by minimizing the total information in $A_k$, while the $C_f$ variant maximizes both diversity within each class and inter-class separation (by maximizing $ f(\\cup_{k = 1}^{|C|} A_k)$). \nThus $L_{C_f}$ emerges as a better variant to learn both diverse and well-separated feature clusters in representation learning tasks which is confirmed by our experiments in sections 4.2 and 4.3 on real-world class-imbalance data.\n\nTo provide further analysis, we examine the objective function $L_{C_f}(\\theta)$ as introduced in Section 3.2 of the paper by separating its formulation $L_{C_f}(\\theta) = \\sum_{k = 1}^{|C|} f(A_k) - f(\\cup_{k = 1}^{|C|} A_k)$ into two parts. The first part resembles the total information function $S_f$ and would enforce intra-class compactness by minimizing the total information in each class $A_k$. Additionally, maximizing $f(\\cup_{k = 1}^{|C|} A_k)$ promotes inter-class separation in a quest to maximize the overall diversity in the ground set $\\mathcal{V}$. This results in increased inter-class separation. \n\n**W2. Limited Experimental Evidence: The experimental results exhibit certain weaknesses: a) The paper compares its approach with well-known metric learning methods but does not utilize popular metric learning datasets, which could potentially limit the generalizability of the findings. b) All experiments are conducted on relatively small datasets, as opposed to widely recognized datasets commonly used in imbalanced classification, such as ImageNet-LT. This choice of datasets might limit the broader applicability and relevance of the research.**\n\n**A2.** We thank the reviewer for the constructive criticism and pointing us to the corresponding benchmark datasets. We do agree that we were unable to conduct experiments on all possible Longtail benchmarks, especially large scale image datasets due to lack of compute resources. \n\na. Metric Learning datasets used in approaches like ArcFace(Deng et al., 2019), CosFace (Wang et al., 2018), LiftedStructure Loss (Song et al., 2016) etc., do not demonstrate class-imbalanced settings as demonstrated by real-world data. On the other hand, some of the existing benchmarks in Longtail recognition tasks like ImageNet-LT and LVIS are pathologically created from existing datasets like ImageNet-full and MS-COCO (Lin et al., 2014) respectively. These also do not represent real-world applications like Medical Image Analysis and Autonomous driving. In contrast, the choice of datasets in SCoRe adopt the India Driving Dataset (Varma et al., 2019) which contains approximately 60,000 images of Indian traffic scenes which demonstrate, large-variability among classes, high traffic density (number of objects per image) and natural imbalance (including few-shot objects). \n\nb. We do agree with the reviewer on the choice of relatively smaller datasets due to lack of compute availability, but the datasets chosen in SCoRe represent real-world conditions in medical imaging and autonomous driving which are mission critical applications. For example, the OrganAMNIST and DermaMNIST datasets demonstrate natural imbalance due to variation in modalities and demographic conditions which is prevalent in the real-world as well, thus generalizing to the real-world conditions.\nNevertheless, the released codebase of SCoRe does have integrations for all possible datasets in metric and longtail learning which the authors will reproduce results in the availability of additional compute as future work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631294136,
                "cdate": 1700631294136,
                "tmdate": 1700631294136,
                "mdate": 1700631294136,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SuZHx6knZs",
            "forum": "jBt8qp1iYK",
            "replyto": "jBt8qp1iYK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission363/Reviewer_zWRY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission363/Reviewer_zWRY"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses class imbalance problem in real world for representation learning tasks.\nFor this purpose, a SCoRe framework and a family of Submodular Combinatorial objectives are proposed to overcome lack of diversity in visual and structural features for rare classes.\nPerformance evaluation is conducted on two image classification benchmarks (pathologically imbalanced CIFAR-10, subsets of MedMNIST) and a real-world road object detection benchmark (India Driving Dataset ). The newly introduced objectives like Facility Location, Graph-Cut \nand Log Determinant can boost the large performance when compared with state-of-the-art metric learners."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The class-imbalance is a challenging problem, and the illustration of motivation is clear. The effect of class-imbalance on the performance metrics (mAP50) is shown for the object detection task of the IDD.\n+ It seems novel by studying metric learners from an assemblage perspective, treating class-specific feature vectors as sets. \n+ There are some useful conclusions, e.g., the submodule combinatorial objective can construct more distinguishable clustering features for representation learning. At the same time, the derivation proves that the existing contrastive learning objectives are either submodular or can be reformulated as submodular functions.\n+ Three novel objective functions: Facility-Location (FL), Graph-Cut (GC), and Log Determinant (LogDet).\n+ Sufficient experiments on datasets with different degrees of class imbalance for different tasks (image classification and image detection), compared to SoTA metric/contrast learners, indicate the importance of combinatorial loss functions."
                },
                "weaknesses": {
                    "value": "- This paper shows comparative analysis related to metric learning and contrastive learning, without focusing on class imbalance issues. Missing some latest methods in Related Work.\n- As far as I know, there are various methods available to address class imbalance or long-tail problems, such as focal loss, WPLoss, OHEM, data augmentation... What are the differences between SCoRe and these methods? And there are no comparative experiments with these methods.\n- The formulas/symbols in the paper are unclear and lack more explanation.\nFor instance, 'f' is used to denote both the feature extractor and the submodular function; 'S' is utilized to represent both similarity kernels and total submodular information.\n- There are minor writing errors, particularly related to subscript issues, concentrated in Section 3.1. For example, Sij(\\theta) , yii=1,2,...|T |."
                },
                "questions": {
                    "value": "- The class imbalance issue may be more pronounced in some other object detection datasets such as the MS COCO[1] or the LVIS[2] which is dedicated to long-tailed object detection. We are looking forward to see some results on them.\n- How does SCoRe solve the localization/regression problem in object detection tasks under the class-imbalanced settings?\n- Can you provide a detailed explanation of equation (1), as well as the distinction between Total Submodular Information and Total Submodular Correlation?\n- Can you provide a visualization of the class distribution in the CIFAR-10 dataset or other dataset?\n- Will codes be released in the future?\n [1] Microsoft COCO: Common Objects in Context. ECCV, 2014. [2] LVIS: A Dataset for Large Vocabulary Instance Segmentation. CVPR, 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721131836,
            "cdate": 1698721131836,
            "tmdate": 1699635963332,
            "mdate": 1699635963332,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gyUzb2E32Q",
                "forum": "jBt8qp1iYK",
                "replyto": "SuZHx6knZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission363/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1. This paper shows comparative analysis related to metric learning and contrastive learning, without focusing on class imbalance issues. Missing some latest methods in Related Work.**\n\n**A1.** We thank the reviewer for bringing this point to light. We do agree that we could not compare against some of the latest methods and have included them as a part of the related work (section 2) section of the main paper. Unfortunately, due to lack of compute resources we are unable to provide the results on all the discussed works.\n\nHowever, a few points towards the choice of contrastive learners in addressing class-imbalance in SCoRe is noteworthy. Recent works (Suh and Seoh, 2023, Cui et al., 2023 etc.) in longtail learning show that inter-class bias (towards abundant head classes) and intra-class variance are the primary bottlenecks in representation learning tasks. Contrastive / Metric Learners (like SupCon (Khosla et al., 2021)) have been shown to form tighter and well-separated feature clusters (for each class) thereby reducing the impact of inter-class bias and intra-class variance. This makes contrastive learners strong candidates for overcoming class-imbalance. Even SoTA approaches in longtail recognition (Cui et al., 2023, Cui et al. 2022, Zhu et al., 2022 etc.) have adopted contrastive learners in overcoming imbalance alongside additional techniques like data-augmentation, re-balancing etc.\n\nConsidering the aforementioned reasons, SCoRe adopts contrastive learning as a premise to **propose a paradigm shift in machine learning by adopting set-based information functions (Submodular functions) as learning objectives and exploiting their combinatorial properties to overcome inter-class bias and intra-class variance.**\nProposed objective functions (Graph-Cut, Facility Location, Log-Determinant etc.) consider each class in the dataset $\\mathcal{T}$ as a set and enforce both intra-class compactness and inter-class separation. For example, the LogDet function introduced in SCoRe minimizes the volume (in the geometric interpretation of LogDet as in (fujishige et al., 2005) of a feature cluster to maintain intra-class compactness while maximizing the volume of the ground set (whole dataset) $\\mathcal{V}$ to maximize inter-class separation.  \n\nOur experimental results contrasts clearly indicates that submodular combinatorial objectives outperform existing contrastive learners for longtail vision tasks. \n\n**W2. As far as I know, there are various methods available to address class imbalance or long-tail problems, such as focal loss, WPLoss, OHEM, data augmentation... What are the differences between SCoRe and these methods? And there are no comparative experiments with these methods.**\n\n**A2.** We thank the reviewer for pointing this out. We have updated the related work (section 2) of the main paper to highlight the differences between various methods in Longtail learning. Our experiments in the context of longtail object detection (Table 3) compare our proposed objectives (FL and GC) against Focal Loss (Lin et al, 2017) which outperforms OHEM and WPLoss. *Our proposed objectives (as in Faster-RCNN + FPN + FL) outperforms Focal Loss by 23.8 $mAP_{50}$ points*. \n\nWe however do not conduct experiments on data-augmentation based architectures as our main focus is to introduce novel objective functions to overcome challenges in longtail settings. Nevertheless our framework facilitates simple integrations of possible backbones and augmentation blocks to further future research in this field.\nWe also do not aim to replace the existing SoTA baselines but augment it by using set-based combinatorial objectives in SCoRe. \n\n**W3. The formulas/symbols in the paper are unclear and lack more explanation. For instance, 'f' is used to denote both the feature extractor and the submodular function; 'S' is utilized to represent both similarity kernels and total submodular information.**\n\n**A3.** We thank the reviewer for bringing this inconsistency in the usage of $f$ to light and we have addressed this issue in the updated submission. In the current version the feature extractor is denoted by $F$ while submodular functions have been denoted by $f$.\nHowever, for the usage of $S$ in our paper, we use $S_{ij}(\\theta)$ to denote the similarity between the $i^{th}$ and the $j^{th}$ feature vectors. On the other hand $S_f$ has been used to denote the variant of submodular functions which implements total information (fujishige, 2005). For better clarity, the explanations of symbols and notations have been included in Table 4 in section A.1 of the appendix.\n\n**W4. There are minor writing errors, particularly related to subscript issues, concentrated in Section 3.1. For example, Sij(\\theta) , yii=1,2,...|T |.**\n\n**A4.** We thank the reviewer for bringing this inconsistency to light and we have addressed this issue in the updated submission."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635688257,
                "cdate": 1700635688257,
                "tmdate": 1700635688257,
                "mdate": 1700635688257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]