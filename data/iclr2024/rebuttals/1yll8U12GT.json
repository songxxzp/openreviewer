[
    {
        "title": "Enhancing Decision Tree Learning with Deep Networks"
    },
    {
        "review": {
            "id": "3stEs7gpnC",
            "forum": "1yll8U12GT",
            "replyto": "1yll8U12GT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7201/Reviewer_ugFu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7201/Reviewer_ugFu"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a decision tree construction algorithm that outperforms traditional methods, especially when dealing with uncorrelated root nodes. It also offers insights into the inner workings of deep neural networks' feature learning mechanisms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is easy to follow. I believe that providing an intuitive visualization of decision boundaries and explanations using figures, such as the algorithm diagram in Figure 3, would be helpful support for readers."
                },
                "weaknesses": {
                    "value": "I believe that constructing a greedy decision tree offers significant advantages in terms of computational time. While it is possible to make the search more complex, I think there is a deliberate choice not to create overly complicated trees in order to balance computation time and performance. In this sense, it seems that the proposed method involves complex processing during tree construction, but there is no evaluation of the computational cost incurred in doing so. I think it's necessary to have a diverse range of evaluations from perspectives other than just accuracy in order to assess the usefulness of the proposed approach. \n\nFurthermore, since the connection between oblique trees and ReLU networks has been extensively studied, it is necessary to clarify their comparison, mention in related work, and the differences in their respective positions. \n\nWhen presenting experimental results such as in Table 1, please evaluate the errors.\n\nThe mention \"Even ODT construction methods that are not purely greedy in nature seem to fail for such labeling functions\" is present in the text, but it appears that there is no supporting experimental or background information for this assertion."
                },
                "questions": {
                    "value": "1: Please provide information about the training time (Check the weaknesses part).\n\n2: I imagine that when using a single decision tree, one may not prioritize accuracy too much. If you want to push for higher accuracy, it's natural to adopt approaches that use multiple trees like Random Forest or Gradient Boosting Decision Trees. However, other factors such as interpretability and processing speed for a single tree might be important. Are there any benefits from that perspective?\n\n3: Section 3.2 contains the mention: \"A trained DLGN shows some interesting properties that are not possible to even check on ReLU networks.\" However, it is well-known that ReLU networks partition feature space linearly. In that sense, I believe hyperplanes can be checked, can't they? (e.g., \u201cNeural Networks are Decision Trees, Caglar Aytekin, (2022)\u201d)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7201/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7201/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7201/Reviewer_ugFu"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697689214574,
            "cdate": 1697689214574,
            "tmdate": 1699636855147,
            "mdate": 1699636855147,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4hasrUWgP2",
                "forum": "1yll8U12GT",
                "replyto": "3stEs7gpnC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7201/Authors"
                ],
                "content": {
                    "title": {
                        "value": "ReLU networks"
                    },
                    "comment": {
                        "value": "Weaknesses:\n\nThe connection between oblique trees and ODTs are well studied, but mostly in an \"existence\" kind of way -- i.e. that there exists an ODT for every ReLU net and vice versa. Not many constructive/analysis results can be found, to the best of our knowledge. For example, we cannot even look at an ODT and a ReLU network and say if they compute the same function, without exhaustively evaluating across the domain.  \n\nWith a DLGN it is at least possible to check if the hyperplanes of discontinuity in the model (the mL hyperplanes corresponding to the neurons) include the hyperplanes in the ODT. e.g. Table 2 in the paper.\n\nThe statement \"ODT construction methods that are not purely greedy in nature seem to fail for such labeling functions\" is mainly supported empirically by our results on ZanDT (and the newer SDT results). As of yet, we do not know the reasons why these methods  also underperform on the synthetic datasets as we only have a procedural understanding of these algorithms. But we do observe that in addition to lower accuracy, these also fail to retrieve the exact hyperplanes corresponding to the true ODT labelling function -- as opposed to DLGN-DT which retrieves the hyperplanes.\n\nQuestions:\n\n1. Training time questions are answered in the common comments above.\n\n2. Random Forests etc. also do not perform particularly well on the synthetic data we construct. This shows that \"feature learning\" is essential. Without capturing the hyperplanes of the true ODT it is not possible to generalize well.\n\n3. Yes, ReLU networks partition the feature space into polyhedra. But a full breakdown requires 2^{total number of hidden neurons} polyhedrons. While it has been conjectured (Rolnick21) that a vast majority of them are empty, apriori it is impossible to know which are empty and which are not. And it is also not possible to break down these exponentially many regions as simple interactions arising from a much smaller elementary set. DLGNs on the other hand are immensely more graspable -- each neuron is just a half-space. The power of the feature space comes from the intersection of these half-spaces. A much larger set (still only m^L as aopposed to 2^{mL} for ReLU nets) that can be viewed as composed of L (out of mL) elemental half-spaces."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586163691,
                "cdate": 1700586163691,
                "tmdate": 1700586163691,
                "mdate": 1700586163691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xi9auDw2Q4",
            "forum": "1yll8U12GT",
            "replyto": "1yll8U12GT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7201/Reviewer_67Vr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7201/Reviewer_67Vr"
            ],
            "content": {
                "summary": {
                    "value": "The paper identifies a family of labelling functions that can be efficiently represented by an oblique decision trees, however existing learning algorithms fail to learn these trees. To overcome this, the paper presents a new splitting criterion (HDS) and present a deep architecture called DLGN that can be used to detect hyperplanes with low HDS to be selected as splits for the internal nodes of the oblique tree."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n- Interesting and seemingly novel intuition/observation that is represented by the proposed hyperplane discontinuity score\n- Experiments seem to support hypothesis on synthetically constructed datasets\n- Generally well-written with useful illustrative figures"
                },
                "weaknesses": {
                    "value": "Weaknesses:\n- The main intuition behind the proposed approach is not established theoretically. Further, even the hypothesis itself is not mathematically and precisely formalized. It seems to be motivated by a specific synthetic construction that is not clear if this construction tends to appears in real problems.\n- The empirical support for the main claim (e.g., Table 2) is also based on experiments with synthetic data\n- Experimental results for the proposed decision tree construction method are not very convincing: The baseline Zan DT does better on real datasets and outperforms DLGN DT in 5 datasets while DLGN DT outperforms Zan DT in only 3 datasets.\n- The experiments could benefit from experiments with additional baselines for oblique decision trees (e.g., TAO [Carreira-Perpinan & Tavallali, 2018] and others mentioned), as well as reporting results on training accuracy. \n- Also, there is no discussion or results on the differences in terms of computational resources (the proposed approach seems to require training a neural network in each node of the tree and running DBSCAN on the whole dataset which may hinder the scalability of the approach)\n- No discussion if/how this can be extended beyond binary classification\n\n\nMinor typos, inconsistencies:\n- space before \"Krishnan et al.\" page 2\n- notation: it looks like $\\gamma$ should be parameterized by D and f* as well"
                },
                "questions": {
                    "value": "I would appreciate the authors' response to the main weaknesses listed above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607292396,
            "cdate": 1698607292396,
            "tmdate": 1699636855035,
            "mdate": 1699636855035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9dThbe2UAr",
                "forum": "1yll8U12GT",
                "replyto": "Xi9auDw2Q4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7201/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experimental results"
                    },
                    "comment": {
                        "value": "As correctly pointed out, the main claim is captured in Table 2 of the paper. It is one of the first known results where Feature learning is demonstrated directly, and not just inferred via a statement like \"Test accuracy improved because of feature learning\". (The paper by Chizat and Bach on optimal transport with 1-Layer ReLU nets is another prominent example).\n\nWhile the DLGN is indeed a much more tractable architecture than the ReLU network, we anticipate that capturing the dynamics of this architecture will still be quite complicated. Case in point: Deep linear networks, which are simply a reparametrised linear model, are still an active area of study for analysing properties of learning. \n\nTheorems, if any, in this setting are going to be quite heavy on assumptions. But a result like Table 2, gives a direction that future papers can try to prove at least in a restricted setting.\n\nThe computational concerns and baseline comparisons are addressed in the common comment. \n\nWhile we show results only on 3 synthetic datasets (where data is labelled by an unknown ODT), we observe similar results in all the synthetic datasets we tried. But they add nothing more than the three datasets studied here, corresponding to small, medium and high-dimensional input data."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585392949,
                "cdate": 1700585392949,
                "tmdate": 1700585392949,
                "mdate": 1700585392949,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X8pfAkhBDk",
                "forum": "1yll8U12GT",
                "replyto": "9dThbe2UAr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7201/Reviewer_67Vr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7201/Reviewer_67Vr"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I think the lack of theoretical results and the fact that the advantage of the proposed method appears primarily in synthetic datasets remain the main weaknesses."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708262765,
                "cdate": 1700708262765,
                "tmdate": 1700708262765,
                "mdate": 1700708262765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nRwzZKednE",
            "forum": "1yll8U12GT",
            "replyto": "1yll8U12GT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7201/Reviewer_cuTu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7201/Reviewer_cuTu"
            ],
            "content": {
                "summary": {
                    "value": "The provided paper introduces an oblique tree learning algorithm that integrates neural networks into its framework. This methodology adheres to a top-down approach in tree construction, where, at each split, a neural network training is employed to separate two classes (thus, applicable to binary classification only). Subsequently, a clustering algorithm is executed to extract a hyperplane from the trained neural network. This hyperplane then serves as the basis for partitioning the data into two subsets, initiating a recursive progression of the algorithm from that point onward.\n\nTo evaluate the efficacy and performance of this algorithm, experiments are conducted across various benchmarks, employing several baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- the method is easy to understand and implement;\n- the same for the paper, easy to follow."
                },
                "weaknesses": {
                    "value": "1. In Section 2.1, when asserting that \"all greedy methods would fail,\" it is essential to state the underlying assumptions supporting this claim. As it stands, I find it challenging to ascertain the veracity of this statement. Consider the dataset below consisting of 2 points (for simplicity):\n\n  x | o\n\nwhere x and o are data points and \"|\" represents the decision boundaries. Any greedy split will find | as a solution...\n\nIf this proposition is intended to be presented as a theorem, then it necessitates a rigorous formulation and a subsequent proof to establish its validity. It is crucial to uphold the highest standards of mathematical rigor when making such assertions, ensuring that they are substantiated by sound theoretical foundations.\n\n2. **Novelty**. The method resembles soft decision trees (SDTs) [1-3] in its formulation in section 3.1. However, instead of learning hyperplane at each node, the method first fits a NN followed by clustering-based heuristics. This is a bit different since it relies on greedy tree growing procedure. However, similar \"neural\" tree growing technique (without clustering) was employed in Guo and Gelfand (1992). Here, the method applies \"postprocessing\" to transform deep NN into hyperplane.\n\n3. **Experiments**. The experiment, as presently conducted, exhibits a notable gap in its evaluation methodology. It notably lacks a comparative analysis against well-established oblique tree learning methods, including those referenced in citations [1-5], as well as the work by Carreira-Perpinan and Tavallali from 2018. Such a comparative assessment is paramount in validating the efficacy and distinctiveness of the proposed approach.\n\n4. The method as is only applicable to binary classification and extending it seems to be nontrivial (except, maybe, one-vs-all)?\n\n---------------\n\n[1] Jordan, M. I. and Jacobs, R. A. (1994). Hierarchical mixtures of experts and the EM algorithm. Neural Computation, 6(2):181\u2013214\n\n[2] Frosst, N. and Hinton, G. (2017). Distilling a neural network into a soft decision tree. arXiv:1711.09784\n\n[3] Hazimeh, H., Ponomareva, N., Mol, P., Tan, Z., and Mazumder, R. (2020). The tree ensemble layer: Differentiability meets conditional computation. In Daum\u00e9 III, H. and Singh, A., editors, Proc. of the 37th Int. Conf. Machine Learning (ICML 2020).\n\n[4] Zharmagambetov, A., Hada, S. S., Gabidolla, M., and Carreira-Perpi\u00f1\u00e1n, M. \u00c1. (2021b). Non-greedy algorithms for decision tree optimization: An experimental comparison. In Int. J. Conf. Neural Networks(IJCNN\u201921).\n\n[5] One possible SDT implementation: https://github.com/xuyxu/Soft-Decision-Tree"
                },
                "questions": {
                    "value": "- What is Zan DT method? I don't see any references to it..."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7201/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701136471,
            "cdate": 1698701136471,
            "tmdate": 1699636854922,
            "mdate": 1699636854922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "szhgzSDeP0",
                "forum": "1yll8U12GT",
                "replyto": "nRwzZKednE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7201/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7201/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Failure of Greedy Methods"
                    },
                    "comment": {
                        "value": "Thanks for the comments. \n\nSome of your other concerns were addressed in the common comment. \n\nWhen we say \"all greedy methods would fail\" we mean the setting where the labels are coming from an ODT with high (>5) dimensional input and balanced splits in each nodes, with the node hyperplanes being close to orthogonal to other nodes. \n\ne.g. a depth 3 ODT over (say) 10-d data, with each node i going qi.x >0 ? where q1, q2, ..., q_7 are all orthogonal vectors in 10-d space. The 8 leaf nodes are labelled such that siblings have opposite labels. Here no greedy procedure for picking the root node would pick q1.x as the split function. Because the information gain would be zero. \n\nHowever, it is important to not that this is not a failure of Decision tree learning itself, and only that of greedy methods. If all the nodes are indeed jointly optimised, the optimal tree would indeed match the true labelling function.\n\n\nPS: ZanDT is a recent oblique tree construction method by Zantedeschi et al. (2020)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7201/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584876253,
                "cdate": 1700584876253,
                "tmdate": 1700584876253,
                "mdate": 1700584876253,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]