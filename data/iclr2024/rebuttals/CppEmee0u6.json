[
    {
        "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities"
    },
    {
        "review": {
            "id": "iVd5WLSUxv",
            "forum": "CppEmee0u6",
            "replyto": "CppEmee0u6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1751/Reviewer_BGNu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1751/Reviewer_BGNu"
            ],
            "content": {
                "summary": {
                    "value": "This work tackles the problem of how to improve the transformer of one modality from the models of other modality with irrelevant data. It proposes a novel method named Multimodal Pathway equipped with cross-modal re-parameterization. It performs experiments with four modalities \u2013 images, videos, point clouds and audio with the datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work addresses an interesting and important problem \u2013 how to transfer knowledge from one modality to another modality.\n\n2. The proposed approach is sound and reasonable."
                },
                "weaknesses": {
                    "value": "1. The problem and motivation that this work focuses are not so novel. \n- Many recent models do not require paired multimodal data for pre-training and fine-tuning. \n- Knowledge transfer from one modality to another is actively studied in many directions. \n- Unfortunately, the related work of this paper is largely cursory. \n- Many previous works have shown that the four modalities that this work considers \u2013 images, videos, point clouds and audio \u2013 are related enough to learn from one modality and help for another modality with no pairing. \n\n2. The effectiveness of the proposed approach is not sufficiently demonstrated in experiments. \n- The reported performance improvements are somewhat marginal over MAE as shown in Table 1 (i.e., mostly less than 1.0% in accuracy and at best <2.0%). \n- Overall, the proposed cross-modal re-parameterization seems reasonable, more thorough experimental supports may be required. \n- More supports include more baselines, other novel modalities, more performance gaps, etc, which can make this submission much stronger."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698482195535,
            "cdate": 1698482195535,
            "tmdate": 1699636104207,
            "mdate": 1699636104207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zCqgoCVXF0",
                "forum": "CppEmee0u6",
                "replyto": "iVd5WLSUxv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BGNu (weakness 1)"
                    },
                    "comment": {
                        "value": "First of all, we would like to respectfully thank the reviewer's time and effort in evaluating our work. However, we would like to clarify that some of the mentioned weaknesses seem to be derived from misunderstandings. We address these concerns below and hope that our explanations will lead to a reconsideration of the paper's evaluation.\n\n**\"Many recent models do not require paired multimodal data for pre-training and fine-tuning.\"**\n\nThe advantages of multimodal pretraining have been widely adopted by the community, such as vision-language pretraining [CLIP, BEIT, Flamingo], and vision-language-audio pretraining [VALOR, VAST, UnIVAL]. Some methods do not use paired data but interleaved data like Flamingo (e.g., a text paragraph together with several images describing objects mentioned in the text), so that the data is still relevant. In this paper, we mentioned prior works with **paired or interleaved data** (e.g., in Figure 1). We will add more detailed discussions in the revised version. However, besides these successful attempts with multimodal relevant data, multimodal pretraining with completely irrelevant data, which is **neither paired nor interleaved**, especially with image, point cloud, audio, and video data, remains under-explored. Therefore, the existing multimodal pretraining works do not undermine the novelty of our work, and we are confident that our work will be inspiring and meaningful for the research area. \n\n**\"Knowledge\u00a0transfer\u00a0from\u00a0one\u00a0modality\u00a0to\u00a0another\u00a0is\u00a0actively\u00a0studied\u00a0in\u00a0many\u00a0directions.\"**\n\nWe understand that the reviewer is familiar with multimodal knowledge transfer. However, these works are mainly focused on transferring knowledge from data-sufficient modalities such as images to data-insufficient modalities such as point cloud or audio. However, bidirectional multimodal knowledge transfer is not mentioned before. Meanwhile, it's the first time to use data-insufficient modality such as point cloud to assist data-sufficient modality such as image in our paper. The proposed method Multimodal Pathway even achieves impressive improvements (e.g. +0.6% on ImageNet, +2.2% on ImageNet Linear Probing, +2.7% on MS COCO object detection, and +2.8% on ADE20K segmentation. )\n\n**Unfortunately, the related work of this paper is largely cursory.**\n\nWe thank the reviewer for the constructive suggestions. We discussed pretraining methods and re-parameterization techniques because they are closely related to our method. If the reviewer could name a few significant references lost in this paper, we are willing to add a thorough discussion. \n\n**\"Many\u00a0previous\u00a0works\u00a0have\u00a0shown\u00a0that\u00a0the\u00a0four\u00a0modalities\u00a0that\u00a0this\u00a0work\u00a0considers\u00a0\u2013\u00a0images,\u00a0videos,\u00a0point\u00a0clouds\u00a0and\u00a0audio\u00a0\u2013\u00a0are\u00a0related\u00a0enough\u00a0to\u00a0learn\u00a0from\u00a0one\u00a0modality\u00a0and\u00a0help\u00a0for\u00a0another\u00a0modality\u00a0with\u00a0no\u00a0pairing.\"**\n\nTo the best of our knowledge, we are the first to conduct research on this topic. We sincerely hope that the reviewer can help us with more relevant references.\n\n(please see our response to Weakness 2 in the next comment)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699710089147,
                "cdate": 1699710089147,
                "tmdate": 1699710124726,
                "mdate": 1699710124726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "64V3LXgshx",
                "forum": "CppEmee0u6",
                "replyto": "iVd5WLSUxv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BGNu (weakness 2)"
                    },
                    "comment": {
                        "value": "(This is the second response. Please see our response to Weakness 1 in the last comment)\n\n**\"The reported performance improvements are somewhat marginal over MAE as shown in Table 1 (i.e., mostly less than 1.0% in accuracy and at best <2.0%).\"**\n\nAs we introduced previously, Multimodal Pathway achieves impressive improvements (e.g. +0.6% on ImageNet, +2.2% on ImageNet Linear Probing, +2.7% on MS COCO object detection, and +2.8% on ADE20K segmentation.) Please refer to the experimental results in Table 1.\n\nMoreover, we would like to highlight that MAE is a powerful pretraining method, and it is challenging to gain further improvements on top of it. Some recent works aimed to improve MAE, e.g., SemMAE(Li et al., 2022a, published at NeurIPS) achieved ImageNet accuracy of 83.4 (+0.1), MFF (Liu et al., 2023, published at ICCV) achieved 83.6 (+0.3), which are cited in Table 1. Our method achieved an accuracy of +0.6, which can be seen as significant.\n\n**\"Overall,\u00a0the\u00a0proposed\u00a0cross-modal\u00a0re-parameterization\u00a0seems\u00a0reasonable,\u00a0more\u00a0thorough\u00a0experimental\u00a0supports\u00a0may\u00a0be\u00a0required. More\u00a0supports\u00a0include\u00a0more\u00a0baselines,\u00a0other\u00a0novel\u00a0modalities,\u00a0more\u00a0performance\u00a0gaps,\u00a0etc,\u00a0which\u00a0can\u00a0make\u00a0this\u00a0submission\u00a0much\u00a0stronger.\"**\n\nWe thank the reviewer for the constructive suggestion. In our manuscript, we have conducted experiments for **4** modalities and constructed **12** types of multimodal pathways on **7** widely adopted benchmarks. More experimental results can be easily found in our paper Table 2 and Table 3. Meanwhile, to fairly compare our method with existing advanced pretraining methods, we take MAE, Point-MAE, AudioMAE, and Video MAE as baselines in these experiments, our methods bring consistent improvements, and 12 combinations between 4 modalities can effectively be improved with multimodal pathway.\n\n**In summary**, we appreciate the reviewer's comments and hope that our clarifications on the misunderstandings will address the concerns. \n\nWe kindly request the reviewer to reconsider the evaluation in light of these explanations. \n\nPlease let us know if there is anything we can do to convince the reviewer to raise the score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699710326859,
                "cdate": 1699710326859,
                "tmdate": 1699710326859,
                "mdate": 1699710326859,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hAJ7At3Lyf",
            "forum": "CppEmee0u6",
            "replyto": "CppEmee0u6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1751/Reviewer_8Bi6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1751/Reviewer_8Bi6"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes Multimodal Pathway Transformer (M2PT), a model to improve target modality from other modalities with non-paired data. M2PT consists of modality-specific tokenizers to transform raw inputs into features, and multiple linear layers inside each transformer block. Given the non-paired data of different modalities,  M2PT processes these data simultaneously and shows that the auxiliary data can improve the model's performance on target modalities."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This method is simple and the paper is easy to understand.\n2. Lots of experiments are conducted to show that auxiliary modality can improve the model's performance on target modality."
                },
                "weaknesses": {
                    "value": "1. The author claims that incorporating the auxiliary modality would improve the model's performance on the target modality, even if there is no any relevance between the data. However, the reasoning behind this enhancement in performance remains unexplained. Furthermore, it cannot be confirmed that non-paired data is entirely unrelated, thus raising doubts about the validity of all related statements.\n\n2. In M2PT, numerous additional parameters were introduced, which could potentially account for the observed model improvement, rather than the inclusion of an auxiliary modality. The author didn't conduct any ablation experiments to investigate this."
                },
                "questions": {
                    "value": "see weakness above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1751/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1751/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1751/Reviewer_8Bi6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748129550,
            "cdate": 1698748129550,
            "tmdate": 1699636104133,
            "mdate": 1699636104133,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WQ3cAS9Pgt",
                "forum": "CppEmee0u6",
                "replyto": "hAJ7At3Lyf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8Bi6 (weakness 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their efforts and valuable feedback. However, we would like to clarify that some of the mentioned weaknesses seem to be derived from misunderstandings. We address these concerns below and hope that our explanations will lead to a reconsideration of the paper's evaluation.\n\nWeakness 1. **\"It cannot be confirmed that non-paired data is entirely unrelated\"**\n\n**Clarification**: We respectfully disagree with this comment. We are confident that the datasets we use are unrelated. For example, the reviewer may be concerned that the image dataset contains images of cars, while the audio dataset contains sounds from cars. Although people may perceive these as related due to their learned concepts, the model has no such concepts. From the model's perspective, the audio and images are completely unrelated. Furthermore, to ensure irrelevance, we avoid using labels during pretraining and employ an MAE-style pretraining. Consequently, without any labels, the model does not even know that the sounds are \"sounds from cars\" or that the images are \"cars,\" so it cannot establish any relation between the images and audio.\n\n**the reasoning behind this enhancement in performance remains unexplained**\n\nThough completely explaining the phenomena (i.e., transformer benefits from irrelevant data from another modality) is beyond the scope of this paper, which may require the research community to gain a deeper understanding of the black box of deep learning. We have some preliminary explanations and intuitions - some modality-agnostic knowledge, which is about general sequence-to-sequence modeling, exists in transformers. We stated this in the paper (\"In other words, apart from the obvious modality-specific knowledge acquired through training on a specific modality, we seek the modality-complementary knowledge of sequence-to-sequence modeling in transformers and will show that it does exist\"). We understand that the reviewer may concern that the specific form and emergence of such  \"modality-agnostic knowledge\" also needs to be explained, which are discussed as follows.\n\nFor example, while a transformer is being pretrained with MAE, it learns both (ability A) how to understand and reconstruct the image and (ability B) how to generally transform the tokens from the lower-level patterns to a higher level without assuming they originally come from images. Meanwhile, as another transformer is being pretrained with audio data, it learns both the different \"ability A\" for audio and similar \"ability B\".\n\nOur experiments verified that \"ability B\" does exist. But it is difficult to formally define it. We reckon that it should be the ability to capture and understand the common structures of data of all modalities - the abstract hierarchy. A model for image recognition gradually extracts textures from lines, colors, and angles, then recognizes components of objects from textures and shapes. Similar hierarchical structures exist in data of other modalities, and the ability to extract information of higher hierarchy from lower-level tokens on a certain modality should be similar to the ability on another modality.\n\nGenerally, this paper is aimed at showing these intriguing phenomena and a simple yet effective method as pioneering research. Though formally defining and mathematically proving the concepts seem infeasible for the research community's latest understanding of the black box nature of deep learning, we believe it is of vital importance to make the community aware of such phenomena and our early exploration, which may deepen the community's understanding of transformers, pretraining, and multimodal learning.\n\n(please see our response to Weakness 2 in the next comment)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699708606161,
                "cdate": 1699708606161,
                "tmdate": 1699708606161,
                "mdate": 1699708606161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uP4WDpipfz",
                "forum": "CppEmee0u6",
                "replyto": "hAJ7At3Lyf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8Bi6 (weakness 2)"
                    },
                    "comment": {
                        "value": "(This is the second response. Please see our response to Weakness 1 in the last comment)\n\nWeakness 2.**\"Numerous additional parameters were introduced\"**\n\n**Clarification**: We would like to clarify two points: **(1)** We do not introduce any extra structures or parameters into the resultant model. We only temporarily use some additional parameters during training, which are merely aimed at changing the parameterization of the existing parameters. Such parameters can be equivalently eliminated so that the resultant model has the same number of parameters and identical structure to the original model, with the only difference being its improved performance. **(2)** Even though we use additional parameters only during training, we have verified that the performance improvements are not merely due to the more training-time parameters. The reviewer commented that we did not conduct any experiments to investigate this, which is not true. \n\n**(1) We do not introduce any extra structures or parameters into the resultant model.** \n\nThe additional parameters during training are merely a temporarily changed parameterization of the existing parameters. To make this clear, we first explain the concept of parameterization in neural networks. For example, given an arbitrary fully connected layer (FC) in a neural network, assume its numbers of input features and output features are C and D, respectively, it requires C x D parameters to finish its computation (linearly mapping the C-dimensional input to D-dimensional output). We refer to such C x D parameters as its essential parameters.\n\n**Vanilla parameterization**: The most natural way to represent C x D parameters is using a C x D matrix, so \"using a matrix to represent the parameters of an FC layer\" is referred to as a \"parameterization\". Since it is the most naive manner, we refer to it as the vanilla parameterization. In this parameterization, let x and y be the inputs and outputs and W be the matrix, respectively, the computation of this FC layer can be represented by y=xW.\n\n**Our proposed parameterization**: The computation of this FC layer is changed to y=x(W+\u03bbW\u2032). In other words, the parameterization is changed from \"using a matrix to represent C x D parameters\" to \"using two matrices and a scalar to represent C x D parameters\". This can be seen as a method to **\"merge\" the auxiliary model into the current model being trained**. But we would like to note that the FC layer still has the same number of essential parameters (since it still maps the same inputs to the same shapes of outputs), and only the parameterization changes. This parameterization is designed in this way because we want to equivalently implement the block shown in the middle of Figure 2. The reviewer may concern that the training-time model will require 2x layers and computations as the original model, which seems to be indicated by the middle figure in Figure 2. While, as we explained in the Introduction and caption of Figure 2, it is merely an abstract high-level idea, which is **efficiently** and **equivalently** realized by merely changing the parameterization (the right figure in Figure 2). In summary, the model being trained has the same number of linear layers as the original model, and **each layer merely needs to compute W+\u03bbW\u2032 before linearly mapping x to y**. \n\n**Convert parameterization**: After training, the eventual weight matrix W^ is derived by W+\u03bbW\u2032 so that the parameterization becomes the same as the original model. We only save this matrix. For inference, we simply construct a regular model and load the converted weights, so our method does not introduce additional parameters.\n\n**(2) Even though we use additional parameters only during training, we have verified that the performance improvements are not merely due to such extra parameters.**\n\nThe results are reported in Table 5 (ImageNet accuracy with changed order of auxiliary weights or fewer pretraining epochs). We observe that changing the order of auxiliary weights and using not fully trained auxiliary weights result in lower accuracy than the fully trained auxiliary weights. If the performance improvements were merely due to more training-time parameters, using more parameters\u2014even if they were not fully trained or randomly initialized\u2014would result in the same performance. Since this is not the case, we can confirm that the performance improvements are not merely due to more training-time parameters.\n\nWe will enhance the description of relevant sections in the revised version. For example, in Section 3.4.1, we will further analyze the results and add the discussions above.\n\nWe appreciate the reviewer's constructive feedback and hope that our clarifications on the misunderstandings will address the concerns. \n\nWe kindly request the reviewer to reconsider the evaluation in light of these explanations.\n\nBest Regards,\nAuthors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699708950076,
                "cdate": 1699708950076,
                "tmdate": 1699708950076,
                "mdate": 1699708950076,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ixVNocmmJa",
            "forum": "CppEmee0u6",
            "replyto": "CppEmee0u6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1751/Reviewer_hWyf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1751/Reviewer_hWyf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a cross-modal re-parameterization method to investigate the usage of irrelevant data to improve the overall performance of models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work present a wide study on multiple modalities of data, as well as tasks, which I think it's a contribution to the community. Moreover, the cross-modal reparameterization seems simple and straightforward yet effective, compared to largely pretrained MAE. Overall, the paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "It would be great or to visualize the intermediate representation/weights with or without the re-parameterization method to see how it shifts. Also, does the re-parameterization also help the performance of the irrelevant dataset?"
                },
                "questions": {
                    "value": "1. Can the authors give a rationale or even guess why including irrelevant data works?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1751/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785834247,
            "cdate": 1698785834247,
            "tmdate": 1699636104036,
            "mdate": 1699636104036,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SmEjDXX3A9",
                "forum": "CppEmee0u6",
                "replyto": "ixVNocmmJa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1751/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hWyf"
                    },
                    "comment": {
                        "value": "Thank you for appreciating the novelty, simplicity, and effectiveness of our method! \n\nWe did not visualize the intermediate representation or weights because there is yet no widely accepted manner to visualize linear layer's weights or tokens in transformers. In fact, such visualization tools usually make sense with some input samples but show almost no changes with some other samples, and we do not want to do cherry-picking. The answer to the first question \"does the re-parameterization also help the performance of the irrelevant dataset\" is yes, because the improvements are bi-directional, as shown in Figure 3. For example, image improves point cloud b 1.8, and ponit cloud improves image by 0.6.\n\nAnswer to the question:  \n\nYes, as we stated in the paper (\"In other words, apart from the obvious modality-specific knowledge acquired through training on a specific modality, we seek the modality-complementary knowledge of sequence-to-sequence modeling in transformers and will show that it does exist\"), we verified that some modality-agnostic knowledge, which is about general sequence-to-sequence modeling, exists in transformers.\n\nFor example, while a transformer is being pretrained with MAE, it learns both (ability A) how to understand and reconstruct the image and (ability B) how to generally transform the tokens from the lower-level patterns to a higher level without assuming they originally come from images. Meanwhile, as another transformer is being pretrained with audio data, it learns both the different \"ability A\" for audio and similar \"ability B\". \n\nOur experiments verified that \"ability B\" does exist. But it is difficult to formally define it. We reckon that it should be the ability to capture and understand the common structures of data of all modalities - the abstract hierarchy. A model for image recognition gradually extracts textures from lines, colors, and angles, then recognizes components of objects from textures and shapes. Similar hierarchical structures exist in data of other modalities, and the ability to extract information of higher hierarchy from lower-level tokens on a certain modality should be similar to the ability on another modality.\n\nHowever, though we have demonstrated that such an ability does exist, we cannot mathematically define it due to the black-box nature of deep learning. In fact, almost every concept we used in the paragraph above has no formal definition in the deep learning literature (e.g., we are still unable to calculate the degree of hierarchy given the values of tokens or ensure if a feature is on a higher level than another feature). Generally, this paper is aimed at showing these intriguing phenomena (i.e., transformer benefits from irrelevant data from another modality) and a simple yet effective method as pioneering research. Completely explaining the phenomena is beyond the scope of this paper, which may require the research community to gain a deeper understanding of the black box of deep learning.\n\nThank you again for appreciating our work, and we would be grateful if you could raise the score.\n\nPlease let us know if there is anything we can do to convince you further raise the score."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1751/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699699842517,
                "cdate": 1699699842517,
                "tmdate": 1699699842517,
                "mdate": 1699699842517,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]