[
    {
        "title": "Federated Offline Policy Learning with Heterogeneous Observational Data"
    },
    {
        "review": {
            "id": "RGbkhiaaEA",
            "forum": "Ki39vo5x1T",
            "replyto": "Ki39vo5x1T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7156/Reviewer_38dU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7156/Reviewer_38dU"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of learning personalized policies on observational data collected from heterogeneous multiple sources. To ensure privacy and other data safety requirements, this paper studies the problem under a specific federated setting with a central server that collects no raw data from individual data sources. \n\nFirst, based on a federated averaging algorithm, this paper proposes a policy learning algorithm that abides by the federation requirement. \n\nThen a regret analysis is provided for the algorithm, which considers two notions called global regret and local regret. For both notions, finite sample regret upper bounds depending on quantified heterogeneity are presented. \n\nFinally, experimental results verify the dependence of the regret on the client heterogeneity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Significance**: this paper studies offline learning under heterogeneous data sources, an important problem setting in machine learning\n\n**Quality**: the quality of the paper is good. Definitions are introduced without ambiguity; theoretical results looks solid to me; experimental details are given.\n\n**Originality**: this paper considers the federation under the problem setting, which I deem as original\n\n**Clarity**: this paper is very well written and easy for the readers to follow."
                },
                "weaknesses": {
                    "value": "I did not detect any major technical flaw or major weakness in this paper. \n\nStill, I have a few questions I hope the author can address. Please see the Questions session. \n\nFurthermore, I think this paper, as a theoretical work, would significantly benefit from adding a sketch of proof for its main results."
                },
                "questions": {
                    "value": "I think the problem setting is original. However, it is unclear to me what the technical novelty of this paper is. \n\nSpecifically, in the analysis/proof of the theorems, does there exist any technical challenge and how are they resolved? \n\nAny novel trick adopted?\n\nIt would be great if the author could elaborate on this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698291572050,
            "cdate": 1698291572050,
            "tmdate": 1699636847891,
            "mdate": 1699636847891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BL0HfFhDuW",
                "forum": "Ki39vo5x1T",
                "replyto": "RGbkhiaaEA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our sincere gratitude to the reviewer for dedicating their time to our submission and for their positive feedback. We are committed to addressing their questions thoughtfully.\n\n*Answering Questions:*\n\nWe appreciate the reviewer's suggestion regarding the inclusion of a proof sketch in the main paper, given our emphasis on the theoretical contributions. While we did provide a brief general proof sketch in Appendix C.1.4 due to space constraints, we acknowledge the potential value of having such a sketch in the main paper could better highlight our theoretical contributions. Although the devil is in the details, we attempt to provide an overview of some of the high-level technical challenges and novelty.\n\nIn the standard approach for proving finite-sample regret bounds in offline policy learning, the focus is on establishing uniform concentration around a proper notion of empirical complexity, which is then further bounded by class-dependent vanishing rates (as seen in Zhou et al. 2023). Typically, offline policy learning in the standard setting involves bounding the Rademacher complexity of an appropriate policy value-based function class. However, this approach is not applicable to our scenario where the data may not come from the same source distribution. In our proof, we draw inspiration from the work on empirical risk bounds in multiple-source supervised learning settings, particularly Mohri et al. (2019), to identify the suitable notion of complexity\u2014namely, the weighted Rademacher complexity of a policy value function class. Bounding the weighted Rademacher complexity required a nuanced understanding of how source heterogeneity, captured by skewness, influences the analysis. This required introducing the local data size scaling assumption stated in Assumption 2 to establish a sharper upper bound on the constant in Talagrand\u2019s inequality to scale as $O(\\sqrt{\\text{skewness}/n})$. This is similar to how the primary theoretical contribution of the work of Zhou et al. (2023) in terms of regret bounds was to refine a similar constant in the analysis for the standard multi-action offline policy learning. Moreover, we note that we were able to establish rates with respect to the total sample size, rather than some other softer quantity of the sample sizes, such as the average or the minimum.\n\nWhile Mohri et al. (2019) provided a starting framework for a multiple-source analysis in supervised learning, the proof techniques for establishing class-dependent uniform concentration results in offline policy learning are typically more involved than those for empirical risk bounds in supervised learning. Our bounds necessitate more complex Dudley-type chaining arguments with applications of Talagrand\u2019s inequalities, as evident in the proof of Proposition 2 in Appendix C. Moreover, our bounds required additional assumptions and understanding of doubly robust machine learning proof techniques to asymptotically bound the remaining approximation error, as seen in the proof of Proposition 4 in Appendix C.\n\nFurthermore, we established bounds for the notion of local regret, unique to our problem setting. This insight arises from recognizing the mismatch between global server-level performance and local client-level performance. We derive a local regret bound dependent on measures of distribution shift between clients, providing valuable insights into the value of information in heterogeneous client participation and how exactly heterogeneity affects policy performance for any given client. This exact quantification is highlighted in our Theorem 3 that decomposes the sources of heterogeneity at the population, environment, and treatment level. We also point to Theorem 4 in Appendix D.3 for an alternative local regret bound that does not require bounded inverse propensity weighted scores.\n\n*References:*\n\nZhou, Zhengyuan, Susan Athey, and Stefan Wager. \"Offline multi-action policy learning: Generalization and optimization.\" Operations Research 71.1 (2023): 148-183.\n\nMohri, Mehryar, Gary Sivek, and Ananda Theertha Suresh. \"Agnostic federated learning.\" International Conference on Machine Learning. PMLR, 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191068017,
                "cdate": 1700191068017,
                "tmdate": 1700191068017,
                "mdate": 1700191068017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "slObWobDuy",
                "forum": "Ki39vo5x1T",
                "replyto": "BL0HfFhDuW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7156/Reviewer_38dU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7156/Reviewer_38dU"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you very much for responding to my questions! \n\nI really appreciate the explanation on the technical novelty of the paper. I would suggest adding the explanation in the revision of the paper, with detailed explanation and probably with mathematical details.\n\nBest, \nAnonymous Reviewer"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547924893,
                "cdate": 1700547924893,
                "tmdate": 1700547924893,
                "mdate": 1700547924893,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JUt4ZZMrwQ",
            "forum": "Ki39vo5x1T",
            "replyto": "Ki39vo5x1T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7156/Reviewer_D9wE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7156/Reviewer_D9wE"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of learning personalized policies from heterogeneous data sources in the federated setting.\nThey proposed a federated policy learning algorithm that averages locally trained policies with doubly robust policy evaluation. And, they provided finite-sample analysis on global and local regret bounds in terms of a mixture of distribution of clients and a relative distribution shift to all other clients, respectively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work provides finite-time regret upper bounds of the proposed policy learning algorithm in global and local perspectives, which characterize the effect of client skewness and client heterogeneity on global policy learning and individual policy learning of clients, respectively.\n2. This work empirically demonstrated the effect of client heterogeneity on federated policy learning and suggested a skewed mixture for global policy training to overcome the performance degradation due to distribution shift."
                },
                "weaknesses": {
                    "value": "1. It seems that the proposed algorithm is an application of FedAvg to CSMC. I wonder if there are some special challenges when extending offline policy learning algorithms to the federated setting with FedAvg, which have not been addressed in other federated learning or federated RL literature.\n2. The regret analysis holds only when the algorithm converges to the optimal policy, which is not always guaranteed. In the paper, the authors claimed that it is still possible to achieve optimal policies via some additive term and appropriate choice of policy class, but it is vague and not convincing enough. It would be nice if you could provide further clarifications on the solution that enables the algorithm to achieve the optimal policy for general policy classes beyond linear policy classes.\n3. It seems that Assumption 1-(c) requires a full exploration of all actions, which is quite strong given that there are some offline RL works suggesting that full coverage of state-action space is not necessary.\n4. In the experiments, the algorithm was demonstrated in very limited settings. I wonder if this could be applied to more general and realistic settings. Also, it would be nice if you could compare the performance with other baseline algorithms."
                },
                "questions": {
                    "value": "See the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7156/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7156/Reviewer_D9wE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614392014,
            "cdate": 1698614392014,
            "tmdate": 1699636847763,
            "mdate": 1699636847763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bVjOPCHfnQ",
                "forum": "Ki39vo5x1T",
                "replyto": "JUt4ZZMrwQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for dedicating their time to our submission. We are committed to addressing their concerns and questions thoroughly.\n\n*Addressing Weaknesses:*\n\n1)\nIndeed, the algorithmic aspect of our work involves adapting federated averaging to offline policy learning with parametric function classes. The purpose of this portion was to present a feasible approach to offline policy learning in a federated setting that supplements our theoretical results. However, we do believe that there are additional questions at the intersection of federated learning and offline policy learning that merit further exploration.  For instance, the doubly robust offline policy learning approach employs a two-stage process of estimating nuisance functions before defining the optimization objective, and we simply employed a local nuisance function estimation procedure. Intriguing questions arise on aggregate nuisance parameter estimation among similar clients and how it could enhance performance. Furthermore, our FedAvg-based approach currently only applies to parametric policy classes. It would be valuable to develop a federated offline policy learning optimization procedure for non-parametric policy classes, such as decision trees, commonly used in real-world applications for their interpretability in decision-making.\n\n\n2)\nThe reviewer astutely pointed out that the optimal policy in the optimization procedure is not always guaranteed, with exceptions for specific cases like linear policy classes as examined in our experiments. However, our analysis can be readily extended to encompass scenarios where the optimal policy is not necessarily attainable, introducing the possibility of approximation errors. In the concluding paragraph of Section 6, we explain how this approximation error simply becomes an additional additive term in the regret bounds, that is, $\\text{Regret bound}+\\epsilon$, where $\\epsilon$ represents the policy value optimality gap\u2014indicating how far the policy value of the learned policy deviates from that of the optimal policy. While this approximation error may not necessarily diminish with increased data, with a judicious choice of policy class and the corresponding optimization procedure, the optimality gap can be rendered insignificant or of a similar order of magnitude as other terms in the regret bounds. This holds particularly true in moderately to highly heterogeneous environments, as the local regret bound contains an additive distribution shift term that is also inherently irreducible and captures the extent of heterogeneity.\n\n    \n3)\nWe agree with the reviewer's observation that the overlap assumption (Assumption 1c) is strong. We note that this assumption is standard and commonly adopted in the offline policy learning literature to facilitate the identification of causal effects in observational studies. However, we agree with the reviewer's point that it may not be necessary as recent work has shown that, under a pessimism principle, overlap only under the optimal policy is sufficient. This insight is discussed in Section 3.4, following Assumption 1. The decision to adopt the more stringent standard assumption was made to simplify our analysis and maintain a focus on our contributions on the effects of data heterogeneity on policy learning.\n\n    Nevertheless, we acknowledge the intriguing questions that arise when considering what the pessimism principle would grant in our setting. Specifically, would it be necessary to have coverage under the locally optimal policy for each data source, or is coverage under the globally optimal policy sufficient? Additionally, how do the mixture weights impact the satisfaction of this assumption? We believe our work provides an initial study that opens up numerous follow-up questions in this direction.\n\n    \n4)\nGiven that our algorithm builds on FedAvg, extending its application to more general settings, such as more general policy classes or cross-device scenarios with thousands of devices, is relatively straightforward. As for baseline comparisons in our empirical evaluation, we compared our approach with the baseline of standard offline policy learning where all the data is assumed to be consolidated into a single source. As our work is pioneering in studying offline policy learning with bandit feedback data in a federated setting, identifying direct comparison baselines relevant to our results proved challenging. Moreover, the objective of our experiments was to contrast the policies learned in a federated setting with those in a non-federated setting. While we stress that our primary contribution is theoretical rather than empirical, we acknowledge the potential for further development in the empirical evaluations. Your feedback is valuable, and we appreciate your understanding of the balance we aimed to strike between the theoretical and empirical aspects of our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190733912,
                "cdate": 1700190733912,
                "tmdate": 1700191146335,
                "mdate": 1700191146335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i9oXdIW1L6",
            "forum": "Ki39vo5x1T",
            "replyto": "Ki39vo5x1T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7156/Reviewer_eGKx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7156/Reviewer_eGKx"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of learning personalized decision policies from observational bandit feedback data across multiple heterogeneous data sources. In the federated setting, a central server aims to train a policy on data distributed across the data sources without directly accessing the raw data. The paper proposed a policy learning algorithm amenable to federation, based on the\nfederated averaging algorithm with local model updates provided by online cost-sensitive classification oracles.  Finite-sample upper bounds are provided for a notion of global regret, and local regrets for each agent. Empirical local and global regret bounds are compared across different experimental settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Overall the paper is written with a good clarity. The authors studied a practically significant problem of learning personalized decision policies from multiple heterogeneous data sources, and demonstrated that the proposed algorithm can be extended to the federated setting. \n- The assumptions and the regret upper bound analysis were detailedly described. In particular the theoretical analysis on local regret was a good compliment to the analysis on global regret, and shows their discrepancy due to client heterogeneity. \n- The local and global regret bounds were compared empirically via simulated data."
                },
                "weaknesses": {
                    "value": "- The upper bounds were based on a few detailed data assumptions, such as local ignorability, unconfoundedness, and overlap. Although the paper mentioned that some of the assumptions can potentially be relaxed, there is a lack of details on the discussion, and which assumption may not be relaxed fundamentally. \n- In the theoretical analysis part (section 4-5), the main innovation part for the algorithm / estimator design and regret analysis in comparison to prior works was not clearly highlighted. \n- The empirical evaluation did not include any comparisons with other baselines, or any real dataset, and the heterogeneous setting was fairly simply constructed."
                },
                "questions": {
                    "value": "- Can the nuisance parameters be learnt jointly instead of being required to be separately known or estimated in the policy value estimates?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625289168,
            "cdate": 1698625289168,
            "tmdate": 1699636847645,
            "mdate": 1699636847645,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V2O6lH0WOZ",
                "forum": "Ki39vo5x1T",
                "replyto": "i9oXdIW1L6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for dedicating their time to our submission. We are committed to addressing their concerns and questions thoroughly.\n\n*Addressing Weaknesses:*\n\n1)\nDue to space limitations, our discussion regarding the necessity of certain assumptions may appear concise. Another contributing factor to this condensed discussion is that these assumptions are relatively standard within the offline policy learning literature for ensuring consistent policy value estimation. The cited works in this section serve as references indicating how some of these assumptions have been relaxed in recent literature. We acknowledge the need for more detailed exploration of these assumptions in future revisions.\n\n2)\nOur primary contribution lies in the adaptation of offline policy learning to novel problem settings, specifically in the context of learning from multiple heterogeneous datasets and federated learning. The theoretical results presented in our work almost directly correspond with established findings in offline policy learning within the standard setting involving a single data source. We made efforts to underscore this parallel throughout the theoretical results section and the paper as a whole. Acknowledging the constraints posed by limited space, we recognize that our discussion might not have fully elaborated on this aspect. In response to your feedback, we have incorporated additional citations and context to enhance clarity. However, the space constraints still limit our ability to delve extensively into this matter. We appreciate the reviewer's input and we are open to addressing any specific concerns regarding comparisons with prior work here. We also kindly encourage you to refer to our responses to other reviewers' comments, where we delve more deeply into the question of the novelty of our work.\n\n3)\nIn our empirical evaluation, we compared our approach with the baseline of standard offline policy learning. This comparison involved assessing our method against the scenario where all the data is assumed to be consolidated in a single source, allowing for the application of standard offline policy learning methods. Given that our work is pioneering in the study of offline policy learning with bandit feedback data in a heterogeneous multiple-source and federated setting, we faced a challenge in identifying direct comparison baselines that we deemed relevant to our results. The objective of our experiments was to contrast the policies learned in a federated setting with those in a non-federated setting.\n\n    It's crucial to emphasize that our primary contribution lies in the theoretical domain rather than the empirical one. While our focus is on advancing theoretical understanding, we acknowledge that there is potential for further development in the empirical evaluations. We appreciate your understanding of the balance we aimed to strike between the theoretical and empirical aspects of our work.\n\n*Answering Questions:*\n\n1. Indeed, if the central server can group similar clients, it can learn the nuisance parameters more effectively. The doubly robust approach relies on a two-stage approach of estimating nuisance functions prior to forming the optimization objective. We relied on a simple local nuisance function estimation procedure. However, there are interesting questions regarding the aggregate nuisance parameter estimation among similar clients and how that may improve performance."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190696426,
                "cdate": 1700190696426,
                "tmdate": 1700191125048,
                "mdate": 1700191125048,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rwXsjkCOTY",
            "forum": "Ki39vo5x1T",
            "replyto": "Ki39vo5x1T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7156/Reviewer_oHDw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7156/Reviewer_oHDw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an federated optimization procedure for off-policy learning over heterogeneous data sources, where the observational data is collected by multiple clients using different behaviour policies and stored only locally.\nSpecifically, the central server needs to maximize the doubly robust policy value estimator (Zhou et al. 2022b) over the (parametric) policy space, without transferring the raw observational data. In the proposed procedure, this is achieved by letting each client locally compute its model update by calling an online const-sensitive multi-class classification (CSMC) oracle and then the server performs a global update via a weighted average over the local model updates.\n\nTo the best of my knowledge, this is the first work that studies off-policy learning under federated setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem of off-policy learning in federated setting is well-motivated, and the authors have provided a solution with regret guarantee."
                },
                "weaknesses": {
                    "value": "1. My main concern is the technical novelty, and I'd appreciate if the authors can provide more clarification on the contribution compared with the existing work mentioned below.\n\nBased on my understanding, the main difference of this paper, compared with existing off-policy learning method using doubly robust estimator, e.g., Zhou et al. (2022b), lie in the optimization oracle. i.e., this paper needs to solve the CSMC problem over multiple heterogeneous clients to update the policy, instead of in a centralized setting. However, I am not sure if this has led to any technical challenge in obtaining the global regret bound in Theorem 1.\n\n2. With CSC, we typicaly have a non-concave non-convex objective function to optimize, which makes finding the policy that maximizes Eq 8 difficult. Therefore, I expected the regret analysis to cover the situation where the FedAVG-CSMC procedure can only provide policy with certain approximation error. Moreover, even if the objective function is easy to optimize, it still seems to be unrealistic to assume we can obtain the exact maximizer of Eq 8 under federated setting, as this requires infinite number of iteration/communication rounds.\nI'd appreciate it if the authors can provide more insights on how the current analysis can be extended to allow for approximation error of Eq 8."
                },
                "questions": {
                    "value": "In Section 3.2, notations like $X^{C}, Y^{C}$ are not formaly defined.\n\nThe author mentioned that Assumption 3 can be easilly satisfied under regularity assumption. Can the authors provide a more formal description of the regularity assumption?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698889024480,
            "cdate": 1698889024480,
            "tmdate": 1699636847535,
            "mdate": 1699636847535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gFLVYfQKa8",
                "forum": "Ki39vo5x1T",
                "replyto": "rwXsjkCOTY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for dedicating their time to our submission. We are committed to addressing their concerns and questions thoroughly.\n\n*Addressing Weaknesses:*\n\n1)\nOur work differs from prior work in two main ways. First, we explore offline policy learning (Zhou et al. 2023) under a novel setting of learning from multiple heterogeneous data sources. Prior studies, such as those by He et al. (2019) and Kallus et al. (2021), have considered a related, yet more restricted, problem of offline policy learning from multiple logging policies, where multiple datasets are also leveraged, but the underlying environment is assumed to be the same and only the action sample strategy differs across datasets. The more general problem setting we consider in this work has not been investigated, possibly due to the challenge of understanding the tradeoffs of a policy learned from diverse provenance. In our study, we are able to exactly quantify the impact of such heterogeneity on downstream performance for any target client by characterizing finite-sample upper bounds on suitable notions of regret. Note that this main result is independent of whatever optimization oracle is used to solve the policy learning problem we consider.\n\n    Second, we considered this policy learning problem in a setting where such heterogeneities may naturally arise in practice: in a federated setting. In federated settings, there tends to be less coordination on data collection procedures among clients (e.g., institutions, devices) compared to centralized learning settings. Consider a consortium of hospitals that conduct randomized control trials on a new treatment as an example; since these hospitals may not fully communicate their idiosyncratic treatment procedures, each hospital might conduct its unique trial on distinct populations, treatment formulations, and treatment assignment mechanisms. How should the central healthcare policymaker account for this heterogeneity across clients when training a treatment policy in a federated manner? With increasing privacy regulations, these coordination challenges may become more common, requiring federated policymakers to design policy learning procedures adaptable to such inherent heterogeneities for the intended purpose.\n\n    Our work demonstrates a practical way of adapting the federated learning framework to policy learning while accounting for underlying heterogeneities. As a first approach to this novel problem setting, we relied on the most well-known federated learning procedure for parametric models: federated averaging. We are not claiming that we are introducing a novel algorithm to the federated learning literature. We are simply adapting a well-known federated learning procedure to solve our optimization problem in a setting of practical interest for the above reasons. We also emphasize that the optimization problem is somewhat orthogonal to the technical analysis we conducted to arrive at our regret bounds. The regret bounds hold for the solution to the optimization problem and our optimization procedure demonstrates the feasibility and simplicity of solving this problem in the federated setting. However, that being said, we believe that the adaptation of FedAvg to offline policy learning is not entirely trivial and there are interesting questions at the intersection of federated learning and offline policy learning that warrant further exploration. For instance, the doubly robust offline policy learning approach employs a two-stage process of estimating nuisance functions before defining the optimization objective, and we simply employed a local nuisance function estimation procedure. Intriguing questions arise on aggregate nuisance parameter estimation among similar clients and how it could enhance performance. Furthermore, our FedAvg-based approach currently only applies to parametric policy classes. It would be valuable to develop a federated offline policy learning optimization procedure for non-parametric policy classes, such as decision trees, commonly used in real-world applications for their interpretability in decision-making."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190443913,
                "cdate": 1700190443913,
                "tmdate": 1700190568713,
                "mdate": 1700190568713,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]