[
    {
        "title": "Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization"
    },
    {
        "review": {
            "id": "EJuPfl3vaD",
            "forum": "0t1O8ziRZp",
            "replyto": "0t1O8ziRZp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_pgL7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_pgL7"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a new method for finding optimal synthesis recipes for unseen netlists. They proposed three methods, one based on MCTS and a trained policy on training netlist (MCTS+L) and the other, and more superior, is using MCTS and a trained policy with a variable to control the reliance on the trained policy or the online MCTS search (ABC-RL). The variable \\alpha is determined based on a nearest neighbor search to determine if the netlist is close to the training netlists or not. The authors split the dataset of netlists into train, validation, and test sets and used the train to train the policy and used the validation set to set the hyperparameter that controls \\alpha. The experimental results show that the ABC-RL can outperform the other methods and SOTA in most of the test netlists and can achieve the best goe-mean performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Extensive experimental results.\n  - The authors provided comparisons with multiple baseline including Online-RL, Simulated Annealing, MCTS, and MCTS+L and MCTS+L+FT.\n  - The study on training the policy on specific benchmarks was helpful showing the effect of \\alpha and closeness of netlist to the training dataset.\n  - They perform an ablation study on the architecture of the policy to determine the impact of the transformer architecture on the performance.\n- The authors motivated the need for a variable that can control the effect of policy vs MCTS very well by an example in the Introduction Section.\n- The paper is well written and organized and can be followed easily by non-experts. \n- The related literature has been sufficiently reviewed and cited."
                },
                "weaknesses": {
                    "value": "- The idea of using \\alpha was interesting and novel to the best of my knowledge, however it is a simple and small contribution."
                },
                "questions": {
                    "value": "- It would be great if the authors can also provide the results for the rest of the benchmark netlists to ensure that the similar performance gains hold up for the training and validation sets.\n- In Section 2.4, in definition of \\sigma_{\\delta_{th}, T}(z), replace \\teta with \\delta_{th}."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698103846253,
            "cdate": 1698103846253,
            "tmdate": 1699636957743,
            "mdate": 1699636957743,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IkXws3Z4gE",
                "forum": "0t1O8ziRZp",
                "replyto": "EJuPfl3vaD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to pgL7"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed and insightful feedback. We address the questions raised:\n\nQ1) *It would be great if the authors can also provide the results for the rest of the benchmark netlists to ensure that the similar performance gains hold up for the training and validation sets.*\n\n**Response:**\n\nThank you for your recommendation. To provide a comprehensive overview, we applied ABC-RL to both training and validation netlists, and the performance improvements are detailed in ***Appendix D7 (Table 9 and 10)***. On MCNC circuits, ABC-RL achieved a geometric mean ADP reduction of **17.84%**, surpassing MCTS, which resulted in a **15.39%** reduction. For EPFL arithmetic circuits, ABC-RL demonstrated an **18.18%** geometric mean reduction compared to MCTS, which yielded a **15.55%** ADP reduction. Finally, on EPFL random control circuits, the geometric mean ADP reduction was **6.19%** and **5.87%** for ABC-RL and MCTS, respectively. \n\n\nQ2) *In Section 2.4, in the definition of $\\sigma_{\\delta_{th}, T}(z)$, replace $\\theta$ with $\\delta_{th}$.*\n\n**Response:**\n\nThanks for pointing out this oversight. We have replaced $\\theta$ with $\\delta_{th}$ in our updated manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263515574,
                "cdate": 1700263515574,
                "tmdate": 1700297899259,
                "mdate": 1700297899259,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U6r8NQ4aGi",
            "forum": "0t1O8ziRZp",
            "replyto": "0t1O8ziRZp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_yxFf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_yxFf"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new logical synthesis optimization sequence method ABC-RL based on MCTS and pertained policy. Unlike previous works, ABC-RL can compute the similarity of the new circuit with previous circuits, thus determining how much experience will be used. Experiments show that ABC-RL achieves SOTA optimization results in most circuits."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea that the degree of retrieval is determined by the similarity function is interesting, and it shows the advanced performance in experiments.\n\n2. The experiments are extensive.\n\n3. The paper is well-organized and easy to read."
                },
                "weaknesses": {
                    "value": "1. Some other ML method baselines are not included. For example, the DRiLLS [1] results are not included in the paper, but it is an important method recently.\n\n2. The retrieval performance is not well reported. For example, it should give us examples of each circuit\u2019s nearest neighbor circuit and similarity factor. \n\n3. ChiPFormer [2]  is an RL placement method with a pretrained policy and should be included in related work.\n\n[1] Hosny, Abdelrahman, et al. \"DRiLLS: Deep reinforcement learning for logic synthesis.\" 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 2020.\n\n[2] Lai, Yao, et al. \"ChiPFormer: Transferable Chip Placement via Offline Decision Transformer.\" (2023)."
                },
                "questions": {
                    "value": "1. Could you compare ABC-RL and DRiLLS methods?\n\n2. As weakness 2, could you give each circuit\u2019s nest neighbor circuit and similarity factor?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Reviewer_yxFf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698332810574,
            "cdate": 1698332810574,
            "tmdate": 1699636957640,
            "mdate": 1699636957640,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xoxGtcFfEP",
                "forum": "0t1O8ziRZp",
                "replyto": "U6r8NQ4aGi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to yxFf"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful feedback and address the concerns raised:\n\nQ1) *Some other ML method baselines are not included. For example, the DRiLLS [1] results are not included in the paper, but it is an important method recently.*\n\n**Response:**\n\nThank you for the excellent suggestion. In the updated draft, we have cited and evaluated against DRiLLS as well, and reported results in ***Table 2***. DRiLLS is competitive with Online-RL (both have a **16.1%** ADP reduction), but has less ADP reduction compared to our proposed ABC-RL (**25.3%**).\n\nQ2) *The retrieval performance is not well reported. For example, it should give us examples of each circuit\u2019s nearest neighbor circuit and similarity factor.*\n\n**Response:**\n\nThank you for the suggestion. This is actually informative data and we have included it in ***Appendix D.4 (Table 6)***. Several interesting observations can be drawn: first, note that the closest circuits in the training set are often semantically similar (*alu2 for alu4, apex3 for apex1, sqrt for multiplier*), suggesting that the learned embeddings are capturing netlist structure/function. Second, netlists for which we found MCTS+L to underperform MCTS alone, for example, square and cavlc have relatively **high** cosine distances of 0.012 and 0.007, respectively, relative to the $\\delta_{th}=0.007$ in Equation 4.\n\n\nQ3) *ChiPFormer [2] is an RL placement method with a pretrained policy and should be included in related work.*\n\n**Response:**\n\nThank you for pointing out this recent work regarding pre-training offline agents for chip placement problems. We have included the citation in our related work section.\n\n[1] Hosny, Abdelrahman, et al. \"DRiLLS: Deep reinforcement learning for logic synthesis.\" 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 2020.\n\n[2] Lai, Yao, et al. \"ChiPFormer: Transferable Chip Placement via Offline Decision Transformer.\" (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263531147,
                "cdate": 1700263531147,
                "tmdate": 1700667201177,
                "mdate": 1700667201177,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QBna1vIOAd",
            "forum": "0t1O8ziRZp",
            "replyto": "0t1O8ziRZp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_qBwB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_qBwB"
            ],
            "content": {
                "summary": {
                    "value": "Summary:\n This papers proposes ABC-RL method to smoothly change the impact of learned policy in the search objective based on the graph similarity. Similar designs to training examples will have more bias from learned policy, while novel circuit will use more search. Compared to prior method, the propose one outperforms in various designs in terms of area-delay product."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strength: \n\n1.\tThe proposed method is based on good observation that the benchmark has a large diversity, and the learned policy sometimes is not helpful for novel circuits.\n\n2.\tThe proposed method smoothly controls the importance of learned policy using GNN-based graph embedding similarity scores. The variable heuristic synthesis solution from the learned policy is encoded with transformer blocks for better performance.\n\n3.\tIt also runs fast with runtime benefits."
                },
                "weaknesses": {
                    "value": "Weakness:\n\n1.\tThe overall framework basically does test-time augmentation. It assumes pre-trained policy is not generalizable to novel circuits, and by comparing the test example with the training example, it dynamically selects among two search strategies, but in a smooth way. The circuit similarity is a performance proxy for pretrained agent and MCTS method, and use that proxy to predict the weights to ensemble two models. The novelty, in this sense, is limited. Is it possible to combine more synthesis strategies based on a more general performance predictor at test time?\n\n2.\tThe usage of BERT is not well justified. There are other simpler methods to encode variable-length sequences, e.g., RNN. The attention model is also data-hungry during training. Why BERT is the most suitable encoder?\n\n3.\tThe assumption that learned policy cannot generalize to diverse benchmarks is not well supported. If there is generalizable knowledge in circuit representation and synthesis strategies, it should try to improve the generalization of the learned policy by using more data/better algorithms. If this problem in nature is not generalizable or learnable, then it is not necessary to use an RL agent to learn the synthesis strategy at the beginning. The proposed method does not fundamentally explain or solve the learnability of circuit synthesis problems, but rather uses two model ensembles to just cover some in-distribution data with learned model and out-of-distribution examples by search."
                },
                "questions": {
                    "value": "Basically, it is listed in the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698635029659,
            "cdate": 1698635029659,
            "tmdate": 1699636957483,
            "mdate": 1699636957483,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nDOVGeDhzq",
                "forum": "0t1O8ziRZp",
                "replyto": "QBna1vIOAd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official comment to qBwB"
                    },
                    "comment": {
                        "value": "We thank the reviewer for appreciating our motivational observation that in practice benchmarks have large diversity and learned policy sometimes do not help for entirely novel circuits. The method also smoothly controls the recommendation from pre-trained agents while performing MCTS for synthesis recipe generation yielding runtime benefits. We address the reviewers' concerns below.\n\n\nQ1) *The overall framework basically does test-time augmentation. It assumes pre-trained policy is not generalizable to novel circuits, and by comparing the test example with the training example, it dynamically selects among two search strategies, but in a smooth way. The circuit similarity is a performance proxy for pre-trained agent and MCTS method, and uses that proxy to predict the weights to ensemble two models. The novelty, in this sense, is limited. Is it possible to combine more synthesis strategies based on a more general performance predictor at test time?*\n\n\n**Response:**\n\nWe thank the reviewer for the question. With regards to novelty, as far as we are aware, adaptively mixing learning and search using an based on retrieval-guided cosine similarity is a new idea in literature that empirically outperforms several state-of-art methods, while a simpler solution without the judicious choice of how much learning to use (MCTS+L) does not. As such, our contributions include the algorithm to weight the pre-trained and pure search agents, how to set its hyper-parameters, and other policy network architecture contributions that are not in prior work and are key to the performance of our method. \n\nThe reviewer makes an interesting point about ensembling other synthesis strategies. We agree that it might be possible to train a test time predictor that selects within this broader class of strategies, for example, SA+Pred etc. This would be an interesting avenue for future work. We do want to highlight one key point here: our pre-trained and MCTS search agents are not ensembles in the traditional sense: in fact, the pre-trained agent is trained to emulate MCTS search (as in alphaGO etc.), thus providing it a performance boost. As such, one could imagine pre-trained agents used to emulate other online search methods like simulated annealing, thus giving these methods a boost as well. Finally, these learning-augmented-search methods could be ensembled in the more traditional sense.\n\nQ2) *The usage of BERT is not well justified. There are other simpler methods to encode variable-length sequences, e.g., RNN. The attention model is also data-hungry during training. Why BERT is the most suitable encoder?*\n\n**Response:**\n\nThank you for the suggestion; a comparison with an RNN (or LSTM) encoder is a good idea. To test this idea, we replaced our BERT-based encoder with an LSTM encoder and retrained our model. Our detailed results in ***Appendix D.5***. We find that our BERT-based encoder outperforms the LSTM-based encoder. Specifically, ABC-RL using an LSTM recipe encoder achieves **22.6%** geo-mean ADP reduction compared to **25.3%** using the existing BERT-based recipe encoder. The table is reproduced below for your convenience.\n\n| Recipe encoder | C1   | C2   | C3   | C4   | C5   | C6   | C7   | C8   | C9   | C10  | C11  | C12  | A1   | A2   | A3   | A4   | R1   | R2   | R3   | R4   | Geo. mean |\n|----------------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|-----------|\n| Naive          | 17.0 | 16.5 | 14.9 | 13.1 | 44.6 | 16.9 | 10.0 | 23.5 | 16.3 | 18.8 | 10.6 | 19.0 | 36.9 | 51.7 | 9.9  | 20.0 | 15.5 | 23.0 | 28.0 | 26.9 | 21.2      |\n| LSTM-based     | 19.5 | 17.8 | 14.9 | 13.3 | 44.5 | 17.8 | 10.3 | 23.5 | 18.8 | 20.1 | 10.6 | 19.6 | 36.9 | 53.4 | 10.9 | 22.4 | 16.8 | 24.3 | 32.3 | 28.7 | 22.6      |\n| BERT-based     | 19.9 | 19.6 | 16.8 | 15.0 | 46.9 | 19.1 | 12.1 | 24.3 | 21.3 | 21.1 | 13.6 | 21.6 | 36.9 | 56.2 | 14.0 | 23.8 | 19.8 | 30.2 | 38.9 | 30.0 | 25.3      |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263628451,
                "cdate": 1700263628451,
                "tmdate": 1700296814466,
                "mdate": 1700296814466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bpEri9G8j2",
                "forum": "0t1O8ziRZp",
                "replyto": "QBna1vIOAd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to Reviewer qBwB (Part 2)"
                    },
                    "comment": {
                        "value": "Q3) *The assumption that learned policy cannot generalize to diverse benchmarks is not well supported. If there is generalizable knowledge in circuit representation and synthesis strategies, it should try to improve the generalization of the learned policy by using more data/better algorithms. If this problem in nature is not generalizable or learnable, then it is not necessary to use an RL agent to learn the synthesis strategy at the beginning. The proposed method does not fundamentally explain or solve the learnability of circuit synthesis problems, but rather uses two model ensembles to just cover some in-distribution data with learned model and out-of-distribution examples by search.*\n\n**Response:**\n\nWe thank the reviewer for raising this point. We would like to emphasize that ABC-RL never defaults to a pure search solution, but instead chooses the right mix of learning and search on a per-benchmark basis. Emprically, across our test circuits, ABC-RL never sets $\\alpha=1$, i.e., never fully turns off the learned agent, thus ensuring that the learned policy is constantly helping, albeit by different amounts. This is evidenced by the fact that ABC-RL outperforms pure MCTS search on all circuits except on one (for which both results are the same). Therefore, in each of these instances, the learned agent is actually generalizing to test inputs and helping to boost performance over the pure search agent, but by different amounts. In sum, our claim is not that learning does not help (in fact, on average even MCTS+L is better than MCTS), but that the amount of learning should be judiciously selected using our proposed scheme. \n\nFinally, one can imagine a test circuit that is so novel that ABC+RL sets $\\alpha=1$ in which case we would default to search. However, in our view, such an \u201cout-of-distribution\u201d (OOD) circuit by itself would not mean that learning is not useful, given that OOD inputs are routinely encountered across a range of ML applications. Nevertheless, we note that ABC-RL itself does not label test circuits explicitly as either in-distribution or OOD."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700264007184,
                "cdate": 1700264007184,
                "tmdate": 1700296884487,
                "mdate": 1700296884487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CtAotUn5rd",
            "forum": "0t1O8ziRZp",
            "replyto": "0t1O8ziRZp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents ABC-RL, a retrieval-guided RL approach to generate an optimized synthesis recipe for logic synthesis. ABC-RL tunes a weight that adeptly adjusts recommendations from pre-trained agents during testing. Given a circuit with high similarity to the circuits in the training dataset, ABC-RL assigns high weight to the policy by the RL agent. Otherwise, ABC-RL tends to rely more on the default searching strategy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper introduces a retrieval-guided RL agent to search for an optimized synthesis recipe for logic synthesis. ABC-RL selectively leverages the knowledge obtained during training based on the similarity between training and testing samples. This approach can address the issue of RL performance decrease caused by different data distributions. \n\n2.\tThe authors claim that ABC-RL can achieve up to 24.8% QoR improvement and reduce runtime up to 9x."
                },
                "weaknesses": {
                    "value": "1.\tABC-RL calculates the cosine similarity between the embeddings of training and testing samples to determine the similarity score. Therefore, the quality of AIG embeddings is the key to the entire methodology. Unfortunately, the authors do not explore this issue and simply use a 3-layer GCN to do it. It's hard to believe such an approach could work properly. A circuit graph is a lot more complicated than a plain graph, not only it's directed, but more importantly, it has unique functions associated with it. Two AIGs could be very similar in terms of structure but differ significantly in terms of functionalities. Consequently, they may require different synthesis recipes, isn't it?\n\n2.\tThe experiments are conducted with a very small dataset, which only includes 23 netlists for training. This is not convincing. The proposed agent should have seen sufficient circuit designs to come up with a good RL strategy."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759895374,
            "cdate": 1698759895374,
            "tmdate": 1700389951501,
            "mdate": 1700389951501,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "efFxFDY7zy",
                "forum": "0t1O8ziRZp",
                "replyto": "CtAotUn5rd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to reviewer 3ENJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for appreciating ABC-tuning strategy to adjust recommendations from pre-trained agents during test time and address the issue of learned policies sometimes underperforming pure search. We address the reviewer\u2019s primary concerns below. \n\n\nQ1) *Quality of  AIG embeddings: The reviewer is concerned about usage of 3-layer GCN network to encode AIG graphs which are directed and represent functionality. Thus, two AIG\u2019s with the same structure can differ in functionality and hence require different synthesis recipes. However, our 3-layer GCN may fail to distinguish between such similar AIG structure but different functionality.*\n\n**Response:**\n\nThank you for the question. We note that unlike CNNs, GNN/GCN architectures tend to be shallower because deeper networks suffer from a well-studied \u201cover-smoothing\u201d [1,2] effect; i.e., as GCN depth increases each node\u2019s features depend on all other nodes, resulting in an ``averaging\u201d effect that causes all nodes to have similar embeddings. Therefore, GNNs often achieve optimal classification performance when networks are shallow, and many widely used GNNs/GCNs architectures are no deeper than 4 layers [2,3,4]. Prior work on ML for logic synthesis, for instance, the Online-RL approach (Zhu et al. 2020), and SA+Pred. (Chowdhury et al., 2022) methods we compare against also use GCNs with up to 4 layers. Mirhoseini et al, (2021) use only a 2-layer GCN in their RL agent for chip placement. As such, we would like to emphasize that our GCN architecture depth is consistent with common practice in the graph learning literature.\n\n\n[1] Li, Qimai et al. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\n[2] Wu, Xinyi, et al. \"A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks.\" in International Conference on Learning Representations (ICLR) 2022.\n\n[3] Errica, F., ert al. A Fair Comparison of Graph Neural Networks for Graph Classification. in International Conference on Learning Representations (ICLR) 2020.\n\n[4] Wu, Zonghan et al. A comprehensive survey on graph neural networks. in IEEE transactions on neural networks and learning systems 32.1 (2020): 4-24.\n\nQ2) *The experiments are conducted with a very small dataset, which only includes 23 netlists for training. This is not convincing. The proposed agent should have seen sufficient circuit designs to come up with a good RL strategy.*\n\n**Response:**\n\nWe note that although we use 23 netlists for training, the actual number of data samples that our agent is trained on is much larger, i.e., about 11,500 different states that are generated during the episodes of training. In each episode, 230 new graphs are generated using most promising synthesis recipes using MCTS, and since we train for 50 episodes, that yields our final tally of 11,500. \n\nRecent work in RL for hardware optimization have similar number of training examples: 20 netlists in [4] and 12 in [5]. In part, the limitation stems from the size of publicly available datasets. In our paper, we have tried to address this issue by combining both commonly used public datasets for logic synthesis, MCNC (1991) and EPFL benchmarks (2015), yielding our eventual tally of 23 training netlists.\n\n[4] Mirhoseini, Azalia, et al. \"A graph placement methodology for fast chip design.\" Nature 2021\n\n[5] Lai, Yao, et al. \"Chipformer: Transferable chip placement via offline decision transformer.\" International Conference on Machine Learning 2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263488762,
                "cdate": 1700263488762,
                "tmdate": 1700296457489,
                "mdate": 1700296457489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KuprroutaQ",
                "forum": "0t1O8ziRZp",
                "replyto": "efFxFDY7zy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the answers. However, they do not address my questions well. \n\n1. Regarding the AIG embedding, the problem is not about the number of layers in the GCN. Circuits are directed graphs with built-in functionalities. Using a simple GCN for AIG embedding, both the structural information and the functional information suffer from great loss. Consequently, it is likely that two vastly different circuits generate similar GCN embedding with this work and hence use similar synthesis recipes, but they shouldn't. \n\n2. Indeed, we don't have lots of publicly available datasets that are readily applicable to this work. On the one hand, the authors should discuss its implications, i.e., what kind of circuits may benefit from this solution. On the other hand, the authors can extract more circuits from OpenCore or GitHub projects to make the work more convincing. I understand this would involve lots of work, but the authors should at least test a few circuits that are outside of the benchmark dataset to demonstrate the generalization capability of the proposed solution."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700357536852,
                "cdate": 1700357536852,
                "tmdate": 1700357536852,
                "mdate": 1700357536852,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iizGpj52e1",
                "forum": "0t1O8ziRZp",
                "replyto": "CtAotUn5rd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for reviewer 3ENJ (Part 2)"
                    },
                    "comment": {
                        "value": "We acknowledge the reviewer's diligence in pointing out that our rebuttal response may not have comprehensively addressed his inquiries. We address the concerns below:\n\n*Q1) Regarding the AIG embedding, the problem is not about the number of layers in the GCN. Circuits are directed graphs with built-in functionalities. Using a simple GCN for AIG embedding, both the structural information and the functional information suffer from great loss. Consequently, it is likely that two vastly different circuits generate similar GCN embedding with this work and hence use similar synthesis recipes, but they shouldn't.*\n\n**Response**:\n\nWe articulate our rationale for utilizing a simple Graph Convolutional Network (GCN) architecture to encode AIGs for the generation of synthesis recipes aimed at optimizing the area-delay product. We elucidate why this approach is effective and support our argument with an experiment that validates its efficacy:\n\n* **Working principle of logic synthesis transformations**: Logic synthesis transformations of ABC (and in general commercial logic synthesis tools) including *rewrite*, *refactor* and *re-substitute* performs transformations at a local subgraph levels rather than the whole AIG structure. For e.g. *rewrite* performs a backward pass from primary outputs to primary inputs, perform k-way cut at each AIG node and replace the functionality with optimized implementation available in the truth table library of ABC. Similarly, *refactor* randomly picks a fan-in cone of an intermediate node of AIG and replaces it with a different implementation if it reduced the nodes of AIG. Thus, effectiveness of any synthesis transformations  do not require deeper GCN layers; capturing neighborhood information upto depth 3 in our case worked well to extract features out of AIG which can help predict which transformation next can help reduce area-delay product.\n\n* **Feature initialization of nodes in AIG**: The node in our AIG encompasses two important feature: i) Type of node (Primary Input, Primary Output and Internal Node) and ii) Number of negated fan-ins. Thus, our feature initialization scheme takes care of the functionality even though the structure of AIG are exactly similar and the functionality. Thus, two AIG having exact same structure but edge types are different (dotted edge represent negation and solid edge represent buffer), the initial node features of AIG itself will be vastly different and thus our 3-layer GCNs have been capable enough to distinguish between them and generate different synthesis recipes.\n\nTo illustrate this point further, we conduct a concise experiment involving a test circuit, 'div.' In this experiment, we generate additional AIGs by randomly flipping the edges of the AIG structure, thereby creating structural variants with distinct functionalities. For example, when we mention '10%,' it signifies that 10% of the edge types in the AIG were flipped. We employ the equivalence check feature of the ABC tool to ascertain whether these newly generated AIGs are structurally identical but functionally divergent. Additionally, we present the similarity score (cosine distance) of these AIGs with respect to the original circuit and the achieved Area-Delay Product (ADP) reduction obtained through Monte Carlo Tree Search (MCTS) and ABC-RL.\n\n| Circuit (div) | Similarity score  | Equivalence check   | MCTS   | ABC-RL |\n|---------------------|------|------|------|------|\n| orig                  | 0.0 | PASS | 46.00 | 56.2 | \n| 10% flipped     | 0.029 | FAIL | 42.36 | 49.99 | \n| 20% flipped     | 0.066 | FAIL | 62.33 | 64.11 | \n| 30% flipped     | 0.145 | FAIL | 57.32 | 57.33|\n| 40% flipped     | 0.239 | FAIL |  41.36| 42.01|\n| 50% flipped     | 0.568 | FAIL | 39.15 | 40.68|\n\nOur results highlight that the feature extracted by our 3-layer GCN architecture is able to capture functional differences having same skeleton AIG structure resulting in different similarity scores and ABC-RL performs better than MCTS."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375815758,
                "cdate": 1700375815758,
                "tmdate": 1700376138118,
                "mdate": 1700376138118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zF0YvinHMT",
                "forum": "0t1O8ziRZp",
                "replyto": "CtAotUn5rd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for reviewer 3ENJ (Part 3)"
                    },
                    "comment": {
                        "value": "*Q2) Indeed, we don't have lots of publicly available datasets that are readily applicable to this work. On the one hand, the authors should discuss its implications, i.e., what kind of circuits may benefit from this solution. On the other hand, the authors can extract more circuits from OpenCore or GitHub projects to make the work more convincing. I understand this would involve lots of work, but the authors should at least test a few circuits that are outside of the benchmark dataset to demonstrate the generalization capability of the proposed solution.*\n\n**Response**:\n\nWe appreciate the reviewer's understanding that extracting the circuits from Opencore or Github projects and training our framework require considerable more time to demonstrate further effectiveness (Our training time on 23 circuits with parallelized implementation takes around 9 days). However, to the best possible extent we provide results on 9 circuits extracted out of Opencore with our ABC-RL model trained on 23 circuits. We report area-delay reduction (in %) obtained by ABC-RL and compare it with MCTS:\n\n| Method | fpu   |  ac97_ctrl  | fir  | iir   | aes  | wb_dma  |\n|----------------|------|------|------|------|------|------|\n| MCTS         | 11.61 | 29.80 | 15.21 | 4.67 | 51.62 | 16.61 |\n| ABC-RL     | 16.86 | 34.35 | 17.46 | 6.09 | 72.36 | 27.55 |\n\nWe also want to highlight the fact that: in hardware domain the distribution study of circuits in terms of functionality and structure is still an open problem to explore. For the kind of circuits that can benefit well from ABC-RL (i.e. circuits on which ABC-RL function well and generalize), we observe our cosine distance capture this notion well during test time.\n\nAlso asked by reviewer yxFf, we presented  this data in ***Appendix D.4 (Table 6)***. Several interesting observations we have drawn from it: first, note that the closest circuits in the training set are often semantically similar (*alu2 for alu4, apex3 for apex1, sqrt for multiplier*), suggesting that the learned embeddings are capturing netlist structure/function. Second, netlists for which we found MCTS+L to underperform MCTS alone, for example, square and cavlc have relatively **high** cosine distances of 0.012 and 0.007, respectively, relative to the $\\delta_{th}=0.007$ in Equation 4."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375846998,
                "cdate": 1700375846998,
                "tmdate": 1700376428694,
                "mdate": 1700376428694,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "niTFgZAhE5",
                "forum": "0t1O8ziRZp",
                "replyto": "iizGpj52e1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for the additional experiments, but again, it is not convincing. First of all, if \"flipping the edges\" means exchanges between NOT and AND gate types, wouldn't it cause illegal circuits? Secondly, how can you say that changing 10% of the gate types as two structurally identical AIGs?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389729732,
                "cdate": 1700389729732,
                "tmdate": 1700389729732,
                "mdate": 1700389729732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1bJfBZmFl5",
                "forum": "0t1O8ziRZp",
                "replyto": "zF0YvinHMT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for doing the experiments on Opencore circuits, and I've raised my score for answering this question."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389899127,
                "cdate": 1700389899127,
                "tmdate": 1700389899127,
                "mdate": 1700389899127,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pmPTGiev3I",
                "forum": "0t1O8ziRZp",
                "replyto": "33ArXPm8e3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Reviewer_3ENJ"
                ],
                "content": {
                    "comment": {
                        "value": "In your previous rebuttal, you mention the AIG as: \"The node in our AIG encompasses two important feature: i) Type of node (Primary Input, Primary Output and Internal Node) and ii) Number of negated fan-ins. \" In the main text of the paper, it is written as: \"The AIG has only AND gates and NOT as edges ...\" In the current rebuttal, it is said that there are actually two types of edges. This is really confusing, and it is essential to make it clear in the main text of the paper. Moreover, under such circumstances, you need to elaborate more on the GNN aggregation procedure because the regular GCN doesn't support different types of edges in aggregation. Do you use separate channels for different types of edges?\n\nAs for the question itself, I cannot accept the claim that replacing many NOT gates in the circuit with wires and vice versa still holds structural similarity. \n\nIn fact, there have been many advancements in circuit embedding techniques in recent years, e.g., the DeepGate family."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538254299,
                "cdate": 1700538254299,
                "tmdate": 1700538254299,
                "mdate": 1700538254299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LS2H9Lwt1Q",
            "forum": "0t1O8ziRZp",
            "replyto": "0t1O8ziRZp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_Pwwd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7821/Reviewer_Pwwd"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces, ABC-RL, an MCTS+Learning algorithm to optimize the transformation recipes in logic synthesis to minimize the area delay product (ADP).  It employs GCN for AIG features and a transformer for applied transformations. It introduces a hyperparameter to adjust recommendations from pre-trained agents during the search. The hyperparameter is determined by similarity computed from GNN features learned during training. The key observation it leverages from hardware designs is that they contain both familiar and entirely new components. The empirical results show ABC-RL improves synthesized circuit Quality-of-Result (QoR) by up to 24.8% over SOTA methods and provides an average runtime speed-up of 1.6\u00d7 compared to baseline MCTS."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The policy network architecture for recipe encoding and AIG embedding is reasonably selected.\n2. The novel introduction of a similarity-score-based hyperparameter effectively enhances ABC-RL convergence, as observed in the results.\n3. The study provides a comprehensive evaluation, comparing various search algorithms for logic synthesis recipe optimization, including MCTS, online RL, and simulated annealing. It also includes comparisons with MCTS+Learning with fine-tuning."
                },
                "weaknesses": {
                    "value": "1. It would be beneficial to provide an estimate of the number of gates in each benchmark circuit. One concern with this approach is whether GCN-based AIG embeddings can effectively scale to real-world circuit designs.\n2. With the integration of GNN + transformer features. It is likely that the compute complexity and runtime of each search iteration would be higher than the baseline MCTS."
                },
                "questions": {
                    "value": "1. How is it compared to non-learning-based recipes? Is there an O3 flag for logic synthesis? \n2. How does its wall clock time (not iterations) compare to other algorithms such as MCTS and SA+Pred?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7821/Reviewer_Pwwd"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7821/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837735965,
            "cdate": 1698837735965,
            "tmdate": 1699636957262,
            "mdate": 1699636957262,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MQeuYuan4P",
                "forum": "0t1O8ziRZp",
                "replyto": "LS2H9Lwt1Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7821/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to reviewer Pwwd"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful feedback and address the concerns raised:\n\nQ1) *It would be beneficial to provide an estimate of the number of gates in each benchmark circuit. One concern with this approach is whether GCN-based AIG embeddings can effectively scale to real-world circuit designs.*\n\n**Response:**\n\nThank you for the suggestion. We provide detailed characterization in ***Appendix C.2*** listing all the circuits from MCNC and EPFL benchmarks about primary inputs and outputs, number of nodes and level of AIG graphs. The benchmarks circuits range from 400-46,000 nodes. In terms of applicability to real-world designs, we note that logic synthesis is often performed hierarchically because Verilog code itself is written in a modular fashion and because of the large runtimes of *flattened* synthesis. As such, our results (and indeed those in the logic synthesis literature that all make use of IWLS/EPFL) can be viewed as synthesis runs on leaf and intermediate nodes of a hierarchical flow.\n\n*With the integration of GNN + transformer features. It is likely that the compute complexity and runtime of each search iteration would be higher than the baseline MCTS. How does its wall clock time (not iterations) compare to other algorithms such as MCTS and SA+Pred?*\n\n**Response:**\n\nThis is an excellent point. We note that over iterations of MCTS, SA+Pred or ABC-RL search it is the actual synthesis of logic circuits using sample recipes that dominates the runtime, leaving GNN inference as a relatively negligible component to wall-clock time. Therefore, the overhead of ABC-RL over MCTS in terms of wall-clock time is only **1.5% (geo. mean)** as shown in ***Appendix D.6 (Table 8)***. Our **wall-clock speed-up** over MCTS at iso-QoR is **3.75x**. \n\n\nQ3) *How is it compared to non-learning-based recipes? Is there an O3 flag for logic synthesis?*\n\n**Response:**\n\nThe publicly known non-learned recipe for the ABC synthesis tool is resyn2 which is heuristically tailored to optimize for area-delay-product (ADP), our QoR metric. As such it can be viewed as akin to an -O3 flag for C/C++. For this reason, all our reported results in ADP improvements are on top of  (i.e., relative to) the resyn2 recipe. Over time, experienced EDA engineers might develop their own intuitions as to which recipes work best for different circuits, but we are not aware of any formal documentation to this effect."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7821/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700263441633,
                "cdate": 1700263441633,
                "tmdate": 1700296360994,
                "mdate": 1700296360994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]