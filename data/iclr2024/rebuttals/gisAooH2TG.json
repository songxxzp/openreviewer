[
    {
        "title": "RePLan: Robotic Replanning with Perception and Language Models"
    },
    {
        "review": {
            "id": "gJYXqJnk1T",
            "forum": "gisAooH2TG",
            "replyto": "gisAooH2TG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8262/Reviewer_6UHr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8262/Reviewer_6UHr"
            ],
            "content": {
                "summary": {
                    "value": "The authors build off recent work on controlling robots via large language models and vision language models. The closest analogue of their work is recent lines of work on generating reward functions via language models (i.e. Zero-Shot Reward Specification via Grounded Natural Language, Language to Rewards for Robotics Skill Synthesis, etc.)\n\nIn general, prior work uses a prompted LLM to generate a reward function based on a natural language command from the user for what task they want to perform, and this reward is optimized by MPC to generate low level actions.\n\nThis work augments this flow by including a VLM \"Perceiver\". After the robot acts according to the generated reward, this perceiver is given an image of the scene + an instruction to either confirm the task is completed or generate a text description of why it has failed. If the robot has failed, this text description feedback is fed back into the high-level planner to generate a new instruction + new reward function. This is the \"replanning\" step of their work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The core idea of using a VLM to generate textual feedback for a model makes sense and is a good addition to this research direction in robot learning. The videos are very helpful for showcasing the resulting method."
                },
                "weaknesses": {
                    "value": "Although the high level idea of the paper is reasonable, it is both not evaluated as extensively as would be helpful and is not put in context of recent literature very well.\n\nAs a minor note: please fix the references to surround the authors of citations in parentheses. This would make the paper much easier to read.\n\nOn the experimental front:\n\n* the exact prompts used do not appear in the paper or appendix, which makes it hard to judge how the LLM was told to incorporate past feedback\n* The evaluation setting is 4 tasks, done for 3 trials each. This is an incredibly small number of tasks and trials for the method. As a point of comparison: the Language to Rewards paper used as a baseline was tested on 17 tasks with 10 generated rewards functions per task, run through MPC 50 times each. My understanding is that in Table 1, RePLan without replanning is almost identical to Language to Rewards, but is only successful 1/3 times in the easiest Task 1 setting, compared to 3/3 from Language to Rewards. To me this seems like it is just because of random noise, but if that's the case, why should we trust any of the other numbers in the table? This is the point of issue I find most important about the paper.\n\nOn the prior literature front:\n\nThere are number of prior works based on providing LLM feedback from the environment, either ground truth or from VLMs. Examples are Voyager by Wang et al, Inner Monologue by Huang et al, and Towards a Unified Agent with Foundation Models by Di Palo et al. I would appreciate some discussion about such lines of work, since to me it is not so clear if this is doing anything very different from these works. I believe at most you can argue that this paper is using MPC instead of RL or imitation learning, but otherwise prior work has used chain-of-thought style prompting to decompose high-level language to low level language, generate rewards from said low level language, provide feedback via VLMs, etc. That is not to say that the combination of prior work cannot be novel, but in this instance, it does not feel like much is coming from said combination. Especially given the weakness of the experimental results.\n\nEdit: some more experimental results were provided and I have adjusted score from 3 -> 5."
                },
                "questions": {
                    "value": "When doing replanning, is there any cap on how many iterations of replanning the agent is allowed to do? Could the authors also discuss if they see failure cases from the Perceiver, or in deciding if a task is completed or not?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8262/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8262/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8262/Reviewer_6UHr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8262/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698104967184,
            "cdate": 1698104967184,
            "tmdate": 1700682878882,
            "mdate": 1700682878882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yEPVFz07Gf",
                "forum": "gisAooH2TG",
                "replyto": "gJYXqJnk1T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your feedback! To address your concerns:\n\n**1. Missing prompts.** We apologize for not originally including the prompts used. In our updated draft, we have included all prompts used in Appendix B and indicate how they are used in Algorithm 1 and throughout the text. \n\n**2. Limited evaluations.** We agree with the reviewer that our experiment setting was limited. We have increased the number of experiments on two fronts. The first is that we added two new environments and three new tasks. The first added environment contains a cabinet that is initially locked, and a lever. The robot has to open the cabinet to deposit an item in there, but it cannot. There is also nothing physically blocking it, and so the robot has to reason that it must somehow interact with the lever in order to unlock the cabinet. The second environment is a scene with a red crate and two cubes, one red and one yellow. In the first task, the robot must place the cube with the same colour as the crate on the crate. This requires conditional reasoning with the Perceiver. The second tasks also requires the robot to place the red cube on the crate, but this time there is a physical blocker: the yellow cube is already on the crate, and there is only room for one. Thus, the robot must first remove the yellow cube in order to accomplish the task. We have also increased the number of runs of each experiment from 3 to 10. We also include a breakdown of the number of actions the robot does in order to do the tasks. In total, the robot performs 401 MPC actions and 101 VLM actions. This is more than previous work (L2R, 170 MPC actions). To address the reviewer\u2019s concerns about experimental errors, we include a more thorough breakdown of model components and an error analysis (see Appendix C2 and C3).  \n\n**3. Prior work.** Thank you to the reviewer for pointing out these prior works. Indeed, we include many of them in our updated draft to clarify our contributions. Specifically, Inner Monologue (IM) does not actively use feedback from VLMs to replan during a task; it uses feedback from humans or object recognition models to know what objects are present or absent (this would be most similar to row 3 in our Table 1). IM uses a success detector in order to determine whether it could resample the low-level policy for a pick-and-place action, but it would not be able to handle obstacles and does not reason on why the action failed. It has been shown in works since binary success detection is not enough to perform complex tasks [1]. Voyager also shows this by receiving feedback from a chat bot in the Minecraft API on why code execution errors were generated. They (in addition to other works) also show that Verifiers are important for long-horizon task completion. We agree that the use of Verifiers and environment feedback is extremely important. We have added these references to our Related Works section. Towards a Unified Agent relies on previously collected observations to finetune a CLIP reward model. Our method does not require any model training. We believe our method is novel because we combine several key insights in order to execute complicated, long-horizon tasks without the need for model re-training or human guidance. First, we use perceiver models to diagnose problems and gather information about object states in order to replan if the robot encounters issues. We create hierarchical plans for both high-level reasoning and low-level reward generation, allowing for adaptive robot behaviour that has only been previously shown on single-step tasks. Finally, we use verifiers at both high-level and low-level reasoning to enable long-horizon tasks of up to 17 steps. \n\n**4. Replanning iterations.** Our replanning framework is set up recursively, so that the agent can replan when performing at any subtask. However, we set a threshold on recursion depth (r=2) and the number of replanning stages allowed (p=2) to lower runtime and API call costs. \n\n**5. Failure cases.** We include a more in-depth error analysis of all components in our system in this draft. Specifically, we showcase Perceiver errors in Appendix C3. The Perceiver is often able to successfully identify what the reason for plan failure is, but sometimes the interaction between the Perceiver and Planner results in the wrong summary reason. We also include an ablation study in C2 that reports the object recognition and object reasoning performance of two state-of-the-art open-source VLMs and GPT-4V. \n\n**6. Citations formatting.** We apologize for the citation issue and have fixed it in the current draft, we thank the reviewer for pointing it out to us. \n\n\n[1] Lei Wang et al, \u201cA Survey on Language Language Model based Autonomous Agents\u201d. (2023)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8262/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338888969,
                "cdate": 1700338888969,
                "tmdate": 1700338888969,
                "mdate": 1700338888969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6aG1b8FULZ",
                "forum": "gisAooH2TG",
                "replyto": "yEPVFz07Gf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8262/Reviewer_6UHr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8262/Reviewer_6UHr"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for replying to concerns. I will increase the score of my review. I do agree with reviewer b9qX that the Perceiver should be analyzed in more detail as it is the more important part of the work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8262/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682844381,
                "cdate": 1700682844381,
                "tmdate": 1700682844381,
                "mdate": 1700682844381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "33hGOvDDN7",
                "forum": "gisAooH2TG",
                "replyto": "gJYXqJnk1T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "content": {
                    "title": {
                        "value": "VLM Perceiver analysis, part 3"
                    },
                    "comment": {
                        "value": "**Task 4: VLM is asked to diagnose why the robot cannot open the microwave.**\n\n\nInput to VLM:\n\n```\n1. A robot is in a simulation environment where it can interact with any object like in the real world. The robot would like to open the microwave but it cannot. Is there something in this scene preventing that, other than the robot? Assume the robot can interact with anything. These are the names of the objects in our scene: blue kettle, microwave, cabinet\n2. In a simulation, a robot wants to open the microwave but can't. Is anything else, besides the robot, blocking it? Check the objects in the scene: blue kettle, microwave, cabinet.\n3. Robot in a simulation wants to open the microwave, can't. Something else stopping it? Objects in scene: blue kettle, microwave, cabinet.\n4. A robot can engage with any item. It wants to open the microwave but can't. Is an object in this scene, apart from the robot, hindering it? Objects present: blue kettle, microwave, cabinet\n5. I would like to open the microwave but I cannot. Is there something in this scene preventing that, other than the robot? These are the objects in the scene: blue kettle, microwave, cabinet\n6. I would like to open the microwave but I am unable to. Is there something in this scene preventing me from doing that? Ignore the robot. These are the names of the objects: blue kettle, microwave, cabinet\n```\n\nOutputs from VLM:\n\n```\n1. There is a blue kettle in the scene, which is preventing the robot from opening the microwave.\n2. In the scene, there is a blue_kettle on the microwave and a cabinet above the microwave. The robot is not blocking the microwave, so the only thing preventing the robot from opening the microwave is the blue_kettle on the microwave.\n3. The robot in the scene is unable to open the cabinet because there is a blue_kettle placed on the surface in front of the cabinet. This object is blocking the robot's access to the cabinet, preventing it from opening it.\n4. There is no existence of a blue kettle in the image description.\n5. There is a blue kettle on the countertop that could be preventing you from opening the microwave.\n6. There is a robot standing on the counter in front of the microwave. It is blocking access to the microwave.\n```\n\nLLM Summary:\n```\nThe most probable reason why the robot cannot open the microwave is that there is a blue kettle placed on the microwave, blocking the robot's access to it.\n```\n\n**In Appendix C3, we also show some error cases of where the interaction betewen the Perceiver and High-Level Planner fails.** As an example, in Task 4, the robot tried to open the microwave door but failed due to a kettle obstructing the path. The Perceiver gave five diagnoses, of which three claimed that the kettle was blocking the way, one claimed the cabinet door was blocking the way, and one did not give any conclusive diagnosis. The summary LLM concluded that it was the cabinet door that blocked the action. The robot went on to interact with the cabinet and never removed the kettle. However, in 9 runs out of 10, the High-LeveL Planner was able to correctly summarize the reason from the VLM outputs.\n\n**PERCEIVER-PLANNER MOTIVATION**\n\nAs an additional experiment, we also report the ability of GPT-4V to solve Tasks 3 and 6 out-of-the-box by prompting it to create a plan for opening the cabinet and placing the cube with the same colour as the crate on the crate (Appendix D). In both cases, the generated plans did not provide a working solution. In the first case, plan did not identify that there was a block across the handles of the cabinet the robot needed to first remove, and in the second case, the plan did not specify the name of the cube to use. This shows combining the VLM with specific LLM prompting is essential in order to solve these tasks.\n\nWe again thank you for your feedback, please let us know if there are additional analyses you wish to see.\n\n[1] Kirillov A et al, Segment Anything. (2023)"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8262/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695465061,
                "cdate": 1700695465061,
                "tmdate": 1700719218783,
                "mdate": 1700719218783,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9G66vyiBLD",
            "forum": "gisAooH2TG",
            "replyto": "gisAooH2TG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8262/Reviewer_Ya9u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8262/Reviewer_Ya9u"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a hierarchical method for solving multi-stage manipulation tasks with language models, consisting of a LLM-based high-level planner as well as a LLM-based low-level reward/cost generator. The high-level planner decomposes the task into multiple stages in natural language, with the option to replan based on perceptual input if needed. The low-level reward generator takes as input the decomposed sub-task and generates reward for an MPC controller, which uses predictive sampling in MuJoCo to generate low-level robot actions. The method is evaluated on 4 tasks in 2 scenes, where it is compared against a recent baseline \u201cLanguage to Reward\u201d and ablated across the method\u2019s different components."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presented idea is clear and well-motivated \u2014 leveraging LLMs in a hierarchical framework for both high-level task planning and low-level motion planning. Compared to prior work, \u201cLanguage to Reward\u201d, it is clear that such hierarchical approach is needed for long-horizon tasks and can also offer additional robustness as the system can replan its high-level action.\n- The literature review is also thorough, covering many recent works in this domain. However, this part can be improved because it is now more like a laundry list instead of putting the work in the context of prior works."
                },
                "weaknesses": {
                    "value": "- Currently the biggest limitation seems to be the lack of thorough experiments, which can use some improvement along two axes. One is the breadth of the tasks: there are only four tasks investigated in this work while there are also quite some similarities between them. An important advantage of using LLMs is that it is possible to apply to a wider set of tasks more easily. The other axis is the quantitative evaluation: currrently only 3 runs are performed for each entry in Table 1, which makes the quantitative results not very convincing as it is also pointed out in the paper that there is \u201chigh variance of completion\u201d. In addition, the paper does not compare to prior methods that are not based on LLMs, e.g., task and motion planning methods or hierarchical RL methods.\n- Another limitation lies in the use of the simulator ground-truth for MPC. This raises the question whether the approach can be applicable to real-world settings. However, the high-level planning part does use VLM for grounding image observations, but it\u2019s unclear how this can be achieved for the attributes referred in MPC, e.g. \u201cblock_r_side\u201d, \u201ccabinet_handle\u201d.\n- Currently the intro reads more like related works, where it may be confusing to readers what the actual motivation of the work is. More care can be taken to improve the intro while appropriately contextualizing the work.\n- The citation format in many places are currently incorrect \u2014 parenthesis often should be used."
                },
                "questions": {
                    "value": "What is the prompt being used for LLMs? Can the authors provide more examples of paired LLM output and environment execution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8262/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818499378,
            "cdate": 1698818499378,
            "tmdate": 1699637027151,
            "mdate": 1699637027151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KLFc370xef",
                "forum": "gisAooH2TG",
                "replyto": "9G66vyiBLD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are glad that the reviewer finds our idea clear and well-motivated. We also appreciate the reviewer\u2019s insightful feedback. Below we address the reviewer\u2019s concerns:\n\n**1. Improved literature review.** We agree with the reviewer\u2019s observation that the literature review could be improved. We have updated our introduction section and related works section to provide a more thorough and structured literature review, and have re-written sections to better put our work in the context of previous papers and ideas.\n\n**2. Lack of thorough experiments and comparison with prior non-LLM works.** We appreciate the reviewer for their suggestion to improve our evaluations. We have worked on both of the axes by including three new tasks in two new scenes and increasing the number of runs from three to ten. One of the new tasks involves reasoning about an interaction with a lever-locking mechanism, and the other new tasks emphasize perceiving and reasoning about colors and executing conditional plans. We believe the inclusion of these new tasks can improve the variety of our evaluation. We also improve the analysis of existing tasks in the Results section and  Appendix C by analyzing the number of actions required by our tasks (401 MPC actions + 101 VLM actions in total), as well as analyzing common errors. Finally, we include two additional baselines for a few tasks: PDDL (Appendix E) and GPT-4V (Appendix D). We find that out-of-the-box, GPT-4V is not able to solve our tasks. We include a PDDL domain and problem definition for Tasks 1-3. We show that they are not trivial to write and rely on human-provided ground truth information (for example, that the door is blocked). We also consider using PDDLStream to solve the problem. In this case, some of the human-provided ground-truth information is certified by Stream (by using a motion planner it verifies the feasibility of action execution at the planning time); however, it\u2019s inefficient to solve. In terms of hierarchical RL, our work assumes text-based goals with no access to pretrained skills. To the best of our knowledge, there are no prior RL works that succeed under these assumptions. The closest one we find is [1], which is only evaluated in single-motion tasks, while the majority of our tasks are multi-step. In addition, our algorithm can be evaluated on the fly while RL methods typically require extensive training, which we believe can be an unfair comparison.\n\n**3. Simulator ground truth for MPC.** We believe using simulator ground truth is a necessary compromise to make, and is currently what state-of-the-art works use (Language to Rewards [2], Inner Monologue [3]). Using a ground truth simulator does not jeopardize transferring the system to the real world. For example, [2] and [3] utilize a vision model to map the real-world objects to the simulator and then execute the simulator-produced actions in real world. Another possible way could be collect demonstrations in the simulator and distill an image-based policy from them, which can be directly applied to the real world.\n\n**4. The introduction reads more like related works.** We apologize for the possible confusion. We have updated our introduction to provide a better motivation for our work. We have also fixed the citation formatting issue. We thank the reviewer for pointing this out.\n\n**5. Missing prompts.** We have included the detailed prompts in Appendix B in our updated manuscript, as well as how they are used in Algorithm 1 and throughout the text. We hope it can provide enough information for the reviewer.\n\n- [1] Wenhao Yu et al, \u201cLanguage to Rewards for Robotic Skill Synthesis\u201d. (2023)\n- [2] Tianbao Xie et al, \u201cText2Reward: Automated Dense Reward Function Generation for Reinforcement Learning\u201d. (2023)\n- [3] Wenlong Huang et al, \u201cInner Monologue: Embodied Reasoning through Planning with Language Models\u201d. (2023)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8262/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338871606,
                "cdate": 1700338871606,
                "tmdate": 1700339606923,
                "mdate": 1700339606923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ghhuqyhIFt",
                "forum": "gisAooH2TG",
                "replyto": "KLFc370xef",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8262/Reviewer_Ya9u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8262/Reviewer_Ya9u"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their response. I believe the updated draft has addressed most of my previous concerns except for \"using simulator ground-truth in MPC\":\n\n- The reason I'm raising this concern is because an important claimed contribution of this work is using a VLM to perform *image-based* reasoning for long-horizon tasks. However, the low-level actions still rely on a ground-truth model of the environment, which makes the assumption unclear and defeats the purpose of using VLMs in the first place. If a ground-truth model is assumed to be available (e.g., the joint angle for drawers is always known), one can easily just use this to feed into a text-only LLM to reason about replanning actions, which is likely more accurate than using VLMs anyway.\n\n- While I agree with authors' response that prior works (e.g., [1]) also assume the ground-truth model is available, these works *do* show the feasibility of transferring to the real world (or even just image-based observations in sim) on certain tasks. But currently this is not shown in the paper.\n\n- Furthermore, assuming ground-truth models is also acceptable if this is not inherently tied to the contributions of the paper. For example, the claim of the paper can be rephrased to only focusing on high-level replanning using VLMs (this should also be well contextualized in the current literature). However, the current claimed contribution is that the framework also enables replanning at the low-level actions, which largely rely on prior work ([1]) and does not offer additional insights to the community.\n\nDue to the above reasons, I'm only able to raise my recommendation to 4."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8262/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698731873,
                "cdate": 1700698731873,
                "tmdate": 1700698731873,
                "mdate": 1700698731873,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xr2EagjKkn",
            "forum": "gisAooH2TG",
            "replyto": "gisAooH2TG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8262/Reviewer_b9qX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8262/Reviewer_b9qX"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a framework called RePLan that integrates multiple foundation model components into a system for iterative high-level robot planning and low-level robot reward code generation. There are five modules in RePLan: 1) LLM for High-level semantic planning to propose intermediate language primitives, 2) VLM for perception for scene state estimation and motion error explanation, 3) LLM for low-level reward code generation given a language primitive and scene state, 4) Motion Controller to translate reward code to robot actions, 5) LLM to verify that 1) and 2) outputs are correct. This system is evaluated on 4 simulated robot manipulation scenarios which require from 1 to 4 subtasks to solve, and is compared against Language2Reward, a method that does not do iterative task decomposition or replanning. The method is compared against ablations which remove specific components; only the full RePLan system is able to achieve non-zero success on all evaluation scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow\n- The motivation of the paper is topical since feedback and adaptive replanning is important for foundation models which may hallucinate or require grounding in physical interactions\n- The method does not require additional human input compared to the baseline method Language2Reward (just one human input at the beggining, the rest of the replanning and execution is autonomously completed by the foundation model submodules)"
                },
                "weaknesses": {
                    "value": "- The VLM Perceiver is one of the most critical parts of the method, but it is not sufficiently explained. Due to the lack of details, I can only assume how it is utilized based on Algorithm 1, in which case I have some major concerns. Since the VLM is the bottleneck for providing feedback for grounding LLM plans and rewards for future LLM iterations. However, details are not shared about how the Perceiver is used, even though it is mentioned that \"The High-Level Planner [is used] to decide what it wants to query from the Perceiver\". From Algorithm 1, the VLM needs to be used for two use cases: #1 scene state generation `VLM(image_observation)` and #2 `VLM(image_observation, motion_error, language_instruction)`. However, these seem to be quite challenging tasks to naively query for off-the-shelf VLMs. While modern VLMs are fairly robust at narrow vision task domains like object detection or image captioning for internet images, more extended reasoning (such as failure explanation with multiple input contexts) or domain-specific understanding (like robotics reasoning from vision) is still an open problem.\n- The evaluation complexity is very limited and does not justify the claims of a \"large-scale and long-horizon kitchen environment\": it is only in simulation, with relatively high-level and short-horizon tasks. There are two issues: 1) the granularity of intermediate primitives (\"pick up the block\") is coarse, 2) the horizon length is short, going only up to 4 subtasks required. Previous works in BEHAVIOR-1K, ALFRED, SayCan have studied robotic reasoning with much longer horizons with similar granularity of intermediate primitives.\n- The evaluation has very few trials (3 seeds only), so it is hard to draw confident conclusions about the method's quantitative performance.\n- The core claim of the work should clarified. If the contribution is the incorporation of LLM and VLM feedback into high level planning, it needs to be compared/discussed against prior works that ground robot planning with additional foundation model feedback. Specifically, [1] incorporates LLM and VLM feedback for closed-loop environment feedback for a High-Level LLM Planner for robot subtasks. If the contribution is the integration of LLM and VLM feedback into code generation, it needs to compared/discussed with other replanning works from code generation [2] or LLM tool use. If the core contribution is the verifier, this needs to be stated more clearly and more details about this module should be provided. If the contribution is the admittedly impressive integration of existing modules, this should be clarified and more details about the bottlenecks of the system (VLM Perceiver, Verification) should be shared in the main text.\n\n[1] \"Inner Monologue: Embodied Reasoning through Planning with Language Models\", Huang et al. 2022\n[2] \"Improving Code Generation by Training with Natural Language Feedback\", Chen et al. 2023"
                },
                "questions": {
                    "value": "- In general, clarifications to my concerns above will be appreciated.\n- Can you explain the VLM Perceiver more? For example: Which VLMs are used? How are they used? Which prompts from the LLM Planner are used? How are the motion errors passed to the VLM? How does the verifier coordinate with the VLM? What is the success rate of the VLM (This is in the context of my concerns in the `Weaknesses` above, where I am doubtful that current VLMs may perform well at extended robot reasoning. Prior works may utilize current VLMs for narrow sub-domains in robotics like object detection or success detection, but RePLan requires textual error explanation \"Block is in the way of opening the door\" which seems quite difficult in an end-to-end zero-shot VLM).\n- Can you clarify the failure reasons in Table 1? For example, does the reward code fail, the high level planner fail, or perceiver fail?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8262/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698863019917,
            "cdate": 1698863019917,
            "tmdate": 1699637027010,
            "mdate": 1699637027010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "66QgqKbuxN",
                "forum": "gisAooH2TG",
                "replyto": "xr2EagjKkn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s detailed comments and suggestions. We address the reviewer\u2019s specific concerns below:\n\n**1. Missing information about VLM Perceiver.** We apologize for not providing sufficient details about the VLM Perceiver. We have updated our draft to include all prompts used by the Planners, Perceiver, and Verifier in Appendix B and annotate each line in Algorithm 1 with corresponding prompts. In developing our framework, our experiences completely align with your comments that vanilla, off-the-shelf VLMs are not sufficient to solve these tasks. As a motivating example, we naively ask GPT-4V (which has demonstrated amazing vision-language capabilities) to create a plan for solving Tasks 3 and 6 (the reviewer can find these in Appendix D). In both cases, GPT-4V was unable to identify the correct plan. For Task 3, it was unable to identify that there was block in between the cabinet handles and in Task 4 it could not name the correct cube right off the bat. However, one key insight we find in this work is that we can use an LLM (which has better reasoning skills) to determine what questions it should ask the VLM in a constrained, VQA-style setting in order to update its understanding of the environment. For example, if the Planner asks the  VLM: \u201cRobot in a simulation wants to open the microwave door, can\u2019t. Something else stopping it?\u201d The VLM is able to provide the caption that there is a kettle in front of the door. To prevent overfitting to one question, the LLM asks the VLM multiple iterations of the question (see Figure B9 in Appendix B) and then based on its own knowledge of the world state, it provides a summary answer based on feedback from the VLM of what the most likely explanation. The Planner can also query the Perceiver to update its knowledge on object states, for example: \u201cWhat is the colour of the crate?\u201d or \u201cDo you see a green apple?\u201d (see Figures B14-B16 in Appendix B). We find that combining LLMs to effectively prompt VLMs and using Verifiers to make sure their outputs are consistent are essential for VLMs to provide useful feedback. We also provide an ablation study in Appendix C2 on the performance of two open-source models and GPT-4V on object recognition and reasoning. \n\n**2. Limited evaluation complexity.** We apologize for the unclear writing and have fixed our description of the number of actions the robot does to accomplish each goal. When we consider an action as any time the robot needs to do something to satisfy an MPC reward or decides to call the VLM to obtain information, most tasks requires on average between 7 and 11 subtasks without human intervention (with some runs using up to 17 depending on how the robot decides to replan). We show a Table in Appendix C (Figure C1) breaking down the number of MPC actions vs VLM actions for each task. This is comparable to the number of actions required by the longest-horizon tasks in SayCan, but those tasks do not have obstacles preventing the robot from following the high-level instructions and do not require replanning based on environment feedback. Furthermore, SayCan utilizes primitive skills to maintain a reliable subtask execution, which we do not assume to have. Not using primitive skills also poses significant challenges to the reliability of the method.\nFew evaluation trials. We have increased the number of trials for each task to 10. We have also added two new environments and three more tasks. \n\n**3. Clarification of core claim.** We apologize for not making the core contributions clear. Our core contribution with this work is as follows: we develop an agent to handle long-term, multi-stage tasks by combining four key aspects: using perceiver models for high-level replanning, creating hierarchical plans with language models, verifying outputs from these language models, and adaptive robot behavior through reward generation. We have updated the introduction to better reflect this. Thank you for pointing out a missing reference to Inner Monologue (IM). While both our works show that robots can perform more complex and longer-horizon tasks by receiving feedback from different sources while executing a task, we are different in the following ways: (a)Their framework uses a VLM at the beginning of the scene for object recognition, but while the scene is progressing, they obtain feedback from humans in some experiments, or an object recognition model to update knowledge about what objects are present/absent in the scene. They do not rely on VLMs for replanning  or providing reasons when the success detector fails. This is most similar to row 3 in our Table 1. Our tasks are not solvable from the presence/absence of objects alone, and our framework is about to solve them without human intervention. (b) When the success detector fails, the robot retries or selects another low-level policy to redo the task (not necessarily to determine reasons why the task failed based on the scene state)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8262/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338823438,
                "cdate": 1700338823438,
                "tmdate": 1700338823438,
                "mdate": 1700338823438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mbKvhmmCD4",
                "forum": "gisAooH2TG",
                "replyto": "E07f7yy6wX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8262/Reviewer_b9qX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8262/Reviewer_b9qX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response. You have clarified some of my concerns, and hope that the main manuscript will be updated to increase the clarity and presentation of your work.\n\nI will discuss with reviewers in the next phase; I am open to increasing my score to a 4."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8262/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690689100,
                "cdate": 1700690689100,
                "tmdate": 1700690689100,
                "mdate": 1700690689100,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jbwFssE1h5",
            "forum": "gisAooH2TG",
            "replyto": "gisAooH2TG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8262/Reviewer_jtHn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8262/Reviewer_jtHn"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a way to perform multi-stage planning with LLMs. the method combines multiple LLMs and VLM to reason about long-horizon tasks in a closed-loop fashion. overall the approach is good but I feel it requires more testing on wide variety of tasks to test the generalization of the approach. Furthermore, the testing of the motion planner needs to be more thourough."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The overall proposed method seems to be interesting and probably could work for a large variety of planning tasks.\n2. The tasks are a bit more complex then simple pick and place and illustrate more complex reasoning.\n\n==================\nI am increasing the score to 6 after rebuttal discussions."
                },
                "weaknesses": {
                    "value": "1. the authors need to provide more environments or tasks to show the robustness of their method. The authors can use LLMs to generate tasks which are long-horizon to come up with more varieties of task so that the proposed method could be more thoroughly tested. I think this remains to be verified."
                },
                "questions": {
                    "value": "1. How do you test your motion planner module?\n2. You say the weakness of your method is the VLM perceiver. Have you tried testing rest of your method by providing text description of the scene by creating a template and keeping the other modules? That can provide more insights of reasoning and control modules of your method?\n3. Can provide more results for your system by generating more long-horizon tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8262/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8262/Reviewer_jtHn",
                        "ICLR.cc/2024/Conference/Submission8262/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8262/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699641887051,
            "cdate": 1699641887051,
            "tmdate": 1700961796407,
            "mdate": 1700961796407,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W1Yep0o3pi",
                "forum": "gisAooH2TG",
                "replyto": "jbwFssE1h5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8262/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback, and we are glad that you find our work interesting. To address your concerns:\n\n**1. More environments and tasks.** We have included two new environments in this iteration with three new tasks. A description can be found in Section 4.1 and the results in Table 1. The environments are a cabinet scene with a lever that unlocks the cabinet (the cabinet is locked when the scene starts). The robot must reason that there are no physical obstacles in the scene and must figure out that pulling the lever unlocks the door in order to accomplish the task. The other environment contains a red crate with two blocks, one red and one yellow. The first task in this environment is to place the cube with the same colour as the crate on the crate, and so the robot must use the Perceiver to determine the correct cube (otherwise it could just get 50% by guessing). The final task is in the same environment; the robot must put the red cube on the crate but there is a yellow cube already on the crate, preventing placement of the red block. The robot must first remove the yellow cube. In addition to integrating these new tasks, we also increased the number of times we ran each of the 7 tasks (from 3 to 10) to further investigate the robustness of our method.\n\n**2. Testing planner module.** For each important motion in our tasks, we first do multiple MPC runs with ground-truth reward functions to test the stability. We pick four of the important motions and include the test results in Appendix C.1 in our updated draft.\n\n**3. VLM Perceiver Weakness.** We thank the reviewer for their suggestion. We have updated our prompting mechanism to alleviate the problems incurred by the VLM perceiver (the reviewer can refer to Appendix B in our updated draft for the detailed prompts, as well as Algorithm 1). As a result, we believe the VLM perceiver is no longer a dominant issue. In Table 1, we demonstrate the impact of only providing a text description of the objects in the scene at start-up (RePLaNn [no Perceiver]), which significantly drops the performance. We include experiments in Appendix C2 demonstrating the object recognition and object reasoning skills of two open-source VLMs and GPT-4V. We find that combining open-source models with segmentation models can also boost object recognition capabilities (Section 4.2 and Appendix C2). We include an additional example of why LLM-constrained VLM prompting is important for our method to work in Appendix D, where we ask GPT-4V to solve Tasks 3 and 6 out-of-the-box, and find that it is unable to. Finally, we include a more in-depth error analysis of the failure cases in Appendix C3 and find that there are still some small issues with the Perceiver-Planner interaction, which could be improved with better prompting. For example, in Task 6, the Planner asks the Perceiver \u201cWhich cube has the same colour?\u201d which is very vague, and the Perceiver ends up reply nonsensically and the robot picks the wrong cube."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8262/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338758684,
                "cdate": 1700338758684,
                "tmdate": 1700338758684,
                "mdate": 1700338758684,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]