[
    {
        "title": "A Foundation Model for Error Correction Codes"
    },
    {
        "review": {
            "id": "Wzgl5HEZwI",
            "forum": "7KDuQPrAF3",
            "replyto": "7KDuQPrAF3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1694/Reviewer_jdxb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1694/Reviewer_jdxb"
            ],
            "content": {
                "summary": {
                    "value": "The paper builds on top of Error Correction Code Transformer(ECCT), made a few structural change to build a \"foundational\" ECCT (FECCT) model for block code decoding. \n\nFECCT is a generalized version of ECCT:\n\n(1) FECCT is not length-dependent, H-dependent, and not even code-dependent, thus can be trained once can be used for wide range of block codes, with good performance (matching/beating ECCT mostly, and beat BP by a large margin.). Some finetuning can even lead to better performance.\n\n(2) FECCT could an important milestone of deploying neural decoders to real world given its potential, still a lot of hard work is required till that day."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) Overall, FECCT is very plausible method, since FECCT is more like an \"decoding algorithm\" rather than simply \"neural decoder\". My definition of \"decoding algorithm\" means the input is H matrix  and received codeword  (just like BP algorithm), and the output should be decoded message. While other neural decoder has dependency on code/length/H/etc. As of my understanding, FECCT has learned some interesting advanced BP-like algorithm, that can be beneficial for a wide family of block codes.\n\n(2) FECCT is built on top of ECCT, the generalization performance on non-zero codewords are preserved, which makes training feasible. FECCT's H-dependent attention is a generalized version of ECCT's attention, which lead to better decoding capability. The proposed neural structure makes sense, and lead to good generalization performance.\n\n(3) The experiment on unseen code with different code family and block length are interesting, which make (1)'s claim stronger that FECCT is more of an \"decoding algorithm\".\n\nOverall, this becomes an interesting work, at least for neural decoder research, first time shows that a \"decoding algorithm\" rather than a complicated mapping can be learned."
                },
                "weaknesses": {
                    "value": "1. The experiments are mostly built on short block codes (<128, test unseen for 255 at most), while typical capacity-approaching codes such as QC-LDPC has much longer block length. Performance on long block length is going to make this paper stronger, due to long block code's capacity-approaching performance.\n\n2. Interpretability: FECCT should have been learned some interesting algorithm, that can be interpreted as an advanced version of BP. We do see some part of interpretations in the appendix, but not solid enough to get insight on what FECCT's algorithm means.\n\n3. Complexity of network: attention-based neural network are very complex. Deploying the FEECT to any real world production requires some hard work on complexity reduction. In its current form, I am not seeing FECCT can be deployed to modem in short time.  Note that channel coding are heavily used in all modern wireless communication systems, which requires minimal latency, high throughput, and low cost."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1694/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698627560274,
            "cdate": 1698627560274,
            "tmdate": 1699636097872,
            "mdate": 1699636097872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3xeKSTrKo8",
                "forum": "7KDuQPrAF3",
                "replyto": "Wzgl5HEZwI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the very supportive review.\n\n## Code Length\nAs with most neural decoding works (cf. Related Works section), we focus on short-length codes for three main reasons. \n\n(i) On very large codes, some decoders (e.g., BP, SC) have been proven to achieve performance close to ML decoding on their respective codes of interest (e.g., LDPC, Polar, respectively), rendering the potential advantage of neural decoding solutions nonexistent/irrelevant.\n\n(ii) The training and/or deployment of the existing neural decoding techniques is not trivial/straightforward for very large codes (i.e., code lengths of thousands of bits).\n\n(iii)  The emergence of applications driven by the Internet of Things created the requirement for optimal decoders of short to moderate codes. For example, 5G Polar codes have code lengths of 32 to 1024 for uplink (UCI) or 128 to 512 for downlink [1].\nThese points are emphasized in the revision.\n\n[1]  ETSI 3GPP TS 38.212 \u201c5G NR Multiplexing and channel coding\u201d, v.16.5.0, 2021-03.\n\n## Interpretability\nWe provide in the revision\u2019s Appendix G the depiction of typical learned self-attention matrices.  As can be observed, initially the attention is mostly focused on the syndrome values in order to perform the decoding, while in the final layer, the focus is also transferred to the information bits.\n\n## Integration\nWhile being out of the scope of the paper, the deployment of the FECCT is indeed a challenge, requiring the application of many deep-learning acceleration techniques such as quantization (binarization) and sparsification (mainly of the self-attention module) as well as its FPGA/ASIC integration."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699960997919,
                "cdate": 1699960997919,
                "tmdate": 1699961203435,
                "mdate": 1699961203435,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8bY4TIg8Rt",
            "forum": "7KDuQPrAF3",
            "replyto": "7KDuQPrAF3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1694/Reviewer_w8i1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1694/Reviewer_w8i1"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts to develop foundation model for Error Correction Codes that is trained on large data so that it can be used later for any downstream task. Specifically, authors aim to adapt the Transformer input embedding for robustness to code length variations. To learn the code structure, they use the positional embedding, that is integrated into the self-attention via a learned mapping of the node distances in the Tanner graph. Moreover, for code awareness and channel noise prediction, the paper employs a size-invariant prediction module that is conditioned on the parity-check matrix. In simulations, they tested on codes that are unseen during training. They showed that the proposed FECCT method matches or sometimes perform better than the\nstate of art."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-the paper takes a foundational approach to the decoding problem in error correcting codes, which is intellectually interesting. Clearly being able to decode any type of code is an interesting intriguing  exercise.\n- the paper advanced the design of generalist decoders relative to existing generalist decoders by using new embedding and grounding techniques."
                },
                "weaknesses": {
                    "value": "-Although the design of foundational decoders are very interesting intellectual exercise, the real world impact of it is close to none if not zero.  The reason is that error correcting codes are designed and deployed once and their training is not a big deal even if someone takes deep neural network decoders as opposed to classical BP methods. But more importantly, there is another argument against the value of these generalist decoders: The important thing to recall is that capacity achieving codes exists for long codes and their BP decoders are close to ML performance, I.e., optimal decoders. So there is no gain of these deep neural decoders in long codes. The focus should be short codes for which we do not have good BP decoders. However, for short codes, we really do not need foundational decoders as one can design and easily train specialized neural decoders for the short codes that will very likely beat the performance of generalist decoders for all lengths. It is clear to believe that a generalist decoder will not be able to perform a specialized deep neural decoder for short lengths, unless the authors can show their generalist decoder can beat the performance of state of art short length (less that 200 bits) code decoders, specialized for that specific code length.\n\n-what is the performance in the tables? I see that 3 different values of Eb/N0 is used as the channel input signal (bit) power to noise ratio but What about reported numbers as performance. What are they? I like to see how these reported numbers translate to the error rate performance, as it is the only thing that matters in communication. It does not look like that the authors picked a particular error rate and report the corresponding Required Eb/N0 to achieve such an error rate. Because in that case the lower number is associated with the better scheme not the higher (as the authors stated in the paper).\n\n-the authors need to compare their scheme with Choukroun & Wolf (2022b) which is shown to be superior to ECCT."
                },
                "questions": {
                    "value": "please compare the proposed generalist decoder at short lengths (less than 200 bits) with that of specialized decoders at those lengths. Because as I pointed out this is where these foundational decoders would show value if any.\n\n-please plot error rate plots rather than the reporting used in the paper which is not insightful.\n\n-it would be helpful to compare your proposed work with that of\nChoukroun & Wolf (2022b) which extends and enhances ECCT via Denoising diffusion in error correction codes and have far superior performance than ECCT.\n\n-the paper novelty is arguable in light of ECCT design architecture. For the most part following similar development as ECCT. Can the authors elaborate on the novelty relative to ECCT."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "There is none."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1694/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719635093,
            "cdate": 1698719635093,
            "tmdate": 1699636097793,
            "mdate": 1699636097793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oxbfkkrPGd",
                "forum": "7KDuQPrAF3",
                "replyto": "8bY4TIg8Rt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. We believe several points have been misunderstood and we have modified the manuscript to make it clearer.\n\n>\u201cThe reason is that error correcting codes are designed and deployed once and their training is not a big deal even if someone takes deep neural network decoders as opposed to classical BP methods\u201d\n\nWhile we agree with the reviewer that training is not a major limitation compared to deployment, we respectfully disagree with the reviewer regarding the integration/deployment side. Current error-correction systems (e.g., base stations) can/need to decode codes of different lengths and/or different families. For example, LDPC codes of different lengths are decoded on HW by omitting some of the integrated processing engines of the BP implementation for shorter codewords.\n\nMoreover, deploying one single neural decoder for several types of codes and/or lengths is fundamentally different from integrating several neural decoders, especially in terms of die space and storage/memory (one model\u2019s weights vs many models\u2019 weights).\n\nWe remind here that existing neural decoders are designed and trained on one single code to be integrated into dedicated HW.\n\nThis point is emphasized in the revision.\n\n>\u201cHowever, for short codes, we really do not need foundational decoders as one can design and easily train specialized neural decoders for the short codes that will very likely beat the performance of generalist decoders for all lengths.\u201d\n\nFollowing our previous answer, the deployment of a specialized neural decoder has major integration drawbacks.\n\n Most importantly, the main goal of our work was to show our method, while being code/length-invariant (referred to as \u201cgeneralist decoder\u201d by the reviewer), can match and even outperform the \u201cspecialized\u201d state-of-the-art neural decoder.\n\nIn addition to the points above that address the direct utility, foundation models support downstream tasks. Specifically, the ability to move quickly between codes enables the sampling and evaluation of random codes. Moreover, since our method defines a universal differentiable neural decoder, it opens the door to the optimization and definition of new families of codes.\n\nThese points are emphasized in the revision.\n\n\n>\u201cWhat is the performance in the tables?What about reported numbers as performance. What are they? I like to see how these reported numbers translate to the error rate performance\u201d\n\nAs described in Section 5, third paragraph, the reported numbers are the \u201cnegative natural logarithm bit error rates (BER) for three different normalized SNR values (Eb/N0)\u201d, meaning we provide $-\\ln(BER)$ (such that higher is better) for three Eb/N0 values. This dense presentation for multiple codes of the BER on the SNR of most interest has been borrowed from many of the previous neural decoding works.\n\n> \u201cplease compare the proposed generalist decoder at short lengths (less than 200 bits) with that of specialized decoders at those lengths. Because as I pointed out this is where these foundational decoders would show value if any.\u201d\n\nWhile our work focuses on the development of neural decoders, the previous work cited by the reviewer (e.g., [1],[2])  already provided a comprehensive comparison with specialized decoders. In our work, we compare with it. \n\nThe revised text now points to these results.\n\n>\u201cit would be helpful to compare your proposed work with that of Choukroun & Wolf (2022b) which extends and enhances ECCT via Denoising diffusion in error correction codes and have far superior performance than ECCT.\u201d\n\nWe wish to remind the main difference between ECCT and DDECC, as described in Section 6 paragraph 2. While ECCT focuses on the design of a new neural *decoding architecture*, DDECC is a generic iterative *decoding algorithm* applied to a given backbone architecture. As described in [2] Section 4.5, the ECCT has been chosen since it is the state-of-the-art decoding architecture.\n\nOur model focuses on the first family of work (i.e., decoding architecture) that can, same as ECCT, be coupled with DDECC and makes it code and length invariant too. While these results would be interesting to see, they require many more experiments and much heavier resources.\n\nThis point is emphasized in the revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699960865320,
                "cdate": 1699960865320,
                "tmdate": 1699961614336,
                "mdate": 1699961614336,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JMsnyrNFfd",
                "forum": "7KDuQPrAF3",
                "replyto": "8bY4TIg8Rt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">\u201cCan the authors elaborate on the novelty relative to ECCT.\u201d\n\nWe believe the paper differs from ECCT as much as ECCT differs from the original Transformer architecture.\n\nECCT brings three modifications to the original Transformer architecture. (i) An initial bit-wise embedding (i.e., $n$ encoding vectors, with $n$ the code length). (ii) Thresholded binary masking. (iii) A Linear (FC) prediction layer.\n\nFECCT redefined these three contributions in order to (1) make them code/length-invariant, and (2) improve over them (c.f. Table 3).\n\n(i) The initial embedding is restricted to three embedding vectors, one vector for each bit to be modulated by the channel\u2019s output magnitude and a one-hot encoding defined by the syndrome values.\n\n(ii) The masking is totally different in its definition and implementation and is defined as \n$softmax(QK^{T})\\odot \\psi(\\mathcal{G}(H))$ compared to $softmax(QK^{T}+g(H))$.\nHere $g(H)\\in \\\\{-\\infty,0\\\\}$ denotes ECCT\u2019s binary thresholding, while FECCT provides a fine-grained masking based on the Tanner graph distances.\n\n(iii) The prediction module is fundamentally different than a Linear layer and is based on the parity-check matrix in order to remain invariant while being code-aware.\n\n>\u201c-please plot error rate plots rather than the reporting used in the paper which is not insightful.\u201d\n\nWe now also provide BER curves for several codes in Appendix F.\n\n## References\n[1] Error Correction Code Transformer, Y. Choukroun and L. Wolf, NeurIPS 2022\n\n[2] Denoising Diffusion Error Correction Codes, Y. Choukroun and L. Wolf, ICLR 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699960889344,
                "cdate": 1699960889344,
                "tmdate": 1699961469890,
                "mdate": 1699961469890,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YTB0dw0woa",
                "forum": "7KDuQPrAF3",
                "replyto": "8bY4TIg8Rt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_w8i1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_w8i1"
                ],
                "content": {
                    "title": {
                        "value": "Short length vs long length & performance issue"
                    },
                    "comment": {
                        "value": "Thanks authors for clarifying a few things. However, I beLieve you misunderstood my inputs. (1) I am questioning the value of this work in real world. Again, I do not debate its intellectual exercise, which is good in my mind. But I disagree with authors in their argument for its real world implications for the following reasons: for long codes, we already have very good capacity achieving codes (e.g., LDPC and polar) that are rate compatible so we don\u2019t have to worry about their rate variations. The authors are likely aware that LDPS codes, for example, can be punctured randomly and create very good rate compatible codes with BP decoding near their ML performance. So we should all agree that no neural based codes are needed for such lengths. Te need is in short lengths from practical impact, that is where all the potential resides for neural solutions. I would be, however, impressed if the authors can find decoders that are good for very short lengths (specially for Reed Muller codes) as we do not have good decoders for such cases.  But the issue is that the authors show in appendix F that Fine tuned ECCT (and even initial ECCT, for the most part) always perform better than their codes in short lengths. So how we can justify the need for generalist decoder from practical point of view?\n\nRegarding hardware complexity of decoding various codes, if the environment is a private network, it is setup to operate on the rate variation of the same type of code so I am not sure we need generalist decoder for that. If it is wireless cellular network, each network provider has its own parameter settings and uses the same code, that are preset, so I do not see any complications arise for decoding.\nSo from practical point of view, I would rather to see a work that  contributes to take one rate adaptive code with very good decoder at various rates rather than a generalist decoder for various types of codes and lengths  at these short lengths. Again, I acknowledge that the work is intellectually good but I am not convinced about the usefulness of this line of work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112260519,
                "cdate": 1700112260519,
                "tmdate": 1700112260519,
                "mdate": 1700112260519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BsZNrUuAvW",
                "forum": "7KDuQPrAF3",
                "replyto": "8bY4TIg8Rt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the prompt reply! We are happy to hear that we have been able to clarify some issues and are very encouraged by the reviewer\u2019s acknowledgment of the value of the work as basic research with good intellectual value. \n\n>The need is in short lengths from practical impact, that is where all the potential resides for neural solutions.\n\nWe agree with the reviewer that short codes are (*including our work*) and should be the focus of neural decoders. In the revision, we have added a paragraph to the Related Work section discussing this point.\n\n>But the issue is that the authors show in appendix F that finetuned ECCT (and even initial ECCT, for the most part) always perform better than their codes in short lengths. So how we can justify the need for a generalist decoder from the practical point of view?\n\nThis is a misunderstanding that stems from an unfortunate typo we had in the figure captions (now fixed in red). We deeply apologize for this. There is no such thing as zero-shot or finetuned ECCT, since ECCT is trained for a single code. Appendix F, as well as the Tables in the main text, show the performance of the SOTA ECCT compared with the performance of the proposed *Foundational* ECCT (FECCT) on zero-shot (i.e., unseen codes) and finetuned codes (i.e., short training on a code of interest) tasks. The results, along with the ablation study, support the fact that our method, while being length/code/rate-invariant and having fewer parameters, is able to reach and even outperform the code-dedicated SOTA ECCT. \n\nTo be clear, for 100% of the cases, our finetuned version outperforms the ECCT. In a limited number of **zero-shot** runs, this is not the case. As described in Section 6, the discrepancy in performance between the different families of codes is due to the underrepresentation of some code families in the training set.\n\n>Each network provider has its own parameter settings and uses the same code, that are preset, so I do not see any complications arise for decoding.\n\nWe respectfully disagree. A network provider does not define its own parameters but must acquire/use telecommunication infrastructures and hardware that follow a given standard. For example, 5G supports both LDPC and Polar codes [1], meaning that modern systems (supporting 2G to 5G standards) must be able to decode (on device) different families of codes (e.g., BCH, LDPC, Polar). We can also find similar multiple decoders in [Concatenated error correction code](https://en.wikipedia.org/wiki/Concatenated_error_correction_code).\n\n[1] ETSI 3GPP TS 38.212 \u201c5G NR Multiplexing and channel coding\u201d, v.16.5.0, 2021-03.\n\n>...\u201dit is setup to operate on the rate variation of the same type of code\u201d... \u201cSo from practical point of view, I would rather to see a work  that contributes to take one rate adaptive code with very good decoder at various rates rather than a generalist decoder for various types of codes and lengths at these short lengths.\u201d\n\nExisting neural decoders are not rate-adaptive at all. For example, the state-of-the-art ECCT requires a new *distinct* model for each rate (or even length) of interest. Our neural decoder is absolutely rate-adaptive by construction. As far as we can ascertain, this is the first time a neural decoder is length/rate invariant, enabling the potential practical integration of one neural decoder for any rate/length. \n\nIf needed, further specialization of the foundation model can be obtained simply by training it on a single family of codes. Since this is against the basic scientific ambition to generalize, we did not attempt this, but it is a valid future direction.\n\nTo summarize: our method is both conceptually and architecturally novel. It has multiple advantages on short/moderate codes: (1) The method reaches and outperforms the SOTA neural decoder (even on unseen codes), (2) The method has a smaller capacity than the SOTA (i.e. fewer parameters), and (3) The method is totally length/rate/code-invariant."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147921894,
                "cdate": 1700147921894,
                "tmdate": 1700309414636,
                "mdate": 1700309414636,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d7Sqhbp1cs",
                "forum": "7KDuQPrAF3",
                "replyto": "BsZNrUuAvW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_w8i1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_w8i1"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for your response and the clarification of your simulation error.\nHowever, your response regarding the usefulness of the generalist code remains unmoving: 1. In communication, every little compromise of rate (relative to capacity) counts. The foundational Mode is making significant sense in ML because there are other downstream tasks that people then wish to manipulate for their downstream objectives. However, in error control coding, we never accept that one to design a decoder that can decode multiple codes at the cost of rate drop (from capacity). The goal is to get best code and best decoder to have near capacity performance, whether in asymptotic regime or finite length regime (I.e. finite length capacity). The generalist code abandons that goal, unless the authors show that their decoder performs better than any other decoder out there, which is not true! Until, then, I maintain my view that these codes will find no use in reality other than intellectual exercise, which I give that to authors. The focus should be either the code design or decoder design for short length regimes with the best performance. Additionally, I wish to see how much rate you are compromising compare to the best decoders (classic or otherwise) Reed Muller codes in short length to arrive at the same error performance. Can the authors provide the comparison state of art decoder for ReedMuller and show how much performance their generalist decoder compromise on? Thanks!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663398966,
                "cdate": 1700663398966,
                "tmdate": 1700663398966,
                "mdate": 1700663398966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "osjhz9Crj9",
                "forum": "7KDuQPrAF3",
                "replyto": "8bY4TIg8Rt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the ongoing discussion and for acknowledging, again, the intellectual value of our work. The remaining concern seems to focus not on the potential of the method but on its capability to provide universally superior decoding performance.\nAllow us to clarify our motivation with respect to your remarks.\n\n> and the clarification of your simulation error.\n\nWe wish to note we did not have any simulation error, just a typo in the caption of one of the rebuttal figures (\u201cfine-tuned ECCT\u201d instead of  \u201cfine-tuned **F**ECCT\u201d). As mentioned in the previous answer, there is no such thing as \u201cfine-tuned ECCT\u201d, since ECCT is trained from scratch on each code. \n\n>unless the authors show that their decoder performs better than any other decoder out there, which is not true!\nThis seems to be the core of the reviewer\u2019s concern and it is exactly what our foundation neural decoder line of work aims to achieve. The main dispute is whether a first work in this direction needs to be the ultimate solution.\n\nWe wish to discuss a few points.\n\n1. Our method obtains state-of-the-art in neural decoding almost always for inference-time zero-shot (remarkably) on codes that are unseen during training, and across all experiments for (i) codes from the training set without further training and (ii) finetuned on unseen codes.\n2. Previous neural decoders have been shown to outperform or almost match the performance of the best classical decoders (cf. [1,2] for LDPC or BCH codes (BP) and [3] for Polar codes (SCL)).\n3. Our method, as a machine learning-based solution in general and a neural decoder in particular, may not be the *\u201cbest possible universal decoder on every possible code\u201d* ([no free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)). \nStill, our approach contributes significant new scientific advancements in pursuit of this objective, benefiting both the machine learning and error-control coding fields.\n4. Our state-of-the-art method is the first neural decoder to remain code, rate, and length invariant, enabling the performance and deployment advantages mentioned in the manuscript and the previous answers.\n5. As previously mentioned, if the universal decoding is not of *practical* interest, our method can be trained or fine-tuned *to decode a few or several families of interest* with high accuracy and potential efficient integration.\n6. Finally and as mentioned by another reviewer, this is the first time a learned \"decoding algorithm rather than a complicated mapping\u201d is proposed, enabling both the analysis of the learned decoding and the potential *differentiable* design of new codes.\n\n> Can the authors provide the comparison state of art decoder for ReedMuller\n\nWe provided the performance of our model on six families of codes and many different rates. Reed-Muller codes (related to the Polar codes) are out-of-domain (OOD) for our model as they are not present in our training set, so it is an interesting experiment, especially since *it has not been tested in previous neural decoding works*. Unfortunately, due to time constraints, we will not be able to provide such results within the remaining hours of the discussion phase.\n\nIn the context of OOD experiments, please note the OOD zero-shot experiment in Appendix H of the revision, in which the code is 7x the length of the maximum code length in our training set.\n\n[1] *Deep Learning Methods for Improved Decoding of Linear Codes*, Nachmani et al., IEEE Journal of Selected Topics in Signal Processing, 2018\n\n[2] *Error Correction Code Transformer*, Y. Choukroun and L. Wolf, NeurIPS 2022 \n\n[3] *Denoising Diffusion Error Correction Codes*, Y. Choukroun and L. Wolf, ICLR 2023"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725939452,
                "cdate": 1700725939452,
                "tmdate": 1700726047612,
                "mdate": 1700726047612,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YdrpK0fTYU",
            "forum": "7KDuQPrAF3",
            "replyto": "7KDuQPrAF3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1694/Reviewer_dPJy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1694/Reviewer_dPJy"
            ],
            "content": {
                "summary": {
                    "value": "This work embarks on a very ambitious journey towards creating a foundation code for all downstream ECCs. The suggested structure here largely depends on a prior NeurIPS paper ([NeurIPS\u201922] Choukroun et al., Error Correction Code Transformer, presumably by the same authors) in that it is a specialized Transformer with bitwise embedding and parity-check-matrix-dependent masking, as appropriate for ECC. Albeit similar to prior work, the present proposal contains enough new materials and gives a highly convincing architecture based on code-aware aggregation that depends on the parity-check matrix as well as code-invariant bitwise embedding."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed idea is very ambitious, and is based on a highly innovative specialization of the Transformer to the classical problem of decoding received codewords of linear codes. The proposed ideas/strategies are very interesting and convincing (such as bitwise embedding independent of particular codes and incorporation of the parity check matrix in the embedding function). The impact on the field of digital communication and data storage could be large."
                },
                "weaknesses": {
                    "value": "The main issue is that the training of the model is done using codes with lengths up to 150 only, hardly a sufficient length to reflect many modern codes of important applications (testing is also done on relatively small codes, with the largest being a 255 bit BCH). The popular LDPC codes are also curiously missing in the training as well as in the performance evaluation. Likewise for meaningfully long Polar codes. In this sense, I am not sure if the term \u201cfoundation model\u201d is justified here. In sum, the idea seems very good, but the validation comes short of a reasonable expectation. I do not feel just saying \"we have limited computing resources\" would be a good enough excuse for such an ambitious title.\n\nWritings on various parts seem direct copies from [NeurIPS\u201922] Choukroun et al., Error Correction Code Transformer. Try to differentiate."
                },
                "questions": {
                    "value": "Please respond to the mentioned weaknesses above.\n\nIn Tables 1 and 2, the proposed method seem noticeably worse than ECCT on larger codes. Also, In Table 3, ECCT+DM+II gives the best results. Explanations would be  good."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1694/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698859863764,
            "cdate": 1698859863764,
            "tmdate": 1699636097726,
            "mdate": 1699636097726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CzV9WNRbaN",
                "forum": "7KDuQPrAF3",
                "replyto": "YdrpK0fTYU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the supportive review and helpful recommendations.\n\n## Code Length and Type\nAs with most neural decoding works (cf. Related Works section), we focus on short/moderate-length codes for three main reasons. \n\n(i) On very large codes, some decoders (e.g., BP, SC) have been proven to achieve performance close to ML decoding on their respective codes of interest (e.g., LDPC, Polar, respectively), rendering the potential advantage of neural decoding solutions nonexistent/irrelevant.\n\n(ii) The training and/or deployment of the existing neural decoding techniques is not trivial/straightforward for very large codes (i.e., code lengths of thousands of bits).\n\n(iii)  The emergence of applications driven by the Internet of Things created the requirement for optimal decoders of short to moderate codes. For example, 5G Polar codes have code lengths of 32 to 1024 for uplink (UCI) or 128 to 512 for downlink [1].\n\nWhile the training/testing codes have been randomly selected from the available codes database, as described in the Appendix, we do provide the performance of LDPC (CCSDS) \nhttps://rptu.de/channel-codes/channel-codes-database/more-ldpc-codes#c94700\n\nFinally, as the reviewer noticed, our current computing resources (described in the paper) cannot allow the efficient training of codes with thousands of bits.\n\nThese points are emphasized in the revision.\n\n\n## Differentiating the Text from ECCT\nAs far as we can see, the only part similar to the ECCT is Section 3.1 (now revised to be just the start of Sec 3), which reviews the standard coding setting background. We made several changes to further differentiate this subsection. \n\n\n## Performance Compared to ECCT on Larger Codes\nWe do not observe worse performance on larger codes (e.g., BCH or CCSDS codes), except for the Polar codes only. We believe that this arises from two main reasons. (i) The dataset is highly unbalanced, inducing a low representation of the Polar codes (cf. Section 4.4 paragraph 6, Section 6 paragraph 3, and Appendix C). (ii) Following [2,3,4], we standardized the parity-check matrices of Polar codes (but not of other codes), which may have had an adverse effect on the learned inductive bias.\nThat being said, after fine-tuning, ECCT\u2019s performance is outperformed on Polar codes as well, as described in Figure 4.\n\nThese points are emphasized in the revision.\n\n## Table 3 Explanation\n\nWe can observe from Table 3 that even while using ECCT masking, invariant initial (II) embedding remains better than using a distinct embedding for each bit (i.e., regular ECCT), where the bit position is taken into account by the FC layer. However, we can observe that using invariant output (IO) coupled with II loses positional information, inducing a drop in accuracy.\n\nA similar effect appears also for ECCT+DM+II which improves over ECCT\u2019s original initial embedding. The conclusion is that ECCT is worse than FECCT in all of its components: initial encoding, masking, and final encoding.\n\nWe note here that, as described in the caption of Table 3, ECCT has a larger capacity which makes an exact comparison difficult. \n\nThese points are emphasized in the revision.\n\n## References\n[1]  ETSI 3GPP TS 38.212 \u201c5G NR Multiplexing and channel coding\u201d, v.16.5.0, 2021-03.\n\n[2] Error Correction Code Transformer, Y. Choukroun and L. Wolf, NeurIPS 2022\n\n[3] https://github.com/yoniLc/ECCT\n\n[4] How to Mask in Error Correction Code Transformer: Systematic and Double Masking, S. Park et al., arXiv:2308.08128"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699960270528,
                "cdate": 1699960270528,
                "tmdate": 1699960782369,
                "mdate": 1699960782369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ze1BndGh1J",
                "forum": "7KDuQPrAF3",
                "replyto": "YdrpK0fTYU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank again the reviewer for the valuable feedback. We hope that we have comprehensively addressed all the raised questions and remarks.\n\nShould you have any additional inquiries, please do not hesitate to inform us.\u00a0If we have satisfactorily addressed your concerns, we would appreciate your consideration in revisiting\u00a0the\u00a0score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549331227,
                "cdate": 1700549331227,
                "tmdate": 1700644576698,
                "mdate": 1700644576698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fQoHAZa1vw",
                "forum": "7KDuQPrAF3",
                "replyto": "ze1BndGh1J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_dPJy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_dPJy"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response, but it appears that the authors are saying that since there already exist good decoders for very large codes, there is no room for a foundational decoder there. Isn't the motivation for a foundation decoder to have a single decoder design that can handle decoding of any codes (or a wide variety of practical codes) with only a little tweaking (finetuning), without having to design separate decoders for different codes? For any specific short or moderate length code, there already are reasonably good practical decoders that are in use today. So following your argument, I do not see a need for a neural decoder solution thetre as well. You also say the training and/or deploying existing neural decoding techniques is not trivial/straightforward for very large codes (i.e., code lengths of thousands of bits). So you were not able to valiadate your method for large codes. Shouldn't you then revise your title to A Foundation Model for Short (or Moderate Length) Error Correction Codes? \n\nOverall, all my concerns remain the same after the author rebuttal, and I am atually tempted to lower my score. Nevertheless, I still feel that the approach is novel and promissing, so I will resist my temptation and keep my score as is."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644271617,
                "cdate": 1700644271617,
                "tmdate": 1700644271617,
                "mdate": 1700644271617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xEU0mhYvfQ",
                "forum": "7KDuQPrAF3",
                "replyto": "0S958jWCYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_dPJy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Reviewer_dPJy"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for clarification and additional validation on a large BCH code. I remain positive and will recommend acceptance."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662000630,
                "cdate": 1700662000630,
                "tmdate": 1700662000630,
                "mdate": 1700662000630,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Oob4Qm7cH",
            "forum": "7KDuQPrAF3",
            "replyto": "7KDuQPrAF3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1694/Reviewer_1rnF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1694/Reviewer_1rnF"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a model and is trained on multiple codes and can then be applied to an\nunseen code.\nTransformer architecture in multipleways: \n(1) a code-invariant initial embedding, which is also position- and lengthinvariant,\n(2) a learned modulation of the attention maps that is conditioned on the Tanner graph\n(3) a length-invariant code-aware noise prediction module that is based on the parity-check matrix"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.Error control coding implemtation on the deep learning technique is highly encourgaing.\n2.Authors got the optimized Results in terms of BER."
                },
                "weaknesses": {
                    "value": "1.Conclusion should be rewritten based on the results presented mentioning the future scope."
                },
                "questions": {
                    "value": "1.Conclusion should be rewritten based on the results presented mentioning the future scope."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1694/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1694/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1694/Reviewer_1rnF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1694/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699070780000,
            "cdate": 1699070780000,
            "tmdate": 1699636097660,
            "mdate": 1699636097660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V4rJiYimFf",
                "forum": "7KDuQPrAF3",
                "replyto": "1Oob4Qm7cH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1694/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the very supportive review.\n\nThe conclusion is rewritten and includes the future scopes of research as follows.\n\n*\u201cNext steps should focus on the deployment of the method on existing error correction embedded systems via the utilization of Transformer acceleration methods along with extensive tuning and training of the codes and channels of interest. Additionally, the definition of a universal differentiable neural decoder may open the door to the optimization of codes and even to the learning of new families of codes.\u201d*"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1694/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699959792455,
                "cdate": 1699959792455,
                "tmdate": 1699959792455,
                "mdate": 1699959792455,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]