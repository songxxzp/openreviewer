[
    {
        "title": "Big Learning Variational Auto-Encoders"
    },
    {
        "review": {
            "id": "A8jluDJSrn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_MAEj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_MAEj"
            ],
            "forum": "pUGjLB0N4l",
            "replyto": "pUGjLB0N4l",
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose big learning VAE to model the inputs from a more general and universal loss perspective. The authors introduce some high-level ideas on big learning, whose objectives should comprise of joint, conditional, and marginal matching tasks. The paper mostly explains the motivation and insights of such big learning concept. The contents also mostly summarizes all different VAE objectives. The final proposed biglearn-VAE is validated on MNIST, FashionMNIST and CelebA, on some inference and in-painting tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provides and describes many insightful thoughts regarding the universal learning objective for VAE. \n2. The core notations and concepts are explained in a self-contained manner.\n3. The experiments showcase some capabilities of biglearn-VAE."
                },
                "weaknesses": {
                    "value": "1. Many claims are made on top of the ideal assumptions, while such assumptions are usually hard to achieve. Actually, VAE framework itself is developed based on the fact that $p(x)$ is intractable.\n2. Even though you are proposing some universal framework, it's still better to introduce some running examples to facilitate the understanding of your claims. For example, you can use a MNIST example for illustration of different modeling scenarios.\n3. During my reading, I do have a feeling that you are more or less re-introducing big learning from a high level, without more concrete or statistical analysis.\n4. Section 3.2.3 essentially proposes a more general form of VAE ELBO. However, regarding the specific cases, one still have to reduce the form to more specific ones that we are more familiar with. Moreover, when you claim sth big, readers expect some more convincing and comprehensive experimental results. \n5. MNIST, FashionMNIST are well-studied datasets. I'm not sure if they really need large foundation models.\n6. Also, the major results you are showing are mostly qualitative, lacking more concrete quantitative ones."
                },
                "questions": {
                    "value": "1.Have you tried your method on some larger-scale datasets?\n2. You mentioned text tasks multiple times in the paper, however, there is no text applications in the experimental section?\n3. As you mentioned the utilization of large foundation models, maybe it's more convincing if you can give a use case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697435560924,
            "cdate": 1697435560924,
            "tmdate": 1699636155704,
            "mdate": 1699636155704,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AIvZtwOaEc",
                "forum": "pUGjLB0N4l",
                "replyto": "A8jluDJSrn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment !"
                    },
                    "comment": {
                        "value": "- **Q1:** Many claims are made on top of the ideal assumptions, while such assumptions are usually hard to achieve.\n\n  **A1:** On one hand, it is an undeniable fact that ideal assumptions are challenging to fully realize in practical applications. However, the value of these ideal assumptions should not be overlooked. They are underpinned by solid theoretical support, providing a necessary theoretical framework that aids in a deeper understanding of the core issues and fundamental laws of this field. By setting idealized conditions, we can more clearly reveal the basic mechanisms behind complex phenomena, laying a solid theoretical foundation for future empirical research and practical application. On the other hand, our method is inspired by insights drawn from foundation models. These foundation models have already demonstrated groundbreaking success and impressive capabilities. Our approach aims to further transfer the model proficiency to the VAE domain, thereby enhancing the performance of VAE models.\n\n- **Q2:** It better to introduce some running examples to facilitate the understanding of your claims.\n\n  **A2:** We have added examples of model capabilities to the revised paper, please refer to Appendix F for details. Using MNIST as an example, we demonstrate the BigLearnVAE's capabilities in ample/generation, inference, reconstruction, in-painting, and conditional sample/generation.\n\n- **Q3:** more or less re-introducing big learning from a high level, without more concrete or statistical analysis.\n\n  **A3:** Reintroducing big learning is intended to facilitate a better understanding of its fundamental concepts, especially for readers who may not have an in-depth familiarity with them. To strengthen our manuscript, we plan to incorporate more detailed statistical evidence and specific examples, which will solidify the theoretical framework we have outlined. \n\n- **Q4:**  Section 3.2.3 essentially proposes a more general form of VAE ELBO. ... reduce the form to more specific ones .... \n\n  **A4:** As defined in Section 3.2.3:\n\n  \u200b\t\tWhere $\\mathbb{T} \\neq \\emptyset$, the analysis under different $(\\mathbb{S},\\mathbb{T})$ settings is as follows\uff1a\t\n\n  \u200b\t\t\t1. When $\\mathbb{S} = \\emptyset, \\mathbb{T} = \\mathbb{U}$, BigLearnELBO degenerates to JointELBO.\n\n  \u200b\t\t\t2. When $\\mathbb{S}=\\emptyset,\\mathbb{T}\\neq\\mathbb{U}$ \uff0cBigLearnELBO degenerates to MarginELBO.\n\n  \u200b         \t3. When $\\mathbb{S}\\neq\\emptyset$ \uff0cBigLearnELBO degenerates to ConditioELBO.\n\n- **Q5:** MNIST, FashionMNIST are well-studied datasets. I'm not sure if they really need large foundation models.\n\n  **A5:** Our focus is not on applying foundation models to datasets like MNIST and FashionMNIST, but rather on utilizing the principles of big learning to upgrade traditional VAEs, maintaining the same amount of parameters as traditional VAEs. Moreover, traditional VAEs lack the versatile capabilities exhibited by big learning VAE.\n\n- **Q6\uff1a** About the larger-scale datasets, text tasks  and a use case  utilization of large foundation models .\n\n  **A6:** Thank you for suggesting experiments on a larger scale or with text datasets. We plan to explore these in future work. The primary aim of our paper is to enhance VAE through insights drawn from foundation models. Our focus has been on demonstrating this effectiveness using well-known and easily understandable VAE datasets. \n  Given that VAEs haven't been used to train a foundation model, we lack a baseline for comparison. Proposing BigLearnVAE as a novel foundation model, with its demonstrated capabilities, would itself be substantial enough for a separate paper. We consider this an avenue for future"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732945375,
                "cdate": 1700732945375,
                "tmdate": 1700732945375,
                "mdate": 1700732945375,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NN4fuDCnmB",
            "forum": "pUGjLB0N4l",
            "replyto": "pUGjLB0N4l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_DcvD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_DcvD"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies learning the Variational Auto-encoder (VAE) with the \"Big-learning\" scheme, which is inspired by the foundation models. Such a learning scheme aims to exploit the large-scale training data with diverse domains exhaustively. The experiments demonstrate the inference capability of such learned VAE."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation for learning a robust VAE is clear and can contribute to such active research fields for generative models. \n2. The organization of the paper is generally well-presented.\n3. Although I have only limited experience with such a \"big-learning\" scheme, I feel like the potential of VAE should be further explored, as many prior works did, and this paper addresses this with a promising direction in a big picture."
                },
                "weaknesses": {
                    "value": "1. I don't have much experience in this particular field, so it is quite difficult for me to follow the paper. The authors intend to apply a robust and general learning scheme for the VAE, with exhaustive data utilization in a multi-modal learning manner (e.g., text and image domains), but some notations seem to be confusing, such as the \"joint matching\" for marginal distribution p(x).\n2. With such a powerful learning scheme, the experiments only demonstrate the inference capability. So I wonder how such a learned VAE performs for generation quality, cross-domain sampling, adversarial robustness, and other standard benchmarks."
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2221/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2221/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2221/Reviewer_DcvD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698072593241,
            "cdate": 1698072593241,
            "tmdate": 1699636155635,
            "mdate": 1699636155635,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I9zPBXornx",
                "forum": "pUGjLB0N4l",
                "replyto": "NN4fuDCnmB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment !"
                    },
                    "comment": {
                        "value": "Thank you for considering our work to have exploratory potential. Indeed, our method is part of a promising direction within the bigger picture. It is orthogonal to other methods of enhancing VAE, representing  a fresh  and innovative direction.\n\n- **Q1:** ''joint matching'' for marginal distribution p(x).\n\n  **A1:** Generally, in traditional VAEs, ''Joint'' refers to the joint distribution $p(x, z)$ of the data x and the latent variable z, whereas ''Margin'' refers to $ p(x)$. However, in the context of big learning, we approach from the perspective of data where ''Joint'' specifically denotes the distribution of the complete data p(x), and ''Margin'' refers to $p(x_{\\mathbb{T}})$. Joint matching refers to $p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) \\longrightarrow q(\\boldsymbol{x})$, where $x \\sim q(x)$ denotes a complete data sample, that is, $\\mathbb{S} \\cup \\mathbb{T} = \\mathbb{L}$. For a more detailed description, you can refer to section 3.2.1 of the manuscript.\n\n- **Q2:** How such a learned VAE performs for generation quality, cross-domain sampling, adversarial robustness, and other standard benchmarks ?\n\n  **A2:** In our manuscript, we demonstrate the model's inference capability, as the inpainting task effectively highlights its ability for conditioned generation. As demonstrated in Appendix C, Figure 8 of the manuscript, our approach showcases a diverse capability in conditional generation. This is particularly notable even when the vast majority of the image area is masked, our model can still generate varied results in terms of hairstyles, facial features, and skin tones, thereby achieving good generative performance. Regarding the cross-domain sampling and related topics you mentioned, we plan to explore these in future work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732143668,
                "cdate": 1700732143668,
                "tmdate": 1700732143668,
                "mdate": 1700732143668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oCupo3wtcA",
            "forum": "pUGjLB0N4l",
            "replyto": "pUGjLB0N4l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_M8cU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_M8cU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed the BigLearn-VAE, inspired by the big learning theorem.  The paper presents extensive analysis and derivation associated with how BigLearn-VAE is motivated, and presents empirical evidences to justify the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: The paper introduces the BigLearn-VAE, drawing inspiration from the big learning theorem. \n\nS2: It provides a comprehensive analysis and derivation of the underlying principles driving the BigLearn-VAE approach, supported by empirical evidence that substantiates its efficacy.\n\nS3: Empirical evidence shows that the proposed method achieves better ELBO than the vanilla VAE model. The proposed method also can lead to better (visually) generative samples."
                },
                "weaknesses": {
                    "value": "However, the paper seems to lack clarity, which makes the algorithm flow hard to interpret. For example, some of my confusions (owing to the clarity issue) are: \n\nW1: Take for instance, what exactly is the conditional distribution $q(x_t, Z| x_s ) is$, and how to sample from it? I am confused why samples $x_t$ should be dependent on other samples such as $x_s$ rather than independent with each other. \n\nW2: This model seems to be exactly the same as in the conditional VAE, e.g., CVAE in [A]. Can you please distinguish your Biglearn with the work in CVAE [A] ?\n\nW3: It seems the algorithm is only compared with vanilla VAE, whereas there have been many other SOTA version of VAE. The empirical evidence does not address the comparisons with these methods. \n\nW4: An algorithmic flow will probabaly help in terms of clarity when presenting the sampling procedure. \n\nReference:\n[A] Kihyuk Sohn et al., Learning Structured Output Representation using Deep Conditional Generative Models. nips 2015"
                },
                "questions": {
                    "value": "Please see the 4 weakness above for my questions. Please correct me during rebuttal if I misunderstood anything."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699039759697,
            "cdate": 1699039759697,
            "tmdate": 1699636155572,
            "mdate": 1699636155572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jWuEoFJ0TR",
                "forum": "pUGjLB0N4l",
                "replyto": "oCupo3wtcA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "- **Q1:***  what exactly is the conditional distribution $q\\left(x_t, Z \\mid x_s\\right)$, and how to sample from it? why samples should be dependent on other samples such as rather than independent with each other.\n\n  **A1:** I assume you are referring to the concept of $p( x_{\\mathbb{T}} | z,x_{\\mathbb{S}} )$, and please correct me if my understanding is incorrect. We begin by sampling the S-ratio and T-ratio from a beta distribution, followed by a random selection of image patches to act as $x_{\\mathbb{S}}$ and $x_{\\mathbb{T}}$. In the context of Conditional Matching  $p_{\\theta}(x_{T} \\mid x_{S})$\u2014> $q(x_{T} \\mid x_{S})$, the encoder's posterior distribution $p_{\\theta}(z \\mid x_{S \\cup T})$ takes the selected $x_{\\mathbb{S}}$ and $x_{\\mathbb{T}}$ as inputs to derive the latent variable z. It's important to note that $\\mathbb{S} \\cup \\mathbb{T}$ can differ from $\\mathbb{L}$. During the decoding process, the input is conditioned on z and $x_{\\mathbb{S}}$, leading to the generation of the predicted $x_{\\mathbb{T}}$. Since $x_{\\mathbb{S}}$ and $x_{\\mathbb{T}}$ originate from the same sample data, they exhibit inherent correlation. For instance, in the CelebA dataset, $x_{\\mathbb{S}}$ might represent the left side of a person's face while $x_{\\mathbb{T}}$ represents the right side.\n\n\n- **Q2:** Difference between Biglearn and the work in CVAE.\n\n  **A2:** We have thoroughly examined the workings of the CVAE. During its training process, CVAE not only trains $p(y|x)$ but also $p(y|x_{s})$, where x represents the original image, and y denotes the corresponding image segmentation, thereby aligning with the inference task. On one hand, Big Learning VAE targets single-modal tasks and has not yet extended to multi-modal scenarios. On the other hand, the multi-modal setting in big learning is more general, with the training approach of CVAE being a specific case within this broader context. \n\n  For example, with the multi-modal setup, where a data sample $X=(y,x)$ contains both feature $\\boldsymbol{x} \\in \\mathbb{R}^{L \\times D}$ and another data modality $\\boldsymbol{y} \\in \\mathbb{R}^{L^y \\times D^y}$ with the X-length index set $\\mathbb{L}^{\\prime}=\\left[\\mathbb{L}^y, \\mathbb{L}\\right]$, its any two nonoverlapping source/target index subsets $\\mathbb{S}^{\\prime}=\\left[\\mathbb{S}^{\\boldsymbol{y}}, \\mathbb{S}\\right]$ and $\\mathbb{T}^{\\prime}=\\left[\\mathbb{T}^y, \\mathbb{T}\\right] \\text { with } \\mathbb{S}^{\\prime} \\subset \\mathbb{L}^{\\prime}, \\mathbb{T}^{\\prime} \\subseteq \\mathbb{L}^{\\prime} \\text {, and } \\mathbb{T}^{\\prime} \\neq \\emptyset$. In $\\mathbb{T}^{\\prime}$, where $\\mathbb{T}^y$ is the universal set and $\\mathbb{T}$ is the empty set, BigLearning degenerates into the $p(y|x)$ of CVAE when in $\\mathbb{S}^{\\prime}$, $\\mathbb{S}^y$ is the empty set and $\\mathbb{S}$ is the universal set. When in $\\mathbb{S}^{\\prime}$, $\\mathbb{S}^y$ is the empty set and $\\mathbb{S}$ is a partial subset, BigLearning degenerates into the $p(y|x_{s})$ of CVAE.\n\n- **Q3:** The empirical evidence does not address the comparisons with many other SOTA version of VAE.\n\n  **A3:** The enhancement of VAE through Big Learning is orthogonal to other methods of improving VAE, which is why we have not made comparisons with other approaches. Our approach is an advancement in the conceptualization of training methodologies, characterized by its higher degree of flexibility. This adaptability allows it to be employed in creating 'big learning' variants of other Variational Autoencoder (VAE) models, exemplified by the development of BigLearn-InfoVAE.\n\n- **Q4:** An algorithmic flow will probabaly help in terms of clarity when presenting the sampling procedure.\n\n  **A4:** The algorithmic flow has been added to the revised paper. Please refer to Appendix D."
                    },
                    "title": {
                        "value": "Thanks for your comment !"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732049256,
                "cdate": 1700732049256,
                "tmdate": 1700732093638,
                "mdate": 1700732093638,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fMwe3hrVlV",
            "forum": "pUGjLB0N4l",
            "replyto": "pUGjLB0N4l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_XfDU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_XfDU"
            ],
            "content": {
                "summary": {
                    "value": "The authors present \"Big Learning\" variational autoencoders that attempt to train VAEs and their respective marginals and conditional distributions simultaneously.  The aim is to demonstrate that such an approach is better able to handle incomplete data than vanilla VAEs.  Experiments are designed to validate these claims."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The key idea is simple, and it extends the capabilities of VAEs -- effectively transferring computational time from inference to training time."
                },
                "weaknesses": {
                    "value": "The novelty of this work is a bit limited over top of Cong and Zhao, 2022.  \n\nThe main idea is relatively simple, but the explanation of that idea is a bit rambling.  In particular, I found Definition 1 initially confusing. The presentation would benefit from a clear problem statement.\n\nThe performance comparison in Figure 2 shows comparable performance to the vanilla VAE (which honestly isn't that surprising given the training setup). These are really the only quantitative results.  To me, it makes more sense to compare against the sampling methods that provide similar capabilities to the big learned model.  In a sense, this approach is pushing the computational load to the training phase instead of the inference phase, which makes sense, but you'd still want to verify that the resulting model has competitive performance on the proposed task against existing approaches.  \n\nOverall, I found the experiments a bit underwhelming and the details of the experimental setup/evaluation are scant."
                },
                "questions": {
                    "value": "See the above.  \n\nAdditional questions:\n-  What is the difference in training time between the vanilla VAE and this approach? \n\n\nMinor typos/suggestions:\n-  The citation style is not correct.  Please use parenthetical citations instead of in-text citations to make the paper more readable.\n-  \"-in-paining\" -> \"in-painting\"\n-  \"review the preliminary Variational\"\n-  \"be selected base on\"\n-  \"space is important and many works\" -> \"space is important, and many works\"\n- \"can not\" -> \"cannot\"\n- \"one need two foundation\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699372431249,
            "cdate": 1699372431249,
            "tmdate": 1699636155506,
            "mdate": 1699636155506,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9jk3CmWUTG",
                "forum": "pUGjLB0N4l",
                "replyto": "fMwe3hrVlV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment !"
                    },
                    "comment": {
                        "value": "- **Q1:** About the novelty.\n\n  **A1:** Our method is based on big learning, a concept initially proposed for foundation models. We have transferred its demonstrated flexibility and practicality to the traditional VAE framework, embodying a pattern of knowledge feedback.\n\n- **Q2:** What is the difference in training time between the vanilla VAE and this approach?\n\n  **A2:** The training time required for vanilla VAE and BigLearnVAE, as well as the curves showing the change in loss during the training process, have been added to the revised paper. Please refer to Appendix E. Under the same model size and experimental settings, the training loss and training time changes for vanilla VAE and BigLearnVAE are shown in Appendix E Figure 13. Theoretically, with the same network architecture, the training of one generation in a traditional VAE and our method are essentially the same, with comparable computational costs. Our method only incurs additional time for sampling. As can be seen from Figure 1, the convergence speed and total training time of BigLearnVAE are roughly the same as those of the vanilla VAE."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733051462,
                "cdate": 1700733051462,
                "tmdate": 1700733051462,
                "mdate": 1700733051462,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C3aep7jOP8",
            "forum": "pUGjLB0N4l",
            "replyto": "pUGjLB0N4l",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_b3xg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2221/Reviewer_b3xg"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a modification to the Variational Autoencoder (VAE) framework by incorporating conditional encoding and decoding processes capable of handling incomplete data inputs. Here's a summary of the modifications:\n\nConditional Encoder: Instead of requiring a complete data point 'x', the new encoder can work with partial data. It is designed as a transformer architecture, where missing values in 'x' are masked. A special 'CLS' token is used in the final position to generate a probability distribution over the latent variable 'z'.\nConditional Decoder: The decoder is adapted to accept both the latent variable 'z' and a partial 'x'. It then reconstructs the missing components of 'x' sequentially. This is achieved through a transformer setup with masking for the missing values, using an index-based approach to handle the decoding of missing elements of 'x'.\nAdditionally, the authors introduce a distribution over which positions in the input are missing. The decoder is trained across all possible combinations of missing and present data."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is an interesting and novel idea to train the VAE across all possible combinations of missing and present data, thereby learning a comprehensive generative process even with incomplete inputs. This approach enhances the model's capability to handle and predict missing data within a given dataset."
                },
                "weaknesses": {
                    "value": "The primary contribution of the paper is section 3.2.2 which is filled with inconsistencies\n 1) > $\\text{The marginal } p_\\theta(x_T, z) \\text{ is readily derived from the joint } p_\\theta(x, z) \\text{ via index selection with } T; \\text{ with specific } T = L, \\text{ the MarginELBO in (4) reduces to the JointELBO in (1);}$\n\nNote that none of these group of distributions are consistent wrt each other. Consider a 2 dimensional x = (x1, x2), then \n$$p(x1| mask, x2, z)p(x2|mask, mask, z) \\neq p(x2|x1, mask, z)p(x1|mask, mask, z)$$\n\nSo, unlike expected by the authors, this approach doesn't allow for modelling arbitrary conditional distributions $p(x_T|x_S, z)$\n\n 2) > $\\text{Both optima have already been modeled in the parameterized } p_\\Phi(z | x_{S'}).$\n\nThis is incorrect. The definition of optimal $q_M(z|x_T)$, that is, $p_\\theta(z|x_T)$, is a distribution that follows from Bayes' rule, that is\n$$p_\\theta(z|x_T) \\propto p_\\theta(x_T|z)p(z) $$\n$p_\\Phi(z | x_{T})$ doesn't model this distribution \n\n3) The notations used throughout the paper are very confusing. The same q had been used to represent the emprical data distribution, the posteriors and the distribution over indices q(S,T). \n\n4) The writing of the paper can be improved. Primarily, it is not a good idea to present the paper as a special case of big-learning [1] (an unknown unpublished/rejected work that claims to be all-encompassing). I tried reading the big-learning paper but there were too many errors in that paper to go-through.\n\n[1] Yulai Cong and Miaoyun Zhao. 2023. \"Big Learning: A Universal Machine Learning Paradigm?\" [Online]. Available at: https://openreview.net/forum?id=UfFXUfAsnPH."
                },
                "questions": {
                    "value": "1) How is q(S,T) chosen in equation (7)? \n2) What is index-based decoding mentioned at the beginning of page 6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2221/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699595703855,
            "cdate": 1699595703855,
            "tmdate": 1699636155418,
            "mdate": 1699636155418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HEDyzOcOp5",
                "forum": "pUGjLB0N4l",
                "replyto": "C3aep7jOP8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2221/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your comment !"
                    },
                    "comment": {
                        "value": "- **Q1:** Consider a 2 dimensional $x = (x1, x2)$, then \n  $$\n  p(x1|\\text{mask}, x2, z)p(x2|\\text{mask}, \\text{mask}, z) \\neq p(x2|x1, \\text{mask}, z)p(x1|\\text{mask}, \\text{mask}, z).\n  $$\n  **A1:** Joint learning cannot automatically provide marginal/conditional matching, for this reason we explicitly introduce big learning. By modeling itself, it won't lead to consistency, that's why we introduce big learning to bring about consistency through learning. For example, under the likelihood architecture, $log q(x1, x2) = log q(x1|x2)q(x2) = log q(x2|x1)q(x1)$, where $q$ represents the data distribution. Big learning explicitly and simultaneously matches the joint distribution under both $log q(x1|x2)q(x2)$ and $log q(x2|x1)q(x1)$ distributions. Ideally, when the above two distributions match well, we can obtain $p(x1|\\text{mask}, x2, z)p(x2|\\text{mask}, \\text{mask}, z)=p(x2|x1, \\text{mask}, z)p(x1|\\text{mask}, \\text{mask}, z)$. In practice, since the optimization goal of both of the above distributions is to match the joint distribution, it is expected that the results will not differ significantly.\n\n- **Q2:** ...$q_M\\left(z \\mid x_T\\right)$, that is $p_\\theta\\left(z \\mid x_T\\right)$, ... follows from Bayes' rule $p_\\theta\\left(z \\mid x_T\\right) \\propto p_\\theta\\left(x_T \\mid z\\right) p(z)$, $p_{\\Phi}\\left(z \\mid x_T\\right)$ doesn't model this distribution.\n\n  **A2:** In the $\\Phi$ space, $p_{\\Phi}\\left(z \\mid x_T\\right)$ is our parameterization method, and we use the $p_{\\Phi}\\left(z \\mid x_T\\right)$ to model the data distribution $q_M\\left(z \\mid x_T\\right)$.\n\n- **Q3:** The same q had been used to represent the emprical data distribution, the posteriors and the distribution over indices q(S,T).\n\n  **A3:** We follow the notation in the VAE paper to represent the empirical data distribution and the posteriors distribution, with ELBO being identical to $ KL[q(x)q(z|x) \\mid \\mid p(x, z)] $. Generally, in VAEs, the generative process is denoted by $p$,  while the data and posterior are represented by $q$. Therefore, here we choose $q$ to represent the $(S,T)$ distribution.\n\n- **Q4:** ...it is not a good idea to present the paper as a special case of big-learning.\n\n   **A4:** We will carefully revise the manuscript to enhance its clarity. Our method is essentially inspired by the big learning method. However, our focus is on the VAE domain. We verify that the big learning idea works effectively.\n\n- **Q5:** How is $q(S,T)$ chosen in equation (7).\n\n  **A5:** We sample the S-ratio and T-ratio from a beta distribution, specifically choosing parameters $(\\beta_1, \\beta_2)=(0.5,3)$ for the S-ratio and $(\\beta_1, \\beta_2)=(3,0.5)$ for the T-ratio. This parameterization for the S-ratio ensures that it is more likely to take smaller values, while the chosen parameters for the T-ratio make it more likely to assume larger values. Subsequently, we perform a random selection of image patches to serve as $x_{\\mathbb{S}}$ and $x_{\\mathbb{T}}$, where notably, $\\mathbb{S} \\subset \\mathbb{L}$, $\\mathbb{T} \\subseteq \\mathbb{L}$, $\\mathbb{T} \\neq \\emptyset$. Through this diverse combination of S-ratio and T-ratio, the model is trained to integrate both generative and inference capabilities.\n\n- **Q6:**  What is index-based decoding mentioned at the beginning of page 6 ?\n\n  **A6:** As illustrated in Fig. 1b of the manuscript, we input into the decoder both the encoded variable z and the selected  $x_{\\mathbb{S}}$, chosen using the method described in question 1. The decoder then outputs the corresponding selected image patches $x_{\\mathbb{T}}$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2221/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723072860,
                "cdate": 1700723072860,
                "tmdate": 1700723072860,
                "mdate": 1700723072860,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]