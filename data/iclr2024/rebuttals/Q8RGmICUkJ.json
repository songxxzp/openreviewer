[
    {
        "title": "SiBBlInGS: Similarity-driven Building Block Inference using Graphs across States"
    },
    {
        "review": {
            "id": "f0CzyYan06",
            "forum": "Q8RGmICUkJ",
            "replyto": "Q8RGmICUkJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4011/Reviewer_WRaP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4011/Reviewer_WRaP"
            ],
            "content": {
                "summary": {
                    "value": "The document introduces SiBBlInGS, a framework designed to identify Building Blocks (BBs) and their temporal profiles within high-dimensional, multi-state time-series data. SiBBlInGS utilizes channel-similarity and state-similarity graphs to uncover interpretable BBs, providing insights into the system's structure and variability across different states. The framework is demonstrated to be applicable across various data modalities, offering a deeper understanding of functional circuits, task encoding, and state modulations. It is validated using neural data from a monkey's somatosensory cortex during a reaching-out movement experiment, showcasing its potential in neuroscience for analyzing complex datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A well-written and structured paper.\n2. The proposed SiBBlInG has the ability to account for variations in temporal activities across trials and subtle differences in the composition of Building Blocks (BBs) across states.\n3. The SiBBlInGS framework offers valuable insights into functional circuits, task encoding, and state modulations across various data modalities. It is versatile and can be applied across diverse fields, including neuroscience, social science, and genetics.\n4. The experimental part is solid and abundant."
                },
                "weaknesses": {
                    "value": "The novelty of the paper mainly comes from borrowing the idea and success of functional Building Blocks (BB) into the neural data modeling. With a stacking of state-of-the-art techniques and domain knowledge, the proposed method achieves effectiveness through empirical evidence. However, these findings are a bit heuristic and empirical. There are few theoretical guarantees in this paper."
                },
                "questions": {
                    "value": "1. How can you prove that the Building Blocks (BB) are capable of modeling the spatio temporal structures within the states and dynamics of neural data well than traditional methods like State Space Models (SSM) and Variational Autoencoders (VAE)?\n2. How does the proposed method ensures to model and distinguish the within-state and between-state variabilities and relationships, which could be crucial for a comprehensive analysis of the data.\n3. What are the bio-plausible insights of this paper and in the model design?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812340887,
            "cdate": 1698812340887,
            "tmdate": 1699636363463,
            "mdate": 1699636363463,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RXauotkaQE",
                "forum": "Q8RGmICUkJ",
                "replyto": "f0CzyYan06",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1: Novelty, empirical results, and theoretical guarantees"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback on our paper and would like to address the concerns below:\n\n**Novelty:**\n\nThe novelty of our work goes far beyond the application of a concept to neuroscience. \nIt lies in the development of a versatile framework that identifies functional building blocks through re-weighted temporal graphs, fostering sparsity in multi-way data while considering within-state and between-state information.\n\nOur method tackles issues of real world data that other models typically overlook, such as varying trial durations, different trial counts per state, and handling of missing data or sampling rates, as well as enabling the discrimination between background and state-specific building blocks. \n\nThe potential applications of this method extend well beyond neuroscience, with potential applications in genomics, consumer behavior, financial markets, environmental sciences, urban planning, and the study of social dynamics \n(For additional details, please refer to the examples provided in our second response to reviewer T8G6). \nWe believe our approach is a standalone method with universal applicability, including contributions to neuroscience, as well as to other fields.\n\n**Empirical results and theoretical guarantees:**\n\nOur paper presents a comprehensive empirical exploration of SiBBlInGS, highlighting its adaptability and broad applicability through various synthetic and real-world examples. The approach is grounded in the theoretical principles of dictionary learning, drawing from seminal works in the field (e.g., [1], [2], [3], [4]). However, we acknowledge that a comprehensive theoretical discourse on dictionary learning is beyond the scope of our 9-page, application-focused paper.\n\nIn particular, in [1], the authors provide a geometric characterization of the optimization landscape in dictionary learning, eliminating concerns over spurious local minimizers. [2] shows theoretical guarantees that detail how searching within an over-realized model space can foster the recovery of dictionaries, conceptually linking this process with generalization bounds and introducing an efficient strategy for extracting accurate model components. [3] establishes the theoretical framework necessary for confidently recovering dictionaries under sparsity constraints, and [4] marks a deep dive into sample complexity, stating how much data is needed to reliably engage in dictionary learning and parallel matrix factorization efforts.\n\nWe will include these references with a brief explanation in the introduction of the updated paper to emphasize the above.\n\n\n[1] Sun, J., Qu, Q., & Wright, J. (2016). Complete dictionary recovery over the sphere I: Overview and the geometric picture. IEEE Transactions on Information Theory, 63(2), 853-884.\n\n[2] Sulam, J., You, C., & Zhu, Z. (2022). Recovery and generalization in over-realized dictionary learning. The Journal of Machine Learning Research, 23(1), 6060-6082.\n\n[3] Gribonval, R., & Schnass, K. (2010). Dictionary Identification\u2014Sparse Matrix-Factorization via $\\ell_1 $-Minimization. IEEE Transactions on Information Theory, 56(7), 3523-3539.\n\n[4] Gribonval, R., Jenatton, R., Bach, F., Kleinsteuber, M., & Seibert, M. (2015). Sample complexity of dictionary learning and other matrix factorizations. IEEE Transactions on Information Theory, 61(6), 3469-3486."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253105857,
                "cdate": 1700253105857,
                "tmdate": 1700253105857,
                "mdate": 1700253105857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vp8Nl2usDS",
                "forum": "Q8RGmICUkJ",
                "replyto": "f0CzyYan06",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2: Comparison to SSMs and VAEs and Distinguishing Within and Between State Variability"
                    },
                    "comment": {
                        "value": "We thank the reviewer for raising concerns about potential similarity to SSMs and VAEs. \n\n\n**Comparison to SSMs and VAEs:**\n\n*SSMs:*\n\nIn State-Space Models (SSMs), latent states evolve according to a transition matrix that dictates how the state evolves, accompanied by an emission matrix that determines how the latent state translates to the observations. The resemblance of SSMs to SiBBlInGS may not be immediately clear\u2014whether it pertains to the structure of the emission matrix or the potential dynamics operators in latent space.\n\nIf the concern was about potential similarities between SiBBlInGS and the emission matrix in SSMs, our approach incorporates re-weighted sparsity as a guiding principle, unlike SSMs' emission. Specifically, we utilize temporal-similarity-driven re-weighted $\\ell_1$ regularization to find cross-state BBs, a feature distinct to SiBBlInGS. Furthermore, SSMs do not focus on studying cross-trial variability or on pinpointing the formation of emergent BBs under specific conditions. \n\nIf the reference is to the potential analogy between the BBs and the transition operators of SSMs or other dynamical systems models (e.g., as seen in the identification of dynamical operators such as those in rSLDS [1] or dLDS [2]), SiBBlINGS find BBs that are not a function of time, and the temporal operators are not evolving by dynamical operators.  SiBBlInGS\u2019 BBs act as sparse interpretable functional clusters of channels found based on co-activity with cross-state constraints, while dynamical operators in [1] or [2] are neither sparse nor integrating temporal-similarity regularization.\n\n*VAEs:*\n\nIn the context of Variational Autoencoders, if the comparison is being drawn between SiBBlInGS' BBs and VAE's latent layer, it's essential to note that VAEs' latent layers are not inherently sparse and do not represent clusters from the original input (channels) space. They are found through sequential non-linear projections, resulting in a loss of interpretability and representing something different from SiBBlInGS' BBs. Even in the context of sparse VAEs (e.g., [3]), it's important to note that there is no co-learning of the components alongside their temporal traces. Additionally, with respect to dynamical VAEs (e.g., [4]),  these models not align with SiBBlInGS's aim of understanding multi-state variability.\n\n[1] Glaser, J., Whiteway, M., Cunningham, J. P., Paninski, L., & Linderman, S. (2020). Recurrent switching dynamical systems models for multiple interacting neural populations. Advances in neural information processing systems, 33, 14867-14878.\n\n[2] Mudrik, N., Chen, Y., Yezerets, E., Rozell, C. J., & Charles, A. S. (2022). Decomposed Linear Dynamical Systems (dLDS) for learning the latent components of neural dynamics. arXiv preprint arXiv:2206.02972\n\n[3] Ashman, M., So, J., Tebbutt, W., Fortuin, V., Pearce, M., & Turner, R. E. (2020). Sparse Gaussian process variational autoencoders. arXiv preprint arXiv:2010.10177.\n\n[4].Girin, L., Leglaive, S., Bie, X., Diard, J., Hueber, T., & Alameda-Pineda, X. (2020). Dynamical variational autoencoders: A comprehensive review. arXiv preprint arXiv:2008.12595.\n\n\n**Distinguishing Within and Between State Variability:**\n\nSiBBlInGS  adeptly integrates analysis both within and across states, maintaining consistency of the structure of within state BBs and accommodating nuanced controlled variations of between-state BBs. The degree of resemblance between corresponding cross-state BBs is methodically adjusted to reflect the corresponding levels of similarity between the states (as seen in the last term of Equation 1)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258660582,
                "cdate": 1700258660582,
                "tmdate": 1700258694779,
                "mdate": 1700258694779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1umhRCtmtn",
                "forum": "Q8RGmICUkJ",
                "replyto": "f0CzyYan06",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4011/Reviewer_WRaP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4011/Reviewer_WRaP"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the author for the detailed response with insights."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632887823,
                "cdate": 1700632887823,
                "tmdate": 1700632887823,
                "mdate": 1700632887823,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8txG4YNyBB",
            "forum": "Q8RGmICUkJ",
            "replyto": "Q8RGmICUkJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4011/Reviewer_xmKq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4011/Reviewer_xmKq"
            ],
            "content": {
                "summary": {
                    "value": "The authors address the task on analysing high-dimensional time-series data.  Explicitly accounting for states and trials, they perfrom a per-state-and-trial matrix factorization. As part of this, they infer factor matrices which they term building blocks (BBs); similarity of BBs is controlled via a state-similarity graph."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In the problem setup, the authors define a setting where allowing for observations stemming from different states, and sessions, as well as allowing for different durations between states/trials.  This setup reflects real-world applications well and as such has received little attention in the literature."
                },
                "weaknesses": {
                    "value": "- The authors compare their model only to very simple baselines, such as PCA and vanilla PARAFAC. In particular in the context of PARAFAC a lot of recent literature exists that generalized PARAFAC to explicitly account for temporal dependencies. Such approaches should be discussed explicitly and systematically benchmarked. In particular the authors should consider [1,2,3], where temporal information and different states over time are modelled via GP priors or parametric regularizers.\n- The experiments are very limited. While there are some analyses on synthetic data, they should be extended to include more baselines and also demonstrate how the proposed Method works in different settings for high-dimensional time series data e.g. such as anslysed in [1]\n- Results from baselines should also be discussed for the real-world applications\n- I find the presentation of the paper could be improved: it requires a lot of jumping back and forth between appendix and main text for important results and to gain a good understanding.  I also found the results of the real world applications hard to understand.\n\n\n\n[1] Wang, Z., & Zhe, S. (2022, June). Nonparametric Factor Trajectory Learning for Dynamic Tensor Decomposition. In International Conference on Machine Learning (pp. 23459-23469). PMLR.\n\n[2] Tillinghast, C., Fang, S., Zhang, K., & Zhe, S. (2020, November). Probabilistic neural-kernel tensor decomposition. In 2020 IEEE International Conference on Data Mining (ICDM) (pp. 531-540). IEEE.\n\n[3] Ahn, D., Jang, J. G., & Kang, U. (2021, October). Time-aware tensor decomposition for sparse tensors. In 2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA) (pp. 1-2). IEEE."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828100347,
            "cdate": 1698828100347,
            "tmdate": 1699636363368,
            "mdate": 1699636363368,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LJRfozMNPQ",
                "forum": "Q8RGmICUkJ",
                "replyto": "8txG4YNyBB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1: Additional Baselines"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's thoughtful feedback on our manuscript and the referral to additional sources and references.\n\n**Additional Baselines:**\n\nWe appreciate the feedback from the reviewer. In response, we have provided a brief description of additional models in the ''BACKGROUND AND RELATED WORK\" (see in blue font) to underscore the distinctions from SiBBlInGS.\n\nFurthermore, we added two more experiments, comparing our method also to: 1) NONFAT [1], and 2) NNDTN (discrete-time NN decomposition with nonlinear dynamics). The modifications include updates to Figure 2 in the paper to incorporate these comparisons, along with the addition of Figures 14 and 15, illustrating the identified components for all methods and the correlation with the ground truth per BB and per state. Please consult the updated paper for details on how we extracted these components, as described in the supplementary material. \n\nBroadly, SiBBlInGS serves a distinct purpose compared to methods mentioned in [1, 2, 3] or other Gaussian Process (GP)-based approaches. While these methods leverage temporal dependencies for robust decompositions with predictive power for future time points, SiBBlInGS aims to identify interpretable functional groups based on co-activity with subtle controlled variability along different axes\u2014the trial and state axes, which are orthogonal to the time axis. \nIn particular, SiBBlInGS focuses on discovering interpretable components with their traces rather than predicting future time points (as presented in [1] for instance) or solely reconstructing test data. \n\nFurthermore, the identified components in some of these other methods are not naturally sparse, and they do not aim to group sparse subsets of channels based on per-state temporal similarity. Hence, their interpretation does not align well with the discovery of functional groups that, in various fields, are assumed to represent co-active components, such as neural ensembles. Moreover, some of these methods cannot naturally work with data of varied duration or rates due to the inherent tensor structure. \n\nAlthough integrating GP ideas and dynamical processes into the analysis, as seen in [1, 2], is promising and could be very interesting for future work to better model within-trial non-stationary temporal conditions, it is not the primary goal of the current framework.\n\n[1] Wang, Z., & Zhe, S. (2022, June). Nonparametric Factor Trajectory Learning for Dynamic Tensor Decomposition. In International Conference on Machine Learning (pp. 23459-23469). PMLR.\n\n[2] Tillinghast, C., Fang, S., Zhang, K., & Zhe, S. (2020, November). Probabilistic neural-kernel tensor decomposition. In 2020 IEEE International Conference on Data Mining (ICDM) (pp. 531-540). IEEE.\n\n[3] Ahn, D., Jang, J. G., & Kang, U. (2021, October). Time-aware tensor decomposition for sparse tensors. In 2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA) (pp. 1-2). IEEE."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453939888,
                "cdate": 1700453939888,
                "tmdate": 1700453939888,
                "mdate": 1700453939888,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hzPpiCZis2",
                "forum": "Q8RGmICUkJ",
                "replyto": "8txG4YNyBB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4011/Reviewer_xmKq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4011/Reviewer_xmKq"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the responses to my questions."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731916381,
                "cdate": 1700731916381,
                "tmdate": 1700731916381,
                "mdate": 1700731916381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xyS0dLn6CY",
            "forum": "Q8RGmICUkJ",
            "replyto": "Q8RGmICUkJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4011/Reviewer_T8G6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4011/Reviewer_T8G6"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework called SiBBlInGS, which stands for Similarity-driven Building Block Inference using Graphs across States. The framework is designed to discover fundamental representational units within multi-dimensional data, which can adjust their temporal activity and component structure across trials to capture the diverse spectrum of cross-trial variability. The paper discusses the limitations of existing methods for understanding multi-dimensional data and how SiBBlInGS addresses these limitations. It also explains how SiBBlInGS employs a graph-based dictionary learning approach for building block discovery, and how it considers shared temporal activity, inter- and intra-state relationships, non-orthogonal components, and variations in session counts and duration across states. Finally, the paper compares SiBBlInGS to other approaches for discovering fundamental representational units within multi-dimensional data and discusses potential applications of this framework in scientific domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The SiBBlInGS framework is a novel approach for discovering fundamental representational units within multi-dimensional data. It addresses the limitations of existing methods and considers shared temporal activity, inter- and intra-state relationships, non-orthogonal components, and variations in session counts and duration across states.\n- SiBBlInGS is designed to be resilient to noise, random initializations, and missing samples. This makes it a robust framework for discovering building blocks in real-world data.\n- The paper includes a thorough evaluation of the SiBBlInGS framework on both synthetic and real-world data. The results demonstrate the effectiveness of the framework in discovering building blocks and its potential for applications in scientific domains."
                },
                "weaknesses": {
                    "value": "- The proposed framework is complicated. While the paper provides a high-level overview of the SiBBlInGS framework, it does not provide detailed implementation instructions or code. This may make it difficult for researchers to replicate the results or apply the framework to their own data.\n- While the paper discusses potential applications of the SiBBlInGS framework in scientific domains, it does not provide concrete examples of real-world applications. This may limit the impact of the framework and its adoption by researchers in different fields. \n- The authors did not provide more details on the limitations of the SiBBlInGS framework and potential areas for improvement? This would help readers understand the scope and applicability of the framework."
                },
                "questions": {
                    "value": "- Can the authors discuss the potential limitations of the SiBBlInGS framework in terms of scalability and computational efficiency? For example, how might the framework perform on larger datasets or in real-time applications, and what steps can be taken to address these limitations?\n- How does the proposed method compare to other approaches for discovering fundamental representational units within multi-dimensional data?\n- How does the shared temporal activity, inter- and intra-state relationships, and non-orthogonal components contribute to the discovery of building blocks and the effectiveness of the framework?\n- Can the authors explain more on how SiBBlInGS handles missing samples and varied sampling rates in multi-dimensional data? How does it ensure that the discovered building blocks are robust to these variations?\n- What advantages does graph-based dictionary learning approach offer different from other dictionary learning approaches?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4011/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699232756764,
            "cdate": 1699232756764,
            "tmdate": 1699636363286,
            "mdate": 1699636363286,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lK5Hkcmg2g",
                "forum": "Q8RGmICUkJ",
                "replyto": "xyS0dLn6CY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1: Model Complexity, Real-World Examples, Limitations, and Scalability"
                    },
                    "comment": {
                        "value": "We appreciate the strengths highlighted by the reviewer, as well as the questions and feedback provided. We will now address the concerns and questions raised below:\n\n**Implementation Instructions and Code:**\nAs presented in Algorithm 1 (page 14) as well as given the code that was attached in the supplementary materials, we believe the algorithm can be used by future researchers as well. Once accepted and dis-anonymized, a GitHub repository with the code will be shared and a Python PyPi package for simple pip installation will be created for simple usage of the algorithm and to enable users to develop custom implementations and modifications.\n\n\n**Examples for Real World Applications:**\nIn addition to the real-world examples mentioned in the ''CONCLUSIONS AND DISCUSSION\" section (section 6), please find below some more details about potential applications of SiBBlInGS:\n- *Consumer Products:* SiBBlInGS could identify groups of products (channels) with similar purchasing patterns over time (time) across \ncountries (states), and explore how these patterns vary by location to inform personalized customer recommendations.\n- *Economic Analysis:* SiBBlInGS could be applied to cluster stock volume data (channels) over time (time) and across countries (states), to uncover patterns reflecting cross-country market trends and differences in investor activity.\n- *Climate Science:* SiBBlInGS could be used to cluster carbon dioxide emission patterns of various countries (channels) across daily intervals (time) and yearly changes (states). This analysis aids in discerning global emission trends and could inform more targeted climate policy initiatives.\n- *Social Dynamics Analysis:* SiBBlInGS could be utilized to analyze and understand the formation and evolution of online user groups (channels\u2019 BBs) based on shared temporal patterns of user activity (time) across different states (e.g. before/ during / after the COVID-19 pandemic). This approach could enable the observation of shifts in social interactions and group dynamics in response to global events.\n- *Genetics Research:* SiBBlInGS could analyze multi-tissue gene expression data (e.g., as the data described in [1]) to identify functional gene groups (channels) and assess how their expression across tissues (~time) differs between individuals (states).\n\n[1] Hore, V., Vinuela, et al. (2016). Tensor decomposition for multiple-tissue gene expression experiments. Nature genetics, 48(9), 1094-1100.\n\n**Potential areas for improvement:**\nAs outlined in the final paragraph of the paper, certain limitations, beyond the current scope of the paper, will be addressed in future work. In particular, currently SiBBlInGS presumes normal noise distribution (e.g., as seen in the $\\ell_2$ norms of the data fidelity terms in equations 1,2) and Euclidean distance between state labels in the supervised approach (definition of $P$), which may not align well with some data (e.g., neural data may be modeled with a Poisson distribution in low spike count rates). Additionally, while the identified BBs capture channel membership and magnitude, they currently do not account for potential directed connectivity within the BBs, presenting an opportunity for future extensions.\nFor future work, we also plan to showcase SiBBlInGS's versatility by integrating diverse data modalities with the same structure (e.g., fMRI vs ultrasound of the same brain area). Additionally, we plan to extend its usability to handle a continuous, infinite number of non-discrete labels, as well as incorporating extra axes for state changes, allowing a trial to belong to several states within a pool of states.\n\n\n\n**Potential limitations and improvements of SiBBlInGS in terms of scalability and computational efficiency:**\nAs shown in Section E on model complexity, the current implementation's speed scales with the dataset's dimensionality, encompassing the number of channels, states, time points, and the rank of each trial. To enhance computational speed, the implementation already incorporates random subsampling of time periods and trials within each state during iterations (as mentioned in the paper after equation 1). This allows the model to process only a portion of the data at each update step, eventually exposing the model to all trials and time points. Additionally, the current implementation offers users to choose between different lasso solvers that vary in speed. Future improvements for efficiency may involve implementing the model in languages other than Python, such as C++ or Julia, and applying parallel computing for faster and more efficient performance.\nWith respect to real-time applications, SiBBlInGS is not intended for real-time inference. However, if BBs are found previously based on some training data for a state, they can be easily used in real-time applications to find their respective traces when new observations arrive (i.e., by solving equation 9)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241876853,
                "cdate": 1700241876853,
                "tmdate": 1700241876853,
                "mdate": 1700241876853,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "knuCglpY7U",
                "forum": "Q8RGmICUkJ",
                "replyto": "xyS0dLn6CY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4011/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2: Comparing SiBBlInGS to Other Approaches, Significance of Temporal Activity, Inter/Intra-State Relationships, and Non-Orthogonality."
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's comments on SiBBlInGS compared to other approaches and its effectiveness.\n\n**Comparing SiBBlInGS with Other Approaches:**\n\nAs demonstrated in the synthetic data results (Figure 2), EEG experiments (Figure 13), and trends experiments (Figure 8), along with additional comparisons detailed in Table 2, we assessed our method against alternative tensor and matrix factorization techniques, through both theoretical analysis and practical experiments.\n\nSiBBlInGS stands apart from classic tensor factorization methods, as it finds similarity-driven sparse BBs with subtle cross-state variations, accommodates variable trial durations and inherently incorporates state information and similarities into the analysis, effectively differentiating intra- and inter-state variability\u2014 aspects not addressed by other approaches. Additionally, certain methods presume orthogonality among components, a condition that may not hold true for some real-world cases (e.g.,  neural ensembles). These alternative approaches often tend to find components based on maximum variance or energy rather than co-activity, and they are not inherently tailored to differentiate state-specific from state-invariant building blocks (as $\\nu$ does in SiBBlInGS).\n\n**Significance of Temporal Activity, Inter/Intra-State Relationships, and Non-Orthogonality:**\n\n*Shared Temporal Activity:*\n\nWe appreciate the reviewer\u2019s question about the contributions of different features of SiBBlInGS to the framework.\nAs detailed following Equation 1 in the paper (starting with 'The weighted evolving regularization terms for each state...'), the channel similarity graph (${H}$) is built based on temporal activity similarities between channels per trial. This graph is integrated into the denominator of the regularization weight ($\\lambda$) in the re-weighted lasso equation (2nd part of Equation 1) and thus fosters the aggregation of channels with co-active patterns into the same BBs. \n\nIn particular, the term \n$H_{n:}A_{:j}$ \nthat appears in the denominator for updating $\\lambda_{n,j}^d$ (2nd formula in Equation 1) reflects the correlation between the activity of the $j$-th BB (as defined by $A_{:j}$) with the similarity levels of all channels to the $n$-th channel (as defined by $H_{n:}$). \nSince that term appears in the denominator of $\\lambda_{n,j}^d$\u2014a higher correlation between $A_{:j}$ and $H_{n:}$ (which implies that the temporal neighbors of the $n$-th channel might be in the $j^{th}$ BB), results in a smaller sparsity weight ($\\lambda_{n,j}^d$ decreases)\u2014thus promoting that $n^{th}$  channel's inclusion in the $j^{th}$ BB and effectively \u2018\u2019pushing\u201d the  channel to the BB that includes its temporal neighbors.\n\nConversely, if the temporal neighbors of channel $n$ mainly do not belong to that $j$-th BB, the term  $H_{n:}A_{:j}$ will be smaller, hence $\\lambda_{n,j}^d$ is larger, resulting in an increased sparsity regularization that effectively pushing that $n$-th channel out of the $j^{th}$ BB. \n\nAnother effect of the shared temporal activity is expressed in the data-driven approach, as the state graph is a function of the temporal similarities between states, and the regularization on the distance between cross-state BBs is proportional to the temporal similarities between these states (stored by $P$, and as seen in Equation 1), thereby encapsulating state-temporal similarities within the analysis. \n\nSpecifically, last term in Equation 1 ($P_{dd'} \\|\\|a_{i:} - a_{j:}\\|\\|_2^2$) modulates the $\\ell_2$ norm on the distance between corresponding BBs across different states in accordance with the states' similarities, ensuring that more similar states are associated with more similar BB patterns.\n\n\n\n*Non-orthogonal components:*\n\nOur method's ability to identify components without enforcing orthogonality constraints, as opposed to techniques like HOSVD or PCA, aligns better with real-world data where such constraints may not naturally exist. For example, in the context of neural ensembles, it is plausible to encounter functional groups that are non-orthogonal, both structurally and temporally.\n\n\n\n*Intra vs inter state relationships:*\n\nAs written just before Equation 1 (the paragraph starting with \u201cAssuming consistent underlying groups between same-state trials,....\u201d), the BBs are kept constant between trials of the same state while supporting subtle controlled structural variations between states, thus addressing both resolutions. Unlike the regularized BBs, SiBBlInGS allows for flexible variations in temporal traces within and between states, capturing the complete spectrum of activity variability while maintaining a shared basis of BBs for interpretability. Additionally, as shown in Figure 4E, the within-state temporal trace correlations are larger than between-states temporal correlations of corresponding ensembles, indicating reduced within state compared to between states variability."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245083613,
                "cdate": 1700245083613,
                "tmdate": 1700245406998,
                "mdate": 1700245406998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QmgYcxlKzv",
                "forum": "Q8RGmICUkJ",
                "replyto": "wzpXKJGK63",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4011/Reviewer_T8G6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4011/Reviewer_T8G6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the responses to my questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4011/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656148494,
                "cdate": 1700656148494,
                "tmdate": 1700656148494,
                "mdate": 1700656148494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]