[
    {
        "title": "CLIP Exhibits Improved Compositional Generalization Through Representation Disentanglement"
    },
    {
        "review": {
            "id": "lo6OmS3vso",
            "forum": "UVSKuh9eK5",
            "replyto": "UVSKuh9eK5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9428/Reviewer_PAYB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9428/Reviewer_PAYB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new dataset to benchmark the compositional capabilities of several CLIP models (OpenAI and OpenCLIP). This dataset is generated using DALLE, and covers the 1000 class names from the ImageNet dataset combined with 30 adjectives. Manual annotators validated the combinations, resulting in ~12k plausible compositions, from which they generated 50k images. The authors also propose to measure the compositional generalization via the normalized mutual information between objects and attributes, and use Z-Diff Score, DCI-Informativeness, Explicitness score, and DCIMIG metrics to evaluate the disentanglement in the embeddings from the models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ This paper proposes an interesting approach to measure the compositional capabilities of large-scale VL models, by leveraging a text-to-\nimage model to generate new images with specific attributes.\n\n+ The authors provide a large set of experimental results in the supplementary materials, showing that CLIP models struggle with their proposed dataset\n\n+ This paper is well-structured, easy to read and follow."
                },
                "weaknesses": {
                    "value": "+ There is no description or motivation for the attribute selection, are those attributes randomly selected or generated? How do the authors guarantee that those attributes are not present or co-occur less in the training data?\n\n+ The human validation seems crucial in generating the proposed benchmark; however, there is no detailed description of how this was performed. \n\n+ In section 1, the authors claim: \"By assessing the captions in the training sets, we guarantee that none of the captions in our test dataset or similar captions are included in the CLIP training data.\" -- however, I couldn't find any empirical or theoretical evidence, nor existing reference for this claim.\n\n+ The human validation only asses for the plausibility of the noun-adjective composition, but are the images generated by DALLE following those compositions? Prior work has shown that Diffusion models \"struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects\"[1]. It is unclear if the generated dataset follows the attribute-noun composition, or falls into this category. See also [2].\n\n+ Most of the conclusions are prevalent in the literature (e.g., the diversity of training captions promotes compositionality [3]), and the mutual information analysis does not seem to provide additional insights [4, 5].\n\n[1] Liu, Nan, et al. \"Compositional visual generation with composable diffusion models.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[2] Park, Dong Huk et al. \u201cBenchmark for Compositional Text-to-Image Synthesis.\u201d NeurIPS Datasets and Benchmarks (2021).\n\n[3] Doveh, Sivan, et al. \"Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models.\" arXiv preprint arXiv:2305.19595 (2023).\n\n[4] Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" International conference on machine learning. PMLR, 2021. \n\n[5] Oquab, Maxime, et al. \"Dinov2: Learning robust visual features without supervision.\" arXiv preprint arXiv:2304.07193 (2023)."
                },
                "questions": {
                    "value": "Is there any particular reason why DINO-v2 and BEiT-v2 are mentioned briefly in the introduction, but no further analysis is done in the following sections?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9428/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9428/Reviewer_PAYB",
                        "ICLR.cc/2024/Conference/Submission9428/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740584377,
            "cdate": 1698740584377,
            "tmdate": 1700680142016,
            "mdate": 1700680142016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0fBKklJ1LX",
                "forum": "UVSKuh9eK5",
                "replyto": "lo6OmS3vso",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Points 1-4"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. I appreciate the time and effort you have put into reviewing my work. Below, I have addressed each of your points in detail.\n\n**Point 1**: For the selection of attributes, we began with a comprehensive list of adjectives derived from various linguistic sources and large language models such as ChatGPT. This initial list was subjected to a practical evaluation phase where preliminary images were generated and assessed for the visual translation of these adjectives. Attributes that were not clearly represented, such as \u2019fast\u2019, were discarded. This process resulted in a refined set of 30 adjectives that were visually discernible.\nAs mentioned in Section 6.2, we combined these attributes with a list of objects. We then searched for these combinations in the captions of the CLIP training datasets (CC,YFCC,LAION,Datacomp). Combinations found in the training datasets, like \"dotty bag\", were removed. The remaining combinations, such as \"hairy jeep\", do not appear in the CLIP training dataset captions. Therefore, these combinations can be considered out-of-distribution data for CLIP models trained on those datasets. It is important to clarify that we do not consider the attributes alone to be out-of-distribution, but rather the full combinations of attributes and objects. For example, \"hairy\" on its own would not necessarily be out-of-distribution, but the novel combination \"hairy jeep\" would be.\n\n**Point 2**: As outlined in Section 6.2 and visually depicted in Figure 3, our human evaluation protocol involved a straightforward yet effective method for ensuring the quality and relevance of the generated images. Two evaluators were assigned the task of assessing each image against two critical criteria to determine its suitability for inclusion in the dataset:\n\n-**Object Recognition**: Any image where the central object was not clearly recognizable or was ambiguous was removed.\n\n-**Attribute Visibility**: Images were excluded if the specified attributes were not evidently depicted.\n\n**Point 3**: Our methodology involved an initial generation of 30,000 attribute-object combinations. We then conducted a comprehensive search for these combinations within the captions of various datasets on which CLIP has been trained, such as LAION. This search was instrumental in identifying and eliminating any combinations that already existed within the training data. Even if the individual attribute and object words appeared at a distance in a caption, we eliminated the combination of them from our combinations.\n\nConsequently, the remaining 12,000 combinations, which did not appear in the training data, were used to generate our test dataset. This rigorous process underpins our confidence in asserting that our test dataset comprises truly OOD examples relative to the CLIP training datasets.\n\n**Point 4**: We acknowledge the valid concerns raised about ensuring the integrity of the attribute-noun compositions in our generated images, and the potential issues with diffusion models identified in prior work. As detailed in Section 6.2 and shown in Figure 3, our human evaluation process was designed to directly address these concerns. The main motivation for human evaluation is that text-to-image models currently have limitations in generating coherent compositions. We first evaluated different models like Stable Diffusion variants, Deep Floyd, and DALL-E, and found DALL-E to be the most capable at generating compositions. However, since its accuracy is imperfect for every composition, we added human evaluation to our pipeline to remove low quality \nand ambiguous images, and also the images whose contents are not consistent with the specified attribute-object."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076902294,
                "cdate": 1700076902294,
                "tmdate": 1700077144530,
                "mdate": 1700077144530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O89vdGtJ9c",
                "forum": "UVSKuh9eK5",
                "replyto": "lo6OmS3vso",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9428/Reviewer_PAYB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9428/Reviewer_PAYB"
                ],
                "content": {
                    "comment": {
                        "value": "After carefully reading all reviews and responses, I'm keeping my original rating (5) and upgrading my confidence to 5. All reviewers have a consensus regarding some claims in the paper, especially regarding attribute selection and compositionality claims. Moreover, there are several aspects that remain unclear: the lack of information regarding human evaluations along with the confusing narrative (as pointed out by Reviewers unRs and qiWV). I would suggest to the authors to revise and resubmit to another venue."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680122193,
                "cdate": 1700680122193,
                "tmdate": 1700680122193,
                "mdate": 1700680122193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oRBTd3XYGq",
            "forum": "UVSKuh9eK5",
            "replyto": "UVSKuh9eK5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9428/Reviewer_qiWV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9428/Reviewer_qiWV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies CLIP models under a different type of distribution shift namely compositional OOD generalization, where the objects and attributes may be individually seen during training, but their composition is unseen. A new dataset, ImageNet-AO is generated using DALL-E, containing such novel compositions for ImageNet classes. It is ensured that the generated compositions are not present in the CLIP training datasets. Key observations are - i) compositional diversity of the training dataset improves the compositional generalization of the CLIP model, ii) image/text representation disentanglement of objects and attributes improves generalization, iii) larger, more diverse datasets leads to better compositional generalization, iv) better disentanglement in representations leads to better compositional generalization."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The experiments are well-designed \n- Conclusions drawn are very interesting and insightful \n- The dataset ImageNet-AO can be helpful for future study as well"
                },
                "weaknesses": {
                    "value": "- \"*the training dataset of the OpenAI CLIP has not been released, which makes designing a test set that has a truly different distribution from the training one challenging. We aim to address these issues in this paper, by focusing our attention on the compositional generalization\nin the single object setting, and designing an authentic test dataset to assess the training data characteristics and mechanisms in the models that lead to the OoD generalization.*\" -- This contradicts the following statement where the authors claim that they verify that the ImageNet-AO images are not a part of several CLIP training dataset -- \"*To ensure these combinations were not present in the CLIP training set, we conducted a thorough search and removed any combinations that were found.*\"\n- \"*By assessing the captions in the training sets, we guarantee that none of the captions in our test dataset or similar captions are included in the CLIP training data.*\" \n    - Is this check done for all the other datasets considered in the paper as well (LAION, YFCC15m, CC12m, and DataComp)?\n    -  A similar check should be done on images as well, it is possible that such images with different captions are present in the training set. Usually, captions from web sources are not exactly descriptive of the image.\n- \"*We also found that the CLIPs that show higher OoD generalization typically exhibit strong disentangled text representations. Furthermore, such CLIPs also enjoy a more disentangled image representation with respect to the attributes and objects as well.*\" -- the experiments in the paper do hint at the above statement. But this does not necessarily imply the following: \"*Specifically, a dataset with diverse compositions of attribute-objects facilitates a more disentangled text representation, which in turn induces a disentangled image representation through contrastive learning.*\" It could be possible that diverse images lead to disentangled image representations as well. \n- \"*To evaluate the degree of disentanglement in the training captions utilized by the CLIP, we conducted an analysis by measuring the normalized mutual information (NMI) between the object class and attribute tokens, whose domains are defined based on the captions in our generated dataset.*\" -- Could the authors explain how the domains are defined based on the captions in the generated dataset? More details on how the NMI is measured would be helpful.\n- Fig.4 - It is not clear how the disentanglement metrics are computed for the image encoder. \n- \"*We aimed for a diverse set of class names to enhance the complexity of the generated images.*\" -- It is not clear if all 1000 classes were used or only a subset. If a subset was used, how was this chosen?\n- \"*This dataset was produced by creating compositional images via a text-to-image model, using an Attribute+Object template.*\" -- could the authors give more details/ a citation for the Attribute+Object template?\n- Could the authors provide details on where the 30 adjectives were chosen from?\n- \"*Lastly, human evaluation was used to validate the generated images, with images not closely aligning with their prompts removed. After this process, around 12000 combinations remained, for which we successfully generated near 50000 accurate, high-quality images.*\" - The order of the two statements may have to be swapped? Could the authors provide details on how this human evaluation was done?\n- \"*For the test sets, all 1000 classes of ImageNet were used as the in-distribution set and expanded the number of classes to approximately 12000 for the OoD set.*\" -- could the authors share how the captions were created for the OOD set? Sharing some examples would be helpful. I believe the 80 captions are used only for the ID set, and single relevant captions are used for the OOD set?\n- In Fig.1, for a more fair comparison, the image-only models such as DINO-v2 and BEiT-v2 should also be trained on the datasets that were used for training CLIP (by using only the images, and ignoring the captions). Without matching at least the image datasets, there is not enough evidence to support the following statement - \"*We interpret these findings as strong evidence that the inclusion of language supervision, particularly during CLIP training, positively impacts the model representation quality, hence making it possible to generalize to unseen compositions, despite the absence of such compositions in their training data.*\"\n\nNitpicks -\n\n- citation format seems non-standard - (x) vs. [x]\n- inline citations should use the format xyz et al., rather than [x] \n- A citation for the work that defines \"compositional OOD generalization\" would be helpful"
                },
                "questions": {
                    "value": "- Although the experiments and conclusions in the paper are interesting and useful, several aspects of the paper need more clarity. These are mentioned in the weaknesses section. I will be happy to update my score based on clarifications provided by the authors. \n- Codes, models, and datasets must be open-sourced for the benefit of future research. Could the authors comment on this? Would these be released upon acceptance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9428/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9428/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9428/Reviewer_qiWV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831177076,
            "cdate": 1698831177076,
            "tmdate": 1700671098788,
            "mdate": 1700671098788,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aYyFBBvCIK",
                "forum": "UVSKuh9eK5",
                "replyto": "oRBTd3XYGq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer Point 1, 2 and 3"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. I appreciate the time and effort you have put into reviewing my work. Below, I have addressed each of your points in detail.\n\n**Point 1**: It is true that the specific training dataset used by OpenAI for CLIP has not been publicly re-\nleased. However, there are several multimodal datasets that have been made available and are commonly\nused for training vision-language models, such as LAION, YFCC, CC, and DataComp. Our work utilizes\nthese publicly available datasets to construct a test as mentioned in Section 6.2. We also evaluate CLIP\nmodels trained on these public training datasets.\n\n**Point 2**: We have thoroughly checked all datasets referenced in our paper, including LAION, YFCC15m,\nCC12m, and DataComp, to ensure that (attribute,object) pairs in our dataset are not present in the\ncaptions of these CLIPs\u2019 training data.\nSince we want to evaluate models on novel compositions, we first found (attribute, object) pairs that\nare not appear in any captions of the available training sets of CLIPs. For these novel (or extremely rare)\ncombinations, the images were also generated which makes the possibility of image overlap extremely\nunlikely. For instance, we have combinations such as \u2018hairy jeep,\u2019 or \u2018luminous golf cart,\u2019 in our dataset,\nas illustrated in Fig. 2, which are unlikely to be encountered on the web.\nWhile we acknowledge the possibility that the model may have been exposed to similar images during\ntraining, we can assert with confidence that the exact image and complete caption pairs used in our\ntest dataset are unique and represent new combinations that the model has not previously encountered.\nThis pairing approach ensures that, despite any potential familiarity with individual images, the model\nis still being evaluated on its ability to generalize to new image-caption combinations, which we have\nengineered to be out-of-distribution (OoD). Additionally, the principal experiments presented in our\npaper, particularly in Section 3.1, as well as the supplementary experiments detailed in Sections 4.2, 6.3,\nand 6.4, incorporate both image and text components within their methodologies. These experiments\ndo not solely concentrate on the image aspect.\n\n**Point 3**: The textual input is inherently disentangled, with adjective and object tokens being *distinctly separate*. In contrast, the visual inputs inherently exhibit entanglement; attributes and objects within images are naturally intertwined. By augmenting the diversity of textual data, we promote disentanglement in text representation, as the model learns to distinguish between the separate tokens for adjectives and objects due to their consistent and isolated presentation. However the entanglement in image representations persists because the attributes and objects are not presented as isolated features but as parts of an integrated whole.  Therefore, while increased diversity can enhance the model's exposure to varied visual concepts, it does not automatically lead to disentangled representations. These claims are also backed by Fig. 4, where we see that the text encoder usually exhibits a more pronounced disentanglement score compared with the image encoder. \n\nIn the table below, we also compare a CLIP model with some image-only models that were trained on large-scale datasets or are among the top-performing models on ImageNet benchmarks. We evaluate these models on different disentanglement metrics  on the ImageNet-AO.\n\n**Table: Disentanglement metrics on different Models**\n\n| Model                     | z_diff \u2191 | DCI-I \u2191 | explicitness \u2191 | DCIMIG \u2191 |\n|---------------------------|----------|---------|----------------|----------|\n| CLIP-ViT-14-datacomp-xl   | 0.955    | 0.1834  | 0.9103         | 0.025    |\n| DINOv2                    | 0.95     | 0.1787  | 0.9022         | 0.021    |\n| Beit                      | 0.901    | 0.1463  | 0.884          | 0.016    |\n| resnext101      | 0.948    | 0.1745  | 0.901          | 0.0209   |\n| Effeicent-b7              | 0.899    | 0.1274  | 0.8531         | 0.0134   |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075249497,
                "cdate": 1700075249497,
                "tmdate": 1700077169507,
                "mdate": 1700077169507,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ARnjl9Bmys",
                "forum": "UVSKuh9eK5",
                "replyto": "oRBTd3XYGq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Points 10,11"
                    },
                    "comment": {
                        "value": "**Point 10**: Our dataset contains images generated from textual prompts, which are then used as captions for evaluation. For example, the \"hairy jeep\" image was generated from the prompt \"hairy jeep\".\n\nIn zero-shot evaluation, ID image embeddings are matched against 1,000 captions for the 1,000 classes. OoD image embeddings are matched against 12,000 captions since each combination is a unique class.\n\nFor both sets, class name captions are put into 80 templates like \"this is a photo of...\" to create multiple embeddings. The final class embedding is the average of these 80 variations.\n\n**Point 11**: As delineated in Section 3.2, the aim of our study was not to forge a direct and fair comparison between CLIP and image-only models like DINO-v2 and BEiT-v2. Instead, our focus was investigating the mechanism that causes the language supervision to improve  CLIP's out-of-distribution (OoD) generalization capabilities as mentioned in the title and abstract of our paper. We selected the supervised and self-supervised models for comparison based on their state-of-the-art ImageNet performance and their training on large-scale datasets, which allowed us to make a closer, albeit approximate, comparison given our practical constraints.\nOnly to see performance of other powerful foundation models (that are not based on language supervision), we show the results of these models on our proposed dataset too. \n\n\nThe evidence supporting the role of language supervision in enhancing CLIP's performance is multifaceted, including:\n- Variations in performance among models trained on Conceptual Captions (CC) and YFCC datasets, particularly with lower Normalized Mutual Information (NMI).\n- A clear disentanglement at the representation level.\n- A well-documented correlation between representation disentanglement and OoD generalization.\n\nOur narrative underscores the distinctive benefits of language supervision and its impact on OoD accuracy, rather than on a direct performance comparison. Given the logistical challenges of compiling a dataset comparable to the one used to train CLIP, we opted to showcase the most relevant models to support our hypothesis.\n\nAt the end, about your question, we plan to open-source all codes and datasets upon the paper\u2019s acceptance to\nsupport future research.\n\n[3] Lewis, Martha, et al. \"Does clip bind concepts? probing compositionality in large image models.\" arXiv preprint arXiv:2212.10537 (2022).\n[4] Nayak, Nihal V., Peilin Yu, and Stephen H. Bach. \"Learning to compose soft prompts for compositional zero-shot learning.\" arXiv preprint arXiv:2204.03574 (2022)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076191249,
                "cdate": 1700076191249,
                "tmdate": 1700076333443,
                "mdate": 1700076333443,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W8cNzUxOWT",
                "forum": "UVSKuh9eK5",
                "replyto": "ARnjl9Bmys",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9428/Reviewer_qiWV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9428/Reviewer_qiWV"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed and thorough response, which addresses several of my concerns. Please find my comments below-\n- **Point 1**: In this case, the following statement needs to be removed from the Introduction - \"For instance, the training dataset of the OpenAI CLIP has not been released, which makes designing a test set that has a truly different distribution from the training one challenging. We aim to address these issues in this paper, by ... \" \nThis is because, the construction of ImageNet-AO assumes access to the training dataset of  CLIP, while this statement makes it seem like access is not needed. \n- **Point 2**: It is possible that synonyms of object-attribute pairs exist as captions to similar images. Although the exact pair may not exist in the training dataset, a more robust means of ensuring that a similar image-caption combination does not exist in the training dataset is required. For example, finding the k nearest neighbors of the caption and image individually, and using human inspection to ensure the k nearest captions and k nearest images are not similar to them. Since the training dataset is very large scale, we cannot rely on the combination being unlikely to exist in the dataset, although it seems to be the case.\n- **Point 3**: Could the authors clarify whether the first row in the table presented in the rebuttal uses text encoder or image encoder? Also, could the authors share the conclusions from the table?\n- **Point 9**: For human evaluation, it is important to also include details on how this was conducted, and how many people participated in the study.\n\nI update my score to 5 based on the rebuttal."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671043145,
                "cdate": 1700671043145,
                "tmdate": 1700671043145,
                "mdate": 1700671043145,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3p9Aw9Vj7G",
                "forum": "UVSKuh9eK5",
                "replyto": "7mhTalO8Xd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9428/Reviewer_qiWV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9428/Reviewer_qiWV"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their prompt response. I have no further questions for the authors.\n\nAlthough the experiments and insights in the paper are useful, I maintain my rating of 5 since the dataset presented requires a more robust human evaluation to ensure that it meets the criteria claimed in the paper. Further, I believe that a more formal human evaluation is important for releasing a dataset. I am open to revising my rating based on discussions with other reviewers/ AC."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720664178,
                "cdate": 1700720664178,
                "tmdate": 1700720664178,
                "mdate": 1700720664178,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AUeGBXg0Pe",
            "forum": "UVSKuh9eK5",
            "replyto": "UVSKuh9eK5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9428/Reviewer_unRs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9428/Reviewer_unRs"
            ],
            "content": {
                "summary": {
                    "value": "In this work, author examine the compositionally generalization in vision language model. By adopting different combination of disentangled attribute in training dataset of CLIP, author generate a authentic test set that is unseen by model but share the same disentangled attribute. Author also argue that the level of feature disentanglement is high correlate to model generalization by presenting various analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper propose a high quality test set measuring compositional generalization with generative model. This benchmark provide a simply and more straightforward measurement for compositional generalization of Top1 accuracy for synthetic dataset, over prior measurement like using Visual Genome, or captions perturbation. This could be significant to the community exploring model generalization.\n2. Author have conducted various analysis over the relationship between compositionally and feature disentanglement, demonstrate the potential influence of the proposed dataset at a large scale."
                },
                "weaknesses": {
                    "value": "1. In 3.2, the statement 'We interpret these findings as strong evidence that the inclusion of language supervision, particularly during CLIP training, positively impacts the model representation quality' might be too strong of a claim. As explored in prior work(\"Data Determines Distributional Robustness in Contrastive Language Image Pre-training (CLIP)\") , language supervision might not be the sole reason for model generalization. There're multiple variance between VLM and other modality and author should not attribute such improvement solely on language supervision. \n2. Conclusion are less convincing due to the limited candidate in each experiment. For instance, in Table 1, it will be interesting to shows the NMI for a subset of LAION with the same number of data to other dataset. Also in table 2, there's only 4 results, please consider adding more variance of dataset and CLIP architecture . \n3. The narrative after section 4 is a bit too rush, it's hard to follow the method and results. For instance, what is 'dimensions' in 4.1 stands for? And more context over 'switching dimension' would be helpful. Moreover, I cannot tell how the conclusion of 'A higher level of accuracy in the image retrieval task indicates that the model embeddings are more disentangled.' can be drawn from experiment in 4.2. \n4. There's some grammar and formatting issue, for instance in section 4, spaces were missing between sentences."
                },
                "questions": {
                    "value": "Page 3: in the imrpoved generalization -> typo\nPlease refer to weakness. While this work could be potential significant to the community, the clarity could be further improve, especially on drawing the connection between compositionally and feature disentanglement."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9428/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9428/Reviewer_unRs",
                        "ICLR.cc/2024/Conference/Submission9428/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9428/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699178742989,
            "cdate": 1699178742989,
            "tmdate": 1700690660071,
            "mdate": 1700690660071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hiOVu4Pt6l",
                "forum": "UVSKuh9eK5",
                "replyto": "AUeGBXg0Pe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9428/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9428/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Point 1"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. I appreciate the time and effort you have put into reviewing my work. Below, I have addressed each of your points in detail.\n\n**Point1** : We acknowledge that, as already mentioned in the paper (first paragraph in the introduction), some prior work such as [1] suggested that the model generalization is indeed influenced by multiple factors beyond language supervision alone.  \n However, we note that there are certain shortcomings in assessing the role of language in generalization of CLIP models in [1]. Specifically, the dataset (ImageNet-Captions) that is used in [1] to investigate the effect of language in domain robustness could lack domain information (e.g. painting, sketch, clip art, etc.) in the captions that are used for training and thus does not utilize the language capability to tackle domain shift. This stems from the fact that in this dataset, the captions are taken from the original text data of the Flickr images. Hence, the language is not used in its maximum capacity to train the model. However, in our case, we expect all the composition constituents, which are attributes and objects, been described numerous times *individually* in the training data. In fact, we investigate compositional OoD generalization, which assumes that the novel combination of *known* concepts appear at the test time and evaluate the role of language in this type of OoD generalization. Aside from this, ImageNet-Captions is orders of magnitude smaller (~460k images) than the datasets that are used to train CLIPs, which are often in the order of 100 millions or few billions of images, which could be insufficient to expect the language to play a significant role in enhancing the generalization. On the other hand, we benchmark CLIPs that are trained with much larger training sets. Therefore, our work makes a better setup to assess the role of language in the generalization. \n Therefore, one of our aims was to re-assess whether the language could bring any additional value to the table beyond what has already been known in the  literature. \n\nWe have to emphasize that our intent was not to attribute the observed improvement in model performance solely to language supervision, but rather to see if the language could be *one of the* the  drivers of improvement in our compositional generalization setting. To make this happen, we conducted *controlled* experiments  to specifically isolate and understand the impact of language factors on model generalization. For instance, we compared models with *identical structures* and *training methodologies*, with the only variable being their *training datasets*, such as the CC and YFCC models. Both models employ the same image and text encoders, yet they yield different outcomes. Further analysis revealed that the captions in the CC dataset are of markedly higher quality (table 1), which provides empirical evidence of the language factor's influence on model performance.\n\nWe believe these controlled comparisons are essential to discern the discrete contributions of different variables to model generalization. In future iterations of our manuscript, we will strive to more precisely articulate the multifaceted nature of model generalization, ensuring that the significance of language supervision is presented as one of several contributing factors, rather than the sole determinant.\n\nWe hope this clarification addresses your concerns, and we are committed to revising our manuscript to reflect a more nuanced understanding of the elements that contribute to the robustness of model generalization.\n\n[1] Fang, Alex, et al. \"Data determines distributional robustness in contrastive language image pre-training (clip).\" International Conference on Machine Learning. PMLR, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074488967,
                "cdate": 1700074488967,
                "tmdate": 1700074488967,
                "mdate": 1700074488967,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rmZCp2xSAv",
                "forum": "UVSKuh9eK5",
                "replyto": "d9lLhXweNl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9428/Reviewer_unRs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9428/Reviewer_unRs"
                ],
                "content": {
                    "title": {
                        "value": "Update score to 6."
                    },
                    "comment": {
                        "value": "Thanks author for the detailed reply. I appreciate update on point 1 and 3, and expect author to revise your draft carefully for more precisely articulation. Besides, I see a nice trend based on including additional experiment for point2, author should also update OpenAI and LAION with more model architecture(like ResNet50-CLIP) to make your experiment more comprehensive. \nI will update my score to 6.\n\nIn general I think the idea proposed by this paper is still interesting and insightful, and constructing a dataset based on compositionality is novel and potential impactful to the community. However, as also mentioned by other reviewer, there still many lack of clarity in explaining motivation and justification of the proposed dataset. Author should continue to work on this for future revision."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9428/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690633503,
                "cdate": 1700690633503,
                "tmdate": 1700690633503,
                "mdate": 1700690633503,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]