[
    {
        "title": "Uncovering hidden geometry in Transformers via disentangling position and context"
    },
    {
        "review": {
            "id": "mzkL1zuEvc",
            "forum": "1M0qIxVKf6",
            "replyto": "1M0qIxVKf6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2630/Reviewer_rGTL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2630/Reviewer_rGTL"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the intermediate representation of Transformer by viewing that each token embedding is decomposed into (i) position-wise information and (ii) sequence-wise information. There are 3 main findings --- (a) (i) forms spiral curves in a low-dimensional space, (b) (ii) contains a cluster structure, (c) (i) and (ii) are almost orthogonal --- are observed on pre-trained language models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The analysis based on the decomposition is original. I haven't seen this type of decomposition of token embeddings.\n\nThe third finding (c) is an interesting property, which might open a new research direction."
                },
                "weaknesses": {
                    "value": "The first two findings (a and b) sound relatively trivial. I think the behavior of (a) mainly comes from the sinusoidal positional embedding. The effect of positional embedding propagates to upcoming layers via skip connections, which would explain why the spiral patterns are consistently observed across layers. For (b), since ctx vector is computed by averaging token embeddings over each sequence, it's natural to contain topic-like information. More precisely, each token embedding should contain context (or topic) related information to predict the next word. Taking the average will emphasize the context information, which should be distinguished from other context information obtained from a different document.\n\nThe paper analyzes the Transformer models in many aspects. However, each analysis is not tightly connected, and it's hard to capture concrete outcomes."
                },
                "questions": {
                    "value": "Why does Equation (7) not include the residual term?\n\nSection 4.2 starts with the following question:\n\"positional information is passed from earlier layers to later layers \u2026 How does transformer layer TFLayer enable this information flow?\"\nIsn't this simply because of the skip connections? Also, why do you consider the low-rank plus diagonal form in Equation (8)? Don't you observe the alignment with the positional basis by svd(W) instead of svd(W - diagg(W))?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2630/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2630/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2630/Reviewer_rGTL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2630/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838053103,
            "cdate": 1698838053103,
            "tmdate": 1700525484877,
            "mdate": 1700525484877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J4IFGSViHm",
                "forum": "1M0qIxVKf6",
                "replyto": "mzkL1zuEvc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for reading this submission and providing feedback. We first present our replies to common concerns, and then respond to specific issues.\n\n*Common concerns*\n1. **Significance of positional basis**. We emphasize that the positional embeddings in many transformers including GPT-2 are **trained from scratch** during pretraining---which is different from the sinusoidal positional embedding originally proposed. We find this nontrivial because\n    - as far as we know, the **low-rank and low-frequency** structure has not been examined extensively before.\n    - The magnitude of the positional basis can change across layers (for aesthetic reasons, Figure 1 is presented as scale-free). We observed the positional basis increases significantly from layer 0 to layer 1. So residual connection is not the sole reason for consistent positional basis.\n    - Our discovery helps elucidate confusing artifacts (e.g., spiral clustering in embeddings) people observed; see Section 6.\n    - **Smoothness is the key to Transformers**. Our analysis on an arithmetic task in Section 5 shows a failure example where transformed exhibit discontinuous patterns that result in failure of length generalization.\n\n2. **Induction head.** One of the most surprising phenomena about LLMs is the emergent abilities, and induction head---which functions as copying even in OOD data---is one simple example. Still, much is unknown about induction head.\n   - Our decomposition-inspired analysis identifies 3 interpretable components. The first two (attention to self-token, attention to neighboring tokens) correspond to exactly positional basis or cvec. \n   - This ``abstract ability'' learned by transformers may allow people to understand the inner workings of LLMs and lead to practical values.\n   - It provides a new visualization idea---which decouples positional effects from non-positional effects.\n\n\n*Specific issues*\n\n- Q1 \"The first two findings (a and b) sound relatively trivial\". We disagree that the finding a is trivial. See our response to common concerns 1. Moreover:\n  \n    - We hope to draw the reviewer's attention to the fact that many Transformers use positional embeddings **entirely trained from scratch**, so the sinusoidal is not known in advance. Even so, the **low-rank and low-freq** structure is not expected. \n    \n    - Note that the positional basis lies approximated in a low-dimensional subspace. As far as we know, this is not known before, and is the reason for many artifacts people observed in embedding visualiation.\n- Q2: \"Why does Equation (7) not include the residual term?\" In this decomposition, we are only interested in separating the positional effects vs non-positional effects, so $h$ is decomposed into $\\mathrm{pos}$ plus $\\mathrm{cvec}$. Note that $\\mathrm{cvec}$ includes the residual term already.\n- Q3: \"How does transformer layer TFLayer enable this information flow\". Our question is to **understand how information flow is processed** in the attention component, when passing through the self-attention in each layer. \n  - The structural decomposition of $W$ shows that there is a positional low-rank part corresponding to exactly the positional subspace of embeddings. As far as we know, this gives the *first consistent identification of interpretable components of pretrained weights*."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351457513,
                "cdate": 1700351457513,
                "tmdate": 1700417321002,
                "mdate": 1700417321002,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yYmtlXx1GC",
                "forum": "1M0qIxVKf6",
                "replyto": "J4IFGSViHm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2630/Reviewer_rGTL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2630/Reviewer_rGTL"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "I appreciate the feedback from the authors. I realized that I had been mistaken about the positional embedding (I assumed it's not trainable). Since now I believe the results are more untrivial, I will raise my score to 5. However, my concern about the concrete outcomes still remains (which aligns with reviewer Jv3k), and I still feel the manuscript is not ready for acceptance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525456488,
                "cdate": 1700525456488,
                "tmdate": 1700525456488,
                "mdate": 1700525456488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BNuWDiM6JC",
            "forum": "1M0qIxVKf6",
            "replyto": "1M0qIxVKf6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2630/Reviewer_Jv3k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2630/Reviewer_Jv3k"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts an investigation of geometric structures of embeddings leaned by transformers. It first proposes a decomposition of the embeddings to a positional component (mean vectors across context) and a contextual component (mean vectors across position). Then, it studies the geometry of each of these components. For the positional component, they find that that it is low-dimensional and smoothly varying. Concretely the Gram matrix of positions is low-rank in the Fourier domain. For the contextual component, they identify clustering structures. Finally, they find that contextual component is incoherent (almost orthogonal) to the positional component."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-- The investigation of geometry of token embeddings is in my opinion interesting and could shed light on the operation of LLMs\n\n-- I like the proposed decomposition of embeddings into their global mean, positional, contextual and residual part. It is simple, but interesting\n\n-- The authors have conducted rather thorough investigation with multiple experiments \n\n-- The discussion on low-rankness of positional embeddings and its connection to smoothness via fourier analysis is interesting"
                },
                "weaknesses": {
                    "value": "I am torn on my decision about the paper. I like the investigation and there are ideas in the paper which I find nice. At the same time though, I  believe the paper could benefit from an attempt to better discern and articulate the messages of the findings. Moreover, my opinion is that by discussing too many (and many of them incomplete) topics, main (and potentially interesting) messages are \"lost\". \n\n-- Several topics discussed feel incomplete, such as (1) clustering of contexts in Sec. 3; (2) content of Sec. 4.2; (3) Last paragraph on Section 5.2 (there doesn't seem to be anything informative being said here including App E other than reporting of figures)\n\n-- The discussion on induction heads is distracting and I don't see the relevance to the rest of the paper\n\n-- I find the presentation of the paper particularly after Sec 2 confusing. There is no clear coherence between sections/subsections. Eg., not made clear how Sec. 4.2 and 4.3 fit within the story. Overall the paper would benefit from a careful read."
                },
                "questions": {
                    "value": "-- Do you have an intuition/interpretation for the spiral shape? I believe I understand the point you are making on smoothness, but what does the particular shape tells us (if anything)? If nothing, then why is it emphasized?\n\n-- Any explanations on the non-spiral trends in Figs 9-11 in the appendix?\n\n-- last paragraph \"Why smoothness\" of Sec 2: Can you please elaborate why smoothness allows attention to neighboring tokens easily? Also, you discuss there about QK scores, but those involve the WQ,WK matrices which from what I understand are not considered in Sec 2 (only gram matrix of positional embeddings)\n\n-- How the clustering property of the contextual part of embeddings on per document basis is informative? Also, how would the results change based on the four sampled documents?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2630/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903868343,
            "cdate": 1698903868343,
            "tmdate": 1699636202609,
            "mdate": 1699636202609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9FTcFPw7Nx",
                "forum": "1M0qIxVKf6",
                "replyto": "BNuWDiM6JC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for reading this submission and providing feedback. We first present our replies to common concerns, and then respond to specific issues.\n\n*Common concerns*\n1. **Significance of positional basis**. We emphasize that the positional embeddings in many transformers including GPT-2 are **trained from scratch** during pretraining---which is different from the sinusoidal positional embedding originally proposed. We find this nontrivial because\n    - as far as we know, the **low-rank and low-frequency** structure has not been examined extensively before.\n    - The magnitude of the positional basis can change across layers (for aesthetic reasons, Figure 1 is presented as scale-free). We observed the positional basis increases significantly from layer 0 to layer 1. So residual connection is not the sole reason for consistent positional basis.\n    - Our discovery helps elucidate confusing artifacts (e.g., spiral clustering in embeddings) people observed; see Section 6.\n    - **Smoothness is the key to Transformers**. Our analysis on an arithmetic task in Section 5 shows a failure example where transformed exhibit discontinuous patterns that result in failure of length generalization.\n\n2. **Induction head.** One of the most surprising phenomena about LLMs is the emergent abilities, and induction head---which functions as copying even in OOD data---is one simple example. Still, much is unknown about induction head.\n   - Our decomposition-inspired analysis identifies 3 interpretable components. The first two (attention to self-token, attention to neighboring tokens) correspond to exactly positional basis or cvec. \n   - This ``abstract ability'' learned by transformers may allow people to understand the inner workings of LLMs and lead to practical values.\n   - It provides a new visualization idea---which decouples positional effects from non-positional effects.\n\n*Specific issues*.\n\n- Q1: \"Several topics discussed feel incomplete\". \n    We agree that some supporting arguments are not presented in the main paper; however, we feel that the appendix does contain enough details and supporting figures.\n- Q2: \"The discussion on induction heads is distracting\". Please refer to our main response on \"Induction head\". \n- Q3: \"I find the presentation of the paper particularly after Sec 2 confusing\". \n  - We provide further analyses based on the decomposition and illustrate why nearly orthogonoal (incoherence) structure is useful. We decided to include Section 4.1 to show that the pos-cvec decomposition can lead to better visualization. \n  - Section 4.2 shows that the weights exhibit a low-rank + diagonal structure. As far as we know, this provides the first interpretable decomposition of weights that is consistent in pretrained models.\n  - Section 4.3 shows that the incoherence structure is quite related to the classical compressed sensing & dictionary learning literature. We provide an analysis via this lens, showing that incoherence does lead to cross interaction in the self-attention being easily captured.\n\n*Additional questions*.\n- \"Do you have an intuition/interpretation for the spiral shape?\" The spiral shape is related to low-frequency structure and smoothness property. In the top-left part of each subplot in Figure 3, we see that the Gram matrix $G$ is smooth. If we simply take $W$ to be an identity matrix (which is closely related to empirical observation in Section 4.2), then\n  $$\n  \\mathrm{softmax} \\big( x_q^\\top W x_k \\big) \\approx \\mathrm{softmax} \\big( G_{qk} \\big)\n  $$\n  gives high values if the positions of query and key are close to the diagnoal part of the Gram matrix. In other words, the attention values are large if and only if $|q-k|$ is small---this is exactly attention to neighboring tokens.\n\n  Note that Gram matrix is used to analyze positional embeddings before [1], but only for the 0-th layer. Our analysis shows (both empirically and theoretically) a connection between Gram matrix and self-attention.\n\n- \"Any explanations on the non-spiral trends in Figs 9-11 in the appendix?\" We checked the frequency coefficients and the low-frequnecy components are also dominant ones. The specific linear combination of Fourier basis may be different from GPT-2, but the low-freqency and smoothness properties still hold.\n\n- \"last paragraph `Why smoothness' of Sec 2\". See the first\n reply in \"Additional questions.\"\n\n - \"How the clustering property of the contextual part of embeddings on per document basis is informative?\" Our context basis essentially ignores the order of input words. In this light, it is similar to the bag-of-words assumption in NLP before deep learning; see [2]. Clustering encodes the topic information.\n\n [1] Benyou Wang, et al. On position embeddings in bert. In International Conference on Learning Representations, 2020.\n\n[2] Arora, Sanjeev, et al. \"Learning topic models--going beyond SVD.\" 2012 IEEE 53rd annual symposium on foundations of computer science. IEEE, 2012."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351382930,
                "cdate": 1700351382930,
                "tmdate": 1700351382930,
                "mdate": 1700351382930,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Iaxr7mv6wN",
            "forum": "1M0qIxVKf6",
            "replyto": "1M0qIxVKf6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2630/Reviewer_K9yz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2630/Reviewer_K9yz"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to demystify the internal workings of transformer models by presenting a novel decomposition of the hidden states (or embeddings) into interpretable components. For a layer's embedding vectors, a decomposition is achieved which distinguishes the mean effects of global, positional, and contextual vectors, as well as residual vectors, providing insights into the input formats and learning mechanisms of transformer"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**S1**. The paper introduces a novel decomposition method that separates the mean effects of global, positional, and contextual vectors within transformer embeddings. This approach offers a fresh perspective on understanding the internal mechanisms of transformers, revealing insights into how they encode and process information.\n\n**S2**. The paper provides extensive numerical experiments on a variety of models, including Llama2-7B and BLOOM, to validate the proposed decomposition approach. These experiments include token randomization and arithmetic tasks, which demonstrate the ability of the decomposition to capture different aspects of transformer embeddings."
                },
                "weaknesses": {
                    "value": "Please see the Questions below."
                },
                "questions": {
                    "value": "**Section 1:**\n\nThe significance of Transformers in research is well-known, but the paper's introduction does not clarify the purpose of the proposed method. Could the authors detail how this new decomposition relates to ANOVA and previous work on positional embeddings and induction heads? Moreover, what practical benefits does this decomposition provide? The main outcomes of the experiments in both the paper and appendix also need clarification.\n\n**Section 2:**\n\n- Please define 'smoothness' in the context of your paper. It is essential to relate this to DFT and IDFT for better understanding.\n\n-  The term $|| ||_{op}$ is used but not defined. \n\n- In Equation (6) (LHS) of Theorem 1, there is a dimension mismatch; the first term is \\(T \\times T\\) and the second is \\(k \\times k\\). \n\n**Sections 3 and 4:**\n\n- The writing in these sections needs improvement. Starting with a summary of the key findings before referring to figures would enhance clarity. What are the main points of these sections?\n\n- What does \\(O(1)\\)-sparse representation by bases mean in Theorem 2?\n\n**Section 6:**\n\nThe claim of providing a \"complete picture\" is too broad. How does this research stand apart from earlier studies on positional embeddings and induction heads?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2630/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2630/Reviewer_K9yz",
                        "ICLR.cc/2024/Conference/Submission2630/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2630/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699577254288,
            "cdate": 1699577254288,
            "tmdate": 1699640006585,
            "mdate": 1699640006585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nm2ki9Z4f0",
                "forum": "1M0qIxVKf6",
                "replyto": "Iaxr7mv6wN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2630/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2630/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for reading this submission and providing feedback. We first present our replies to common concerns, and then respond to specific issues.\n\n*Common concerns*\n1. **Significance of positional basis**. We emphasize that the positional embeddings in many transformers including GPT-2 are **trained from scratch** during pretraining---which is different from the sinusoidal positional embedding originally proposed. We find this nontrivial because\n    - as far as we know, the **low-rank and low-frequency** structure has not been examined extensively before.\n    - The magnitude of the positional basis can change across layers (for aesthetic reasons, Figure 1 is presented as scale-free). We observed the positional basis increases significantly from layer 0 to layer 1. So residual connection is not the sole reason for consistent positional basis.\n    - Our discovery helps elucidate confusing artifacts (e.g., spiral clustering in embeddings) people observed; see Section 6.\n    - **Smoothness is the key to Transformers**. Our analysis on an arithmetic task in Section 5 shows a failure example where transformed exhibit discontinuous patterns that result in failure of length generalization.\n\n2. **Induction head.** One of the most surprising phenomena about LLMs is the emergent abilities, and induction head---which functions as copying even in OOD data---is one simple example. Still, much is unknown about induction head.\n   - Our decomposition-inspired analysis identifies 3 interpretable components. The first two (attention to self-token, attention to neighboring tokens) correspond to exactly positional basis or cvec. \n   - This ``abstract ability'' learned by transformers may allow people to understand the inner workings of LLMs and lead to practical values.\n   - It provides a new visualization idea---which decouples positional effects from non-positional effects.\n\n*Specific issues*.\n\n- Q1 \"Could the authors detail how this new decomposition relates to ANOVA and previous work on positional embeddings and induction heads?\" As far as we know, ANOVA used in analysis of transformers is new. \n\n- Q2 \"Moreover, what practical benefits does this decomposition provide?\" We offer improved interpretations of tranformers---which is important considering the potential risks of LLMs. More specifically,\n  - We highlight that **smoothness is the key to Transformers** by analyzing the positional basis and a failure study on addition tasks where discontinuous structure prevents length generalization.\n  - We identify a consistent geometrical structure. This is connected to classical literature in compressed sensing and dictionary learning.\n  - We also have a new visualization idea based on our decomposition and apply this idea to induction head.\n\n*Additional questions*. \n- O(1)-sparse representation by bases is defined in the paper in Section 4.3 Equation (11).\n- Operator norm (usually denoted by \"op\") is commonly defined as $||A|| = \\sup_{v} \\frac{||Av||_2}{||v||_2}$ .\n- Thank you for pointing out the typo in Equation (6). It should be $(F_{\\le k}B)(F_{\\le k}B)^T$."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2630/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351124936,
                "cdate": 1700351124936,
                "tmdate": 1700351124936,
                "mdate": 1700351124936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]