[
    {
        "title": "Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval"
    },
    {
        "review": {
            "id": "CnPMtigEHp",
            "forum": "5BXAXOpaWu",
            "replyto": "5BXAXOpaWu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2541/Reviewer_zR31"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2541/Reviewer_zR31"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims at the task of composed image retrieval (CIR) which retrieves images by providing a multimodal query such as a query image and additional text which describes the user's further query intention. Usually, such task is resolved by aligning the multimodal query and gallery features with vision-and-language pretraining and finetuning techniques. The authors argue that existing methods are not feasible to mobile applications due to expensive computational costs by forwarding large multimodal foundation models on mobile devices. The proposed solution is adopting a lightweight model to process the query while still maintaining the large foundation model for the gallery side. In order to bridge the representation gap between the lightweight and large model, the adaptive token learner is proposed to map an image to a sentence in the language model space. Finally, the authors verify their contributions on three evaluation benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[1] Overall, this paper is well written and easy to follow. \n\n[2] I like the motivation of this work since deploying original large foundation model is almost impossible in mobile applications. Different from pruning or distilling such heavy models, the authors proposed the lightweight encoder with a tunable adaptive token learner. The idea behind is borrowed from the LLM-Adapters. \n\n[3] The proposed modules are technically sound and the experimental resutls are sufficient. The training resources are extremly friendly with 4 RTX 3090 GPUs."
                },
                "weaknesses": {
                    "value": "[1] Since this work is a retrieval task, it is important to report how the retrieval performance varies as the gallery size scales up. I understand that the three evalutation datasets are standard benchmarks. However, it would make the contribution more solid if millions distractors could be involved in the gallery, although most existing SOTAs didn't report such results. \n\n[2] The inference time should be reported including in the query side and in the cloud side.\n\n[3] Some minor issues are listed in the next part."
                },
                "questions": {
                    "value": "[1] Figure 1 could be further improved. Specifically, the pink rectangle and trapezoid could be shrunk and the blue trapezoids could be enlarged. As a result, it is much easier to quickly grasp the idea at first glance for readers. \n\n[2] Table 5 provides a viriant mapping network which is actually a MLP. Are there other options?\n\n[3] Figure 3 reveals a fact that larger token lengths could degrade the retrieval performance. The authors attribute such fact to the background noise or the trivial patterns. Are there any deeper insights or visualization analysis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Reviewer_zR31"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698053325478,
            "cdate": 1698053325478,
            "tmdate": 1700619451258,
            "mdate": 1700619451258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZaHtftuUuI",
                "forum": "5BXAXOpaWu",
                "replyto": "CnPMtigEHp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer zR31"
                    },
                    "comment": {
                        "value": "We appreciate the detailed comments and acknowledgment of our contributions. We provide the responses as follows.\n\n**Q1**: Since this work is a retrieval task, it is important to report how the retrieval performance varies as the gallery size scales up. I understand that the three evalutation datasets are standard benchmarks. However, it would make the contribution more solid if millions distractors could be involved in the gallery, although most existing SOTAs didn't report such results.\n\n**R1**: We adopt the distractors from R1M in [4] to scale up the gallery set during evaluation. Note that for CIRR and CIRCO, the evaluation on test set could only be conducted on the official cloud server, thus we add distractors to gallery for the validation set. The results on three benchmarks are shown as below, which reveal that the distractors in the gallery would dampen the retrieval accuracy, however our method is still superior on three benchmarks. \n\n#### Scaling up the gallery of FashionIQ validation set\nMethod | Scale of distractors | Dress R@10 | Dress R@50 | Shirt R@10 | Shirt R@50 | Toptee R@10 | Toptee R@50 | R@10 Avg | R@50 Avg |\n| :------: | :------:| :------:| :------: | :------: | :------: | :------: | :------: | :------: | :------: | \n| Pic2word | 0 | 21.35 | 42.68 | 27.51 | 46.01 | 29.12 | 49.33 | 25.99 | 46.00 |\n| Pic2word | 1M | 19.88 | 39.94 | 26.30 | 44.99 | 27.40 | 47.73 | 24.53 | 44.22 |\n| SEARLE | 0 | 22.11 | 41.79 | 29.72 | 48.53 | 31.03 | 52.37 | 27.62 | 47.56 |\n| SEARLE | 1M | 21.24 | 39.77 | 28.95 | 47.79 | 30.90 | 50.95 | 27.03 | 46.17 |\n| Ours | 0 | 25.33 | 46.26 | 30.03 | 48.58 | 33.45 | 53.80 | 29.60 | 49.54 |\n| Ours | 1M | 25.24 | 45.46 |  29.54 | 47.30 | 32.18 | 52.32 | 28.99 | 48.36 |\n\n#### Scaling up the gallery of CIRR validation set\n\n| Method | Scale of distractors | R@1 | R@5 | R@10 | R@50 | Avg |\n| :------: | :------:| :------:| :------: | :------: | :------: | :------: |\n| Pic2word | 0 |  27.89 | 58.15 | 69.42 | 87.53 | 60.75 |\n| Pic2word | 1M | 19.47 | 38.01 | 46.76 | 64.15 | 42.10 |\n| SEARLE | 0 | 28.03 | 60.74 | 71.41 | 88.82 | 62.25 |\n| SEARLE | 1M | 21.84 | 43.70 | 53.55 | 70.96 | 47.51\n| Ours | 0 | 31.48 | 63.42 | 77.27 | 93.16 | 66.33 |\n| Ours | 1M | 22.77 | 45.64 | 55.49 | 72.26 | 49.04 |\n\n\n#### Scaling up the gallery of CIRCO validation set\n\n| Method | Scale of distractors | mAP@5 | mAP@10 | mAP@25 | mAP@50 | Avg |\n| :------: | :------:| :------:| :------: | :------: | :------: | :------: |\n| Pic2word | 0 | 7.91 | 8.27 | 9.05 | 9.56 | 8.70 |\n| Pic2word | 1M | 4.85 | 5.12 | 5.58 | 5.81 | 5.34 |\n| SEARLE | 0 |  11.52 | 11.74 | 12.91 | 13.56 | 12.43 |\n| SEARLE | 1M | 7.14 | 7.55 | 8.15 | 8.47 | 7.83 | \n| Ours | 0 | 13.19 | 13.83 | 15.20 | 15.85 | 14.52 |\n| Ours | 1M | 8.66 | 9.16 | 10.20 | 10.61 | 9.66 |\n\n[4] Radenovi\u0107, F., Iscen, A., Tolias, G., Avrithis, Y., & Chum, O. (2018). Revisiting oxford and paris: Large-scale image retrieval benchmarking. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 5706-5715).\n\n**Q2**: The inference time should be reported including in the query side and in the cloud side.\n\n**R2**: We report the latency on both query side and cloud side on CIRCO vailidation set. For query side, we report the inference latency for different query models. We find that lightweight encoders are generally twice faster than large visual encoder (BLIP VE). For cloud side, since the gallery models are the same (BLIP TE & VE), the retrieval latency is the same. \n\n| Query model | Query inference latency (ms, per query) | Gallery model | Retrieval latency (ms, per query) | Total retrieval latency (ms, per query) |\n| :------:| :------:| :------: | :------: | :------: |\n| EfficientNet B0 | 3.07 | BLIP VE | 3.58 | 6.65 |\n| EfficientNet B2 | 3.24 | BLIP VE | 3.58 | 6.82 |\n| EfficientViT M2 | 3.27 | BLIP VE | 3.58 | 6.85 |\n| MobileNet V2 | 3.03 | BLIP VE | 3.58 | 6.61 |\n| MobileViT V2 | 3.40 | BLIP VE | 3.58 | 6.98 |\n| BLIP VE | 6.53 | BLIP VE | 3.58 | 10.11 |\n\n**Q3**: Figure 1 could be further improved. Specifically, the pink rectangle and trapezoid could be shrunk and the blue trapezoids could be enlarged. As a result, it is much easier to quickly grasp the idea at first glance for readers.\n\n**R3**: Thanks for the suggestions of Figure 1. We have revised Figure 1 for better clarity."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578075754,
                "cdate": 1700578075754,
                "tmdate": 1700578075754,
                "mdate": 1700578075754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LYMpxQnlrK",
                "forum": "5BXAXOpaWu",
                "replyto": "acMVwMThLW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_zR31"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_zR31"
                ],
                "content": {
                    "title": {
                        "value": "Response to the commets"
                    },
                    "comment": {
                        "value": "Aftering reading the author's detailed response, I think my concerns have already been resolved. Thus, I just upgraded my rating score. Besides, I suggest incorporating these additional results and tables into the final supplementary materials."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619849195,
                "cdate": 1700619849195,
                "tmdate": 1700619849195,
                "mdate": 1700619849195,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BHtq8sUKKo",
            "forum": "5BXAXOpaWu",
            "replyto": "5BXAXOpaWu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2541/Reviewer_K4T5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2541/Reviewer_K4T5"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an asymmetric zero-shot composed image retrieval framework. The asymmetric retrieval pipeline is established using a lightweight model for query images and a large foundation model for gallery images, enabling feature extraction. Composed image retrieval is achieved by concatenating the sentence representation mapped from the image with a text modifier. To align the features extracted by the lightweight model and the large foundation model, two techniques, namely global contrastive distillation and local alignment regularization, are proposed. Extensive experiments and an ablation study conducted on benchmark datasets have demonstrated the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. An asymmetric zero-shot composed image retrieval framework is proposed. \n2. Global contrastive distillation and local alignment regularization techniques are proposed to align features from different models.\n3. Extensive experiments are conducted."
                },
                "weaknesses": {
                    "value": "1. The clarity of the writing, particularly in the methods section, requires improvement.\n\n2. For image-only retrieval, could you provide results using the DINO-V2 and MoCo-V3 pretrained models? The CLIP model is typically used for content matching between image and text features."
                },
                "questions": {
                    "value": "t-SNE visualizations could be shown to illustrate the differences between the different methods. This would provide a more intuitive understanding of the feature distributions and separability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Reviewer_K4T5",
                        "ICLR.cc/2024/Conference/Submission2541/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698559949738,
            "cdate": 1698559949738,
            "tmdate": 1700631528838,
            "mdate": 1700631528838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IHfostMbzc",
                "forum": "5BXAXOpaWu",
                "replyto": "BHtq8sUKKo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer K4T5"
                    },
                    "comment": {
                        "value": "We appreciate the detailed comments and acknowledgment of our contributions. We provide the responses as follows.\n\n**Q1**: The clarity of the writing, particularly in the methods section, requires improvement.\n\n**R1**: Thanks for the suggestion, we have carefully revised our manuscript, with changes marked in blue for clarity. The major modifications include:\n- the description of the framework in section 3\n- the merits of adaptive token learner in section 3.1\n- the clarification of inference phase in section 3.3\n\nPlease feel free to share if you have further concerns, we would be keen to address them as well.\n\n**Q2**: For image-only retrieval, could you provide results using the DINO-V2 and MoCo-V3 pretrained models? The CLIP model is typically used for content matching between image and text features.\n\n**R2**: We use the pretrained MoCo-V3 models (resnet50, ViT-base) and DINO-V2 models (ViT-base, ViT-large) to test the retrieval performance on three benchmarks. Note that self-supervised pretrained models are designed for visual modality and do not include the text encoder, thus we merely report the image-only retrieval results for them. For comparison, we also report the retrieval performance of our method that integrates both visual and textual information for retrieval. \n\n#### The image-only results on FashionIQ validation set\n\n| Network | Dress R@10 | Dress R@50 | Shirt R@10 | Shirt R@50 | Toptee R@10 | Toptee R@50 | R@10 Avg | R@50 Avg |\n| :------:| :------:| :------: | :------: | :------: | :------: | :------: | :------: | :------: |\n| CLIP VE | 5.40 | 13.90 | 9.90 | 20.80 | 8.30 | 17.70 | 7.90 | 17.50 |\n| BLIP VE | 4.07 | 11.35 | 7.07 | 16.39 | 6.88 | 13.67 | 6.01 | 13.67 |\n| MoCo-V3 resnet50 | 2.88 | 7.49 | 4.71 | 9.42 | 3.77 | 8.67 | 3.79 | 8.53 | \n| MoCo-V3 ViT-base | 2.93 | 8.23 | 4.32 | 9.81 | 3.67 | 8.62 | 3.64 | 8.89 |\n| DINO-V2 ViT-base | 4.31 | 10.66 | 6.92 | 13.05 | 5.10 | 11.73 | 5.44 | 11.81 |\n| DINO-V2 ViT-large | 4.46 | 11.06 | 6.77 | 12.71 | 6.12 | 12.80 | 5.78 | 12.19 |\n| Ours (multi-modal) | 25.33 | 46.26 | 30.03 | 48.58 | 33.45 | 53.80  | 29.60 | 49.54 |\n\n\n#### The image-only results on CIRR test set\n\n| Network | R@1 | R@5 | R@10 | R@50 | Avg |\n| :------:| :------:| :------: | :------: | :------: | :------: |\n| CLIP VE | 7.40 | 23.60 | 34.00 | 57.40 | 30.60 |\n| BLIP VE | 7.52 | 21.83 | 31.21 | 52.94 | 28.37 |\n| MoCo-V3 resnet50 | 8.53 | 30.58 | 43.69 | 73.35 | 39.04 |\n| MoCo-V3 ViT-base | 8.60 | 30.02 | 42.92 | 70.89 | 38.11 | \n| DINO-V2 ViT-base | 7.88 | 27.06 | 39.04 | 66.27 | 35.06 |\n| DINO-V2 ViT-large | 7.78 | 25.95 | 37.04 | 62.55 | 33.33 |\n| Ours (multi-modal) | 30.84 | 61.06 | 73.57 | 92.43 | 64.48 |\n\n#### The image-only results on CIRCO test set\n\n| Network | mAP@5 | mAP@10 | mAP@25 | mAP@50 | Avg |\n| :------:| :------:| :------: | :------: | :------: | :------: |\n| CLIP VE | 1.69 | 2.18 | 2.78 | 3.22 | 2.47 |\n| BLIP VE | 1.28 | 1.48 | 1.88 | 2.13 | 1.69 |\n| MoCo-V3 resnet50 | 0.83 | 1.01 | 1.28 | 1.41 | 1.13 | \n| MoCo-V3 ViT-base | 0.91 | 1.08 | 1.33 | 1.48 | 1.20 | \n| DINO-V2 ViT-base | 2.03 | 2.49 | 3.10 | 3.48 | 2.78 |\n| DINO-V2 ViT-large | 2.00 | 2.40 | 2.98 | 3.37 | 2.69 |\n| Ours (multi-modal) | 11.33 | 12.25 | 13.42 | 13.97 | 12.74 |\n\nGenerally, self-supervised models could improve the performance of image-only retrieval performance. However, we could see that they fail to make a difference for composed image retrieval compared with our method, since the CIR task requires comprehension of multi-modalities and the interaction between visual and textual information."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577325207,
                "cdate": 1700577325207,
                "tmdate": 1700578834423,
                "mdate": 1700578834423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2BMj6BjrTD",
                "forum": "5BXAXOpaWu",
                "replyto": "J7YScJcWAK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_K4T5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_K4T5"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "After reading other reviews and rebuttals, I decide to improve the final rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631504579,
                "cdate": 1700631504579,
                "tmdate": 1700631504579,
                "mdate": 1700631504579,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FcDWJQNTXF",
            "forum": "5BXAXOpaWu",
            "replyto": "5BXAXOpaWu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2541/Reviewer_WHDg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2541/Reviewer_WHDg"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to composed image retrieval (CIR), emphasizing on the challenges associated with composed image retrieval that needs understanding of both visual and textual data. To address data scarcity in CIR, the authors introduce a new task paradigm named \"zero-shot composed image retrieval\" (ZSCIR) that transforms image retrieval to a text-to-image format, allowing for a more intuitive mapping between images and descriptive text. However, the methods presented face challenges with large-scale models which are not suitable for deployment on resource-constrained platforms, such as mobile devices. To mitigate this, the authors propose an asymmetric approach, termed Image2Sentence based Asymmetric ZSCIR, that uses different models for query and database extraction. This method utilizes a lightweight model for the user's device and a heavier model for cloud processing. The core of this approach is an adaptive token learner which converts visual features into textual tokens, thus enhancing the representation. The proposed framework was tested on various benchmarks, demonstrating its efficiency and effectiveness compared to existing state-of-the-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper addresses the challenges in Composed Image Retrieval by introducing the zero-shot composed image retrieval (ZSCIR). This method offers a fresh perspective on image retrieval by transforming it to a text-to-image format, thereby providing a more direct linkage between descriptive text and its corresponding image.\n\n2. The introduction of an adaptive token learner, which effectively translates visual features into textual tokens, stands out as a major strength. This conversion mechanism is pivotal in enhancing the representation of images and ensuring that the retrieval process is both accurate and efficient. The adaptive nature of the learner means that it can adjust and improve over time, potentially leading to even better retrieval results in the future."
                },
                "weaknesses": {
                    "value": "1. While the adaptive token learner is a strength in terms of converting visual features to textual tokens, there's a risk that the system could become overly reliant on this component. If the learner fails or encounters unanticipated scenarios, it might compromise the effectiveness of the entire retrieval process.\n\n2. Introducing an asymmetric text-to-image retrieval approach, while innovative, adds an extra layer of complexity to the system. This might present challenges in terms of maintainability, debugging, and further development of the system.\n\n3. The transformation of the retrieval problem from image-to-image to text-to-image inherently assumes that the descriptive texts are of high quality and detailed. Any inaccuracies or vagueness in the text could lead to inefficient or incorrect image retrievals.\n\n4. The paper presentation is not very attractive. It is difficult to understand the novelties / contributions after reading the introduction of the paper."
                },
                "questions": {
                    "value": "1.  How does the ZSCIR approach compare in performance and efficiency with state-of-the-art image retrieval methods that don't employ a text-to-image asymmetry? Are there scenarios where a traditional symmetric approach might outperform ZSCIR?\n\n2. In terms of training data apart from the image augmentation, did you employ any data augmentation techniques to enhance the performance and robustness of the ZSCIR model? Furthermore, how did you ensure the diversity and representativeness of the descriptive texts used in the system?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Reviewer_WHDg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669489007,
            "cdate": 1698669489007,
            "tmdate": 1700655491805,
            "mdate": 1700655491805,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "06EwaSekdD",
                "forum": "5BXAXOpaWu",
                "replyto": "FcDWJQNTXF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer WHDg"
                    },
                    "comment": {
                        "value": "We appreciate the detailed comments and acknowledgment of our contributions. We provide the responses as follows.\n\nWe first make a clarification of composed image retrieval task and our method. As mentioned in Introduction (section 1) in the manuscript, the task of composed image retrieval allows multi-modal queries to retrieve the images in the database, i.e., the query includes an image, and a text that describes specified modification to the image. In other words, composed image retrieval is a task of \"image + text \u2192 image\". In practice, the text is given by the user to represent the accurate retrieval intent. Therefore, users could adapt the text modifier to be more detailed and precise to obtain more accurate search results.\n\nFor our method, we convert the \"image + text \u2192 image\" retrieval to the \"text-to-image\" retrieval by mapping the visual information to the word embedding space of the text encoder in large foundation model. During inference, the query image is converted as a pseudo sentence to concatenate with the text modifier, and they are together fed into the text encoder to extract the multi-modal query feature. The database image features are extracted by the visual encoder in large foundation model. Then by utilizing the text-image alignment capability of large foundation model, we are able to do accurate image retrieval with multi-modal query. Note that the asymmetry of our framework does not lie in the difference of modalities for the query and database side (text-to-image), but in adopting a lightweight model for the query side while adopting a large model for the database side to improve deployment flexibility and retrieval efficiency.\n\n**Q1**: While the adaptive token learner is a strength in terms of converting visual features to textual tokens, there's a risk that the system could become overly reliant on this component. If the learner fails or encounters unanticipated scenarios, it might compromise the effectiveness of the entire retrieval process.\n\n**R1**: In the context of **zero-shot** composed image retrieval, we do not have access to the labeled triplets (reference image, text modifier, target image) as direct supervision, which is necessary to train the multimodal composition capabilities of the model. Consequently, we convert visual information in the word embedding space as a descriptive pseudo sentence. This approach enables us to utilize the intrinsic composition ability of the text encoder, facilitating efficient interactions between visual information and the text modifier. This process is essential for extracting the multimodal query feature for retrieval. Therefore, it is critical to learn an effective adaptive token learner in our framework for the conversion, which is our main contribution. Our adaptive token learner is trained through global contrastive distillation loss and local alignment constraint with sufficient training data (3M), to align with the image feature space of BLIP model. Since BLIP model is pretrained on large-scale image-text dataset (129M), it possesses rich knowledge to guide the token learner to focus on prominent visual information and interact more effectively with textual information. \n\nWe admit that sometimes there will be difficulty for the adaptive token learner to catch the intended target in some complicated scenarios, as shown in the failure cases of Figure 6 in the appendix. Nevertheless, in most cases, our adaptive token learner could focus on the discriminative patterns and interact well with the text modifier as shown in the good example of Figure 5 in the appendix, which plays an important role in our framework to achieve outstanding retrieval performance, as shown in Table 2,3,4 in the manuscript. \n\nIn practical application scenarios, we can obtain a better token learner by enlarging the amount of training data and incorporating better multimodal foundation models to guide the learning process.\n\n**Q2**: Introducing an asymmetric text-to-image retrieval approach, while innovative, adds an extra layer of complexity to the system. This might present challenges in terms of maintainability, debugging, and further development of the system.\n\n**R2**: The lightweight visual encoders in our framework are much smaller and more efficient compared with the large foundation model, as they have fewer parameters and computations shown in Table 1. Therefore, it is much easier for maintainability, debugging and development compared with large foundation model. Although the asymmetrical structure brings about more complexity to the system, it enables faster inference for retrieval and deployment on resource-constrained platforms such as mobile phones or edge devices. Therefore, the asymmetrical structure helps to improve the retrieval efficiency. Moreover, the asymmetrical structure could also protect user privacy, since it only uploads the extracted tokens instead of real images on the Internet."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576561019,
                "cdate": 1700576561019,
                "tmdate": 1700625916035,
                "mdate": 1700625916035,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BVzalblODy",
                "forum": "5BXAXOpaWu",
                "replyto": "FcDWJQNTXF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer WHDg"
                    },
                    "comment": {
                        "value": "**Q3**: The transformation of the retrieval problem from image-to-image to text-to-image inherently assumes that the descriptive texts are of high quality and detailed. Any inaccuracies or vagueness in the text could lead to inefficient or incorrect image retrievals.\n\n**R3**: As we mentioned earlier, different from \"image-to-image\" uni-modal retrieval that merely takes image as query, the task of composed image retrieval allows multi-modal queries to retrieve the images in the database, which is a task of \"image + text \u2192 image\". In practice, the text is given by the user to represent the accurate retrieval intent. Therefore, users could adapt the text modifier to be more detailed and precise to obtain more accurate search results.\n    \nIn our framework, we convert the \"image + text \u2192 image\" problem to the \"text-to-image\" problem by mapping the visual information to the word embedding space of the text encoder in large foundation model as pseudo sentence. Therefore, the visual information could interact with the text modifier more efficiently in the text domain to obtain the multimodal composed features. We further utilize the text-image alignment capability of large foundation models to do accurate image retrieval, and achieve the best performance on three benchmarks.\n\n**Q4**: The paper presentation is not very attractive. It is difficult to understand the novelties / contributions after reading the introduction of the paper.\n\n**R4**: We are sorry for that we did not present clear contributions in the manuscript. Here we summarize our contributions as follows:\n    \n1. We propose a new framework for **zero-shot** composed image retrieval, which solves composition learning without the expensive labeled-triplets for supervised training.  \n\n2. We propose an asymmetrical structure for composed image retrieval to enable flexible deployment on resource-constrained platforms and improve the retrieval efficiency.\n\n3. We propose an adaptive token learner to map image as a sentence in the word embedding space to interact with text information, and automatically filter out noisy visual information to preserve discriminative semantics.\n\n4. Global constrastive distillation and local alignment regularization are proposed to guide the lightweight encoder learning with the knowledge of large foundation model, and our method achieves significant retrieval performance on CIRR, CIRCO and FashionIQ datasets.\n\n**Q5**: How does the ZSCIR approach compare in performance and efficiency with state-of-the-art image retrieval methods that don't employ a text-to-image asymmetry? Are there scenarios where a traditional symmetric approach might outperform ZSCIR?\n\n**R5**: As report in Table 2,3,4 in the manuscript, our asymmetrical method outperforms the state-of-the-art methods Pic2word and SEARLE in terms of retrieval performance, which adopt symmetrical structure. In Table 1 in the manuscript, we report the model parameter size and computational consumption for different query models. Here we consider Pic2word and SEARLE adopting BLIP VE as both query and gallery model. Compared with the symmetrical structure that adopts large visual encoder (BLIP VE), lightweight models require much fewer computational resources. Furthermore, we report the latency on both query side and cloud side on CIRCO validation set. For query side, we report the inference latency for different query models in the following table. We find that lightweight encoders are generally twice faster than the large visual encoder (BLIP VE). For cloud side, since the gallery models are the same (BLIP TE & VE), the retrieval latency is the same. Generally, the lightweight encoders are much more efficient than the heavy foundation model in terms of resource consumption and inference latency.\n\n| Query model | Query inference latency (ms, per query) | Gallery model | Retrieval latency (ms, per query) | Total retrieval latency (ms, per query) |\n| :------:| :------:| :------: | :------: | :------: |\n| EfficientNet B0 | 3.07 | BLIP VE | 3.58 | 6.65 |\n| EfficientNet B2 | 3.24 | BLIP VE | 3.58 | 6.82 |\n| EfficientViT M2 | 3.27 | BLIP VE | 3.58 | 6.85 |\n| MobileNet V2 | 3.03 | BLIP VE | 3.58 | 6.61 |\n| MobileViT V2 | 3.40 | BLIP VE | 3.58 | 6.98 |\n| BLIP VE | 6.53 | BLIP VE | 3.58 | 10.11 |\n\nTo train the symmetrical framework, the large visual encoder needs to be updated during training. Since the large visual encoder is much heavier than our lightweight encoder, it is more likely to overfit and requires larger GPU memory to reach the same batch size for good performance during training. With more training data and more GPU resources, we believe that the symmetrical setting would outperform the asymmetrical structure in terms of retrieval accuracy."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576984036,
                "cdate": 1700576984036,
                "tmdate": 1700626179235,
                "mdate": 1700626179235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ADukhJeL4k",
                "forum": "5BXAXOpaWu",
                "replyto": "FcDWJQNTXF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_WHDg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2541/Reviewer_WHDg"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for writing very detailed rebuttal and reply all the weaknesses and questions that I mentioned. After looking into the rebuttal and also going through other reviews and replies, I think the paper is making a good contributions to the community. So I have decided to increase my rating. I hope the authors would address the comments in the final version of the paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655455357,
                "cdate": 1700655455357,
                "tmdate": 1700655455357,
                "mdate": 1700655455357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ljoI7SCM0D",
            "forum": "5BXAXOpaWu",
            "replyto": "5BXAXOpaWu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2541/Reviewer_ZKF9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2541/Reviewer_ZKF9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an image2sentence based asymmetric framework for zero-shot composed image retrieval tasks. In particular, a lightweight visual encoder and a consequent adaptive token learner are designed to effectively extract the visual features from query images for the mobile side. By doing so, the learned features could be generated as a good visual prompt as with the text intent for conventional LLM to deal with image retrieval tasks. In addition, a local alignment regularization term is added to further improve the training. The experiments conducted on several benchmark datasets verify the effectiveness of the proposed method compared with existing SOTA ones."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is of good written quality that makes the readers easy to follow. The logic, the notion expression, and the experiments are all very clear.\n\n2. The asymmetric design is interesting and this design has been proven an efficient way to deal with resource-limited circumstances.\n\n3. The experiments conducted are very convincing to support the contribution claimed by the authors. The properties of the proposed method are well demonstrated in the ablation study."
                },
                "weaknesses": {
                    "value": "1. It could be better to discuss more about the number of token selections in detail. According to Fig 3, it seems the performance is a bit sensitive to the number of tokens used in the proposed method. Then, a more detailed discussion of this observation with a visual example of the same query but a different number of tokens could help to better demonstrate the impact brought by the tokens.\n\n2. It could be good to add a discussion of the relationship between the proposed adaptive token learner and the similar approach used in the following papers [1,2]. They have a similar structure, a discussion would help to better locate the position of the token learned in this work.\n\n [1] Wu, Hui, et al. \"Learning token-based representation for image retrieval.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 3. 2022.\n [2] Locatello, Francesco, et al. \"Object-centric learning with slot attention.\" Advances in Neural Information Processing Systems 33 (2020): 11525-11538."
                },
                "questions": {
                    "value": "Please check the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Nil"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2541/Reviewer_ZKF9"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699266015265,
            "cdate": 1699266015265,
            "tmdate": 1699636190632,
            "mdate": 1699636190632,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zePg9MaSL5",
                "forum": "5BXAXOpaWu",
                "replyto": "ljoI7SCM0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2541/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer ZKF9"
                    },
                    "comment": {
                        "value": "We appreciate the detailed comments and acknowledgment of our contributions. We provide the responses as follows.\n\n**Q1**: It could be better to discuss more about the number of token selections in detail. According to Figure 3, it seems the performance is a bit sensitive to the number of tokens used in the proposed method. Then, a more detailed discussion of this observation with a visual example of the same query but a different number of tokens could help to better demonstrate the impact brought by the tokens.\n\n**R1**: Generally, to achieve accurate retrieval, the pseudo sentence tokens need to accomplish two aspects: extracting effective visual information and interacting with textual information. When the token length is very small, there would be drop in performance due to the lack of capacity to extract sufficient visual information. As shown in Figure 8 in the appendix of manuscript, query model with very small token length performs worse even if the attention maps are reasonable.\n\nHowever, when the token length increases for too long, on one hand, some tokens may focus on trivial and noisy visual patterns; on the other hand, these tokens may interact incorrectly with the text modifier, and may impact the correct token-text interaction. In Figure 4 in the manuscript, we have demonstrated the comparison of different token lengths with the same query. With the visualization of the attention maps of adaptive token learner and the retrieval results, it could be seen that some sentence tokens may tend to focus on the background or the trivial patterns with excessively large token lengths, which would likely introduce noise to degrade the retrieval performance. We further show more comparisons of different token lengths, including the retrieval results, attention maps of sentence tokens with text modifier and the visual feature map in Figure 9 in the manuscript appendix. As shown in Figure 9, the excessive tokens associate the visual patterns with incorrect text information, potentially interfering with the interaction of multi-modal information. Even if there are accurate associations between other sentence tokens and text modifier, this interference would still degrade the retrieval accuracy.\n\n**Q2**: It could be good to add a discussion of the relationship between the proposed adaptive token learner and the similar approach used in the following papers [1,2]. They have a similar structure, a discussion would help to better locate the position of the token learned in this work.\n\n**R2**: In [1], the token learner is adopted to jointly learn local feature representation and aggregation in a unified framework. The output of token learner is the local features, which are concatenated as the aggregated global feature for image retrieval. In [2], the token learner maps a set of N input feature vectors to a set of K output vectors as slots, each of which may describe an object or entity in the input for the task of object discovery and set prediction. \n\nAs pointed out in [1] and [2], the token learner is utilized to focus on multiple discriminative visual patterns or entities and filter noise information such as background and indiscriminative image regions. Therefore, this structure could serve as an efficient visual extractor in various visual tasks. \n\nAlthough our method also uses adaptive token learner, the semantics of token learner output are different from theirs. The output of our token learner resides in the word embedding space of the text encoder, which serves as a pseudo sentence, with each token acting like a word to interact with information of text modifier for further multimodal composition in CIR task. While in [1] and [2], the outputs are used directly for uni-modal vision tasks like similarity retrieval and object recognition. Therefore, the utilization of token learner in [1] and [2] is not suitable for our composed image retrieval task, while our token learner is devised for this task and achieves promising retrieval results in experiments.\n\n[1] Wu H, Wang M, Zhou W, et al. Learning token-based representation for image retrieval. Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(3): 2703-2711. \n\n[2] Locatello, Francesco, et al. \"Object-centric learning with slot attention.\" Advances in Neural Information Processing Systems 33 (2020): 11525-11538.\n\n\nFinally, thank you again for your recognition and positive review to our work. We will incorporate your suggestions into our next revision.\n\nNote: since OpenReview does not allow image uploading in the comment box, we place the additional figures in the appendix of revised manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576355235,
                "cdate": 1700576355235,
                "tmdate": 1700578365023,
                "mdate": 1700578365023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]