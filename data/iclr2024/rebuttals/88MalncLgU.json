[
    {
        "title": "GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Networks Explanations"
    },
    {
        "review": {
            "id": "6Y5IPVRkeY",
            "forum": "88MalncLgU",
            "replyto": "88MalncLgU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3765/Reviewer_DCst"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3765/Reviewer_DCst"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of out-of-distribution explanations, which means in the explainability tasks of graph neural networks, the highlighted explanation subgraph\u2019s distribution differs from the training data. The existing evaluation metrics such as faithfulness or fidelity score couldn\u2019t evaluate the explanation well due to the OOD issue. The author proposed GInX-Eval to better evaluate the explainers by retraining the GNN model and showed its great evaluation performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper has a great impact on the domain of the XAIG. It addresses a common concern about how the OOD problem affect the performance of the commonly used faithfulness metric.\n2. Figure 2 shows the effectiveness of the proposed methods greatly.\n3. The claims in section 4 are easy to follow and good to refer to.\n4. This paper has a good presentation and is easy to follow. \n5. The experiments are solid and sufficient."
                },
                "weaknesses": {
                    "value": "1. GInX-Eval has to treat the pre-trained model as a white box because it needs to retrain the model during the whole procedure. However, the pre-trained to-be-explained model is not always a white box, especially in real-life applications. The training dataset may not be able to be accessed, or the training cost is high, even the model itself may be not accessible. So, this approach is not easy to apply.\n2. The methodology itself is not novel enough. Remove and retrain is not new in the machine learning community, eg: \u201cSara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim, 2019, A Benchmark for Interpretability Methods in Deep Neural Networks\u201d.\n3. The contributions are over-claimed. Some previous work have also addressed the OOD problem, eg: \n\n[1] \u201cJunfeng Fang, Xiang Wang, An Zhang, Zemin Liu, Xiangnan He, and Tat-Seng Chua. 2023. Cooperative Explanations of Graph Neural Networks. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining (WSDM '23). Association for Computing Machinery, New York, NY, USA, 616\u2013624. https://doi.org/10.1145/3539597.3570378\"\n\n[2] \u201cJ Fang, W Liu, A Zhang, X Wang, X He, K Wang, TS Chua. On Regularization for Explaining Graph Neural Networks: An Information Theory Perspective \u201d\n\n[3] \u201cJiaxing Zhang, Dongsheng Luo, Hua Wei. 2023. MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. SIGKDD\u201923\u201d\n\n[4] \"Ying-Xin Wu, Xiang Wang, An Zhang, Xia Hu, Fuli Feng, Xiangnan He and Tat-Seng Chua, 2022, Deconfounding to Explanation Evaluation in Graph Neural Networks.\u201d"
                },
                "questions": {
                    "value": "Comments:\n1. \u201cThe highlighted explanation subgraph\u2019s distribution is different from the training data.\u201d Why is different and what\u2019s the nature where this difference comes from?\n2. Why did the explanations\u2019 distribution shift to a better side but not a worse side? For example: the prediction label is 50. A good explanation prediction should be 50. A bad explanation prediction should be 20. However, due to the  OOD, the explanation prediction shifts. Why a bad prediction would shift from 20 to 45 and cause an incorrect high faithfulness score, instead of shifts from 20 to 5? As it\u2019s claimed: \u201cHowever, this edge masking strategy creates Out-Of-Distribution (OOD) graph inputs, so it is unclear if a high faithfulness score comes from the fact that the edge is important or from the distribution shift induced by the edge removal (section 1 paragraph 1)\u201d.\n3. There are two removal strategies: \u201chard\u201d and \u201csoft\u201d removal strategies. I wonder is there any difference between them toward the GNN output? If the outputs f(G_e_hard) and f(G_e_soft) are different, what\u2019s the reason for that? \n4. There should be many hyper-parameters to tune for the evaluated explainer methods, eg: size regularization and temperature in GNNExplainer/PGExplainer. How do you set them and have you tuned them to the best? It would be good to include these details in the main text or supplementary and motion them in the main text since this paper emphasizes on the experiments.\n5. In Figure 1, what\u2019s the random seed for the random baseline, and how many times the experiments are repeated? For AUC evaluation, how do you compute the AUC score? Specifically, for other explainers, we could have an edge weight vector as the explanation and compute the AUC with the ground truth. But for a random baseline, how to decide the weight of the edge?\n6. The GInX-Eval is computined via retraining, and finally evaluating the quality of the explanation of the original on the original pretrained GNN model. However, the GNN behavior would change during retraining. For example: GNN model f_a is trained on the complete training dataset, it could predict the classification according to the explanation sub-graph. But GNN model f_a is trained on the training dataset which frop 50% edges in each graph. If the explanation sub-graphs are already dropped, how could f_b predicts the graphs into correct classifications? Would the behavior of the retrained GNN models change and how would it affect the accuracy evaluation? Thus, the experiments are not fully convincing. It would be good to make some clarify.\n\n\nTypos:\n1. In section 2, \u201cSolving the OOD problem\u201d should be \u201cSolving the OOD Problem\u201d.\n2. In section 3, \u201cEdge removal strategies\u201d, \u201cPrior work\u201d should be \u201cEdge Removal Strategies\u201d and \u201cPrior Work\u201d to be consistent with \u201cOut-Of-Distribution Explanations\u201d\n3. In section 4, \u201cExperimental setting\u201d should be \u201cExperimental Setting\u201d.\n4. In the \u201cExperimental setting\u201d section, \u201cWe test two \u2026, because they score high on \u2026\u201d: should it be \u201cbecause their scores are high on\u2026\u201d?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3765/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698430381560,
            "cdate": 1698430381560,
            "tmdate": 1699636332721,
            "mdate": 1699636332721,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EFcNooeR47",
                "forum": "88MalncLgU",
                "replyto": "6Y5IPVRkeY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3765/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3765/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer DCst"
                    },
                    "comment": {
                        "value": "We thank Reviewer DCst for his/her insightful comments.\n\nOne major comment of the reviewer concerns the limited applicability of the method to white-box models. Now we propose a fine-tuning strategy that enables both to overcome the OOD problem and to be generalizable to any black-box models only accessible through API calls. See general comments to all reviewers. \n\nAlthough the methodology is not novel, the whole purpose of this paper is to bring a new evaluation tool to the xAI and GNN community to avoid common mistakes and get meaningful insights into the true informative power of explainability methods as well as validate ground-truth explanations, something that is currently missing in the community!\n\nThis paper doesn\u2019t claim to solve the OOD problem, it shows first the limitation of a popular evaluation metric in xAI that is not robust to the OOD problem as well as the consequences (observation1,2,3) and proposes a novel evaluation strategy that overcomes this.\n\n1. *\u201cThe highlighted explanation subgraph\u2019s distribution is different from the training data.\u201d Why is different and what\u2019s the nature where this difference comes from?*\n\nWe have added Figure 2 to illustrate the OOD problem.\n\n2. *Why did the explanations\u2019 distribution shift to a better side but not a worse side? ... (section 1 paragraph 1)\u201d*.\n\nSince explanations are out-of-distribution, there is no general rule beyond the observed faithfulness scores. The idea here is that the GNN is fooled and therefore has not the capacity to correctly assess the importance of the remaining edges. Figure 2 very well illustrates that explanations are closer to non-toxic molecules and will therefore be misclassified even though they are good explanations for toxic molecules (if we look at the GT or GinX-Eval results).\n\n3. *There are two removal strategies: \u201chard\u201d and \u201csoft\u201d removal strategies. I wonder is there any difference between them toward the GNN output? If the outputs f(G_e_hard) and f(G_e_soft) are different, what\u2019s the reason for that?*\n\nWe refer the reviewer to the Appendix where he will find a more detailed analysis of the differences in the GNN output between hard and soft edge removal strategies. In Appendix A.3  we analyze the differences in the GNN output between hard and soft edge removal and in Appendix C.2 we also provide the results for the soft edge removal strategy and describe the similarities and differences with the results with hard selection. \n\n4. *There should be many hyper-parameters to tune ...*\n\nWe have included the details of hyperparameters for GNNEXplainer and PGExplainer in the Appendix. We fully agree with the reviewer that PGExplainer is extremely sensitive to the choice of hyper-parameters and our final choice is the result of hyper-parameter tuning. \n\n5. *In Figure 1, what\u2019s the random seed for the random baseline...*\n\nFor each experiment (the random baseline included), we have run the experiments on five different seeds (0,1,2,3,4). The random baseline estimates the edge importance sampling from uniform distribution values between 0 and 1. The weight of the edge for the random explanation is therefore a random value between 0 and 1. To compute the AUC score, we simply compare this random edge importance assignment to the ground-truth one. We have added a sentence in section 4.1 to clarify the AUC computation. \u201cThe AUC score is computed between the explanatory weighted edge mask and the ground-truth edge mask with binary values in {0,1}.\u201d\n\n6. *The GInX-Eval is computined via retraining, and finally evaluating the quality of the explanation of the original on the original pretrained GNN model. However, the GNN behavior... clarify.*\n\nAs mentioned in the general comment to all reviewers, we now propose a fine-tuning strategy. When removing t% of the informative edges, f_b(fine-tuned on the reduced train dataset) will not find the important information in the reduced test set - therefore we expect f_b to make wrong predictions and the test accuracy to drop and the GinX score to increase."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3765/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699799910935,
                "cdate": 1699799910935,
                "tmdate": 1699799910935,
                "mdate": 1699799910935,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c7JSEzSQja",
            "forum": "88MalncLgU",
            "replyto": "88MalncLgU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3765/Reviewer_qa24"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3765/Reviewer_qa24"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the evaluation method of explanatory techniques for GNNs. It argues that the faithfulness measure commonly used in the GNN explainability research area suffers from the out-of-distribution (OOD) problem where removing uninformative edges can decrease accuracy because they lead to the OOD. To tackle this problem it proposes GInX-Eval that evaluates explanatory techniques according to the decrease in test accuracy on GNNs retrained by using training data in which the highly-ranked edges are subtracted. It empirically shows that the faithfulness score is inconsistence with accuracy and decreases by removing even the uninformative edges, whereas GInX-Eval does not suffer from removing the uninformative edges. The results based on GInX-Eval indicate that some explanatory techniques like gradient-based methods have not good performance whereas others such as GNNExplainer and D4Explainer can provide good explanations of GNN predictions, which are consistent with the results of previous works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, this paper is well-organized and clearly written. This paper clearly proves the problem of the faithfulness measure widely used in the GNN explainability research community by using carefully designed experiments. The proposed measure, GInX-Eval, can overcome the OOD problem from which the faithfulness measure suffers, by observing the test accuracy on GNNs retrained by using the training data. The evaluation based on GInX-Eval is consistent with the results of the previous works."
                },
                "weaknesses": {
                    "value": "Though GInX-Eval is designed so that it can be applied to graph data and it provides good contributions to the graph learning research area, the idea of evaluating explanatory techniques by retraining the prediction methods has already been proposed in previous works such as Hooker et al (2018).\n\nAdditionally, there are several drawbacks to readability:\n- In 3.3.1 GINX SCORE, the description of \"top-k edges\" is confusing because t is already used as the fraction of the ordered edge set.\n- In equation 3, the superscript for G\\G_e^t is used without explanation despite the superscript is not used in equation 2.\n- It is very hard for readers to distinguish different colors used in Figures. Some efforts are required for readability such as using different marks.\n- Several references such as Faber et al, Hooker et al, Hsieh et al, and Hu et al lack names of conferences or years of publishing."
                },
                "questions": {
                    "value": "What is the difficulty of applying the idea of retraining to the evaluation of explanatory techniques for GNNs compared to those for CNNs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3765/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698481955698,
            "cdate": 1698481955698,
            "tmdate": 1699636332641,
            "mdate": 1699636332641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z4qiXUVsck",
                "forum": "88MalncLgU",
                "replyto": "c7JSEzSQja",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3765/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3765/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer qa24"
                    },
                    "comment": {
                        "value": "We thank Reviewer qa24 for his/her insightful comments.\n\n*What is the difficulty of applying the idea of retraining to the evaluation of explanatory techniques for GNNs compared to those for CNNs?*\n\nThanks for the question. Theoretically, GInX-Eval should be easily transferable to CNN models as well. However, the difficulty with CNNs comes from the nature of the inputs themselves, i.e. images. The challenges arise when removing pixels from images. The removal strategy of pixels when using GInX-Eval with CNNs has to be well justified. Two main challenges should be addressed: \n\n- As mentioned by Hsieh et al. [1], assigning features with the baseline value might lead to a bias toward features that are far from the baseline. For instance, with images the baseline is black color. If we set the pixels to black in RGB images, this introduces a bias favoring bright pixels. Therefore, CNN fine-tuned on those modified images will often omit important dark objects.\n- In addition, removing pixels by setting their value to zero (black pixel) might lead to the CNN confusion between no signal composing the black background and pertinent negative pixels, which are relevant values that are dismissed [2]. \n\nTherefore, applying GInX-Eval for CNN models requires defining an appropriate pixel removal strategy to overcome those two difficulties.\n\n[1] Hsieh, Cheng-Yu, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Ravikumar, Seungyeon Kim, Sanjiv Kumar, and Cho-Jui Hsieh. n.d. \u201cEvaluations and Methods for Explanation through Robustness Analysis.\u201d https://openreview.net/pdf?id=4dXmpCDGNp7.\n\n[2] Dhurandhar, Amit, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, and Payel Das. 2018. \u201cExplanations Based on the Missing: Towards Contrastive Explanations with Pertinent Negatives.\u201d arXiv [cs.AI]. arXiv. https://proceedings.neurips.cc/paper_files/paper/2018/file/c5ff2543b53f4cc0ad3819a36752467b-Paper.pdf.\n\n\nWe have addressed the concerns of the reviewer in the updated version of the paper. As mentioned in the general comment for all reviewers, we have modified section 3 with more consistent mathematical notations and this also modified Equation 3. In 3.3.1, we have corrected the \u201ctop-k edges\u201d and clarified that t is the fraction/ratio of edges. We have also completed the incomplete references when papers were published at conferences and added the year for all of them. Concerning the colors, we have chosen colorings to reflect the similarity of methods (green colors for gradient-based methods, warm colors for non-genative ones, and cold colors for generative ones).\n\nAs mentioned in the general comment to all reviewers, we now proposed a fine-tuning strategy rather than re-training from scratch the GNN model. Therefore, the GInX-Eval method can now be applied to any black-box models, including language models and CNNs."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3765/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699799589628,
                "cdate": 1699799589628,
                "tmdate": 1699799589628,
                "mdate": 1699799589628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0VsFqfzwou",
                "forum": "88MalncLgU",
                "replyto": "z4qiXUVsck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3765/Reviewer_qa24"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3765/Reviewer_qa24"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed comments. I understood that there are different difficulties between the evaluation of the CNN explanation and that of the GNN explanation. Even taking into consideration, the strengths and weaknesses of this paper have not changed so much for me, thus I keep the rating."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3765/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287715522,
                "cdate": 1700287715522,
                "tmdate": 1700287715522,
                "mdate": 1700287715522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1KTGEriV2g",
            "forum": "88MalncLgU",
            "replyto": "88MalncLgU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3765/Reviewer_EeHX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3765/Reviewer_EeHX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new evaluation procedure for graph neural network (GNN) explanations called GInX-Eval. The authors argue that current evaluation metrics have limitations, particularly in evaluating out-of-distribution explanations. GInX-Eval addresses this issue by measuring the informativeness of removed edges and the correctness of explanatory edge ordering. The authors also introduce a new dataset for evaluating GNN explanations and demonstrate the effectiveness of GInX-Eval through experiments on this dataset. Overall, the paper's contributions include a new evaluation metric for GNN explanations, a new dataset for evaluation, and experimental results demonstrating the effectiveness of GInX-Eval."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Proposes a novel evaluation metric, GInX-Eval, that measures the informativeness of removed edges and the correctness of explanatory edge ordering.\n- Addresses an important issue in current evaluation metrics, namely the problem of out-of-distribution explanations.\n- Clear and well-organized writing that makes it easy to follow the authors' arguments and contributions."
                },
                "weaknesses": {
                    "value": "1. Certain aspects of the design are not intuitively clear. Specifically, the rationale behind Equation 4 is not well-explained. Elaborating on the underlying intuition would aid in understanding its relevance and function within the model.\n2. The terms \"hard selection\" and \"soft selection\" are used without formal definitions. Providing precise mathematical formulas for these concepts would clarify their meaning and implementation in the context of the proposed method.\n3. A major concern with GINX-EVAL is that it necessitates the re-training of the evaluated model. This process alters the original model, potentially leading to explanations that do not accurately reflect the model's decision-making process in its original state.\n4. The utility of edge ranking as a metric is questionable. It assumes that the importance of individual edges correlates directly with subgraph importance, an assumption that may not hold true in all cases. Further justification or alternative metrics should be considered.\n5. The range of GNN backbones tested is somewhat limited. Incorporating more diverse architectures, such as GCN, would provide a more comprehensive evaluation of the proposed method's effectiveness across different models.\n\nIn summary, while the paper introduces an intriguing approach for GNN evaluation, there are several areas where clarity and methodological rigor could be improved. Addressing these concerns would significantly enhance the paper's contribution and applicability."
                },
                "questions": {
                    "value": "In weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3765/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3765/Reviewer_EeHX",
                        "ICLR.cc/2024/Conference/Submission3765/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3765/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846183472,
            "cdate": 1698846183472,
            "tmdate": 1700646949053,
            "mdate": 1700646949053,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I4v2Hb4iI4",
                "forum": "88MalncLgU",
                "replyto": "1KTGEriV2g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3765/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3765/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Reviewer EeHX"
                    },
                    "comment": {
                        "value": "We thank Reviewer EeHX for his/her insightful comments and address them one by one.\n\n1. *Certain aspects of the design are not intuitively clear. Specifically, the rationale behind Equation 4 is not well-explained. Elaborating on the underlying intuition would aid in understanding its relevance and function within the model.*\n\nAs explained in comment 5. below, we have re-formulated the EdgeRank score to the HomophilicRank score and elaborated more on the meaning of this score in the new version of the paper.\n\n2. *The terms \"hard selection\" and \"soft selection\" are used without formal definitions. Providing precise mathematical formulas for these concepts would clarify their meaning and implementation in the context of the proposed method.*\n\nWe have added a mathematical definition of the hard and soft edge selection functions using the notations of the paper to bring more clarity to the paper.\n\n3. *A major concern with GINX-EVAL is that it necessitates the re-training of the evaluated model. This process alters the original model, potentially leading to explanations that do not accurately reflect the model's decision-making process in its original state.*\n\nWe have addressed this major issue by proposing instead a fine-tuning strategy that enables both to overcome the OOD problem and to have an evaluation method that can be widely used for black-box models.\n\n4. *The utility of edge ranking as a metric is questionable. It assumes that the importance of individual edges correlates directly with subgraph importance, an assumption that may not hold true in all cases. Further justification or alternative metrics should be considered.*\n\nThe EdgeRank score does not evaluate the importance of individual edges but the importance of the first fraction $t\\in[0.1,0.2,\u20260.9]$ of edges. We agree with the reviewer that the 10% most important edges might be correlated with the 10-20% edges, leading to an increase in the GInX score only after removing the full 20% edges. However, observing the size of ground-truth explanations for the 6 datasets (between 10 and 30%), we expect the sharpest drop in test accuracy in the first 3 removal steps (t=0.1, t=0.2, t=0.3). Therefore assigning more importance to these first steps makes sense. Now, it is true that within the 3 first steps, important edges might be correlated. \n\nWe follow the reviewer\u2019s suggestion and re-formulate the EdgeRank score to the HomophilicRank score. The HomophilicRank score measures the capacity of a method to rank edges by their correct importance ordering while assigning similar importance weights to correlated edges. This score penalizes methods that for instance only discover a subset of important edges and do not account for the edges correlated to those. The HomophilicRank score favors methods that treat correlated edges on equal footing. It measures the capacity to uniformly assign importance to redundant information rather than only putting importance to a single representative edge and none to the correlated edges.\n\n5. *The range of GNN backbones tested is somewhat limited. Incorporating more diverse architectures, such as GCN, would provide a more comprehensive evaluation of the proposed method's effectiveness across different models.*\n\nWe followed the reviewer\u2019s suggestion and added the GCN model. Since the GCN does not take edge features into account we only run GInX-Eval with GCN on the four datasets: BA-2Motifs, BA-HouseGrid, MUTAG, and MNIST_BIN where edge features could be removed without or with small loss of information."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3765/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699799355662,
                "cdate": 1699799355662,
                "tmdate": 1699799355662,
                "mdate": 1699799355662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7esH8uaQ3s",
                "forum": "88MalncLgU",
                "replyto": "I4v2Hb4iI4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3765/Reviewer_EeHX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3765/Reviewer_EeHX"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your feedback. After considering the revised strengths and weaknesses of this paper, I acknowledge its improvements but believe it is not yet ready for publication. Consequently, I have increased its score to 5."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3765/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646961429,
                "cdate": 1700646961429,
                "tmdate": 1700646961429,
                "mdate": 1700646961429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]