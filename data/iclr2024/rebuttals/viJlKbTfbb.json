[
    {
        "title": "What If You Were Not There? Learning Causally-Aware Representations of Multi-Agent Interactions"
    },
    {
        "review": {
            "id": "rgNP7H4sMZ",
            "forum": "viJlKbTfbb",
            "replyto": "viJlKbTfbb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4815/Reviewer_FV83"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4815/Reviewer_FV83"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the extent to which modern representations can capture the causal relationships in agent interactions. It challenges the concept of causal robustness proposed by a recent work. With a new diagnose dataset, this paper shows that recent representations are partially resilient to non-causal agent perturbations but struggle with modeling indirect causal effects involving mediator agents. The study introduces two simple regularization approaches using causal annotations of varying granularity, which enhances causal awareness and out-of-distribution robustness. Additionally, they tested the effectiveness of the proposed method in a sim-to-real causal transfer scenario. This work aims to shed light on the challenges and opportunities in learning causally-aware representations in multi-agent scenarios and takes a step toward practical solutions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The definitions for social causality and the following computational methods are clearly defined and well motived. The overall logic flow is smooth and is a pleasure to read.\n(2) The experiments evaluations are thorough.\n(3) The writing is clear and helps the understanding of the motivation, desiderata, solution."
                },
                "weaknesses": {
                    "value": "(1) The proposed method is highly confined to the simulator. As the author already noted, counterfactual data is impractical to collect in real world scenario and they instead took a sim-to-real transfer learning approach. But the assumption that the underlying causal mechanisms stay the same is simply too fragile to be true. For example, in simulation, the author define the visibility range as 210 degree in front of the driver ignoring the fact that drivers also check rearview mirrors for road information. And the simulator itself focuses heavily on collision avoidance. However, human drivers are not perfectly rational and have various driving preferences. Instead of \u201ccausally-aware\u201d representations, it\u2019s more accurate to say the proposed method learns \u201ccollision-aware representations\u201d. Rather than assuming \u201cthe causal mechanisms\u201d satisfying some vague constraints, I would recommend the authors model the problem more rigorously with causal theories [1,2]. \n\n(2) Even if we only consider the collision-free driving style, the proposed method\u2019s effectiveness is not convincing enough. Firstly, if you have done multiple runs with different random seeds for the same experiment, I am expecting to see confidence intervals or error bars in the results. Secondly, the author introduced three metrics for evaluation, Average Displacement Error (ADE), FinalDisplacementError(FDE), and Average Causal Error (ACE). However, figure 6 is missing FDE and figure 8 is missing ACE. Thirdly, for the data reported, the contrastive method seems to be on par with the \u201caugment\u201d. When in sim-to-real, it\u2019s sometimes even worse than \u201caugment\u201d and the improvement is rather marginal. We do see notable improvements of the \u201crank\u201d method over baselines but I am not sure if it\u2019s a fair comparison because the \u201crank\u201d method actually provides way more labeling information than binary labels (causal/non-causal). Does those baselines also have access to such labels? At last, when using 100% percent data in the sim-to-real test, the proposed method (contrast) barely beats those baselines leaving doubts on whether the performance gaps in 25% and 50% scenarios are due to learning efficiency difference instead of representation quality.\n\n[1] Bareinboim, Elias, and Judea Pearl. \"Causal inference and the data-fusion problem.\" Proceedings of the National Academy of Sciences 113.27 (2016): 7345-7352.  \n[2] Hern\u00e1n, Miguel A., and Tyler J. VanderWeele. \"Compound treatments and transportability of causal inference.\" Epidemiology (Cambridge, Mass.) 22.3 (2011): 368."
                },
                "questions": {
                    "value": "(1) Could you extend the experiments to multiple random seeds and report the variances in the figures?\n(2) Could you report all three metrics for figure 6 and 8?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4815/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4815/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4815/Reviewer_FV83"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719973683,
            "cdate": 1698719973683,
            "tmdate": 1699636464918,
            "mdate": 1699636464918,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7YLxaPZg9D",
                "forum": "viJlKbTfbb",
                "replyto": "rgNP7H4sMZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4815/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer FV83,\n\nThank you for your insightful feedback. Please find our response to your comments below.\n\n> The assumption that the underlying causal mechanisms stay the same between the simulation and real-world is simply too fragile to be true. For example, ignoring the fact that drivers also check rearview mirrors.\n\n* We acknowledge your concern and clarify that we do not assume all causal mechanisms stay the same. Instead, we assume they differ between the simulation and real-world in a sparse way (cf. sparse mechanism shift [1,2]). For the mechanisms shared across the two domains, we can first annotate them in the simulation and further transfer them to the real-world.\n* This assumption aligns well with our considered setting, where the simulator is designed and calibrated to mimic pedestrian behaviors (not drivers) in the real-world, including a 210-degree field of view to match human visual perception [3].\n* To the best of our knowledge, existing research in causal representation learning is still confined to synthetic/simulated settings, and struggles to scale to the real-world. Our proposed sim-to-real causal transfer provides a new perspective to bridge this gap and may hold great promise in broader settings beyond the considered application.\n\n> Could you report all three metrics for figure 6 and 8?\n* We\u2019d like to clarify that it is not possible to evaluate ACE for Figure 8, due to the absence of causal annotations in the real-world ETH-UCY dataset.\n* We have added FDE for Figure 6 into the appendix (Figure 9). We didn\u2019t report it in the previous version, because the pattern of FDE is similar to that of ADE and does not provide much additional information. \n\n> We do see notable improvements of the \u201crank\u201d method over baselines but I am not sure if it\u2019s a fair comparison because the \u201crank\u201d method actually provides way more labeling information than binary labels (causal/non-causal).\n* We\u2019d like to clarify that the goal of the experiment is not about which method is better given the same annotations. Instead, we aim to examine the relative benefits of fine-grained annotations over coarse-grained labels, as noted at the beginning of the experiment section (Q3). \n* Our result shows that learning from fine-grained annotations through a ranking regularizer yields substantial improvements over the other counterparts. This result provides valuable guidance for causal annotation strategies in simulation and real-world scenarios.\n\n> At last, when using 100% percent data in the sim-to-real test, the proposed method (contrast) barely beats those baselines leaving doubts on whether the performance gaps in 25% and 50% scenarios are due to learning efficiency difference instead of representation quality.\n* We appreciate your observation regarding the performance gaps in different data scenarios. We\u2019d like to clarify that we do not claim superiority of our contrastive variant over the augment baseline; their performance is comparable across most experiments. The main finding we\u2019d like to highlight is the strength of our ranking-based variant, including its high learning efficiency in the low data regime, e.g., reaching similar or even better accuracies with 1/2 less of the real-world data.\n\n> Could you extend the experiments to multiple random seeds and report the variances in the figures? \n\nThank you for the suggestion. \n* We have updated Figure 6 in our manuscript to report the variance across 5 random seeds. \n* We are still running the experiments on different seeds for Figure 8. Below are the preliminary results on the full (100%) ETH-UCY dataset. We anticipate completing the experiments by next week and will update Figure 8 accordingly.\n\n|             | ETH                       | Hotel                     | Univ                      | Zara1                     | Zara2                     |\n| ----------- | ------------------------- | ------------------------- | ------------------------- | ------------------------- | ------------------------- |\n| Baseline    | 0.932\u00b10.02 / 1.870\u00b10.032  | 0.322\u00b10.008 / 0.646\u00b10.026 | 0.540\u00b10.005 / 1.154\u00b10.006 | 0.420\u00b10.002 / 0.909\u00b10.006 | 0.344\u00b10.004 / 0.766\u00b10.004 |\n| Ranking | 0.907\u00b10.007 / 1.825\u00b10.014 | 0.308\u00b10.006 / 0.581\u00b10.021 | 0.534\u00b10.001 / 1.143\u00b10.001 | 0.410\u00b10.002 / 0.893\u00b10.003 | 0.327\u00b10.001 / 0.718\u00b10.005 |\n\n\n[1] Towards Causal Representation Learning, 2021 \\\n[2] From Statistical to Causal Learning, 2022 \\\n[3] Seven Myths on Crowding and Peripheral Vision, 2020"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307466625,
                "cdate": 1700307466625,
                "tmdate": 1700489239816,
                "mdate": 1700489239816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ujFq3Dg0dd",
                "forum": "viJlKbTfbb",
                "replyto": "rgNP7H4sMZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4815/Reviewer_FV83"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4815/Reviewer_FV83"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response. Most of my concerns have been clarified. I am looking forward to more experiment results about Fig. 8. However, I would like to pose three follow up questions based on your response. \n\n1. From the current table, it seems that \"ranking\" is still not completely beating the baselines, which casts doubts on \"the strength of our ranking-based variant\". Is this a contradiction to your claim?\n\n2. Though the references the authors provided are not formatted, I managed to find the papers, which I believe should be the right ones. It seems that the sparse mechanism shift is defined under the Causal Markov Condition, which basically means that there will be no unobserved confounders. I am not sure if it is realistic to model the pedestrian behaviors assuming that there are no spurious correlations among them. And this question could have been better clarified if the author had defined the type of causal model they were using exactly with the formal causal language [1]. \n\n3. If the dataset is all about pedestrian behaviors, Fig 2 seems to be misleading since in that figure the ego agent is a car instead of a pedestrian."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603653075,
                "cdate": 1700603653075,
                "tmdate": 1700603754247,
                "mdate": 1700603754247,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nat3Q46Mr7",
                "forum": "viJlKbTfbb",
                "replyto": "rgNP7H4sMZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4815/Reviewer_FV83"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4815/Reviewer_FV83"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding my point 2 previously, modeling the causal effect rigorously should be the foundation of the problem instead of a \u201cfascinating future work\u201d. And if you check the references you provided, they clearly stated in the context that they described the sparse mechanism shift under the markovian causal model, where there is no confounders.  Citing another vague statement does not help in this case since there are already established causal literatures solving this exactly. You may refer to the line of transportability in causal inference for more details. \n\nRegarding my point 3, as the authors stated in their previous response, the simulator they used \u201cmodels the behavior of pedestrians\u201d not \u201cdrivers\u201d. If your goal is to \u201cexplore casually aware representations in a broad multi agent  setting\u201d, you should include more general simulators instead of a pedestrian simulator and should not assuming that there are no confounders between the agents behavior. Again, this confusion is also partially due to the problem that the authors failed to model the problem using precise causal language properly.\n\nThus, I\u2019ll keep the score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734566592,
                "cdate": 1700734566592,
                "tmdate": 1700734595440,
                "mdate": 1700734595440,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TNCRY16Bxs",
                "forum": "viJlKbTfbb",
                "replyto": "rgNP7H4sMZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4815/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer FV83,\n\nWe would like to reiterate the three main contributions of our submission.\n1. Reveal the annotation and evaluation issues in the recent large-scale CausalAgent benchmark\n2. Introduce a causal metric learning approach that boosts causal awareness and out-of-distribution robustness in controlled simulations\n3. Propose a sim-to-real causal transfer technique that yields encouraging results without relying on real-world annotations and thus holds great promise for practical problems\n\nWe acknowledge the limitations of our study, as you rightly pointed out, and have explicitly discussed some of them in the `Limitations` and `Additional Discussions` paragraphs in our manuscript.\n\nGiven these contributions and discussions, we respectfully disagree with your conclusion that our submission should be rejected, and we believe that it fills critical knowledge gaps in the field.\n\nFinally, we would like to express our sincere gratitude for the time and effort that you have invested in reviewing our work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738277415,
                "cdate": 1700738277415,
                "tmdate": 1700740893322,
                "mdate": 1700740893322,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NFHRwWHr0p",
            "forum": "viJlKbTfbb",
            "replyto": "viJlKbTfbb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4815/Reviewer_1Bm7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4815/Reviewer_1Bm7"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the problem of learning causally-aware representations in the context of multi-agent interactions. The authors first raise concern about the recently proposed social causality benchmark, pointing out the flaws in the annotation process and evaluation protocol. As a result, the authors introduce a diagnostic dataset based on ORCA, with fine-grained causal annotations where each agent is labeled as non-causal, direct causal, or indirect causal, and propose a new evaluation metric, namely average causal error (ACE). The paper then proposes two strategies exploiting fine-grained causal annotations: regularization using contrastive loss or ranking loss. For the real-world scenarios where obtaining counterfactual samples is impossible, they introduce sim-to-real causal transfer strategy where the model is jointly trained on the task in hand and its simulation counterparts. The experiments demonstrate the effectiveness of the proposed regularization strategy using fine-grained causal annotations and causal transfer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is easy to follow and well-written. The problem of learning causally-aware representations is also significant.\n- The paper raises concerns about the CausalAgents benchmark, i.e., the annotation process and evaluation protocol. These points seem reasonable to me and serve as a new perspective at minimum. It could also bring further development in the field (e.g., new benchmarks and evaluation metrics).\n- Leveraging fine-grained annotations to further improve the robustness of the representations is convincing. The proposed two instantiations are intuitive and reasonable.\n- The motivation and idea of sim-to-real causal transfer are convincing and interesting. It is simple and works well (at least in the ETH-UCY dataset)."
                },
                "weaknesses": {
                    "value": "- In the annotation process of the proposed dataset, each $i$-th agent is labeled as non-causal, direct causal, or indirect causal based on the influence the ego agent $\\mathcal{E}_i$, i.e., measured by removing a *single agent*. However, the causal effect of the agent (to the ego agent) could be different based on the other agent (as Fig 2 illustrates).\n    - For example, for indirect causal agents  $i, j$ (i.e., $\\mathcal{E}_i, \\mathcal{E}_j \\gg 0$), it is possible that removing both of them does not influence the ego agent, i.e., $\\mathcal{E}\\_{ij} \\simeq 0$.\n    - For another example, it is also possible that $\\mathcal{E}\\_{ij} \\gg 0$ when $\\mathcal{E}_i, \\mathcal{E}_j \\simeq 0$ (e.g., when there are two bikes in the same lane in Fig 2, removing only one of them does not influence the ego agent but it does when removing them together).\n- Similarly, the proposed evaluation metric ACE also considers removing only a single agent. It is not clear **why and how this metric serves as a reliable indicator** of the causal awareness and robustness of a learned representation. The authors argued that Eq. 3 using $\\mathcal{E}_\\mathcal{R}$ *overestimates* the robustness issue as described in Caveats in page 4 and Fig 3. However, one could also view that the proposed metric **ACE may *underestimate* the robustness issue**. This is related to \u201cTakeaway 1\u201d where the authors claim that recent methods are already partially robust w.r.t. non-causal agent removal. But this only considers a single agent removal and therefore may underestimate the robustness issue. It would be appreciated if the authors could provide justification for their annotation process and evaluation metric which is based on the influence measured by removing *a single agent*.\n- The paper only considers a single dataset (ETH-UCY), and the proposed regularization strategy is only applied to a single backbone (AutoBots), which makes it difficult to assess its wide applicability. In other words, it is unclear how the proposed regularization strategy would work on other datasets and with other backbone methods.\n- The paper lacks a detailed description of the proposed diagnostic dataset based on ORCA (in both the main body and the appendix). For example, what is the difference between OOD-Context and OOD-Density-Context? (They seem to be both dense.) Maybe a figure or illustration would be much appreciated. Also, as the one who is not directly working in this field, it is hard for me to understand the similarity and difference between ORCA simulation and ETH-UCY dataset, and thus it is difficult to assess how reasonable the proposed causal transfer strategy is.\n\n**Justification for the rating**\n\nDespite several concerns I listed above, my initial assessment weighed more on its positive (potential) values. I look forward to the feedback from the authors and discussions with other reviewers."
                },
                "questions": {
                    "value": "My major concerns and questions are listed above. Some minor comments:\n\n- Fig 2 is not color friendly (it is hard to distinguish cyan and blue).\n- The paper sometimes interchangeably uses \u201ccausal representation\u201d and \u201ccausally-aware representation\u201d. They have different meanings in the literature and I think the latter fits the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4815/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4815/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4815/Reviewer_1Bm7"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821292027,
            "cdate": 1698821292027,
            "tmdate": 1699636464816,
            "mdate": 1699636464816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LqHmHglZqf",
                "forum": "viJlKbTfbb",
                "replyto": "NFHRwWHr0p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4815/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 1Bm7,\n\nThank you for your insightful feedback. Please find our response to your comments below.\n\n> It would be appreciated if the authors could provide justification for their annotation process and evaluation metric which is based on the influence measured by removing a single agent.\n\n* Indeed, the causal effect in the multi-agent context can be studied at both the individual agent level (removing one agent at a time) and the group level (removing a group of multiple agents). Our focus on agent-level causal effects was motivated by two factors:\n  * It aligns with the notion of Causal Agents, i.e., a neighboring agent has a certain causal relationship with the ego agent. This allows us to revisit the root cause of the robustness issues in the prior benchmark.\n  * It permits efficient experiments, i.e., studying causal effects at the agent level presents a linear computational complexity O(N), whereas studying that at the group level could lead to an exponential increase in complexity, up to O(2^N).\n* Having said that, we acknowledge that exploring the latter can be an exciting avenue for future research. We have updated our manuscript to reflect this perspective, framing our finding about `partially robust` in the context of agent-level causal effects and discussing its limitation in Appendix C.\n\n> How the proposed regularization strategy would work on other datasets and with other backbone methods.\n\n* We are currently running experiments on another backbone (D-LSTM) benchmarked in Tab 1. Our preliminary result shows larger performance gains on D-LSTM than on AutoBots. We anticipate completing the experiments soon and will update our response accordingly.\n\n> Lacks a detailed description of the proposed diagnostic dataset. For example, what is the difference between OOD-Context and OOD-Density-Context? Maybe a figure or illustration would be much appreciated. \n\n* The key difference between OOD-Context and OOD-Density-Context is that the former introduces major changes in scene contexts, whereas the latter introduces more significant increases in the number of causal agents as well as modest changes in scene contexts.\n* As per your suggestion, we have added a more detailed description in the `dataset details` paragraph in Appendix B, along with animations in [our public repository](https://github.com/socialcausality/socialcausality#illustrations) for better illustration.\n\n> Fig 2 is not color friendly (it is hard to distinguish cyan and blue).\n\n* Thank you for bringing this to our attention. We have updated the figure, changing blue to red.\n\n> The paper sometimes interchangeably uses \u201ccausal representation\u201d and \u201ccausally-aware representation\u201d. I think the latter fits the paper.\n\n* Thank you for the suggestion. We have revised the manuscript, using the term \u201ccausally-aware representation\u201d more uniformly across the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560115498,
                "cdate": 1700560115498,
                "tmdate": 1700560115498,
                "mdate": 1700560115498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p4rrVWP92l",
            "forum": "viJlKbTfbb",
            "replyto": "viJlKbTfbb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4815/Reviewer_d8CZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4815/Reviewer_d8CZ"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on the problem of causal representations in multi-agent forecasting problems.\nAfter assessing the CausalAgents benchmark and its shortcomings, the authors employ the ORCA simulator to generate counterfactual scenes which results in a diagnostic dataset.\nThis dataset includes annotations of individual agents' causal effects on the ego agent through the euclidean distance of its trajectory in both scenes.\nTwo methods for promoting causal awareness are introduced, the basis of which is that latent representations of the ego agent should be invariant to the removal or addition of non-causal agents.\nSince this approach heavily relies on the causal annotations, its application to real-world datasets is non-trivial.\nTo bridge this gap, this work proposes a sim-to-real causal transfer approach where data with annotated causal effects is used in conjunction with real human trajectory data to train a motion forecasting model.\nResults are presented on the diagnostic dataset to evaluate the causal awareness of current motion prediction models, and how the proposed approaches affect the causal performance of such models.\nFurther results are presented to show how the proposed approaches can help current prediction models on real-world datasets using the proposed sim-to-real training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The literature review is complete and addresses the relevant categories for this work. These include multi-agent interactions, robust representations and causal learning. \n\n- The motivation for this work, presented in Section 3.1 and 3.2, is informative and provides the background for the subsequent sections. I particularly liked section 3.2 which outlined the shortcomings of the CausalAgents Benchmark.\n\n- I thought the experiment showing how promoting causal invariance leads to better prediction accuracy with low amounts of real-world data interesting. This is possibly undersold in the paper, but an interesting result."
                },
                "weaknesses": {
                    "value": "- The main issue I have is that the proposed approach seems to provide marginal improvements across all metrics. \nPerhaps this is down to the point made by the authors, that all methods are already pretty strong for non-causal agents, but not so much for causal agents. \nIn Figure 6, the within quantile difference seems quite small. \nI think it would be much more interesting if the ACE metrics in Table 1 were replicated with the proposed approaches, showing how these values (particularly ACE-DC/IC) are presumably improved with the promotion of causal invariance. Why is the split done using quantiles here instead of spliting across the ACE-X metrics?"
                },
                "questions": {
                    "value": "- Figure 4 is not referenced anywhere.\n\n- in Figure 8, what is the difference between baseline and vanilla? \n\n- In relation to the previous point, I think all Figure captions should include a legend telling the reader the different variants. For examples, Figure 6 has both a baseline and augment. But the text says that data augmentation is a baseline. This feels a little ambiguous. If I understood correctly, augment is the data augmentation baseline, and baseline is actually vanilla Autobots. Is this correct?\n\n- I wonder if it would be interesting to also look at the number(or percentage) of ego-collisions in the sim-to-real experiments. It could give a better picture on the importance of promoting causal representations.\n\n- What exactly is the unseen context variant in the OOD experiments? This and high density should be clearly defined."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4815/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699139035276,
            "cdate": 1699139035276,
            "tmdate": 1699636464743,
            "mdate": 1699636464743,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "67ABx2SXkA",
                "forum": "viJlKbTfbb",
                "replyto": "p4rrVWP92l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4815/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4815/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer d8CZ,\n\nThank you for your insightful feedback. Please find our response to your comments below.\n\n> The proposed approach seems to provide marginal improvements across all metrics\n\n* We would like to highlight that our method was evaluated on a state-of-the-art baseline, where significant leaps in performance are inherently challenging. Despite this, our ranking-based method yields ~10% performance gain on several metrics, e.g., ACE in the low-quantile causal effect scenarios and ADE in the OOD density and context settings. Most notably, the final version of our method enables the model to reach better accuracy while requiring only half the real-world data.\n\n> It would be much more interesting if the ACE metrics in Table 1 were replicated with the proposed approaches, showing how these values (particularly ACE-DC/IC) are presumably improved. Why is the split done using quantiles here instead of spliting across the ACE-X metrics?\n\n* The quantile split was chosen to better understand the strengths and limits of the proposed method in causal effect estimates. As shown in Fig 6a, the improvement over ACE is greater with smaller ground-truth causal effects. We conjecture this is in large part due to the imperfect decoding process.\n  * When the causal effect of the neighboring agent is small (affects the ground-truth trajectory only locally), encoding the paired scenes into nearby embeddings results in similar prediction outputs and hence small ACE, canceling out the influence of the decoding error. \n  * In contrast, when the causal effect of the neighboring agent is large (affects the ground-truth trajectory at macro scale), even if encoding the paired scenes into properly separated embeddings, the decoding errors of each trajectory may compound and lead to large ACE. \n* Following your suggestion, we have extended our evaluation to include ACE-DC and ACE-IC metrics. The results are summarized in Figure 10 in our updated manuscript. Our proposed method yields comparable improvements on the two metrics. We conjecture this is because our proposed method treats DC and IC agents uniformly.\n\n> Figure 4 is not referenced anywhere.\n\n* Thank you for bringing it to our attention! In our updated manuscript, we have added reference to Fig 4 in the paragraph right after Eq 5.\n\n> In Figure 8, what is the difference between baseline and vanilla?\n\n* The baseline refers to training the model on real-world data only, whereas the vanilla sim-to-real refers to training the model on simulated and real-world data jointly. In our updated manuscript, we elaborate on the details of each method in the `baseline details` paragraph in Appendix B.\n\n> If I understood correctly, augment is the data augmentation baseline, and baseline is actually vanilla Autobots. Is this correct?\n\n* Yes, the baseline refers to the original Autobots throughout our manuscript, whereas the augment refers to the causal data augmentation technique in Roelofs el al. 2022. We have updated figure captions accordingly in our updated manuscript.\n\n> I wonder if it would be interesting to also look at the number(or percentage) of ego-collisions in the sim-to-real experiments.\n\n* Thank you for the suggestion. Our method does not significantly reduce collision rates compared to the baseline. This is because the AutoBots baseline already exhibits exceptionally low collision rates on the ETH-UCY dataset (0.5%), leaving marginal room for improvements.\n\n> What exactly is the unseen context variant in the OOD experiments?\n\n* The training context comprises a simulated open area with randomized agent directions, whereas the OOD context features a simulated street with constrained agent movements. Please refer to more detailed description in the `dataset details` paragraph in Appendix B, along with animations in [our public repository](https://github.com/socialcausality/socialcausality#illustrations) for illustration."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4815/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644219014,
                "cdate": 1700644219014,
                "tmdate": 1700644219014,
                "mdate": 1700644219014,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]