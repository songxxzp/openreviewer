[
    {
        "title": "Simplifying Referred Visual Search with Conditional Contrastive Learning"
    },
    {
        "review": {
            "id": "2JPQSLrBnX",
            "forum": "Z8uFGTNXIF",
            "replyto": "Z8uFGTNXIF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7672/Reviewer_sg3G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7672/Reviewer_sg3G"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new task called referred visual search. A new dataset is also created to achieve this task. This paper uses contrastive learning for extracting referred embeddings. Experiments achieve promising results in different tasks in LRVS-F dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes a challenging task for image similarity search in the context of fashion. A new dataset is also proposed at the same time.\n2. Conditional embedding is properly used to achieve this task. Experiments demonstrate the effectiveness of the method.\n3. This paper can be a baseline to do more relevant work."
                },
                "weaknesses": {
                    "value": "1.This task is similar to composed image retrieval. Composed image retrieval aims to find the target image based on the reference image and text description. I have some doubts about the contribution and meaning of the task. \n2.The structure of the model is simple. It lacks innovation. The description of the model is not specific enough. Contrastive learning is often used in the task of composed image retrieval, so it is not an innovative method.\n3.Experiments are basically a comparison with other models, but the ablation experiment of your own model and visualization is lacking."
                },
                "questions": {
                    "value": "1. You say it extracts referred embedding using weakly-supervised training. Why it is a weakly-supervised training?\n2. In fig2, it shares weight between the two vision transformers. In this model, it is whether all parameters are shared or only parts of the parameters are shared. And I want to know why to share the weight.\n3. In table1 and table2, you say you report bootstrapped means and standards deviations for 0K distractors, but I don\u2019t see the result of the 0K distractors. In addition, your model is similar to some models that used in the task of composed image retrieval. FashionIQ is also a dataset about clothes. Do you try this dataset using your method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759853840,
            "cdate": 1698759853840,
            "tmdate": 1699636933365,
            "mdate": 1699636933365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gz88KRTT3B",
                "forum": "Z8uFGTNXIF",
                "replyto": "2JPQSLrBnX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7672/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and address their main concerns below :\n\n> **W1.** This task is similar to composed image retrieval. Composed image retrieval aims to find the target image based on the reference image and text description. I have some doubts about the contribution and meaning of the task. \n\nOur RVS task is different from Composed Image Retrieval in that in RVS the object in the query image and in the target image are exactly the same. The use case is retrieving that object from a catalog, given a picture that contains many other different objects. The goal of the model is thus to be able to extract the correct information from the query image. In contrast, in CIR, the target object is not the same as any of the objects present in the query image. The goal of a CIR model is to modify the query image such that it hallucinates an object that is similar to what is in the catalog. Both tasks have relevant applications, but they are sufficiently different to be addressed with different methods. Indeed, we show that models made for CIR perform poorly in RVS, which is intuitive: they are not designed to extract the relevant information from the image, but to modify it to hallucinate an object that matches the textual description.\n\n> **W2.** The structure of the model is simple. It lacks innovation. The description of the model is not specific enough. Contrastive learning is often used in the task of composed image retrieval, so it is not an innovative method. \n\nIt works, and the goal of the paper was explicitly to show that complex pipelines are not needed to reach good performances. Simplification of complex tasks so they become accessible to more people and robust has often more impact than a complex system prone to brittleness \n\n> **W3.** Experiments are basically a comparison with other models, but the ablation experiment of your own model and visualization is lacking.\n\nDue to space limitations, we rejected 2 ablations in the Appendix related to our method: how to condition (symmetric/asymmetric) and where to condition (insertion depth), with plenty of retrieval results, attention visualization and PCA, as well as failure cases. Squeezing them into the main paper would force us to cut related work or experimental descriptions.\n\n> **Q1.** You say it extracts referred embedding using weakly-supervised training. Why it is a weakly-supervised training?\n\nWe call it weakly-supervised because we don\u2019t have access to localized information (such as bounding boxes) about the object of interest. In the computer vision literature, this is referred to as weak supervision, in contrast to having access to bounding box or segmentation maps which are usually referred to as supervised methods.\n\n> **Q2.** In fig2, it shares weight between the two vision transformers. In this model, it is whether all parameters are shared or only parts of the parameters are shared. And I want to know why to share the weight.\n\nOur method uses a single model to process both unconditional and conditional images, so the weights are the same. It is less memory intensive to have the same model and we did not find any negative impact.\n\n> **Q3.** In table1 and table2, you say you report bootstrapped means and standards deviations for 0K distractors, but I don\u2019t see the result of the 0K distractors. In addition, your model is similar to some models that used in the task of composed image retrieval. FashionIQ is also a dataset about clothes. Do you try this dataset using your method?\n\nThanks for pointing out the mistake in the table caption, we removed the results with 0 distractors to make the table more legible. The results were not really interesting as performances in this very simple setting tend to saturate. \n\nWe are currently running experiments on FashionIQ and will report the results here. However please note that this is a different task and we do not expect state-of-the-art results."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699991234311,
                "cdate": 1699991234311,
                "tmdate": 1699991234311,
                "mdate": 1699991234311,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DLOYiYFPWl",
            "forum": "Z8uFGTNXIF",
            "replyto": "Z8uFGTNXIF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7672/Reviewer_vVJq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7672/Reviewer_vVJq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new task, named Referred Visual Search (RVS). It aims to search the specific part under the condition of category. The new task sounds good. The authors also introduce a corresponding dataset and framework. Some experimental results look good."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "## Strengths\n\n1. The writing is good. It is easy to follow this paper.\n\n2. The motivation sounds reasonable.\n\n3. Some experimental results look good."
                },
                "weaknesses": {
                    "value": "## Weaknesses\n\n1. It may be not appropriate to use the entire image (even given the conditions) to search for an part area, such as pants.\n\n2. Why not crop the part area according to the given condition and then use it to search?\n\n3. How to collect the LAION-RVS-Fashion dataset? How to ensure the accuracy of the labels? The original labels are not clear.\n\n4. The main content of this paper has 10 pages."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698983518368,
            "cdate": 1698983518368,
            "tmdate": 1699636933122,
            "mdate": 1699636933122,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K1hQYWGpux",
                "forum": "Z8uFGTNXIF",
                "replyto": "DLOYiYFPWl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7672/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback and address their main concerns below: \n\n> **W1.** It may be not appropriate to use the entire image (even given the conditions) to search for an part area, such as pants.\n\nWhile surprising, the point of this paper is to demonstrate that it is currently the best performing approach (see results in Table 2).\n\n> **W2.** Why not crop the part area according to the given condition and then use it to search?\n\nVery good point. Maybe it wasn\u2019t stressed enough in the paper but it is exactly our baseline with GroundingDINO + ViT (Section 5.2, \u00a7Detection-based Baseline, and Table 1). \n\n> **W3.** How to collect the LAION-RVS-Fashion dataset? How to ensure the accuracy of the labels? The original labels are not clear.\n\nWe will add a script for direct downloading to a public repository (using img2dataset on the provided parquet files).\n\nWe are very confident on the labels since the tuples are made from a limited number of shop websites using rule-based parsing. \n\n> **W4.** The main content of this paper has 10 pages.\n\nWe followed the author guidelines stating that ethics and reproducibility statements are \u201cnot counted against the maximum page limit\u201d. See https://iclr.cc/Conferences/2024/AuthorGuide ."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699991119125,
                "cdate": 1699991119125,
                "tmdate": 1699991119125,
                "mdate": 1699991119125,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Snn56C0TYr",
            "forum": "Z8uFGTNXIF",
            "replyto": "Z8uFGTNXIF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7672/Reviewer_F8wp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7672/Reviewer_F8wp"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims at fashion retrieval conditioned on images and texts. Particularly, the text conditions can be categories and captions. This task is tackled via learning the joint embedding of texts and images, similar to conventional multi-modal metric learning methods. A dataset that is extracted from the publicly available dataset LAION-5B is constructed to validate the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed dataset can facilitate research on multi-modal fashion retrieval.\n2. Extensive experiments have been conducted to provide insightful information on this task.\n3. The writing of this paper is excellent and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Although this paper claims its target task is new, I still consider it to belong to multi-modal fashion retrieval. That is, one can include classes, attributes, captions, or even negative prompts in the textual conditions, and then leverage LLMs to process them uniformly. \n2. The failure cases suggest image features are dominant, and hence the proposed method or the task might not be as convenient as it claims. For example, what if the user wants to find clothes with similar styles but different colors, or of the same brand? Moreover, the text conditions seem rather simple, so whether the proposed method can handle fine-grained queries is unclear.\n3. The comparison between the proposed method and SOTAs might be unfair, e.g., ASEN is implemented partially and it only uses attributes. Other baselines with similar architectures, like FashionBert should be considered as well. Besides, how is the performance of the proposed method on other fashion retrieval benchmarks?"
                },
                "questions": {
                    "value": "Please refer to the weakness part. I will adjust my score according if my concerns can be addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7672/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7672/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7672/Reviewer_F8wp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7672/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699091099365,
            "cdate": 1699091099365,
            "tmdate": 1699636933026,
            "mdate": 1699636933026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sWiEmFwZk8",
                "forum": "Z8uFGTNXIF",
                "replyto": "Snn56C0TYr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7672/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7672/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback, and address their main concerns below :\n\n> **W1.** Although this paper claims its target task is new, I still consider it to belong to multi-modal fashion retrieval. That is, one can include classes, attributes, captions, or even negative prompts in the textual conditions, and then leverage LLMs to process them uniformly.\n\nWe agree that our RVS task belongs to multi-modal fashion retrieval. However, it has not been tackled in the literature, contrary to text-to-image or image-to-text retrieval and composed image retrieval for which known datasets and benchmarks exist.\n\nFor text, we do use a T5-XL to produce our embeddings and enable free-form conditioning. We find experimentally that using a single word (~category) as input works well (Fig. 9 in the appendix) even though BLIP2 captions are longer. \n\n> **W2.** The failure cases suggest image features are dominant, and hence the proposed method or the task might not be as convenient as it claims. For example, what if the user wants to find clothes with similar styles but different colors, or of the same brand? Moreover, the text conditions seem rather simple, so whether the proposed method can handle fine-grained queries is unclear.\n\nComposed Image Retrieval is a different task: in our task we need to retrieve the exact garment depicted on the image. We think that paying more attention to the image is a good property, text is only here to designate an object in the scene. Unifying RVS & CIR is indeed an interesting direction, as stated in our conclusion.\n\nRegarding fine-grained text conditioning, we are currently running experiments on a Fashion CIR dataset (FashionIQ) and will report the results here. However please note that this is a different task and we do not expect state-of-the-art results.\n\n\n> **W3.** The comparison between the proposed method and SOTAs might be unfair, e.g., ASEN is implemented partially and it only uses attributes. Other baselines with similar architectures, like FashionBert should be considered as well. Besides, how is the performance of the proposed method on other fashion retrieval benchmarks?\n\nWe fully trained ASEN using the author\u2019s implementation and reported the best obtained results (attained during stage 1). The results obtained with stage 2 are 14.6 R@1 against2M. We believe this is due to the asymmetry of this task in which the gallery is composed of simple images containing only a single object that break key assumptions of ASEN. This result highlights the difference of our RVS task compared to other retrieval tasks for which ASEN was designed.\n\nSince FashionBERT/UNITER are designed for cross-modal retrieval, it is unclear how they could be used for conditional retrieval. A naive approach would result in a quadratic cost with respect to the number of products which is prohibitive here (in our case, 2k queries times 2M distractors results in 4B forward passes which would take more than a year of compute at 100 forward passes per second). \n\nTo our knowledge, there exists no other public RVS benchmark, where the task is to conditionally extract an embedding of a subpart of the image. Our method has not been designed for unconditional Image to Text retrieval or Text to Image retrieval tasks.\n\nWe are currently running experiments on a Fashion CIR dataset (FashionIQ) and will report the results here. However please note that this is a different task and we do not expect state-of-the-art results."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7672/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699991016654,
                "cdate": 1699991016654,
                "tmdate": 1699991016654,
                "mdate": 1699991016654,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]