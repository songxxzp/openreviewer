[
    {
        "title": "Spatially-Aware Transformers for Embodied Agents"
    },
    {
        "review": {
            "id": "PkPYVhbNEz",
            "forum": "Ts95eXsPBc",
            "replyto": "Ts95eXsPBc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6931/Reviewer_44cr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6931/Reviewer_44cr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Spatially-Aware Transformers (SAT) to keep spatial information in place-centric episodic memory.\nSAT maintains multiple spatial memories for respective places while ensuring the \"total\" size of memory (i.e., $L / K$).\nFor memory management, the author proposes the Adaptive Memory Allocator (AMA) that adaptively finds the best memory management policy by learning $\\pi(\\sigma | \\tau)$ using Q-learning given a task description, $\\tau$.\nThe proposed AMA outperforms the baselines (i.e., model w/o AMA) by noticeable margins regarding effectiveness and efficiency in various downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is generally written well and easy to follow.\n- Extending Transformer-based memory architecture to spatial domains is well-motivated and sounds sensible.\n- Learning to choose the best memory management strategy from multiple candidates looks reasonable.\n- Experiments on various downstream tasks supports the generality of the proposed approach.\n- The proposed approach achieves strong performance gain with large margins."
                },
                "weaknesses": {
                    "value": "- The novelty of AMA seems a bit weak, as it is basically policy learning that chooses the best action (here, strategy) that maximizes rewards. What are some core differences from conventional policy learning, especially related to memory management for spatial information?\n- The key idea for SAT seems to use separate networks for respective places. But can the architecture be useful in the case of a large number of places (i.e. what if $K -> \\infty$ that results in almost zero size of memory for each place)?"
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6931/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6931/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6931/Reviewer_44cr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6931/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697997217288,
            "cdate": 1697997217288,
            "tmdate": 1699636808131,
            "mdate": 1699636808131,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qMJaJATabP",
                "forum": "Ts95eXsPBc",
                "replyto": "PkPYVhbNEz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful to Reviewer 44cr for their thoughtful critique and acknowledgment of the well-written nature of our manuscript and its contributions to spatial memory in transformer architectures. Your feedback has been invaluable in enhancing the quality of our paper.\n\n### **Novelty of the Adaptive Memory Allocator (AMA)**\n\nWe answered this in Common Response CR3.\n\n### **Scalability with a Large Number of Places**\n\nRegarding scalability, our Spatially-Aware Transformer (SAT) architecture is designed with parameter sharing across place memories to encode observations efficiently. By employing a common network for all place memories, the SAT model can effectively attend to and process information pertinent to each specific place. This architecture enables the transformer to selectively retrieve relevant memory in response to queries, regardless of the number of places. The number of place $K$ could not be infinite, the maximum number of place is the number of memory, in which, the Place Memory will be same to SAT memory without the hierarchy."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738322234,
                "cdate": 1700738322234,
                "tmdate": 1700738342231,
                "mdate": 1700738342231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KAfk4GBkKo",
            "forum": "Ts95eXsPBc",
            "replyto": "Ts95eXsPBc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6931/Reviewer_QvRD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6931/Reviewer_QvRD"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the spatially-aware transformers (SAT) that incorporate agent's spatial and temporal experiences important to solve embodied problems. Specifically, it studies the influence of spatial and decisional information, and explores multiple memory management strategies (e.g., Place-centric vs Time-centric, First-In-First-Out vs Last-In-First-Out, and Adaptive Memory Allocator) on various downstream tasks, showing insights of different memory utilization and management approaches in addressing many machine learning problems (e.g., supervised prediction, image generation), and justifying the proposed SAT-AMA."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper studies the important problem of utilizing and managing spatial and temporal memory experienced by embodied agents that are essential to address various downstream tasks. Particularly, it considers the practical issue of memory constraint and perform experiments based on the popular transformer architectures. The research presented in this paper is very well motivated.\n\n- The paper is very technical solid. Almost all arguments are well-supported/justified by the highly-relevant and classic references and thorough experiments (in Appendix). It carefully compares place- and time-centric store and read, and progressively studies FIFO to the proposed Adaptive Memory Allocator based on SAT. I believe the extensive settings and results presented in this paper have the potential to inspire many future works.\n\n- Overall, this paper was a very enjoyable read to me. All details have been clearly presented (especially with the Appendix and all nice visualizations); The paper is nicely-structured, concise but contains massive valuable information. I believe many arguments and thoughts presented in this paper will be very constructive to relevant future research."
                },
                "weaknesses": {
                    "value": "- The title of this paper is very misleading\n    - The paper does not propose any new formation of the transformer architecture to better model spatial information, but focuses on managing agents' episodic memory for addressing different downstream tasks. \n\n- This paper overclaims several contributions.\n    - The paper says \"we are the first to motivate, conceptualize, and introduce the notion of transformers capable of utilizing explicit spatial information\", but as the authors mentioned that defining and managing external memory have been extensively studied in previous machine learning literatures. It simply compares place- and time-centric store and hierarchical read methods that are intuitively suitable for different downstream tasks.\n    - One good example is the use of topological graph that stores observations of keypoints for agents in visual navigation, e.g., the widely applied DUET agent [1] for vision-and-language navigation [2], which is essentially close to the proposed SAT-PM-PH model. \n    - The experiments of action-conditioned image generation (Exp-5) and reinforcement learning agents (3.3) are not very convincing to me. Exp-5 is not a practical setting and 3.3 is relatively simple that cannot represent other reinforcement learning agents, see more below.\n\n- Missing experiments.\n    - I think this paper lacks investigation on the more recent embodied agents and their memory management approaches. \n    - The experiments presented in this paper are relatively small-scale and simple. I am concerning how the proposed methods might impact recent research that often apply more capable networks (and massive data) to learn generic spatiotemporal priors to facilitate representing and memorizing the observations. e.g., Figure 3 - an agent might be able to create a very compact representation for all ballet rooms from a single visit to each room? \n    - Following my previous point, I think this study emphasizes the scenario of memory constraint, but the experiments only considers small memory capacity, negelating the difference in representation (i.e., how to store an observation) and the difficulty in grounding from the queries to relevant memories. Overall, the generalization and practical influence of the proposed methods is not very clear to me.\n    - I think a valuable baseline is missing here: without explicitly defining any memory management strategies but gives the agent a certain memory budget and asks it to learn to update the memory by itself by training the agent on a mixture of data and tasks (e.g., a more general version of DNC mentioned in Appendix D.4).\n\n[1] Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation. Chen et al., CVPR2022.\n\n[2] Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments. Anderson et al., CVPR2018."
                },
                "questions": {
                    "value": "I hope the authors can address some of my concerns in Weaknesses. I don't have any other questions here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6931/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698458179906,
            "cdate": 1698458179906,
            "tmdate": 1699636808017,
            "mdate": 1699636808017,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tquQswzpEx",
                "forum": "Ts95eXsPBc",
                "replyto": "KAfk4GBkKo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to extend our sincere thanks to Reviewer QvRD for their positive and encouraging feedback on our manuscript. We are grateful for the recognition of the importance of our research and the comprehensive nature of our work. Your constructive comments have been instrumental in refining our paper.\n\n### **Concerns on Misleading Title**\n\nYour observation regarding the title's potential to mislead readers is well-noted. We added 'Memory' in the title.\n\n### **Concerns on Over-claiming**\n\n>The paper says \"we are the first to motivate, conceptualize, and introduce the notion of transformers capable of utilizing explicit spatial information\", but as the authors mentioned that defining and managing external memory have been extensively studied in previous machine learning literatures. One good example is the use of topological graph that stores observations of keypoints for agents in visual navigation, e.g., the widely applied DUET agent [1] for vision-and-language navigation [2], which is essentially close to the proposed SAT-PM-PH model.\n> \n\nWe acknowledge the similarities between our SAT-PM-PH model and the topological map based memory used in the DUET agent for vision-language navigation. However, our work extends beyond the storage of keypoint observations by embedding the spatial knowledge within the transformer\u2019s attention mechanism, which allows for a more nuanced and integrated processing of spatial information. For example, the place memory could be constructed per place, the each memory is encoded with more detailed spatial information. Other differences are that our model is designed to be retrieved without the spatial information and to optimize the efficacy on the retrieval through the hierarchical architecture, while the topological map based memory doesn\u2019t. \n\nIn light of your feedback, we will revise our manuscript to more accurately represent the state of the art and clearly delineate the innovative aspects of our work. We will adjust our language to avoid overstatements and ensure that our contributions are contextualized appropriately within the broader landscape of research in this area.\n\n> The experiments of action-conditioned image generation (Exp-5) and reinforcement learning agents (3.3) are not very convincing to me. Exp-5 is not a practical setting and 3.3 is relatively simple that cannot represent other reinforcement learning agents, see more below.\n\nTo handle this, we additionally evaluated our model in realistic 3D environment, Habitat environment (suggested by reviewer XL1m). See Common Response CR2 for more details.\n\n### **Missing Experiments**\n\n> I think this paper lacks investigation on the more recent embodied agents and their memory management approaches.\n> \n\nWe thank the reviewer for this constructive feedback. We acknowledge that we overlooked some recent embodied agents such as DUET agent. We will update the experiments with the more recent embodied agents in our revision.\n\n> The experiments presented in this paper are relatively small-scale and simple. I am concerning how the proposed methods might impact recent research that often apply more capable networks (and massive data) to learn generic spatiotemporal priors to facilitate representing and memorizing the observations. e.g., Figure 3 - an agent might be able to create a very compact representation for all ballet rooms from a single visit to each room?\n> \n\nWe appreciate your constructive feedback. As we replied for your concern above, we additionally evaluated our model in the more realistic 3D environment, Habitat environment, which is discussed in our common responses.\n\n> Following my previous point, I think this study emphasizes the scenario of memory constraint, but the experiments only considers small memory capacity, neglating the difference in representation (i.e., how to store an observation) and the difficulty in grounding from the queries to relevant memories. Overall, the generalization and practical influence of the proposed methods is not very clear to me.\n> \n\n We are grateful for your critical insight regarding our treatment of memory storage strategy and retrieval of relevant memories from queries.\n\n We recognize that the memory storage strategy based on observation differences represents a valuable line of inquiry [1, 2]. As mentioned in our common responses, our primary research question in this paper is \u201cwhat if spatial information is available as time indices to transformer-based episodic memory?\u201d. While the memory storage strategy based on observation difference is indeed an important aspect, it is somewhat orthogonal to the main focus of our study. Nevertheless, we agree that such a strategy could be a robust alternative to FIFO memory and could complement our spatially-aware memory framework."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738192857,
                "cdate": 1700738192857,
                "tmdate": 1700741222010,
                "mdate": 1700741222010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZQS9hIUs82",
                "forum": "Ts95eXsPBc",
                "replyto": "KAfk4GBkKo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "For retrieving the relevant memories, we think that it is shown in **Exp-5**. In the experiment, the model should generate the mini-patch on the facial image even though sometimes, the appearance looks similar (e.g., when the patch is near to the eye). Figure 7 shows that SAT-AMA can generate the mini-patches accurately based on the actions and spatial knowledge. This result suggests that SAT-AMA can effectively discern relevant memories through the spatial knowledge even when observation differences are subtle. \n\nWe thank the reviewer for giving this insightful comment. We will try to add this strategy as one of our baselines in our revision.\n\n>I think a valuable baseline is missing here: without explicitly defining any memory management strategies but gives the agent a certain memory budget and asks it to learn to update the memory by itself by training the agent on a mixture of data and tasks (e.g., a more general version of DNC mentioned in Appendix D.4).\n\nWe appreciate your suggestion to evaluate the Differentiable Neural Computer (DNC) across a mix of tasks. Even though we evaluated DNC for a single task in the Room Ballet environment, but we extended this experiment to cover the setting with multiple tasks. During this process, we observed that the memory capacity allocated to the DNC in prior experiments was less than that of the SAT-AMA. To address this discrepancy, we reassessed the DNC performance on a single task with a memory capacity now comparable to that of SAT-AMA.\n\nThese results are detailed in **Appendix D.6**. In summary, when memory capacities are matched, the DNC demonstrates quicker learning than SAT-AMA in terms of training steps. However, it is notably less efficient in terms of wall-clock time, and the performance is unstable throughout learning process. The slow learning speed in terms of wall-clock time is attributed to the recurrent nature of the DNC's architecture, which, unlike the parallel training capabilities of the Transformer, necessitates a slower training process.\n\nFurthermore, the AMA's policy requires a period of exploration during the initial stages of learning, which may contribute to a comparative delay in learning speed when measured by training steps. These insights and additional details are expounded upon in the Appendix. We are grateful for your constructive feedback.\n\n[1] Le, Hung, et al. \"Intrinsic Motivation via Surprise Memory.\"\u00a0*arXiv preprint arXiv:2308.04836*\u00a0(2023).\n\n[2] Saade, Alaa, et al. \"Robust Exploration via Clustering-based Online Density Estimation.\" (2022)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738248042,
                "cdate": 1700738248042,
                "tmdate": 1700738448097,
                "mdate": 1700738448097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HCcdCy9cfh",
            "forum": "Ts95eXsPBc",
            "replyto": "Ts95eXsPBc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6931/Reviewer_XL1m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6931/Reviewer_XL1m"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by the significance of spatial context in the formation and retrieval of episodic memory, the paper proposes to add a spatial embedding into transformers and organize the episodic memory in a place-centric way to obtain a spatially aware Transformer model. The paper also proposes the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize the efficiency of memory utilization. The experiments on several environments demonstrate the advantages of the proposed model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- It's exciting and of significance to have a transformer capable of utilizing explicit spatial information and can act as better episodic memory.\n\n- Designing an adaptive memory allocator is useful\n\n- Extensive experiments on various environments and tasks.\n\n- The idea of incorporating spatial information into episodic memory is novel and interesting"
                },
                "weaknesses": {
                    "value": "- It seems the \"spatial-aware transformers\" are achieved solely by adding a spatial embedding. And its specific design or implementation is not clearly mentioned. Only a sinusoidal positional embedding is mentioned in Exp-1, do other experiments also use this? How is this positional embedding enough to represent spatial relations, since space is not unidirectional as time? \n\n- The idea of an ADAPTIVE MEMORY ALLOCATOR is very interesting, but the implementation is quite trivial to me. What's the difficulty of learning this policy through Q-learning?\n\n- The organization of the paper could use some improvements, such as the missing Table 1 in the paper though mentioned in Exp-2, the confusing combination of the figures from different experiments, and many details deferred to the appendix, making it difficult to understand the details of the model or the experiments.\n\n- Much more details of the experiments are needed. What are the exact input and output of these environments? How is the input represented and fed into the transformers? How is the training implemented for different baselines?\n\n- The experiment environments are too toy setting for \"embodied agents\", it would strengthen the paper to add some real embodied environments, such as the Habitat[1] and TDW[2].\n\n[1] Habitat: A Platform for Embodied AI Research\n\n[2] ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation"
                },
                "questions": {
                    "value": "See concerns in the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6931/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623768965,
            "cdate": 1698623768965,
            "tmdate": 1699636807908,
            "mdate": 1699636807908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ex6ISVlpnQ",
                "forum": "Ts95eXsPBc",
                "replyto": "HCcdCy9cfh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to Reviewer XL1m for their insightful feedback and for recognizing the potential of our work to enhance episodic memory with spatial context. We are particularly appreciative of the acknowledgment of the significance of our proposed Adaptive Memory Allocator (AMA) and the comprehensiveness of our experimental validation.\n\n### **Clarification on the Design and Implementation of Spatial Embeddings**\n\nThank you for the constructive suggestion. We agree that our paper would become more clear and complete with the suggested improvement. As suggested, we made the following improvements. \n\nIn **Section 2.1**, we added the following.\n\n\u201cFor the time and spatial embedding, the learnable embedding \\citep{bert} or sinusoidal positional embedding \\citep{transformer} could be applicable, but in this literature, for simplicity, the sinusoidal positional embedding is used.\u201d\n\nIn **Appendix C.2**, we provided a comprehensive description of the spatial embedding construction for each experiment. For example, we clarified the fact that we used 2-D sinusoidal positional embedding in **Exp-5** (the FFHQ generation experiment) while sinusoidal positional embeddings were primarily employed in other experiments. \n\nIn **Appendix D.9**, we added a comparative analysis of sinusoidal, 2-D sinusoidal (sinusoidal embedding for x and y axes), and 2-D learnable positional embeddings. This suggests that that 2-D sinusoidal and learnable embeddings are comparable while the sinusoidal embedding is clearly worse than them.\n\n### **What\u2019s the difficulty of learning AMA policy**\n\nWe answered this in Common Response CR3.\n\n### **Improving the Organization of the Paper**\n\nThank you for the constructive suggestion. We agree with the potential confusions and need for update. Unfortunately, due to the page limit, we are currently facing difficulties in incorporating certain Figures and Tables from the Appendix into the main paper. We will, however, continue exploring options to appropriately arrange the content. For instance, if the paper is accepted, it would be much easier to make the suggested updates as we would be allowed an additional page. In the meantime, we have clearly indicated in the paper that Table 1 is located in the Appendix.\n\n### ****Application to the more realistic embodied environments****\n\nThank you for the suggestion. As suggested, we conducted additional experiment in a complex environment Habitat. See Common Response CR2 for more details.  \n\n### **Details of the experiments are missing. Input & Output of each environment, and training algorithms for different baselines**\n\nThank you for the suggestion. As per your advice, we have included the suggested details of the experiments in **Appendix C.1**. These details include the exact input and output of each environment, as well as the representations that are fed into transformer memories.\n\nRegarding the training algorithm of the baselines, we have provided the details in **Appendix B.3**. The training algorithm for SAT-FIFO is the same as SAT-AMA, except that it does not require learning to choose a strategy and therefore does not involve policy learning. In the case of DNC, we sampled a batch of episodes and directly optimized the downstream task loss function.\n\nThe detail models and hyperparameters of SAT variants and DNC are provided in **Appendix B.4** and **Appendix D.6**, respectively. For reproducibility, we will also release the code upon the acceptance of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737882470,
                "cdate": 1700737882470,
                "tmdate": 1700737882470,
                "mdate": 1700737882470,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "46bvrbUyVU",
            "forum": "Ts95eXsPBc",
            "replyto": "Ts95eXsPBc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6931/Reviewer_BEnq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6931/Reviewer_BEnq"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces and evaluates a novel framework comprising the Spatially-Aware Transformer (SAT) and Adaptive Memory Allocator (AMA) for a range of tasks demanding spatial awareness and effective memory management. Through a series of well-structured experiments, the authors demonstrate the capabilities of their proposed methods in various environments, such as the Room Ballet environment for prediction tasks, as well as in action-conditioned world modeling and spatially-aware image generation. The comprehensiveness of the methodology is evident, as it details how to incorporate spatial information into transformer models and effectively manage memory for different types of tasks. The experiments are extensive and cover different scenarios to validate the efficacy of the proposed framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "**Great Motivation:** The paper addresses a critical gap in existing models\u2019 inability to effectively integrate spatial information, which is very crucial for tasks in various domains. The introduction and literature review provides a compelling argument for why this integration is necessary, setting a solid foundation for the rest of the paper.\n\n**Comprehensive Method Explanation:** The authors provide a thorough and clear explanation of the Spatially-Aware Transformer and Adaptive Memory Allocator. The methodology section is well-structured, detailing each component of the system, the underlying theory, and the implementation specifics, which aids in the reproducibility of the results.\n\n**Extensive Experiments:** The paper goes beyond theoretical claims and validates the proposed framework through a series of diverse and challenging experiments. These experiments not only demonstrate the strengths of the SAT-AMA combination but also highlight its versatility across different tasks and scenarios. The image generation experiments are especially interesting and seem not to exist in its ancestor work of Towards mental time travel: a hierarchical memory for reinforcement learning agents."
                },
                "weaknesses": {
                    "value": "**Usability in Complex Embodied AI Scenarios:** The paper, while comprehensive, could benefit from a deeper discussion on the applicability and scalability of the proposed methods in more complex embodied AI scenarios. Given the rising interest in virtual homes and ThreeDWorld with many rooms for task operation, readers would appreciate some discussion or insights into how the SAT-AMA framework can be adapted or scaled to meet the challenges presented by these intricate environments. A similar discussion could be like, \"Is the method scalable to larger environments?\" and \"How can the SAT-AMA framework be adapted for more intricate task operations?\" to provide a complete picture to the readers.\n\n**More discussion on AMA:** I appreciate the introduction and application of the Adaptive Memory Allocator (AMA) within the Spatial Awareness Transformer framework, as it presents a novel and promising approach to dynamically allocate memory based on task requirements. However, I find that there could be a more detailed analysis and discussion of AMA\u2019s strategy selection across different experimental settings and tasks. Understanding how AMA decides on specific spatial strategies could uncover valuable priors for selecting appropriate strategies based on the nature of the task, which would immensely benefit future work exploring new methods in this domain. It would be beneficial for the readers if the authors could provide insights into the distribution of strategies chosen by AMA, its dependency on the nature of tasks, and the correlation between strategy choices and task performance. Such analysis would not only deepen our understanding of AMA\u2019s workings but also offer practical guidance for researchers aiming to employ similar adaptive mechanisms in their work. A discussion on these aspects would significantly enhance the completeness and depth of the paper, providing valuable context and potentially guiding future innovations in the field."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6931/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6931/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6931/Reviewer_BEnq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6931/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698868008246,
            "cdate": 1698868008246,
            "tmdate": 1699636807805,
            "mdate": 1699636807805,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QnNUrZklz1",
                "forum": "Ts95eXsPBc",
                "replyto": "46bvrbUyVU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6931/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We express our gratitude to the reviewer for the recognition of our paper's motivation and the breadth of our experiments. As addressed in the common responses, we have discussed the extension of our work to the complex embodied AI experiment in the Habitat environment. \n\n### **Usability in more complex embodied AI scenarios**\n\nAs suggested, we added additional experiment results in a complex environment Habitat (suggested by reviewer XL1m). See Common Response CR2 for more details.  \n\n### **More discussion on AMA**\n\nWe thank the reviewer for their insightful comments regarding the need for more in-depth discussion on the Adaptive Memory Allocator (AMA). We agree that providing a more comprehensive discussion on AMA would significantly enhance the paper. \n\nAs suggested, we investigated further to show the distribution of strategies chosen by AMA, and discuss its dependency on the nature of tasks, and the correlation between strategy choices and task performance. The table below presents the distribution of strategies selected by AMA, along with corresponding performance metrics, derived from the Ballet-FIFO task in **Exp-3**. We note that each column represents the probability of each strategy being selected, the accuracy when each strategy was selected. The last row represents the accuracies normalized for the four strategies.\n\nThe strategy distribution was averaged across three distinct runs (random seeds) and was ascertained by passing the output of the AMA's policy network through SoftMax. The results elucidate a clear correlation between the chosen strategies and task performance, reinforcing the premise that AMA's strategic choices are inherently linked to efficacy.\n\n|  | FIFO | LIFO | MVFO | LVFO |\n| --- | --- | --- | --- | --- |\n| AMA dist. | 0.475 | 0.073 | 0.15 | 0.302 |\n| Accuracy | 0.999 | 0.146 | 0.325 | 0.714 |\n| Normalized Acc. | 0.457 | 0.067 | 0.149 | 0.327 |\n\nWe added this new results in **Appendix D.4** of the revised version.\n\nAgain, we appreciate your suggestion. We believe that by incorporating this analysis, the paper will become more comprehensive."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737745555,
                "cdate": 1700737745555,
                "tmdate": 1700738729065,
                "mdate": 1700738729065,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]