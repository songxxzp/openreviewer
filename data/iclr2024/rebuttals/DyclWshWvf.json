[
    {
        "title": "Causal-based Analysis on Credibility of Feedforward Neural Network"
    },
    {
        "review": {
            "id": "3nZVW1eJeC",
            "forum": "DyclWshWvf",
            "replyto": "DyclWshWvf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7579/Reviewer_wRkd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7579/Reviewer_wRkd"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to address the problem of feedforward neural network (FNN)\u2019s credibility by discovering and quantifying the causal relationships in the input and output layers of FNN. To estimate the causal effect of intervening in an input neuron, the authors categorize the causal structure of the input layer into 3 different substructures and calculate the corresponding average treatment effects. Experiments are conducted in a medical setting to demonstrate the effectiveness of the proposed methodology."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\u2022\tThe issue of FNN\u2019s credibility is important and holds practical value in various settings, including medical applications.\n\n\u2022\tThe authors provide a sufficient introduction to the background knowledge of causal inference and feedforward neural networks before presenting the main methodology.\n\n\u2022\tThe paper is well organized."
                },
                "weaknesses": {
                    "value": "\u2022\tThe definitions and illustrations of \u201ccausal substructure\u201d are ambiguous. The authors give explanations using individual neurons $l_{1a}$, $l_{1b}$, and $l_{nc}$, but this gives me an intuition that cases (b) and (c) in Figure 3 need not to be treated separately. For example, if I **swap $l_{1a}$ and $l_{1b}$ in case (c) and intervene on $l_{1b}$**, then case (c) is just equivalent to case (b). In other words, every time I encounter a confounding substructure, I can just swap the order of input features and intervene on the same neuron to transform it into a mediation substructure. Is that right? Please correct me if I misunderstood anything.\n\n\u2022\tThe notations in Section 4 are very confusing as the authors use $l_{1a}, l_{1b}$ and $x_{i}$ interchangeably to represent input neurons. My understanding is that $x_{i}$ represents some value that the neuron $l_{1i}$ can take. If so, then the expressions $E[\\cdot|x_{i} = \\alpha]$ and $E[\\cdot|do(x_{i} = \\alpha)]$ again become very confusing.\n\nMinor suggestions:\n\n\u2022\tFor the definition of d-separation, it might be better to note that the set $Z$ is measured/observed so that the remaining unobserved variables can be described by distributions conditional on $Z$.\n\n\u2022\tTo make the illustration clear, it might be better to clearly mark $f^{*}$ and $f\u2019$ in Figure 1(c). \n\n\u2022\tEquations 7-9 appear to have similar expressions but are applied in different contexts. The authors might consider condensing this section by providing more detailed context and presenting a single equation."
                },
                "questions": {
                    "value": "\u2022\tCould you elaborate more on the \u201ccausal substructure\u201d as I mentioned in the first bullet point in the Weaknesses section?\n\n\u2022\tRight above Equation 4, you assume that $f\u2019_{y}$ is smooth. Does this mean you assume there are no non-smooth activation functions in the FNN (e.g., ReLU whose derivative does not exist at zero)?\n\n\u2022\tIn Section 3, why do you choose threshold values of 1% and 9% for the error rates?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Reviewer_wRkd"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698349600021,
            "cdate": 1698349600021,
            "tmdate": 1699636917769,
            "mdate": 1699636917769,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8luzZF4hPW",
                "forum": "DyclWshWvf",
                "replyto": "3nZVW1eJeC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7579/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for your positive and constructive feedbacks. We have modified the annotations in Section 4  to make our expression clearer. To make the illustration clear, we mark\u00a0$f^*$\u00a0and\u00a0$f\u2019$\u00a0in Figure 1(c). We also have condensed Eq.7-9 by presenting a single equation. Regarding the issue you mentioned in Figure 3, we analyze the three figures in Figure 3 with neuron l1a as the protagonist, so in Figures 3b and 3c, neuron l1a is in different causal situations. On the other hand, your viewpoint is also very correct. If we exchange the positions between the two input layer neurons l1a and l1b in Figures 3b and 3c, their situations can also be classified into one category. So from another perspective, it can also be divided into two types of substructures, but this is not the original intention we want to showcase. After observing the experimental results, we choose threshold values of 1% and 9%, which may lack some rigor. In addition, we supplemented the ablation experiment, and without considering the causal relationship between input layer neurons. It can be seen that our ATE slope has significantly decreased, making the analysis of neural network credibility more difficult. We apologize for not clarifying all questions given the limited space and many reviews. We will fix all typos and add missing references in the next revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656303975,
                "cdate": 1700656303975,
                "tmdate": 1700656303975,
                "mdate": 1700656303975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xM4cuHOinF",
                "forum": "DyclWshWvf",
                "replyto": "8luzZF4hPW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7579/Reviewer_wRkd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7579/Reviewer_wRkd"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response and effort to update the manuscript. I will keep my score unchanged for now as I think my concerns are only partially addressed based on this response."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726635933,
                "cdate": 1700726635933,
                "tmdate": 1700726635933,
                "mdate": 1700726635933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ogE5e4wN8d",
            "forum": "DyclWshWvf",
            "replyto": "DyclWshWvf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7579/Reviewer_3wbn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7579/Reviewer_3wbn"
            ],
            "content": {
                "summary": {
                    "value": "To enhance the credibility of feedforward neural networks (FNNs), the paper transforms the FNN model into a causal structure with different causal relationships between the nodes of the input layer. Based on three categories of causal structures in the input layer, the causal effect of the potentials interventions are calculated. The proposed method is evaluated and validated with experiments in the field of pediatric ophthalmology."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem studied is fundamental and important.\n\n2. The idea is natural."
                },
                "weaknesses": {
                    "value": "The validation is not very convincing. To validate the effectivensss of the method, I think randomized controlled trials to establish the causal relationship between nodes of the input layer and that between the input and output nodes. The experimental results should serve as the ground truth to evaluate the method."
                },
                "questions": {
                    "value": "How can we design randomized controlled trials to validate the effectivenss of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Reviewer_3wbn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698551389784,
            "cdate": 1698551389784,
            "tmdate": 1699636917655,
            "mdate": 1699636917655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zx60CGY2UI",
                "forum": "DyclWshWvf",
                "replyto": "ogE5e4wN8d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7579/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the helpful comments!  We believe that our method can be further expanded by combining deep neural networks. We supplemented the ablation experiment, and without considering the causal relationship between input layer neurons. It can be seen that our ATE slope has significantly decreased, making the analysis of neural network credibility more difficult. We will address all remaining minor suggestions in the final revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652211255,
                "cdate": 1700652211255,
                "tmdate": 1700652211255,
                "mdate": 1700652211255,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zv6C95FaN6",
            "forum": "DyclWshWvf",
            "replyto": "DyclWshWvf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7579/Reviewer_EaUh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7579/Reviewer_EaUh"
            ],
            "content": {
                "summary": {
                    "value": "The authors aim to use causal structure learning to help establish the credibility of causal interpretations of neural networks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I like the idea of using causal structure learning algorithms to help identify or verify causal structures in neural networks. Finding a good relationship between causal structure learning and NN learning would really help to clarify the NN field focused on giving causal interpretations of NNs."
                },
                "weaknesses": {
                    "value": "The paper could be focused a bit more, I think, by a couple of rounds of rewriting. Some specific points:\n\n1. The flowchart in Figure 2 did not completely make sense. For instance, it looks like adding causal is independent of FNN, and it also looks like deleting hidden layers is an effect that has no downstream effects. Neither of these makes sense to me, given the discussion.\n\n2. Rhetorically, the first paragraph is somewhat disorganized and could potentially be broken into a few paragraphs for clarity."
                },
                "questions": {
                    "value": "I have some questions.\n\n1.\tIs Degenerate Gaussian being used because the variables are mixed continuous/discrete? It seems that a neural net is being used as well, which suggests that, in that case, the variables are being treated as continuous--unless there is some encoding that I haven\u2019t seen here. Figure 6 suggests that all of the variables are being treated as continuous, in which case the Degenerate Gaussian test is unnecessary; one could simply use (in Tetrad) the Fisher Z test (i.e., conditional correlation).\n\n2.\tAlso, for Figure 4, is tiered knowledge in Tetrad being assumed here, with edges forbidden in Tier 0? Otherwise, why are there no edges in Tier 1? This should be clarified. It seems unreasonable in Tetrad for edges concurrent in a tier not to exist, unless they are explicitly forbidden.\n\n3.\tSuppose knowledge is being assumed with 3 tiers as depicted with edges forbidden in Tier 0. In that case, all possible edges between tiers are represented here, which suggests that no edge has been ruled out. Does this make sense? Is this observed even if the algorithm, score, or test are varied, with variations in parameters? (I.e., it seems one needs a sensitivity test here.)\n\n4.\tAlso, it was unclear to me why the neurons in the hidden layer were left out of account if data for those is available.\n\n5.\tAnother issue is that this neural net consists of just three layers. Can this analysis extend to deep neural nets, a subject of considerable interest in recent years?\n\n6.\tThe flowchart in Figure 2 did not completely make sense. For instance, it looks like adding causal is independent of FNN, and it also looks like deleting hidden layers is an effect that has no downstream effects. Neither of these makes sense to me, given the discussion.\n\n7. The NN considered consists of just three layers. Can this analysis extend to deep neural nets, a subject of considerable interest in recent years?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681214708,
            "cdate": 1698681214708,
            "tmdate": 1699636917543,
            "mdate": 1699636917543,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BCSJKgK6w1",
                "forum": "DyclWshWvf",
                "replyto": "zv6C95FaN6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7579/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the helpful comments!  We believe that our method can be further expanded by combining deep neural networks. We supplemented the ablation experiment, and without considering the causal relationship between input layer neurons. It can be seen that our ATE slope has significantly decreased, making the analysis of neural network credibility more difficult. The causal diagram in Figure 4 is a comprehensive causal diagram obtained by subtracting unnecessary edges based on the clinical experience of professional doctors after performing causal discovery algorithms on the data. To simplify our causal structure, we delete the hidden layers because we focus more on input and output layer neurons. We focus on the causal relationship and effects between input layer neurons and output layer neurons, and further analyze the credibility of neural network decisions. To simplify the analysis process, we have omitted hidden layer neurons. We will thoroughly check and fix grammatical errors in the final submission."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651855643,
                "cdate": 1700651855643,
                "tmdate": 1700651855643,
                "mdate": 1700651855643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IAkB164wnZ",
            "forum": "DyclWshWvf",
            "replyto": "DyclWshWvf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7579/Reviewer_HFVt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7579/Reviewer_HFVt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a credibility analysis method for FNN from a causal perspective. It transforms FNN into three different causal sub-structures to calculate its causal effect. The authors of this paper conducted full-flow experiments on different sub-structures from the discovery of causal relations to the calculation of causal effect. At the same time, it conducts validation experiments on different causal effects and proves their accuracy. The results demonstrate the validity of the method of causal-based analysis on FNN. This work provides a new idea for applying and researching deep learning in risk-sensitive fields."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper effectively conducts a full-flow analysis of the credibility of feedforward neural networks from a causal perspective.\n2.\tThis paper unifies the relations between neurons in the input and output layers into three sub-structures and do causal analysis for each of them.\n3.\tThe experimental results in the field of pediatric ophthalmology demonstrate the validity of the proposed causal-based analysis method."
                },
                "weaknesses": {
                    "value": "1.\tThe motivation of this paper seems unclear. It would be beneficial to provide a clear explanation of its underlying principles that establish the credibility of causal analysis methods in risk-sensitive domains.\n2.\tIn Figure 1, the subfigures (a), (b), and (c) depict causal structures, but the explanatory text and the symbols are not clearly defined.\n3.\tIn Figure 2, the authors divide the causal structure into three distinct substructures to analyze the credibility of the feedforward neural network. However, there is a lack of sufficient description regarding the reasons for dividing the causal structure into three separate substructures, as well as the relationships and differences between each structure shown in Figure 3.\n4.\tThe confusion substructure proposed in Figure 3(c) lacks a clear explanation of the elements depicted in the figure, such as what each element represents. Additionally, the paper does not utilize mainstream causal structure models and does not provide specific elements of the causal graph. Furthermore, there is a lack of comparative description between Figure 3(c) and Figure 3(b).\n5.\tThe experiments are limited by only using one dataset. The paper lacks specific case studies and ablation experiments, making it difficult to verify the robustness and generalization of the algorithm. Additionally, there is no comparison with the state-of-the-art methods."
                },
                "questions": {
                    "value": "1. The effectiveness of this method was only validated in the medical field during the experiment. Could the authors validate the method's ability to analyze credibility in multiple domains using more diverse datasets?\n2. In Figure 1, there is a lack of text description and symbol interpretation.\n[1] In Figure 1(a), it should be indicated in the figure what each layer represents, and labels should be assigned to each layer from bottom to top, such as \"input layer\", \"hidden layer\", and \"output layer\" for clarity.\n[2] In Figure 1(b), why are A, B, D, E, F, and C represented in different colors? Do they have any specific meanings?\n[3] In Figure 1(b) and (c), what do the paths A\uf0e0B and D\uf0e0E\uf0e0F respectively represent?\n3. The work in this paper is based on causal analysis methods. However, it lacks a description of causal theory and related work in the field of deep learning, e.g.\n[1] Liu Y, Wei Y S, Yan H, et al. Causal reasoning meets visual representation learning: A prospective study[J]. Machine Intelligence Research, 2022, 19(6): 485-511.\n[2] Mao C, Cha A, Gupta A, et al. Generative interventions for causal learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 3947-3956.\n4. Regarding Figure 3, there is a need for a clear explanation of subfigures (b) and (c), including the meaning of each path and how confounding factors are introduced, such as llb serves a confounding factor between lla and lnc to bring the confounding effects. Additionally, it should be clarified which paths are identified as backdoor or frontdoor paths that require intervention.\n5. This paper uses Bootstrap Validation (BV), Add Random Common Cause (ARCC), and Data Subsets Validation (DSV) to assess the reliability of the Average Treatment Effect (ATE), ignoring its effectiveness and universality. Could you utilize more widely used evaluation metrics to prove them of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7579/Reviewer_HFVt"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729381455,
            "cdate": 1698729381455,
            "tmdate": 1699636917397,
            "mdate": 1699636917397,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GTvnyJlg5T",
                "forum": "DyclWshWvf",
                "replyto": "IAkB164wnZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7579/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the feedback and suggestions. We believe that we can try to prove our method in multiple fields in the future. We marked neurons with causal relationships between input layers in green, while independent neurons remained in blue. The red arrows between the input layer neurons represent the causal relationship between them. We have added text descriptions and symbol explanations for figure1 in the latest version. In the first part, we described the current work on improving the credibility of neural networks. In figure3(c), l1b is the confounding factor between l1a and lnc, and path l1a\uf0e0lnc need to intervene the backdoor path from l1b. In addition, we supplemented the ablation experiment, and without considering the causal relationship between input layer neurons. It can be seen that our ATE slope has significantly decreased, making the analysis of neural network credibility more difficult. We apologize for not clarifying all questions given the limited space and many reviews."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649404907,
                "cdate": 1700649404907,
                "tmdate": 1700650084353,
                "mdate": 1700650084353,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]