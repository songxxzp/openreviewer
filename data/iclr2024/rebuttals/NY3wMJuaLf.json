[
    {
        "title": "Fake It Till Make It: Federated Learning with Consensus-Oriented Generation"
    },
    {
        "review": {
            "id": "bJ53da3Cw7",
            "forum": "NY3wMJuaLf",
            "replyto": "NY3wMJuaLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed FedCOG, a synthetic data-assisted federated learning system, to mitigate the data heterogeneity in the training. The design mainly focused on the local training part. In the local training part, FedCOG first generates task-specific and client-specific data, and then uses knowledge distillation to train the local model. The experiment on computer vision benchmark datasets demonstrates that FedCOG performs well compared to existing FL baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is easy to follow. Different from other synthetic-data based methods, the FedCOG proposed task-specific and client-specific data for generation, which is novel and practical.\n\n2. The paper is well-structured. The experiments include several existing FL baselines. The usage of the real-world FL multilabel dataset FLAIR is very rare in the FL literature."
                },
                "weaknesses": {
                    "value": "1. I am confused about the data generation part of the reading. To my understanding, FedCOG took a learnable parameter for the data generator. What is the structure of the data generator? Does FedCOG update the weight of the data generator during the training as well? Could the author address more about how the data is generated locally?\n\n2. In the experiment part, what are the sample numbers of the synthetic data in your setup? \n\n3. The client number is so limited for the experiment related to standard datasets.\n\n4. I am concerned that none of the selected baselines is a synthetic data-based method. I see the paper cites FedGen in the related work section. Why does the author not compare with the recent synthetic data-based methods such as FedGen[1] and DynaFed[2]?\n\n[1]. Zhu, Zhuangdi et al. \u201cData-Free Knowledge Distillation for Heterogeneous Federated Learning.\u201d Proceedings of machine learning research 139 (2021): 12878-12889 .\n\n[2]. Pi, Renjie et al. \u201cDYNAFED: Tackling Client Data Heterogeneity with Global Dynamics.\u201d 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2022): 12177-12186."
                },
                "questions": {
                    "value": "1. How would the FedCOG be harmonized with FedProx? I am curious about how the FedCOG does the local proximal term in the KD-based model training?\n\n2. In Table 4, why FedProx took longer local training time compared to the FedCOG?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6970/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6970/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734162403,
            "cdate": 1698734162403,
            "tmdate": 1700505422915,
            "mdate": 1700505422915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vGFhMdDkFC",
                "forum": "NY3wMJuaLf",
                "replyto": "bJ53da3Cw7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and valuable suggestions. Here are our detailed responses.\n\n---\n\n**W1-1:** What is the structure of the data generator? Could the author address more about how the data is generated locally?\n\n**Response:** Sorry for the confusion.\n\nActually, for computational efficiency, we do not introduce a data generator. Rather, the only learnable parameters are the generating inputs themselves. To be more specific, suppose we want to generate N samples and H,W,C are the height, width, and number of channels. The learnable parameter is a tensor with size of N\\*H\\*W\\*C. To generate data, **only the inputs are set to be learnable** while the models are kept fixed.\n\nWe also have explored the effects of introducing a generator (a linear layer with hidden size 8192, a upsampling and three 3x3 convolution layers) in Appendix (bottom of page 17), where we find that the achieved performance is comparable. Thus, we do not introduce a generator as it is more computation-efficient.\n\n**W1-2:** Does FedCOG update the weight of the data generator during the training as well?\n\n**Response:** No, the parameters related to data generation are fixed during training the local model; also see on the right part of Figure 1.\n\n---\n\n**W2:** What are the sample numbers of the synthetic data in your setup?\n\n**Response:** The sample number of the synthetic data in our setup is 256. Please also note that this number is significantly smaller than the sample number of real data. For example, when the client number is 10 for CIFAR-10, each client has 5,000 samples on average.\n\n---\n\n**W3:** The client number is limited.\n\n**Response:** Please note that we have shown these experiments in Table 7 and Table 8 in the Appendix with different client numbers, including 5, 10, 20, 30, 50.\n\n---\n\n**W4:** None of the selected baselines is a synthetic data-based method? Why does the author not compare with methods such as FedGen?\n\n**Response:** Thanks for the advice. Actually, **FedReg (ICLR 2022) [3] in Table 1 is a synthetic data-based method**, where we can see that our method achieves significantly better performance.\n\nFollowing your advice, we now include comparison between with FedGen (ICML 2021) in Table 1 in the revision. Here, we show the comparison between FedCOG and FedGen below for convenience.\n\nFrom the table, we see that **FedCOG outperforms FedGen** across different datasets and heterogeneity types. This could result from the fact that FedCOG modifies the dataset itself to adjust the training of the whole model, which more fundamentally addresses the issue of data heterogeneity, while FedGen generates features to adjust the last layer of model only. Beside performance improvement, another key advantage of FedCOG is that FedCOG is compatible with Secure Aggregation while FedGen is not, because in FedGen the server needs to access each individual local model.\n\n[**Table R1.** Comparison between FedCOG and FedGen.]\n|   Method   | FMNIST-1  | FMNIST-2  | CIFAR10-1 | CIFAR10-2 | CIFAR100-1 | CIFAR100-2 |    Avg    |\n|:--:|:--:|:--:|:--:|:--:|:---:|:-:|:--:|\n|   FedGen   |   72.64   |   61.34   |   62.45   |   49.12   |   37.99    |   26.93    |   51.75   |\n| **FedCOG** | **77.34** | **73.68** | **64.83** | **54.00** | **42.88**  | **34.80**  | **57.92** |\n\n[3] Xu et al., Acceleration of Federated Learning with Alleviated Forgetting in Local Training, ICLR 2022.\n\n---\n\n**Q1:** 1. How would the FedCOG be harmonized with FedProx? Curious about how FedCOG does the local proximal term in the KD-based training?\n\n**Response:** FedCOG can be seamlessly harmonized with FedProx. Take a conventional classification task as an example. \n\nFedCOG has two loss terms, which are both applied at the logit level (one for cross-entropy loss and one for KD-based loss), while FedProx introduces a model-level loss, which calculates element-wise l2 distance between local and global models. Thus, these **three loss terms** can be applied together to compute the loss and backward to update local model.\n\n---\n\n**Q2:** In Table 4, why FedProx took longer local time compared to FedCOG?\n\n**Response:** \nThe reason is that during local training, FedProx needs to launch two models, while FedCOG only needs to launch one model.\n\nIn FedCOG, the soft labels of generated data are obtained by a one-time inference of the global model, which is fast. After this, during training, **only one model** (the local model) needs to be launched.\n\nIn FedProx, **one local model and one global model** need to be launched during local training because proximal term in FedProx is computed by computing element-wise distance between local and global model. This cost will be larger, especially when the model size is larger.\n\nPlease note that for FedCOG, the generation time, time for inferring soft labels and training time are all included in Table 4, making it a fair comparison.\n\n---\n\nOverall, we hope that our responses can fully address your concerns and will be grateful for any feedback."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700142221430,
                "cdate": 1700142221430,
                "tmdate": 1700142373971,
                "mdate": 1700142373971,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pCnpnUyxUC",
                "forum": "NY3wMJuaLf",
                "replyto": "vGFhMdDkFC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the effort in the response. I have some following questions regarding about it.\n\nQ2: I have a question regarding the \"the soft labels of generated data are obtained by a one-time inference of the global model\". At the 4th line in Algorithm 1, FedCOG performs complementary data generation in every round of the training. Do you mean the data generation in FedCOG is faster than the proximal term in FedProx?\n\nAdditional Q1: Does the FedCOG only limit in training image modality? Would the FedCOG work on other data modality such as audio or language?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368314071,
                "cdate": 1700368314071,
                "tmdate": 1700368314071,
                "mdate": 1700368314071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gFxlZMxDbP",
                "forum": "NY3wMJuaLf",
                "replyto": "gtCj2A1Mtj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_3E8Z"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks! I have raised up my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505410397,
                "cdate": 1700505410397,
                "tmdate": 1700505410397,
                "mdate": 1700505410397,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kZ2jlsK9Ui",
            "forum": "NY3wMJuaLf",
            "replyto": "NY3wMJuaLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6970/Reviewer_SDr1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6970/Reviewer_SDr1"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced a novel consensus scheme based on data generation to solve data heterogeneity problems in federated learning. It achieved a relatively higher accuracy on four public datasets with different degrees of heterogeneity. In the context of each individual client, the present study implemented a methodology wherein the global mode and local model which is extracted from the previous epoch were frozen. The objective was to train the generated data with the aim of optimizing the disparity between predictions made by the global model and those made by the local model, all the while mitigating any potential impact on the overall accuracy of the global model. All goals are evaluated on the generated dataset. Unlike current works focusing on the model, this paper provides a novel perspective from the local dataset. By enhancing the distribution of the local dataset, it claims to achieve better convergence. It achieved relatively higher accuracy on public datasets (FLAIR, Fashion-MNIST, CIFAR-10, and CIFAR-100) with different degrees of heterogeneity compared with federated learning (FL) algorithms like FedProx."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The novel proposed method has a low overhead on each client, making it easy to apply in general FL tasks.\n- Extensive experiments have been done to prove the advantages of the applied method.\n- The paper is well organized and it's easy to follow."
                },
                "weaknesses": {
                    "value": "- As far as I know, an enormous amount of work has proved that in the vision task, introducing unbalanced label distribution will influence the performance of the global model, and according to the results of the experiments, it's possible that this empirical idea is true. For details, please refer to the detailed comment C1.\n- Evaluation is not strong enough; For details, please refer to the detailed comment C1~C3.\n- No analysis of convergence is provided. For details, please refer to the detailed comment C3.\n\nDetailed comments:\n- C1 In the experiment results, the final accuracy on CIFAR-10 is relatively low, please try some more complicated networks other than the 5-layer CNN. \n- C2 It's possible the network is not converged. To eliminate such a possibility, please provide a graph depicting the trend of convergence with the number of rounds on the server side. \n- C3 What's more, the proposed method only achieved a little improvement in accuracy, it's not sure whether it's caused by insufficient experiments, please repeat and provide mean and standard error for all results.\n- C4 We kindly request further experimentations involving the generation of datasets of varying sizes, with corresponding meticulous documentation of the associated overhead. Furthermore, if feasible, we encourage experimentation on datasets comprising high-resolution images uniformly, to enhance the comprehensiveness of the analysis.\n- C5 Please add proofs for the convergence analysis. If possible, please add a formal security analysis to your method."
                },
                "questions": {
                    "value": "1. This paper introduced data distribution from other clients, will this cause privacy leakage, making it easier for the attacker to learn data information from the clients? \n\n2. Will generating new data for each client be identical to amplifying the global weight update direction collected in the last epoch?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6970/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6970/Reviewer_SDr1",
                        "ICLR.cc/2024/Conference/Submission6970/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698935798627,
            "cdate": 1698935798627,
            "tmdate": 1700715831094,
            "mdate": 1700715831094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2CFKJGnuaO",
                "forum": "NY3wMJuaLf",
                "replyto": "kZ2jlsK9Ui",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and valuable suggestions. Here are our detailed responses.\n\n---\n\n**W1:** Please try some more complicated networks other than the 5-layer CNN.\n\n**Response:** Thanks for the advice. Here, we run experiments on ResNet18. To demonstrate the effectiveness of our method, we only run one round of FedCOG.\n\nFrom the table below, we see that the accuracy on NIID-1 is much higher now while the accuracy on NIID-2 (more heterogeneous) is still similar to previous results. Nevertheless, our method still demonstrates evident effectiveness.\n\n[**Table R1.** Results on larger models.]\n| Setting | FedAvg | FedAvg+FedCOG | FedProx | FedProx+FedCOG |\n|:-------:| :----: | :-----------: | :-----: | :------------: |\n| NIID-1  | 77.64  | **78.34**     | 76.06   | **77.39**      |\n| NIID-2  | 44.81  | **48.34**     | 54.08   | **55.50**      | \n\n---\n\n**W2:** Please provide a graph depicting the trend of convergence with the number of rounds on the server side.\n\n**Response:** Thanks for your advice, we include the convergence figures in Figure 6 in the revision. We show that the network has converged, and our method achieves the best performance.\n\n---\n\n**W3:** Please repeat and provide mean and standard error.\n\n**Response:** Thanks for the advice. Please note that:\n\n(1) In Table 1, we have reported mean and standard error, which shows that our method can significantly and consistently outperform other methods.\n\n(2) Sorry for the confusion. For other results, the reported values are mean values of 3-5 trials, and we did not include standard error due to space limits, such that we could show more comparisons.\n\nHere we list several examples.\n\n[**Table R2.** Mean and std.]\n| Method |     FMNIST-1     |     FMNIST-2     |    CIFAR10-1     |    CIFAR10-2     |\n|:------:|:----------------:|:----------------:|:----------------:|:----------------:|\n| FedAvg | 73.07 $\\pm$ 0.08 | 64.11 $\\pm$ 0.78 | 64.36 $\\pm$ 0.11 | 50.55 $\\pm$ 0.45 |\n| FedCOG | **77.34** $\\pm$ 0.07 | **73.68** $\\pm$ 0.38 | **64.83** $\\pm$ 0.12 | **54.00** $\\pm$ 0.84 | \n\n---\n\n**W4-1:**  Request further experimentations involving the generation of datasets of varying sizes, with corresponding meticulous documentation of the associated overhead.\n\n**Response:** Thanks for the helpful advice! We now include a table to demonstrate the relationship between generation time and number of generated samples in Table 11 in the revision. For convenience, we also put the table here. We can see that the generation time is little (we generate 256 samples throughout the paper), indicating the efficiency of our method. As a reference, the conventional local training takes 14.81 seconds.\n\n[**Table R3.** Generation time (seconds) v.s. number of generated samples.]\n| Number of generated samples  | 16   | 32   | 64   | 128  | 256  | 512  | 1024 |\n| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |\n| Generation time (s) | 0.72 | 0.78 | 0.79 | 0.80 | 0.81 | 0.83 | 1.27 | \n\n**W4-2:** If feasible, we encourage experimentation on datasets comprising high-resolution images.\n\n**Response:** Please note the images in the FLAIR dataset (Table 2) have a high resolution of 256\\*256."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141771525,
                "cdate": 1700141771525,
                "tmdate": 1700141771525,
                "mdate": 1700141771525,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QCahXQxtUV",
                "forum": "NY3wMJuaLf",
                "replyto": "kZ2jlsK9Ui",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W5:** Please add proofs for the convergence analysis. If possible, please add a formal security analysis to your method.\n\n**Response:** \n\nHere, we provide convergence analysis and some interpretations. \n\n**1. Convergence:**\n\nPlease refer to Section A.6 in the Appendix in the revision. We show that our method has a **theoretical convergence guarantee besides achieving pleasant performance**.\n\n**2. Interpretations:**\n\nFrom standard FL theoretical literature, it is well-known that lower dissimilarity contributes to faster convergence both empirically and theoretically [2,3]. At the same time, our method contributes to **lower model difference**, which corresponds to lower dissimilarity among clients in the standard FL theoretical literature (e.g., bounded dissimilarity in [1,2]); please refer to evidence in Figure 2 (a). \n\nThese two key observations can provide implications and evidence for the convergence of our method.\n\nPlease also note that the focus of our paper is to improve the performance of federated learning from a practical view and our contribution lies on proposing a new attempt on tackling data heterogeneity but not theory. Thus, we did not include a convergence analysis previously. \n\n[1] Karimireddy, Sai Praneeth, et al. \"Scaffold: Stochastic controlled averaging for federated learning.\" International conference on machine learning. PMLR, 2020.\n\n[2] Wang, Jianyu, et al. \"Tackling the objective inconsistency problem in heterogeneous federated optimization.\" Advances in neural information processing systems 33 (2020): 7611-7623.\n\n[3] Li, Tian, et al. \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n---\n\n**Q1:** This paper introduced data distribution from other clients, will this cause privacy leakage, making it easier for the attacker to learn data information from the clients?\n\n**Response:** Sorry for the potential confusion. We would like to emphasize that **our method does not introduce data distribution from other clients, which does not cause privacy leakage**. The shared information is exactly the same as FedAvg: the local model. No information about distribution is required for sharing.\n\n---\n\n**Q2:** Will generating new data for each client be identical to amplifying the global weight update direction collected in the last epoch?\n\n**Response:** We believe that generating new data for each client is not equivalent to amplifying the global weight update direction. The reasons are twofold. On one hand, this can be supported by the results of FedAvgM, which introduces momentum-based global update for each round. On the other hand, we keep the number of local model updates the same for all methods, making sure that all methods are with the same update scale.\n\n---\n\nOverall, we hope that our responses can fully address your concerns and will be grateful for any feedback."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141858598,
                "cdate": 1700141858598,
                "tmdate": 1700141889730,
                "mdate": 1700141889730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9NcDn0EQ9b",
                "forum": "NY3wMJuaLf",
                "replyto": "kZ2jlsK9Ui",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer:\n\nThanks again for the valuable comments.\n\nWe have now provided more clarifications, explanations, and experiments to address your concerns. We also included the convergence analysis as requested by you and followed the advice of all reviewers to improve our paper.\n\nPlease kindly let us know if anything is unclear. We truly appreciate this opportunity to improve our work and shall be most grateful for any feedback you could give to us."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586049135,
                "cdate": 1700586049135,
                "tmdate": 1700586049135,
                "mdate": 1700586049135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xTgaq50jsJ",
                "forum": "NY3wMJuaLf",
                "replyto": "kZ2jlsK9Ui",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely anticipate your feedback as the Discussion stage will end in 18 hours."
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe have followed your advice to improve our work including more experiments and convergence analysis. As Discussion Stage will end in 18 hours, we would be grateful if you could check our responses and reconsider your rating.\n\nBest regards,\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677561357,
                "cdate": 1700677561357,
                "tmdate": 1700677561357,
                "mdate": 1700677561357,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XNptqmgJtl",
                "forum": "NY3wMJuaLf",
                "replyto": "xTgaq50jsJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_SDr1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_SDr1"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's response and adjusted my rating accordingly."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715964651,
                "cdate": 1700715964651,
                "tmdate": 1700715964651,
                "mdate": 1700715964651,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cG2gpTFzko",
            "forum": "NY3wMJuaLf",
            "replyto": "NY3wMJuaLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6970/Reviewer_wN99"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6970/Reviewer_wN99"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed FedCOG, a Federated Learning (FL) algorithm that facilitates learning via augmented data generated from the global model, which is later used for knowledge distillation between the global and client models. This scheme is compatible with most existing FL algorithms. Its effects have been empirically verified on real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "\\+ This paper tackles a crucial challenge in FL which is Data heterogeneity. Their core idea of data correction for achieving global data consensus is well-motivated.\n\n\\+ Data generation by capturing the residual knowledge between the global and the client model is novel.\n\n\\+ Sensitivity analysis is well designed and conducted.\n\n\\+ This paper is clearly written. Related work is comprehensive."
                },
                "weaknesses": {
                    "value": "\\- Data generation on the client step brings extra computation workload compared with classic FL or FL with data generation on the server side.\n\n\\- The method seems to be designed purely for vision tasks. Authors could discuss if the proposed method can be extended to scenarios with other input modalities, such as text inputs.\n\n\\- This paper would further benefit from theoretical derivations to interpret why generated data on the client side helps in improving global model performance.\n\n\\- Concerns on Experiments: All methods in Table 1 achieve notably lower accuracies than SOTA FL methods. I am concerned that the model arch, communication round, or optimizer setting is not well set up for appropriate comparison."
                },
                "questions": {
                    "value": "\\- Since tackling data heterogeneity is the key of this paper, I suggest authors conduct more experiments on data with Dirichlet distribution by varying the hyper-parameter $\\beta$. More results with changing heterogeneities would further validate the effects of the proposed methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699025049402,
            "cdate": 1699025049402,
            "tmdate": 1699636814441,
            "mdate": 1699636814441,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Esvpeqejwf",
                "forum": "NY3wMJuaLf",
                "replyto": "cG2gpTFzko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and valuable suggestions. Here are our detailed responses.\n\n---\n\n**W1:** Data generation on the client step brings extra computation workload compared with classic FL or FL with data generation on the server side.\n\n**Response:** Thanks for you comments. We would like to response from three aspects.\n\n1. **Computation efficiency.** Our method introduces **minor** computation workload compared with FedAvg and is more efficient than many FL baselines. To be more specific, on CIFAR-10, the generation only takes **0.81 seconds**, while as a reference the conventional SGD-based training takes 14.81 seconds (FedAvg). Compared with other baselines, our method only needs to launch one model during local training while baselines such as FedProx needs to launch two models and MOON needs to launch three models.\n\n2. **Compatibility with standard FL protocols.** Our method is compatible with **secure aggregation** [1]. In practice, the secure aggregation is required to further protect the communicated model parameters such that the server only obtains the aggregated information (i.e., model). However, those methods that generate data on the server side require that the server obtains each specific local model from client, making them incompatible with secure aggregation technique. In contrast, the server in our method only needs to obtain the aggregated model, making it compatible with secure aggregation. This is critical as the local model itself may reveal some sensitive information and thus should not be overlooked. \n\n3. **Communication and privacy are two first-order concerns** in FL as pointed by the influential and pioneering paper [2]. Following this spirit, we decided to introduce minor computation cost for improving algorithm utility, which is unavoidable [3].\n\n[1] Bonawitz, Keith, et al. \"Practical secure aggregation for privacy-preserving machine learning.\" proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security. 2017.\n\n[2] Kairouz, Peter, et al. \"Advances and open problems in federated learning.\" Foundations and Trends\u00ae in Machine Learning 14.1\u20132 (2021): 1-210.\n\n[3] Zhang, Xiaojin, et al. \"Trading off privacy, utility and efficiency in federated learning.\" ACM Transactions on Intelligent Systems and Technology (2022).\n\n---\n\n**W2:** Authors could discuss if the proposed method can be extended to scenarios with other input modalities, such as text inputs.\n\n**Response:** That is a good point. Though we primarily focus on vision tasks to verify our idea of tackling data heterogeneity from data perspective, our method can also extend to other input modalities. Here, we elaborate this point on two types of modalities.\n\n(1) For modalities such as continuous signals and images where the raw inputs can be optimized by gradient-based optimization, the pipeline is exactly the same as illustrated in this paper, since we can optimize the inputs in an end-to-end manner.\n\n(2) For modalities such as text, where the raw inputs are harder to be directly optimized, it can be achieved with a slight difference. Since the raw inputs can not be optimized via end-to-end gradient-based optimization, we do not optimize/generate raw inputs (which is discrete) but optimize/generate the intermediate embeddings (which is continuous). For example, suppose the overall pipeline of the model is: [a] discrete inputs are transformed into continous embeddings after going through tokenizing and embedding layers, [b] continuous embeddings go through the downstream task model, [c] the model gives outputs. Then, our data generation process is generating the continuous embeddings at step [b], which are regarded as consensus data."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141429436,
                "cdate": 1700141429436,
                "tmdate": 1700141429436,
                "mdate": 1700141429436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aSSTkGrTZw",
                "forum": "NY3wMJuaLf",
                "replyto": "cG2gpTFzko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W3:** This paper would further benefit from theoretical derivations to interpret why generated data on the client side helps in improving global model performance.\n\n**Response:** \n\nHere, we provide some interpretations and a theoretical convergence analysis of our method.\n\n**1. Interpretations:**\n\nFrom standard FL theoretical literature, it is well-known that lower dissimilarity (between clients) contributes to faster convergence both empirically and theoretically [2,3]. At the same time, our method contributes to lower model difference, which corresponds to lower dissimilarity among clients in the standard FL theoretical literature (e.g., bounded dissimilarity in [1,2]); please refer to evidence in Figure 2 (a). These two key observations can provide implications and evidence for the convergence of our method.\n\n**2. Convergence:**\n\nPlease refer to Section A.6 in the Appendix in the revision. We show that our method has a **theoretical convergence guarantee** besides achieving pleasant performance.\n\nPlease also note that the focus of our paper is to improve the performance of federated learning from a practical view and our contribution lies on proposing a new attempt on tackling data heterogeneity but not theory. Thus, we did not include a convergence analysis previously. \n\n[1] Karimireddy, Sai Praneeth, et al. \"Scaffold: Stochastic controlled averaging for federated learning.\" International conference on machine learning. PMLR, 2020.\n\n[2] Wang, Jianyu, et al. \"Tackling the objective inconsistency problem in heterogeneous federated optimization.\" Advances in neural information processing systems 33 (2020): 7611-7623.\n\n[3] Li, Tian, et al. \"Federated optimization in heterogeneous networks.\" Proceedings of Machine learning and systems 2 (2020): 429-450.\n\n---\n\n**W4:** I am concerned that the model arch, communication round, or optimizer setting is not well set up for appropriate comparison.\n\n**Response:** We would like to emphasize that we follow exactly the same setups as [1,2] including model architecture (CNN), optimizer (SGD), and learning rate (0.01). We set the number of rounds as 70 because we find that after 70 rounds the performance remains nearly the same as the 70th round's.\n\nThe reason why the performance seems low on average is that many methods are not robust towards diverse settings (e.g., datasets and heterogeneity types). For example, SCAFFOLD achieves 4% higher than FedAvg on NIID-1 of Fashion-MNIST but 6% lower than FedAvg on NIID-2 of Fashion-MNIST. \n\n[1] Li, Qinbin, Bingsheng He, and Dawn Song. \"Model-contrastive federated learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n[2] Luo, Mi, et al. \"No fear of heterogeneity: Classifier calibration for federated learning with non-iid data.\" Advances in Neural Information Processing Systems 34 (2021): 5972-5984.\n\n---\n\n**Q1:** I suggest authors conduct more experiments on data with Dirichlet distribution by varying the hyper-parameter beta.\n\n**Response:** Thanks for the advice. Actually, we have included such experiments in Figure 3 (a). Here, we show the results again for convenience. We see that our proposed FedCOG consistently performs the best at different beta.\n\n[**Table R1.** Effects of hyper-parameter beta.]\n| Beta       | 0.1       | 0.5       | 1.0       | 5.0       |\n| ---------- | --------- | --------- | --------- | --------- |\n| FedAvg     | 63.50     | 64.51     | 66.55     | 67.70     |\n| FedProx    | 63.00     | 63.94     | 67.29     | 67.01     |\n| MOON       | 63.28     | 64.09     | 64.07     | 67.78     |\n| **FedCOG** | **64.46** | **65.87** | **67.99** | **69.40** | \n\n---\n\nOverall, we hope that our responses can fully address your concerns and will be grateful for any feedback."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141593100,
                "cdate": 1700141593100,
                "tmdate": 1700141593100,
                "mdate": 1700141593100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aytk1300VO",
                "forum": "NY3wMJuaLf",
                "replyto": "cG2gpTFzko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer:\n\nThanks again for the valuable comments.\n\nWe have now provided more clarifications, explanations, and experiments to address your concerns and followed the advice of all reviewers to improve our paper.\n\nPlease kindly let us know if anything is unclear. We truly appreciate this opportunity to improve our work and shall be most grateful for any feedback you could give to us."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585966749,
                "cdate": 1700585966749,
                "tmdate": 1700585966749,
                "mdate": 1700585966749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FrM9o8so01",
            "forum": "NY3wMJuaLf",
            "replyto": "NY3wMJuaLf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6970/Reviewer_iXGo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6970/Reviewer_iXGo"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on federated learning (FL) in the presence of data heterogeneity. Different from the existing methods which usually consider this data heterogeneity as an inherent property and attempt to mitigate the adverse effects, this paper proposes to handle the heterogeneity by generating new data, called FedCOG. There are two key components in FedCOG, including complementary data generation and knowledge-distillation-based model training. It can be plug-and-play, and naturally compatible with standard federated learning protocols.  Extensive experiments on classical and real-world datasets proved the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method is novel, handling the data heterogeneity from the perspective of the data generation, instead of correcting the model.\n2. The proposed method can be a plug-and-play model, and it is naturally compatible with standard FL protocols."
                },
                "weaknesses": {
                    "value": "1. The motivation for why we need to use data generation, instead of recent popular methods based on model correction, is somewhat not clear. When the training dataset is very large, the proposed method therefore needs to generate a large amount of data in order to achieve alignment, which is costly, then it seems like model correction is a better choice in such a scenario.  \n2. As the paper mentioned, the proposed method FedCOG has two advantages, i.e., plug-and-play and compatibility with standard FL protocols, however, the unique advantages of this data generation method, compared to previous model correction methods, are still ambiguous. Could you please elaborate more regarding them?"
                },
                "questions": {
                    "value": "(see above)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6970/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6970/Reviewer_iXGo",
                        "ICLR.cc/2024/Conference/Submission6970/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699506270623,
            "cdate": 1699506270623,
            "tmdate": 1700591876050,
            "mdate": 1700591876050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mgjFrGgwrP",
                "forum": "NY3wMJuaLf",
                "replyto": "FrM9o8so01",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time and valuable suggestions. Here are our detailed responses.\n\n---\n\n**W1-1:** The motivation for why we need to use data generation, instead of recent popular methods based on model correction, is somewhat not clear.\n\n**Response:** Sorry for the confusion. Our motivations are twofold. \n\n1. **Heterogeneous data is the fundamental cause of performance degradation while model-correction-based methods do not directly operate on data.** Previous model correction methods focus on remedying the negative effects of data heterogeneity by model correction techniques while leaving the dataset as heterogeneous as before. While data is the root of performance degradation, this motivates us to deal with the heterogenous datasets themselves to more fundamentally address the issue of data heterogeneity. We believe that this will be an interesting future direction, especially because there are more and more advanced data generation techniques.\n2. **Merely relying on correction-based methods cannot fully address data heterogeneity,** which motivates us to explore a new direction that addresses this from an orthoganal perspective. We expect to see that in the future the issue of data heterogeneity can potentially be organically addressed by integration of techniques from diverse perspectives.\n\n**W1-2:** The proposed method needs to generate more data when training dataset is large?\n\n**Response:** Sorry for the potential misunderstanding we have caused. Actually, the generated number does not scale up with the number of training samples. Throughout the paper, **the generated number keeps the same (256)** no matter how many samples each client has (e.g., 10k, 5k). Note that the required generation time is only **0.81 seconds** (on CIFAR-10) since the only learnable parameters are the inputs while the models are fixed, and **only 100 steps** of optimization is sufficient.\n\nHere is the rationale. In FL, the number of local updates is strongly related to convergence, thus it is set to a fixed moderate number no matter how many data samples the client has. This means that for each round, the number of samples used for updating the model is the same no matter how many data samples the client has (number of local updates multiplied by batch size). Thus, the generated number for each round can also be set as the same number no matter how many data samples the client has.\n\nBesides, our method is **actually efficient** because our method only needs to launch **one model** during local model training (the soft labels from global model is obtained for only **one-time inference** and only the local model needs to be launched). On the contrary, for example, during local model training, FedProx needs to launch **two models** (one local model and one global model); MOON needs to launch **three models** (two local models and one global model); SCAFFOLD needs to launch **three models** (one local model, one local control variate and one global control variate).\n\n---\n\n**W2:** The unique advantages of this data generation method, compared to previous model correction methods.\n\n**Response:** The unique advantage of our method is that our method not only more fundamentally alleviates the heterogeneity level of clients' data, but also enhance alignment among local models at the same time. \n\nFedCOG consists of two key parts: data generation and knowledge distillation, which organically alleviate the issue of data heterogeneity. (1) First, the generated consensus data can effectively decrease the difference among clients' datasets, therefore reducing the data heterogeneity level to more fundamentally alleviate this issue. (2) Second, based on the consensus data, we force each local model's outputs to align with global model's outputs, which functions to implicitly align local models like correction-based methods. Besides, unlike correction-based methods that need to launch several models, our method only needs to launch one local model during training as the soft labels of consensus data can be obtained through one-time inference, which is efficient.\n\nPlease note that besides the technical contribution, our contribution also lies on exploring a **novel perspective of dealing with data heterogeneity**, which can potentially inspire more future works to effectively address data heterogeneity.\n\n---\n\nOverall, we hope that our responses can fully address your concerns and will be grateful for any feedback."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700141283564,
                "cdate": 1700141283564,
                "tmdate": 1700141283564,
                "mdate": 1700141283564,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D4knYOxCWE",
                "forum": "NY3wMJuaLf",
                "replyto": "FrM9o8so01",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer:\n\nThanks again for the valuable comments.\n\nWe have now provided more clarifications and explanations to address your concerns and followed the advice of all reviewers to improve our paper.\n\nPlease kindly let us know if anything is unclear. We truly appreciate this opportunity to improve our work and shall be most grateful for any feedback you could give to us."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585893172,
                "cdate": 1700585893172,
                "tmdate": 1700585893172,
                "mdate": 1700585893172,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qp2JYmflry",
                "forum": "NY3wMJuaLf",
                "replyto": "D4knYOxCWE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_iXGo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6970/Reviewer_iXGo"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for the responses, which addressed my concerns, therefore, I updated my rating to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591851119,
                "cdate": 1700591851119,
                "tmdate": 1700591851119,
                "mdate": 1700591851119,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]