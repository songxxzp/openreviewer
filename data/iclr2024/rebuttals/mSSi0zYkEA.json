[
    {
        "title": "Initializing the Layer-wise Learning Rate"
    },
    {
        "review": {
            "id": "FyVLUY39QA",
            "forum": "mSSi0zYkEA",
            "replyto": "mSSi0zYkEA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8568/Reviewer_R9Q4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8568/Reviewer_R9Q4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to set learning rates for each parameter individually in neural networks.\nThese learning rates are computed from the reciprocal of the gradient magnitude for each parameter at initialisation time.\nExperiments on ImageNet and CIFAR-100 show promising results that confirm the hypothesis that learning rate initialisation can speed up training significantly."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- (clarity) The method and results are presented clearly.\n - (significance) Speeding up SGD with a simple learning rate initialisation could be a cost-effective alternative to adaptive optimisation algorithms."
                },
                "weaknesses": {
                    "value": "- (clarity) Layer-wise learning rates, which are probably also addressed in the first edition of tricks of the trade (1998), were probably the only way to make deep networks trainable.\n   The main advantage of adaptive optimisers has always been that the painfull process of finding learning rates for each layer is no longer necessary.\n - (clarity) It is unclear how important the choice for $T$ in algorithm 1 is.\n   In the experiments, $T$ is the number of batches in one epoch, but there are no ablations for different choices of $T$. \n - (originality) This paper fails to mention its relation to Adagrad (Duchi et al., 2011).\n   This is especially relevant because Adagrad can also be interpreted as dividing the learning rate by a running average of the norm.\n - (originality) Algorithm 1 looks a lot like running an adaptive optimiser with a learning rate of zero for a number of mini-batches.\n   This connection is completely ignored in the current manuscript.\n - (quality) In this work, the hyper-parameters seem to be shared for the different methods.\n   For a proper comparison, hyper-parameters should be tuned for each method individually.\n - (significance) The inspection of the assigned learning rates seem to provide more information about the model than about the method.\n   I think these could provide more information when compared to the learning rates of adaptive optimisation algorithms.\n - (significance) The experimental setup is too complex to properly evaluate the merits of the proposed method.\n   By evaluating on these large models, confounding factors like learning rate schedules become necessary, making it hard to evaluate the generality of the method.\n   Furthermore, these large models typically make it impractical to provide error bars and establish the statistical significance of the presented results.\n\n### Minor Comments\n - There are quite a number of typos (e.g.: touse in abstract, $n$ on line 4 of algorithm 1)\n - Technically, the learning curves of the layer-wise learning rates should be shifted by one epoch, since they are one epoch ahead of the single learning rate baselines.\n\n### References\n\n - Duchi, J., Hazan, E., & Singer, Y. (2011). \n   Adaptive subgradient methods for online learning and stochastic optimization. \n   Journal of machine learning research, 12(7). https://www.jmlr.org/papers/v12/duchi11a.html"
                },
                "questions": {
                    "value": "1. Please, rewrite the motivation to better reflect the historical evolution of adaptive optimisation methods.\n 2. How does this method relate to Adagrad and other adaptive optimization methods?\n 3. Is it possible to include simple experiments (cf. Kingma et al., 2015) with error bars?\n 4. How much does the learning rate schedule affect the performance of the proposed method?\n 5. Is it possible to include a run where Adagrad (or other adaptive methods) iterates the data for one epoch with learning rate zero.\n 6. How does the setting above compare to the baseline performance and the proposed layer-wise learning rate?\n 7. How important is the choice for $T$ and does this relate in some way to learning rate warmup?\n 8. Can you tune the hyper-parameters (most notably the learning rate) for each algorithm individually to provide a fair method comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698314375806,
            "cdate": 1698314375806,
            "tmdate": 1699637071574,
            "mdate": 1699637071574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qWJNayO9vO",
                "forum": "mSSi0zYkEA",
                "replyto": "FyVLUY39QA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer R9Q4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable review and feedback\n\n> Relation to adaptive optimisation methods\n\nWe believe that explicit layer-wise learning rates has an effect that is different and mostly independent from adaptive optimization methods. Figure 8 in the Appendix of the updated draft shows the adaptive step size of AdamW $1/(\\sqrt{\\hat{v}_{t}}+\\epsilon)$ when training ResNet-50 for 90 epochs with single and the proposed layer-wise learning rate. The adaptive step sizes show a low to high trend according to layer depth on both single and the proposed method. If the proposed method was an cost-effective alternative to adaptive optimisation algorithms we believe the adaptive step sizes should have flattened out. \n\nWe would like to note that the observation layer-wise learning rates can improve training was on experimental settings where adaptive methods are known to achieve lower validation accuracy. We have updated the draft to better reflect the relation with adaptive methods.\n\n> Ablation study on $T$ of Algorithm 1\n\nWe performed an ablation study on $T$ when training ResNet-50 for 90 epochs with SGD by measuring the deviation of per-layer assigned learning rates compared to the $T$ used in the paper. We report the results below and have included it in the appendix.\n\n$T$ | Max deviation % | Average deviation % | Validation accuracy %|\n-----------   | :-------------:| :-------------:| :-------------:|\n1    | 11.02            | 2.66                 | 77.07               | \n10   | 3.51             | 0.69                 |  77.01              | \n100  | 1.01             | 0.18                 |   77.04             | \n1000 | 0.32             | 0.06                 |   77.07             |\n5004 | 0               | 0                     |    77.15             |\n\n> Additional results with multiple runs\n\nWe provide additional results that reinforce the claim that the proposed method has a different effect compared to adaptive optimization methods. We report the result of training ResNet-50 with AdamW for 90 epochs below. While we used a T of 5004, the ablation study of T above showed the choice of T can be as low as 100 and is not significant, and for single learning rate we additionally iterate with learning rate of 0 for 100 iterations. We report the average and standard deviation of three runs below, and it can be seen that layer-wise learning rates have favorable performance even when using adaptive optimization methods. \n\nInitial LR | Single | Layer-wise | \n-----------   |  :-------------:| :-------------:| \n0.0015 | **75.96\u00b10.11**    | 75.45\u00b10.16 | \n0.003 |  76.45\u00b10.07 | **76.61\u00b10.15** | \n0.006 |  75.93\u00b10.10 | **77.00\u00b10.23** | \n\n> Relation with warmup\n\nWe consider learning rate warmup to be largely independent and not a setup to be replaced or removed. Our experience is that it is beneficial to both single and layer-wise learning rate, and all experiments are performed with learning rate warmup.\n\n> Regarding the complex experiment setup and confounding factors\n\nWhile we agree that the experimental setup can be affected by confounding factors such as learning rate schedule and warmup, ImageNet-1k classification has been extensively studied with established baselines and reported hyperparameters. We believe evaluating on such setups is effective in demonstrating methods that work on large datasets and architectures."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643692577,
                "cdate": 1700643692577,
                "tmdate": 1700643692577,
                "mdate": 1700643692577,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nQ6wPZfmSV",
            "forum": "mSSi0zYkEA",
            "replyto": "mSSi0zYkEA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8568/Reviewer_cEsi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8568/Reviewer_cEsi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to assign a learning rate to each layer. This layer-wise learning is computed by using the norm of the backpropagated gradients. Basically, the learning rate of assigned to a layer $l$ is inversely proportional to the square root of the $\\mathcal{L}^1$-norm of the backpropagated gradient (according to the tensor of weights of $l$).\n\nTo evaluate their heuristic, the authors run two series of experiments: one with a single learning rate, and one with their heuristic. The tested setups include two optimizers: SGD and AdamW. In each tested case, the reported performance is greater with their method than with theit single learning rate counterparts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "## Originality\n\nTo my knowledge, the proposed heuristic for a layer-wise learning rate is new.\n\n## Clarity\n\nOverall, the proposed method is easy to understand.\n\n## Quality\n\nThe authors have well explained (introduction of Section 3) why one should look for a working and well-justified heuristic for layer-wise learning rates, computed *before* training. This problem deserves to be studied, both from a practical and a theoretical point of view.\n\nThe experimental results are very encouraging."
                },
                "weaknesses": {
                    "value": "## Clarity\n\nThe idea behind Algorithm 1 is easy to understand, but several details are missing or seem to be erroneous:\n * line 4: replace $n$ by $t$;\n * apparently, the $G_i$ are incremented $T$ times, but they are not normalized by $T$ or any other quantity depending on $T$. So, two questions arise: how do we choose $T$? Or should we normalize the $G_i$ somewhere?\n\nMore importantly, many choices in Algorithm 1 are not explained by the authors:\n * line 8: why do the authors use the $\\mathcal{L}^1$-norm over $\\mathbf{g}$, and not the $\\mathcal{L}^2$-norm or any other norm?\n * line 10: why choosing the inverse of the square root of $G_i$, and to the inverse of $G_i$ or any other quantity?\n * lines 11-13: what is the justification for such a computation?\n\nOverall, Algorithm 1, which describes the entire method proposed by the authors, is incomplete and lacks justification. This crucial weakness can be solved by adding subsections in Section 1, proving mathematically all the choices made in Algorithm 1 (at least in simple cases). Otherwise, these choices remain arbitrary."
                },
                "questions": {
                    "value": "Could the authors provide at least a short analysis of their method in simple cases, or in extreme cases (layer size tending to infinity)? It would be interesting to observe what happens at the first training step.\n\nWhat do the authors think about the paper *Neural tangent kernel: Convergence and generalization in neural networks*, Jacot et al., 2018? In this paper, each weight tensor is scaled by $1/\\sqrt{f_{\\text{in}}}$. This setting, combined with a unique learning rate for all layers, is equivalent to the \"normal\" setting (without scaling) with a learning rate per layer, proportional to $1/f_{\\text{in}}$. How the learning rates computed by the authors compare to these?\n\nExperimental results: are the results consistent when we change the learning rate? Does the proposed method perform better than the \"single lr method\" in any circumstance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8568/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8568/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8568/Reviewer_cEsi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698876581678,
            "cdate": 1698876581678,
            "tmdate": 1700036243179,
            "mdate": 1700036243179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SNpZBEYlfj",
                "forum": "mSSi0zYkEA",
                "replyto": "nQ6wPZfmSV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cEsi"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable review and feedback\n\n> Details on Algorithm 1\n\nFor $T$ we find a value of 100 or higher is sufficient to obtain resonable values. $G_{i}$ is incremented $T$ times in line 8, and is used to calculate the relative learning rates in line 10. After obtaining the relative values, the normalization is performed on lines 11-13 such that the average per-parameter learning rate is one.\n\n> Choice of $\\mathcal{L}^1$ over $\\mathbf{g}$ and inverse of the square root of $G_i$\n\nAlgorithm 1 is developed with the interpretation that the gradient magnitude itself is a objective to be minimized, so Algorithm 1 uses the gradient magnitude directly instead of performing $\\mathcal{L}^2$-norm. That said, we find using $\\mathcal{L}^2$-norm over the per-parameter gradient instead of $\\mathcal{L}^1$-norm assigns very similar layer-wise learning rates, with an average deviation of 0.97%, and achieves similar performance of 77.24% on ResNet-50 when trained with SGD for 90 epochs.\n\nThe square root over $G_i$ in line 10 is performed because without it we found the difference in scale of assigned learning rates varies widely and results in drastically reduced performance of 75.99%.\n\n> Comparison to $1/f_{\\text{in}}$ learning rate scaling\n\nOn ResNet-50, we find $1/f_{\\text{in}}$ tends to assign assigns higher learning rates to initial layers and lower learning rates to later layers, which is directly opposite to the proposed method. Under the same unit per-parameter normalization of line 11-13 in Algorithm 1, $1/f_{\\text{in}}$ scaling assigns 6.295 to the first 7*7 convolution layer, and 14.459 to the directly succeding convolution layer, which is in stark contrast to the 0.038 and 0.109 assigned by Algorithm 1 in Figure 4. \n\nFor ViT-S/16, $1/f_{\\text{in}}$ scaling assigns identical learning rates to all the query, key and value layers in the self attention block, while the proposed method assigns higher learning rates to the query and key layers. We believe the practice of normalizing query and key layers in self attention [1][2] further validates the empirical significance of our method.\n\n[1] Henry, Alex, et al. \"Query-Key Normalization for Transformers.\" Findings of the Association for Computational Linguistics: EMNLP 2020. 2020.\n\n[2] Dehghani, Mostafa, et al. \"Scaling vision transformers to 22 billion parameters.\" International Conference on Machine Learning. PMLR, 2023.\n\n> Results under different learning rates\n\nWe provide additional results when training ResNet-50 with half and double the base learning rate for 90 epochs with SGD below. It shows that the proposed method performs well over various learning rates.\n\nInitial LR | Single | Layer-wise | \n-----------   |  :-------------:| :-------------:| \n0.05 | 76.61    | **76.67** | \n0.1 | 76.80    | **77.12** | \n0.2 | 76.33    | **76.90** |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643686639,
                "cdate": 1700643686639,
                "tmdate": 1700643686639,
                "mdate": 1700643686639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oVKvoty9ru",
                "forum": "mSSi0zYkEA",
                "replyto": "SNpZBEYlfj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8568/Reviewer_cEsi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8568/Reviewer_cEsi"
                ],
                "content": {
                    "comment": {
                        "value": "My main concern about this paper remains the absence of a section where the choices made by the authors in Algorithm 1 are grounded by some theoretical evidence (with a theorem or a heuristic, even in a very simple case). \n\nMy argument is: it is always possible to invent some method adding several hyperparameters, fine-tune these parameters by an expensive try-and-error process in a small number of configurations, and beat some baseline in these configurations. It is neither surprising nor useful to beat the baseline (by a narrow margin) by adding hand-tuned hyperparameters. Since the benefits of the proposed algorithm are not outstanding in the proposed setups, at least a theoretical explanation is needed.\n\n> Algorithm 1 uses the gradient magnitude directly instead of performing $\\mathcal{L}^2$-norm\n\nApparently, there is a lack of clarity when using the term \"gradient magnitude\". In many of the papers cited (Zhang 2020b, Yu 2017, Balles 2018, etc.), \"gradient magnitude\" either refers explicitly to the $\\mathcal{L}^2$-norm, either means informally \"the size of the gradient\", without being specific. In any case, \"gradient magnitude\" does not mean unambiguously \"$\\mathcal{L}^1$-norm\""
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648931762,
                "cdate": 1700648931762,
                "tmdate": 1700648931762,
                "mdate": 1700648931762,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aAe9dYqg4r",
            "forum": "mSSi0zYkEA",
            "replyto": "mSSi0zYkEA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8568/Reviewer_LTJF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8568/Reviewer_LTJF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a systematic layer-wise learning rate adjusting scheme according to the layer-wise gradient magnitude at initialization, improving training performance and stability on convolutional and transformer architectures. Competitive results on convolutional and transformer architectures on CIFAR100 and ImageNet-1k validate the proposed hypothesis."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method is easy to understand.\nThe experiment results are convincing, removing the fluctuations and improving accuracy."
                },
                "weaknesses": {
                    "value": "1. Lots of typos and confusing statement. Such as \"Figure 7: Train loss for 2-layer MLP trained on CIFAR-10 trained.\", Sec.2: \"Another direction previous works indirectly modified the learning rate is through the use of scale factors...\"\n2. In Algorithm 1, the reason of choice of T and corresponding ablation study is missing which may be vital to the performance of the proposed algorithm.\n3. The novelty is limited; more theoretical analyses are needed. \n4. Related works are not clear enough."
                },
                "questions": {
                    "value": "1. What is the motivation of proposed Algorithm 1?\n2. Why layer-wise learning rate scheme performs not so good on Swin-T and ConvNeXt-T when using AdamW? ResNet-50 and SGD are no longer mainstream models or algorithms in 2023. What are the impacts of proposed algorithms on different model structures and modules?\n3. Figure 6 is not intuitive, even seems that single way is better than proposed methods.\n4. More ablation studies are needed to validate the influence of different hyper-parameters in Algorithm 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8568/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8568/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8568/Reviewer_LTJF"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699328483272,
            "cdate": 1699328483272,
            "tmdate": 1699637071286,
            "mdate": 1699637071286,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vJnSqqcv7y",
                "forum": "mSSi0zYkEA",
                "replyto": "aAe9dYqg4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer LTJF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable review and feedback\n\n> Motivation of proposed Algorithm 1\n\nAlgorithm 1 adjusts the layer-wise learning rate to regularize architecture-induced convergence bias, measured as the gradient magnitude at initialization. It is motivated from an observation that layer-wise learning rate can further improve training of SGD in settings where adaptive methods are considered to have lower validation accuracy due to higher generalization gap.\n\n> Effect and performance of different architectures\n\nInspecting the assigned learning rates show that the convergence bias of Swin-T and ConvNeXt-T resembles that of vision transformer more than convolutional networks. While there could be various factors, we think lack of hyperparameter tuning could be a major reason for the lack of performance gain on Swin-T and ConvNeXt-T when using AdamW. Given how sensitive performance is to learning rates, we believe achieving competitive performance when learning rates of layers can differ by an order of magnitude is still a very interesting phenomenon.\n\nSwin-T and ConvNeXt-T are complex architectures that incorporate design choices of both convolutional and transformer architectures. Swin-T incorporates hierarchical feature maps which is similar to feature map hierarchy in convolutional networks, and ConvNeXt-T stems from introducing vision transformer designs on convolutional networks. We mainly focus on ResNet and vision transformer as they are representative of the two main architecture family in vision tasks, and we believe performance on SGD is a good indication of training stablity and generalizability. \n\n> In Figure 6, single seems better than proposed method\n\nIn terms of final accuracy we agree Figure 6 shows using a single learning rate is better in such settings. However CIFAR-100 differs widely from ImageNet-1k in terms of dataset size, resolution and difficulty, and we believe methods that are beneficial on larger dataset size and difficulty is of interest even if it isn't necessarily beneficial on smaller tasks.\n\n> Ablation studies to validate the influence of hyper-parameters in Algorithm 1\n\nWe performed an ablation study on $T$ when training ResNet-50 for 90 epochs with SGD by measuring the deviation of per-layer assigned learning rates compared to the $T$ used in the paper. We report the results below and have included it in the appendix.\n$T$ | Max deviation % | Average deviation % | Validation accuracy %|\n-----------   | :-------------:| :-------------:| :-------------:|\n1    | 11.02            | 2.66                 | 77.07               | \n10   | 3.51             | 0.69                 |  77.01              | \n100  | 1.01             | 0.18                 |   77.04             | \n1000 | 0.32             | 0.06                 |   77.07             |\n5004 | 0               | 0                     |    77.15             |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643676722,
                "cdate": 1700643676722,
                "tmdate": 1700643676722,
                "mdate": 1700643676722,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5WbAYNijfQ",
            "forum": "mSSi0zYkEA",
            "replyto": "mSSi0zYkEA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8568/Reviewer_5Czw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8568/Reviewer_5Czw"
            ],
            "content": {
                "summary": {
                    "value": "This paper focused on the problem about initialization method of layer-wise learning rate. The authors use gradient magnitude as a\nmeasure of architecture-induced convergence bias. Based on that, they try to adjust the layer-wise learning rate opposite to its gradient magnitude at initialization. The experimental results illustrate that the proposed initialization method can obtain a better performance on CIFAR, ImageNet."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper focus on an important problem. In my past experience about neurwal network training, layer-wise learning rate is very sensitive to the initialization method of each layer. For example, LAMB use the ratio between weight norm and gradient norm to determine the layer-wise learning rate. \n2. The proposed method is very easy to understand. We can estimate gradient magnitude and then determine the layer-wise learning rate."
                },
                "weaknesses": {
                    "value": "1. I'm not sure whether the proposed method can scale. Although the proposed method and intuition are easy to understand, the method is still not simple enough. So that make me consider the performance when we scale to a very large model, such as a language model, and whether this can be a general method. I know this is very difficult and I'm just considering. If possible, you could provide some results on NLP task. \n2.  You need to compare the proposed method with more layer-wise optimization method, such as LARS and LAMB. I noticed that the main baseline is SGD and Adam. and these methods are not layer-wose methods. Although their performance is very strong, I think LRAS / LAMB can also further improve the performance of SGD / Adam. To better illustrate the performance gain of your method, maybe you should provide these results on layer-wise method."
                },
                "questions": {
                    "value": "1. I would like to ask the training cost of Algorithm 1. Since the method need to estimate the gradient magnitude and other methods don't need it. Therefore, I would like to ask the cost, such as time. In addition, the proposed need to use T steps in Algorithm 1 and that means we need more steps to finish the training with the proposed method. If we add these T steps to the baselines, such as SGD and Adam, whether we can further improve their performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699628026570,
            "cdate": 1699628026570,
            "tmdate": 1699637071090,
            "mdate": 1699637071090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bgm9HbAAub",
                "forum": "mSSi0zYkEA",
                "replyto": "5WbAYNijfQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 5Czw"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable review and feedback\n\n> Regarding the simplicity of the method and comparison with LARS, LAMB\n\nWe would like to emphasize that while LARS, LAMB are also layer-wise methods, the underlyring mechanisms and interpretation are very different in that LARS and LAMB relies on per-layer gradient normalization and additional weight norm scaling, while the proposed method is concered with architecture-induced convergence bias. In practice it means the learning rates of LARS and LAMB is insensitive to gradient of other layers, and due to the additional weight norm scaling layers initialized 0 are in principle not updated.\n\nWe have provided results of layer-wise gradient normalization techniques on ResNet-50 with SGD on Table 2. We found that the batchnorm scale and bias layers had to be excluded from LAMB scaling for LAMB to achieve competitive performace, perhaps due to its additional weight norm scaling. In that sense, we believe the proposed method can be considered simpler as the step size remains dependent only the gradient and does not modifiy the optimizer.\n\n> Training cost of Algorithm 1\n\nWe performed an ablation study on $T$ when training ResNet-50 for 90 epochs with SGD by measuring the deviation of per-layer assigned learning rates compared to the $T$ used in the paper. We report the results below and have included it in the appendix. We also ran a baseline experiement of 91 epochs, but found the final validation accuracy to be lower that the reported 76.80%, suggesting that other factors are more dominant compared to a single epoch difference. \n$T$ | Max deviation % | Average deviation % | Validation accuracy %|\n-----------   | :-------------:| :-------------:| :-------------:|\n1    | 11.02            | 2.66                 | 77.07               | \n10   | 3.51             | 0.69                 |  77.01              | \n100  | 1.01             | 0.18                 |   77.04             | \n1000 | 0.32             | 0.06                 |   77.07             |\n5004 | 0               | 0                     |    77.15             |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643669000,
                "cdate": 1700643669000,
                "tmdate": 1700643669000,
                "mdate": 1700643669000,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]