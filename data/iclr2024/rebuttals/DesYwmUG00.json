[
    {
        "title": "The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing"
    },
    {
        "review": {
            "id": "A6I4KwVQI8",
            "forum": "DesYwmUG00",
            "replyto": "DesYwmUG00",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4591/Reviewer_mBr1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4591/Reviewer_mBr1"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a unified probabilistic formulation for diffusion-based image editing and introduces a simple yet effective dragging algorithm based on this formulation. The authors conduct experiments on various tasks, including inpainting, image-to-image translation, and dragging, demonstrating the superiority of their SDE-based approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a comprehensive theoretical analysis of the SDE and ODE formulations for general image editing.\n2. The authors propose the SDE-Drag algorithm for dragging.\n3. The experiments across different tasks, including inpainting, image-to-image translation, and dragging, demonstrate the effectiveness of the SDE formulation in improving image editing tasks compared with the ODE baselines.\n4. The authors also provide the code, which shows the solidness of the work."
                },
                "weaknesses": {
                    "value": "1. While the paper shows that SDE outperforms ODE baselines in inpainting and image-to-image translation, it lacks a comparison with the latest methods in these tasks.\n2. The paper provides the cost time for the dragging task but does not provide similar information for inpainting and image-to-image translation."
                },
                "questions": {
                    "value": "1. It is suggested to compare the performance of SDE in inpainting and image-to-image translation with the latest methods.\n2. Provide the running time of their proposed methods for inpainting and image-to-image translation tasks. And including the running time of the latest methods in these tasks to provide a more complete picture of the computational efficiency."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4591/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4591/Reviewer_mBr1",
                        "ICLR.cc/2024/Conference/Submission4591/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4591/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697698759181,
            "cdate": 1697698759181,
            "tmdate": 1700657725792,
            "mdate": 1700657725792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0aBYlX6gVV",
                "forum": "DesYwmUG00",
                "replyto": "A6I4KwVQI8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mBr1"
                    },
                    "comment": {
                        "value": "We thank reviewer mBr1 for the acknowledgment of our contributions and the valuable comments. We respond below to your questions and concerns.\n\n## Weakness1: Comparison with the latest methods.\n\n\nThank you for your valuable comment. The preliminary goal of the inpainting and image-to-image translation experiments is to show the versatility of SDE in general image editing, instead of improving the latest methods. Our dragging experiment against the latest baselines is able to show the effectiveness of the SDE formulation. Besides, based on the results, we believe that the SDE formulation is orthogonal to other improvements for a specific task and applies to the latest ODE-based methods.\n\nMoreover, in our submission, we conduct experiments on recent work including SD (CVPR 2022), DiffEdit (ICLR 2023), DDIB (ICLR 2023), and DragDiffusion (arXiv 2023.06). Nevertheless, if you would like to point out specific papers, we are happy to discuss the relation and will try to do the experiments.\n \n\n## Weakness2: Costing time for inpainting and image-to-image translation tasks.\nThank you for your valuable question. In the last paragraph (on page 9) of Sec. 5.4 in our submission, we introduce that the SDE counterpart takes nearly the same time as the direct ODE baseline in all experiments. To more clearly demonstrate the time efficiency, we list the cost time per image for all inpainting and image-to-image translation experiments here and we will add it to the revision as soon as possible.\n\n\n|       | Inpainting | DiffEdit   | DDIB                   |\n|-------|------------|------------|------------------------|\n| SDE   | 3.06s      | 4.82s      | 299.20s                |\n| ODE   | 3.11s      | 4.86s      | 300.18s                |\n| Device| NVIDIA A100 | NVIDIA A100 | NVIDIA GeForce RTX 3090|\n\n\n\nFrom the table, we can see that the time efficiency of both SDE and ODE mainly depends on the number of function evaluations, which are the same using the same sampling strategy. Therefore, when applied to the latest ODE baselines, SDE-based methods would have a similar time cost. Also see the discussion on comparison with the latest methods in our response to your weakness1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189286021,
                "cdate": 1700189286021,
                "tmdate": 1700189286021,
                "mdate": 1700189286021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m3Q7OpDqTM",
                "forum": "DesYwmUG00",
                "replyto": "0aBYlX6gVV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Reviewer_mBr1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Reviewer_mBr1"
                ],
                "content": {
                    "title": {
                        "value": "After rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal. The author's explanation regarding the completeness of the comparison is satisfactory to me. Additionally, they provide detailed cost-time comparisons. Overall, the author has addressed my concerns. Therefore, I would like to maintain my rating as borderline accept."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656468851,
                "cdate": 1700656468851,
                "tmdate": 1700656468851,
                "mdate": 1700656468851,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QRUf5AU3cv",
            "forum": "DesYwmUG00",
            "replyto": "DesYwmUG00",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4591/Reviewer_UXKo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4591/Reviewer_UXKo"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on image editing using pre-trained diffusion models. The authors propose a unified probabilistic formulation for diffusion-based image editing, including inpainting, image-to-image translation, and dragging, based on existing methods. Experiments show that the proposed method achieves better performance than the original ODE version."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors provide SDE versions for existing ODE-based editing methods and achieve better performance.\n\n2. The authors build a benchmark called DragBench for evaluation, which may benefit the community.\n\n3. The paper is well written, and the experiment and presentation are solid."
                },
                "weaknesses": {
                    "value": "1. The core idea that SDE beats ODE in image editing seems similar to CycleDiffusion [6]. This article seems to just generalize this phenomenon to multiple tasks and verify them. Overall, the innovative contribution seems insufficient. \n\n2. The authors claim to propose a unified probabilistic formulation for diffusion-based image editing. However, many related image editing/I2I methods are not mentioned, e.g., RePaint [1], DDNM [2], DPS [3], T2I-Adapter [4], and ControlNet [5].\n\nReferences:\n\n[1] Lugmayr et al., Repaint: inpainting using denoising diffusion probabilistic models, CVPR 22\n\n[2] Wang et al., Zero-shot image restoration using denoising diffusion null-space model, ICLR 23\n\n[3] Chung et al., Diffusion posterior sampling for general noisy inverse problems, ICLR 23\n\n[4] Mou et al., T2I-adapter: learning adapters to dig out more controllable ability for text-to-image diffusion models, arXiv 23\n\n[5] Zhang et al., Adding conditional control to text-to-image diffusion models, ICCV 23\n\n[6] Wu et al., A latent space of stochastic diffusion models for zero-shot image editing and guidance, ICCV 23"
                },
                "questions": {
                    "value": "Please see the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4591/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4591/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4591/Reviewer_UXKo"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4591/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698225510757,
            "cdate": 1698225510757,
            "tmdate": 1699636437140,
            "mdate": 1699636437140,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2C7t5rsyek",
                "forum": "DesYwmUG00",
                "replyto": "QRUf5AU3cv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UXKo"
                    },
                    "comment": {
                        "value": "We thank reviewer UXKo for the acknowledgment of our contributions and the valuable comments. We respond below to your questions and concerns.\n\n## Weakness1: The relationship between our work and CycleDiffusion.\n\nThank you for your valuable question. we emphasize our contributions and explain the differences and links between our work and CycleDiffusion in detail:\n\n1. We introduce a unified perspective for multiple diffusion-based image editing tasks (I2I, inpainting, dragging) that encompasses various editing techniques. In contrast, CycleDiffusion focuses on I2I.  **Given the fact that existing research in such tasks is mainly independent, we believe the unified perspective is a nontrivial and valuable contribution.**\n2. **To our knowledge, we present the FIRST theoretical analysis (which is one of our core contributions) on the superiority of SDE over ODE in image editing, while all existing methods including Cycle-Diffusion are purely empirical.**\n3. We present a new algorithm (SDE-Drag) as well as a new benchmark for open-set image dragging. Besides the SDE formulation, **the core contribution of SDE-Drag is a simple copy&paste manipulation strategy for the latent variables**, which significantly outperforms existing optimization-based methods. Such contributions are unique in our paper compared to existing work including Cycle-Diffusion.\n4. **The core idea of Cycle-Diffusion is to construct an invertible SDE algorithm**, inspired by the ODE methods. **It is mainly compared with non-invertible SDE baselines**. However, our core idea is that **the general SDE formulation (including Cycle-Diffusion and others) is better than ODE in editing**. Our theory applies to other SDE formulations and we apply other SDE-based methods in the inpainting task other than Cycle-Diffusion.\n\nWe believe the above contributions make our paper distinct from Cycle-Diffusion.\n\n## Weakness2: More image editing methods are not mentioned.\n\nThank you for your insightful suggestion and for pointing out the related works, as briefly discussed below.\n\nOur framework analyzes the difference between SDE and ODE when the prior distribution mismatch is due to manipulation or domain transformation during inference. Therefore, it also applies to RePaint, DDNM, and DPS. However, training-based methods like T2I-Adapter and ControlNet without mismatch, are excluded from our framework.\n\nWe will add the above discussion in our revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700188618606,
                "cdate": 1700188618606,
                "tmdate": 1700188618606,
                "mdate": 1700188618606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qL3atD076G",
                "forum": "DesYwmUG00",
                "replyto": "2C7t5rsyek",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Reviewer_UXKo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Reviewer_UXKo"
                ],
                "content": {
                    "comment": {
                        "value": "The author's rebuttal addresses most of my concerns, and I'll keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554376884,
                "cdate": 1700554376884,
                "tmdate": 1700554376884,
                "mdate": 1700554376884,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wy3epfpsSN",
            "forum": "DesYwmUG00",
            "replyto": "DesYwmUG00",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4591/Reviewer_NG3Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4591/Reviewer_NG3Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper show theoretically and experimentally the benefit of using diffusion model SDEs over ODEs for image editing. The authors formulate the image editing with diffusion model process in three steps : 1) encoding with deterministic or random noise 2) alteration of the latent which means modification of the prior distribution representing this latent 3) SDE or ODE sampling starting from the altered latent.\nOn the theoretical side, it is proven that during step 3, the KL distribution between the SDE marginals : a) when sampling from the altered latent and b) sampling from the original latent, decreases, while remaining constant when sampling with ODEs. On the experimental side, it is analyzed, from different works, the benefit of using SDEs over ODEs in step 3."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The argumentation is limpid, and the contributions are clearly stated.  \n\nAlthough I am not an expert in image editing, I think that the main strength of the paper is its experimental study, which looks impressive. It contains numerous comparisons in three different tasks.  Moreover, the authors created an evaluation benchmark for point dragging and conducted a user study for evaluation on this problem. The advantage of SDEs over ODEs is clearly demonstrated experimentally.\n\nI like the fact that the authors took care to expose a very easy Gaussian toy example to illustrate the theorems."
                },
                "weaknesses": {
                    "value": "Major weaknesses : \n- The problematic of the paper (end of Section 3.1) is never clearly answered. Actually, I do not think that the paper properly gives the answer to this question. This is link to the following point.\n- It is not clear that the proposed theoretical arguments prove the right point. Given $x_0$ (resp. $\\tilde x_0$) sampled from the latent $x_{t_0}$ (resp.  $\\tilde x_{t_0}$. ), with Theorem 3.1, it is proven that the $x_0$ and $\\tilde x_0$ are closer \u201cin distribution\u201d than $x_{t_0}$ and $\\tilde x_{t_0}$. Why is that desired ? \nI think that the authors see $p_0$ as the distribution of clean images $q_0$, and then wish to minimize $KL(p_0, \\tilde p_0)$ to get well-looking images. However, $p_0$ is very different from $q_0$ because the score is not (and for from being) perfectly matched with the denoiser. From this fact, could the authors explain why it makes sense to try to minimize $KL(p_0, \\tilde p_0)$ ? I am more likely to think that, for the purpose of image editing, it does not make sense to try to minimize the distance between these two distributions. \n- In the experimental section, some important information is missing : which model are you using, trained on which dataset ? \n\nMinor weaknesses : \n- In the paragraph \"Samples\" from Section 2. The term \"equivalent\" is not true. Sampling is not \"equivalent\" to discretization ! Moreover, discretizing (4) or (5) does not enable to sample from $q_0$ if the score is not perfectly matched with $\\epsilon_\\theta$.\n- The ODE inversion process explanation should be clarified. The links and differences between deterministic ODE inversion and random Gaussian noise should be explained. \n- The \"mild assumption\" should be detailed, at least in the Appendix."
                },
                "questions": {
                    "value": "- If the noise is fixed, is CycleSDE still an SDE ? \n- Is the log-Sobolev inequality likely to be verified in practice ?\n\nI am prepared to improve my score by taking into account the author's feedback."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4591/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4591/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4591/Reviewer_NG3Y"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4591/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698396649581,
            "cdate": 1698396649581,
            "tmdate": 1699636437052,
            "mdate": 1699636437052,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sM5zktBDrH",
                "forum": "DesYwmUG00",
                "replyto": "wy3epfpsSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NG3Y Part1"
                    },
                    "comment": {
                        "value": "We thank reviewer NG3Y for the acknowledgment of our contributions and the valuable comments. We respond below to your questions and concerns.\n\n## Weakness1: The reason to minimize $KL(\\tilde p_0 || p_0)$ .\n\n\nThank you for your meticulous and insightful comment. We agree that the final objective of image editing is to analyze $\\tilde p_0$ and $q_0$ under certain divergence. Below, we explain **why making $\\tilde p_0$ and $p_0$ close (i.e., the contribution of this paper) is meaningful both theoretically and empirically**.\n\nTheoretically, we indeed assume that the diffusion model characterizes the true score functions, namely $KL(p_0 || q_0)=0$ $(p_0=q_0)$ in the submission (See Line 3, Paragraph 1 in Sec. 3.2). Then our theory on $KL(\\tilde p_0|| p_0)$ holds for $KL(\\tilde p_0||q_0)$ as desired. **Intuitively, the assumption can be relaxed to a bounded approximation error of the score function**, i.e., $\\mathbb E_{q_t}[ \\|s_{\\mathbf \\theta}(\\mathbf x_t, t) - \\nabla_{\\mathbf x_t} \\ln q_t \\|^2]  < \\epsilon$ based on the latest theoretical work [a, b, c]. In particular, if $\\epsilon \\rightarrow 0$, then the total variation distance (TV) for $q_0$ and $p_0$ tends to zero, namely, $TV(q_0, p_0) \\rightarrow 0$, making $p_0$ an accurate approximation for $q_0$.\n\nEmpirically, **experimental results suggest that $p_0$ is a good surrogate for $q_0$, especially compared to $\\tilde p_0$**. For instance, Stable Diffusion [d] can generate high fidelity images in human perception. Besides, Tab. 5\uff08on page 26\uff09in our submission shows that **the FID for sampling (namely $p_0$) is significantly lower than the FID for editing (ODE baseline, namely $\\tilde p_0$)**, suggesting that $p_0$ is much closer to $q_0$ than $\\tilde p_0$, making it meaningful to minimize $KL(\\tilde p_0|| p_0)$.\n\nWe sincerely appreciate the valuable comments. We will revise the paper to clarify that the final objective of image editing is to analyze $\\tilde p_0$ and $q_0$ and present the above discussion on why minimizing $KL(\\tilde p_0 || p_0)$ is meaningful. If you have any further concerns, please feel free to post more comments.\n\n\n[a] Chen, Sitan, et al. \"Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions.\" *arXiv preprint arXiv:2209.11215* (2022).\n\n[b] Lee, Holden, Jianfeng Lu, and Yixin Tan. \"Convergence of score-based generative modeling for general data distributions.\" *International Conference on Algorithmic Learning Theory*. PMLR, 2023.\n\n[c] Chen, Sitan, Giannis Daras, and Alex Dimakis. \"Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers.\" International Conference on Machine Learning. PMLR, 2023.\n\n[d] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n\n## Weakness2: Pretrained models and datasets in the experimental section.\n\nThank you for thoroughly reading our paper and highlighting the issues in detail. **For fairness, we use the same pretrained models (including the training dataset) as the corresponding baselines for all experiments.** In particular, we use Stable Diffusion 1.5 [d] trained on the LAION dataset [e] for the inpainting, DiffEdit and dragging tasks. We use ADM [f] trained on the ImageNet dataset [g] for the DDIB experiment.\n\nWe will add the information in Sec. 5 in the revision.\n\n[e] Schuhmann, Christoph, et al. \"Laion-5b: An open large-scale dataset for training next generation image-text models.\" *Advances in Neural Information Processing Systems* 35 (2022): 25278-25294.\n\n[f] Dhariwal, Prafulla, and Alexander Nichol. \"Diffusion models beat gans on image synthesis.\" Advances in neural information processing systems 34 (2021): 8780-8794.\n\n[g] Deng, Jia, et al. \"Imagenet: A large-scale hierarchical image database.\" 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700188120137,
                "cdate": 1700188120137,
                "tmdate": 1700188120137,
                "mdate": 1700188120137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J1hxRQMjUt",
                "forum": "DesYwmUG00",
                "replyto": "wy3epfpsSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NG3Y Part2"
                    },
                    "comment": {
                        "value": "## Weakness3: Expression in the paragraph \"Samplers\" from Section 2.\n\nThank you very much for pointing out our mistakes. We will change our expression: **Samples can be generated from the diffusion model by discretizing the reverse SDE (Eq.(4)) or ODE (Eq.(5)) from $T$ to $0$**.\n\n## Weakness4: Details about ODE inversion.\n\nThank you for your insightful suggestion. We are not sure about the \"random Gaussian noise\" in your comment. We guess you mean the $\\bar{w}_s$ in Eq. (6) in the submission. **Note that when using ODE inversion, we set $\\eta=0$, which implies that $\\sigma_s = 0$. Namely, the coefficient of the random Gaussian noise in Eq. (6) is zero** and it is a deterministic process. If we misunderstand your comment, please feel free to post further questions.\n\nIn case you are not familiar with ODE inversion, it generally aims to obtain $\\mathbf x_T$ given $\\mathbf x_0$ based on the invertibility of the ODE. According to Eq. (5) in the submission, the general formulation of ODE inversion is defined as $\\mathbf x_T = \\mathbf x_0 + \\int_0^T \\left[f(t) \\mathbf x_t + \\frac{g^2(t)}{2 \\sqrt{1 - \\alpha_t}}  \\mathbf \\epsilon_{\\mathbf \\theta}(\\mathbf x_t, t)\\right] dt$. This implies that for each given data point $\\mathbf x_0$, there exists a uniquely determined $\\mathbf x_T$ associated with it through ODE inversion. \n\nAs for the specific DDIM sampler, one step is Eq. (6) in our submission. We rewrite it here for clarity (only ODE formulation, i.e., $\\eta=0$): $\\mathbf x_t = \\sqrt{\\alpha_{t}} \\left( \\frac{\\mathbf x_s - \\sqrt{1 - \\alpha_s} \\mathbf \\epsilon_{\\mathbf \\theta}(\\mathbf x_s, s)}{\\sqrt{\\alpha_s}} \\right) + \\sqrt{1 - \\alpha_{t}} \\mathbf \\epsilon_{\\mathbf \\theta}(\\mathbf x_s, s)$. When $t>s$, it is a step of DDIM inversion. \n\nWe will provide the details of ODE inversion in the Appendix in the revision.\n\n\n## Weakness5\uff1aAssumption in our theoretical analyses.\n\nThank you for your insightful suggestion. For completeness, we will list the assumption in Appendix D, which is the same as Assumption A.1. of [h].\n\n[h] Lu, Cheng, et al. \"Maximum likelihood training for score-based diffusion odes by high order denoising score matching.\" *International Conference on Machine Learning*. PMLR, 2022.\n\n## Question1: Is CycleSDE still an SDE?\n\nThank you for your question. We interpret \"noise is fixed\" to mean that the noises in the backward process of Cycle-SDE are solved by the forward process. Actually, given $\\mathbf x_{t_0}$, the noises in the backward process are not iid standard Gaussian noise like the origin SDE, but instead a combination of noises in the forward process.  Consequently, the reverse process is stochastic, and thus Cycle-SDE is still an SDE.\n\n\n## Question2: Is the log-Sobolev inequality hold for the data distribution in practice?\n\nThank you for your question. The log-Sobolev inequality is a **classical and standard assumption in theoretical analyses of Langevin-type algorithms [i, j], which cannot be verified in large-scale and high-dimensional real data** like LAION [e]. Note that without the assumption, although it is nontrivial to obtain the strong linear convergence result, our main results (i.e., Theorem 1,2&3) still hold. \n\n[i] Chewi, Sinho, et al. \"Analysis of Langevin Monte Carlo from Poincar\\'e to Log-Sobolev.\" arXiv preprint arXiv:2112.12662 (2021).\n\n[j] Vempala, Santosh, and Andre Wibisono. \"Rapid convergence of the unadjusted Langevin algorithm: Isoperimetry suffices.\" Advances in neural information processing systems 32 (2019).\n\n\n**Finally, we believe the quality of the paper has been improved following your insightful suggestions. If you have any more questions, we are happy to discuss them and will do our best to address them!**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700188406286,
                "cdate": 1700188406286,
                "tmdate": 1700188406286,
                "mdate": 1700188406286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L1ERVDdefh",
                "forum": "DesYwmUG00",
                "replyto": "wy3epfpsSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to further feedbacks"
                    },
                    "comment": {
                        "value": "Dear Reviewer NG3Y,\n\nThank you again for your great efforts and valuable comments. We have carefully addressed the main concerns in detail. We hope you will find the response satisfactory. As the discussion phase is about to close, we are very much looking forward to hearing from you about any further feedback. We will be very happy to clarify further concerns (if any).\n\nBest, Authors"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488416259,
                "cdate": 1700488416259,
                "tmdate": 1700488416259,
                "mdate": 1700488416259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mlYC0PwP8b",
                "forum": "DesYwmUG00",
                "replyto": "L1ERVDdefh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Reviewer_NG3Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Reviewer_NG3Y"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your careful rebuttal. The authors answered all my interrogations. I will take a bit more time to think about my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555242118,
                "cdate": 1700555242118,
                "tmdate": 1700555242118,
                "mdate": 1700555242118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vBwPwjn4Ud",
                "forum": "DesYwmUG00",
                "replyto": "wy3epfpsSN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4591/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer NG3Y for the time and consideration. We are happy to see the comment *\"The authors answered all my interrogations\"* and there is no further question. We hope the Reviewer NG3Y can update the score accordingly based on the initial review *\"I am prepared to improve my score by taking into account the author's feedback.\"*\n\nWe understand that Reviewer NG3Y can be very busy but we kindly remind Reviewer NG3Y that the author-reviewer discussion period ends in one day."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4591/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617029629,
                "cdate": 1700617029629,
                "tmdate": 1700625860800,
                "mdate": 1700625860800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]