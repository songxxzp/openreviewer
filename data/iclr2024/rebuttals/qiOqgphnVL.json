[
    {
        "title": "Democratized Diffusion Language Model"
    },
    {
        "review": {
            "id": "CWwKPtMqAp",
            "forum": "qiOqgphnVL",
            "replyto": "qiOqgphnVL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2395/Reviewer_UKex"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2395/Reviewer_UKex"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies Diffusion Models for text generation. It exams the runtime distinction between different diffusion models such as SSD, Plaid and CDCD. \n\nA key observation from the research is the ability of most models to halt the generation process, enabling a faster text generation termed as \"adaptive early exit\" without diminishing the quality of the output.\n\nThe author also shares an open source re-implementation of the Diffusion LM trained with CDCD framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies a new field for text generation using diffusion models.  Given the inherent complexities and resource-intensive nature of running diffusion models continuously during generation, the research investigates the feasibility of early exiting by monitoring token switches across various pre-training checkpoints. The methodology of evaluating Cos between the score function and L2 norm the sample embeddings, and subsequently observing score angle changes, provides a novel insights to assess diffusion models."
                },
                "weaknesses": {
                    "value": "The paper focuses on the concept of early stopping in diffusion models, which is an idea that has been previously explored, as noted in \"Accelerating Diffusion Models via Early Stop of the Diffusion Process\" as an example. The contribution to extend to text generation needs to be assessed. The technique of early stopping is a recognized practice during the inference stage of diffusion models. While the current paper's examination of token switches across different pre-training checkpoints offers a fresh angle, the approach's broader implications and significance in comparison to established methodologies could be further elucidated. \n\nFrom Table 1 main results, we can see the choice of steps also provides very marginal impact to the final performance. It might be beneficial for the research to delve deeper into how this method stands out from or builds upon existing techniques in the field of diffusion models."
                },
                "questions": {
                    "value": "LLM as Judge has some known weakness, from the provided prompts [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena], it seems the prompt is very simple and does not consider LLM bias. Can author comment on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2395/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2395/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2395/Reviewer_UKex"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2395/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698611565054,
            "cdate": 1698611565054,
            "tmdate": 1699636174731,
            "mdate": 1699636174731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lebcl9l6H4",
                "forum": "qiOqgphnVL",
                "replyto": "CWwKPtMqAp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ukex answer"
                    },
                    "comment": {
                        "value": "Thank you for your review. While performing an early exit is a well-established approach in Deep Learning, performing it for text diffusion was not previously explored. Furthermore, since the generation is performed in categorical space, we were able to study novel exit methods for diffusion early exiting.\n\n- From Table 1 main results, we can see the choice of steps also provides very marginal impact to the final performance. It might be beneficial for the research to delve deeper into how this method stands out from or builds upon existing techniques in the field of diffusion models.\n\nThis is not actually correct. As seen with the Unconditional generation setup, reducing the total number of steps could lead to a 0.2-0.3 drop in the AR-NLL metric, which is not marginal for fixed dist-N values. Furthermore, based on our experiments, early exiting could be used in two scenarios \u2013 when we want to achieve a better quality of samples with a given number of generation steps or when we want to reduce the number of generation steps for a given quality of samples. Thus studying number of generation steps is orthogonal to early exiting methods.\n\nWith our paper, we aimed to understand the behavior of DLMs, which is different from continuous diffusion models. We established new sampling methods and provided insights into reasons for such behavior. Understanding this behavior makes it possible to develop connections to other models and approaches, but without precisely studied groundings, it is impossible.\n\n- LLM as Judge has some known weakness, from the provided prompts [Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena], it seems the prompt is very simple and does not consider LLM bias. Can author comment on this?\n\nOur task is significantly easier than usually accessed with LLM as a Judge. We do not want to select the best answer across a set of responses (e.g., alignment side-by-side setup). Instead, we used LLM to compare a generation from a specific step with the final generation (grammar, similar words, etc.). We observed that LLM provided a valuable estimation of such differences."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205405820,
                "cdate": 1700205405820,
                "tmdate": 1700205405820,
                "mdate": 1700205405820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gkjl9NM6UX",
                "forum": "qiOqgphnVL",
                "replyto": "lebcl9l6H4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2395/Reviewer_UKex"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2395/Reviewer_UKex"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification. My feedback aligns with other reviewers regarding the use of GPT-4 for evaluating relative scores, which requires further validation. \n\nThe author should provide more empirical study to support \"Our task is significantly easier than usually accessed with LLM as a Judge. We do not want to select the best answer across a set of responses (e.g., alignment side-by-side setup). Instead, we used LLM to compare a generation from a specific step with the final generation (grammar, similar words, etc.). We observed that LLM provided a valuable estimation of such differences.\" \n\nThe current \"GPT-SCORE DETAILS\nThe instruction contained a request to evaluate a text\u2019s spelling, consistency, and coherence with a\nnumber from 1 to 10 compared to the sampling from the last 1000-th generation step, which served\nas a reference. Also, we included requesting for ignoring abrupt endings of texts since all models\nwere evaluated with sample length equal to 64.\" is questionable in terms of whether LLM will likely produce judgement bias towards certain number (e.g. smaller or larger numbers)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590150498,
                "cdate": 1700590150498,
                "tmdate": 1700590150498,
                "mdate": 1700590150498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Mk69bc3ec4",
            "forum": "qiOqgphnVL",
            "replyto": "qiOqgphnVL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2395/Reviewer_Sk8v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2395/Reviewer_Sk8v"
            ],
            "content": {
                "summary": {
                    "value": "This paper reimplemented a Diffusion LM (DLM) trained with the CDCD framework and provided some analysis of DLMs. Besides, the paper showed that the generation process of most DLMs for general text generation can be halted."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper reimplemented the CDCD framework. If the code and checkpoint can be open-sourced, it can provide support for the research of DLMs.\n\n2. This paper makes sufficient experiments and analysis on the existing DLMs, and obtains the early stopping strategy of DLMs by observing the AR-NLL curve."
                },
                "weaknesses": {
                    "value": "1. The innovation of the paper is insufficient. The main contribution is to reproduce the CDCD structure and analyze the existing DLMs, without proposing new models or methods.\n\n2. The length of the trained model is limited to 64, and it is not clear whether there will be different conclusions for longer lengths. The length of 64 is still a bit far from actual application.\n\n3. We still care about the performance of pre-trained models on downstream tasks, and the paper did not select some downstream tasks for evaluation.\n\n4. Writing issues:\n\n    (1) The main contribution of the paper, such as the analysis of DLMs, is not given in the title. The writing style is a bit messy.\n\n    (2) It is recommended to add the model parameter quantity to the comparison in Table 1."
                },
                "questions": {
                    "value": "Does the model have the ability to output the </s> token? The sentences in Appendix D are all truncated results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2395/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2395/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2395/Reviewer_Sk8v"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2395/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822618733,
            "cdate": 1698822618733,
            "tmdate": 1699636174627,
            "mdate": 1699636174627,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IexW7rI255",
                "forum": "qiOqgphnVL",
                "replyto": "Mk69bc3ec4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sk8v answer"
                    },
                    "comment": {
                        "value": "Thank you for your review!\n\n- The innovation of the paper is insufficient. The main contribution is to reproduce the CDCD structure and analyze the existing DLMs, without proposing new models or methods.\n\nThe main aim of our paper is to understand the generation process of DLMs. With novel methodology in this direction, we observed that existing DLMs allow performing early exiting. We proposed several methods for performing early exiting (See Sections 4.2, 4.3). This work proposes faster sampling algorithms for DLMs and also lightens future research directions to further understanding and analysis of DLMs, which could lead to more new methods.\n\n- The length of the trained model is limited to 64, and it is not clear whether there will be different conclusions for longer lengths. The length of 64 is still a bit far from actual application.\n\nWe will add experiments with longer sequences to the rebuttal revision soon. Please consider these new experiments when reevaluating your score.\n\n- We still care about the performance of pre-trained models on downstream tasks, and the paper did not select some downstream tasks for evaluation.\n\nPlease note that we do not propose new DLMs with our paper. Instead, we studied sampling algorithms for existing models. These sampling algorithms are not connected to downstream task performance.\n\n- (1) The main contribution of the paper, such as the analysis of DLMs, is not given in the title. The writing style is a bit messy.\n\nWe desired to highlight our trained DDLM model. Also, we wanted to have a connection with early exiting algorithms that allow faster sampling, thus making DLMs closer to practical usage by other people\n\n- (2) It is recommended to add the model parameter quantity to the comparison in Table 1.\n\nThank you for this proposal; we have added the number of model parameters in the latest revision.\n\n- Does the model have the ability to output the </s> token? The sentences in Appendix D are all truncated results.\n\nSentences in the Appendix D section were sampled with a sequence length of 64. Thus, they are truncated. All used in our experiments model could produce </s> tokens. For example, it is possible to set it as a condition at any specific point within the sequence, or the model itself could generate it for long enough sequences."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700206792184,
                "cdate": 1700206792184,
                "tmdate": 1700206792184,
                "mdate": 1700206792184,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZYyAbFr1si",
                "forum": "qiOqgphnVL",
                "replyto": "Mk69bc3ec4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2395/Reviewer_Sk8v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2395/Reviewer_Sk8v"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification. But my concerns are merely addressed.\n\n(1) I still harbor doubts regarding the accuracy of the title, and the innovativeness of the paper may not meet the standards set by ICLR.\n\n(2) Despite the authors' claim of not proposing a new DLM, the downstream tasks remain a subject worthy of investigation.\n\n(3) Experiments with longer text have yet to be provided."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641894758,
                "cdate": 1700641894758,
                "tmdate": 1700641939516,
                "mdate": 1700641939516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7d1ZiY1M1t",
                "forum": "qiOqgphnVL",
                "replyto": "D2NkeEwNdj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2395/Reviewer_Sk8v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2395/Reviewer_Sk8v"
                ],
                "content": {
                    "comment": {
                        "value": "If I haven't missed anything, experiments with longer textual content are only displayed in Figure 8. However, the paper's most crucial DDLM lacks experimental results. Additionally, there are no actual text generation results presented, including longer texts and results with </s>."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661706016,
                "cdate": 1700661706016,
                "tmdate": 1700661706016,
                "mdate": 1700661706016,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kyv735YEON",
            "forum": "qiOqgphnVL",
            "replyto": "qiOqgphnVL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2395/Reviewer_E1db"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2395/Reviewer_E1db"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the use of diffusion models for text generation and compares different frameworks used in the process (CDCD, Plaid, SSD). The authors focus on the sampling process and propose an adaptive early exit mechanism to accelerate text generation without compromising quality. The main contributions:\n- re-implementation of the diffusion language model trained with the CDCD framework.\n- propose and evaluate three adaptive criteria for early exiting\n- side-by-side assessment to show the convergence of generation"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Re-implementation benifts the community.\n- The step-by-step analysis could help us understand the generation process of diffusion models."
                },
                "weaknesses": {
                    "value": "- It is good to see analysis of sampling between different diffusion models, however, no further explanation about the deep reason to cause these differences.\n- The advantage of DDLM is to early exit and speedup the generation process. However, compared with some faster ODE solvers (e.g. DPM-solver[1]), early exit of DDLM maybe not superior than them.\n- Early exit leads to the downgrade of generation diversity."
                },
                "questions": {
                    "value": "- Why after your findings that using a noise scale of 0.9 is optimal, you still use a scale of 1.0 in later experiments\n- In table 2, it is weird AR-NLL=0.44 and dist_1=0 with noise=0, do you have generation examples?\n- The GPT-Score is a relative value, with step-1000 as the reference text. However, the reference text may not be fluent. Is it possible to obtain the absolute value?\n- Can you also compare with discrete diffusion models?\n- Why in Fig5, the NLL of Plaid (c) (<3.6) is lower than the DDLM (a) (>3.68)? This contradicts to the main table."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2395/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698895513740,
            "cdate": 1698895513740,
            "tmdate": 1699636174559,
            "mdate": 1699636174559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pn2wqIGo7x",
                "forum": "qiOqgphnVL",
                "replyto": "Kyv735YEON",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2395/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2395/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer E1db"
                    },
                    "comment": {
                        "value": "Thank you for your review!\n\n- It is good to see analysis of sampling between different diffusion models, however, no further explanation about the deep reason to cause these differences.\n\nDifferences between different diffusion models are specific to different training objectives and could be caused by noise scheduling. Experiments with varying noise scales ground this hypothesis. However, we did not dig into this and focused on proposing and understanding sampling methods for different DLMs and establishing an experimental setup for such research direction. Understanding various ways to perform an early exit for various models first could provide more evidence and grounding for further research on this topic rather than making solid assumptions without proper evaluation.\n\n- The advantage of DDLM is to early exit and speedup the generation process. However, compared with some faster ODE solvers (e.g. DPM-solver[1]), early exit of DDLM maybe not superior than them.\n\nFirst, let us clarify that DPM-solver is an second-order method, which implies two model\u2019s forward passes during one step. Also note that DPM-solver is very similar to Heun sampler from [1]. In our experiments this type of samplers do not outperform Euler sampler with similar number of forward passes. \n\n[1] https://arxiv.org/abs/2206.00364\n\n- Early exit leads to the downgrade of generation diversity.\n\nWe did not observe such behavior during our experiments. We hypothesize that you refer to Table 2 with experiments on noise scaling to allow faster convergence of the sampling algorithm. Lower diversity is expected since trimming the noise scale leads to \u201cmore deterministic\u201d sampling. We will add plots with sampling diversity akin to Figure 5 in Section 4.3 to prove that early exiting does not reduce samples diversity in the rebuttal revision. Please refer to it for this weak point when reevaluating your score.\n\n- Why after your findings that using a noise scale of 0.9 is optimal, you still use a scale of 1.0 in later experiments\n\nThe purpose of experiments with noise scaling is to understand the underlying behavior of DLMs, leading to emerging capabilities to perform an early exiting. However, we observed that performing generation with a slightly reduced noise scale without hurting AR-NLL and Dist values, such a setup is not conventional. In our case, this finding could provide insights on early exiting for DLMs for future works, but performing experiments in such configuration for other models is not conventional.\n\n- In table 2, it is weird AR-NLL=0.44 and dist_1=0 with noise=0, do you have generation examples?\n\nThis is caused by AR-NLL metric issues since it is evaluated with external LM. It is conventional to obtain small values for repetitive samplings (e.g., one word repeated 64 times). Because of the issues with this metric, we also incorporated other metrics, such as a number of distinct tokens and self-bleu. E.g. Table 2 shows that the Dist value reached 0, indicating that AR-NLL does not provide helpful information.\n\n- The GPT-Score is a relative value, with step-1000 as the reference text. However, the reference text may not be fluent. Is it possible to obtain the absolute value?\n\nIt is possible, although we observed that absolute value without side-by-side comparison with GPT-4 had a large variation. Performing a side-by-side comparison is established by other works (e.g., in Direct Preference Optimization), and to the best of our knowledge, no works used absolute evaluation with GPT-4 (doing so is an exciting research idea itself). In our case, we used it to understand the convergence of sampling algorithms. Even though the final sampling could not be fluent, this sampling is the one that is obtained with the original models as is. So, this experiment shows that early exiting samples after a specific step do not differ from the final sampling (see Appendix Section D).\n\n- Can you also compare with discrete diffusion models?\n\nWe are unaware of discrete diffusion models capable of generating unconditional text. Existing unconditional general-purpose text generation models are continuous, so we performed experiments with them.\n\n- Why in Fig5, the NLL of Plaid (c) (<3.6) is lower than the DDLM (a) (>3.68)? This contradicts to the main table.\n\nFor Figure 5, we used 1000 samples to evaluate the models, while the main table used 5000 samples. This led to a slight variation in AR-NLL values. We added an explanation to the latest revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207380398,
                "cdate": 1700207380398,
                "tmdate": 1700207380398,
                "mdate": 1700207380398,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pmMFEMQKRh",
                "forum": "qiOqgphnVL",
                "replyto": "pn2wqIGo7x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2395/Reviewer_E1db"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2395/Reviewer_E1db"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for clarification"
                    },
                    "comment": {
                        "value": "Thank you for your clarification. Some of my questions are resolved. But I still maintain these concerns: (1) Early stop, as one of your main contribution, is saving ~30% steps according to Fig 5 and this improvement is limited. (2) Whether using GPT-4 to evaluate the relative score is responsible or not needs further discussion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2395/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575570921,
                "cdate": 1700575570921,
                "tmdate": 1700575570921,
                "mdate": 1700575570921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]