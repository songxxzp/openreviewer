[
    {
        "title": "Jointly-Learned Exit and Inference for a Dynamic Neural Network"
    },
    {
        "review": {
            "id": "jA5NQAqqo9",
            "forum": "jX2DT7qDam",
            "replyto": "jX2DT7qDam",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8069/Reviewer_smKZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8069/Reviewer_smKZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a novel mechanism to add trainable early exits (EEs) to a pre-trained neural network. It addresses a common concern of the majority of previous works, where the train procedure and the test procedure of the EEs were mismatched (e.g., jointly training all the early exits during training while thresholding their outputs during test). \n\nIn the proposed framework, each EE is associated to a probabilistic gate, and everything is trained jointly with a bilevel optimization problem, where the outer problem is defined over the gates and the inner problem over the EEs. The system is trained to also minimize execution time by training the gates to select the first exit having good accuracy for each pattern.\n\nThey also propose a novel conformal prediction (CP) strategy for EE networks, where different thresholds are selected for each exit (since each exit will observe a different subset of values at inference times)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The topic of the paper is important, since the method described can be used to reduce inference time (hence, power consumption) of any pre-trained model. The paper is generally well written and easy to read, although some additional visualization could be useful (see below). Results are relatively comprehensive, although I am concerned by the transfer learning from a large dataset (ImageNet) to a smaller, fundamentally similar dataset (CIFAR, also see below)."
                },
                "weaknesses": {
                    "value": "I find it hard to fully understand the method before reading through the entire paper. In particular: (a) the abstract has no mention of the paper's contributions; (b) the \"Contributions\" also does not explain how the method works; (c) there is no visual schema of the model; (d) some important details are described very late, for example \"Gate design\" is part of the experimental section but it is a crucial design decision. Overall it is understandable, but I think some minor reorganization and some additional visual descriptions (maybe a shortened pseudo-code?) can significantly help the reader.\n\nOn the novelty, I think a few methods could be added and discussed more. For example, the zero-time waste model (cited in the paper) has a geometric ensemble of the early exits which is very similar to their gating probability, if I understand correctly. As another example, differentiable branching (https://ieeexplore.ieee.org/document/9054209) considers a trainable exit strategy that also combines the EEs and that can be trained end-to-end with the rest of the network. Some of these methods can be also added to the experimental comparison.\n\nConcerning the last point, I am curious about transfer learning from a large, generalist dataset (ImageNet) to a small dataset which is basically in the same domain (CIFAR-10). Would it not make more sense to test directly on ImageNet? Improvements would be directly comparable with the original pre-trained model.\n\nEDIT AFTER REBUTTAL: the authors have added several baselines and improved the presentation of the method. I have raised my score to 8."
                },
                "questions": {
                    "value": "Apart from the topics discussed above, I have a general question about the bilevel task. Can you clarify why this is needed? There are techniques to train probabilistic blocks (e.g., reparameterization, score function estimators), which would allow to train all blocks simultaneously, or I am missing something? Also, what is the overall training time of your model compared to the alternatives?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Reviewer_smKZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698313807192,
            "cdate": 1698313807192,
            "tmdate": 1700486929303,
            "mdate": 1700486929303,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3lx6SbzmAg",
                "forum": "jX2DT7qDam",
                "replyto": "jA5NQAqqo9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/3"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and perceptive review, which has brought a number of issues to our attention, leading to modifications that we believe have improved the paper.\n\n### 4.1. Reorganization and visual descriptions:\nThank you for the suggestions to improve the clarity of the paper. Unfortunately, it is forbidden to modify the abstract. We agree that an additional sentence would help to clarify the contribution. However, following your suggestion, we have added a sentence to the introduction in the ''Contributions'' section that provides a high-level description of how the proposed method works. Also in line with your suggestion, we have added Figure 1, which provides a visual schema of the model and describes the core steps of the methodology. In the original version of the paper, we elected to include ''Gate design'' in the experimental section to separate the core components of the methodology from what we considered to be implementation decisions. For example, it is possible to use the softmax output directly rather than construct uncertainty features (this achieves similar performance but at the expense of a larger gate). We have now moved the gate and IM design into the methodology, as recommended."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375949995,
                "cdate": 1700375949995,
                "tmdate": 1700376163701,
                "mdate": 1700376163701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hDLhv0UKq5",
                "forum": "jX2DT7qDam",
                "replyto": "kfWR0enH1O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Reviewer_smKZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Reviewer_smKZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the significant work in the rebuttal. I have increased the presentation score to 3 and the overall score to 8 (accept). I have no specific questions left."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700486978689,
                "cdate": 1700486978689,
                "tmdate": 1700486978689,
                "mdate": 1700486978689,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lo47I5A08b",
            "forum": "jX2DT7qDam",
            "replyto": "jX2DT7qDam",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8069/Reviewer_1vfw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8069/Reviewer_1vfw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to jointly learn the backbone parameters and the exiting strategies in a dynamic multi-exit model. Intermediate classifiers are constructed in a pre-trained backbone, and exiting gates are built as a binary classification head. The joint learning problem is formulated as a bi-level optimization problem. Experiments on several small datasets demonstrate that 1) a better trade-off between accuracy and efficiency is achieved; and 2) a better estimation of uncertainty (calibration of classification confidence) is achieved."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The studied problem is of great interest;\n\n2. The motivation is clear;\n\n3. The proposed method is technically sound;\n\n4. The literature review is comprehensive;\n\n5. The experiments show the effectiveness in both accuracy-efficiency tradeoff and calibrated confidence."
                },
                "weaknesses": {
                    "value": "1. **Lack of experiment on more advanced architectures**. Why did the authors select a T2T-ViT to construct the early-exiting model? Straightforwardly, the joint learning procedure can be directly applied in mature multi-exit models, such as the compared MSDNet, RANet, and the cited Dynamic Perceiver.  To my understanding, Dynamic Perceiver is the most recent work in this field, achieving SOTA performance in dynamic early exiting.\n    \n2. **Overclaiming contributions**. Based on the above point, the contribution of this paper should not include a new architecture. It is recommended to summarize the contribution as the joint learning procedure only. I believe this would already be significant enough if the learning method is shown effective on more SOTA architectures and on the ImageNet dataset.\n\n3. **Lack of experiments on ImageNet**. To my understanding, the learning approach does not need to be applied in a downstream task on the toy small-scale datasets. Experiments on ImageNet would be more convincing.\n\n4. **Presentation**. It is recommended to use figures to clearly show the motivation and the method pipeline."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Reviewer_1vfw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698315106279,
            "cdate": 1698315106279,
            "tmdate": 1700712191232,
            "mdate": 1700712191232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ykOi8UJcg6",
                "forum": "jX2DT7qDam",
                "replyto": "Lo47I5A08b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We would like to express our appreciation for the thorough review, and in particular, the suggestions to conduct experiments with Dynamic Perceiver. We believe that the experimental results have strengthened the paper. \n\n### 3.1. Lack of experiments on more advanced architectures; Dynamic Perceiver: \nThank you for the suggestion to experiment on more advanced architectures and, in particular, those tailored for early-exit. It prompted some interesting and fruitful experiments with Dynamic Perceiver. We initially selected T2T-ViT as an example transformer model that performs well on the tasks under study. But we agree that combination with a state-of-the-art early-exit model would be more compelling.\n\nIn Observation 5 in the revised paper, we include results where we combine the Dynamic Perceiver with JEI-DNN (our proposed method). We use the Dynamic Perceiver framework to jointly train the backbone   and the IMs and then use JEI-DNN to jointly train the gates and the IMS.  Figure 6 shows that using JEI-DNN achieves an accuracy improvement of approximately 0.5-1\\% compared to Dynamic Perceiver on the CIFAR100 dataset when using MobileNetV3 as a backbone, when using 50-60 percent of the mul-add operations that are used in the original architecture. We are conducting similar experiments on ImageNet and hope to report the results before the end of the response period. \n\n### 3.2. Overclaiming contributions: \nThank you for pointing this out. Indeed, we agree that we do not propose a new architecture, and we have removed the word ''architecture'' from the claimed contributions, both in the introduction and the conclusion, in the revised paper.\n    \n### 3.3. Lack of experiments on ImageNet:\nWe have commenced experiments on ImageNet and obtained preliminary results. The experiments are ongoing, and we will finalize and report the results at least one day before the end of the response period. Although our introduced gates and IMs are lightweight and have few parameters, training with ImageNet is time-consuming, even with relatively powerful servers.\n    \n### 3.4. Presentation:\nFollowing your recommendation, we have added Figure 1 at the start of the methodology section to help the reader understand the approach."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375736787,
                "cdate": 1700375736787,
                "tmdate": 1700375736787,
                "mdate": 1700375736787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5GkNNLjruI",
                "forum": "jX2DT7qDam",
                "replyto": "ykOi8UJcg6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Reviewer_1vfw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Reviewer_1vfw"
                ],
                "content": {
                    "comment": {
                        "value": "After reading the authors' responses, I have increased my rating."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712223679,
                "cdate": 1700712223679,
                "tmdate": 1700712223679,
                "mdate": 1700712223679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6wULHk9xbn",
            "forum": "jX2DT7qDam",
            "replyto": "jX2DT7qDam",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8069/Reviewer_CAtU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8069/Reviewer_CAtU"
            ],
            "content": {
                "summary": {
                    "value": "This submission introduces JEI-DNN, an early-exit model approach that can be applied on top of off-the-shelf backbones for image classification. The proposed methodology appends light-weight classifiers and trainable gates along the depth of a frozen backbone model and jointly optimises them with a custom and insightfully designed loss function. As a result, a better speed-accuracy trade-off is provided compared to several baselines considering both learnable and non-parametric exit policies, while offering improved prediction uncertainty."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-The submission studies a very interesting problem, focusing on the emerging inference setting of input-dependent computation via early-exiting. \n\n-The manuscript offers a beautiful formulation of the examined task and provided solution, that can benefit the community as a principled definition of early-exit models. Additionally, the discussion of the manuscripts findings presents useful insights that can guide practitioners and inspire future research in the direction of EE models.\n\n-Most design choices in the adopted solution are well motivated and backed by practical insights.\n\n-The presented results indicate that the proposed method is effective and achieves a superior speed-accuracy trade-off to a wide set of baselines approaches."
                },
                "weaknesses": {
                    "value": "-A few technical aspects of the paper remain unclear. Specifically, the use of the min operator between two terms in Eq.8 is not fully justified in the manuscript, nor experimentally verified by a relevant ablation. \n\n-The proposed methodology is only evaluated on frozen-backbone CNNs. Although this a practical sub-category of EE-models, further evaluation on end-to-end EE models (jointly training the backbone and exits), which are shown to achieve superior speed-accuracy trade-off would increase the contribution of the paper.\n\n-Although the proposed approach is compared with a wide set of baseline, ImageNet experiments are omitted, rendering the presented results less convincing. \n\nShould the above limitations be satisfactorily addressed, I am inclined to increase my score."
                },
                "questions": {
                    "value": "1. What is the role of the second (1-sum) term in Eq.8 ? Potentially, its use could be validated through an ablation experiment.\n\n2. How would the method perform on end-to-end trainable early-exit models? Would the training of the gates affect the convergence of the overall model? Should the method be considerably adjusted to be applicable in this setting? \n\n3. Is the proposed approach equally effective on the most commonly used ImageNet-1K benchmark?\n\nMinor Comments:\nSec3: The term \"IC-Only training\" is widely used in the community (including the cited survey paper) to denote \"Intermediate-classifier only training\", rather \"than inference cost-only training\" as stated in the manuscript\n\nPost-Rebuttal Edit: Following the clarification on Q1 and additional results included in the updated version of the manuscript for Q2 and Q3, I am increasing my score from 6(=BA) to 8(=A)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Reviewer_CAtU"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766607377,
            "cdate": 1698766607377,
            "tmdate": 1700664776442,
            "mdate": 1700664776442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "inqkxtCJcY",
                "forum": "jX2DT7qDam",
                "replyto": "6wULHk9xbn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your careful and insightful review of the paper and for your positive comments concerning its strengths. Your constructive criticisms have prompted us to make some improvements to the paper. \n\n### 2.1. Use of the min operator between two terms in Eq.8: \nThank you for highlighting this issue. We require that the $P_{\\phi} (G | \\mathbf{x_i})$ sum to one. This is important because it means that our loss function in Eq. (6) is a correct approximation of the expectation in Eq. (5). Moreover, during inference, we use Eq.(4) to determine $ P(E^l=1 | \\mathbf{x_i} ) $ at a gate, and this calculation can lead to probabilities greater than 1 if $P_{\\phi}(G | \\mathbf{x_i})$ does not represent a correctly normalized probability distribution. The question is then how to map from the unnormalized $\\{g^l_{\\phi}\\}$ provided as output from the neural networks to normalized $P(G|\\mathbf{x}_i)$. An immediate thought would be to divide by the sum. But unfortunately, during inference, we are evaluating $g^l$ successively and we do not have access to the sum when we decide to exit. Therefore, we employ the $\\min$ operator to ensure that the sum of  $P(G^l|\\mathbf{x}_i)$ never exceeds $1$. We have added some text around equation (8) to explain this design choice. We could consider other forms of normalization, provided they can be implemented sequentially, i.e., we need to be able to perform the proposed normalization at layer $l$ without the knowledge of any $g^{l'}(\\mathbf{x}_i)$ values for $l'>l$. We would be happy to conduct and include an ablation study if there is an approach that seems a natural alternative to you, but we are not sure what the obvious alternative would be.\n\n### 2.2 Evaluation only on frozen-backbones: \nThis is a valid point. We intentionally focused on fixed backbones because our goal was to develop a mechanism suitable even for very large models where even fine-tuning is a major burden. But we have conducted some additional experiments to explore this direction. In the Observation 5 in the revised paper, we include results where we combine the Dynamic Perceiver with JEI-DNN (our proposed method). We use the Dynamic Perceiver framework to jointly train the backbone and the IMs and then use JEI-DNN to jointly train the gates and the IMS. In our current experiment, this is not true end-to-end joint learning, because we do not jointly train IMs, gates, and backbone, but the combined process does train all three components. Figure 6 shows that using JEI-DNN achieves an accuracy improvement of approximately 0.5-1\\% compared to Dynamic Perceiver on the CIFAR100 dataset when using MobileNetV3 as a backbone, when using 50-60 percent of the mul-add operations that are used in the original architecture. We are conducting similar experiments on ImageNet and hope to report the results before the end of the response period. We are also exploring performance for true end-to-end training when we allow the backbone model parameters to update as well as the IMs and gates. Here we retain our JEI-DNN bi-level optimization learning procedure, but update the backbone model parameters at the same time as the IM parameters.\n\n### 2.3. ImageNet experiments: \nWe have commenced experiments on ImageNet and obtained preliminary results. The experiments are ongoing, and we will finalize and report the results at least one day before the end of the response period. Although our introduced gates and IMs are lightweight and have few parameters, training with ImageNet is time-consuming, even with relatively powerful servers.\n\n### Minor comments: \nThat was a mistake, thank you for pointing it out."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700375595851,
                "cdate": 1700375595851,
                "tmdate": 1700375595851,
                "mdate": 1700375595851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ykZarswekb",
                "forum": "jX2DT7qDam",
                "replyto": "inqkxtCJcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Reviewer_CAtU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Reviewer_CAtU"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, \nThank you very much for your commitment to address all raised comments, and the significant work it required to conduct all the additional experiments included in the rebuttal.\n\nAt this stage, all of my raised concerns have been satisfactorily addressed. Additionally, I believe that the additional results targeting ImageNet as well as the application of the proposed approach to Dynamic Perceiver render the effectiveness of the JEI-DNN notably more convincing. \n\nAs such, I am increasing my score to Accept."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664529352,
                "cdate": 1700664529352,
                "tmdate": 1700664529352,
                "mdate": 1700664529352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0pY7Wq1l26",
            "forum": "jX2DT7qDam",
            "replyto": "jX2DT7qDam",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8069/Reviewer_KZAV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8069/Reviewer_KZAV"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an early-exit dynamic neural network architecture, JEI-DNN, that augments a backbone with lightweight, trainable gates and intermediate classifier that are jointly optimized. The gates are trained through a surrogate binary classification task, focusing on the optimization for assigning the most cost-effective early-exit classifier for each input. The results are presented on the cifar and svhn datasets for the T2T-ViT backbone."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Analysing the conformal intervals for the early exiting classifiers is interesting and it's promising to see the proposed method yields better uncertainty characterisation.\n- The ablation study in 8.6 clearly shows the merits of learnable GMs versus other design choices.\n- Figure 2 a) and b) very clearly show the early exiting patterns for the method and competing approaches."
                },
                "weaknesses": {
                    "value": "- It is unclear what the major contribution of the paper is. The use of learned early exiting gates and intermediate classifiers, trained jointly with the backbone is common practice for many early exiting architectures. After all the approximations described in 4.1, the loss term for the gating modules turnes into the common practice of summing the losses of independent binary early-exiting classifiers. The main task loss is similarly aligned with prior work from Han et al. (2022b).\n\n- Constructing surrogate binary targets for the learned gates is also common, e.g. FrameExit by Ghodrati et al. CVPR 2021. \n\n- Figure 2a) The fact that samples do not exit at all from the first few early exiting layers in JEI-DNN is puzzling and I am wondering if it is due to the choice of specific hyperparameters that prevent early exiting from these layers. As can be seen, the accuracy of the IM at layer 5 & 6 is far higher that the overall performance. It is conceivable that a good accuracy higher than the green dashed line is still achievable by the earlier classifiers at least for a proportion of samples. Is there any hyperparamer that could potentially give more control into the exiting pattens of JEI-DNN?\n\n\n- All the results in the paper are limited to the T2T-ViT backbone. The authors should show the efficacy of their method for a larger variety of backbones, preferably to models that are established in the early exiting literature such as MSDNet, DenseNet, etc.\n\n- Comparison of the accuracies among T2T-Vit and MSDNet architectures in Fig 5 seem unfair. Most of the gain in accuracy comes because of the more powerful transformer-based backbone and not because of the efficacy of the early exiting approaches. In fact, MSDNet and RANet show more robust early exiting results compared to the results reported for T2T. E.g. MSDNet retains the original accuracy of the model after almost 50% compression. The performance of the proposed JEI-DNN approach in comparison drops very rapidly even with 25% compression.\n\n- The method is only evaluated on three small-scale datasets: CIFAR10 & 100, SVHN. The authors should consider expanding the evaluation to ImageNet."
                },
                "questions": {
                    "value": "- The assumption that a sample exited at a gate at layer $l$ should also exit from any late stage gate seems against the prevalent view of $overthinking$:\n\"Overthinking is computationally wasteful, and it can also be destructive when, by the final layer, a correct prediction changes into a misclassification.\" by Kaya et al. ICML 2019.\nHow did you make this assumption?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8069/Reviewer_KZAV"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8069/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794950045,
            "cdate": 1698794950045,
            "tmdate": 1700668497252,
            "mdate": 1700668497252,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "adhpZIZezO",
                "forum": "jX2DT7qDam",
                "replyto": "0pY7Wq1l26",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/4"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and thoughtful review. You raised some excellent points that made us think carefully, leading to modifications and experiments that have provided insights and improved the paper.\n\n### 1.1 Contributions: \nThe major contributions of the paper are as follows. (1) We propose a novel, robust approach to jointly train the inference modules and decision gates in early-exit neural networks. As explained below, we disagree with the reviewer that this is common practice. (2) We introduce a novel modeling of the probability of exiting, representing it as a constrained sum, rather than a product. This leads to considerably more robust learning performance. (3) We provide a bi-level optimization formulation of the learning task. By decomposing the problem this way, we further encourage stable and efficient training. (4) We demonstrate that our approach results in much better calibration leading to significantly improved conformal intervals. \n\n### 1.2 Prior Work: \nWe disagree that the use of learned early exiting gates and intermediate classifiers, trained jointly with the backbone is a common practice for early exiting architectures. If the reviewer could provide a reference, we would be very happy to discuss more concretely and compare with our approach. We compiled an extensive table (Table 4 in the Appendix) of the existing literature with this specific categorization (non-fixed backbone) to highlight this.\n    \nFor those methods that train their proposed backbone architecture and the IMs jointly (non-fixed backbone), the architectures are specifically designed for early-exit. Even the more flexible models, such as the Dynamic Perceiver (Han et al., 2023) are restricted in their application; for example the Dynamic Perceiver, in its proposed form, can only be used with CNNs. In contrast, our approach can be easily applied to a transformer, a CNN, or a graph neural network. We demonstrate this better in supplementary experiments that we have done at the request of reviewers. But beyond this, to the best of our knowledge, all of the state-of-the-art methods that train the backbone use fixed-threshold gating mechanisms. For instance, the SOTA early-exit models (e.g. Dynamic Perceiver - Han et al. (2023)), do not train gates.\n\nFor the fixed backbone case, which is the setting of our work, the most common approach is to either focus (i) on training the IMs using fixed gates; or (ii) on training only the GMs with a fixed backbone and pre-trained IMs. For example, the method proposed by Han et al. (2022b), L2W, does use a trainable gating mechanism. It trains the inference modules. It is this specific difference in training approach that leads to a major difference in exit behaviour, which the reviewer correctly pointed out in Figure 2. \n\nAnother reviewer kindly pointed us to a work - Scardapane et al. (2019) -that does perform joint training of the gates and the IMs (with a fixed backbone). We have added it in our Table 4, and provided results comparing to it in Section 8.13, showing that our JEI-DNN significantly outperforms.\nCompared to the proposal in Scardapane et al. (2019), a key innovation of our work is (i) a novel modelling the exit probabilities; and (ii) the bi-level optimization strategy. These combine to make the learning procedure much more robust."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374230748,
                "cdate": 1700374230748,
                "tmdate": 1700374893981,
                "mdate": 1700374893981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "68MJPpVJvQ",
                "forum": "jX2DT7qDam",
                "replyto": "0pY7Wq1l26",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/4"
                    },
                    "comment": {
                        "value": "### 1.3. Similarity of Loss Terms to Prior Work:\n The critical aspect of our loss formulation and bi-level optimization is the interaction of the learning for the gates and IMs. While there is a superficial similarity between the forms of the losses and the losses in previous work, the model parameterizations and learning process are considerably different. Specifically, it is the combination of the losses that is important. The two loss terms interact via the bi-level optimization; updates of the IM parameterss continually influences the target of the binary tasks of the GMs, and updates of the GM parameters change the weights in our weighted cross entropy loss. \n\nStarting with the gate loss, while it is true that we sum the losses of independent binary variables $g^l_{\\phi}$, our binary variables $g^l_{\\phi}$ do not directly correspond to exit probability in the way that is common in the literature. In particular, we do not set $ P(G=l)$ $= g^l_{\\phi} \\prod_{i=1} ^{l-1} (1-g^i_{\\phi}) $, as most learnable GMs approaches do. The parameterization of our exit probabilities given by the equations (7) and (8) (and therefore our whole gating mechanism) is different and novel.  We acknowledge that the way we described our gate mechanism might have been too compact, so we have provided an additional figure to clarify our architecture.\n\nNext, we turn to the IMs and our main task loss. By dividing the parameters in our bi-level approach, we obtain two losses, one for the gate parameters $\\phi$ and one for the IMs parameters $\\theta$. The loss for the IM parameters includes a term with a weighted cross entropy loss, but, critically, the weights are driven by the probability of a sample exiting at that given gate ($P_{\\phi}(G=l|\\mathbf{x}_i)$). In contrast, the weights in the formulation of Han et al. (2022b) are not connected to the exit probability. \n\nWe arrive at the IM loss by starting from a principled combined loss that encompasses both the GMs and the IMs, balancing accuracy and computation. In contrast, Han et al. directly start from the weighted IM loss formulation, with the weights being modeled by a weight predictor network (WPN). There is no interaction with the IM parameters during training. The WPN takes as input the losses at each IM to predict the weight, so it cannot be used at inference. As a result, whereas our weights are tied to the probability of exiting, as they should be, the weights in the loss of Han et al. (2022b) are heuristic, selected somewhat arbitrarily to reduce the train-test mismatch. Our loss incorporates the gate parameters that are actually used at inference, while Han et al. (2022b)'s loss does not. \n\nAll of these differences are major. We can confirm empirically that these differences are major because they lead to significantly different behavior, as shown in Figure 2.\n\n### 1.4. Constructing surrogate problems:\nWe agree that constructing a surrogate problem is not novel and is not part of our claimed contribution (it does not appear in the paragraph at the end of the introduction). It is merely a detail of the optimization process.\n\n### 1.5 Figure 2a exit behaviour:\nThis is a very interesting point and we appreciate that you have raised it. When examining the accuracies, we need to be careful because the accuracies at layers 5 \\& 6 in Figure 2a) are only *for the samples that exit there*. These accuracies are very high, showing that the gates have effectively learned to exit the correct samples.  The accuracy at layers 5 \\& 6 for all the samples of the dataset is much lower, as we can see in Figure 2b). This figure is for a specific $\\lambda=1$ (the corresponding operating point is marked with a star in the left panel of Figure 1). For that particular parameter, it is not unexpected that the gates have learned to avoid the first 3 gates entirely and only start to exit a very few samples at gate 4. \nThe decision being made is whether the drop in cross-entropy loss (in going to gate 5 from gate 4, for example) is worth the additional cost of inference. For the choice $\\lambda=1$, it is not the case for many samples. While IM 4 may still be able to classify some subset of the points with accuracy of 90 percent, IM 5 would classify that same subgroup with accuracy 95 percent. In Appendix 8.15. of the revised paper we now provide the same figure for a higher $\\lambda=3$,  corresponding to a greater penalty on computation, and $\\lambda=0.5$, corresponding to a reduced penalty on computation, with more focus on accuracy. For the higher $\\lambda=3$, we observe that now the gates decide to exit many more samples at earlier gates (2,3,4).  This behavior is a consequence of our novel early-exit framework. These results indicate that the $\\lambda$ parameter does provide meaningful control over where samples exit."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700374704483,
                "cdate": 1700374704483,
                "tmdate": 1700376213389,
                "mdate": 1700376213389,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "njWpcpvaoR",
                "forum": "jX2DT7qDam",
                "replyto": "MuysMN8DPc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8069/Reviewer_KZAV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8069/Reviewer_KZAV"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their detailed responses and the newly added experiments that resolve most of my concerns (including adding new backbones and experiments on ImageNet). I raise the contribution score to 3, and the overall score to 6, accordingly."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8069/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668471563,
                "cdate": 1700668471563,
                "tmdate": 1700668471563,
                "mdate": 1700668471563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]