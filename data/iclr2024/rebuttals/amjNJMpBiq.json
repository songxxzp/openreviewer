[
    {
        "title": "Getting a-Round Guarantees: Floating-Point Attacks on Certified Robustness"
    },
    {
        "review": {
            "id": "m9nDe7zxaw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5156/Reviewer_GFuC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5156/Reviewer_GFuC"
            ],
            "forum": "amjNJMpBiq",
            "replyto": "amjNJMpBiq",
            "content": {
                "summary": {
                    "value": "The paper investigates the floating-point soundness of adversarial robustness certificates in two settings: First, a \"weak threat model\" where both the perturbation norm and the certified radius are computed using floating-point arithmetic and second, a \"strong threat model\" where a floating-point sound upper-bound on the perturbation norm is used instead. In the strong setting, the paper shows soundness violations for both random linear models and SVMs trained on the binary distinction of MNIST digits, where the (up to floating-point soundness) exact certified radius can be computed trivially. In the weak setting, the paper additionally shows floating point violations for small (positive) neural networks, analyzed with popular methods. Their attack is based on a PGD variant combined with a random search over neighboring floating-point values. Finally, the paper proposes to address this issue by using interval arithmetic to compute the robustness certificates."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "## Strengths  \n* The tackled issue of (certified) adversarial robustness is of high importance.\n* The paper is mostly easy to understand.\n* To the best of my knowledge, this paper is the first to demonstrate floating point attacks around test-set images on unperturbed networks.\n* While simple, the random floating point neighbour search (Alg. 3) seems effective."
                },
                "weaknesses": {
                    "value": "## Weaknesses\n* The proposed ReluPGD (Algorithm 1 and 2) seems to simply recover standard PGD with $\\ell_2$ projections, which the authors do not acknowledge.\n* The literature review is missing key works such as Singh et al. (2018 and 2019), which propose floating-point sound neural network verifiers leveraging interval arithmetic, the exact approach proposed as novel by this paper. Similarly although less crucial, related work on floating-point sound Randomised Smoothing is missing (Vor\u00e1\u010dek and Hein 2022).\n* Experimental details are missing at many points (e.g. 32 vs 64-bit floating-point precision, Gurobi tolerances and precisions) that make it not only hard to reproduce results but even to assess their significance.\n* In many settings, only the \"weak threat model\" is effective, which mostly seems to attack the floating-point soundness of the norm computation (see Figure 2a) and not the network verifier. As success rates for non-random linear classifiers are less than 0.2 % in the strong setting (with no success for non-linear classifiers), this should be communicated clearly.\n* The claim of verifying \"Neural Nets\" with \"conservative\" methods is misleading given that 2 of the 3 considered neural nets are actually linear (positive inputs and positive parameters ensure all ReLUs become identity functions), in which case verification is exact and the last one is only analyzed with MILP, which for a network of such small size should not only theoretically but also practically be exact.\n\n**References**  \n* Vor\u00e1\u010dek, V\u00e1clav, and Matthias Hein. \"Sound randomized smoothing in floating-point arithmetics.\"\u00a0arXiv 2022\n* Singh, Gagandeep, et al. \"An abstract domain for certifying neural networks.\"\u00a0 POPL 2019\n* Singh, Gagandeep, et al. \"Fast and effective robustness certification.\" NeurIPS 2018"
                },
                "questions": {
                    "value": "### Questions\n1) It seems that the gradients of the linearized network are identical to those of the standard model. Thus Alg. 1 seems to simply recover the gradient $\\nabla_x F^t(x) - F^l(x)$ and thus, Alg.2 standard gradient descent. Can you comment on this?\n2) Why did you choose $x_i = 3.3 \\times 10^9$ for Figure 2b? From reading just the text, one would assume $x_i \\in [-1, 1]$, making this slightly misleading. This also seems to be the source for the upper bound on floating-point errors cited in the intro. Given that this work mostly claims to be the first to show attacks on unperturbed networks and regions around test-set images, this seems misleading.\n3) Your conclusion that \u201cwith the increasing rounding error, a greater leeway is left between the real certified radius R and the computed certified radius $\\tilde{R}$ for our method to exploit, so the attack success rate increases.\u201d seems to contradict the results of Figure 2a), which show that while the rounding error in computing $||\\delta||$ becomes increasingly more pronounced as dimensionality is increased (Gap between strong and weak threat model), the attackability under the strong threat model (ignoring this error) steadily decreases.\n4) What precision was used to compute the robust radii for linear SVMs? And similarly what precision is used to compute the perturbation norm in Section 4.3? Can you clarify what settings are used for Gurobi, in particular, what precision and feasibility tolerances?\n5) For positive inputs, any ReLU network with positive weights and biases is linear, under these circumstances, linear bound propagation methods such as $\\beta-CROWN$ are exact (up to floating point errors) and should thus not be considered \u201cconservative\u201d. Similarly, for ReLU networks, verification via MILP is complete, thus while the obtained certificate is not \u201ccomplete\u201d using the definition in this work, it should not be considered \u201cconservative\u201d. Does your approach also work for settings where the analysis is actually conservative? E.g. using $\\beta$-CROWN on a network with 2 hidden layers of 20 Neurons each?\n7) Could you discuss why you expect verification (not norm computation) with interval arithmetic to reduce the success rate of your attack to 0% under the weak threat model? It seems that the norm computation should still be attackable in the weak setting (unless interval arithmetic is also used there, which should be clarified). Possibly, analysis with interval arithmetic merely introduces enough conservativeness to prevent this type of attack.\n\n### Comments\n* When using a citation marker as a grammatical element of a sentence, I suggest using \"\\citet\" and otherwise \"\\citep\" to comply with the author guide section 4.1.\n* I would suggest you rewrite the second bullet point in core contributions to more clearly distinguish the two threat models.\n* Typically, a different definition of completeness of robustness verifiers is used in the literature. That is, a method is called complete if it can prove every robustness property that holds (see e.g. your closest related work Jia and Rinard (2021)). I would consider adapting this definition or at least highlighting this difference.\n* Python permits exact rational arithmetic and infinite precision floating point real arithmetic with suitable packages. I would consider this when discussing the strong threat model in Section 3.\n* Should $R$ in the first paragraph of Section 4 not be $\\tilde{R}$ ?\n* I suggest using different line types in Figure 2 a to make W and S easier to distinguish\n* Table captions should be placed above a table not below to comply with the author guide Section 4.4.\n\n### Conclusion\nIn summary, while the goal of this paper to demonstrate the vulnerability of neural network verification methods to floating-point attacks on unmodified networks around test-set samples to be interesting and novel, I believe this claim is not sufficiently substantiated by the conducted experiments. Only a single non-linear neural network is considered and only attacked successfully under the \"weak threat model\" which merely demonstrates that the used norm computation can be attacked. Further, there are substantial questions regarding the novelty of core parts of the paper, with Alg. 1 and 2 seeming to recover standard PGD and the suggestion of using interval arithmetic for verification already being established 5 years ago."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5156/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5156/Reviewer_GFuC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697440729659,
            "cdate": 1697440729659,
            "tmdate": 1699636510286,
            "mdate": 1699636510286,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oLNwppGust",
                "forum": "amjNJMpBiq",
                "replyto": "m9nDe7zxaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "$\\mathbf{Weakness}$\n\n$\\mathbf{Ans.1}$\nReluPGD is only half of our attack (in the case of ReLU nets), \nand it can be replaced by any standard approach to finding directions for adversarial perturbations. \nAs the name was already suggesting, ReluPGD is indeed an instantiation of PGD. \nHowever we acknowledge that we can better highlight this (obvious) connection. \nOur inclusion of ReluPGD (Algorithms 1 and 2), derived by hand, \nwas to emphasize that it is exact for a local linearization. \nThe choice of ReLU network was two-fold: (1) it is an important architecture in practice \n(2) it permits strong attacks. ReluPGD is intended to transparently highlight this second point. \nHand derivation is by no means an interesting contribution, which is why the details are relegated to the appendix. \nThe second part of our attack (Algorithm 3) is where we innovate \nby leveraging the structure of certificates to isolate floating-point rounding problems. \nThis simple configuration of the step size is enough to demonstrate violations of certification guarantees.\n\n$\\mathbf{Ans.2}$\nThank you for listing these works. We will add them to the paper.\nSingh et al. (2018 and 2019) also use interval arithmetic in neural network certification, and are sound w.r.t. floating-point arithmetic.\nHowever, Singh et al. (2018 and 2019) use the $\\ell_{\\inf}$ metric in neural network certification,\nwhich avoids the influence of potential rounding errors in the calculation of the perturbation norm.\nWe use the $\\ell_2$ metric, and handle the impact of rounding errors in the calculation of \nthe perturbation norm for certification in the strong threat model.\n\nVor\u00e1\u010dek and Hein (2022) attacked randomized smoothing by also exploiting the finite representation of floating points.\nHowever, they attacked synthetic data and model, while we show attacks on unperturbed networks and test-set images.\n\n$\\mathbf{Ans.3}$\nFor random linear classifiers in Section 4.1, \nwe use both 32-bit and 64-bit floating-point precision, as shown in Fig 2(a).\nFor Linear SVM, we use 64-bit floating-point precision.\nFor neural nets, we use 32-bit floating-point precision.\nGurobi tolerance is $2\\times10^{-5}$.\n\n$\\mathbf{Ans.4}$\nIn the strong threat model, we used the upper bound of the perturbation norm $\\overline{\\|\\boldsymbol{\\delta}\\|}$, not the real norm $\\Delta$.\nThe difference between the weak ($\\tilde{R}-\\|\\boldsymbol{\\delta}\\|$) and \nstrong ($\\tilde{R}-\\overline{\\|\\boldsymbol{\\delta}\\|}$) models is not the floating-point soundness of the norm computation.\nIt can be the floating-point soundness of the norm computation if we attack ($\\tilde{R}-\\Delta$) in the strong threat model instead.\n\n$\\mathbf{Ans.5}$\nFor MILP, we verify two 3-layer neural network multiclass classifiers with 100 nodes in their hidden layer,\none of them is trained normally, whose parameters (weights and biases) can be negtive.\nFor Randomized Smoothing, we also attack a 3-layer network with negtive parameters.\nThe verification from MILP could potentially be conservative,\nand the verification from Randomized Smoothing should mostly be conservative.\n\n$\\mathbf{Questions}$\n\n$\\mathbf{Ans.1}$\nPlease refer to the answer of W.1.\n\n$\\mathbf{Ans.2}$\nWe want to show how big rounding errors could potentially be in Figure 2b.\nIn the paper, we do state that the rounding errors exploited by our attack are small. \nPlease see the second paragraph of Section 5:\n\u201cFor example, the rounding error for the certified radius of the first MNIST image of Figure 3 (Appendix A) is in the 13th decimal place\u201d.\n\n$\\mathbf{Ans.3}$\nIn the strong threat model, we use the upper bound of the perturbation norm $\\overline{\\|\\boldsymbol{\\delta}\\|}$ estimated with interval arithmetic.\nWith increasing dimension, more operations are involved, interval arithmetic gets more uncertain (loose) about its estimation, \nand outputs a bigger interval that contains the real result.\nThat is, the upper bound $\\overline{\\|\\boldsymbol{\\delta}\\|}$ gets bigger (estimated more loosely).\n\nIn all, with increasing dimension, $\\tilde{R}$ gets bigger, but $\\overline{\\|\\boldsymbol{\\delta}\\|}$ also gets bigger.\nHence, the leeway $\\tilde{R}-\\overline{\\|\\boldsymbol{\\delta}\\|}$ exploited by our attack in strong models\nfirst gets bigger ($D\\le18$) and then gets smaller ($19\\le D\\le100$),\nand our success rate first increases then decreases.\n\n$\\mathbf{Ans.4}$ \nPlease refer to the answer of W.3.\n\n$\\mathbf{Ans.5}$\nPlease refer to the answer of W.5.\n\n$\\mathbf{Ans.6}$\nOur mitigation introduces theoretically enough conservativeness (i.e., $\\gamma = \\tilde{R} - \\underline{R}$) \nby providing $\\underline{R}$ as a robustness certificate.\n$\\underline{R}$ is safe in the strong threat model.\nIn the weak threat model, this $\\gamma$ will not have same provable guarantees. We will clarify this."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615805745,
                "cdate": 1700615805745,
                "tmdate": 1700615805745,
                "mdate": 1700615805745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YECCtuGfvq",
                "forum": "amjNJMpBiq",
                "replyto": "oLNwppGust",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5156/Reviewer_GFuC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5156/Reviewer_GFuC"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I greatly appreciate the authors' efforts in addressing my questions, however several of my key concerns are not sufficiently addressed:\n\nWeaknesses: A1, A2, A4, A5\nQuestions: A2, A3\n\nI thus maintain my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726477421,
                "cdate": 1700726477421,
                "tmdate": 1700726477421,
                "mdate": 1700726477421,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "285u0xrbfP",
            "forum": "amjNJMpBiq",
            "replyto": "amjNJMpBiq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5156/Reviewer_musS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5156/Reviewer_musS"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript pokes a hole on robustness certificates due to the implementation issue. That  is, the floating point representations could result in unsound certifiable radius for many methods as listed in this paper. In general this is a system bug in the certification process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-motivated and well-written. The problem it identifies is novel and also critical to the application of certifiable robustness in real-world systems. Float32 is probably the standard one we will use, and in the real-world we will have more quantizations for resource constraints. If this bug needs to be gone only with float128 or even more, I don\u2019t think we would actually deploy the certification method."
                },
                "weaknesses": {
                    "value": "There are no significant weaknesses in this paper. One minor thing is that it only experiments with linear models and pretty small networks. The impact of this bug could be much more serious if the authors can experiment with larger datasets and larger models. Furthermore, there is no discussion on non-ReLU models and Lipschitz-based certification methods. Does the bug also appear in those models? Can the attack generalize the MinMax models? I did not see real issues why one could\u2019t try to find unsound points for non-ReLU networks. Can the authors explain the blockers here? With that being said, I would appreciate the authors to have a paragraph to discuss what the limit of the attack they find. The family of methods will be affected and the family of methods that won\u2019t be affected. This would add great value to the current paper."
                },
                "questions": {
                    "value": "My questions are included in the Weakness part already."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786136748,
            "cdate": 1698786136748,
            "tmdate": 1699636510182,
            "mdate": 1699636510182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VAU9Y7ng64",
                "forum": "amjNJMpBiq",
                "replyto": "285u0xrbfP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "$\\mathbf{Ans.1}$\nVerification methods such as $\\beta$-CROWN and MIP tend to \ngive tighter estimates of robustness certificates for smaller and simple neural nets.\nThat is, those methods can verify a relatively bigger (tighter) $\\tilde{R}$ for a small model,\nwhich results in a bigger leeway $\\tilde{R}-R$ for our attack to exploit and allows higher attack success rates.\nFor larger neural nets, the certificates could be more conservative,\nbut larger neural nets (more operations) could also entail larger rounding errors. \nWe thank the reviewer for this suggestion. We hope that any violations of the intended certificate by a mechanism on any sized network would demonstrate the issue to researchers in the area. We hope that before future deployments researchers communicate potential implementation flaws to end users, or adopt mitigations like those proposed here.\n\n\n$\\mathbf{Ans.2}$\nThough in theory the attacks should still hold, attacking MinMax models and Lipschitz-based certificates in practice\nis an interesting direction for future work.\nWe chose ReLU nets owing to it being (1) used frequently in practice \n(2) being readily linearizable, with the intention of simply demonstrating a range of certificate violations.\n\nOur current limitations were summarised in the submission in Appendix G (being in the supplemental it may have been missed). We identify the following limitations, that could also be addressed as future work. \nFirst, our attacks against randomized smoothing do not study the relationship \nbetween the attack success rate and the soundness probability of these methods due to their probabilistic nature. \nA possible future direction would be to study how the two interact.\nSecond, we showed how to adopt our mitigation based on interval arithmetic generally, \nand demonstrate this mitigation for linear models and exact certifications. \nDetailed exploration of embedding these mitigations in other verifiers and models is left as future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614034188,
                "cdate": 1700614034188,
                "tmdate": 1700614034188,
                "mdate": 1700614034188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kyvfSvM8Jk",
                "forum": "amjNJMpBiq",
                "replyto": "VAU9Y7ng64",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5156/Reviewer_musS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5156/Reviewer_musS"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for there reply. These responses should go into the main body of the paper. Similarly, the limitation of the work should be placed in the main body as well (there is no reason to put them into the Appendix)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679031613,
                "cdate": 1700679031613,
                "tmdate": 1700679031613,
                "mdate": 1700679031613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zF4cRMh5KK",
            "forum": "amjNJMpBiq",
            "replyto": "amjNJMpBiq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5156/Reviewer_s866"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5156/Reviewer_s866"
            ],
            "content": {
                "summary": {
                    "value": "Certified defenses against adversarial attacks for ML classifiers provide a mathematical upper bound $R(x)$ on the amount of corruption needed to misclassify a given input $x$. This paper studies the discrepancy between the radius $\\bar R(x)$ output by an implementation of such defenses on systems supporting finite-precision arithmetic, say $1.0$, and the _real_ radius $R(x)$ guaranteed by the theory, say $0.999999$, demonstrating adversarial perturbations that _break_ such certified defenses by exploiting floating point approximations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper highlights a potentially overlooked problem with trusting certified defenses too much: the finite precision implementation of such defenses on computers might lead to slightly optimistic certificates. Hence, this paper highlights the fact that practitioners critically relying on such systems must use a marginally lower value of the certified radius than reported."
                },
                "weaknesses": {
                    "value": "1.\tRelevance: The greatest weakness of this paper is the lack of relevance of the problem studied. Specifically, it is very well known that computers perform finite-precision arithmetic, and for any computation (much beyond adversarial robustness), the results might have a minuscule margin of error due to the floating point approximations involved. In the light of this, any application that requires exact precision must implement explicit safeguards (e.g. interval arithmetic) to protect against rounding problems. For the case of adversarial examples, it would be good if the authors could provide examples of applications where an application would care about getting the certified radius exactly right (this is not true for instance on many commonly studied applications like image classification, object detection, etc.). \u2028\n\n2.\tTrivial Solution?: A na\u00efve fix to the above problem, in the context of certified robustness, which the paper also mentions on P2, is that for super-sensitive applications, one can simply report $R(x) - \\gamma$ as the certificate for a small $\\gamma$, instead of $R(x)$ to ensure that the output certificate is valid. $\\gamma$ can be really small, for instance, P8 mentions that for MNIST, the approximation errors are at the 13th decimal point. The paper argues that finding a fixed $\\gamma$ is not possible in general, but fails to provide convincing evidence on any real tasks. On a synthetically constructed task, $\\gamma$ is showed to be potentially large (0.1), but then the scale of $R$ is orders of magnitude larger leading to the same solution. Generally, depending on the task at hand, there should be a principled way to choose $\\gamma$ depending on scale of the error that can be tolerated for the problem.\u2028\n\n3.\tProposed method exponential in dimension?: The proposed approach to finding adversarial perturbations, is roughly to perturb each dimension of the input a bit till a misclassification is achieved. A slightly smarter way of doing these floating point flips is utilized, but the approach is essentially exponential in dimension. How much time does the proposed algorithm take per image? from P8, it seems like a time-out of 15 minutes is set for every image. \u2028\n\n4.\tProposed Method ReluPGD same as PGD applied to max-margin objective? Could the authors comment on how the ReLUPGD algorithm differs from a standard PGD attack using the max-margin loss (e.g., loss(positive class) - loss(negative class) for the binary case). \u2028\n5.\tProposed Method\u2019s applicability in real scenarios: In practice, most certification methods for Neural Networks produce certificates that are very pessimistic for most input points. In light of this, it is hard to believe that a very high decimal place attack would even be successful for real networks. The examples shown in this work linearize the network or study synthetic problems with linear boundaries, and this effect does not show up. \u2028\n\n6.\tWriting: The writing could be improved at several places, and some portions could be cut to make way for the core contributions:\n\t1.\tP2 Contribution 1: \u201cinvalidate the implementations\u201d \u2014  invalidate might be a too strong word here.\n\t2.\tP2 Contribution 2: \u201cfloating point norm\u201d is not defined till now. \n\t3.\tStatement unclear: \u201cin the case where the library computes \u2026\u201d, which library are we talking about here?\n\t4.\tP2 Contribution 3: Cohen et. al.\u2019s main certificate is probabilistic. \n\t5.\tP3: No reference needed for distance of a point to a plane. \n\t6.\tP4: The main contribution, the \u201cneighbors\u201d algorithm could be moved to the main text for clarity, currently it is unclear what this is. \n\t7.\tP5: It is unclear whether the entire warmup section is needed, since the end result is a PGD-like algorithm, and PGD is quite standard in the literature now. \n\t8.\tP5: The algorithm ReLUPGD should appear in the main paper, since it is one of main contributions. \n\t9.\tFig 2, P6: For the synthetic experiments, what is the setup, is $x, w, b$ fixed and $D$ is changing? If so, how were these values decided? Are there any average statistics over these values?\n\t10.\tP9: Theorem 1 is a standard statement of interval arithmetic, and as such it is unclear whether it should be a theorem."
                },
                "questions": {
                    "value": "Questions are mentioned above alongside the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5156/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5156/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5156/Reviewer_s866"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808094827,
            "cdate": 1698808094827,
            "tmdate": 1699636510081,
            "mdate": 1699636510081,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jyKKupXNCC",
                "forum": "amjNJMpBiq",
                "replyto": "zF4cRMh5KK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "$\\mathbf{Ans.1}$\nThe reviewer is right that it is well-known that flaws are known in regards to finite-precision arithmetic.\nHowever, this observation was neither made before against exact certification mechanisms nor shown to work against conservative mechanisms.\nOur work has demonstrated robustness violations against a wide range of certifications on unperturbed networks and test-set images,\nand reminds practitioners that if certified robustness is to be used for security-critical applications, \ntheir guarantees and implementations need to account for limitations of modern computing architecture. \n\n\n$\\mathbf{Ans.2}$\nWe introduced the idea of a radius correction $\\tilde{R}-\\gamma$ as a straw argument: \nit is meant to be flawed. We hope the reviewers and AC understand that non-zero attack success rate is not acceptable. \nThis is because such a mechanism (with constant $\\gamma$) would not be sound and introduce uncontrolled errors.\nWe showed in Section 5 that we can still find violations if we set certificate as $\\tilde{R}-0.1$.\n\nThe reviewer asks if there is a principled way to find $\\gamma$. The only approach known to us is input dependent \n(non-constant) $\\gamma = \\tilde{R} - \\underline{R}$, such that $\\tilde{R}-\\gamma = \\underline{R}$,\nwhere $\\underline{R}$ is the lower bound of the certified radius estimated with interval arithmetic, \nand we propose to use it in our mitigation as the certificate.\n\n$\\mathbf{Ans.3}$\nWe avoid exponential runtime by adopting random sampling in the neighboring search algorithm as follows.\nWe randomly sample $N$ floating-point neighbors of the adversarial instance $\\mathbf{x'}$ to \nexplore robustness violations close to the decision boundary due to rounding errors.\nFor example, $N$ is 5000 in Section 4.2.\n\n\n$\\mathbf{Ans.4}$\nReluPGD is only half of our attack (in the case of ReLU nets), \nand it can be replaced by any standard approach to find directions for adversarial perturbations. \nAs the name was already suggesting, ReluPGD is indeed an instantiation of PGD. \nOur inclusion of ReluPGD (Algorithms 1 and 2), derived by hand, \nwas to emphasize that it is exact for a local linearization. \nThe choice of ReLU network was two-fold: (1) it is an important architecture in practice \n(2) it permits strong attacks. ReluPGD is intended to transparently highlight the second point. \nHand derivation is by no means an interesting contribution, which is why the details are relegated to the appendix. \n\nThere maybe confusion from Reviewer s866 on where our contribution lies \n(i.e., it is not a new general-purpose PGD algorithm).\nFor example the second part of our attack (Algorithm 3) is where we innovate \nby leveraging the structure of certificates to isolate floating-point rounding problems. \nThis simple configuration of the step size is enough to demonstrate violations of certification guarantees.\n\n\n$\\mathbf{Ans.5}$\nVerification methods such as $\\beta$-CROWN and MIP tend to give tighter estimates of robustness certificates for smaller neural nets.\nThat is, those methods can verify a relatively bigger (tighter) $\\tilde{R}$ for a small model,\nwhich results in a bigger leeway $\\tilde{R}-R$ for our attack to exploit and allows higher attack success rates. \nWe experiment with small neural nets that are normally trained and have 100 neurons in the hidden layer on the real MNIST dataset in Section 4.3, \nand show that even if the robustness certificates are potentially conservative,\nwe can still find violations for those that are relatively tight, although not exact.\nFor larger neural nets, the certificates could be more conservative,\nbut larger neural nets (more operations) could also entail larger rounding errors, so a successful attack is still possible. \nMoreover, our attack exploits subtle floating-point errors, as we explained in the second paragraph of Section 5:\nFor example, the rounding error for the certified radius of the first MNIST image of Figure 3 (Appendix A) is in the 13th decimal place.\n\nThe purpose of our work is to show that certifications can be attacked.\nThe fact that all the certification mechanisms we examined are unsound should be of serious concern.\nThere is nothing to indicate that higher levels of rounding (and hence rounding attacks) \nwouldn't be possible in other datasets, or domains. Without sound mechanisms, \none cannot trust implementations to perform as advertised.\n\n\n$\\mathbf{Ans.6}$\nFor the library, we refer to the library that computes the square root for the norm \nand represents it as an interval, i.e., PyInterval in our study. \nFor Fig 2, we experiment with models of different dimensions, with $D$ ranging from 1 to 100,\nas we want to explore how the dimension influences our attack.\nOne dot in Fig 2 stands for an experiment on a model of a certain dimension (e.g., $D=50$)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615170353,
                "cdate": 1700615170353,
                "tmdate": 1700638980648,
                "mdate": 1700638980648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tSRYzPwaRC",
                "forum": "amjNJMpBiq",
                "replyto": "jyKKupXNCC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5156/Reviewer_s866"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5156/Reviewer_s866"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses. As other reviewers pointed out, the relevance of this study is unclear in light of simple fixes (non-constant $\\gamma$ as simply a small constant fraction of $R$), and existence of prior work as well as general intuition around the floating point problem (the authors say that prior work study other norms or synthetic settings, however the  experiments in the present work are arguably pretty toyish), and unclear experimental choices (scale of $R$ vs $\\gamma$, lack of aggregate statistics). In light of these issues, I will keep my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683143993,
                "cdate": 1700683143993,
                "tmdate": 1700683143993,
                "mdate": 1700683143993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Niv76UDgHZ",
            "forum": "amjNJMpBiq",
            "replyto": "amjNJMpBiq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5156/Reviewer_FP18"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5156/Reviewer_FP18"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new attack methodology to break robustness certificates given by existing methods. It exploits the rounding errors present in the floating point computation of the certificates. The authors test the efficacy of their attack on linear classifiers, linear SVM, and neural network models and empirically show that such exploits exist for all these models. Finally, the authors propose a formal mitigation approach based on bounded interval arithmetic to prevent such exploits."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The high success rate of the proposed attack shows the magnitude of the problem and the need for it to be addressed.\n- The initial proposed solution might help spark some more research for making existing certification more robust to such exploits."
                },
                "weaknesses": {
                    "value": "- While the paper states that it is not easy to fix the problem by just replacing the certified radius $\\tilde{R}$ with $\\tilde{R} - \\gamma$, ($\\gamma << 1$), it is not clear what the attack success rate looks like when a $\\gamma$ that is a small fraction of $\\tilde{R}$ is used. As the strong threat model already leads to a huge drop in attack success, it seems plausible that just certifying a slightly more conservative radius might suffice for practical use (as the bounds in some cases like Randomized Smoothing are already probabilistic).\n- The proposed method is not shown for most of the state-of-the-art certification methods. It would be great to see the effective drop in certified radius that happens when using the proposed method and see the computation cost as well."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses section for the questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5156/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820076327,
            "cdate": 1698820076327,
            "tmdate": 1699636509966,
            "mdate": 1699636509966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IraFk3G4ZR",
                "forum": "amjNJMpBiq",
                "replyto": "Niv76UDgHZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5156/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5156/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "$\\mathbf{Ans.1}$\nWe introduced the idea of a radius correction $\\tilde{R}-\\gamma$ as a straw argument: it is meant to be flawed. \nWhile we understand that Reviewer FP18 is interested in attack success rates for small constant $\\gamma$, \nwe hope the reviewers and AC understand that non-zero attack success rate is not acceptable. \nThis is because such a mechanism (with constant $\\gamma$) would not be sound. \nWe showed in Section 5 that we can still find violations if we set certificate as $\\tilde{R}-0.1$; \nwith increasing dimension there is increased opportunity for undesirable rounding.\n\nReviewer FP18 highlights the probabilistic nature of certificates for Randomized Smoothing: \nwhile it is true that such mechanisms have a failure probability, \nthis probability is still controlled and only accounts for sampling error in the Monte Carlo estimates \nmade by Randomized Smoothing. Any non-zero failure rate due to rounding (which is inevitable with constant $\\gamma$) \nrepresents a greater failure rate, that is no longer controlled in any way. \nMoreover, for the vast literature on deterministic certificates, \nthe goal is a consistent prohibition of adversarial examples. \nOur proposed mitigation (based on interval arithmetic) uses $\\underline{R}$ as a robustness certificate.\nThis can be understood as a principled way to choose an input-dependent (non-constant) $\\gamma = \\tilde{R} - \\underline{R}$, \nsuch that $\\tilde{R}-\\gamma = \\underline{R}$. \nHence, interval arithmetic provides a sound approach with a provable attack success rate of zero.\n\n$\\mathbf{Ans.2}$\nWe report experiments on representative certification mechanisms of the Exact, Conservative and Randomized types. \nFor the latter two types we used some of the seminal works. \nFor example, MIP certification used for neural networks in Section 4.3 is known for its tight\nestimate for robustness certificates, although its slow running speed on big models is a problem.\nNevertheless, our MIP certifications on small neural networks\ngive tight estimates for robustness certificates with acceptable runtime, and allow our attack to have high success rates.\nThough it is interesting to see our methods against all certification methods, \nour contribution is against the fundamental observation that theoretical (real arithmetic based) radius does not match the one computed in practice."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5156/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612689023,
                "cdate": 1700612689023,
                "tmdate": 1700613689018,
                "mdate": 1700613689018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]