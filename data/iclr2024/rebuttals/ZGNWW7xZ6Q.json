[
    {
        "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning"
    },
    {
        "review": {
            "id": "AUL0HqYBw6",
            "forum": "ZGNWW7xZ6Q",
            "replyto": "ZGNWW7xZ6Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5213/Reviewer_cFys"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5213/Reviewer_cFys"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new framework using external KG to enhance the reasoning ability of LM. The authors finetune LMs (e.g., LLaMA) as the planning and retrieval modules for performing reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Generally, this paper explores an interesting topic with a reasonable method design. Experiment results also look good.\n\n1. Framework design. This reasoning framework design is reasonable: the explicit KG usage makes the reasoning process interpretable and controllable. \n\n2. The two modules (planning and retrieval) finetuning works well. Experimental results show that indeed using the RoG framework with the two tuned modules, better reasoning performance can be achieved."
                },
                "weaknesses": {
                    "value": "However, I have several concerns about this work.\n\n1. From equation 1, I understand the authors want to decompose the prediction task into two parts: getting hidden variable z and then predicting the answer with the hidden variable. However, it's not clear whether equations 2-6 are necessary. Could you provide more intuition behind for explanation? \n\n      Even if motivation exists, I'm not sure why equation 3 holds. Why is there only one path / relation path and it's faithful? For example, there could be multiple solutions/paths; if so, equation 3 might not hold. The authors should provide more justification for the equation 3.\n\n2. One of the key points in the framework is the finetuned LLM (i.e., \\theta). However, in the main paper, it's not clear how the LLM is finetuned. It seems the objective functions follow the equation 7. However, these two modules are not evaluated individually and there is no validation loss provided. Only the final framework performance can prove that these modules work as expected, which is not sufficient for readers to know why they can work.\n\n3. Over-claim sentences. The authors claim that their RoG framework can address the hallucination issue and lack of knowledge issue. However, studies with several cases are definitely not sufficient to prove them. I would suggest adding more comprehensive results or changing the claim."
                },
                "questions": {
                    "value": "1. For the KG, is that constructed by yourself and used for all tasks, or is it provided in the dataset?\n\n2. In equation 4, the final equality, the constant term is missing.\n\n3. In Table 2, RoG is finetuned LM, but it's compared with LLMs under the zero/few-shot setting. Is that a fair comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Reviewer_cFys"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698410482875,
            "cdate": 1698410482875,
            "tmdate": 1700127798089,
            "mdate": 1700127798089,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G5JeczceJj",
                "forum": "ZGNWW7xZ6Q",
                "replyto": "AUL0HqYBw6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cFys"
                    },
                    "comment": {
                        "value": "## Weakness 1\n> It's not clear whether equations 2-6 are necessary. Could you provide more intuition behind for explanation?\n> There could be multiple solutions/paths; if so, equation 3 might not hold. The authors should provide more justification for the equation 3.\n\n### Response\nWe thank the reviewer for the question and for pointing out the issue in Equation 3. \n\nIn equation 1, we want to illustrate the general purpose of RoG, which first generates a set of plans (planning module) and then conducts the reasoning on KGs with the plans (retrieval-reasoning module). However, directly optimizing Equation 1 is intractable due to the combination of hidden variable $z$ [1]. Therefore, we derive the evidence lower bound (ELBO) of Equation 1 in Equation 2-6 and come up with the viable optimization objective in Equation 7.\n\nTo clarify the misunderstanding, we want to correct Equation 3 in our paper as follows:\n$$\n\\begin{equation}\n    Q(z) \\simeq Q(z|a,q,\\mathcal{G}) = \\begin{cases}\n    & \\frac{1}{|\\mathcal{Z}|}, \\exists w_z(e_q,e_a)\\in \\mathcal{G},\\\\\\\\\n    & 0, \\text{else},\n    \\end{cases}\n\\end{equation}\n$$\nwhere we assume there are multiple plans $z\\in\\mathcal{Z}$ which obey a uniform distribution [1].\n\n[1] Singh, D., Reddy, S., Hamilton, W., Dyer, C., & Yogatama, D. (2021). End-to-end training of multi-document reader and retriever for open-domain question answering. NeurIPS 2021, 34, 25968-25981.\n\n## Weakness 2\n> One of the key points in the framework is the finetuned LLM (i.e., \\theta). However, in the main paper, it's not clear how the LLM is finetuned. These two modules are not evaluated individually.\n\n### Response\nWe thank the reviewer for the question. We have revised the paper to clarify the training process of RoG in equation 7 (based on the comments of Reviewer 1bmQ). The objective in equation 7 contains two terms: retrieval-reasoning optimization and planning optimization.  We optimize the objective in Equation 7 by maximizing the likelihood of predicting the correct answer and generating valid plans. \n\nTo demonstrate the performance of each module individually, we conduct ablation studies in Table 3, where we remove the planning and reasoning modules, respectively. From the results, we can see that both these modules are helpful for the performance of RoG.\n\nAblation studies of RoG.\n\n| Method            | WebQSP (F1) | CWQ (F1) |\n| ----------------- | ----------- | -------- |\n| RoG               | 70.81       | 56.17    |\n| RoG w/o planning  | 49.69       | 33.76    |\n| RoG w/o reasoning | 49.56       | 22.26    |\n\n## Weakness 3\n> Over-claim sentences. The authors claim that their RoG framework can address the hallucination issue and lack of knowledge issue. However, studies with several cases are definitely not sufficient to prove them. I would suggest adding more comprehensive results or changing the claim.\n\n### Response\nThank you for the comments. We have revised the paper by changing the claim to \"RoG can **alleviate** the hallucination issue and lack of knowledge issue\".\n\n\n## Question 1\n> For the KG, is that constructed by yourself and used for all tasks, or is it provided in the dataset?\n\n### Response\nWe use the KGs preprocessed by previous works [2].\n\n[2] He, G., Lan, Y., Jiang, J., Zhao, W. X., & Wen, J. R. (2021, March). Improving multi-hop knowledge base question answering by learning intermediate supervision signals. WSDM 2021\n\n## Question 2\n> In equation 4, the final equality, the constant term is missing.\n\n### Response\nThanks for pointing it out, we have revised Equation 4. We omit the const in the final objective since it makes no contributions to the optimization. The detailed derivation can be found in Appendix A.1 and in our response to Reviewer 1bmQ.\n$$\n\\begin{align}\n    \\mathcal{L}\\_{\\text{plan}} = D\\_{KL}(Q(z)||P\\_\\theta(z|q)) &= D\\_{KL}(Q(z|a, q, \\mathcal{G})||P\\_\\theta(z|q)), \\\\\\\\\n    &\\simeq - \\frac{1}{|\\mathcal{Z^*}|}\\sum\\_{z\\in\\mathcal{Z^*}} \\log P\\_\\theta(z|q),\n\\end{align}\n$$\n\n## Question 3\n> In Table 2, RoG is finetuned LM, but it's compared with LLMs under the zero/few-shot setting. Is that a fair comparison?\n\n### Response\nThanks for the questions, we have revised our paper by providing the performance of LLM finetuned on the training set of WebQSP and CWQ. From the results, we can see that RoG still outperforms the LLMs finetuned on the training set of WebQSP and CWQ.  We have revised the paper by providing the results in Appendix A.7.3.\n\nPerformance comparison with various finetuned LLMs.\n\n| Method                       | WebQSP (Hits@1) | CWQ (Hits@1) |\n| ---------------------------- | --------------- | ------------ |\n| Alpaca-7B  (Zero Shot)         | 51.78           | 27.44        |\n| LLaMA2-Chat-7B  (Zero Shot) | 64.37           | 34.60        |\n| Alpaca-7B (Finetuned)           | 74.57           | 55.98        |\n| LLaMA2-Chat-7B  (Finetuned)  | 73.89           | 53.49        |\n| RoG                          | 85.75           | 62.65        |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088435086,
                "cdate": 1700088435086,
                "tmdate": 1700088435086,
                "mdate": 1700088435086,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "71Bu6MktnJ",
                "forum": "ZGNWW7xZ6Q",
                "replyto": "G5JeczceJj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5213/Reviewer_cFys"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5213/Reviewer_cFys"
                ],
                "content": {
                    "title": {
                        "value": "Response to the reply"
                    },
                    "comment": {
                        "value": "Thanks for the detailed explanation! My concerns are resolved, and I'll raise my score to accept. Please update the paper correspondingly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127771953,
                "cdate": 1700127771953,
                "tmdate": 1700127771953,
                "mdate": 1700127771953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KR9Wg6qpDn",
            "forum": "ZGNWW7xZ6Q",
            "replyto": "ZGNWW7xZ6Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5213/Reviewer_1bmQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5213/Reviewer_1bmQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new knowledge graph retrieval-based fine-tuning algorithm of LLM, RoG, which shows significant improvement over many baselines, including chatGPT, on two KGQA datasets. The method has two training objectives, one retrieval objective, and one planning objective. The LLM is trained to first generate several reasoning paths and then verify and select the best paths based on a KG. The ablation study shows that both objectives are crucial. The fine-tuned LLM can be regarded as a stand-alone planning module for other LLMs, like ChatGPT, and improve their performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The empirical performance of the proposed method seems to be pretty strong on the two KGQA datasets, compared to many baselines.\n\n2. The proposed method seems to be able to better combine the reasoning power of both LLM and KG."
                },
                "weaknesses": {
                    "value": "1. There is some nonrigorous math in the paper. e.g. in equation 4, the expectation and Q should not coexist. It's either $\\mathbb{E}_Q \\log P$ or $\\sum_z Q \\log P$. In the next line, $z \\in Q$ does not make sense as $Q$ is a probability distribution. Also, the equality does not make sense as there is a CONST in equation 4. Also, I don't think it's a good idea to use equality for an approximation. Similar nonregorousness happens in equation 6. The marginalization in equation 10 does not make sense, as the authors are marginalizing over the conditions. The correctness of the final training objective needs to be double-checked.\n\n2. More datasets to showcase the effectiveness of the proposed method would be great, as there are currently only two in the paper. Would the fine-tuned LLM generalize to other QA datasets, in addition to the datasets that it is fine-tuned on?\n\n3. About RoG as a planning module for other LLMs: I understand that the fine-tuned LLM can also be combined with other LLMs, and improve the performance of these not fine-tuned LLMs. However, according to Table 4, even combining with a stronger LLM (e.g. ChatGPT) cannot improve upon the original fine-tuned LLM. I don't see the usefulness of having this sort of integrability."
                },
                "questions": {
                    "value": "1. Is RoG trained on both WebQSP and CWQ at the same time or is it trained separately on these two datasets? I'm not super familiar with the KGQA baselines, but I wonder if all baselines are trained on the same data as RoG. If the baselines are only trained on one of the datasets each time, then it's not fair to compare RoG with them, if RoG is trained on both of them at the same time. \n\nI'm willing to raise my score if my concerns are properly addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Reviewer_1bmQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594476865,
            "cdate": 1698594476865,
            "tmdate": 1700099720862,
            "mdate": 1700099720862,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rwvJf4bgV4",
                "forum": "ZGNWW7xZ6Q",
                "replyto": "KR9Wg6qpDn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1bmQ"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for specific comments. We have provided a detailed response to each comment below. We hope our answers can properly address your concerns.\n\n## Weakness 1\n> There is some nonrigorous math in the paper. \n\n### Response\nWe truly thank the reviewer for pointing out these issues! We have carefully revised the paper to fix the nonrigorous math equations.\n\nThe revised Equation 4 in our paper is shown here. \n$$\n\\begin{align}\n    \\mathcal{L}\\_{\\text{plan}} = D\\_{KL}(Q(z)||P\\_\\theta(z|q)) &= D\\_{KL}(Q(z|a, q, \\mathcal{G})||P\\_\\theta(z|q)),\\tag{1} \\\\\\\\\n    &\\simeq - \\frac{1}{|\\mathcal{Z^*}|}\\sum\\_{z\\in\\mathcal{Z^*}} \\log P\\_\\theta(z|q),\\tag{2}\n\\end{align}\n$$\nwhere $\\mathcal{Z}^*\\subseteq\\mathcal{Z}$ denotes the set of shortest paths between $e_q$ and $e_a$ in KGs, and $\\mathcal{Z}$ denotes all valid relation paths in KGs.\n\nDue to the limitation of space, we put the detailed derivation in Appendix A.1, which is also shown as follows.\n\nThe posterior distribution $Q(z)$ can be approximated by $Q(z|a, q, \\mathcal{G})$, given as\n$$\n\\begin{equation}\n    \\tag{3}\n    Q(z) \\simeq Q(z|a,q,\\mathcal{G}) = \\begin{cases}\n    & \\frac{1}{|\\mathcal{Z}|}, \\exists w_z(e_q,e_a)\\in \\mathcal{G},\\\\\\\\\n    & 0, \\text{else},\n    \\end{cases}\n\\end{equation}\n$$\nwhere we assume a uniform distribution over all valid relation paths $\\mathcal{Z}$ [1].\n\nThus, the KL divergence can be calculated as\n$$\n\\begin{align}\n\\mathcal{L}\\_{\\text{plan}} = D\\_{KL}(Q(z)||P\\_\\theta(z|q)) &= D\\_{KL}(Q(z|a, q, \\mathcal{G})||P\\_\\theta(z|q)), \\tag{4}\\\\\\\\\n&=\\mathbb{E}\\_{z\\sim Q(z|a, q, \\mathcal{G})}[\\log Q(z|a, q, \\mathcal{G})-\\log P\\_\\theta(z|q)], \\tag{5}\\\\\\\\\n&= - \\mathbb{E}\\_{z\\sim Q(z|a, q, \\mathcal{G})}\\log P\\_\\theta(z|q) + \\text{CONST}, \\tag{6}\n\\end{align}\n$$\nwhere the expectations cannot be computed exactly because of the large number of valid relation paths $\\mathcal{Z}$, so we approximate it by using the set of shortest paths $\\mathcal{Z}^*\\subseteq\\mathcal{Z}$ between $e_q$ and $e_a$ in KGs. This can be formulated as\n$$\n\\begin{align}\n\\mathcal{L}\\_{\\text{plan}} = - \\sum\\_{z\\in\\mathcal{Z^*}} Q(z|a, q, \\mathcal{G}) \\log P\\_\\theta(z|q) + \\text{CONST}. \\tag{7}\n\\end{align}\n$$\nBy assuming a uniform distribution over the set of shortest paths $\\mathcal{Z}^*$, we can rewrite Eq. (7) as\n$$\n\\begin{align}\n\\mathcal{L}\\_{\\text{plan}} & = - \\frac{1}{|\\mathcal{Z^*}|}\\sum\\_{z\\in\\mathcal{Z^*}} \\log P\\_\\theta(z|q) + \\text{CONST}.\\tag{8}\n\\end{align}\n$$\nWe keep the CONST in the equation. However, we omit it in the final objective since it makes no contributions to the optimization. \n\nThe revised Equation 6 in our paper is presented here.\n$$\n\\begin{align}\n    \\mathcal{L}\\_{\\text{reason}} &= \\mathbb{E}\\_{z\\sim Q(z|a, q, \\mathcal{G})}[\\log P\\_\\theta(a|q,z,\\mathcal{G})],\\tag{9}\\\\\\\\ \n    &= \\sum\\_{z\\in\\mathcal{Z}^*\\_K} \\log P\\_\\theta(a|q,z,\\mathcal{G}), \\tag{10}\\\\\\\\ \n    &=  \\log P\\_\\theta(a|q,\\mathcal{Z}^*\\_{K},\\mathcal{G}). \\tag{11}\n\\end{align}\n$$\nThe expectation is approximated by sampling $K$ plans from the $\\mathcal{Z^*}$, denoted as $\\mathcal{Z}^*_K\\subseteq \\mathcal{Z}^*$. The conversion from Eq. (9) to Eq. (10) is based on the FiD framework [1], where we can simultaneously use multiple plans for reasoning. The FiD framework is introduced in Equation 5 of our paper, which is also shown here.\n\nThe FiD framework assumes each $z\\in \\mathcal{Z}$ contributes independently. The probability $P_\\theta(a|q,\\mathcal{Z},\\mathcal{G})$ can be approximated as the product of each $P_\\theta(a|q,z,\\mathcal{G})$, which is formulated as\n$$\n\\begin{equation}\n    P\\_\\theta(a|q,\\mathcal{Z},\\mathcal{G}) = \\prod\\_{z\\in\\mathcal{Z}} P\\_\\theta(a|q,z,\\mathcal{G}). \\tag{12}\n\\end{equation}\n$$\n\nWe also want to clarify the marginalization in Equation 10 of our paper is reasonable based on the FiD framework shown in Eq. (12), which can be derived as\n\n$$\n\\begin{align*}\n&\\text{Because:} \\\\\\\\\n& P\\_\\theta(a|q,\\mathcal{Z},\\mathcal{G}) = \\prod\\_{z\\in\\mathcal{Z}} P\\_\\theta(a|q,z,\\mathcal{G}), \\\\\\\\\n&\\Rightarrow \\\\\\\\\n&\\log P\\_\\theta(a|q,\\mathcal{Z}^*\\_K, \\mathcal{G}) = \\log \\sum\\_{z\\in\\mathcal{Z}^*\\_K} P\\_\\theta(a|q,z,\\mathcal{G}), \\\\\\\\\n&\\Rightarrow \\\\\\\\\n&\\underset{\\theta}{\\operatorname{arg max}}\\log P\\_\\theta(a|q,\\mathcal{Z}^*\\_K, \\mathcal{G}) = \\underset{\\theta}{\\operatorname{arg max}} \\log \\sum\\_{z\\in\\mathcal{Z}^*\\_K} P\\_\\theta(a|q,z,\\mathcal{G}), \\\\\\\\\n&= \\underset{\\theta}{\\operatorname{arg max}} \\log \\sum\\_{z\\in\\mathcal{Z}^*\\_K}\\sum\\_{w\\_z\\in\\mathcal{W}\\_z}\\prod\\_{t=1}^{|a|}P\\_\\theta(t_i|t_{<i}, q, w_z), \\\\\\\\\n&\\text{Q.E.D.}\n\\end{align*}\n$$\n\n[1] Singh, D., Reddy, S., Hamilton, W., Dyer, C., & Yogatama, D. (2021). End-to-end training of multi-document reader and retriever for open-domain question answering. Advances in Neural Information Processing Systems, 34, 25968-25981."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088186488,
                "cdate": 1700088186488,
                "tmdate": 1700088659755,
                "mdate": 1700088659755,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HCd5CJgK1b",
                "forum": "ZGNWW7xZ6Q",
                "replyto": "WgPiW6rSTv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5213/Reviewer_1bmQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5213/Reviewer_1bmQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. It addresses most of my concerns. I still feel the integrability point is a bit weak. I raised my score to 6."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100243050,
                "cdate": 1700100243050,
                "tmdate": 1700100243050,
                "mdate": 1700100243050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8LRgPcxYV2",
            "forum": "ZGNWW7xZ6Q",
            "replyto": "ZGNWW7xZ6Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5213/Reviewer_qFDd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5213/Reviewer_qFDd"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the increasingly important problem of integrating Large Language Models (LLMs) into a more general support framework that can overcome their shortcomings and limitations using axillary techniques.  A compelling 'Reasoning on Graphs' (RoG) approach is introduced to enhance the reasoning capabilities of LLMs by leveraging the structural information of Knowledge Graphs (KGs).  The RoG concept emphasizes the importance of KGs' relational structures in the reasoning processes. The proposed method consists of a planning-retrieval-reasoning framework that generates relation paths grounded by KGs, which serve as reliable plans for subsequent reasoning tasks. These plans guide the retrieval of valid reasoning paths that facilitate faithful and interpretable reasoning by LLMs. The paper addresses two main issues prevalent in previous methods: the tendency of LLMs to produce hallucinated content and the underutilization of KGs' structural data. RoG is optimized through planning optimization, which distills KG structure into LLMs, and retrieval-reasoning optimization, which enables LLMs to produce accurate, KG-supported conclusions. The paper also situates RoG in the context of existing research, identifying its methodological advancements over semantic parsing and retrieval-augmented reasoning approaches."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality: This paper presents a solid concept for addressing weaknesses in pure LLM model-driven inference by coupling the LLM with a reasoning system.\n\nQuality:  The concept is sensible, compelling, well described, and thoroughly evaluated.  The breadth of comparison techniques is appreciated.\n\nClarity:  All aspects of the concept, relationship to existing literature, and experimental evaluation are well described.\n\nSignificance:  The application community needs actionable approaches to addressing shortcomings to LLMs, and this paper provides one such compelling example.  This result will likely be impactful to future research and implementations."
                },
                "weaknesses": {
                    "value": "Clarity:  The evaluation against ChatGPT appears to use 3.5-turbo.  Please clarify, including the dates of the evaluations -- the implementation of ChatGPT changes over time."
                },
                "questions": {
                    "value": "1. Can you clarify which version of ChatGPT was used?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Reviewer_qFDd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699134504923,
            "cdate": 1699134504923,
            "tmdate": 1699636518842,
            "mdate": 1699636518842,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3BysTuuz1A",
                "forum": "ZGNWW7xZ6Q",
                "replyto": "8LRgPcxYV2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qFDd"
                    },
                    "comment": {
                        "value": "## Question 1\n> Can you clarify which version of ChatGPT was used?\n\n### Response\nWe greatly appreciate the reviewer\u2019s recognition of our work. We want to clarify that we use ChatGPT-3.5-turbo in our experiments. Experiments are conducted with models released between July. to Sept., 2023. We have revised the paper (Appendix A.5) to clarify this point."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087619871,
                "cdate": 1700087619871,
                "tmdate": 1700087619871,
                "mdate": 1700087619871,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9c3dHVfWyA",
                "forum": "ZGNWW7xZ6Q",
                "replyto": "3BysTuuz1A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5213/Reviewer_qFDd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5213/Reviewer_qFDd"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's update on the model information.  I am convinced of the originality and quality of the paper, and appreciate the clarity of presentation.  A rating of 8 is well deserved."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324488701,
                "cdate": 1700324488701,
                "tmdate": 1700324488701,
                "mdate": 1700324488701,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "940sfv6miJ",
            "forum": "ZGNWW7xZ6Q",
            "replyto": "ZGNWW7xZ6Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5213/Reviewer_ZzpQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5213/Reviewer_ZzpQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new approach for questions answering over knowledge graphs. The idea is to use LLMs for QA while exploiting the information in the KG and reasoning over that to alleviate the issues of lack of knowledge and hallucination of LLMs. The main idea is to tune the LLM to generate the relation path needed for finding the final answer, and then instantiate the paths to the answer by searching in the KG. Then feed the instantiated paths that use the actual entities back to the LLM to find the answers that are more faithful to the path of reasoning and less pruned to the hallucination. The experiments are done over two KGQA benchmarks with up to 4 hops of reasoning.  Multiple LLMs (GPT, T5, LLAma, Alpaca) are used and tested. The results show significant improvements compared to a variety of baselines and existing SOTA."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The approach is novel and interesting. \nThe experiments show strong results and improvements over SOTA. \nThe paper is well written though the organization of the approach description can be improved."
                },
                "weaknesses": {
                    "value": "--The approach section was hard to read.\n    --- More specifically, the order of explanation was a bit hard to follow. Before explaining the optimization, I think explaining the flow of information step-by-step will be helpful when you point to Figure 3 in the beginning. In the optimization part, explaining what kind of ground-truth supervision is used was not very explicit. Using the retrieved paths from the KG as a source of supervision could be made clear earlier in the approach.  \n\n--The training approach seems to be very costly.  It needs training and instruction-tuning for the LLMs to generate the relation and KG-specific paths. If we train with a specific KG the results will improve in answering questions from that specific KG --which of course is the scope of this work. However, I am not sure if this helps LLM's QA capability in general and the issues set in front including hallucination and lack of knowledge in general."
                },
                "questions": {
                    "value": "--If I understood correctly when you refer to retrieval and reasoning/planning modules of ROG, those are the outcome of instruction-tuning of a specific large language model. When you discussed the ROG model, it was not clear to me what was the base LLM; Which language model was used and tuned for those results of ROG?  when you combine ROG with other language models in Table 4, which one has been used again in the planning module?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5213/Reviewer_ZzpQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5213/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699152057598,
            "cdate": 1699152057598,
            "tmdate": 1699636518742,
            "mdate": 1699636518742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BvoQ9XzTCR",
                "forum": "ZGNWW7xZ6Q",
                "replyto": "940sfv6miJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5213/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5213/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZzpQ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive comments. We have revised the manuscript according to your comments and provided a detailed response to each comment below.  We hope our answers can address your questions.\n\n## Weakness 1\n>  The approach section was hard to read. --- More specifically, the order of explanation was a bit hard to follow. Before explaining the optimization, I think explaining the flow of information step-by-step will be helpful when you point to Figure 3 in the beginning. In the optimization part, explaining what kind of ground-truth supervision is used was not very explicit. Using the retrieved paths from the KG as a source of supervision could be made clear earlier in the approach.\n\n### Response\nThank you for your comment. We have revised the approach section to improve its readability. Regarding the framework, we have introduced the components of our method at the beginning and described them in the caption of Figure 2, where we provide a step-by-step description of our approach. \n\nAdditionally, regarding the issue of what ground-truth supervision is, we would like to clarify that we use the shortest paths that connect the questions and answer entities in the KG as the supervision signals. We have revised Equation 4 in our paper to clarify this point. The detailed derivation can be found in Appendix A.1 and our responses to Reviewer 1bmQ.\n\n## Weakness 2\n> The training approach seems to be very costly. It needs training and instruction-tuning for the LLMs to generate the relation and KG-specific paths. If we train with a specific KG the results will improve in answering questions from that specific KG --which of course is the scope of this work. However, I am not sure if this helps LLM's QA capability in general and the issues set in front including hallucination and lack of knowledge in general.\n\n### Response\nAdmittedly, the training process of our method could be costly. However, we want to emphasize that after training, our method enables the generalizability of transferring to new KGs and QA datasets with only a few steps of finetuning. \n\nIn Appendix A.7.1, we evaluate the transferability of RoG to other KGs. We select the MetaQA-3hop dataset, which is based on Wiki-Movies KGs. The experiment results show that RoG trained on Freebase KGs can be effectively transferred to Wiki-Movies KGs with only 1000 samples. The performance of transferring is better than training from scratch. This indicates that RoG empowers the reasoning on graph ability via training, which can be transferred to new KGs to improve the model's performance on this new KG. We also present the training and transferring time below (also in Table 12). As can be seen, the transferring time is much less than the training time.\n\nPerformance on MetaQA-3hop (F1)\n\n|  Method   | Only Train on Wiki-Movies | Transferring from Freebase |\n| :---: | :-----------------------: | :------------------------: |\n|  RoG  |           41.32           |           50.68            |\n\nTraining time comparison.\n| Time | Training on Freebase | Transferring to Wiki-Movies |\n| :--: | :------------------: | :-------------------------: |\n| RoG  |        38 hours        |            2 hours            |\n\n## Question 1: \n> If I understood correctly when you refer to retrieval and reasoning/planning modules of ROG, those are the outcome of instruction-tuning of a specific large language model. When you discussed the ROG model, it was not clear to me what was the base LLM; Which language model was used and tuned for those results of ROG? when you combine ROG with other language models in Table 4, which one has been used again in the planning module?\n\n### Response\nWe use LLaMA2-Chat-7B as the LLMs in our experiments. When we combine RoG with other language models, we use the planning module to generate plans (relation paths). The plans are executed on KGs to retrieve the reasoning paths. The retrieved paths are fed into different LLMs during inference using the reasoning prompts template shown in Appendix A.10. This enables them to be combined with RoG without retraining. We have revised the paper by adding more details to the implementation settings in Appendix A.6."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700087494201,
                "cdate": 1700087494201,
                "tmdate": 1700087494201,
                "mdate": 1700087494201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W4PvxwmdUa",
                "forum": "ZGNWW7xZ6Q",
                "replyto": "940sfv6miJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5213/Reviewer_ZzpQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5213/Reviewer_ZzpQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response and clarifying the points. I think apart from writing clarity this is a very good paper. I agree the cost of training is worth the improved generalizability."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5213/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675039079,
                "cdate": 1700675039079,
                "tmdate": 1700675039079,
                "mdate": 1700675039079,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]