[
    {
        "title": "Ditto: Quantization-Aware Secure Inference of Transformers upon MPC"
    },
    {
        "review": {
            "id": "R0AAw0b8Lv",
            "forum": "n39ilTxSDY",
            "replyto": "n39ilTxSDY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1687/Reviewer_iucu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1687/Reviewer_iucu"
            ],
            "content": {
                "summary": {
                    "value": "Author introduce Ditto -- a framework designed to enhance the efficiency of secure inference in Transformer models using multi-party computation (MPC). It incorporates MPC-friendly quantization and a quantization-aware distillation procedure to both reduce computational overhead and maintain model utility. Empirical tests on Bert and GPT2 models show that Ditto significantly outperforms existing solutions, being 3.14 to 4.40 times faster than MPCFormer and 1.44 to 2.35 times faster than PUMA, with negligible loss in utility."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors present a solution that addresses multiple bottlenecks in secure multi-party computation (MPC) for Transformer models. For example, challenges like handling non-linear functions and dynamic quantization in an MPC context. They also offer a solution such as modified dyadic quantization and static dyadic quantization for these issues. \n\n* The paper highlights and addresses the often-overlooked disconnect between the expertise in machine learning and multi-party computation. For example, it effectively integrates best practices from MPC-friendly quantization and type-conversion primitives, thereby enhancing end-to-end secure inference efficiency.\n\n* The authors show empirical evidence that their contributions are valid. They compared Ditto against existing state-of-the-art frameworks like MPCFormer and PUMA, the authors make a compelling case for the performance advantages of their approach."
                },
                "weaknesses": {
                    "value": "* The paper acknowledges that both Ditto and MPCFormer exhibit noticeable utility drops in Bert tasks when employing ReLU approximation for Softmax. They offer Quad approximation for GeLU to maintain a balance between utility and efficiency, but this limitation may constrain the applicability of the framework for tasks where such approximations are not tolerable.\n\n* The paper in general is hard to read and require additional proof-reading. I would recommend making the paper to be easier to read by highlighting important concepts, introducing figures that support main results, and describing contributions and future work."
                },
                "questions": {
                    "value": "What are the primary limitations of using more aggressive quantization methods, as mentioned in the future work section, in the context of secure inference? Would it significantly affect model utility, or are there other challenges like security vulnerabilities that need to be addressed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698428461163,
            "cdate": 1698428461163,
            "tmdate": 1699636097097,
            "mdate": 1699636097097,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SwxsDBbCTo",
                "forum": "n39ilTxSDY",
                "replyto": "R0AAw0b8Lv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer iucu for the positive feedback on \"The paper highlights and addresses the often-overlooked disconnect between the expertise in machine learning and multi-party computation.\". We then answer the specific question below.\n\n> They offer Quad approximation for GeLU to maintain a balance between utility and efficiency, but this limitation may constrain the applicability of the framework for tasks where such approximations are not tolerable.\n\nWe first note that **the approximation of GeLU is not coupled to Ditto.** In Section 5.2, we present the efficiency of Ditto without approximations, which leads to about $2\\times$ communication reduction than prior works. For scenarios where this approximation is not applicable, we can resort to using the standard GeLU or investigating more accurate approximations.\n\nNevertheless, our comprehensive evaluations, which include models like Bert and GPT, reveal that **the approximated GeLU offers a more favorable tradeoff between utility and efficiency.** As a result, we currently believe that it is feasible to incorporate this approximation into Ditto.\n\n> The paper in general is hard to read and require additional proof-reading.\n\nWe thank the detailed review of the paper writing. We will highlight the important concepts in quantization-aware secure inference in the final version.\n\nTo summarize, we propose Ditto to bridge the gap between plaintext quantization and MPC-based secure inference. The gap mainly results from two aspects: \n\n1. (Section 4.3) **Type conversions are difficult in MPC**: MPC encodes **floating-point plaintext** into **fixed-point ciphertexts** (over rings). Typically, the ciphertexts are encoded over a uniform ring of fixed size (like $2^\\ell$). In this case, although plaintexts can be quantized into low-bit representation (e.g., from fp16 to int8), the ciphertexts remain the same size (i.e., $\\ell$-bit fixed-point numbers). **To incorporate low-bit quantization into MPC, we need to allow ciphertext conversion among different rings. This is why we propose two novel type conversion MPC primitives.** Compared to SOTA type conversion primitives, our protocol incurs much lower communication overhead. Table below lists the theoretical communication complexity against SOTA works (cast from $\\mathbb{Z}\\_{2^\\ell}$ to $\\mathbb{Z}\\_{2^{\\ell'}}$. $\\lambda$ is the security parameter, 128 in SIRNN[2]).\n\n|||Round|Size (bits)|\n|:--:|:--:|:--:|:--:|\n|Downcast|SecureML[1]|0|0|\n||SIRNN[2]|0|0|\n||Ditto|0|0|\n|Upcast|SIRNN[2]|$\\log\\ell+2$|$\\lambda(\\ell+1)+13\\ell+\\ell'$|\n||Baccarini et al.[3]|$\\log\\ell+4$|$2(\\ell+\\ell')+\\log\\ell+2$|\n||Ditto|3|$2\\ell+\\ell'$|\n\n2. (Section 4.2) **Dynamic quantization is expensive in MPC**: Standard plaintext quantization requires additional operations like `clamp` and `min/max` during runtime to achieve low-bit quantization (e.g., int8). These operations involve non-linear comparisons that are cheap in plaintext yet quite expensive in MPC. **As a result, we propose an MPC-friendly quantization scheme to improve efficiency.** Besides, we implement the quantization-aware distillation to retain the model utility.\n\nDitto effectively bridges the aforementioned gaps. **By reducing the size of ciphertexts, we are able to mitigate communication overhead, resulting in improved efficiency against SOTA works.**\n\n> What are the primary limitations of using more aggressive quantization methods, as mentioned in the future work section, in the context of secure inference? Would it significantly affect model utility, or are there other challenges like security vulnerabilities that need to be addressed?\n\nThe challenge of employing more aggressive quantization is model utility. In MPC, fixed-point representation, where lower $f$ bits represent the fractional part, is typically used to encode floating-point numbers. **A multiplication doubles the precision to $2f$. If we opt for a more aggressive quantization such as INT16, it would restrict $f$ to be smaller than 8 (since we require $2f < 16$), resulting in a precision lower than $1/2^8$.** As a result, there might be a significant utility drop.\nIn the future, we plan to incorporate quantized models into Ditto. This involves taking INT8 weights and INT8 activations as inputs instead of floating-point numbers, allowing us to treat the variables as integers and avoid fixed-point representation. However, achieving this would require additional efforts in terms of re-quantization, which we consider as future work. We believe that incorporating quantized models is an intriguing direction to explore in the realm of MPC-based secure inference.\n\n[1]: Mohassel and Zhang. SecureML: A System for Scalable Privacy-Preserving Machine Learning. IEEE Symposium on Security and Privacy 2017\n\n[2]: Rathee et al. SiRnn: A Math Library for Secure RNN Inference. SP 2021\n\n[3]: Baccarini  et al. Multi-Party Replicated Secret Sharing over a Ring with Applications to Privacy-Preserving Machine Learning. Proc. PETS. 2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699779032406,
                "cdate": 1699779032406,
                "tmdate": 1699779032406,
                "mdate": 1699779032406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QK95KzJqrE",
            "forum": "n39ilTxSDY",
            "replyto": "n39ilTxSDY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MPC primitives to support quantization-aware private inference. Moreover, the authors propose a MPC-friendly quantization-aware distillation to retrain the model utility."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper targets an important problem in private inference.\n\n2. The proposed type conversion protocols are creative solutions to a key challenge in quantization-aware secure inference.\n\n3. Extensive evaluations analyzing efficiency, utility, scalability, and communication costs and latency on factors like sequence length and batch size."
                },
                "weaknesses": {
                    "value": "1. Lack of comparison to the latest related work."
                },
                "questions": {
                    "value": "How would the proposed DITTO be compared with Iron [1]?\n\n[1] Hao, Meng, et al. \"Iron: Private inference on transformers.\" Advances in Neural Information Processing Systems 35 (2022): 15718-15731."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1687/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1687/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821348217,
            "cdate": 1698821348217,
            "tmdate": 1699636096995,
            "mdate": 1699636096995,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d9kRn4W2lw",
                "forum": "n39ilTxSDY",
                "replyto": "QK95KzJqrE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer MwvF for the positive feedback on \"creative solutions to a key challenge in quantization-aware secure inference.\". It is encouraging for us to further narrow the gap between the two entirely different worlds of plaintext quantization and MPC-based secure inference.\n\nWe then answer the specific question below.\n\n> Lack of comparison to the latest related work.\n\nThank you for mentioning Iron. We first present a short comparison against Iron. In general, Ditto is orders of magnitude faster than Iron on Bert (base and large) models with input sequence length of 128.\n\n|       |     Bert-base     |                     |    Bert-large     |                      |\n| :---: | :---------------: | :-----------------: | :---------------: | :------------------: |\n| Iron  | $\\approx$ 30 min  |                     | $\\approx$ 100 min |                      |\n| Ditto | $\\approx$ 0.5 min | $\\uparrow 60\\times$ |  $\\approx$ 1 min  | $\\uparrow 100\\times$ |\n\nThe reason we do not compare Ditto with Iron is because the settings of these two works are not the same. As stated in Section 2, Iron is based on 2PC (2-party computation) setting, while Ditto is 3PC, thus leading to an unfair comparison. \n\nWe note that we indeed comprehensively compare with the SOTA works. MPCFormer (ICLR 2023) and PUMA (2023.09) are two latest 3PC works of the same setting as Ditto."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699778985797,
                "cdate": 1699778985797,
                "tmdate": 1699778985797,
                "mdate": 1699778985797,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jtxmuMMIcG",
                "forum": "n39ilTxSDY",
                "replyto": "d9kRn4W2lw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. After the second time review, I have two more questions:\n\n1. MPCFormer uses Quad for GeLU and 2Quad for the softmax. Why the authors compare MPCFormer with Quad+2ReLU rather than Quad+2Quad?\n\n2. Are there experimental results on the effects of quantization level on both computational costs and network performance?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689452688,
                "cdate": 1700689452688,
                "tmdate": 1700689452688,
                "mdate": 1700689452688,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kwf1VHi9FH",
                "forum": "n39ilTxSDY",
                "replyto": "xGoRczVZx8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1687/Reviewer_MwvF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. It would be good to show Comm. Time as well as the utility for a comprehensive comparison. I will keep my score as it is."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722856485,
                "cdate": 1700722856485,
                "tmdate": 1700722856485,
                "mdate": 1700722856485,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "14u4QJcuuF",
            "forum": "n39ilTxSDY",
            "replyto": "n39ilTxSDY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1687/Reviewer_sjar"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1687/Reviewer_sjar"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a framework for quantization-aware secure Transformer inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ MPC-friendly Quantization-Aware Distillation.\n+ MPC primitives for scale down and scale up.\n+ Comparison with SOTA."
                },
                "weaknesses": {
                    "value": "- Distillation is widely used in MPC-based secure inference works.\n- It seems limited contributions of MPC protocols."
                },
                "questions": {
                    "value": "1. Does the  Downcast protocol have a probabilistic error? What is the difference compared with the truncation of SecureML?\n2. In Upcast, what distribution is $r$ sampled from? How to ensure the input is positive?\n3. Could you provide the theoretical or experimental advantages of the proposed Downcast and Upcast protocols compared with SOTA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1687/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833120606,
            "cdate": 1698833120606,
            "tmdate": 1699636096908,
            "mdate": 1699636096908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8cUqk2RmYH",
                "forum": "n39ilTxSDY",
                "replyto": "14u4QJcuuF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1687/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1687/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer sjar for the questions on MPC-side contributions. We are sorry that we defer some detailed descriptions regarding MPC protocols to Appendix due to page limitation.\nWe hereby answer the specific questions below.\n\n> Distillation is widely used in MPC-based secure inference works.\n\nWe clarify that our main contribution is proposing MPC-friendly quantization and quantization-aware distillation, which effectively bridges the gap between plaintext quantization and MPC-based inference.\nDifferent from prior works that leverage distillation, we further incorporate MPC-friendly quantization into distillation, which is not trivial. **This involves converting the computations in the student model into fixed-point arithmetic (illustrated in Figure 5, Appendix A.7) and utilizing them as inputs to the distillation process.** With quantization-aware distillation, we can enhance efficiency while maintaining a negligible utility drop.\n\n> It seems limited contributions of MPC protocols.\n\nDue to page limitation, we are unable to provide a detailed description in Section 4.3.1. The **formulated protocol construction**, **correctness analysis** and **security proofs** are presented in Appendix A.2~A.4.\nBesides, we emphasize that the Downcast and UpCast are different from prior works and exhibit better performance, which is the key to quantization-aware secure inference. We believe that the following explanations will help demonstrate the contribution of MPC protocols.\n\n> Does the Downcast protocol have a probabilistic error? What is the difference compared with the truncation of SecureML?\n\nTo summarize, there are distinct differences between Downcast and the truncation method in SecureML. **Downcast introduces a probabilistic error solely in the least significant bit (LSB), whereas SecureML introduces probabilistic errors in both the LSB and MSB.** The impact of LSB error is negligible, while the MSB error may cause significant deviations.\n\nAs shown in Equation 2, Appendix A.3,\n\n$x'=x/2^t$ \n\n$=((x_0+x_1+x_2)\\mod 2^\\ell)/2^t$\n\n$=x_0/2^t+x_1/2^t+x_2/2^t-w\\cdot 2^{\\ell-t}+w'$\n\n$=(x_0/2^t+x_1/2^t+x_2/2^t-w\\cdot 2^{\\ell-t}+w')\\mod 2^{\\ell'}$\n\n$=(x_0/2^t\\mod 2^{\\ell'})+(x_1/2^t\\mod 2^{\\ell'})+(x_2/2^t\\mod 2^{\\ell'})-(w\\cdot 2^{\\ell-t}\\mod 2^{\\ell'})+w'\\mod 2^{\\ell'}$\n\nthere are two potential wraps, i.e., $w$ and $w'$. $w$ leads to the MSB error, while $w'$ leads to the LSB error. In Ditto, since we have $\\ell\u2212t=64\u2212(18\u22128)=54$ and $\\ell'=32$, we can get $w \\cdot 2^{\\ell\u2212t}\\mod 2^{\\ell'}=0$. That is, we can implicitly erase the MSB error in Ditto. While in SecureML, the potential wrap happens with probability and may lead to significant errors.\n\n> Could you provide the theoretical or experimental advantages of the proposed Downcast and Upcast protocols compared with SOTA?\n\nConsider casting from $\\mathbb{Z}\\_{2^\\ell}$ to $\\mathbb{Z}\\_{2^{\\ell'}}$. $\\lambda$ denotes the computational security parameter, typically 128 in SIRNN[2].\n\nThe table below demonstrates that Ditto surpasses previous works in terms of the upcast protocol, excelling in both communication round and size. These two factors are crucial in measuring the efficiency of protocols. **Notably, Upcast in Ditto is constant-round (3 rounds), which is much better than SOTA works ($\\mathcal{O}(\\log\\ell)$).**\n\nIn terms of downcast, **Ditto incurs no communication like prior works, while avoiding MSB error**, which was a drawback in the SecureML approach.\n\n|||Round|Size (bits)|\n|:------:|:------------------:|:-------------:|:---------------------------------:|\n|Downcast|SecureML[1]|0|0|\n||SIRNN[2]|0|0|\n||Ditto|0|0|\n|Upcast|SIRNN[2]| $\\log \\ell + 2$ | $\\lambda (\\ell+1) + 13\\ell + \\ell'$ |\n||Baccarini et al.[3]| $\\log \\ell + 4$ |  $2(\\ell + \\ell') + \\log \\ell + 2$  |\n||Ditto|3|$2\\ell + \\ell'$|\n\n> In Upcast, what distribution is r sampled from? \n\nThe randomness $r$ is a random $\\ell$-bit integer, uniformly sampled from $\\mathbb{Z}_{2^{\\ell}}$ to ensure perfect security during the mask-and-open process of $x$. That is, $r \\in [-2^{\\ell-1}, 2^{\\ell-1}-1]$.\n\n> How to ensure the input is positive?\n\nWe have a trick that restricts the range of $x\\in[-2^{\\ell-2},2^{\\ell-2}-1]$. As a result, $x+2^{\\ell-2}\\in[0,2^{\\ell-1}-1]$, thus ensuring that the input to Upcast is positive. \nAfter the conversion, the bias term $2^{\\ell-2}$ can be conveniently subtracted to eliminate its influence.\nThis approach allows us to achieve performance improvements at the sacrifice of one bit without compromising the overall security of the system.\nThe correctness analysis is provided in Appendix A.3.\n\n[1]: Mohassel and Zhang. SecureML: A System for Scalable Privacy-Preserving Machine Learning. IEEE Symposium on Security and Privacy 2017\n\n[2]: Rathee et al. SiRnn: A Math Library for Secure RNN Inference. SP 2021\n\n[3]: Baccarini et al. Multi-Party Replicated Secret Sharing over a Ring with Applications to Privacy-Preserving Machine Learning. Proc. Priv. Enhancing Technol. 2023"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1687/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699778946254,
                "cdate": 1699778946254,
                "tmdate": 1699778946254,
                "mdate": 1699778946254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]