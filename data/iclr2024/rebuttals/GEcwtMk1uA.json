[
    {
        "title": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox"
    },
    {
        "review": {
            "id": "RKg4kyPovr",
            "forum": "GEcwtMk1uA",
            "replyto": "GEcwtMk1uA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4592/Reviewer_nwq5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4592/Reviewer_nwq5"
            ],
            "content": {
                "summary": {
                    "value": "The paper is ambitious and tackles a complex and timely issue\u2014evaluating the safety and effectiveness of Language Models (LMs) in various tool use scenarios. The approach of using LMs as both tool emulators and safety evaluators is interesting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-organized and written, with extensive supporting materials in Appendix. The main strengths of this paper include:\n\u2022 Automatic Safety Evaluator: The development of an automatic safety evaluator using LMs is a significant contribution of the paper.\n\u2022 Comprehensive Test Cases: The inclusion of a variety of test cases and toolkits, some of which have not been evaluated in previous benchmarks, adds value to the paper."
                },
                "weaknesses": {
                    "value": "The methodology of using LMs to evaluate other LMs raises concerns such as reliability."
                },
                "questions": {
                    "value": "Methodology:\n- As the authors point out, \u201cLM agents may fail in a variety of unpredictable ways\u201d (Sec 3.2), \u201cLM-based emulators and evaluators might occasionally not meet the requirements, leading to critical issues or incorrect evaluations\u201d(Sec 6, Discussion-Limitations). \n- Also, it is unclear how biases or limitations in the evaluating LM might affect the evaluation of the LM being tested, or if ToolEmu, based on LMs, will have/incur potential risks itself.\n\nOther than this, I have two minor comments:\n- Introduction. \u201cOut of these failures, we inspected the 7 severe failures of ChatGPT-3.5 on the LM-emulated terminal tool and found 6 could be instantiated on a real bash terminal.\u201d There are no clear criteria for what makes a failure \u201csevere.\u201d Also, the fact that 6 out of 7 severe failures could be instantiated on a real bash terminal is interesting but lacks statistical context. Are these failures representative?\n- Introduction. It is not clear how the authors measure the time it took to instantiate these failures. Was it a straightforward process, or were there complexities that could affect the time? Are the time metrics (8 hours vs. under 15 mins) average times or one-time measurements?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4592/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698455869125,
            "cdate": 1698455869125,
            "tmdate": 1699636437359,
            "mdate": 1699636437359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "llmz3FeRLL",
                "forum": "GEcwtMk1uA",
                "replyto": "RKg4kyPovr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4592/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4592/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for dedicating your time to review and for providing valuable feedback. We are glad that you find our work \u201cambitious and tackles a complex and timely issue\u201d, our approach \u201cinteresting\u201d, and our paper \u201cwell-organized and written\u201d. If these address your concerns, we would appreciate it if you could reconsider and possibly increase your score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4592/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021574612,
                "cdate": 1700021574612,
                "tmdate": 1700021574612,
                "mdate": 1700021574612,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1nHn43GPnQ",
                "forum": "GEcwtMk1uA",
                "replyto": "iRkfeGqSak",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4592/Reviewer_nwq5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4592/Reviewer_nwq5"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for the response and clarification."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4592/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704946696,
                "cdate": 1700704946696,
                "tmdate": 1700704946696,
                "mdate": 1700704946696,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qfmthuoggc",
            "forum": "GEcwtMk1uA",
            "replyto": "GEcwtMk1uA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4592/Reviewer_5zXC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4592/Reviewer_5zXC"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies a challenge of testing LLM integrations with tools and plugins: how can we test LLM behavior and risks in the context of an open ended set of plugin capabilities and user scenarios?   To address this problem, the paper uses a tool emulator, built using an LLM itself, to emulate potential behaviors of arbitrary plugins.  The paper benchmarks LLM+tool risks in the context of a variety of scenarios, and validates the emulator's identified failures with human annotators."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is an important problem: As LLM agents are integrated with a wide variety of tools and plugins, conventional software testing methodologies fail to scale to evaluate reliability.  LLM agents with plugins are becoming more and more widely deployed, and finding ways to evaluate LLM performance across an open-ended world of potential plugins is critical.\n\nThe design of ToolEmu's curated toolkit is well motivated, with a broad range of risk scenarios.  The evaluation of ToolEmu's identified failures with human annotators is a strength."
                },
                "weaknesses": {
                    "value": "It's not clear that the range of plugin behaviors that can be emulated with ToolEmu matches the range of real-world software plugins being developed.\n\nNot all identified failures are true failures, either because of invalid emulator behavior or invalid classification."
                },
                "questions": {
                    "value": "Can ToolEmu scale to handle scenarios involving multiple plugins? Does this introduce new risk scenarios?\n\nGiven a range of real-world inspired plugins and user scenarios, could ToolEmu be used to identify the relative risk of real-world scenarios?\n\u00a0\nHow might ToolEmu's results provide insights for debugging and fixing problems?\n\nHow sensitive is ToolEmu's findings to very minor variations in prompting and/or minor variations in emulated plugin responses?   (i.e., individual word choices, punctuation, etc)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4592/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699003478321,
            "cdate": 1699003478321,
            "tmdate": 1699636437261,
            "mdate": 1699636437261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6DctzTolPX",
                "forum": "GEcwtMk1uA",
                "replyto": "qfmthuoggc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4592/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4592/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for dedicating your time to review and for providing valuable feedback. We appreciate that you found our work focuses on an \u201cimportant problem\u201d, the design of ToolEmu is \u201cwell-motivated\u201d, and our evaluation is a \u201cstrength\u201d.  We would like to address your remaining concerns with the following responses."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4592/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021439106,
                "cdate": 1700021439106,
                "tmdate": 1700021439106,
                "mdate": 1700021439106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MCqJZ2Nl3J",
                "forum": "GEcwtMk1uA",
                "replyto": "6DctzTolPX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4592/Reviewer_5zXC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4592/Reviewer_5zXC"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses to my questions."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4592/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560638750,
                "cdate": 1700560638750,
                "tmdate": 1700560638750,
                "mdate": 1700560638750,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MXNph5uskQ",
            "forum": "GEcwtMk1uA",
            "replyto": "GEcwtMk1uA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4592/Reviewer_3bW4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4592/Reviewer_3bW4"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces ToolEmu, a framework that utilizes a language model to mimic tool execution, allowing for scalable testing of language model agents across a range of tools and scenarios. This includes an LM-based automatic safety evaluator that quantifies associated risks and investigates agent failures. Extensive experiments showcase ToolEmu's effectiveness and efficiency. In terms of effectiveness, it demonstrates, through human evaluation, that 68.8% of the failures identified with ToolEmu would indeed be considered real-world agent failures. Regarding efficiency, it significantly reduces testing time, generating failures in less than 15 minutes compared to the 8 hours required by existing sandboxes for the bash terminal. ToolEmu assesses the safety and usefulness of various LM agents, offering insights into their performance and the impact of prompt tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tCompared to some traditional methods, Agent-based ToolEmu reduces the labor needed to construct a testing environment for simulation by utilizing the general intelligence of LLM and various pretended tool functions. \n2.\tToolEmu dramatically reduces testing time, generating failures in less than 15 minutes, a significant improvement compared to the 8 hours typically needed by existing bash terminal sandboxes. This notably enhances testing efficiency.\n3.\tToolEmu effectively captures potential failures, as demonstrated through human evaluation, where 68.8% of identified failures were validated as real-world agent failures."
                },
                "weaknesses": {
                    "value": "1.\tThe paper could benefit from reorganization to enhance clarity. While it's understandable that due to space constraints, much information had to be placed in the appendices, the frequent transitions between the main text and the appendices could be confusing. I would suggest the authors consider optimizing this structure.\n2.\tIn Table 3, it may be overly simplistic to validate effectiveness by comparing the Cohen\u2019s \u03ba between human annotators and the Cohen\u2019s \u03ba between human annotators and automatic evaluators. Furthermore, if the value of Cohen\u2019s \u03ba between human annotators is only less than 0.5, it raises questions about whether the annotated results of human annotators can be considered as the ground truth.\n3.\tThe contribution of this work could be further improved by providing more interpretability."
                },
                "questions": {
                    "value": "It's sound work, but I have a couple of queries I'd like to discuss with the author. I'm curious about the reasoning behind the choice of a relatively small sample size of 144 test cases. I would like to understand what level of coverage the research aims to achieve with this number. Additionally, I'm interested in the rationale for using only 100 test cases from the curated dataset for validation. I have concerns about whether such a small sample size is sufficient to validate the experiments effectively."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4592/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699355238119,
            "cdate": 1699355238119,
            "tmdate": 1699636437142,
            "mdate": 1699636437142,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sWZpaAKrKF",
                "forum": "GEcwtMk1uA",
                "replyto": "MXNph5uskQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4592/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4592/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and suggestions. We appreciate that you acknowledged the soundness of our work, as well as the advantages of our ToolEmu framework in that it \u201cdramatically reduces testing time\u201d and \u201ceffectively captures potential failures\u201d. We would like to address your remaining concerns with the following responses."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4592/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021093000,
                "cdate": 1700021093000,
                "tmdate": 1700021093000,
                "mdate": 1700021093000,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LIMWGk6QcN",
                "forum": "GEcwtMk1uA",
                "replyto": "MXNph5uskQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4592/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4592/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We invested our resource budget in the quality of our curated test cases and annotation results over the quantity"
                    },
                    "comment": {
                        "value": "> *\u201cI'm curious about the reasoning behind the choice of a relatively small sample size of 144 test cases. I would like to understand what level of coverage the research aims to achieve with this number.\u201d*\n\nOur curated test set serves as a proof of concept to demonstrate how ToolEmu can be used to build a quantitative evaluation benchmark that flexibly tests LM agents across different tools & scenarios. Therefore, we didn\u2019t optimize for the total number of test cases and instead chose to spend our resources (e.g., human efforts) on vetting each test case and validating our framework to make a more reliable benchmark. The test set, though seemingly small, still achieves a reasonable coverage over 36 toolkits and 9 risk types, due to our deliberate avoidance of redundancy in curating test cases; we tried to avoid replicating similar failures across different scenarios or tools.\n\nWith further investment, we may be able to automate the entire data curation pipeline with LMs, which would enable the generation of orders of magnitude more test cases. However, we think this is better as future work, given the difficulty of fitting all the content in the current paper.\n\n\n\n> *\u201c Additionally, I'm interested in the rationale for using only 100 test cases from the curated dataset for validation. I have concerns about whether such a small sample size is sufficient to validate the experiments effectively.\u201d*\n\nThe reason is similar: given the resource budget (i.e., the total time each annotator was able to contribute) and the fact that our annotation task is non-trivial, we chose to focus on the annotation quality. During the time budget of about 25 hours, the annotators were able to finish the annotation on 100 test cases (200 paired trajectories). \n\nConsidering the relatively small sample size, we also conducted an internal annotation by the authors to confirm these validation results (Appx. D.1)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4592/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700021313981,
                "cdate": 1700021313981,
                "tmdate": 1700022030210,
                "mdate": 1700022030210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]