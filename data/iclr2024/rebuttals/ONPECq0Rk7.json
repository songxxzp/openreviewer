[
    {
        "title": "Headless Language Models: Learning without Predicting with Contrastive Weight Tying"
    },
    {
        "review": {
            "id": "122jHxwB4W",
            "forum": "ONPECq0Rk7",
            "replyto": "ONPECq0Rk7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2843/Reviewer_QSPQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2843/Reviewer_QSPQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a simple method for pre-training LMs with contrastive loss. Instead of evaluating logits for all vocabulary, only embeddings from the batch are used. Such an approach allowed us to pre-train LMs more effectively by reducing memory constraints. Surprisingly, this approach also performed well for autoregressive language modeling. For both autoregressive and masked language modeling, the proposed method showed improved results on downstream tasks, as well as improved training efficiency with respect to training hours."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is straightforward yet shows promising results. \nPerformance on down-stream tasks is marginally improved with respect to vanilla models\nPre-training is more efficient with respect to training time and the total number of tokens\nThe paper is easy to follow, and the motivation is clear."
                },
                "weaknesses": {
                    "value": "I would like to see more insights from the paper. It shows that HLMs provide better efficiency and down-stream performance, while such results are slightly contr-intuitive. More analysis could make the paper better. I.e., it would be beneficial to answer the question, \"what makes HLMs special, and why it outperforms vanilla LMs?\"."
                },
                "questions": {
                    "value": "Please, refer to the Weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755000459,
            "cdate": 1698755000459,
            "tmdate": 1699636228098,
            "mdate": 1699636228098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K1tZ6XHLRX",
                "forum": "ONPECq0Rk7",
                "replyto": "122jHxwB4W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We warmly thank the reviewer for its enthusiasm with regard to our work. We addressed the main question of the reviewer in **Appendix A** and in the General rebuttal."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699919664847,
                "cdate": 1699919664847,
                "tmdate": 1699919664847,
                "mdate": 1699919664847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6O61GTdSgH",
            "forum": "ONPECq0Rk7",
            "replyto": "ONPECq0Rk7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2843/Reviewer_CHGU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2843/Reviewer_CHGU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to replace the cross-entropy minimization by a new pre-training objective in language models: Contrastive Weight Tying (CWT). It consists in contrastively predicting the representation of the missing word. Instead of projecting the representation output by the model into the vocabulary, and using a softmax, the goal is to predict directly this static embedding of the reference word, which the model is trained for by (1) using a contrastive objective with the rest of the batch as negative examples (2) using input static embeddings as reference output embeddings, which is traditionally referred as *weight tying*. After a presentation of previous work, the paper introduces the method, consisting in weight tying, the contrastive objective with no projection, and how to adapt it for text generation, plus computational requirements. Experiments are performed on a Monolingual Encoder (GLUE), Monolingual Decoder (LAMBADA/LM Evaluation Harness), and Multilingual Encoder (XNLI Benchmark), with CWT showing improvements on all settings compared to the model trained classically. Lastly, the paper presents experiments on vocabulary size, as expanding the vocabulary is less costly and may provides some advantages with CWT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper presents a simple and well motivated method for pre-training a language model, CWT.\n- The paper implements various experiments showing that CWT consistently improves performance, training efficiency, or both, over the classical version of the model."
                },
                "weaknesses": {
                    "value": "The main issue I found with this paper is a huge lack of bibliographic work. It mainly keeps the paper from proper contextualization but I believe a proper related work would have greatly helped in making the experiments less shallow. \n- The lack of reference to relevant previous work keeps the paper from being well-positioned within the literature, especially when it comes to comparison with the numerous pre-existing similar methods,\n- It also keeps it from investigating relevant experimental settings - for example, comparison with similar objectives and ablation studies which could shed some light on what makes CWT work better than some previous methods."
                },
                "questions": {
                    "value": "- On the missing related work: I will detail what I estimate to be essential. I believe you could find most of these references (and many other) in papers you have yourself cited (the papers describing ELECTRA and ELECTRIC, if only).\n    - You write \"This work stands as the first attempt to train language models using an explicit contrastive loss\".  The first that I am aware of would be [1], but there are many more recent references, especially those making use of Noise Contrastive Estimation; with [2] which, while binary in nature, has been adapted into a loss resembling your own in [3], though I believe the first use of a direct softmax-like objective with negative examples coming from the batch is from [4].\n    - Though I believe input/output embeddings weight sharing was a common approach that was very certainly implemented in most early languages models prior to 2017, your work combines this approach with a contrastive objective in order to get rid of the output vocabulary, which is a very nice idea. On that matter, I will mainly cite [5], a dedicated loss aiming at predicting static embeddings too; another paper [6] studies how to choose the target embeddings (which you use weight tying for).     \n    - Language modelling without an output vocabulary (contrastive objective or not) has been explored quite a lot in the past, with the specific problem you address in subsection 3.3 with fine-tuning having had many proposed solutions. I believe a nearest neighbour approach has been used in [5]\u00a0and [6], but [7] proposes an energy-based model to repurpose a well-trained encoder (outputting *embeddings*) into a generative model.\n    - I believe you will find in this previous work many insights which will help you improve the design of your experiments and directions for improvement.\n\n- On experiments:\n    - Do you have any hypothesis regarding the better performance of CWT than the vanilla model *with the same number of training token* on most GLUE tasks ? \n    - In fine-tuning a causal LM for text generation, is the supplementary amount of compute received by the fine-tuned model compared to the vanilla model significant ? \n\n\n\n\n- [1] Quick Training of Probabilistic Neural Nets by Importance Sampling (Bengio and S\u00e9nacal, 2003)\n- [2] A fast and simple algorithm for training neural probabilistic language models (Mnih and Teh, 2013)\n- [3] Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency (Ma and Collins, 2018)\n- [4] On Using Very Large Target Vocabulary for Neural Machine Translation (Jean et al, 2014)\n- [5] Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs (Kumar and Tsekov, 2019)\n- [6] On Target Representation in Continuous-output Neural Machine Translation (Tokarchuk and Niculae, 2022).\n- [7] Residual Energy-Based Models for Text Generation (Deng et al, 2019)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2843/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2843/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2843/Reviewer_CHGU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795734993,
            "cdate": 1698795734993,
            "tmdate": 1700668248659,
            "mdate": 1700668248659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4pyvE6g3s5",
                "forum": "ONPECq0Rk7",
                "replyto": "6O61GTdSgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "- *On missing references*\n\nWe are truly grateful to the reviewer for pointing out these very interesting references. We had indeed missed to what extent this part of the literature was relevant to our work. When we wrote the Related Work section, we wanted to focus on the specific context of (pre-)training models in a self-supervised fashion. Hence, we specifically cited concurrent works that used contrastive learning in that specific context.\n\nWe still advocate that our work is novel *in that specific context*, but we will adjust some phrasings in our article to avoid conveying any impression of over-statement. In the next few days, we will upload a revised version of our paper with an updated Related Work section that engages better with the line of work you refer to. \n\nWe would also like to underline **the differences between our work and the references the reviewer suggested**. A significant difference between our approach and importance sampling methods [1,2,4] is that our regularization is done on the hidden representation distribution and not on its support. As a matter of fact, we sample the negative samples uniformly from the batch, which means several instances of a given token may be found among them. The sorting objective of [3] is indeed an application of the NCE loss for the language modeling task, but they do not use the input embeddings as targets, resulting in a memory overhead. While [5] uses a tied input/output approach, they freeze the input embeddings and use trained static embeddings as targets. In [6], the authors also use various trained non-contextual embeddings to anchor their predictions. Although it could be interesting to see how the EBM approach from [7] works with our encoder, it is not clear how it could solve the problem of calibration in the decoder case, as it is used to sort sequences sampled from the pretrained LM, while our causal headless model needs to be adapted for token-level generation.\n\n**Questions**\n\n- *On better data-efficiency*\n\nWe attempt to explain the better performance of our approach in **Appendix A**, as detailed in the General rebuttal. Our argument is also used by [4] (section 3.1.1), which we will mention accordingly.\n\n- *Supplementary compute*\n\nSee *General rebuttal / On the additional cost of causal fine-tuning*. The additional fine-tuning costs 1,7% of the pre-training compute."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699918417346,
                "cdate": 1699918417346,
                "tmdate": 1699918417346,
                "mdate": 1699918417346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qJxl4jjpTR",
                "forum": "ONPECq0Rk7",
                "replyto": "6O61GTdSgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer CHGU,\n\nWe thank you again for your remarks and suggestions. We have integrated the missing literature in the updated version of the article, and we have rewritten sentences that you have rightfully pointed out as misleading.\nConsidering that the discussion phase is coming to an end, we would be happy to receive your feedback and to answer any supplementary questions that you may have.\n\nBest regards,\nSubmission 2843 Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588088226,
                "cdate": 1700588088226,
                "tmdate": 1700588088226,
                "mdate": 1700588088226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1StpprlZdo",
                "forum": "ONPECq0Rk7",
                "replyto": "6O61GTdSgH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2843/Reviewer_CHGU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2843/Reviewer_CHGU"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the author's detailed response to my and other reviewer's comments. I better understand your goal and positioning with respect to the literature. However, I believe the current update made to the related works is lacking - as an enumeration of papers, with no real positioning - while your rebuttal here clearly position your work with respect to these papers. I also appreciate the added ablation study, and the hypothesis and visualization proposed in Appendix A. \nOverall, following these developments, I am raising my score to 6 (and would have liked to go in between 6 and 8, if the rating system allowed) - assuming that the authors would continue improving their related work section to provide accurate positioning."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668218966,
                "cdate": 1700668218966,
                "tmdate": 1700668376467,
                "mdate": 1700668376467,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z4T376hwqH",
            "forum": "ONPECq0Rk7",
            "replyto": "ONPECq0Rk7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2843/Reviewer_ccZo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2843/Reviewer_ccZo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes replacing the standard cross-entropy-based language modeling objective (classification over token vocabulary) with a \"contrastive weight tying\" objective, which is more memory and compute efficient during training. Computing the full cross-entropy loss requires loading and multiplying the final model embeddings by a $d_{model} \\times V$ matrix, where $V$ is the vocabulary size (often $\\geq 30k$), which can be quite expensive (especially for small models, with few layers and $d_{model} \\ll V$). Instead, this paper proposes using a contrastive objective, where the targets are the input token embeddings for the masked tokens (\"weight tying\"), and the negative samples are taken from the other \"masked\" tokens. It shows in experiments that the proposed method is generally more efficient, and attains better performance, than the standard cross-entropy-based training. Lastly, to allow for generating text (using next token prediction) with these \"headless\" models, the paper proposes performing fine-tuning with a LM head for a small number of tokens ($< 2$% of pre-training dataset)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method can reduce the number of trainable parameters meaningfully for small models with large vocabulary sizes (e.g., by my calculations, reduce # parameters of BERT-base by ~18%).\n- The proposed method generally yields better accuracy on downstream tasks than standard LM training (across mono-lingual encoder experiments with BERT-base, mono-lingual decoder experiments with Pythia-70m, and multi-lingual encoder experiments with distilled multi-lingual BERT).\n- This method decouples the training speed from the number of tokens in the vocabulary, which allows for choosing the vocabulary size that attains best performance."
                },
                "weaknesses": {
                    "value": "- The proposed method is not very novel --- straightforward application of contrastive loss function to LM training.\n- The experiments are relatively small-scale, only considering models < 140m parameters. Unclear if any of the benefits of this method hold for larger models.\n- The paper does not discuss that for larger models, the $d_{model} \\times V$ classification matrix corresponds to a tiny percentage of the total number of model parameters (and thus, total memory and computation during training). For example, it corresponds to roughly 2.5% and 0.4% of the Llama2 (7b) and Llama2 (70b) model parameters, respectively.\n- The proposed method adds a step to decoder-only LM training, as the model must first be pre-trained in a \"headless\" way, and then fine-tuned with a LM classification head. This adds complexity to the training process.\n- The paper does not perform careful ablations to help understand why the proposed method attains better model performance than standard cross-entropy (CE) LM training. For example, it could have considered weight-tying with the CE loss, or a contrastive loss without weight tying. This latter option could still be implemented in a compute-efficient way during training (since only a subset of columns of the classification matrix would need to be used). It could have also tried using different numbers of negative samples for the contrastive loss function (assuming # negative samples was decoupled from # masked samples), such that on one extreme (# neg samples = vocab size) the loss would be equivalent to the cross-entropy loss function (modulo the weight tying). Overall, I was confused by why the proposed method would yield better-performing models than cross-entropy.\n\nNote that I was torn on whether to assign this paper \"marginal accept\" or \"marginal reject\", but chose marginal accept given that I see the efficiency benefit of this approach for smaller models, and was intrigued that this approach could improve model quality. The reasons I was considering marginal reject were that I felt the method was not very novel, only leads to meaningful efficiency gains for small models (and no experiments with medium/large models were performed), and because I felt the paper didn't do a good job explaining why the proposed method performs better. Open to being swayed."
                },
                "questions": {
                    "value": "- Am I understanding correctly that for the monolingual decoder experiments, the proposed method offers no speed advantages, because  \"negative samples correspond to every input embedding at a different position in the batch\"? Would it make sense to decouple the number of negative samples from the number of \"masked\" samples, to allow for controlling these two hyperparameters separately?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815541636,
            "cdate": 1698815541636,
            "tmdate": 1699636227835,
            "mdate": 1699636227835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SzhBgFpkhE",
                "forum": "ONPECq0Rk7",
                "replyto": "Z4T376hwqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for its thorough review, and for its careful technical considerations.\n\n**Weaknesses**\n- Points 1, 2, 4 and 5 are addressed in the *General Rebuttal*\n- *Scaling of the input embeddings proportion*\n\nWe entirely agree that as monolingual models scale, the benefits of our approach in terms of memory and compute efficiency will progressively decrease. However, we argue that the benefits of our approach in terms of data-efficiency, i.e. the final performance of a model trained on the same data, can be expected to scale. We addressed this point in **Appendix B**.\n\nMoreover, we believe that the computational benefits theoretically hold for large language models with larger vocabularies. For mGPT (1.3B parameters, 100000 tokens vocabulary) and Bloom-7B1 (250880 tokens vocabulary), input embeddings represent ~15% of the model parameters. If one used the XLM-V vocabulary (1M tokens) to train the architecture of Llama-7B, the input embeddings would represent ~60% of the trainable parameters. We advocate that these use cases could greatly benefit from our approach.\n\n- *On the lack of ablations*\n\nWe thank the reviewer for this very relevant question. We addressed most of its concerns in *General Rebuttal / On ablation study*. As a clarification, we would like to point out that increasing the number of negative samples would not make the CWT loss equivalent to the cross-entropy loss function. The negative samples would indeed follow the Zipfian distribution of the data instead of the uniform distribution implicitly used in the cross-entropy loss. We hypothesize that regularizing based on the unigram distribution instead of the uniform vocabulary distribution may also explain the performance improvement of CWT. \n\nWe also invite the reviewer to read **Appendix A** that gives some insight on the reasons of the effectiveness of our method.\n\n**Questions**\n- *On choosing the number of negative samples*\n\nThe speed benefit of our method is indeed reduced in this specific setting, as we used a micro-batch size large enough to make the number of negative samples comparable with the vocabulary size. We still observed a training speed-up, which may be explained by optimization considerations (with fewer parameters to update) and/or hardware considerations.\n\nWe mention the idea of considering different amounts of negative samples in a footnote (page 4), but we left the exploration of this idea for future work. Preliminary results indicate that we can indeed lower the number of negative samples without loss of performance, thus further increasing the training throughput."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699912811960,
                "cdate": 1699912811960,
                "tmdate": 1699912811960,
                "mdate": 1699912811960,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "78LOkfAE2Z",
                "forum": "ONPECq0Rk7",
                "replyto": "Z4T376hwqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer ccZo,\n\nWe thank you again for your remarks and questions. We have addressed your concerns on novelty, and conducted an ablation study similar to the one you suggested that was added to the paper.\nConsidering that the discussion phase is coming to an end, we would be happy to receive your feedback and to answer any supplementary questions that you may have.\n\nBest regards,\nSubmission 2843 Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587935220,
                "cdate": 1700587935220,
                "tmdate": 1700587935220,
                "mdate": 1700587935220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "X9ytLn66RI",
            "forum": "ONPECq0Rk7",
            "replyto": "ONPECq0Rk7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2843/Reviewer_A6os"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2843/Reviewer_A6os"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces an innovative approach to self-supervised pre-training of language models. Instead of predicting probability distributions over token vocabularies, the method focuses on reconstructing input embeddings contrastively using Contrastive Weight Tying (CWT). This approach is applied to pretrain Headless Language Models in both monolingual and multilingual contexts, offering practical advantages. It is significantly more compute-efficient, data-efficient, and performant than classical predictive methods. The paper's contributions include the introduction of a new pretraining objective, the pretraining of encoder and decoder models for English and a multilingual encoder model, demonstrating the benefits of headless training, and exploring the effects of pretraining hyperparameters on downstream performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper maintains a high standard of quality. The methodology is well-structured and explained clearly, making it accessible to readers. The empirical results demonstrate substantial reductions in training computational requirements, improved downstream performance, and increased data efficiency. The study includes a significant increase in GLUE score and LAMBADA accuracy, indicating a quality improvement over classical LMs.\n\n2. The paper is well-written and clear in its explanations. It effectively communicates the core concepts and methodologies involved in the proposed approach. The objectives and benefits of headless training are well-articulated, and the exploration of pretraining hyperparameters adds to the clarity of the study.\n\n3. The paper's significance lies in its departure from traditional language model pre-training by introducing a contrastive objective. Sufficient experiments validate the advantages of the proposed contrastive learning loss over traditional cross-entropy loss, and the experimental results are relatively convincing.\n\nIn summary, it is a valuable contribution to the field of self-supervised language model pre-training. The proposed approach has the potential to impact the efficiency and effectiveness of language models in various applications."
                },
                "weaknesses": {
                    "value": "1. The proposed method is not very novel and original. Although it departs from traditional probability prediction, offering a unique perspective on language model pre-training, there is lots of similar work that leverages the similar contrastive learning method. Thus, the use of Contrastive Weight Tying (CWT) in this context almost combines the existing ideas into the pretraining model, and it is not a very novel and creative methodology;\n\n2. For the currently most popular CLMs, the proposed method still requires fine-tuning on a small amount of training data; otherwise, the performance would be significantly worse than the vanilla approach, adding complexity to the training process;\n\n3. With the current trend of scaling up language models, it would be even more valuable to validate the proposed method on larger generative LLMs, such as ChatGLM or LLaMa."
                },
                "questions": {
                    "value": "1. I'm curious about the experimental results in Table 2. I wonder why there is a significant improvement on all test sets except the CB test set. Why does the Headless model perform so much worse than the vanilla model on the CB test set? Could the authors provide a detailed analysis and explanation for this? Is there anything special about the CB test set?\n\n2. In Table 3, why is there no output for the Headless model's perplexity (ppl) on the validation set?\n\n3. I believe it's reasonable that the Headless model is more memory-efficient and exhibits higher training and inference efficiency on the XNLI benchmark compared to the Vanilla model. However, why is there such a significant improvement in quality as well? Where do these quality improvements primarily come from? Could the authors provide a more specific explanation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2843/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835305262,
            "cdate": 1698835305262,
            "tmdate": 1699636227766,
            "mdate": 1699636227766,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1NcyXtL9u8",
                "forum": "ONPECq0Rk7",
                "replyto": "X9ytLn66RI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for its clear and relevant review.\n\n**Weaknesses**\n\n1, 2, 3 : see *General Rebuttal*\n\n**Questions**\n\n1. *On the CB dataset*\n\nWe analyzed the reason for this discrepancy more thoroughly. We found that the CB validation set we used for our analysis **only counts 56 examples**. While the standard deviation of the performance of each model is close to 1, making the score gap meaningful across seeds, it only requires a correct classification on two more samples to explain such a gap with this sample size. We believe that this explains the observed discrepancy.\n\n2. *No validation perplexity in Table 3*\n\nWe did not report perplexity for headless models during our initial training, as it would have required us adapting the forward procedure to use the transposed input embeddings matrix as a projection head during validation steps only. Moreover, we did not realize *before training* that we would need to put the model through a fine-tuning phase after pre-training. Finally, as shown by the evaluation on the Lambada dataset, the perplexity level of the headless model before fine-tuning is not really relevant for comparison with the other models.\n\nWe would very much like to compute a perplexity score now to answer your question, but the OpenWebText2 dataset is not hosted anymore (https://huggingface.co/datasets/the_pile_openwebtext2/discussions/5), which does not let us do so.\n\n3. see *General Rebuttal / On understanding the performance gap*"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699910829480,
                "cdate": 1699910829480,
                "tmdate": 1699910829480,
                "mdate": 1699910829480,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QEiuU9o13y",
                "forum": "ONPECq0Rk7",
                "replyto": "X9ytLn66RI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2843/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer A6os,\n\nWe thank you again for your remarks and questions. We hope to have addressed them with our rebuttal and the updated version of our article. \nConsidering that the discussion phase is coming to an end, we would be happy to receive your feedback and to answer any supplementary questions that you may have.\n\nBest regards,\nSubmission 2843 Authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2843/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587664562,
                "cdate": 1700587664562,
                "tmdate": 1700587664562,
                "mdate": 1700587664562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]