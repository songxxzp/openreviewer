[
    {
        "title": "How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation"
    },
    {
        "review": {
            "id": "ZXzkWJ7NVc",
            "forum": "v0zNCwwkaV",
            "replyto": "v0zNCwwkaV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1402/Reviewer_7Xxi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1402/Reviewer_7Xxi"
            ],
            "content": {
                "summary": {
                    "value": "This paper solves the transformer attention scheme over triplets under some mild complexity theoretical assumptions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\nThis work is original to the best of my knowledge.\n\nQuality:\nQuality is high.\n\nClarity:\nWriting is clear.\n\nSignificance:\nThe results of this paper are important due to their connections to LLMs and other AI applications."
                },
                "weaknesses": {
                    "value": "None."
                },
                "questions": {
                    "value": "Page 2:\nCould you please explain the column-wise Kronecker product of V_1 and V_2 here as well?\n\nPage 3:\nPlease elaborate on SETH.\nI know it is standard, but perhaps too strong? :)\n\nPage 4:\nPlease provide some intuition for Definition 1.5.\n\nPage 5:\nI do not understand \"Approximating A.\"\n\nPage 6:\nCould you please sketch an short example for the reduction from GapMaxIP to ATAttc?\n\nPage 8:\nCould you please add some discussion about Theorem 4.7?\n\nPage 9:\nLine 12 from bottom:\nWhy is there such a \\mu?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1402/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1402/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1402/Reviewer_7Xxi"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697904851044,
            "cdate": 1697904851044,
            "tmdate": 1699636067958,
            "mdate": 1699636067958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0CpdEhiNKw",
                "forum": "v0zNCwwkaV",
                "replyto": "ZXzkWJ7NVc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1402/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 7Xxi"
                    },
                    "comment": {
                        "value": "Thank you to the reviewer for the feedback and detailed suggestions.\n\nQ: Could you please explain the column-wise Kronecker product of V_1 and V_2 here as well?\n\nA: Yes, thanks.\n\nQ: Please elaborate on SETH. I know it is standard, but perhaps too strong? :)\n\nA: SETH roughly says that one cannot improve too much on our current best algorithms for solving CNF-SAT. This is one of the most well-studied algorithms problems, yet improvements have been rare and very small. Although the word \u201cstrong\u201d appears in its name, the majority of complexity theorists believe it is true, and much of the area of fine-grained complexity theory is built off of it.\n\n \n\nQ: I do not understand \"Approximating A.\"\n\nA: The matrix $\\widetilde{A}$ is an ell_infinity approximation of A.\n\nQ: Could you please sketch an short example for the reduction from GapMaxIP to ATAttc?\n\nA: The reduction is depicted at the bottom of page 22; we will refer to it above.\n\nQ: Could you please add some discussion about Theorem 4.7?\n\nA: Yes, this is the result we discuss in section 3.2 (\u201cHardness of Gap-MaxIP\u201d).\n\nQ: Line 12 from bottom: Why is there such a $\\mu$?\n\nA: Sorry, there is a typo here. If the three are _not_ orthogonal then there is no such $\\mu$, and if they are orthogonal, then there is such a $\\mu$. The message $\\mu$ is the one Merlin would send to get the players to accept that the vectors are orthogonal."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508198692,
                "cdate": 1700508198692,
                "tmdate": 1700508221316,
                "mdate": 1700508221316,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JJq04YVtq8",
            "forum": "v0zNCwwkaV",
            "replyto": "v0zNCwwkaV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1402/Reviewer_FrM6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1402/Reviewer_FrM6"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the computational aspects of a so-called \"third-order tensor attention variant\" recently defined by Sanford et al (2023). While standard attention captures pairwise interactions between tokens, the third-order tensor variant was suggested in the context of capturing triplet interactions.\u00a0\n\nThe current paper shows that\u00a0if the input entries are bounded, and if it suffices to compute each entry of the attention output approximately rather than exactly, then there is an almost-linear time algorithm for computing this operation. The bound on the entry magnitude is asymptotically (conditionally) tight, as the paper also shows that without this bound, computing this attention output in time significantly better than the trivial n^3 time computation is SETH-hard, in the complexity theoretic sense. Both the algorithm and the hardness result are generalizations (in terms of both the results themselves, and the techniques used to prove them) of a recent work of Alman and Song (2023) that proved them for standard attention. The results also extend to tensors of higher orders (than 3)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is generally well-written and the mathematical content seems interesting."
                },
                "weaknesses": {
                    "value": "The paper is purely theoretical and seems quite removed from application. It is entirely about a form of tensor attention that has been suggested as a bit of an afterthought in a recent work (Sanford 2023), that deemed it likely impractical and anyway did not implement it. Thus it is not about an architecture that is actually used or presently considered usable. This raises the question of what is the import of showing the existence of an almost-linear time approximation algorithm for it, and whether this algorithm makes sense as part of a neural network (I believe the paper does not touch on this point). A negative outlook on this paper would be that the interesting results and the conceptual message in this line of research were already given in Alman and Song (2023), and the extension to higher-order tensors in this manuscript might be an elegant intellectual and mathematical exercise, albeit without much consequence to the ML community. Nonetheless, since the content does seem elegant, and there is always the issue of benefit-of-doubt about whether a piece of theoretical research would have implications down the road, I prefer to vote for acceptance."
                },
                "questions": {
                    "value": "I'd be interested to hear from the authors what do they consider to be the importance and implication of their algorithm, and whether they deem it implementable within a neural network architecture?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668971107,
            "cdate": 1698668971107,
            "tmdate": 1699636067875,
            "mdate": 1699636067875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bezqnv1G95",
                "forum": "v0zNCwwkaV",
                "replyto": "JJq04YVtq8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1402/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer FrM6"
                    },
                    "comment": {
                        "value": "Thank you to the reviewer for the feedback and detailed suggestions.\n\nQ: I'd be interested to hear from the authors what do they consider to be the importance and implication of their algorithm, and whether they deem it implementable within a neural network architecture?\n\n\nA: One of the biggest takeaways for us is that tensor attention can actually, perhaps unintuitively, be computed in near-linear time. The prior work by Sanford et al. proposed tensor attention as a solution to the weaknesses of matrix attention which they studied, but they remarked that its unclear how to compute it quickly, and thus stopped considering it. We hope our result that it actually could be computed quickly will spark more work on determining both the theoretical and practical use of this generalization.\n\nWe also find it intriguing that the bound on the entries which one needs to fast tensor attention decreases with the dimension of the tensor: for dimension k tensors, it grows like $(\\log n)^{1/k}$. This may lead to a tradeoff in future work: one would pick the largest $k$ for which their entries are small enough, and thus get the most expressive version of attention which can still be computed quickly.\n\nWhether it is implementable within a neural network architecture will evidently require more engineering work beyond our theoretical paper, but we are optimistic that it is possible, as the polynomial method is not ultimately too complicated."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508040996,
                "cdate": 1700508040996,
                "tmdate": 1700508122172,
                "mdate": 1700508122172,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BT2Pm2aiPB",
                "forum": "v0zNCwwkaV",
                "replyto": "bezqnv1G95",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1402/Reviewer_FrM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1402/Reviewer_FrM6"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their answers."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676957951,
                "cdate": 1700676957951,
                "tmdate": 1700676957951,
                "mdate": 1700676957951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "75riXveNic",
            "forum": "v0zNCwwkaV",
            "replyto": "v0zNCwwkaV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1402/Reviewer_dthX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1402/Reviewer_dthX"
            ],
            "content": {
                "summary": {
                    "value": "This work explores the computational complexity of generalized matrix attention scheme, which is used to capture high-order correlation among the input sequence. \nTo capture the correlation among triplets of tokens in an input sequence of length $n$, the generalized attention scheme outputs an attention matrix by computing the column-wise Kronecker products based on one query matrix, two key matrices and two value matrices. \nIn such a case, this work shows if the max entries of the query, key and value matrices are bounded by $o(\\sqrt[3]{\\log n})$, one can compute an approximation to the generalized attention matrix in almost linear time. On the other hand, if the max entries of the input matrices are at least $\\Omega(\\sqrt[3]{\\log n}$), one cannot hope to efficiently approximate  the attention matrix in less than cubic time, assuming SETH. The latter hardness result is shown by reducing from the Gap Max IP problem, whose hardness is then shown through a combination and generalization of previous techniques.  Furthermore, the work shows the techniques developed above can be extended to characterize the gap in computational complexity in $k$-th order generalization of the attention scheme."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This work considers an interesting problem of computing generalized matrix attention scheme. Since the generalized schemes involves computing the Kronecker products between a set of matrices, this is apparently a computationally expensive operation. It is hence natural to explore the computational complexity of this problem.\n\n- Both the upper bound and the lower bound results (especially the latter one) presented in the work are interesting. \n\n- A high-level summarization and intuition behind the techniques used to derive the upper and the lower bound helps the reader."
                },
                "weaknesses": {
                    "value": "The presentation needs to be improved. \n\n- In Section 3.2 hardness, \u201c3-Max IP\u201d and \u201cGap-MaxIP\u201d seem to refer to the same problem. It is confusing to give two names to the same problem.\n\n- It might be clearer to present the upper bound (UB) and the lower bound (LB) in two separate sections, instead of giving an overview of the UB + LB, and then elaborate on the LB.\n\n- Is it possible to give a few sentences of description of the mysterious $U_1, U_2, U_3$ matrices and how to compute them in Section 3.1?\n\n- In Section 4 \u201chardness\u201d which elaborates on key steps in showing the LB, presenting Hypothesis 4.1, Definition 4.2, Conjecture 4.3 and Theorem 4.4, all of which are from prior works, in the main paper do not help much on understanding and appreciating the novelty / challenges addressed in extending and generalizing current proof techniques to show the LB on computational complexity for approximating the generalized attention matrix. Some of them can indeed be moved to the Appendix. It would be better to give more intuition on the (technical) difference between the three-party and four-party communication protocol that computes set disjointness, how algebraic geometry code is applied in extending the proof from three-party to four-party communication and how the new protocol is used in showing the LB on computation time of Gap Max-IP.\n\n- Minor issue: the last paragraph in page 5 states \u201cshowing that the same line of attack\u201d? \u201cattack\u201d here means \u201ctechniques\u201d, I assume?"
                },
                "questions": {
                    "value": "I am not a complexity expert. I have no comments on the proof techniques presented in this work. However, I do have a few questions for the authors.\n\n- In Definition 1.2, why is approximating the higher order attention matrix in the $\\ell_{\\infty}$ norm considered a good metric to evaluate the approximation of a matrix that captures higher order correlation? Why not the other norms?\n\n- In Section 3.1, does \u201c$\\widetilde{D} \\approx D$\u201d mean $\\tilde{D}$ and $D$ close in the $\\ell_{\\infty}$ norm? (and so does \u201c$\\widetilde{A} \\approx A$\u201d?)\n\n- In Section 3.1, why can $\\widetilde{A}(V_1 / V_2)$ be computed in $O(n^{1+o(1)})$ time, while $\\widetilde{D}$ needs to be computed in $O(nd)$ time?\n\n- Why does the construction of the algorithm in Section 3.1 fail when there are large entries $\\Omega(\\sqrt[3]{\\log n})$ in the input matrices?\n\n- In Section 4, what is the major challenge of extending the three-party communication protocol to a four-party communication protocol in Section 4.2? Why does one need to use the algebraic geometry code?\n\n- In Section 4, where does $B = O(\\sqrt[3]{\\log n})$ pop up in the LB proof?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1402/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1402/Reviewer_dthX",
                        "ICLR.cc/2024/Conference/Submission1402/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892942200,
            "cdate": 1698892942200,
            "tmdate": 1700509931288,
            "mdate": 1700509931288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4KvrCAWnId",
                "forum": "v0zNCwwkaV",
                "replyto": "75riXveNic",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1402/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer dthX"
                    },
                    "comment": {
                        "value": "Thank you to the reviewer for the feedback and detailed suggestions.\n\nQ: In Section 3.2 hardness, \u201c3-Max IP\u201d and \u201cGap-MaxIP\u201d seem to refer to the same problem. It is confusing to give two names to the same problem.\n\nA: Yes, our new problem \u201cGap-MaxIP\u201d is a promise version of the more common \u201c3-Max IP\u201d problem from the literature. We\u2019ve changed the heading of the paragraph to no longer say \u201c3-MaxIP\u201d, and discuss it in the text instead.\n\nQ: It might be clearer to present the upper bound (UB) and the lower bound (LB) in two separate sections, instead of giving an overview of the UB + LB, and then elaborate on the LB.\n\nA: Thank you. We arranged the paper as we did to fit in the page limit, but will try a clearer arrangement for the camera ready version.\n\nQ: Is it possible to give a few sentences of description of the mysterious $U_1, U_2, U_3$ matrices and how to compute them in Section 3.1?\n\nA: Yes we can elaborate on this. Just as showing a matrix is low-rank is equivalent to showing it is the product of two rectangular matrices, showing that a tensor is low-rank is equivalent to showing that it is the \u201cproduct\u201d of three rectangular matrices; $U_1, U_2, U_3$ are those three matrices.\n\nQ: In Section 4 \u201chardness\u201d which elaborates on key steps in showing the LB, presenting Hypothesis 4.1, Definition 4.2, Conjecture 4.3 and Theorem 4.4, all of which are from prior works, ...... the proof from three-party to four-party communication and how the new protocol is used in showing the LB on computation time of Gap Max-IP.\n\nA: We tried to focus in the technique overview on the novel ideas here that don\u2019t already appear in prior work on these communication protocols and fine-grained hardness results. One of the main challenges here was to determine the correct definitions and generalizations so that techniques like AG codes from prior work continue to apply, but the problem can still be reduced to tensor attention. See below for more details.\n\nQ: Minor issue: the last paragraph in page 5 states \u201cshowing that the same line of attack\u201d? \u201cattack\u201d here means \u201ctechniques\u201d, I assume?\n\nA: Yes, by \u201cline of attack\u201d we mean \u201csequence of techniques\u201d.\n\n\nQ: In Definition 1.2, why is approximating the higher order attention matrix in the $\\ell_{\\infty}$ norm considered a good metric to evaluate the approximation of a matrix that captures higher order correlation? Why not the other norms?\n\nA: The $ell_\\infty$ norm can be translated into other reasonable norms, our dependence on eps in the running time is only $\\log(1 / \\epsilon)$. Thus, for instance, if you pay $10 \\log(n/ \\epsilon)$ in the running time, you will get $\\epsilon / n^{10}$ error. Then, if you have $\\ell_\\infty$ norm only $\\epsilon / n^{10}$, then you also have very small matrix spectral norm and matrix  Frobenius norm bounded by $( \\epsilon / n^{10} )  * n^2$.\n\nQ: In Section 3.1, does \u201c$\\widetilde{D} \\approx D$\u201d mean $\\widetilde{D}$ and $D$ close in the  norm? (and so does $\\widetilde{A}$ and $A$?)\n\nA: Yes.\n\nQ: In Section 3.1, why \\widetilde{A} (V_1 V2) can be computed in $n^{1+o(1)}$  time, while $\\widetilde{D}$ needs to be computed in $O(nd)$  time?\n\nA: Here $d = O(\\log n)$, so $O(nd) = O(n \\log n)$ is faster than $n^{1 + o(1)}$. \n\nQ: Why does the construction of the algorithm in Section 3.1 fail when there are large entries  in the input matrices?\n\nA: The bound on B is needed to have a low-degree polynomial approximation of the exponential function. The larger B is, the higher degree one needs, and eventually it gets too large and the approach no longer gives a nontrivial algorithm.\n\nQ: In Section 4, what is the major challenge of extending the three-party communication protocol to a four-party communication protocol in Section 4.2? Why does one need to use the algebraic geometry code?\n\nA: The challenging aspect is to determine what the correct problem is that the four players should be solving. Prior work ultimately showed hardness of an \u201capproximate closest pair\u201d problem, which does not easily generalize to yield hardness for our tensor setting. Indeed, natural options like \u201capproximate closest triple\u201d aren\u2019t easy to reduce to attention. Once the correct problem definitions are in place, the actual protocol is not so difficult given prior work. AG codes are important here to save log factors compared to simpler constructions like Reed-Solomon codes. Saving the log factor is important since it ultimately appears in the exponent of the running time of our reduction in section E.\n\nQ: In Section 4, where does $B = O(\\sqrt[3]{\\log n})$  pop up in the LB proof?\n\nA: The bound B on the entries in attention arises in section E in the appendix, where we reduce from Gap-MaxIP to attention and complete the hardness proof. Section 4 proves hardness of Gap-MaxIP, and doesn\u2019t yet involve the entry bound."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507991274,
                "cdate": 1700507991274,
                "tmdate": 1700508569194,
                "mdate": 1700508569194,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5lPVejvdaq",
                "forum": "v0zNCwwkaV",
                "replyto": "4KvrCAWnId",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1402/Reviewer_dthX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1402/Reviewer_dthX"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' comments"
                    },
                    "comment": {
                        "value": "Thank you very much for the clarification. I would like to raise the score from 6 to8."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509988137,
                "cdate": 1700509988137,
                "tmdate": 1700509988137,
                "mdate": 1700509988137,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]