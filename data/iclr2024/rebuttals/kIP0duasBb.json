[
    {
        "title": "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models"
    },
    {
        "review": {
            "id": "UwMAkiCRxr",
            "forum": "kIP0duasBb",
            "replyto": "kIP0duasBb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4239/Reviewer_efhx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4239/Reviewer_efhx"
            ],
            "content": {
                "summary": {
                    "value": "This paper solves test-time adaptation of Vision-Language-Models to improve the zero-shot generalization performances. Unlike previous works that rely on entropy of model outputs, the authors propose to leverage reward from CLIP model as feedback to adapt models. The proposed method based on reinforcement learning with CLIP feedback is evaluated on three tasks, including zero-shot image classification, zero-shot text-image retrieval, zero-shot and cross-domain image captioning, showing improved performances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper solves an important task of test-time adaptation of vision-language models. VLMs have played a critical role in many CV and NLP tasks.  How to improve their zero-shot generalization ability, especially in the challenging task of test-time adaptation, is a worthwhile research problem.\n+ The authors propose to leverage CLIP feedback for adapting models. This seems to be novel and is interesting to me. As many vision-language learning methods are built upon CLIP, the intergration of CLIP reward is natural.\n+ The proposed RLCF method universally applies to different tasks, like zero-shot classification, text-image retrieval, image captioning. Hence, the idea may inspire research in many other VL tasks.\n+ The method is simple yet effective. The experiments and analyses are extensive in the paper. For each task, RLCF is compared with a few  baseline methods, showing significant improvements.\n+ The paper is very easy to follow with a clear motivation and method descriptions. Visualizations (e.g. fig.1) also look nice."
                },
                "weaknesses": {
                    "value": "- The method relies on good quality of CLIP feedback. This may restrict its applications in tasks other than images with generic objects. For example, CLIP shows less satisfactory accuracies on fine-grained datasets like FGVC Aircraft, EuroSAT, CUB, etc. CLIPScore may be not informative in the fine-grained classification.\n- The authors use CLIP-ViT-L and a few other variants for calculating reward. The commonly used backbone in VL learning is CLIP-ViT-B. It is understandable that CLIP-ViT-L leads to better text-image alignment. But discussions on CLIPScore with CLIP-ViT-B would be helpful to understand how robust RLCF is to unreliable rewards."
                },
                "questions": {
                    "value": "- In Eq.(4), why need to clip CLIP score to be non-negative? What 'it encourages all model behaviors' means? In Eq.(5), any difference if not subtracting the expectation term?\n- The authors mention RLCF-S adopts weighted reward sum and RLCF-S-M adds a momentum buffer. Could the authors explain more about this? Also as mentioned in the weakness, more comments on using less reliable CLIP like CLIP-RN50, CLIP-B would be appreciated."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4239/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4239/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4239/Reviewer_efhx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698644653183,
            "cdate": 1698644653183,
            "tmdate": 1699636391114,
            "mdate": 1699636391114,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JD8Y4UMoLx",
                "forum": "kIP0duasBb",
                "replyto": "UwMAkiCRxr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal - Part I"
                    },
                    "comment": {
                        "value": "- **1.** Different choices of reward models and how robust RLCF is to unreliable rewards\n\n    To figure out how robust RLCF is to different reward models, we first test RLCF with different reward models in image classification. As shown in Table efhx-1, RLCF is robust to different reward models. Compared to the baseline CoOp, RLCF can achieve improvements even with a CLIP-RN50x4 as the reward model, which is worse than the prompt tuning model CLIP-ViT-B/16. During TTA, RLCF will first sample a few image-text pairs and the reward model will only give feedback to the sampled feedback. RLCF will not require the reward model to provide feedback for the image-text pairs not sampled. Namely, RLCF will not totally follow the output of the reward model. This reduces the risk of model collapse when the reward model is less informative than the tuning model.\n\n**Table efhx-1. Different reward models with RLCF for prompt tuning with CLIP-ViT-B/16 on OOD data.**\n| Method                              | Reward Model  | IN-A      | IN-V2     | IN-R      | IN-Sketch | OOD Avg.  |\n|-------------------------------------|---------------|-----------|-----------|-----------|-----------|-----------|\n| **Zero-shot**                       |               |           |           |           |           |           |\n| CLIP-RN50x4                         |               | 39.66     | 58.72     | 69.10     | 42.73     | 52.55     |\n| CLIP-ViT-B/16                       |               | 47.87     | 60.86     | 73.98     | 46.09     | 57.20     |\n| CLIP-ViT-L/14                       |               | 68.82     | 67.80     | 85.40     | 57.84     | 69.97     |\n|                                     |               |           |           |           |           |           |\n| **Prompt Tuning for CLIP-ViT-B/16** |               |           |           |           |           |           |\n| CoOp                                |               | 49.71     | 64.20     | 75.21     | 47.99     | 59.28     |\n| TPT + CoOp                          |               | 57.95     | 66.83     | 77.27     | 49.29     | 62.84     |\n| RLCF + CoOp                         | CLIP-RN50x4   | 52.06     | 64.62     | 76.00     | 48.42     | 60.28     |\n| RLCF + CoOp                         | CLIP-ViT-B/16 | 61.66     | 67.04     | 78.06     | 49.70     | 64.12     |\n| RLCF + CoOp                         | CLIP-ViT-L/14 | **69.74** | **70.62** | **84.51** | **56.49** | **70.34** |\n\nWe also test different reward models in image captioning. Results are shown in Table efhx-2. CapDec and CLIPCap both use CLIP-ViT-B/16 as the image embedding extractor. RLCF with different reward models can always achieve significant improvements in the \nnear and out domain data. **This shows the robustness of RLCF to open domain scenarios with different CLIP reward models.**\n\n**Table efhx-2. Different reward models with RLCF in Image captioning.** Train on COCO and test on NoCaps.\n|           | Reward Model  |          |          |          | COCO     | --->     | NoCaps   |          |          |         |\n|-----------|---------------|----------|----------|----------|----------|----------|----------|----------|----------|---------|\n|           |               |          | in       |          |          | near     |          |          | out      |         |\n|           |               | METEOR   | CIDEr    | SPICE    | METEOR   | CIDEr    | SPICE    | METEOR   | CIDEr    | SPICE   |\n| CapDec    |               | 23.9     | 62.6     | 10.3     | 22.3     | 54.0     | 9.6      | 17.2     | 31.7     | 6.4     |\n| **+RLCF** | CLIP-ViT-B/16 |   24.3   |   65.5   |   10.7   |   22.7   |   57.2   |   10.0   |   17.7   |   35.0   |   6.8   |\n| **+RLCF** | CLIP-ViT-L/14 | **24.6** | **68.0** | **10.7** | **23.0** | **57.9** | **10.3** | **17.9** | **35.5** | **6.9** |\n|           |               |          |          |          |          |          |          |          |          |         |\n| CLIPCap   |               | 26.4     | 76.9     | 11.9     | 24.8     | 73.5     | 11.0     | 20.3     | 54.6     | 8.6     |\n| **+RLCF** | CLIP-ViT-B/16 |   26.9   |   81.1   |   12.3   |   25.5   |   78.1   |   11.7   |   21.2   |   62.0   |   9.5   |\n| **+RLCF** | CLIP-ViT-L/14 | **27.2** | **84.0** | **12.5** | **25.7** | **79.6** | **11.8** | **21.5** | **63.8** | **9.6** |\n\nWe add these contents to Section B.2 DIFFERENT REWARD MODELS in the appendix of the revised paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307661786,
                "cdate": 1700307661786,
                "tmdate": 1700307661786,
                "mdate": 1700307661786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dxNceHp58y",
                "forum": "kIP0duasBb",
                "replyto": "Fwzd6G9vEJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4239/Reviewer_efhx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4239/Reviewer_efhx"
                ],
                "content": {
                    "title": {
                        "value": "response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the authors' rebuttal. The explaination and results are reasonable to me, thus I maintain my original score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676515271,
                "cdate": 1700676515271,
                "tmdate": 1700676515271,
                "mdate": 1700676515271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s1tW9BmQxs",
            "forum": "kIP0duasBb",
            "replyto": "kIP0duasBb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4239/Reviewer_uHKV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4239/Reviewer_uHKV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the test time adaptation methods for vision-language models. The authors propose a framework, called reinforcement learning with CLIP feedback  (RLCF).  Specifically, given a test sample, the VLM is optimized to maximize the CLIP reward, which is provided by a CLIP model.  RLCF can be applied to image classification, text-to-image/image-to-text retrieval, and image captioning.  Extensive experiments demonstrate the effectiveness of RLCF."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well-organized and easy to follow.\n\n- The proposed RLCF framework is universal and applicable across various VL tasks.\n\n- The experiments on three tasks show the superior effectiveness of RLCF, compared to TPT, KD, and Pseudo-label."
                },
                "weaknesses": {
                    "value": "- The proposed RLCF utilizes the CLIP model to provide a reward score, which is very similar to a recent work [1]. The two tweaks are sampling strategies and adding a baseline to the reward function. It weakens the novelty of the proposed method. Besides, the comparison between the proposed RLCH and [1] is missing.\n\n- The authors state that compared to pseudo-label and KD, the feedback mechanism combines the merits of both the student and teacher. However, the authors only investigate the original version of KD proposed in 2015. Moreover, I recommend that the authors evaluate the ensemble of the student and teacher model, which can directly combine their merits.\n\n- For the experiments on image captioning, I suggest that the authors add the results of  CLIPCap and CapDec with the CLIP-ViT-L/14 architecture (the teacher model) for a more comprehensive comparison.\n\n- Could the authors provide more implementation details of TPT+KD?\n\n- Typo: kD->KD on page 8\n\n\n[1] Cho, Jaemin, et al. Fine-grained Image Captioning with CLIP Reward. In Findings of NAACL, 2022."
                },
                "questions": {
                    "value": "- More advanced KD and the ensemble of the student and teacher model.\n- The results of  CLIPCap and  CapDec with the CLIP-ViT-L/14 architecture.\n- More implementation details of TPT+KD."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4239/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4239/Reviewer_uHKV",
                        "ICLR.cc/2024/Conference/Submission4239/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653851449,
            "cdate": 1698653851449,
            "tmdate": 1700704393519,
            "mdate": 1700704393519,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iXrE970cQy",
                "forum": "kIP0duasBb",
                "replyto": "s1tW9BmQxs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal - Part I"
                    },
                    "comment": {
                        "value": "- **1.** Different between RLCF and the previous work [1]\n\n    - In [1], CLIPScore works with grammar regularization or CIDEr metric during training, while RLCF works with independent CLIP reward function at test time. The training of the grammar head and calculation of the CIDEr metric in [1] both need reference text data, which is unavailable at test time. When CLIPScore is used as the independent reward function in [1] (its Table 2), 11.2 CIDEr is achieved, while the baseline has 110.3 CIDEr. [1] actually demonstrate that CLIP cannot be used as the sole reward function. This is different from our findings that CLIP reward can also be used as an independent reward function at test time.\n    - The difference of the problems is non-negligible. [1] adopt CLIPScore together with grammar regularization or CIDEr metric at the training stage, where batched image-text pairs are available. Thus no sampling strategies are required. By contrast, RLCF works at the test time, where only a single test sample is available. Therefore, task-specific sampling strategies are necessary when using CLIP as the reward model. [1] cannot be directly transferred to test time due to the lack of reference text data, this is also why we do not provide a direct comparison with [1].\n    - We contribute the novel reward baseline for CLIP reward as well as task-specific TTA pipelines along with sampling strategies. These contributions are important for CLIP reward to work as the independent reward function across different tasks at test time. We also demonstrate the effectiveness of CLIP reward across different tasks. These contributions are non-trivial to the community.\n\n    We add more discussion with [1] at the Section RELATED WORK in the revised paper.\n\n\n\n- **2.** More advanced KD and the ensemble of the student and teacher model.\n\n    We provide the ensemble results and two more advanced KD results (DKD[2] and ATKD[3]). The learning rates and TTA steps of the two methods are set the same asn RLCF. We set the hyperparameters of the two methods as the recommendations of the original papers.\n\n    The results are shown in **Tabel uHKV-1**. RLCF is also better than the two KD methods. It is worth noting that RLCF with CLIP-ViT-B/16 is even better than the teacher CLIP-ViT-L/14 on ImageNet-A/ImageNet-V2/ImageNet-R. By contrast, the performance of the student models with KD methods is generally worse than the teacher models (This is a general belief in KD for the image classification problem).\n\n    RLCF is also better than the ensemble results of CLIP-ViT-B/16+CLIP-ViT-L/14 by ~2% on average on the 4 OOD datasets. Compared to the ensemble method, RLCF can adapt to the test distribution with the feedback mechanism. This is why RLCF shows better performance than the ensemble results.\n\n**Tabel uHKV-1 More advanced KD and ensemble results on OOD data**\n|                        | IN-A      | IN-V2     | IN-R        | IN-Sketch  | OOD Avg.  |\n|------------------------|-----------|-----------|-------------|------------|-----------|\n| **_zero-shot_**        |           |           |             |            |           |\n| CLIP-ViT-B/16          | 47.87     | 60.86     | 73.98       | 46.09      | 57.20     |\n| CLIP-ViT-L/14          | 68.82     | 67.80     | 85.40       | 57.84      | 69.97     |\n| Ensemble (B/16 + L/14) | 65.94     | 69.02     | 85.92       | **57.98**  | 69.72     |\n|                        |           |           |             |            |           |\n| **CLIP-ViT-B/16**      |           | **Image** | **Encoder** | **Tuning** |           |\n| DKD [2]                | 67.95     | 65.28     | 82.70       | 53.65      | 67.40     |\n| ATKD [3]               | 70.66     | 65.54     | 85.12       | 53.56      | 68.72     |\n| RLCF                   | **73.71** | **69.77** | **86.19**   | 57.10      | **71.69** |\n\n  We add ATKD in Table 1 and ensemble results in Table 7 in the revised paper.\n\n[1] Cho, Jaemin, et al. Fine-grained Image Captioning with CLIP Reward. In Findings of NAACL, 2022.\n\n[2] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, Jiajun Liang.Decoupled Knowledge Distillation. CVPR 2022.\n\n[3] Guo, Jia, et al. \"Reducing the teacher-student gap via spherical knowledge disitllation.\" arXiv preprint arXiv:2010.07485 (2020)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700306903179,
                "cdate": 1700306903179,
                "tmdate": 1700307084303,
                "mdate": 1700307084303,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J8l2n0dnRT",
                "forum": "kIP0duasBb",
                "replyto": "s1tW9BmQxs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal - Part II"
                    },
                    "comment": {
                        "value": "- **3.** CLIPCap and CapDec with the CLIP-ViT-L/14 architecture \n\n    We show the image captioning results with CLIP-ViT-L/14 in Tabel uHKV-2. With CLIP-ViT-L/14 as the image embedding extractor, RLCF with CLIP-ViT-L/14 as the reward model still achieves significant improvements. Specifically, RLCF+CLIPCap achieve more than +6% improvement in the CIDEr metric on the out domain data of NoCaps.\n    \n    With different configurations of image embedding extractor and CLIP reward model, RLCF can always achieve substantial improvements in the near domain and out domain data. This shows that RLCF is robust to the domain shift. It is desirable for the TTA problem.\n\n**Tabel uHKV-2 CapDec and CLIPCap with CLIP-ViT-L/14 architecture on NoCaps** (train on COCO image or text, test on NoCaps). The reward model is always CLIP-ViT-L/14.\n|           | CLIP Projector |          |          |          | COCO     |  --->    | NoCaps   |          |          |         |\n|-----------|----------------|----------|----------|----------|----------|----------|----------|----------|----------|---------|\n|           |                |          | in       |          |          | near     |          |          | out      |         |\n|           |                |  METEOR  | CIDEr    | SPICE    |  METEOR  | CIDEr    | SPICE    |  METEOR  | CIDEr    | SPICE   |\n| CapDec    | CLIP-ViT-B/16  |   23.9   | 62.6     | 10.3     |   22.3   | 54.0     | 9.6      |   17.2   | 31.7     | 6.4     |\n| **+RLCF** | CLIP-ViT-B/16  | **24.6** | **68.0** | **10.7** | **23.0** | **57.9** | **10.3** | **17.9** | **35.5** | **6.9** |\n|           |                |          |          |          |          |          |          |          |          |         |\n| CapDec    | CLIP-ViT-L/14  |   24.3   |   65.5   |   10.5   |   22.7   |   57.4   |    9.9   |   17.5   |   32.8   |   6.3   |\n| **+RLCF** | CLIP-ViT-L/14  | **24.9** | **66.7** | **11.1** | **23.2** | **60.6** | **10.5** | **18.3** | **36.8** | **6.8** |\n|           |                |          |          |          |          |          |          |          |          |         |\n| CLIPCap   | CLIP-ViT-B/16  |   26.4   | 76.9     | 11.9     |   24.8   | 73.5     | 11.0     |   20.3   | 54.6     | 8.6     |\n| **+RLCF** | CLIP-ViT-B/16  | **27.2** | **84.0** | **12.5** | **25.7** | **79.6** | **11.8** | **21.5** | **63.8** | **9.6** |\n|           |                |          |          |          |          |          |          |          |          |         |\n| CLIPCap   | CLIP-ViT-L/14  |   26.2   |   77.9   |   11.4   |   25.2   |   76.7   |   11.3   |   20.7   |   58.4   |   8.9   |\n| **+RLCF** | CLIP-ViT-L/14  | **27.1** | **80.5** | **12.4** | **26.0** | **81.1** | **12.1** | **21.6** | **65.2** | **9.8** |\n\nWe also add CapDec and CLIPCap with CLIP-ViT-B/16 as the reward model in Table 7 in the revised paper.\n\n\n- **4.** More implementation details of TPT+KD \n\n  For test time prompt tuning with TPT+KD in image classification, we inherit the prompt weights of CoOp. During TTA, we minimize the cross entropy between logits from the teacher and student model for the samples after confidence selection. The teacher model is always CLIP-ViT-L/14.\n\n  As for the learn hyper-parameters, the learning rate and TTA steps are the same as RLCF, i.e., 7e-3 and 3 TTA steps. More TTA steps will not benefit TPT+KD as shown in the retrieval task in Table 2.\n\n[1] Cho, Jaemin, et al. Fine-grained Image Captioning with CLIP Reward. In Findings of NAACL, 2022.\n\n[2] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, Jiajun Liang.Decoupled Knowledge Distillation. CVPR 2022.\n\n[3] Guo, Jia, et al. \"Reducing the teacher-student gap via spherical knowledge disitllation.\" arXiv preprint arXiv:2010.07485 (2020)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307039671,
                "cdate": 1700307039671,
                "tmdate": 1700307072713,
                "mdate": 1700307072713,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1dbNs12Ow9",
                "forum": "kIP0duasBb",
                "replyto": "J8l2n0dnRT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4239/Reviewer_uHKV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4239/Reviewer_uHKV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. It helps clear up my concerns. I have thus decided to increase my score to 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704455122,
                "cdate": 1700704455122,
                "tmdate": 1700704455122,
                "mdate": 1700704455122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BKiayngifM",
            "forum": "kIP0duasBb",
            "replyto": "kIP0duasBb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4239/Reviewer_9Cat"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4239/Reviewer_9Cat"
            ],
            "content": {
                "summary": {
                    "value": "This article suggests a Reinforcement Learning strategy using a CLIP-based model suitable for implementation during test time in the zero-shot learning framework. The suggested concept was tested on three distinct tasks within the zero-shot framework: image classification, image retrieval, and image captioning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1- The paper introduces a novel reinforcement learning-based reward function to enhance the efficacy of the CLIP-based approach, which can be employed during test time for a new dataset.\n\n2- The suggested concept is applied to three distinct tasks: image classification, test to image retrieval, and image captioning, all of which are experimented within the zero-shot framework.\n\n3-  The experiments were conducted on the Imagenet dataset for all three tasks - image classification, image retrieval, and image captioning, demonstrating superior performance compared to recent state-of-the-art methodologies."
                },
                "weaknesses": {
                    "value": "1- The proposed method is employed within the zero-shot learning framework for all three tasks: image classification, image retrieval, and image captioning. It would indeed be intriguing to explore whether it can also be applied within a few-shot learning framework?\n\n2-  Figure-2 illustrates the application of a data augmentation strategy for image classification. Is this same strategy for augmentation also utilized for textual data?\n\n3-  Figure-6 demonstrates the visual outcomes of step-1 and step-4, depicting the progression from the worst to the best. Including results from all steps, such as step-1, step-2, step-3, and step-4, would offer a more comprehensive understanding of how the transformation from the worst results to the best unfolds."
                },
                "questions": {
                    "value": "Please see the questions raised in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698949931333,
            "cdate": 1698949931333,
            "tmdate": 1699636390788,
            "mdate": 1699636390788,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qJ7kxmV60D",
                "forum": "kIP0duasBb",
                "replyto": "BKiayngifM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "- **1.** Can RLCF be applied within a few-shot learning framework.\n\n    Actually, in Table 1 for image classification, for prompt tuning with CLIP-ViT-B-16, RLCF inherits the prompt weights from CoOp[1]. CoOp is a few-shot learning method (16 shots on ImageNet). RLCF works well with weights from CoOp and surpasses the CoOp baseline by a large margin. This case demonstrates that RLCF can also work well with a few-shot method.\n\nWe modify Table 1 in the revised paper to make this point clearer. Specifically, we add the test prompt tuning without CoOp pre-trained prompt weights and highlight the usage of CoOp weights (In the original submission version, we mention the usage of CoOp in the implementation details). These contents are also shown in Table 9Cat-1.\n\n**Table 9Cat-1. RLCF with and without CoOp.**\n| CLIP-ViT-B/16     | ImageNet  | IN-A      | IN-V2     | IN-R      | IN-Sketch |\n|-------------------|-----------|-----------|-----------|-----------|-----------|\n| **Prompt tuning** |           |           |           |           |           |\n|        CoOp       |   71.51   |   49.71   |   64.20   |   75.21   |   47.99   |\n| TPT               | 68.98     | 54.77     | 63.45     | 77.06     | 47.94     |\n| TPT + CoOp        | 73.61     | 57.95     | 66.83     | 77.27     | 49.29     |\n| RLCF              |   73.23   | 65.45     | 67.77     | 83.35     | 54.74     |\n| RLCF + CoOp       | **76.05** | **69.74** | **70.62** | **84.51** | **56.49** |\n\n\n- **2.** Data augmentation for textual data\n\nWe apply RLCF in image classification, text-image retrieval, and image captioning. Different datasets including ImageNet/MS COCO/Flickr30K/NoCaps are used. For all experiments, **no augmentations are applied for the textual data during TTA**.\n\n- **3.** All steps in Figure 6\n\n  Thanks for your suggestions. We add a new Figure 7 in the appendix of the revised paper to show the results at all TTA steps. \n\n  We display the top-4 generated captions with their CLIP reward as each TTA steps. These  samples illustrate how CLIP reward helps the captioning model select a description that matches the picture more closely\n\n[1] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision language models. IJCV, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307308621,
                "cdate": 1700307308621,
                "tmdate": 1700307308621,
                "mdate": 1700307308621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]