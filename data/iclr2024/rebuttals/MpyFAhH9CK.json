[
    {
        "title": "Morphological Maze: Control Reconfigurable Soft Robots with Fine-grained Morphology Change"
    },
    {
        "review": {
            "id": "jiJLCenTH9",
            "forum": "MpyFAhH9CK",
            "replyto": "MpyFAhH9CK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2963/Reviewer_UEFW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2963/Reviewer_UEFW"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of reconfigurable robots, which can change morphology to accomplish a task. The paper makes the following contributions: (a) a new simulator to model both elastic and plastic deformation; (b) a benchmark based on the aforementioned simulator, and (c) a coarse to fine reinforcement learning approach for controlling such robots."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed simulator and benchmark fill a hole in the current literature on morphology-changing robots\n2. The proposed hierarchical control approach outperforms other ablations and another baseline in a set of experiments."
                },
                "weaknesses": {
                    "value": "Overall, I found this paper quite interesting to read. The main limitations, I think, are:\n1. It is unclear whether the proposed simulator and benchmark is a good model of reality. Is it modeling a specific robot or class of robots? Could there be some experiments to show the validity of the simulator? I'm not necessarily referring to simulation to reality transfer, but rather to show that the simulator dynamics are correlated with the dynamics of a real robot.\n2. The proposed hierarchical reinforcement learning approach is relatively standard. Its application to the problem of morphology-changing robots might be new, though. \n3. I don't understand the reasoning behind the proposed baselines. There seem to be other approaches that are more related to the problem (e.g. Phatak et al., Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity; Whitman et al., Learning Modular Robot Control Policies). Having a more detailed comparison could give a better idea of how difficult is the proposed benchmark for current algorithms.\n\nMinor points:\n1. Why are these tasks selected? What exactly is it that they evaluate? Why is it important to have them instead of others?\n2. I am not sure I understand how the baseline of Neural Field Policy is implemented and whether this is a novel contribution of the paper or some other work proposed before for this problem.\n3. I don't think Fig. 6 is helpful, the concept it represents is trivial and easily explained in a sentence"
                },
                "questions": {
                    "value": "It would be great if the authors could clarify why the proposed simulator is a good model of real robots, why the proposed tasks were selected, and clarify the experimental setup. In addition, it would be great to include more relevant benchmarks for this problem."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Reviewer_UEFW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822198734,
            "cdate": 1698822198734,
            "tmdate": 1700713199777,
            "mdate": 1700713199777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aIjK9TKwKb",
                "forum": "MpyFAhH9CK",
                "replyto": "jiJLCenTH9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Part (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments. We respond to your comments below as well as adding more baselines.\n\n> It is unclear whether the proposed simulator and benchmark is a good model of reality. Is it modeling a specific robot or class of robots? Could there be some experiments to show the validity of the simulator? I'm not necessarily referring to simulation to reality transfer, but rather to show that the simulator dynamics are correlated with the dynamics of a real robot.\n\nThank you for pointing out the real world connection! We took great caution throughout the project to make sure everything is governed by real-world physical equations. Our physical simulation is based on MPM, a widely used simulation algorithm that\u2019s designed to simulate real world materials like liquids and plastics. Our robot material is simulated in a way governed by equations of the real stress-strain curve of plastic-elasto materials. The actuation is governed by that of magnetic field (which is used to control the first real-world highly reconfigurable robot [26]), and follows the original implementation in Taichi that simulates point gravity attraction. The only change we made is to normalize the actuation to follow conservation of momentum because we are parameterizing internal forces only. This is done before feeding actuation into simulation and doesn\u2019t break any physical equation itself.\n\n> The proposed hierarchical reinforcement learning approach is relatively standard. Its application to the problem of morphology-changing robots might be new, though.\n\nThanks for mentioning HRL, a nice potential addition to our related work. While our method has a spatial resolution hierarchy, it\u2019s quite different from the hierarchy in HRL. In HRL, high-level policy and low-level policies are designed to be disentangled, such that they never share the input-output behavior. Rather, the high-level policy\u2019s output is usually fed to low-level policy as input. For example, The famous D4RL [27] environment \u201cant maze\u201d is widely used in HRL [28][29], where a high-level policy outputs local goals and low-level policy outputs actuations. \n\nIn contrast, our policy of different resolutions shares the same 2D input and output space. The action field of high-resolution policy and that of low-resolution are directly added algebraically (The weighting mask and feed of coarse action into fine policy are optional). This is because of our hierarchy in the sense of spatial hierarchy, like CNNs and Feature Pyramids in computer vision, rather than the hierarchy in HRL. We appreciate the relation you pointed out and would add explanations to our paper upon acceptance.\n\n> I don't understand the reasoning behind the proposed baselines. There seem to be other approaches that are more related to the problem (e.g. Pathak et al., Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity; Whitman et al., Learning Modular Robot Control Policies). Having a more detailed comparison could give a better idea of how difficult the proposed benchmark for current algorithms is. \n\nThanks for bringing up these baselines. We\u2019d like to clarify that suggested baselines aren\u2019t directly applicable here while also providing results of the adopted version of suggested baseline [30].\n\n* Clarification: highly-reconfigurable robots lack articulated joints, links and thus lack of explicit topological structure these baselines need, as we highlighted in general response. \n\n* **Additional experiments**: Despite that, we adopt the suggested Modular Policy baseline[28] by assigning topology with k-means clustering so GNN can be applied. We provide an additional baseline by letting an attention mechanism to figure out topology itself. We benchmark the baselines on 4 of the 8 tasks due to time constraints with the same number of seeds.\n\nDetails of both baselines and result curves are added to our [rebuttal website](https://sites.google.com/view/morphologicalmazerebuttal). Figure 1.1 in the website illustrates the cluster while figure 1.2 shows our method\u2019s performance is still much higher than the added baselines. This is not surprising, as GNN is only suited for robots with clear articulation, which slime robots lack."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453124519,
                "cdate": 1700453124519,
                "tmdate": 1700612388075,
                "mdate": 1700612388075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XXJnhJxxu4",
                "forum": "MpyFAhH9CK",
                "replyto": "jiJLCenTH9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Part (2/2)"
                    },
                    "comment": {
                        "value": "> Why are these tasks selected? What exactly is it that they evaluate? Why is it important to have them instead of others? Why the proposed tasks were selected, and clarify the experimental setup. In addition, it would be great to include more relevant benchmarks for this problem.\n\nOur task selection follows both generic RL benchmark design principles and the specific needs of reconfigurable robots. We highlight the factors we hoped to cover in our benchmark and the corresponding factor vector of each environment:\n* Tasks have to cover a range of difficulties (A=easy, B=hard)\n* Some tasks evaluates robot itself, others requires the robot\u2019s ability to interact with external objects (A=not require, B=require)\n* Some tasks require just one morphology change, while others need changes of morphology multiple times (A=one change, B=multiple changes)\n* Some have non-convex reward landscape that requires algorithm to be long-horizon (A=short-horizon, B=long-horizon)\n* Some features softer material and some features less soft material (A=softer, B=less softer)\n\n  | Task        | Factor Vector     |\n  |-------------|-------------------|\n  | SHAPE_MATCH | [A, A, A, A, A]   |\n  | RUN         | [A, A, A, A, B]   |\n  | GROW        | [A, A, A, A, A]   |\n  | KICK        | [B, B, A, A, B]   |\n  | DIG         | [B, B, A, A, B]   |\n  | OBSTACLE    | [B, A, B, B, B]   |\n  | CATCH       | [B, B, B, B, A]   |\n  | SLOT        | [B, B, B, B, B]   |\n\n\n\t\nEach task is designed to represent certain factor combinations that we deem important, and we believe they together can give users a comprehensive understanding that a lot of RL ablations do through choices of tasks.\n\nWe agree that more benchmarks would be helpful! However, our problem is the first of its kind. Reconfigurable robot benchmarks are not possible before us due to the lack of formalization and difficulty to make physical simulations, which is our main contribution. While we only have 8 environments in our paper, these two contributions make it much easier for the RL community to contribute more reconfigurable robot tasks. We plan to gradually add more tasks to the benchmark to achieve higher impact in the long term but we also believe that quality of benchmark is more significant in a first benchmark.\n\n> I am not sure I understand how the baseline of Neural Field Policy is implemented and whether this is a novel contribution of the paper or some other work proposed before for this problem.\n\nSorry for the confusion. Neural Field Policy is designed to be a method that can take advantage of our infinitely dimensional action space by querying values in a coordinate-conditioned way. We recognize the confusion it caused, and will replace it in the camera-ready version with the two added baselines following your suggestion.\n\n> I don't think Fig. 6 is helpful, the concept it represents is trivial and easily explained in a sentence\n\nThank you for your suggestion! While the concept is simple, Fig. 6 is also designed to be a **visualization** of the resulting shape of different policies, just like how RL paper also demonstrates suboptimal runs of baselines in their videos to give viewers more insights. We will add more visualizations of different tasks in the camera-ready version.\n\n***\n\n[26] Sun et al. (2022). Reconfigurable magnetic slime robot: deformation, adaptability, and multifunction. Advanced Functional Materials, 32(26), 2112508.\n\n[27] Fu et al. (2020). D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219.\n\n[28] Li et al. (2022). Hierarchical planning through goal-conditioned offline reinforcement learning. IEEE Robotics and Automation Letters, 7(4), 10216-10223.\n\n[29] Nachum et al. (2019). Why does hierarchy (sometimes) work so well in reinforcement learning?. arXiv preprint arXiv:1909.10618.\n\n[30] Pathak et al. (2019). Learning to control self-assembling morphologies: a study of generalization via modularity. Advances in Neural Information Processing Systems, 32."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453743433,
                "cdate": 1700453743433,
                "tmdate": 1700453743433,
                "mdate": 1700453743433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C24Ju4DTOK",
                "forum": "MpyFAhH9CK",
                "replyto": "jiJLCenTH9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer UEFW,\n\nWe first thank you again for your comments and suggestions. \n\nIn our earlier response, we have provided detailed clarification based on your questions about the reality meanings of our proposed method and the settings of our benchmark. In addition, we illustrated the unique technique contributions of our method. Furthermore, we have conducted additional experiments with strong baselines to illustrate the effectiveness of our proposed method on our [rebuttal website](https://sites.google.com/view/morphologicalmazerebuttal) following your great suggestion. Finally, we have provided detailed analysis about the characteristics of tasks in our benchmark to illustrate the rationality of the task designs. \n\nAs we are ending the stage of the author-reviewer discussion soon, we kindly ask you to review our revised paper and our response and reconsider the scores if our response has addressed your concerns. \n\nIf you have any other questions, we are also pleased to respond. We sincerely look forward to your response.\n\nBest wishes!\n\nThe authors"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668514466,
                "cdate": 1700668514466,
                "tmdate": 1700668514466,
                "mdate": 1700668514466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t1YSsFCQy6",
                "forum": "MpyFAhH9CK",
                "replyto": "aIjK9TKwKb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_UEFW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_UEFW"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your rebuttal"
                    },
                    "comment": {
                        "value": "```\nWhile our method has a spatial resolution hierarchy, it\u2019s quite different from the hierarchy in HRL. In HRL, high-level policy and low-level policies are designed to be disentangled, such that they never share the input-output behavior\n```\nThanks for clarifying this. I am, however, unsure why the term hierarchical was used. This is much more reminiscent of the RL technique of iterative skill learning, which is used for simulation to reality transfer (see, for example, Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real by Xie et al, CoRL 2019).\n\n```\nDetails of both baselines and result curves are added to our rebuttal website. Figure 1.1 in the website illustrates the cluster while Figure 1.2 shows our method\u2019s performance is still much higher than the added baselines. This is not surprising, as GNN is only suited for robots with clear articulation, which slime robots lack.\n```\nThanks for adding the new results!\n\nOverall, I am happy to increase my score. However, I'll not go beyond the acceptance threshold since I am not convinced the real-world applicability or usefulness was well justified."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713180357,
                "cdate": 1700713180357,
                "tmdate": 1700713180357,
                "mdate": 1700713180357,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "55TxRUrhor",
                "forum": "MpyFAhH9CK",
                "replyto": "jiJLCenTH9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for raising the score!\n\n>  why the term hierarchical was used\n\nSorry for the confusion. While hierarchical is widely used in HRL, it's also a term in computer vision used widely in super-resolution and feature pyramid (anything that has multi-resolution stage or representations). We will try to avoid such conflict of terms in camera-ready."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714897548,
                "cdate": 1700714897548,
                "tmdate": 1700714905249,
                "mdate": 1700714905249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2f2Mphae7b",
            "forum": "MpyFAhH9CK",
            "replyto": "MpyFAhH9CK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2963/Reviewer_mQFN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2963/Reviewer_mQFN"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a reinforcement learning approach able to control a reconfigurable and simulated soft robot that has to change its shape to perform different tasks. In addition to the novel RL algorithm, which first modifies the coarse structure of the robot and then more fine-grained details, the authors also introduce a new benchmark environment for reconfigurable soft robots. The presented control approach is compared to various baselines, which do not employ a coarse-to-fine control but instead use fine-grained control directly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Exciting research with a path to being employed on real robots such as ones based on ferromagnetic slime\n- Interesting multi-scale muscle field control, where course and fine grain actions can affect the robots morphology\n- Introduced a new interesting benchmark \u201cMorphological Maze\u201d that tests algorithms for their ability to perform morphological change"
                },
                "weaknesses": {
                    "value": "- A potential weakness of the approach, especially when it should ultimately be transferred to real robots, is that the control of the robot's shape do not happen based on local interactions, i.e. the employed controller has to take into account the complete shape of the robot and its environment\n- Missing relevant literature on evolving soft robots and simulators (e.g.VoxCad). \"Cheney, Nick, et al. \"Unshackling evolution: evolving soft robots with multiple materials and a powerful generative encoding.\" ACM SIGEVOlution 7.1 (2014): 11-23.\" In fact, in this work, the authors use a CPPN-encoding which is the foundation for neural field models and should be mentioned.\n- \"However, these search-based zeroth-order optimization methods are computationally demanding and inefficient\u201d - is this shown in the paper? It's a common argument but it would be good to have some references here backing it up.\n- Importantly, what are the lessons for the larger machine learning community? e.g. for which other tasks could the course-to-fine (CFP) algorithm be useful? What about other hierarchical RL methods? The research is exciting from an alife/robot perspective but it could be better motivated for an ICLR audience \n- Since this conference is about representation learning, it would be interesting to investigate further what type of representations the RL algorithms learn to control these soft robots. How do the coarse and fine-grained control interact to solve the tasks at hand?"
                },
                "questions": {
                    "value": "- Is the code available? \n- How computationally expensive is the simulation environment?\n- How large is the action space?\n- how is the upsampling of coarse actions done?\n- What are the network details (e.g. number of layers etc.?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Reviewer_mQFN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826428417,
            "cdate": 1698826428417,
            "tmdate": 1700652102482,
            "mdate": 1700652102482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hqWRRTMYcS",
                "forum": "MpyFAhH9CK",
                "replyto": "2f2Mphae7b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Part (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments. We respond to your comments below.\n\n> A potential weakness of the approach, especially when it should ultimately be transferred to real robots, is that the control of the robot's shape does not happen based on local interactions, i.e. the employed controller has to take into account the complete shape of the robot and its environment.\n\nThank you for making the connection to real robots. First-person-view observation and third-person view observation are both common choices in real robot reinforcement learning papers. We focus on third-person-view exactly because the current real-world hardware implementations [20] [21] of the robot uses a third-person view setup. While we recognize that reconfigurable robot research may evolve in the long-run, we believe our setup and benchmark is best suited for current hardware. \n\n> Missing relevant literature on evolving soft robots and simulators (e.g.VoxCad). \"Cheney, Nick, et al. \"Unshackling evolution: evolving soft robots with multiple materials and a powerful generative encoding.\" ACM SIGEVOlution 7.1 (2014): 11-23.\"\n\nThank you for the mention and we will add it to related work! However, we'd still like to emphasize that our problem is very different from co-design, as we explained in the general response.\n\n> However, these search-based zeroth-order optimization methods are computationally demanding and inefficient\u201d - is this shown in the paper? It's a common argument but it would be good to have some references here backing it up.\n\nThanks for your suggestion. We will add more references to support this claim in camera-ready.\n\n> Importantly, what are the lessons for the larger machine learning community? e.g. for which other tasks could the coarse-to-fine (CFP) algorithm be useful? \n\nCoarse-to-fine has seen its success in many machine learning domains like  image and video generation [22][23]. However, it\u2019s been largely ignored by the RL community because not many RL problems largely ignore the structures of action space. The lesson is that whenever resolutional hierarchy exists, don\u2019t overlook it just because this is not traditionally used in your domain.\n\n> Since this conference is about representation learning, it would be interesting to investigate further what type of representations the RL algorithms learn to control these soft robots. How do the coarse and fine-grained control interact to solve the tasks at hand?\n\nThank you for defending the relevance of the conference. From the \u201cAbout Us\u201d page of ICLR, \u201cThe International Conference on Learning Representations (ICLR) is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence called representation learning, but **generally referred to as deep learning**.\u201d \n\nTherefore, as a deep reinforcement learning paper we naturally fit into the focus of ICLR.\n\n> Is the code available? \n\nYes. We plan to release the code for both benchmark and algorithm upon acceptance.\n\n> How computationally expensive is the simulation environment?\n\nWe made it really fast! The simulation is GPU based. We report the usage and speed of our most computation-intensive task DIG (because it contains the most particle numbers). Simulation alone, the environment runs at an average speed of 70.1 FPS on a RTX4090 GPU, with only 20-30% volatile utility usage and 15% memory usage. Therefore theoretically you can run 5 of such simulations on RTX4090 at similar speed.\n\n> How large is the action space?\n\nAlthough our formalization allows an infinite dimensional action space, our finest policy runs with an output of size 2*16*16 = 512 dims, which gets interpolated to this infinite dimensional action space.\n\n> How is the upsampling of coarse actions done? What are the network details (e.g. number of layers etc.?)\n\nSorry about the confusion. The upsampling of coarse action is done by the bicubic interpolation while the network details are shown below:\n\n* Critic Network:\nThe 3-layer CNN based encoder can embed the input of 5x64x64 (3 channels for state, 2 channels for the upsampled action) images into a vector with 512 dims following a CNN architecture widely used in RL papers and frameworks [24][25]. Then it will get through a 3-layer MLP with 256 dims latent space to yield the Q value.\n* Policy Network:\nThe similar 3-layer CNN based encoder can embed the input of 3x64x64 images into a embedding shaped 32x4x4 (still 512 dims). Then it will get through a 3-layer Deconv to yield grid-like action (2x8x8 or 2x16x16).\n\nIn the residual training stage, the policy network\u2019s input channel will also be 5 (3 channels for state, 2 channels for the upsampled coarse action)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452375818,
                "cdate": 1700452375818,
                "tmdate": 1700532307768,
                "mdate": 1700532307768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fLFVuRaTIn",
                "forum": "MpyFAhH9CK",
                "replyto": "2f2Mphae7b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Part (2/2)"
                    },
                    "comment": {
                        "value": "[20] Jangir et al. (2022). Look closer: Bridging egocentric and third-person views with transformers for robotic manipulation. IEEE Robotics and Automation Letters, 7(2), 3046-3053.\n\n[21] Zhang et al. (2019, May). Solar: Deep structured representations for model-based reinforcement learning. In International conference on machine learning (pp. 7444-7453). PMLR.\n\n[22] Cho et al. (2021). Rethinking coarse-to-fine approach in single image deblurring. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 4641-4650).\n\n[23] Dai et al. (2023). Emu: Enhancing image generation models using photogenic needles in a haystack. arXiv preprint arXiv:2309.15807.\n\n[24] Mnih et al. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\n\n[25] Raffin et al. (2019). Stable baselines3."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700452431067,
                "cdate": 1700452431067,
                "tmdate": 1700452431067,
                "mdate": 1700452431067,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z4n0C07aRy",
                "forum": "MpyFAhH9CK",
                "replyto": "hqWRRTMYcS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_mQFN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_mQFN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. Regarding my comment \"it would be interesting to investigate further what type of representations the RL algorithms learn to control these soft robots. How do the coarse and fine-grained control interact to solve the tasks at hand?\". I didn't doubt that the paper fits ICLR but nevertheless, it would still be interesting to investigate how the RL algorithms does what it does and what representations it did learn (at least for future work). \n\nIt would be good if the model details were added to the paper to facilitate reproducing the results."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515061544,
                "cdate": 1700515061544,
                "tmdate": 1700515061544,
                "mdate": 1700515061544,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZV37OZcv1T",
                "forum": "MpyFAhH9CK",
                "replyto": "z4n0C07aRy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_mQFN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_mQFN"
                ],
                "content": {
                    "comment": {
                        "value": "Given that my questions were answered, I'm happy to increase my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652212432,
                "cdate": 1700652212432,
                "tmdate": 1700652212432,
                "mdate": 1700652212432,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z9THMqVU5B",
            "forum": "MpyFAhH9CK",
            "replyto": "MpyFAhH9CK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2963/Reviewer_NgHH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2963/Reviewer_NgHH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach to learning morphology for soft robot control. The paper formulates reconfigurable soft robot control as a high-dimensional reinforcement learning problem in a continuous 2D muscle field and designs a coarse-to-fine hierarchical policy (CFP) to expedite the exploration of the action space. The paper also implements a benchmark that allows simulating the plastic deformation of robots for various tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Existing methods are well-reviewed, and the classification of existing challenges in reconfigurable soft robots is a strength.\n\n+ The proposed benchmark is non-trivial, and the demo video supports the proposed method.\n\n+ If applicable to real robots, the problem of controlling reconfigurable soft robots is important."
                },
                "weaknesses": {
                    "value": "- The explanation of the full approach is unclear. For example, multiple variables and terms are not defined or explained in the figures, including vx, vy, and SHAPE in Figure 3, the dimension of actions, architectures of the encoder, Coarse, and Residual policy. In the appendix, the paper briefly explains that the core framework is based on existing works SAC and Nature CNN with minor modifications, but it is unclear how to reproduce the full approach.\n\n- Given the concern above, the theoretical novelty seems not high, as the main theoretical novelty of CFP is the introduction of adding residual action to coarse action.\n\n- The paper lacks comparisons with existing state-of-the-art methods. The experiments only evaluate the performance of two ablation baseline methods and one NFP method. More comparisons will be appreciated. Besides the evaluation metric on reward, if more metrics can be used to evaluate real-robot applications, it will be appreciated, such as the successful rate or time efficiency.\n\n- As one of the main challenges proposed to address in this paper, the justification of lifetime adaptation is insufficient in the experiments. For example, if the approach can adapt to noise or actuator failures during the lifetime operation?\n\n- Even though the simulation experiments are impressive, there is still a gap between applying the proposed work to real-world robots, as shown in the demo video. An explanation of how to extend the work from simulations to real-world robots will strengthen the paper significantly."
                },
                "questions": {
                    "value": "Please see the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698884189516,
            "cdate": 1698884189516,
            "tmdate": 1699636240161,
            "mdate": 1699636240161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yxnIcoNcRF",
                "forum": "MpyFAhH9CK",
                "replyto": "z9THMqVU5B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Part (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments. We respond to your comments below.\n\n> The explanation of the full approach is unclear. For example, multiple variables and terms are not defined or explained in the figures, including vx, vy, and SHAPE in Figure 3.\n\nSorry for the confusion. Our vx and vy are tracked velocities of particles, rasterized to pixels. SHAPE is the rasterized occupancy of particles. We will update the paper accordingly in the camera-ready version.\n\n> What is the dimension for actions?\n\nAs we mentioned in section 5.2, the action output is 16x16x2 before interpolation, although the action space itself is infinite dimensional in our formalization.\n\n> What are the architectures of encoder, Coarse, Residual policy?\n\nSorry about the confusion. We follow the natural CNN architecture widely used in RL. The network details are shown below:\n* Critic Network:\nThe 3-layer CNN based encoder can embed the input of 5x64x64 (3 channels for state, 2 channels for the upsampled action) images into a vector with 512 dims following a CNN architecture widely used in RL papers and frameworks [11][12]. Then it will get through a 3-layer MLP with 256 dims latent space to yield the Q value.\n* Policy Network:\nThe similar 3-layer CNN based encoder can embed the input of 3x64x64 images into a embedding shaped 32x4x4 (still 512 dims). Then it will get through a 3-layer Deconv to yield grid-like action (2x8x8 or 2x16x16).\n\nIn the residual training stage, the policy network\u2019s input channel will also be 5 (3 channels for state, 2 channels for the upsampled coarse action).\n\nWe have added more detailed illustrations of our method in figure 2 on the [rebuttal website](https://sites.google.com/view/morphologicalmazerebuttal).\n\n> In the appendix, the paper briefly explains that the core framework is based on existing works SAC and Nature CNN with minor modifications, but it is unclear how to reproduce the full approach.\n\nWe will open-source the entire code base upon acceptance. We will also update the paper to include these details.\n\n> Given the concern above, the theoretical novelty is not high, as the main theoretical novelty of CFP is the introduction of adding residual action to coarse action.\n\nWhile our paper is not a ML theory paper, we still believe we have strong technical contributions that can inspire broader research from the community. First, formalize a novel control problem that has never been studied algorithmically into a Markov Decision Problem. This establishes a platform that future researchers can build on. To our knowledge, we are the first paper to let robots control its muscle to change its own morphology, not to mention the slime robot. Secondly, we overcome the challenges of simulation and provide the RL community with a set of fast benchmarks of this interesting but novel task. Our effort on the simulation is non-trivial and such novel benchmarks alone have been traditionally regarded as technical contributions in many top ML conferences [13] [14] [15]. Thirdly, CFP is one of the first RL algorithms that uses a 2D action space. Many design choices here such as fully convolutional network or coarse to fine are largely overlooked by the RL community, which traditionally focuses on unstructured action space while our insight brings attention to such 2D structure. We believe our formalization, benchmarks and algorithm together shall constitute technical contributions above the acceptance threshold. \n\n> The paper lacks comparisons with existing state-of-the-art methods.\n\nThanks for your suggestion. We added two more related baselines and corresponding experiments following Reviewer UEFW\u2019s suggestion, although we\u2019d like to clarify that these baselines have to undergo significant adoption as we have a very novel task. \n\nWe take state-of-the-art algorithms in the field of articulated reconfigurable robots, Modular Policy [16],  and adopt it for highly-reconfigurable robots by using k-means to assign topological structures to robot muscles. We provide an additional baseline by letting an attention mechanism to figure out topology itself. We benchmark the baselines on 4 of the 8 tasks due to time constraints with the same number of seeds.\n\nDetails of both baselines and result curves are added to our [rebuttal website](https://sites.google.com/view/morphologicalmazerebuttal). Figure 1.1 in the website illustrates the cluster while figure 1.2 shows our method\u2019s performance is still much higher than the added baselines. This is not surprising, as GNN is only suited for robots with clear articulation, which slime robots lack."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451677406,
                "cdate": 1700451677406,
                "tmdate": 1700532221784,
                "mdate": 1700532221784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UjIkKwXM8p",
                "forum": "MpyFAhH9CK",
                "replyto": "z9THMqVU5B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Part (2/2)"
                    },
                    "comment": {
                        "value": "> Besides the evaluation metric on reward, if more metrics can be used to evaluate real-robot applications, it will be appreciated, such as the successful rate or time efficiency.\n\nThank you for the feedback. First we\u2019d like to clarify that our provided metric, maximum episode reward and sample efficiency curves are usually regarded as the most fundamental metrics in the DRL community. In fact, a lot of highly cited DRL papers choose sample efficiency curve as the only metric [17][18]. This is because metrics like success rate can only be defined in tasks like manipulation, while a task like running forward doesn\u2019t clearly have a threshold called \u201csuccess\u201d. Instead, designers may only want the robot to run as far as possible instead of letting it stop at a point. \n\nDespite this, we report the success rate of some manipulation or reaching tasks whose success can be clearly defined. In addition, we also provide the wall-clock-time metric for reference. In figure 3 of our [rebuttal website](https://sites.google.com/view/morphologicalmazerebuttal)), we report these metrics of our algorithm & baselines in two challenging manipulation tasks. We hope such information can give you readers more insight and we are happy to provide additional results if you have additional questions!\n\n> As one of the main challenges proposed to address in this paper, the justification of lifetime adaptation is insufficient in the experiments. For example, if the approach can adapt to noise or actuator failures during the lifetime operation?\n\nThanks for your suggestion. We follow your suggestion and add 2 additional sets of experiments that emulate actuator failure and observation failure. \n\nRegarding noise adaptation, we\u2019d like to clarify that we already added gaussian noise to action output when getting our result in the main paper, to emulate noisy control. The SAC algorithm naturally optimizes expected return under high entropy (noisy) actions, which explains why our result is already good in the paper.\n\nOur two added experiments randomly zeros out 20% of the action output / observation input respectively when evaluating our expert policy. This emulates actuator / sensor failures respectively. Figure 4 on our [rebuttal website](https://sites.google.com/view/morphologicalmazerebuttal) shows their performance as a ratio to original performance. As shown in the figure, our performance is negligibly impacted (<10%) by action failure except 1 out of the 8 tasks, DIG, which shows a performance drop of only 30%. The policy also seems to show some robustness to sensor failure from the plot, a less common setting.\n\n> Even though the simulation experiments are impressive, there is still a gap between applying the proposed work to real-world robots, as shown in the demo video. An explanation of how to extend the work from simulations to real-world robots will strengthen the paper significantly.\n\nThanks for your comment. In our project, we tried to mimic the settings of real-world slime robots implementation in [19]. In this paper, the researchers use cameras and magnetic fields to demonstrate basic movements of the slime robot. In our benchmark, we adopt the gravity field simulation from Taichi to simulate magnetic fields and adopt the same real-world robot observation hardware researchers use. While we recognize the current framework has its assumptions, we believe our implementation is well-aligned with the hardware progress of real-world magnetic robots, which are bottlenecked by algorithms and benchmarks to which our paper aims to contribute. \n\n***\n\n[11] Mnih et al. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\n\n[12] Raffin et al.(2019). Stable baselines3.\n\n[13] Huang et al. (2021). Plasticinelab: A soft-body manipulation benchmark with differentiable physics. arXiv preprint arXiv:2104.03311.\n\n[14] Bhatia et al. (2021). Evolution gym: A large-scale benchmark for evolving soft robots. Advances in Neural Information Processing Systems, 34, 2201-2214.\n\n[15] James et al. (2020). Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2), 3019-3026.\n\n[16] Pathak et al. (2019). Learning to control self-assembling morphologies: a study of generalization via modularity. Advances in Neural Information Processing Systems, 32.\n\n[17] Hafner et al. (2019). Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603.\n\n[18] Laskin et al. (2020). Reinforcement learning with augmented data. Advances in neural information processing systems, 33, 19884-19895.\n\n[19] Sun et al. (2022). Reconfigurable magnetic slime robot: deformation, adaptability, and multifunction. Advanced Functional Materials, 32(26), 2112508."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451849158,
                "cdate": 1700451849158,
                "tmdate": 1700451849158,
                "mdate": 1700451849158,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h1N1nl7Hf6",
                "forum": "MpyFAhH9CK",
                "replyto": "z9THMqVU5B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer NgHH,\n\nWe first thank you again for your comments and suggestions.\u00a0\n\nIn our earlier response, we have provided detailed clarification based on your questions about the method details. In addition to that, we also added additional method details in appendix section E and will release the code to make this fully reproducible. \n\nWe have also conducted additional experiments with strong baselines to illustrate the effectiveness of our proposed method on our [rebuttal website](https://sites.google.com/view/morphologicalmazerebuttal) and we also compared the performance loss when faced with observation or action failures following your great suggestion. Furthermore, we have also listened to your useful advice to put up more metrics such as time efficiency and success rate to comprehensively evaluate our method. Finally, we have provided illustrations about how we can extend our project with real-world settings.\n\nAs we are ending the stage of the author-reviewer discussion soon, we kindly ask you to review our revised paper and our response and reconsider the scores if our response has addressed your concerns.\u00a0\n\nIf you have any other questions, we are also pleased to respond. We sincerely look forward to your response.\n\nBest wishes!\n\nThe authors"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668423125,
                "cdate": 1700668423125,
                "tmdate": 1700720180800,
                "mdate": 1700720180800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dr2RpqjZcm",
            "forum": "MpyFAhH9CK",
            "replyto": "MpyFAhH9CK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new task of **reconfigurable** soft robot design. The goal of this task is to **continuously** optimize the morphology of soft robots **during** the episodes of accomplishing a long-horizon task. \n\nThis paper first develops a simulation platform that implements the action space and transition dynamics of soft robots and environments. It also implements diverse tasks for benchmarking. Specifically, they define long-horizon tasks that require multiple times of morphology changes.\n\nTo address the reconfigurable robot design task, this paper proposes a novel RL algorithm that enables morphology changes from coarse to fine. This algorithm leads to efficient morphology optimization.\n\nThis paper conducts experiments on the proposed simulator with 4 types of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The task of reconfigurable robot design is important. This paper formulates this task as an MDP and provides a simulator that implements the action space & transition dynamics of the MDP.\n2. This paper proposes a novel residual RL algorithm that shows impressive results in the 4 types of tasks."
                },
                "weaknesses": {
                    "value": "1. There is no adequate comparison with classical robot design baselines, e.g., Bayesian optimization, genetic algorithms, etc."
                },
                "questions": {
                    "value": "1. Is the action space or environment 2D or 3D? Why use 2D images as raw states for policy instead of using ground truth robot states(i.e., exact morphology parameterizations)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2963/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903647004,
            "cdate": 1698903647004,
            "tmdate": 1699636240100,
            "mdate": 1699636240100,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yVcSDyqWfx",
                "forum": "MpyFAhH9CK",
                "replyto": "Dr2RpqjZcm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your helpful comments. We respond to your comments below.\n\n> There is no adequate comparison with classical robot design baselines, e.g. Bayesian optimization, genetic algorithms, etc.\n\nThanks for your suggestion about the baselines. While co-design is closely related in our related work section, we\u2019d like to clarify that we are working on a novel problem that is drastically **different** from co-design, rendering the co-design baselines unusable. As we mentioned in the general response, co-design problems **fix** a robot morphology during execution of the task once the algorithm finds an optimal morphology. In contrast, the goal of our project is to control a type of robot that does **not** have a fixed morphology. In our problem, the slime robot has to actively control and **change** its morphology throughout the task. In fact, our robot always begins with a circle or square shape in any task.\n\nDespite this, we added two more related baselines and corresponding experiments following Reviewer UEFW\u2019s suggestion. We take state-of-the-art algorithms in the field of articulated reconfigurable robots, Modular Policy [4],  and adopt it for highly-reconfigurable robots by using k-means to assign topological structures to robot muscles. We provide an additional baseline by letting an attention mechanism to figure out topology itself. We benchmark the baselines on 4 of the 8 tasks due to time constraints with the same number of seeds.\n\nDetails of both baselines and result curves are added to our [rebuttal website](https://sites.google.com/view/morphologicalmazerebuttal). Figure 1.1 in the website illustrates the cluster while figure 1.2 shows our method\u2019s performance is still much higher than the added baselines. This is not surprising, as GNN is only suited for robots with clear articulation, which slime robots lack. \n\n> Is the action space or environment 2D or 3D? \n\nThe action space is represented as a 2-D grid, although our simulator can be easily extended to 3D.\n\n> Why use 2D images as raw states for policy instead of using ground truth robot states(i.e., exact morphology parameterizations)?\n\nWe choose image observations because pixels are the most general representation of tasks. e.g. Millions of video games are played by humans through pixels; typical real robot manipulation tasks in RL literatures. This has been widely recognized by the deep reinforcement community [5] [6], which considers control from pixels as a more challenging problem [7] [8]. \n\nA second reason we consider pixel observation is its potential for real-world transfer. Slime robots, for example the real-world implementation [9] [10], are highly unstructured and hard to obtain an intrinsic state. Pixel observation not only addresses the challenge for us but also organically capturess external world.\n\n***\n\n[4] Pathak et al. (2019). Learning to control self-assembling morphologies: a study of generalization via modularity. Advances in Neural Information Processing Systems, 32.\n\n[5] Mnih et al. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.\n\n[6] Shah et al. Rrl: Resnet as representation for reinforcement learning. arXiv preprint arXiv:2107.03380.\n\n[7] Laskin et al. (2020, November). Curl: Contrastive unsupervised representations for reinforcement learning. In International Conference on Machine Learning (pp. 5639-5650). PMLR.\n\n[8] Laskin et al. (2020). Reinforcement learning with augmented data. Advances in neural information processing systems, 33, 19884-19895.\n\n[9] Sun et al. (2022). Reconfigurable magnetic slime robot: deformation, adaptability, and multifunction. Advanced Functional Materials, 32(26), 2112508.\n\n[10] Wang et al. (2023). Reconfigurable Liquid\u2010Bodied Miniature Machines: Magnetic Control and Microrobotic Applications. Advanced Intelligent Systems, 2300108."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700451051223,
                "cdate": 1700451051223,
                "tmdate": 1700454137308,
                "mdate": 1700454137308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rjruwjsYeA",
                "forum": "MpyFAhH9CK",
                "replyto": "yVcSDyqWfx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification!\n\n>The action space is represented as a 2-D grid, although our simulator can be easily extended to 3D.\n\nI'm not familiar with what work should be done to extend the 2D soft robot simulator to 3D. Could the author provide more details? Because I think 3D robots and environment simulations are significantly more realistic than 2D ones, it would be more beneficial to claim directly that this simulator can be extended to 3D and how to. \n\nAlso, a detailed comparison with the current soft robot simulator would be appreciated. Similar to the one in Table 5 Page15: https://arxiv.org/pdf/2303.09555.pdf\n\n\n>We choose image observations because pixels are the most general representation of tasks. e.g. Millions of video games are played by humans through pixels; typical real robot manipulation tasks in RL literatures. This has been widely recognized by the deep reinforcement community [5] [6], which considers control from pixels as a more challenging problem [7] [8].\n\nI'm kind of not convinced by this reason. Visual RL is important, but I didn't see the rationale of using pixel observation for robot design. I think robot parameterization is much more essential for robot design. If the exact state is hard to obtain in the real world (I admit that it is hard in real world), I'm wondering to what extent using pixel observation could affect the task formulation/difficulty, i.e., would you provide results of your proposed model&baselines that use accurate robot parameterization as the state? Since everything is in simulation, I suppose retrieve intrinsic robot parameterization from simulation is not difficult. \n\n>A second reason we consider pixel observation is its potential for real-world transfer. Slime robots, for example the real-world implementation [9] [10], are highly unstructured and hard to obtain an intrinsic state. Pixel observation not only addresses the challenge for us but also organically capturess external world.\n\n2D pixel observation can be ambiguous in 3D environment. I'm wondering what type of observations should be adopted if extend this simulation to 3D?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535253118,
                "cdate": 1700535253118,
                "tmdate": 1700535253118,
                "mdate": 1700535253118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "buS9ueJAj2",
                "forum": "MpyFAhH9CK",
                "replyto": "Dr2RpqjZcm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback! We sincerely wish our written justification provides additional insight to you in case we cannot get additional results done in time. \n\n> extend the 2D soft robot simulator to 3D ... more beneficial to claim directly that this simulator can be extended to 3D and how to\n\nThank you for your question and suggestions! Our physics simulation is coded with [Taichi](https://github.com/taichi-dev/taichi), which simulates 2D and 3D alike with same equations and algorithms (MPM). A widely used piece of sample code for 3D MPM simulation can be found [here](https://github.com/hzaskywalker/PlasticineLab/blob/main/plb/engine/mpm_simulator.py), (it also demonstrates how to make taichi mpm differentiable) which shares the same material equations as [2D code sample](https://github.com/hzaskywalker/PlasticineLab/blob/main/plb/engine/mpm_simulator.py) except you change things like 2D rotation matrix to 3D and change 2 loops from 3 loops; even the variable names are shared. One can put these two files side-by-side and translate our simulation in 3D without much problem.\n\n> detailed comparison with the current soft robot simulator\nWe provide the suggested table below. Since we are not a co-design benchmark like the one you mentioned and we cited, our environments uniquely allows and requires robots to actively change their shape to accomplish a task (initial shape is always sphere or cube). We think it's an amazing suggestion will include it in our camera ready.\n\n| Platform                       | Simulation Method    | Tasks                      |  Multiphysical Materials                | Morphology Changes During Task | Morphology Changes Need Active Control  | Fine-grained Morphology Changes |  Differentiability|\n|:--------------------------------:|:----------------------:|:----------------------------:|:-----------------------------------------:|:---------------------------------:|:-----------------------------------------:|:------------------:|:------------------:|\n| SoMoGym (Graule et al., 2022) | Rigid-link System     | Mostly Manipulation        |                                         |                                  |                                        |                   |                |\n| DiffAqua (Ma et al., 2021)    | FEM                   |  Swimmer                   |                                          |                                  |                                       |        \u2713          |          \u2713          | \n| EvoGym (Bhatia et al., 2021)  | 2D Mass-spring System | Locomotion & Manipulation  |                                          |                                  |                                       |                   |                  |\n| SoftZoo (Wang et al., 2023)   | MPM                   | Locomotion                 |                    \u2713                      |                                 |                                       |         \u2713           |      \u2713           |\n| Morphological Zoo (Ours)      | MPM                   | Locomotion & Manipulation  |                     \u2713                     |                  \u2713               |                  \u2713                   |         \u2713           |                 |\n\n> Visual RL is important, but I didn't see the rationale of using pixel observation for robot design\n\nThis is because we are not doing robot design, but learning to control a new type of robot! The reason why early RL researchers use state is that control from pixel is hard [7][8], and early algorithms can hardly work unless one gives it the state. From 2018, with the emergence of a good visual-motor-control algorithm, RL community and robot learning researchers have started criticizing state-observation as they can hardly transfer to real world [5] [6] [7] [8]. \n\nOn the other hand, this was not a problem for co-design community, many of whom also do computer graphics and material science, fields which assume materials can be simulated and analyzed in simulators (and often differentiable). Under such assumption, one use computer algorithms to optimize for a design in a simulator before one can 3D print or manufacture a robot. However, to our best-knowledge, such robots cannot be controlled in real world due to state-spaced policy. \n\nWhile we recognize the merit of both school of thoughts in the problems they care about, our goal aligns better with RL & robot learning community. \n\nWe will try our best to run state-based baselines before rebuttal deadline, though it may take a while to train enough seeds. Still, many important works in RL [5][6][7][8] point out that control from image observation is much harder and useful for real-world. \n\n> 2D pixel observation can be ambiguous in 3D environment\n\nGood question. To solve this, multi-camera observation has become standard in real-robot robot learning research, especially for robotics manipulation. For example, a popular [imitation learning method](https://arxiv.org/abs/2304.13705)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541069030,
                "cdate": 1700541069030,
                "tmdate": 1700541158511,
                "mdate": 1700541158511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ze5uzOwTAP",
                "forum": "MpyFAhH9CK",
                "replyto": "buS9ueJAj2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "content": {
                    "comment": {
                        "value": ">Our physics simulation is coded with Taichi, which simulates 2D and 3D alike with same equations and algorithms (MPM). A widely used piece of sample code for 3D MPM simulation can be found here, (it also demonstrates how to make taichi mpm differentiable) which shares the same material equations as 2D code sample except you change things like 2D rotation matrix to 3D and change 2 loops from 3 loops; even the variable names are shared. One can put these two files side-by-side and translate our simulation in 3D without much problem.\n\nThanks for providing those contexts!\n\n>detailed comparison with the current soft robot simulator We provide the suggested table below.\n\nWould you like to explain the lack of **Differentiability**? And what does it mean for the proposed task formulation (which aspect and to what extent)? \n\n>We will try our best to run state-based baselines before rebuttal deadline, though it may take a while to train enough seeds. \n\nThanks for considering adding those results! It will be helpful to understand how much extent using third-person pixel observations would affect the task properties, especially when the task itself is a novel contribution. \n\nAlso, how are the third-person visual observations obtained? Are they from a fixed camera or a moving camera?\n\n>Still, many important works in RL [5][6][7][8] point out that control from image observation is much harder and useful for real-world.\n\nI acknowledge the importance of control on visual observations in some robotic tasks. On the other hand, I think for this task, it's beneficial to understand the influence of different types of observation on the task itself and the potential future methods."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608862528,
                "cdate": 1700608862528,
                "tmdate": 1700608862528,
                "mdate": 1700608862528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rjK1mzRDWd",
                "forum": "MpyFAhH9CK",
                "replyto": "Dr2RpqjZcm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "content": {
                    "comment": {
                        "value": ">The lack of our differentiability is from our motivation.\n\nThanks for providing the helpful context and references!\n\n>Our lately added GNN Modular Policy baseline on our rebuttal website uses state space observation. It is the state-of-the-art method to leverage structure of state space in a closely related problem. As you can see from figure 1.2, this baseline method is actually much lower than us.\n\nI think the more important comparisons are:\n1. your model with accurate state vs. your model with pixel observation;\n2. baselines with accurate state vs. baselines with pixel observation.\n\nBecause those comparisons are more for understanding the problem setting than for your method justification."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613743249,
                "cdate": 1700613743249,
                "tmdate": 1700613851799,
                "mdate": 1700613851799,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y5ApzvDAsx",
                "forum": "MpyFAhH9CK",
                "replyto": "s34gjxz7y5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "content": {
                    "comment": {
                        "value": "> Due to such fundamental differences in the problem, many of the suggested co-design baselines such as Bayesian Optimization and Genetic Algorithm [2] cannot be used on reconfigurable robots, as we are **controlling** morphological changes by muscles rather than **searching for** a mophology.\n\nI'm confused about the essential differences between controlling morphological changes and searching for a mophology.\n\nMy understanding of **controlling** morphological changes is continuously searching for optimal morphologies. Can the former one be considered as a continuous loop of the latter one?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614671473,
                "cdate": 1700614671473,
                "tmdate": 1700614671473,
                "mdate": 1700614671473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VMOn7MbSik",
                "forum": "MpyFAhH9CK",
                "replyto": "Dr2RpqjZcm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nWe will try our best to include these results.\n\nWe really appreciate the time you spent on understanding our work. If you don't feel comfortable enough raising the scores, please consider raise your confidence for our clarifications!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616659991,
                "cdate": 1700616659991,
                "tmdate": 1700616690611,
                "mdate": 1700616690611,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6wtnWbZuy2",
                "forum": "MpyFAhH9CK",
                "replyto": "VMOn7MbSik",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2963/Reviewer_5Sv3"
                ],
                "content": {
                    "comment": {
                        "value": "All my concerns are addressed.\nThe remaining two comparisons mentioned above will be helpful and appreciated.\n\nI maintain my scores as Rating: 6 and Confidence: 2."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2963/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617642095,
                "cdate": 1700617642095,
                "tmdate": 1700617642095,
                "mdate": 1700617642095,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]