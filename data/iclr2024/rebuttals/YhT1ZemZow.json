[
    {
        "title": "Sobolev acceleration for neural networks"
    },
    {
        "review": {
            "id": "FBP7y4ZPXm",
            "forum": "YhT1ZemZow",
            "replyto": "YhT1ZemZow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1589/Reviewer_BYTD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1589/Reviewer_BYTD"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the effect of Sobolev acceleration for neural networks, focusing on the case of a ReLU network in the teacher student setting. They theoretically show that replacing the L2 loss by the corresponding Sobolev (H1 or H2) loss can increase of the rate of convergence. They consider the use of Chebyshev spectral differentiation to approximate the derivatives in the case where one does not have access to an analytic form. They also provide experiments which show that in a variety of settings, training using the Sobolev norm leads to faster and better convergence of the MSE loss."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The experimental evidence for the speed up of Sobolev training seems quite strong\n- In the experiments they also consider a setting beyond the teacher student setup, where the goal is to learn a general function the derivative of which is then approximated using Chebyshev spectral differentiation.\n- The high-level presentation of their work is clear"
                },
                "weaknesses": {
                    "value": "The theoretical analysis seems a bit shallow, see also questions"
                },
                "questions": {
                    "value": "- In both theorem 2 and 3 it is written that  the convergence $\\omega \\to \\omega^*$ is accelerated. How is this statement formally defined?\n- Related to the above question, the bound on the derivative of $V$ in theorem 2 and 3 only shows that Sobolev training is at least as good as L2 training, but it is not clear how to derive any concrete quantitative bound from this result.\n- In the beginning of section 4 it is stated that $\\omega^*$ and $e$ are sampled randomly, what distributions are they sampled from?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1589/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1589/Reviewer_BYTD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698414615104,
            "cdate": 1698414615104,
            "tmdate": 1699636087574,
            "mdate": 1699636087574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z0KmhjrRJh",
                "forum": "YhT1ZemZow",
                "replyto": "FBP7y4ZPXm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer BYTD."
                    },
                    "comment": {
                        "value": "We appreciate your valuable comments.\n\nWe will address your questions one by one:\n\n1. In Theorem 2 and 3, $w\\to w^\\star$ is defined by $V(w) = \\Vert w-w^\\star \\Vert_2^2 \\to 0$ as $w$ follows the gradient flow $\\dot{w} = -\\nabla_w \\mathcal{H}$. We apologize for the possible misleading. \n\n2. Regarding Theorem 2, there was a typo in our original statement. The correct version is: \n$\\frac{dV}{dt} = -(w-w^*)^T \\nabla_w \\mathcal{H} < -(w-w^*)^T \\nabla_w \\mathcal{L} <0,$ demonstrating that Sobolev training ensures strictly faster convergence.  \n\n3. For the sampling process, we generated $w^\\star$ from the standard normal distribution and $e$ from the uniform distribution. We have updated our manuscript accordingly (refer to Section 4.1).\n\nWe are open to further discussion. Please leave comments if you have any. Thank you."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699846660976,
                "cdate": 1699846660976,
                "tmdate": 1699846660976,
                "mdate": 1699846660976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zr4VNApgkF",
            "forum": "YhT1ZemZow",
            "replyto": "YhT1ZemZow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1589/Reviewer_ePTL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1589/Reviewer_ePTL"
            ],
            "content": {
                "summary": {
                    "value": "This study investigates Sobolev training for neural networks, demonstrating its ability to expedite convergence and reduce test errors in comparison to traditional training methods, particularly for rectified linear unit (ReLU) networks within the student\u2013teacher framework. By leveraging the analytical formula for population gradients of ReLU networks and extending findings through numerical examples to various network architectures, the paper enhances the understanding of ReLU network dynamics. Additionally, it introduces Chebyshev spectral differentiation as a solution for approximating target derivatives, addressing previous challenges and underscoring the convergence benefits, or \"Sobolev acceleration,\" of this training approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Sobolev training is an interesting topic in recent years. Authors theoretically and empirically  showed the acceleration effect for a specific NN. \nIt's interesting to see authors proposed to approximate target function derivatives by using Chebyshev spectral differentiation"
                },
                "weaknesses": {
                    "value": "The works is restricted to a relatively simple architecture. \nI am not sure its practical usage for other communities."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819882244,
            "cdate": 1698819882244,
            "tmdate": 1699636087505,
            "mdate": 1699636087505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7lhfD69Py9",
                "forum": "YhT1ZemZow",
                "replyto": "Zr4VNApgkF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ePTL."
                    },
                    "comment": {
                        "value": "We appreciate your kind consideration of our manuscript. \n\nWe have revised our manuscript to address the weakness that the original manuscript is restricted to a relatively simple architecture.\nWe have revised our manuscript to include the experimental results of the denoising autoencoder task with CNN applied to the MNIST dataset. Please refer to the revised manuscript and the official comment we provided.\n\nWe are open to further discussion. Please leave comments if you have any.\nThank you."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699845820705,
                "cdate": 1699845820705,
                "tmdate": 1699845820705,
                "mdate": 1699845820705,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZwkgqOZSnC",
            "forum": "YhT1ZemZow",
            "replyto": "YhT1ZemZow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1589/Reviewer_cgse"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1589/Reviewer_cgse"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies empirically, the effects of sobolev acceleration on training neural networks. It has some theoretical guarantees on very specific usecases, and has some experimental evidence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is quite interesting, and it has promise in training neural networks. It also studies various different non-linearities."
                },
                "weaknesses": {
                    "value": "The paper studies, both theoretically and empirically, very niche use-cases, which do not represent real architectures or real datasets. This dramatically decreases the effect of the work, as it is not clear these results mirror the true reality of what goes on in deep neural networks."
                },
                "questions": {
                    "value": "Could the authors provide results on more \"real\" dataset, e.g. CIFAR10, CIFAR100, Imagenet? Using modern architectures such as ConvNext, Resnet, RegNet, ViT? this will make the paper much stronger."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829213499,
            "cdate": 1698829213499,
            "tmdate": 1699636087419,
            "mdate": 1699636087419,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LEPvjHHmBz",
                "forum": "YhT1ZemZow",
                "replyto": "ZwkgqOZSnC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cgse."
                    },
                    "comment": {
                        "value": "We appreciate your valuable comments.\n\nWe agree that our original manuscript lacks experimental results demonstrating practical usage.\nThus, we have revised our manuscript to include the experimental results of the denoising autoencoder task with CNN applied to the MNIST dataset. \nPlease refer to the revised manuscript and the official comment we provided. \n\nWe are open to further discussion. Please leave comments if you have any.\nThank you."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699845320388,
                "cdate": 1699845320388,
                "tmdate": 1699845794230,
                "mdate": 1699845794230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SMzEcQi31R",
            "forum": "YhT1ZemZow",
            "replyto": "YhT1ZemZow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1589/Reviewer_dcHW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1589/Reviewer_dcHW"
            ],
            "content": {
                "summary": {
                    "value": "This article presents a theoretical study of Sobolev Training, an alternative to least-squares fitting that replaces an $\\mathcal{L}^2$ fitting error with $\\mathcal{H}^p$ fitting error, for some $p > 0$. This method fits the target function by not only matching its values on the training data, but also matching higher order derivatives, leading to improved convergence speed. \n\nTo understand this effect, the authors study a toy model for training neural networks. They focus on one layer, single node RELU and RELU^2 networks, and they study the dynamics of model parameters under gradient flow-based minimization of the Sobolev loss. The Jacobian of the Sobolev loss has additional terms which are positive semidefinite when $w_0$ is sufficiently close to $w^*$, leading to (potentially) improved convergence speeds. These additional terms are not surprising, since the Sobolev loss can be thought of as a sum of $\\mathcal{L}^2$ loss and additional terms measuring the $\\mathcal{L}^2$ mismatch of gradients, which improve the convergence speed of gradient flow. \n\nFinally, to approximate Sobolev Training in practice, the authors propose to use Chebyshev spectral differentiation, a popular numerical differentiation method. Across a variety of toy experiments, Sobolev Training is shown to accelerate convergence of GD and SGD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Simple idea and analysis: the idea behind Sobolev training is clearly presented and easy to understand. The authors identify a simple and straightforward theoretical explanation for empirical observations that Sobolev training improves convergence speed. \n- Clear and correct proofs: the proofs in this work are presented clearly and they are correct to the best of my knowledge."
                },
                "weaknesses": {
                    "value": "- Unfair/vacuous comparison: it is unfair to compare the convergence of gradient flow on $\\mathcal{H}(w)$ and on $\\mathcal{L}(w)$, since $\\mathcal{H}(w)$ has extra terms that are added in an unnormalized way. To understand why this is unfair, consider another (trivial) way to accelerate the convergence speed of gradient flow, by rescaling the loss.  Set $\\tilde{\\mathcal{L}}(w) = \\mathcal{L}(w) + \\mathcal{L}(w)$, then in the notation of Theorem 1, $\\partial \\tilde{V} / \\partial t = -2 (w - w^*)^T \\nabla_w \\mathcal{L} \\leq 2 \\partial V / \\partial t < 0$, which is already a significantly stronger 'acceleration' than Theorems 2 and 3 because it converges twice as fast (whereas Theorem 2 and 3 show only convergence that is not any slower than Theorem 1). Simply adding extra convex terms to the loss isn't sufficient to argue an acceleration phenomena, any more than rescaling the loss would be. It would be more fair to average the terms in the Sobolev loss, or to rescale so that $\\mathcal{L}(0) = \\mathcal{H}(0)$, but then Theorems 2 and 3 might not hold.\n- Synthetic/toy experiments: the experimental evaluation in this work is entirely focused training toy objectives. While the Sobolev loss is shown to improve convergence on these objectives, it's not clear whether this benefit extends to actual datasets."
                },
                "questions": {
                    "value": "In Figure 3, Sobolev training seems to induce training instability in the form of large loss spikes during late training. Does the Sobolev training loss have higher variance over random initializations of GD? It would be helpful to add error bars to Figures 3, 4, and 5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699302691840,
            "cdate": 1699302691840,
            "tmdate": 1699636087335,
            "mdate": 1699636087335,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LYHGDoetBC",
                "forum": "YhT1ZemZow",
                "replyto": "SMzEcQi31R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1589/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer dcHW."
                    },
                    "comment": {
                        "value": "We appreciate your time dedicated to reviewing our work and valuable comments.\n\nWe acknowledge that one may perceive the theoretical results as suggesting an unfair comparison. However, our primary objective in presenting the theorems is to highlight the convergence acceleration of Sobolev training in the $H^1$-norm compared to $L^2$-training. It's worth noting that the specific example $\\tilde{\\mathcal{L}}(w) = 2\\mathcal{L}(w)$  you mentioned may seem to accelerate convergence, but this effect arises from the infinitesimal learning rate assumption and the student-teacher assumption in theoretical settings. Moreover, comparing $L^2$ and $H^1$ loss functions is a standard practice widely used in the literature [1,2].\n\nIn summary, our focus is on demonstrating the acceleration of $H^1$-training rather than suggesting a new loss function (such as $\\tilde{\\mathcal{L}}(w)$) for faster convergence. We hope you understand our intent.\n\nIn regards to experiments, we have revised our manuscript to include more practical experiments. Please refer to the updated manuscript and the official comment we have provided.\n\nWe are open to further discussion. Please leave comments if you have any.\nThank you.\n\n\n[1] Wojciech M Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz Swirszcz, and Razvan Pascanu. Sobolev training for neural networks. Advances in neural information processing systems, 30, 2017.\n\n[2] Hwijae Son, Jin Woo Jang, Woo Jin Han, and Hyung Ju Hwang. Sobolev training for physics-informed neural networks. Communications in Mathematical Sciences, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699844977963,
                "cdate": 1699844977963,
                "tmdate": 1699845664131,
                "mdate": 1699845664131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P18T5Sb5wD",
                "forum": "YhT1ZemZow",
                "replyto": "LYHGDoetBC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1589/Reviewer_dcHW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1589/Reviewer_dcHW"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to answer my questions and to add experiments. I appreciate the clarification in your intent behind writing the paper. I feel that my score remains an accurate assessment of this work and I am not currently planning to change it."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152784011,
                "cdate": 1700152784011,
                "tmdate": 1700152784011,
                "mdate": 1700152784011,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]