[
    {
        "title": "MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data"
    },
    {
        "review": {
            "id": "58DBldWKtg",
            "forum": "8xliOUg9EW",
            "replyto": "8xliOUg9EW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2311/Reviewer_Mb77"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2311/Reviewer_Mb77"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method, Mustard, for the synthesis of informal and formal theorems and proofs. MUSTARD is then applied to generate MustardSauce, a dataset of seven thousand mathematical datapoints in subjects ranging from elementary school to undergraduate curricula. The authors demonstrated that language models can improve their informal and formal reasoning abilities by fine-tuning on this large and diverse dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed pipeline that synthesises informal and formal theorems and proofs is conceptually simple.\n- The resulting dataset is diverse and contains step-by-step reasoning processes, which are very useful for chain-of-thought reasoning.\n- There is a thorough analysis of the dataset which helps the understanding.\n- It is validated that there are considerable gains to be made by fine-tuning language models on this dataset.\n- The formalising step has validated effectiveness since by a simple filtering of the valid formal statements and proofs, language models can have better performance."
                },
                "weaknesses": {
                    "value": "- Generating data from a language model always has the risk of data contamination. It would improve the confidence in the conclusions if it can be shown that no serious data contamination took place."
                },
                "questions": {
                    "value": "What is the \"valid\" rate of the generated formal statements and proofs? I can't seem to find it in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697893911996,
            "cdate": 1697893911996,
            "tmdate": 1699636163605,
            "mdate": 1699636163605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2ODRf2938X",
                "forum": "8xliOUg9EW",
                "replyto": "58DBldWKtg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Mb77"
                    },
                    "comment": {
                        "value": "**R3Q1. The risk of data contamination.** \n\nThank you for the valuable question. \nWe check cross-contamination between MUSTARDSAUCE and the evaluation datasets inspired by the GPT-4 Technical Report [1].\nHowever, instead of using a substring match that may result in false negatives and false positives, we compute cosine similarities based on text-embedding-ada-002 [3] and then inspect the nearest data points in the paired datasets. \nThe automated theorem proving (ATP) dataset miniF2F only contains formal statements and proofs, while the math word problem (MWP) dataset GSM8K only contains informal statements and proofs. \nFor a more detailed inspection, we check the corresponding fractions in MUSTARDSAUCE. \nWe demonstrate the inspected cases as follows. \nThe nearest data points are dissimilar. \nTherefore, we exclude data contamination of the generated MUSTARDSAUCE according to these observations.\n\n[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\n\n[3] Greene, R., Sanders, T., Weng, L., & Neelakantan, A. (2022). New and improved embedding model. https://openai.com/blog/new-and-improved-embedding-model\n\n\n\n> **MUSTARDSAUCE v.s. miniF2F (cosine similarity = 0.6439)**\n> \n> **MUSTARDSAUCE Case** \n>\n> Informal Statement: \n> Alex has 5 ten-dollar bills and 3 one-dollar bills. How much money does Alex have in total? \n> \n> Informal Proof: \n> To find out how much money Alex has in total, we need to multiply the number of each type of bill by its value. So, Alex has 5 ten-dollar bills, which equals 5 * 10 = 50 dollars. He also has 3 one-dollar bills, which equals 3 * 1 = 3 dollars. Adding these two amounts together gives 50 + 3 = 53 dollars. Therefore, Alex has 53 dollars in total. \n> \n> Formal Statement and Proof: \n> ```\n> def calculate_money (tens : \u2115) (ones : \u2115) : \u2115 := tens * 10 + ones * 1\n> \n> example : calculate_money 5 3 = 53 :=\n> begin\n>   rw calculate_money,\n>   refl,\n> end\n> ```\n>\n>\n> **miniF2F Case** \n> ```\n> theorem algebra_sqineq_unitcircatbpamblt1\n>   (a b: \u211d)\n>   (h\u2080 : a^2 + b^2 = 1) :\n>   a * b + (a - b) \u2264 1 :=\n> begin\n>   nlinarith [sq_nonneg (a - b)],\n> end\n> ```\n\n\n\n> **MUSTARDSAUCE v.s. GSM8K (cosine similarity = 0.5975)** \n> \n> **MUSTARDSAUCE Case** \n> \n> Informal Statement: \n> Given two vectors u = (u1,u2) and v = (v1,v2) in R^2, prove that the operation of vector addition, defined as u + v = (u1+v1, u2+v2), is commutative. That is, prove that for all u and v in R^2, u + v = v + u. \n> \n> Informal Proof: \n> We can prove this by direct computation. Given the vectors u = (u1, u2) and v = (v1, v2), we have: \n> u + v = (u1 + v1, u2 + v2) \n> and\n> v + u = (v1 + u1, v2 + u2). \n> Since addition is commutative in R (the set of all real numbers), we have u1 + v1 = v1 + u1 and u2 + v2 = v2 + u2. Thus, we conclude that (u1 + v1, u2 + v2) = (v1 + u1, v2 + u2), which means that u + v = v + u. \n> \n> Formal Statement and Proof:\n> ```\n> import data.real.basic \n> \n> variables (u1 u2 v1 v2 : \u211d)\n> \n> -- defining vectors as pair of real numbers\n> def vector := \u211d \u00d7 \u211d\n> -- defining vector addition\n> def vadd (u v : vector) : vector := (u.1 + v.1, u.2 + v.2)\n> \n> -- defining vectors u and v\n> def u : vector := (u1, u2)\n> def v : vector := (v1, v2)\n> \n> -- commutativity of vector addition\n> theorem vadd_comm : vadd (u u1 u2) (v v1 v2) = vadd (v v1 v2) (u u1 u2) :=\n> begin\n>   unfold vadd,\n>   unfold u,\n>   unfold v,\n>   rw add_comm u1 v1,\n>   rw add_comm u2 v2,\n> end\n> ```\n> **GSM8K Case** \n>\n> Question: The local firefighters are doing a \"fill the boot\" fundraiser. Their goal is to raise \\\\$6300. \n> After the first 3 hours, they have raised \n> \\\\$2100. For how many hours do they have to fundraise in total to reach their goal, assuming an equal amount raised in every hour? \n> \n> Answer: \n> The fireman raise 2100 / 3 = \\\\$<<2100/3=700>>700 per hour. \n> They have to fundraise for 6300 / 700 = <<6300/700=9>>9 hours. \n> \n> \\#\\#\\#\\# 9 \n\n\n\n\n\n**R3Q2. What is the \"valid\" rate of the generated formal statements and proofs?** \n\nThank you for the question. The \"valid\" rate, or the \"pass rate\", is the proportion of generated data (obtained from the 2nd stage of MUSTARD: Proof Generation) that are validated by the Lean Prover (obtained from the 3rd stage of MUSTARD: Proof Filtering)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700342740080,
                "cdate": 1700342740080,
                "tmdate": 1700342740080,
                "mdate": 1700342740080,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mac75ETt1L",
                "forum": "8xliOUg9EW",
                "replyto": "DVwu465yZn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Reviewer_Mb77"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Reviewer_Mb77"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the data contamination analysis. My confidence in the soundness improves."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392813758,
                "cdate": 1700392813758,
                "tmdate": 1700392813758,
                "mdate": 1700392813758,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5e87swgM13",
            "forum": "8xliOUg9EW",
            "replyto": "8xliOUg9EW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2311/Reviewer_9Wae"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2311/Reviewer_9Wae"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework called MUSTARD that can automatically generate natural language mathematical problems, reasoning steps, and corresponding formalized language (Lean) versions through the interaction of LLM and theorem provers. The experimental results show that the 7,335 data points generated by MUSTARD can improve the performance of Llama 2-7B and GPT2 on the GSM8k and Mathlib datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The topic of this paper is of great significance to the mathematical reasoning of large language models, and how to produce high-quality reasoning step data at low cost is indeed one of the directions the community is working towards. This paper offers certain reference value for this field.\n- The writing of the paper is enjoyable, featuring a clear method description, and the experiment is easy to follow as well.\n- Technically, the paper presents several innovations. For example, this paper combines natural language generation and formalized language verification, using theorem provers to verify whether the generated natural language reasoning steps are valid.\n- The experimental data in this paper effectively demonstrate the value of the data generated by this framework. The evaluation section analyzes two models, multiple fine-tuning strategies, and training data, showcasing rich results."
                },
                "weaknesses": {
                    "value": "The paper has some limitations:\n- The improvement of involving the theorem prover is relatively minor (e.g., see $\\texttt{in}$, $\\texttt{ra}$, and $\\texttt{va}$ in Table 4), and it raises the question of whether the best result should be derived from direct fine-tuning without any filtering and sampling.\n- The subset sampled for manual verification is small (only 88 data points). Given the variance in the dataset due to the different performance of the theorem prover on various types of math problems, the results on such a small dataset may not accurately represent the statistical characteristics of the whole dataset.\n\nAdditionally, there are some minor issues in the paper:\n- It is unclear to me which model is used for generating the MUSTARD dataset. It would be helpful if the authors provided the performance of the LLM used for data generation as a reference.\n- The scalability of the framework is still unknown. Only 7,335 data points are generated, which is a small token size for LLM fine-tuning, and the results after fine-tuning are mediocre. Furthermore, the effectiveness of this framework for larger LLM models (e.g., 34B or 70B models or GPT3 models) remains to be investigated.\n- Since both natural language problem-solving and autoformalization are generated by LLM, and only ATP is used to verify the entire scheme, it is unclear how the authors handle inconsistencies between the final result calculated by the #reduce statement (as shown in Figure 2) and the natural language. The filtering strategy in Table 13 does not cover this situation (as ATP will not report an error).\n- There seems to be a significant misalignment between the generated 7,335 data points and the test set. For instance, the paper showcases a problem about the convergence of alternating series, which is within the scope of undergraduate mathematics, while the test set GSM8k only covers elementary school-level problems. Additionally, the paper mentions using the #reduce statement in Lean, which is an approach for math word problems but is not consistent with the style of various theorem proofs in Mathlib."
                },
                "questions": {
                    "value": "1. Which LLM is used for data generation? \n\n2. Does the method remain more effective when compared to fine-tuning the model using the entire generated dataset?\n\n2. Would the method potentially be more effective if more data points were generated and a larger model was employed for fine-tuning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2311/Reviewer_9Wae",
                        "ICLR.cc/2024/Conference/Submission2311/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698375514116,
            "cdate": 1698375514116,
            "tmdate": 1700444809101,
            "mdate": 1700444809101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IGNABawFKX",
                "forum": "8xliOUg9EW",
                "replyto": "5e87swgM13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Wae"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for all the instructive comments and valuable suggestions. We have added the following experiments to address the corresponding questions. \n\n1. We have further conducted experiments with (1) fine-tuning in the entire generated dataset (denoted as \"total\", <tt>tt</tt> for short), (2) fine-tuning larger Llama 2-70B, (3) evaluation on another challenging dataset MATH [2], (4) ablation study of data scalability by fine-tuning models with different ratios of data,\nto further study the benefits of the proposed framework. We have the following findings: \n    \n    1) In general, fine-tuning with MUSTARDSAUCE-valid is superior to fine-tuning with MUSTARDSAUCE-invalid, MUSTARDSAUCE-random, and also the entire generated dataset. This indicates the benefits of involving formal validation in our framework. \n\n    2) The performance gaps between MUSTARDSAUCE-valid and MUSTARDSAUCE-random are generally significant. The average performance gain in automated theorem proving (Table 5) is 18.15\\%, and in math word problems (Table 4) is 6.78\\%. The results on the MATH dataset indicate that fine-tuning with MUSTARDSAUCE-valid is considerably beneficial. For example, GPT2-large fine-tuned with MUSTARDSAUCE-valid set has a 40\\% performance gain in a zero-shot manner, and the performance of Llama 2-7B in a few-shot manner increases by 11.76\\%. \n\n    3) Fine-tuning the larger Llama 2-70B shows greater performance improvements, which demonstrates the scalability of this framework.\n\n    4) The model performances increase as the generated data scales up. Therefore we expect further performance improvements when more high-quality data are included.  \n\n    In summary, the extended experiments demonstrate that the proposed method generates high-quality data that are beneficial to improving language models' mathematical reasoning performance. The method is scalable to larger fine-tuned LMs and more generated data points and maintains its effectiveness. \n\n2. We have expanded the sampled subset for manual verification from 88 data points to 200 data points.\n\n3. The LLM we used for generating the MUSTARDSAUCE dataset is GPT-4 [1] and we have added the details in our revised manuscript.\n\n[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\n\n[2] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021.\n\nPlease check our detailed results and response in the following."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340575340,
                "cdate": 1700340575340,
                "tmdate": 1700340575340,
                "mdate": 1700340575340,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8izd02I2nO",
                "forum": "8xliOUg9EW",
                "replyto": "5e87swgM13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Updated Experimental Results: Table 4"
                    },
                    "comment": {
                        "value": "\\\nTable 4: Maj1@1 results on GSM8K (G) and MATH (M). Zero: Zero-shot. Few: Few-shot. > denotes a fine-tuning step. in: MUSTARDSAUCE-invalid. ra: MUSTARDSAUCE-random. va: MUSTARDSAUCE-valid. tt: The entire generated data. \ngt: GSM8K training split. \nThe updated results are ***bold and italic***.\nThe values in parentheses are performance gaps between fine-tuning with MUSTARDSAUCE-valid and MUSTARDSAUCE-random.\n\n| MODEL                   | Zero (G)     | Few (G)       | Zero (M)      | Few (M)       | MODEL                 | Zero (G)      | Few (G)       | Zero (M)      | Few (M)       |\n| :---------------        | :----------- | :------------ | :------------ | :------------ | :-------------------- | :------------ | :------------ | :------------ | :------------ |\n| *Baselines*             |              |               |               |               |                       |               |               |               |               |\n| GPT2-large              | 3.4          | 5.1           | ***0.6***           | ***1.0***           | GPT2-large > gt       | 14.6          | 17.4          | ***4.6***           | ***6.8***           |\n| Llama 2-7B              | 7.2          | 12.8          | ***2.0***           | ***2.6***           | Llama 2-7B > gt       | 24.5          | 28.2          | ***10.4***          | ***12.6***          |\n| *Fine-tuning*           |              |               |               |               |                       |               |               |               |               |\n| ***GPT2-large > tt***   | ***3.9***    | ***6.3***     | ***1.2***     | ***2.0***     | ***GPT2-large > tt > gt***  | ***15.9***          | ***18.5***          | ***5.0***           | ***7.8***           |\n| GPT2-large > in         | 3.6          | 5.8           | ***1.0***           | ***1.8***           | GPT2-large > in > gt  | 14.6          | 17.9          | ***4.8***           | ***7.4***           |\n| GPT2-large > ra         | 3.8          | 6.3           | ***1.0***           | ***2.0***           | GPT2-large > ra > gt  | 15.8          | 18.4          | ***4.8***           | ***7.6***           |\n| GPT2-large > va         | 4.1  ***(+7.89%)*** | 6.1  ***(-3.17%)***  | ***1.4 (+40.00%)*** |   ***2.2 (+10.00%)*** | GPT2-large > va > gt  | 16.0  ***(+1.27%)*** | 18.7  ***(+1.63%)*** |  ***5.2 (+8.33%)***  |  ***7.8 (+2.63%)***  |\n| ***Llama 2-7B > tt***   | ***9.0***    | ***15.5***    | ***3.0***     | ***3.4***     | ***Llama 2-7B > tt > gt***  | ***26.1***           | ***30.2***           | ***11.8***          | ***13.8***          |\n| Llama 2-7B > in         | 8.3          | 14.4          | ***2.4***           | ***3.2***           | Llama 2-7B > in > gt  | 25.4          | 28.2          | ***10.8***          | ***12.8***          |\n| Llama 2-7B > ra         | 8.9          | 14.9          | ***2.8***           | ***3.4***           | Llama 2-7B > ra > gt  | 26.1          | 29.9          | ***11.6***          | ***13.6***          |\n| Llama 2-7B > va         | 9.1  ***(+2.25%)*** | 15.7  ***(+5.37%)*** |  ***3.0 (+7.14%)***  |  ***3.8 (+11.76%)*** | Llama 2-7B > va > gt  | 26.3  ***(+0.77%)*** | 30.8  ***(+3.01%)*** |  ***12.2 (+5.17%)*** | ***14.2 (+4.41%)*** |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340771684,
                "cdate": 1700340771684,
                "tmdate": 1700343625662,
                "mdate": 1700343625662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7mLT4HNw7l",
                "forum": "8xliOUg9EW",
                "replyto": "5e87swgM13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to R2Q1, R2Q2, R2Q3"
                    },
                    "comment": {
                        "value": "**R2Q1. The improvement of involving the theorem prover is relatively minor (e.g., see in, ra, and va in Table 4), and it raises the question of whether the best result should be derived from direct fine-tuning without any filtering and sampling.**\n\nThank you for the question. \nWe have further conducted experiments with fine-tuning in the entire generated dataset (28,320 data points, denoted as \u201ctotal\u201d, tt for short). \nThe results are demonstrated in the revised Tables 4 and 5. \nWe also mark and color code the performance gaps between models fine-tuned with MUSTARDSAUCE-random and MUSTARDSAUCE-valid in the tables. \nWe find that models fine-tuned with the entire generated data are inferior to models fine-tuned with MUSTARDSAUCE-valid.\nAlthough the increase in the amount of fine-tuned data makes the model perform better compared to fine-tuning on MUSTARDSAUCE-invalid and MUSTARDSAUCE-random, the model's performance still lags behind that of fine-tuning on smaller amounts but higher quality data.\nTherefore, our proposed framework that introduces the theorem prover is effective and beneficial. \n\n**R2Q2. The subset sampled for manual verification is small.**\n\nThank you for the valuable comments. We expand the manual verification subset by randomly selecting and manually checking another 112 data points. \nThe results of the 200 data points in total are demonstrated in Table 3 as follows.\nIt is shown that the results are consistent with more inspected data. \n(D4) and (D6) show significant differences in accuracy between the two groups.\nMoreover, (D1) also shows significance with the inspected data scaled up.\nThe results demonstrate that the validated data have a higher quality than those without a formal validation process.\n\nTable 3: Inspection dimensions and requirements in human evaluation. IS: Informal Statement. IP: Informal Proof. FS: Formal Statement. FP: Formal Proof. RT: Reasoning Type. \nSignificant $p<0.005$ are marked with **bold**.\n\n| Inspection Dimension | Requirement | Valid | Invalid | *p*-value | \n| ------- | ------- | ------- | ------- | ------- |\n| (D1) IS Correctness | *Whether the informal statement is factually correct.* | 93.50 | 83.50 | **0.00167** |\n| (D2) IS Relevance | *Whether the informal statement is relevant to each seed concept.* | 87.50 | 92.50 | 0.09604 |\n| (D3) RT Classification | *Whether the informal statement is of the required question type.* |67.00 | 68.50 | 0.74903 |\n| (D4) IP Correctness | *Whether the informal proof correctly solves the informal statement.* | 88.50 | 73.50 | **0.00012** |\n| (D5) IS-FS Alignment | *Whether the informal statement and the formal statement describe the same problem and are aligned with each other.* | 74.00 | 66.50 | 0.10138 |\n| (D6) IP-FP Alignment | *Whether the informal proof and the formal proof describe the same solution and have aligned proof steps.* |72.00 | 54.00 | **0.00018** |\n\n**R2Q3. The model used for generating the MUSTARD dataset.** \n\nWe use GPT-4 [1] for generating the MUSTARD dataset. We have added the details to our revised version.\n\n[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340901268,
                "cdate": 1700340901268,
                "tmdate": 1700340901268,
                "mdate": 1700340901268,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bzgFbdusHD",
                "forum": "8xliOUg9EW",
                "replyto": "5e87swgM13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to R2Q4, R2Q5, R2Q6"
                    },
                    "comment": {
                        "value": "**R2Q4. The effectiveness of this framework for larger LLM models (e.g., 34B or 70B models or GPT3 models) remains to be investigated.** \n\nThank you for the instructive comment. We further conduct experiments on Llama 2-70B. \nSince the Llama 2-34B and the GPT-3 parameters are not released, we are unable to experiment with these two models. \nThe compared performances between Llama 2-7B and Llama 2-70B are demonstrated in Table 8 below. The results are also updated in Table 28 in Appendix F in the main paper. \nDue to time limitations, we demonstrate results on GSM8K first.\nLlama 2-70B fine-tuned with MUSTARDSAUCE-valid performs 9.42\\% higher than fine-tuned with MUSTARDSAUCE-random in the zero-shot manner, and 6.68\\% higher in the few-shot manner. \nThe performance gains are more significant compared to the results of Llama 2-7B.\nTherefore, the proposed framework remains effective with larger fine-tuned language models. \n    \nTable 8: Compared performances on GSM8K between Llama 2-7B and Llama 2-70B.\n\n| MODEL            | Zero (G)                                       | Few (G)                                        |\n| ---------------- | ---------------------------------------------- | ---------------------------------------------- |\n| *Baselines*      |                                                |                                                |\n| Llama 2-7B       | 7.2                                            | 12.8                                           |\n| Llama 2-70B      | 31.7                                           | 54.1                                           |\n| *Fine-tuning*    |                                                |                                                |\n| Llama 2-7B > tt  | 9.0                                            | 15.5                                           |\n| Llama 2-7B > in  | 8.3                                            | 14.4                                           |\n| Llama 2-7B > ra  | 8.9                                            | 14.9                                           |\n| Llama 2-7B > va  | 9.1 (+2.25%)  | 15.7 (+5.37%) |\n| Llama 2-70B > tt | 36.6                                           | 55.8                                           |\n| Llama 2-70B > in | 33.4                                           | 53.7                                           |\n| Llama 2-70B > ra | 36.1                                           | 55.4                                           |\n| Llama 2-70B > va | 39.5 (+9.42%) | 59.1 (+6.68%) |\n\n\n\n**R2Q5. Would the method potentially be more effective if more data points were generated and a larger model was employed for fine-tuning?** \n\nThank you for the constructive comment. \nSince generating and filtering new data with GPT-4 is costly, we instead use 75%, 50%, 25%, and 0% of current MUSTARDSAUCE-valid for fine-tuning Llama 2-7b to observe the effect of data scalability. \nWe evaluate the model on all datasets. \nThe results are demonstrated in Table 9 as follows. \nWe have also uploaded the results in Figure 3 in the main paper. \nIn general, the results on all datasets increase as the fine-tuning data scales up. \nSpecifically, performances on the MUSTARD-test and mathlib show the most significant growth, and there is no decreasing trend in the growth rate.\nTherefore we expect further performance improvements when more high-quality data are included. \n\n\nTable 9: Llama 2-7b performances with different data scales for fine-tuning. \n\n| Data Scale | GSM8K | MATH | mathlib | miniF2F | test |\n| :--------- | :---- | :--- | :------ | :------ | :--- |\n| 0%         | 12.8  | 2.6  | 0.0     | 0.0     | 0.0  |\n| 25%        | 13.4  | 2.8  | 3.9     | 0.9     | 4.3  |\n| 50%        | 14.2  | 3.1  | 5.3     | 1.4     | 7.5  |\n| 75%        | 14.9  | 3.6  | 7.4     | 2.0     | 9.4  |\n| 100%       | 15.7  | 3.8  | 8.7     | 2.9     | 12.2 |\n\n\n\n\n**R2Q6. Does the method remain more effective when compared to fine-tuning the model using the entire generated dataset?** \n\nThank you for the valuable comment. We have fine-tuned GPT2-large and Llama 2-7B with the entire generation dataset (contains 28,320 data points in total), and the results are demonstrated in Table 4 and Table 5 in the lines \"GPT2-large > tt\" and \"Llama 2-7B > tt\". \nWe have also updated the results and marked them with blue in Tables 4 and 5 in the main paper.\nWe find that the models fine-tuned with the entire generated dataset perform better than those fine-tuned with MUSTARDSAUCE-random and MUSTARDSAUCE-invalid but are still inferior to those fine-tuned with MUSTARDSAUCE-valid. \nTherefore, more fine-tuning data improves the model performances as usual, but the smaller MUSTARDSAUCE-valid turns out to be more helpful. \nThis finding again demonstrates our motivation that the validated theorem and proof data are of higher quality and are beneficial for reasoning."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341219172,
                "cdate": 1700341219172,
                "tmdate": 1700737466900,
                "mdate": 1700737466900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CO4IIJqUyc",
                "forum": "8xliOUg9EW",
                "replyto": "5e87swgM13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to R2Q7, R2Q8"
                    },
                    "comment": {
                        "value": "**R2Q7. The inconsistency between the #reduce statements and various theorem proofs.** \n\nThank you for the question. \nThe proportion of proofs with #reduce statements in the current MUSTARDSAUCE-valid set is 19.3%. \nMUSTARD aims to provide a uniform synthesis of theorem and proof data. \nTherefore, we have further filtered out the data points with the #reduce statements, which results in 5,922 data points. \n\n \n\n**R2Q8. The test set GSM8K only covers elementary school-level problems.** \n\nThank you for the valuable question. We have added experiments on the MATH dataset [2] that contain more challenging math word problems, and the results are demonstrated in Table 4 as shown above. \nBoth GPT2-large and Llama 2-7B show significant improvements after fine-tuning with MUSTARDSAUCE-valid. For example, \"GPT2-large > va\" performs 40\\% higher than \"GPT2-large > ra\" in the zero-shot manner. And \"Llama 2-7B > va\" has an 11.76\\% increase compared to \"Llama 2-7B > ra\".\nThe observations further demonstrate the effectiveness of generated and validated MUSTARDSAUCE on math word problems.  \n\n[2] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341430815,
                "cdate": 1700341430815,
                "tmdate": 1700341430815,
                "mdate": 1700341430815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "drWRyTWY2F",
                "forum": "8xliOUg9EW",
                "replyto": "IGNABawFKX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Reviewer_9Wae"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Reviewer_9Wae"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thank you for the response. I do believe that these extensive experimental results can greatly enhance the completeness and soundness of the paper. Therefore, I have increased the score of the paper to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700444792260,
                "cdate": 1700444792260,
                "tmdate": 1700444792260,
                "mdate": 1700444792260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2hAns7R9v6",
                "forum": "8xliOUg9EW",
                "replyto": "5e87swgM13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The Contributions of This Paper"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9Wae,\n\nWe sincerely thank you again for your time and effort in reviewing this paper! We also thank you for your positive recognition of our extensive experiments and our contributions, as we quote as follows: \"The topic of this paper is of great significance to the mathematical reasoning of large language models, ... This paper offers certain reference value for this field.\" \"Technically, the paper presents several innovations.\" \"The experimental data in this paper effectively demonstrate the value of the data generated by this framework.\"\n\nOur paper proposes a novel framework MUSTARD that synthesizes high-quality theorem and proof data and further extensively facilitates language models' mathematical reasoning capabilities. \n* The human evaluation results on 200 randomly selected data points indicate that the formal validation in MUSTARD makes a remarkable contribution to the data quality (Table 3). \n* The extensive experimental results with GPT2-large, Llama 2-7B, and Llama 2-70B on the two word math problem benchmarks (GSM8K and MATH, demonstrated in Table 4) and two automated theorem proving benchmarks (mathlib and miniF2F, shown in Table 5) demonstrate that the generated data significantly facilitates the smaller language models to perform more accurate mathematical reasoning. \n* Moreover, the ablation study on data scalability shows consistent performance increases when more data from MUSTARD are introduced, suggesting a great potential for scalability (Table 9 in the previous response and Figure 3 in the revised main paper). Fortunately, MUSTARD reduces the cost of acquiring such high-quality step-by-step complex reasoning data and obtains correct, scalable, and reusable data. Therefore, in future work, we would love to build a community in which all members can join the data synthesis process, and acquire and share more high-quality data with the whole community."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649255558,
                "cdate": 1700649255558,
                "tmdate": 1700733255129,
                "mdate": 1700733255129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rcmS457o3S",
            "forum": "8xliOUg9EW",
            "replyto": "8xliOUg9EW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2311/Reviewer_LURb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2311/Reviewer_LURb"
            ],
            "content": {
                "summary": {
                    "value": "In this submission, the authors consider formal theorem proving, especially the data generation for this task. They present an approach to generate theorem and proof data. Their approach, which they call mustard performs two steps: given a \"concept\" such as \"basic geometry\", problems in natural language are generated with LLMs. In step 2 an informal proof in natural langauge is generated which is then passed iteratively into a formal theorem prover, namely lean, to generate a formal proof by refining.\nThis results in a theorem + proof dataset/benchmark called mustardsauce with 7335 examples including natural language problem statements and proofs and their corresponding formal proof.\nAdditionally, the authors provide experiments on the quality of the benchmark; where especially the experiment on fine-tuning on mustardsauce is interesting, as it significantly improves the reasoning capabilities even of small open-source models significantly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Interesting and relevant research area\n- Experiments on the smaller models are interesting baseline for research and academic community\n- The novel dataset/benchmark is valuable\n- Expert user study is interesting and often not done in this field; interesting experiment that validates the quality of the benchmark"
                },
                "weaknesses": {
                    "value": "- Since the informal data is generated by an off-the-shelf LLM, it is unclear how this approach could be scaled to the best performing models.\n- It was also unclear to me which LLM is used for the informal data generation"
                },
                "questions": {
                    "value": "I thank the authors for their submission!\n- Could you elaborate on the low value of D3 in Table 3, the sentence on that in the paper was unclear to me?\n- Which LLM is used for the informal data generation?\n- If the LLM used is GPT-4 or another large off-the-shelf model, what would be the argument for this approach to help in performance of the best models? (if I misunderstood something, please correct me / give a short clarification)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2311/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2311/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2311/Reviewer_LURb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2311/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820997406,
            "cdate": 1698820997406,
            "tmdate": 1700632296748,
            "mdate": 1700632296748,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e87rIZCJcm",
                "forum": "8xliOUg9EW",
                "replyto": "rcmS457o3S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LURb"
                    },
                    "comment": {
                        "value": "**R1Q1. The LLM we used for the informal data generation.**\n\nWe use GPT-4 [1] for generating both the informal and formal data.\n\n[1] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\n \n\n**R1Q2. How this approach could be scaled to the best performing models.**\n\nThank you for the question. \nThe proposed MUSTARD contains two language models: (1) The large language model (hereinafter the LLM) that is prompt for generating the theorem and proof data, and specifically we use GPT-4 [1] in our experiments; (2) The fine-tuned language model using the generated MUSTARDSAUCE dataset (hereinafter as the fine-tuned LM) for miniF2F, mathlib, and GSM8K, and in our experiments we use GPT-2 and Llama 2. \nFor LLM, we have used the best-performing GPT-4. \nFor fine-tuned LMs, we fine-tune a larger and better-performing model Llama 2-70B with MUSTARDSAUCE.\nThe results are demonstrated in Table 8 as follows. \nIn general, larger fine-tuned LMs achieve more significant performance gains. \nFor example, Llama 2-70B fine-tuned with MUSTARDSAUCE-valid performs 9.42% higher than fine-tuned with MUSTARDSAUCE-invalid in the zero-shot manner, and 6.68% higher in the few-shot manner.\nThe performance gains are higher compared to the results of Llama 2-7B.\nTherefore, the proposed framework remains effective when scaled to better-performing fine-tuned LMs. \n\n    \nTable 8: Compared performances on GSM8K between Llama 2-7B and Llama 2-70B.\n\n| MODEL            | Zero (G)                                       | Few (G)                                        |\n| ---------------- | ---------------------------------------------- | ---------------------------------------------- |\n| *Baselines*      |                                                |                                                |\n| Llama 2-7B       | 7.2                                            | 12.8                                           |\n| Llama 2-70B      | 31.7                                           | 54.1                                           |\n| *Fine-tuning*    |                                                |                                                |\n| Llama 2-7B > tt  | 9.0                                            | 15.5                                           |\n| Llama 2-7B > in  | 8.3                                            | 14.4                                           |\n| Llama 2-7B > ra  | 8.9                                            | 14.9                                           |\n| Llama 2-7B > va  | 9.1 (+2.25%)  | 15.7 (+5.37%) |\n| Llama 2-70B > tt | 36.6                                           | 55.8                                           |\n| Llama 2-70B > in | 33.4                                           | 53.7                                           |\n| Llama 2-70B > ra | 36.1                                           | 55.4                                           |\n| Llama 2-70B > va | 39.5 (+9.42%) | 59.1 (+6.68%) |\n\n\n**R1Q3. The low value of D3 in Table 3.**\n\nAs introduced in Section 3.2, MUSTARD generates both theorem-proving problems and math word problems. D3 in Table 3 inspects whether MUSTARD accurately classifies the question types during generation. \nSpecifically, we investigate instances where theorem-proving problems might be misclassified as math word problems, and vice versa, and assess if validation using Lean Prover reduces such misclassifications.\nWe find that the $p$-value of D3 is greater than 0.005, showing that Lean Prover validation does not significantly influence the classification.\nFurthermore, the proportions in both Group Valid and Group Invalid in D3 are relatively lower than other inspection dimensions. One of the reasons is that MUSTARD generates a diverse range of problems from the elementary to higher educational levels, and generating theorem-proving problems at the elementary school level is specifically challenging."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700340345113,
                "cdate": 1700340345113,
                "tmdate": 1700740116635,
                "mdate": 1700740116635,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KnFvnQhjxH",
                "forum": "8xliOUg9EW",
                "replyto": "rcmS457o3S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Many Thanks and Updated Results for R1Q2"
                    },
                    "comment": {
                        "value": "We thank Reviewer LURb again for your time and effort of reviewing this paper!\n\nWe have updated the results of the fine-tuned Llama 2-70B on the MATH dataset, which are demonstrated in Table 8 as follows and also updated to Table 28 in the main paper. \nLlama 2-70B fine-tuned with MUSTARDSAUCE-valid consistently outperforms the model fine-tuned with MUSTARDSAUCE-random by 8.33\\% in the zero-shot manner and 5.30\\% in the few-shot manner. It also surpasses the model fine-tuned with the invalid subset and the entire generated dataset. \nThe results also suggest the effectiveness of the framework when scaled to better-performing fine-tuned LMs. \n\n\n    \nTable 8: Compared performances on GSM8K and MATH between Llama 2-7B and Llama 2-70B.\n\n| MODEL            | Zero (G)                                       | Few (G)                                        | Zero (M)                                       | Few (M)                                         |\n| ---------------- | ---------------------------------------------- | ---------------------------------------------- | ---------------------------------------------- | ----------------------------------------------- |\n| *Baselines*      |                                                |                                                |                                                |                                                 |\n| Llama 2-7B       | 7.2                                            | 12.8                                           | 2.0                                            | 2.6                                             |\n| Llama 2-70B      | 31.7                                           | 54.1                                           | 8.8                                            | 13.4                                            |\n| *Fine-tuning*    |                                                |                                                |                                                |                                                 |\n| Llama 2-7B > tt  | 9.0                                            | 15.5                                           | 3.0                                            | 3.4                                             |\n| Llama 2-7B > in  | 8.3                                            | 14.4                                           | 2.4                                            | 3.2                                             |\n| Llama 2-7B > ra  | 8.9                                            | 14.9                                           | 2.8                                            | 3.4                                             |\n| Llama 2-7B > va  | 9.1 (+2.25%)  | 15.7 (+5.37%) | 3.0 (+7.14%)  | 3.8  (+11.76%) |\n| Llama 2-70B > tt | 36.6                                           | 55.8                                           | 10.0                                           | 14.4                                            |\n| Llama 2-70B > in | 33.4                                           | 53.7                                           | 9.2                                            | 13.6                                            |\n| Llama 2-70B > ra | 36.1                                           | 55.4                                           | 9.6                                            | 14.2                                            |\n| Llama 2-70B > va | 39.5 (+9.42%) | 59.1 (+6.68%) | 10.4 (+8.33%) | 15.0  (+5.30%)  |"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456296910,
                "cdate": 1700456296910,
                "tmdate": 1700456832063,
                "mdate": 1700456832063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fUlasfvoLY",
                "forum": "8xliOUg9EW",
                "replyto": "rcmS457o3S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Elaboration on Table 3 on the Expert User Study"
                    },
                    "comment": {
                        "value": "Dear Reviewer LURb,\n\nWe have expanded the manual verification subset by randomly selecting and manually checking another 112 data points, resulting in 200 verified data points in total, 100 of which pass the Lean Prover (Group Valid) and 100 of which do not (Group Invalid).\nThe results of the 200 data points are demonstrated in Table 3 as follows.\nWith the larger verified subset, we observe that the D3 value of the **Valid** group becomes on par with the **Invalid** group. Moreover, the $p$-value of D3 is greater than 0.005, which is in line with our intuition that the Lean Prover does not help the question type classification, as we explained in the previous response that MUSTARD generates a diverse range of problems from the elementary to higher educational levels, and generating theorem-proving problems at the elementary school level is specifically challenging.\n\nMore importantly, \n(D4) and (D6) show significant differences in accuracy between the two groups, which is consistent with our previous observations.\nMoreover, (D1) also shows significance with the inspected data scaled up.\nThe results demonstrate that the validated data have a higher quality than those without a formal validation process.\nTherefore, our proposed MUSTARD framework is beneficial for providing such higher-quality data and being helpful for LLM mathematical reasoning. \n\n\n\nTable 3: Inspection dimensions and requirements in human evaluation. IS: Informal Statement. IP: Informal Proof. FS: Formal Statement. FP: Formal Proof. RT: Reasoning Type. \nSignificant $p<0.005$ are marked with **bold**.\n\n| Inspection Dimension | Requirement | Valid | Invalid | *p*-value | \n| ------- | ------- | ------- | ------- | ------- |\n| (D1) IS Correctness | *Whether the informal statement is factually correct.* | 93.50 | 83.50 | **0.00167** |\n| (D2) IS Relevance | *Whether the informal statement is relevant to each seed concept.* | 87.50 | 92.50 | 0.09604 |\n| (D3) RT Classification | *Whether the informal statement is of the required question type.* |67.00 | 68.50 | 0.74903 |\n| (D4) IP Correctness | *Whether the informal proof correctly solves the informal statement.* | 88.50 | 73.50 | **0.00012** |\n| (D5) IS-FS Alignment | *Whether the informal statement and the formal statement describe the same problem and are aligned with each other.* | 74.00 | 66.50 | 0.10138 |\n| (D6) IP-FP Alignment | *Whether the informal proof and the formal proof describe the same solution and have aligned proof steps.* |72.00 | 54.00 | **0.00018** |"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625682278,
                "cdate": 1700625682278,
                "tmdate": 1700626017281,
                "mdate": 1700626017281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lU4VIPGR37",
                "forum": "8xliOUg9EW",
                "replyto": "fUlasfvoLY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2311/Reviewer_LURb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2311/Reviewer_LURb"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "I thank the authors for addressing my questions and providing more experiments on Llama models. One question remains for future work: can this approach still be applied if informal data is generated by model M (which is not GPT-4) and then M will be fine-tuned with the approach (which was my initial question and might've been a misunderstanding). Nevertheless, the dataset could be an important benchmark for the neural theorem proving community in the future. I've increased my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2311/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632279573,
                "cdate": 1700632279573,
                "tmdate": 1700632279573,
                "mdate": 1700632279573,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]