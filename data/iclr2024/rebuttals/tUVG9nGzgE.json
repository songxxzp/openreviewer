[
    {
        "title": "Learning Conditional Invariances through Non-Commutativity"
    },
    {
        "review": {
            "id": "ddJJvTt9EE",
            "forum": "tUVG9nGzgE",
            "replyto": "tUVG9nGzgE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1260/Reviewer_C1qz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1260/Reviewer_C1qz"
            ],
            "content": {
                "summary": {
                    "value": "The author present an idea of invariant learning, that is learning non-commutative invariances (NCI), which is conditioned on the target domain and therefore preserves target-specific information. When the target domain contains more semantically relevant information than the source domain, the authors theoretically show that the NCI approach is beneficial for domain adaptation. Empirical results are also provided and show the benefit of NCI approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The empirical results do show the benefit of the proposed NCI approach in domain adaptation tasks. The assumptions of asymmetry and non-commutative seems to be novel in the context of invariant learning. Also the authors aim to provide a general theoretical framework that captures the properties of invariant learning algorithms (through the lens of commutativity) and can explain the benefit of NCI."
                },
                "weaknesses": {
                    "value": "The theory part of this work is very difficult to understand. Lack of necessary explanations of notations and definitions. See the questions for details."
                },
                "questions": {
                    "value": "1. What is an optimal encoder $\\Phi^{\\star}$ (in what sense it is optimal, i.e., what is the objective function that it minimizes)?\n\n2. In definition 1, what does an operator mean in the context of invariant learning (or domain adaptation)? Can you give an example of an operator in an invariant learning algorithm (e.g., DANN)?\n\n3. You only give the definition of NCI for operators (in definition 1). What is a NCI encoder $\\phi^{\\star}$? Again, what does optimal mean in   Result 1?\n\n4. In Theorem 1,  you write $s=\\tau + \\delta$. But how to do calculations between domain $s$ and $\\tau$? You do not give any definition.\n\n5. All the risks are not defined."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1260/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1260/Reviewer_C1qz",
                        "ICLR.cc/2024/Conference/Submission1260/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698266616389,
            "cdate": 1698266616389,
            "tmdate": 1700661213194,
            "mdate": 1700661213194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PvLICMCG2y",
                "forum": "tUVG9nGzgE",
                "replyto": "ddJJvTt9EE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1260/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for precisely pointing out the theoretical aspects of our work that require further clarification. To this end, we have introduced a number of subsections, primarily in the Appendix, that attempt to address such lack of clarity. Below, we discuss our changes in further detail.\n\n0. **General theoretical clarity:** Through our update, we make the notions of optimality (**Appendix A.3.4, Definitions 3 and 4**) and risk (**Appendix A.3.3**) more explicit, elaborate upon the necessity of introducing the construct of operators and their connection with encoders (**Appendix A.3.2**), as well as formalize the notion of separation on the space of domains (**Appendix A.3.1**). Following the suggestions of Reviewer tufY, we also expand upon the proof of Theorem 2, disambiguate the usage of $\\delta$ by introducing $\\Delta$ as a separate symbol for inter-domain distance, as well as state the assumption of asymmetry within a separate mathematical environment.\n\n1. **Optimality of** $\\Phi^*$: We have now concretized all the notions of optimality, i.e., domain-specific ($\\Phi^*$) and domain-agnostic ($\\varphi^*$) in **Appendix A.3.4, Definitions 3 (Domain-Specific Optimality) and 4 (Optimal-on-Average)**, which details the exact conditions under which an encoder can be deemed as being optimal. From these definitions, it can be seen that the objective function that is minimized is the empirical risk, denoted by $R(\\cdot)$, further details about which are provided in Section 3.4 (Training with NCI) of the main paper and Appendix A.3.3 (Risks).\n\n2. and 3. **Meaning and example of operators, and their connection with encoders:** We have now added a dedicated subsection, **Appendix A.3.2 (Operator-Encoder Duality)**, that discusses all of these. It first establishes the motivation for introducing the notion of operators into our theoretical framework, and then discusses the connection between the abstract operators and more concrete / practical, real-world encoders (which are essentially neural networks in our case). It accompanies the discussion with examples, both from the paradigm of NCI (non-commutatively invariant operator) and DANN (commutatively invariant operator) to make the ideas more relatable to practice.\n####\n4. **Calculations in the space of domains:** We have now described in **Appendix A.3.1 (Metric Space of Domains)**, how the discrepancy between two domains can be calculated. In summary, the idea of $H_\\eta$-divergence (Definition 5) allows us to quantify the separation between $s$ and $\\tau$ as $s = \\tau + \\Delta$. In practice, this is calculated by the discriminator network $\\eta$ that measures the separability of domains in the representation space of the encoder $\\varphi$ through optimizing $\\mathcal{L}_\\eta$ (Equation 2).\n\n5. **Risks:** We have now introduced **Appendix A.3.3 (Risks)** that explicitly defines the general form of the risks under consideration. In general, we minimize the empirical risk in all cases. Their specific forms depend on the kind of function that is being learned, as stated in Section 3.4 (Training with NCI) of the main paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497613921,
                "cdate": 1700497613921,
                "tmdate": 1700499048666,
                "mdate": 1700499048666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vE1XvHurVV",
                "forum": "tUVG9nGzgE",
                "replyto": "PvLICMCG2y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1260/Reviewer_C1qz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1260/Reviewer_C1qz"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thanks for your detailed explanation. I highly appreciate your attempt to address these issues in revised version, which resolves my concerns with respect to writing. Since I am not very familiar with this area, and may not fairly evaluate your contribution, I will raise my score to 5/6, with confidence 2. I hope other more knowledgable reviewers or AC can provide more accurate evaluations, and I will rely on their idea."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661182888,
                "cdate": 1700661182888,
                "tmdate": 1700661182888,
                "mdate": 1700661182888,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jPHJbdIkqv",
            "forum": "tUVG9nGzgE",
            "replyto": "tUVG9nGzgE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1260/Reviewer_a8sS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1260/Reviewer_a8sS"
            ],
            "content": {
                "summary": {
                    "value": "This paper shows that a provably optimal and sampleefficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Both their theory and experiments show that non-commutative invariance can leverage source domain samples to meet the sample complexity of learning the optimal target-specific encoder, surpassing SOTA invariance learning algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The theoretical analysis is comprehensive.\n2. The experimental results are solid."
                },
                "weaknesses": {
                    "value": "none"
                },
                "questions": {
                    "value": "none"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1260/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1260/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1260/Reviewer_a8sS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698584428450,
            "cdate": 1698584428450,
            "tmdate": 1699636052587,
            "mdate": 1699636052587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "2XWklnrtBv",
            "forum": "tUVG9nGzgE",
            "replyto": "tUVG9nGzgE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1260/Reviewer_tufY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1260/Reviewer_tufY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies Non-Commutative Invariance (NCI) as an efficient way of learning conditional invariances. The authors propose a method that leverages domain-specific information and focuses on the target domain while learning. They argue that relaxing the invariance criterion to be non-commutatively directed towards the target domain results in a more optimal and sample-efficient learning of conditional invariances. The paper highlights the superiority of NCI over SOTA invariance learning algorithms for domain adaptation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**S1** The paper is well-written, with math notations clearly explained.\n\n**S2** The empirical observations are supported by theoretical guarantees.\n\n**S3** The discussion surrounding each theorem makes the theoretical results more understandable.\n\n**S4** The paper presents extensive numerical experiments and demonstrates that their NCI-based approach surpasses existing SOTA algorithms in invariance learning."
                },
                "weaknesses": {
                    "value": "Please see below."
                },
                "questions": {
                    "value": "Section 3.1:\n\nI suggest presenting the following statement as a formal assumption within a mathematical environment for better clarity.\n\n\"For the remainder of our analysis, we assume that the target domain contains more semantically relevant information than the source domain, i.e., ...\"\n\nSection 3.2:\n\nPlease ensure consistency in the use of symbols across theorems. If the symbol $\\delta$ in Theorems 1 and 2 refers to different variables, this should be corrected to avoid confusion.\n\nClarification in Proof: The proof of Theorem 2 requires more clarity, particularly in the concluding part i.e. \u201c \u2026 samples from the target domain, which is the \u2026 This completes the proof of the theorem\u201d. It's currently difficult to understand how the final result is derived from the given explanation. \n\nRelated Work:\n\nThe technical comparison with the paper \"Learning Conditional Invariance through Cycle Consistency\" requires more detail."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1260/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1260/Reviewer_tufY",
                        "ICLR.cc/2024/Conference/Submission1260/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1260/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699635513411,
            "cdate": 1699635513411,
            "tmdate": 1701059162541,
            "mdate": 1701059162541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pG2ciCgEXP",
                "forum": "tUVG9nGzgE",
                "replyto": "2XWklnrtBv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1260/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1260/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response to Reviewer tufY"
                    },
                    "comment": {
                        "value": "We thank the reviewer for suggestions that not only improve the readability and comprehensibility of our paper, but also make our work more complete and thorough. To this end, we incorporate a number of changes throughout the main manuscript and the Appendix which we detail below.\n\n1. **Section 3.1 Assumption:** We have now stated this as a separate, formal assumption as **Assumption 1 (page 3)**, with the corresponding mathematical formulation.\n    \n2. **Usage of $\\delta$:** We thank the reviewer for pointing out this error. We have now corrected and replaced the notation for distance between domains with $\\Delta$ across the full paper. So now, Theorem 1 and all associated results use $\\Delta$ to denote distance between domains, whereas $\\delta$ is used in Theorem 2 and all associated discussions to denote probability of success.\n    \n3. **Proof of Theorem 2:** We agree with the reviewer that the jump to the conclusion about the sample complexity of learning the optimal target domain encoder $\\Phi^*_\\tau$ was indeed too abrupt in the proof of Theorem 2. In **page 20**, we have now extended the proof of Theorem 2 to address this by explicitly specifying the link between how non-commutatively combining the source and the target domains maps all samples to the target domain, and the sample complexity of learning $\\Phi^*_\\tau$. If there is some aspect of the proof that is still unclear, we would be happy to have further inputs from the reviewer about the same.\n    \n4. **Technical comparison with Samarin et al., 2021:** We thank the reviewer for pointing this out, as despite discussing this work in the introduction as being very relevant to our paradigm of conditional invariance learning, we missed out on a more detailed technical comparison of our NCI with theirs. As part of this rebuttal, we have now addressed this by evaluating their method (dubbed CICyc in our manuscript) on the PACS, Office-Home, and DomainNet benchmarks, reporting the results in **Tables 1 and 4**. We have accompanied this with a technical discussion on their work and how our proposed theoretical framework can explain the empirical behaviour of their algorithm in **page 21**."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1260/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497098337,
                "cdate": 1700497098337,
                "tmdate": 1700497098337,
                "mdate": 1700497098337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]