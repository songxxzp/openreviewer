[
    {
        "title": "Explore, Establish, Exploit: Red Teaming Language Models from Scratch"
    },
    {
        "review": {
            "id": "s0GJIs2dKM",
            "forum": "zSwH0Wo2wo",
            "replyto": "zSwH0Wo2wo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a 3-step human-in-the-loop red teaming framework that can red team LMs from scratch (without any previously given red team classifiers).\nThe proposed framework consists of the following 3 steps:\n1. Explore LM behaviors and collect data.\n2. Human-label data from the first step and train a red team classifier using these data.\n3. Train an RL agent to generate adversarial prompts, which elicit responses from the model that are classified as harmful.\n\nThe experimental results show that the proposed method can successfully red-team GPT-3 and GPT-2-xl models. \nMoreover, the authors produce the CommonClaim dataset consisting of 20,000 completions by GPT-3-text-davinci-002 human labelled as 'true', 'false', or 'neither'."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to understand\n- The idea of the paper is simple and clear.\n- The proposed method adapts diversity sampling techniques during exploration and exploitation steps to enhance the diversity of the resulting adversarial prompts. The empirical results show that diversity sampling techniques are key to avoiding mode collapses.\n- The authors contribute to society by providing the CommonClaim dataset."
                },
                "weaknesses": {
                    "value": "- Lack of novelty\n- Lack of quantitative comparisons\n- Limited applicability scenarios\n- Some missing references\n\nPlease refer to the questions for the details."
                },
                "questions": {
                    "value": "(Novelty) There exists a sort of study that tried to build the red team dataset and a classifier based on the human-in-the-loop framework [1,2,3]. These methods utilize human resources to generate adversarial prompts and label the harmfulness of the model response to construct a dataset and train a classifier. If I understood correctly, the main difference between existing studies can be written as following:\n1. For the \"Attack\" part, the proposed \"from scratch\" framework utilized a rl-based attack algorithm instead of human attackers as in [1,2]. However, as stated in the paper, the rl-based attack algorithm has been used in prior works such as [Deng et al., 2022] or [Perez et al., 2022b].\n2. The authors utilize diversity sampling to avoid mode collapses of the rl-based attack method. However, [Perez et al., 2022b] already emphasize the importance of diversity in red-teaming. Moreover, the other study proposed a red teaming approach that incorporates the diversity of adversarial prompts into the objective function throughout the red teaming process [4]. \n\nCan you clarify the novelty of the proposed \"from scratch\" red teaming method in detail?\n\n(Quantitative Analysis) The paper provides a few quantitative analyses. Most of the experimental results are qualitative. The examples in Appendix B about mode collapse seems clear, but it would be more credible if you could provide quantitative red-teaming results with and without each diversity sampling in step 1 and 3 with appropriate diversity metric and performance metric.\n\n(Limited Scenarios) I cannot agree with the justification of the scenarios. My questions can be divided into the following two parts:\n1. In the paper, the authors state that \"Most importantly, if failures can already be efficiently identified in advance, then red-teaming has limited value because bad text could simply be filtered from the model\u2019s training data and/or outputs.\" However, filtering in training data or outputs can degrade the model performance. Model unlearning can be another solution to this problem, but it doesn't work well in my knowledge. Can you provide any references to your statement? \n2. Once we made datasets and classifiers for some purposes like toxicity or fact-checking, we can re-use these again and again to red-team the different LMs. If you already have data and classifiers, it seems strange not to utilize them. Therefore, the situation, in which the proposed scenario is valid, is limited to cases where the target criteria of harmfulness are significantly different from the existing collected data, so the existing data cannot be utilized. I can't agree on whether there will be many cases like this, can you explain more? Or can you provide quantitative evidence that the proposed method is effective when there is a lot of data already collected and the classifier is learned?\n\n(Missing References)\n- [1] Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack, Dinan et al., 2019.\n- [2] Bot-Adversarial Dialogue for Safe Conversational Agents, Xu et al., 2021.\n- [3] SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration, Lee et al., ACL 2023.\n- [4] Query-Efficient Black-Box Red Teaming via Bayesian Optimization, Lee et al., ACL 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper contains several subsections discussing the ethical perspectives associated with the research."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8302/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe",
                        "ICLR.cc/2024/Conference/Submission8302/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698181379523,
            "cdate": 1698181379523,
            "tmdate": 1700538716929,
            "mdate": 1700538716929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "59F48OGNl4",
                "forum": "zSwH0Wo2wo",
                "replyto": "s0GJIs2dKM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8302/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to tVNe"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and comments. We are glad to hear that you found the paper to be well-written and that the CommonClaim dataset will be valuable. Below are replies to weaknesses and questions. \n\n### W1 & W4 \u2013 Re: Comparisons to prior work and missing references\n\n**New discussions of related work:** Thank you for the suggestions about prior work. In particular, we think that it will be useful for us to cite [Xu et al.  (2021)](https://aclanthology.org/2021.naacl-main.235/)([2]) as an example of the effectiveness of a filtering baseline. We added this. Meanwhile, we have added [Lee et al. (2023a)](https://arxiv.org/abs/2305.17696) [(3]) and [Ziegler et al. (2022)](https://arxiv.org/abs/2205.01663) as examples of human-generated attacks against language generators, and [Lee et al. (2023b)](https://arxiv.org/abs/2305.17444)([4]) next to our discussion of [Shi et al., (2022)](https://arxiv.org/abs/2212.10539) and [Kumar et al. (2022)](https://arxiv.org/abs/2205.12558) which use related langevin-dynamics-based approaches to automated red teaming.\n\n**In comparison to prior works that rely entirely on humans to generate adversarial prompts, our work is novel in its use of automated attacks in the Exploit step.** We agree that human-in-the-loop attacks are interesting and important. However, we believe that automated attacks are an important complement to these methods. In practice, we expect that a combination of both will be used. One advantage of our approach is that a fixed amount of human effort in the Establish step can be used to enable very large amounts of cheap, automated attacks. Another advantage is that sometimes it might be easier to define a target category of text than to attack a model to produce that target category. Also automated methods and humans simply tend to produce different types of prompts, so these different methods for attacking networks can be complementary.\n\n**Our attack method adds to the method used by [Perez et al. (2022)](https://arxiv.org/abs/2202.03286).** We are sorry that the paper was unclear about our contribution. Our diversity objective for RL is novel. While Perez et al. talk about the importance of diversity in RL-based attacks, they do not use our diversity objective (see, e.g., Figure 2 of their paper or the last paragraph of their section 2.2.) We have updated our discussion of the ablation experiments for this diversity term to clarify that the ablation experiment was with the same method as [Perez et al. (2022)](https://arxiv.org/abs/2202.03286) and [Deng et al. (2022)](https://arxiv.org/abs/2205.12548). We also added vocabulary size analysis to better quantify mode collapse. Across 100 samples, the prompt vocabulary size decreased from 116 to 3 and 81 to 9 for the toxicity and untruthfulness red teaming experiments without our diversity method.  \n\n**We added a discussion of other prior techniques to improve diversity and noted that they are not applicable to RL attacks.** We added mention of how [Lee et al. (2023b)](https://arxiv.org/abs/2305.17444)([4]), [Shi et al., (2022)](https://arxiv.org/abs/2212.10539), and [Kumar et al. (2022)](https://arxiv.org/abs/2205.12558) use an analogous but non-RL technique for this. Thank you for pointing this out!\n\n**See also our global response.** We give an overview the paper\u2019s novel contributions. \n\n### W2 \u2013 Re: Quantitative analysis\n\n**We would like to clarify that we have a fair amount of quantitative evaluations of our method.** These include analysis of how much our attacks increased toxicity, mode collapse without our diversity technique, label agreement between humans and models, and the success of the prompt generators at eliciting harmful responses according to each classifier we used.  We discuss these in detail in the global response. Are there any other quantitative experiments you would like to see us add?\n\n**We are running a new experiment** in which we omit the diversity term used in the first step (Explore) for toxicity experiments. Thank you for the suggestion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166294086,
                "cdate": 1700166294086,
                "tmdate": 1700166294086,
                "mdate": 1700166294086,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WkcR2Eo5mH",
                "forum": "zSwH0Wo2wo",
                "replyto": "s0GJIs2dKM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your kind responses. Global response and your responses sufficiently addressed most of my concerns on your paper. \n\nThere was a line of research about detoxifying pre-trained language models [1,2,3]. \nMy previous comment \u201cHowever, filtering in training data or outputs can degrade the model performance.\u201d in W3.1 is based on [1], which shows that filtering toxic contents in training data can help detoxify the LM but also can result in performance degradation. \n\nI appreciate the inclusion of additional references, such as [4], in the revised version of the paper, which adequately addressed my concerns regarding this issue.\n\nIn W3.2, I requested you to justify the \"why from scratch\" part. \nYour explanation, that the context of toxicity can be very diverse, was helpful in understanding the necessity. \nHowever, as you agreed, we still do not need to red-team a new model from scratch if we already have a dataset and a classifier sharing the same context. \nIn this regard, I still think that the proposed red-teaming method has limited-applicability.\nFor example, since the authors released a valuable dataset CommonClaim in this paper, we can red-team other new models for the purpose of fact-checking by re-using this dataset and classifier, not from scratch. \n\nTherefore, I still think that the proposed method will only be used in limited situations from the perspective of red teaming. However, from the perspective of utilizing the automatic red-teaming method to **collect datasets** and **train classifiers** corresponding to toxicity in certain new contexts, it seems like a nice research approach. \nIn this regard, I raised my scores to 5.\n\nBut, I think it would be better to emphasize the **collect datasets** and **train classifiers** parts, more than the **red-team** part in the paper. \n\n\n[1] Challenges in Detoxifying Language Models, Welbl et al., EMNLP findings 2021.\n\n[2] Detoxifying Language Models Risks Marginalizing Minority Voices, Xu et al., NAACL 2021.\n\n[3] Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP, Schick et al., TACL 2021.\n\n[4] Pretraining language models with human preferences, Korbak et al., ICML 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538699096,
                "cdate": 1700538699096,
                "tmdate": 1700538814658,
                "mdate": 1700538814658,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4XZjVeYALQ",
                "forum": "zSwH0Wo2wo",
                "replyto": "6bFOJldBAC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe"
                ],
                "content": {
                    "title": {
                        "value": "Thanks + Additional Comments"
                    },
                    "comment": {
                        "value": "Thank you for getting back to me. Your last response helps me understand more about your work. Thank you!\n\nIf I understood correctly, your work is in the line \"human-in-the-loop toxic data construction method\" [1,2,4]. According to your response, your work is distinguished from other previous works in the sense of using automated red-teaming methods instead of manual red-teaming.\n\nHowever, there is already a line of research that has attempted to construct data automatically using a language model [3,4]. Furthermore,  I noticed that Lee et al., (2023) [4] have already proposed a toxic data construction method that employs an automated red teaming approach. Please refer to Figure 1 in their paper.\n\nThis leads me to a point of confusion regarding your definition of \"automated red teaming.\" Hence, I would appreciate an explanation as to why you categorize the method used in [4] as a manual red-teaming approach in your related work section. Also, can you clarify the differences between your work and [4]?\n\n- [1] Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack, Dinan et al., 2019.\n- [2] Bot-Adversarial Dialogue for Safe Conversational Agents, Xu et al., 2021.\n- [3] WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation, Liu et al., EMNLP findings 2022.\n- [4] SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration, Lee et al., ACL 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593069768,
                "cdate": 1700593069768,
                "tmdate": 1700593069768,
                "mdate": 1700593069768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uAJY5Zo5n7",
                "forum": "zSwH0Wo2wo",
                "replyto": "s0GJIs2dKM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8302/Reviewer_tVNe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the update. The revised introduction appears to be clearer in conveying the contribution of your work. However, I still have difficulty understanding which point of your work is more beneficial than [1]. I have some additional questions for clarification.\n\nIn your previous responses, you commented that:\n```\nBoth types of methods are clearly valuable, but one useful property of our approach is that a fixed amount of human effort can then allow for highly scalable automated adversarial example generation.\n```\nCan you provide more details or evidence to support this statement? \n\nBased on your responses, the primary distinction between your work and [1] appears to be the presence of annotation during the exploitation step, with your approach offering a lower annotation cost compared to previous *partially automated human-in-the-loop* methods [1,2]. However, [1] addressed the problem of high annotation cost by only annotating candidates in the gray area, where filter predictions are ambiguous. Also, it is worth noting that [1] also can be *fully automated* by stopping annotation and using only filter predictions to label candidates. \n\nIn this regard, if you were to allocate the same annotation budget in both approaches, can you clarify why your approach is beneficial than [1] in such a scenario?\n\n[1] SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created Through Human-Machine Collaboration, Lee et al., ACL 2023.\n[2] WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation, Liu et al., EMNLP findings 2022."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618775031,
                "cdate": 1700618775031,
                "tmdate": 1700627702980,
                "mdate": 1700627702980,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9ykazP7DMV",
            "forum": "zSwH0Wo2wo",
            "replyto": "zSwH0Wo2wo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8302/Reviewer_FuAh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8302/Reviewer_FuAh"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new LLM red-teaming methodology with three steps: explore, establish, and exploit. The purpose of this method is to enable red-teaming of LLMs in cases where there is no pre-existing understanding of what kinds of outputs would be considered \u201cbad\u201d for the model. In the first step, the method samples prompts and outputs as an exploratory stage. Then, using humans in the loop, examples from the previous stage are labeled and a model and task specific classifier is trained with a label set defined based on the types of outputs seen in the explore stage. Finally, the exploit stage uses reinforcement learning and the output classifier to train an LLM capable of generating adversarial prompts for the LLM being red-teamed. In two experimental settings, the paper shows results that suggest the method does allow for improved generation of prompts that elicit forbidden outputs. The paper also introduces a dataset, CommonClaims, that contains statements that are labeled as common-knowledge-true, common-knowledge-false, or neither."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The primary strength of the paper is in the novel method it introduces. The work systematically breaks down a sensible approach to red-teaming an LLM, and the results seem to indicate that it works reasonably well. The paper is very clearly written and each step of the proposed method is motivated and explained well. Overall, the work is a very solid contribution and fills a gap in the LLM evaluation literature."
                },
                "weaknesses": {
                    "value": "The primary weaknesses of the paper are its limited evaluation and the discussion of the approach\u2019s limitations overall. The paper tests out the red-teaming approach on two different models, both GPT based, to attempt to elicit the model to produce toxic or false statements. Both of these are targeted at GPT-based models (GPT-2 and GPT-3). It would have been nice to see evaluations on other common LLMs, such as Bard, Llama, or Claude. This is only a mild weakness of the work, however, because the paper is mostly about introducing the new method. The second weakness is just that I would have liked to see more discussion of the limitations of the approach at a high-level. For example, are there alternatives to using an LLM to generate adversarial prompts? What kinds of biases might this introduce? How scalable is the method when the second step requires human input? The lack of discussion of these questions is not a major issue, but it would be nice in a future version to see the limitations section fleshed out a bit more."
                },
                "questions": {
                    "value": "The paper mentions that human input can be used to determine the set of labels used to train the classifier in step 2 of the method. Can the authors describe a bit more what that process looks like in practice?\n\nTable 4 shows some examples of completions. I notice that some of them seem a bit nonsensical or have strange spacing and grammar in places (for example, the completion that ends in ButTONIC). Can the authors expand on why this happens, and how does this affect the evaluation of which outputs are problematic?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614039974,
            "cdate": 1698614039974,
            "tmdate": 1699637032153,
            "mdate": 1699637032153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DB8hJvlgw8",
                "forum": "zSwH0Wo2wo",
                "replyto": "9ykazP7DMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8302/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to FuAh"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and comments. We are glad to hear that you found the paper to be well-written and to address an important gap in the literature. Below are replies to weaknesses in order. \n\n### W1 \u2013 Re: Concerns about only using GPT-2 and GPT-3\n\n**We agree** that working with Llama, Claude, and Bard-type models would be useful for establishing clearer relevance. We added this to our discussion of future work. \n\n**We also emphasize that our method is black-box, and the RL attacks that we use in the exploit step could be replaced with other attack methods.** In the adversarial attacks literature, findings with individual methods/models often quickly become outdated, but these two facts make us expect that our work may have some implications that may continue to be valuable further in the future. \n\n### W2 \u2013 Re: adding to the discussion of limitations\n\n**We have added to Section 2, and Section 5 to discuss these.** Thank you for pointing these out. We discuss how alternatives could have been used other than the RL attack we used (see the \u201cRed-teaming with automated searches for natural language prompts\u201d paragraph). We added references to works on AI oversight ([Bai et al., 2022](https://arxiv.org/abs/2212.08073)), AI feedback ([Lee et al., 2023](https://arxiv.org/abs/2309.00267)), active learning ([Zhen et al., 2022](https://arxiv.org/abs/2203.13450)), and weak supervision ([Boecking, 2020](https://arxiv.org/abs/2012.06046)) to discuss how human involvement might be effectively scaled. \n\n### Q1 \u2013 Re:  \u201chuman input can be used to determine the set of labels used to train the classifier\u2026Can the authors describe a bit more what that process looks like in practice?\u201d\n\n**Abstractly, the establish step can be divided into three substeps.** After the red team has expired the data, they pick a label set, obtain labels, and then train a classifier for harmful text on those labels. So in our case, the answers that we obtained from crowdworkers were not used to select the label set. The label set was selected by us after our own analysis of the data. \n\n**Consider how this compares to prior approaches.** Naively (e.g. without developing a contextual understanding of failure modes), it may seem reasonable to use a classifier trained on the CREAK dataset of true and false statements. But as we find, this does not work well, presumably because of the distributional differences and the lack of a \u201cneither\u201d label. \n\n### Q2 \u2013 Re: Concerns about dysfluencies in prompts\n\n**We agree that the results in Table 4 have dysfluencies and repetition, but we emphasize two points, and we updated the discussion of these results accordingly.** \n- Prompts appearing as fluent English is not required for them to be adversarial. For example, [Zou et al., 2023](https://arxiv.org/abs/2307.15043) studied jailbreaks with triggers such as \u201crestored into one sentence grammar using proper colon\u201d. We only focus on developing diverse and adversarial prompts. Making plain-English ones is not a goal of ours, however, it would be possible to do with a stronger KL penalty on the generator\u2019s outputs. \n- We agree that there is room to further improve the diversity of the attack distribution. However, our method is much better than the baseline. Qualitatively, the results in Table 4 are much more diverse than the results from the baseline in Table 6. And without our diversity reward, 61 out of the 100 sentences we sampled were all identical. \n\nThank you again for your time and help! We are looking forward to the rest of the discussion period."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166071182,
                "cdate": 1700166071182,
                "tmdate": 1700166071182,
                "mdate": 1700166071182,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HN54Ju3OQ6",
            "forum": "zSwH0Wo2wo",
            "replyto": "zSwH0Wo2wo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8302/Reviewer_weY9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8302/Reviewer_weY9"
            ],
            "content": {
                "summary": {
                    "value": "The paper describe a framework for redteaming a LLM from scratch, that is, it consists of finding possible behaviour problems of the model, labelling the vulnerabilities and finding malicious or adversarial prompts that will elicit such undesirable behaviour. \n\nThe proposed method is just a compilation of relevant known techniques from the literature. The set of experiments are quite comprehensive (but still lack comparison) and the results (although shown against GPT3-davinci-002) indicates the importance of dealing with this problem. Overall, this paper describes a good engineering solution to an important problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes an effective solution to a very important problem to find prompts that will generate undesirable contents. \n\n2. The proposed methods are all plausible and easy to apply in a similar settings in practice. \n\n3. The evaluation is quite good but lack comparative studie and many sane and helpful conclusions are drawn."
                },
                "weaknesses": {
                    "value": "1. The proposed method is claimed to be mainly different from the previous work in that it has two more steps: exploration and establishment. However, these two steps are straight-forward (e.g., using existing diversification technique to explore the output space) or still mainly rely on human annotation (interaction) (e.g., the \"establish\" step). Therefore, it is not essentially different or more challenging than what was for the previous work. \n\n2. For the problem setting considered by the paper (i.e., from scratch), the exploration step may be the most critical. The current proposal require internal state information of the LLM, and hence cannot be used in close-source or API-only LLMs, which limits the potential impact of this study. \n\n3. It is desirable to compare with previous work and other adversarial prompt generation techniques (in a as fair setting as possible) to better evaluate the performance of the proposed method. Currently, this is unknown. \n\n4. The generated adversarial prompts seem to have some repeating words from time to time and also seem to be not that diversified. Is it the nature of the problem or some artifact of the method?"
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8302/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8302/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8302/Reviewer_weY9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698972197386,
            "cdate": 1698972197386,
            "tmdate": 1699637031959,
            "mdate": 1699637031959,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ucBQPm3KRe",
                "forum": "zSwH0Wo2wo",
                "replyto": "HN54Ju3OQ6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8302/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to weY9"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and comments. We are glad to hear that you found the paper to contribute an effective solution to a very important problem. We do our best to address your concerns below. Please let us know if there is anything else that we can do to help.\n\n### W1 \u2014 Re: Straightforwardness of the method\n\n**We agree that this is a natural way to approach the problem. However, there has not been prior work to show the benefits of a contextually defined target for automated red teaming.** Prior works on red teaming have tended to neglect the role of preference formation and human factors in AI oversight, and simple filtering-based baselines. To the best of our knowledge, ours is the first work on automated attack methods that addresses these. \n\n**The Explore and Establish steps were the reason why our red teaming was more successful than the experiment with the pre-existing CREAK data.** Based on prior work (e.g. [Perez et al. (2022)](https://arxiv.org/abs/2202.03286)) one might suspect that the best way to approach automatic red-teaming for false statements is to use an off-the-shelf classifier for true vs false statements. However, we show that using a classifier trained on the CREAK dataset, was not effective (see Section 3.2.1 and Appendix D). Contextual red teaming was needed. \n\n### W2 \u2013 Re: Concerns about using the target model\u2019s latents\n\n**Sorry for the lack of clarity \u2013 we updated Section 2 and Figure 2 to clarify that we do not need the target model\u2019s latents.** We apologize that we were unclear about whether we need the target model\u2019s latents. We only use these when available. We have updated our description of the methods to say that when latents are not available, we use another model\u2019s embeddings as done in Section 3.2. Internal activations of GPT-3-text-davinci-002 were not available via API, so we instead used embeddings from GPT-3-text-ada-002, a dedicated text encoder.\n\n### W3 \u2013 Re: Making comparisons to other methods\n\n**We want to clarify that we compare our method to conventional RL-based attacks and quantitatively analyze the results.** We improve on the RL attack method from [Perez et al. (2022)](https://arxiv.org/abs/2202.03286) and [Deng et al. (2022)](https://arxiv.org/abs/2205.12548) with our approach to producing diverse adversarial prompts. In our control experiments that did not include the diversity technique, the prompt generators exhibited mode collapse. We are sorry that this was not clear, and we updated our description of these experiments to point out that this was the method used by these two prior works. Is there another experiment that you would recommend that we run?\n\n**Comprehensive benchmarking is important, but out of scope.** In this paper, our goal was to run experiments to validate the benefits/performance of the improvements that we make to red teaming and RL attacks. We agree that other approaches to automated red teaming are interesting to compare, but we think that this would best be explored in separate work. We note that there currently are [two](https://trojandetection.ai/) [competitions](https://satml.org/participate-competitions/) being run to do this, and we are contributing to this by testing our RL attacks with one of them. \n\n**We have added a discussion of benchmarking to our future work section and clarified the goal of our experiments/evaluations.**\n\n### W4 \u2013 Re: Concerns about dysfluencies in prompts\n\n**We agree that the results in Table 4 have dysfluencies and repetition, but we emphasize two points, and we updated the discussion of these results accordingly.**\n- Prompts appearing as fluent English is not required for them to be adversarial. For example, [Zou et al., 2023](https://arxiv.org/abs/2307.15043) studied jailbreaks with triggers such as \u201crestored into one sentence grammar using proper colon\u201d. We only focus on developing diverse and adversarial prompts. Making plain-English ones is not a goal of ours, however, it would be possible to do with a stronger KL penalty on the generator\u2019s outputs. \n- We agree that there is room to further improve the diversity of the attack distribution. However, our method is much better than the baseline. Qualitatively, the results in Table 4 are much more diverse than the results from the baseline in Table 6. And without our diversity reward, 61 out of the 100 sentences we sampled were all identical. \n\nThank you again for your time and help! We are looking forward to the rest of the discussion period."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165948247,
                "cdate": 1700165948247,
                "tmdate": 1700165976656,
                "mdate": 1700165976656,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UKC1wMR8MD",
            "forum": "zSwH0Wo2wo",
            "replyto": "zSwH0Wo2wo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8302/Reviewer_8GJm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8302/Reviewer_8GJm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a three-step framework  for red-teaming \u201cfrom scratch\u201d in which the adversary does not begin with a way to classify failures. They also construct the CommonClaim dataset of 20,000 statements labeled by humans as common-knowledge-true, common knowledge-false, or neither."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Implementation of each step are introduced in details in Section 2 (Method) and 3 (Experiment)\n1. The design of three steps in the proposed framework is clearly described in Section 2."
                },
                "weaknesses": {
                    "value": "1. Red teaming normally uses manual or automated methods to adversarially probe a language model for harmful outputs, and then updates the model to avoid such outputs. However, in this work, only some case studies have been conducted to show the proposed framework is able to generate prompts elicit harmful content from LLMs. The work would be more complete if more quantitative results are presented and the follow-up model update is accomplished.\n\n2. Some other red teaming methods are not compared, e.g.,\n\nPerez, Ethan, et al. \"Red teaming language models with language models.\"\u00a0arXiv preprint arXiv:2202.03286 (2022).\n\n3. There is no ablation study to justify the effectiveness of the design of each step in the proposed framework"
                },
                "questions": {
                    "value": "1. Although it is said that the data and code are available, not the actual location to fetch those resources is not provided. In the supplementary materials, only the code is included.\n\n2. According to the abstract and introduction section, the goal of this work is to red-team from scratch. However, in step 2 of the proposed framework, we still need to choose a label set such that one of the labels represents undesirable outputs.  This indicates the category of undesirable output is pre-defined, which is inconsistent with the goal of this work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8302/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698984384548,
            "cdate": 1698984384548,
            "tmdate": 1699637031698,
            "mdate": 1699637031698,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ACiykgawDF",
                "forum": "zSwH0Wo2wo",
                "replyto": "UKC1wMR8MD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8302/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8302/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 8GJm"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and comments. We are glad to hear that you found the design and implementation of our experiments to be good. Here are replies to weaknesses and questions in order. \n\n### W1 \u2013 Re: Adversarially training the target model after attacking it. \n\n**We agree that adversarial training is an important next step.** In the \u201cWhat comes after Explore/Establish/Exploit?\u201d paragraph, we discuss this and others. Enabling this is a motivation of ours. \n\n**Our contributions are focused on red teaming but we believe they are sufficient and valuable in their own right.** Please see our global response for a summary of contributions, quantitative results, and qualitative results. Prior works on red teaming such as [Perez et al. (2022)](https://arxiv.org/abs/2202.03286) have not performed adversarial training either. Meanwhile, our work is the first to our knowledge to use automated attacks to red team a model  of a size comparable to GPT-3 for untruthful text. We also release the CommonClaim dataset. We updated the \u201cFuture work\u201d paragraph to discuss how we do not perform any outer applications of the dataset, classifier, or adversarial data. \n\n### W2 \u2013 Re: Comparisons to other methods such as Perez et al. (2022a).\n\n**We want to clarify that we compare our method to the RL-based one used by [Perez et al. (2022)](https://arxiv.org/abs/2202.03286) and quantitatively analyze the results.** We improve on the RL attack method from [Perez et al. (2022)](https://arxiv.org/abs/2202.03286) and [Deng et al. (2022)](https://arxiv.org/abs/2205.12548) with our approach to producing diverse adversarial prompts. In our control experiments that did not include the diversity technique, the prompt generators exhibited mode collapse. We are sorry that this was not clear, and we updated our description of these experiments to point out that this was the method used by these two prior works. We also added vocabulary size analysis to better quantify mode collapse. Across 100 samples, the prompt vocabulary size decreased from 116 to 3 and 81 to 9 for the toxicity and untruthfulness red teaming experiments without our diversity method.  Is there another experiment that you would recommend that we run?\n\n### W3 \u2013 Re: Ablation studies.\n\n**We want to clarify that we performed ablation experiments. We also have a new one underway.** Sorry that this was unclear. We updated our descriptions of these tests in the paper to clarify this. Thank you for this suggestion. The ablation tests were ones in which we removed the diversity term in the exploit step (Section 3.1.1, Section 3.2.1, Appendix B), replaced our false statement classifier with an off-the-shelf one (Section 3.2.1, Appendix D), and replaced our false statement classifier with one trained on chatbot labels (Section 3.2.1, Appendix E). We are currently running a new ablation test in which we omit the diversity subsampling from the Explore step. Please see the global response for all details. Are there other ablation tests you recommend we do?\n\n### Q1 \u2013 Re: CommonClaim in supplement.\n\n**We updated the supplementary materials to include CommonClaim.** We apologize for not including this in the supplement. Thank you for pointing this out.\n\n### Q2 \u2013 Re: \u201c...the goal of this work is to red-team from scratch. However, in step 2 of the proposed framework, we still need to choose a label set\u2026\u201d\n\n**Could you please clarify this concern?** In our framework, the process of choosing a label set is how we define the target for the red team. For example, in our truthfulness red-teaming, we defined the categories as part of the process, based on interaction with the model outputs. \n\n**Our results show the benefits of contextual red teaming in comparison to a baseline that uses an off-the-shelf truthfulness classifier based on the pre-existing CREAK dataset.** When we red team for truthfulness in Section 3.2, we identified the need for a third additional category of \u201cneither\u201d true nor false to account for statements that were opinions, vague, obscure, uncommon knowledge, or otherwise hard to categorize as true or false by common knowledge. This third label is an advantage that our approach had over the control experiment with the CREAK dataset which used a classifier trained on a pre-existing dataset that did not include a \u201cneither\u201d category. \n\nThank you again for your time and help! We are looking forward to the rest of the discussion period."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8302/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165816600,
                "cdate": 1700165816600,
                "tmdate": 1700165816600,
                "mdate": 1700165816600,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]