[
    {
        "title": "Separate and Diffuse: Using a Pretrained Diffusion Model for Better Source Separation"
    },
    {
        "review": {
            "id": "EdzRPGS77p",
            "forum": "UXALv0lJZS",
            "replyto": "UXALv0lJZS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3618/Reviewer_9rFE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3618/Reviewer_9rFE"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a single-channel speech separation method using a combination of deterministic and generative models. An upper bound on the signal distortion ratio (SDR) of this combined method is derived, suggesting that it has the potential to be better than the deterministic model alone."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors innovatively integrate deterministic models with generative models for speech separation and theoretically demonstrate the performance boundaries of this approach.\n2. To address the phase shift issue between the output estimates of the deterministic and generative models, an alignment network is employed to estimate two parameters for the fusion of the outputs from both models.\n3. Results across multiple datasets and models indicate that this method can further improve the performance of existing models."
                },
                "weaknesses": {
                    "value": "1. **Generative Model Selection:** The authors' choice of utilizing only one diffusion-based generative model to validate the performance enhancement brought by noise introduction appears to be limiting. Although the theoretical incapacity of deterministic generative models to achieve performance enhancement has been demonstrated, the naturalness in generation by HiFiGAN is inherently lower than that of DiffWave. This raises concerns about whether this disparity is the reason for HiFiGAN's lack of improvement. I would recommend the authors consider incorporating other diffusion-based generative models, such as FastDiff [1], or superior deterministic models like UnivNet [2], to bolster the robustness of their results.\n2. **Deterministic Model Upper Bound:** I disagree with the notion that deterministic models possess an upper bound. Recently, TF-GridNet results surpassed the so-called upper bound of deterministic models, attaining an SI-SDRi of 23.4, which is strikingly close to the SepFormer + DiffWave model. It would be prudent for the authors to include results on TF-GridNet to underscore the necessity of noise introduction in diffusion-based generative models. Training a generative model alone can be computationally demanding, and might not be the optimal solution just for a marginal performance gain.\n3. **Alignment Network Concerns:** Regarding the alignment network, the authors utilize the relative phase difference between $V_g$ and $V_d$ as well as the phase of $V_d$ as inputs to align the phase of the fused output with $V_d$. This poses a question: Is the observed enhancement in model performance a result of the phase alignment between $V_g$ and $V_d$, or is it due to the introduction of additional parameters, i.e., the alignment network? Another hypothesis worth considering is if the alignment network, when directly using the phase of $V_g$ and $V_d$ as inputs, would produce a similar effect.\n4. **Testing on Noisy Datasets:** One notable observation from the manuscript is its primary focus on clean datasets for evaluation. It would be beneficial to see how the proposed combined model performs on noisy datasets, such as WHAM! or the noisy version of Librimix. Evaluating on these datasets can provide insights into the model's robustness in more realistic scenarios, where environmental noise might significantly impact the performance of the generative model. Such an evaluation will offer a more comprehensive understanding of the model's real-world applicability and its ability to tackle inherent challenges posed by noisy environments.\n5. **Training Concerns:** The manuscript should clearly specify whether the separation and generative models were involved in the training of the alignment network $F$.\n6. **Symbol Representation:** Please ensure a consistent and standardized representation of symbols throughout the paper. Conventionally, vectors are denoted in boldface.\n\n[1] Huang R, Lam M W Y, Wang J, et al. Fastdiff: A fast conditional diffusion model for high-quality speech synthesis[J]. arXiv preprint arXiv:2204.09934, 2022.\n\n[2] Jang W, Lim D, Yoon J, et al. Univnet: A neural vocoder with multi-resolution spectrogram discriminators for high-fidelity waveform generation[J]. arXiv preprint arXiv:2106.07889, 2021."
                },
                "questions": {
                    "value": "My detailed questions are as described above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3618/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3618/Reviewer_9rFE",
                        "ICLR.cc/2024/Conference/Submission3618/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697641792994,
            "cdate": 1697641792994,
            "tmdate": 1700563188772,
            "mdate": 1700563188772,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QaB9zfP0Xz",
                "forum": "UXALv0lJZS",
                "replyto": "EdzRPGS77p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3618/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your very insightful feedback, which we will address as soon as possible + a quick note"
                    },
                    "comment": {
                        "value": "We would like to promptly address the second item, which suggests our work might lack theoretical soundness\n\nAs the reviewer correctly noted, TF GridNet achieved a 23.4dB gain with SISDR, while remaining deterministic. However, as Lutati et al. (2022) highlighted, this upper bound is valid only when each chunk is processed independently of the others. They write: \n>\u201cA clear limitation of the bound is that it holds only for a network D that jointly processes i.i.d stationary segments. This is not the case if a neural network processes the entire signal without segmentation.\u201c\n\nIt is important to note that TF GridNet's gain is influenced by the signal's length due to time-frequency ambiguity, while in real-world applications, signals often vary in length.\n\nWe will highlight this limitation in the revised version of our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699640699689,
                "cdate": 1699640699689,
                "tmdate": 1699640699689,
                "mdate": 1699640699689,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "495jUGDDbf",
                "forum": "UXALv0lJZS",
                "replyto": "EdzRPGS77p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3618/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comprehensive feedback.\n\n*Generative model selection:*\n\nOur proposed method relies on conditioning of the generative signal based on deterministic estimation of the signal. \n\nFastDiff\u2019s conditioning is based on text and thus is not in the scope of this work. \n\nUnivNet is a non-deterministic GAN, which has a similar input to that of the diffusion-based model that we use (DiffWave). It is indeed very interesting to evaluate since the GAN we employed in the paper (HiFiGAN) is deterministic and was employed to show that deterministic de-noising methods do not improve performance.\n\nFollowing the review, we have run UnivNet in combination with Gated LSTM and our phase alignment network. The results for Libri2Mix are reported in the table below (and added to Table 1 of the manuscript). Evidently, UnivNet and DiffWaveresults are within negligible differences of around 0.1dB. This strongly reinforces our main claim. \n\n*Deterministic Model Upper Bound:*\n\nAs the reviewer correctly stated, our upper bound depends on that of Lutati et. al., which assumes sequential processing of uncorrelated chunks. The bound does not hold for the time-frequency approach of TF-GridNet. While TF-GridNet obtains SOTA results on WSJ2 for two speakers (only), it does not mean that chunking-based methods are inferior: (1) TF-GridNet is unable to deal with non-stationary segments, such as silence, which do not appear in WSJ2 but are frequent in the actual applications, (2) TF-GridNet needs to be retrained for every signal length, and even worse (3) TF-GridNet can only handle relatively short signals. It is also worth noting that (4) time-frequency methods have a sharp decrease in performance as the sample frequency drops since the frequency domain estimation (using FFT) becomes blurred. Finally, (5) TF-GridNet was not shown to be effective for more than two speakers.\n\n\n*Alignment Network Concerns*\n\nFollowing the review, we have conducted three additional ablations that are added to the revised transcript:\nUsing the alignment network with $v_d$ twice.\nUsing  the alignment network with $v_d$ and phase-shifted $v_d$\nRetraining the alignment network with only the absolute phase without the difference of the phase. Then running as in our original method.\n\nIn all tests, the results of the ablation were worse,\n| Ablation Type                       | Libri2Mix | WSJ2MIX | Libri5Mix |\n|-------------------------------------|-----------|---------|-----------|\n| (a) Only $v_d$                            | 20.2      | 22.0    | 12.4      |\n| (b) Only $v_d$ shifted                    | 20.3      | 22.1    | 12.6      |\n| (c) Re-trained without phase difference | 21.3      | 23.3    | 13.7      |\n| Our Method                          | 21.5      | 23.9    | 14.2      |\n\nAs can be seen, the phase alignment network does not improve performance when the generative model\u2019s output is left out.\n\nThe alternative network architecture in (c) is somewhat inferior. In this case, the phase difference information exists only implicitly (i.e., it can be inferred from the two absolute phases but is not given explicitly). \n\n*Testing on Noisy Datasets:*\n\nFollowing the review, we evaluated our method on the WHAM! benchmark. We used a pretrained version of SepFormer over WHAM found in hugging faces. \n\n| Method                      | WSJ2Mix + WHAM |\n|-----------------------------|----------------|\n| Sepformer                   | 16.3           |\n| SepFormer + DiffWave (ours) | 16.8           |\n\nAs a side note: the additional independent noise in WHAM can be modeled, and a tighter upper bound can be computed for this dataset, using the insights of Fig. 1.\n\n*Training Concerns:*\n\nBoth pretrained networks are not fine-tuned and are not part of the training of the alignment network. \n\n*Symbol Representation:*\n\nWe thank the reviewer and we will make sure that the symbols are consistent."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561914749,
                "cdate": 1700561914749,
                "tmdate": 1700561914749,
                "mdate": 1700561914749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UW8bnKcxw8",
                "forum": "UXALv0lJZS",
                "replyto": "495jUGDDbf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3618/Reviewer_9rFE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3618/Reviewer_9rFE"
                ],
                "content": {
                    "title": {
                        "value": "Response authors"
                    },
                    "comment": {
                        "value": "For your response regarding TF-GridNet, I have some differing views. \n\n1. I don't believe that TF-GridNet cannot handle audio of varying lengths because, to my knowledge, the lengths of the data in the WSJ0 test set are also different. If you have conclusive evidence, please provide relevant literature or experimental results. \n\n2. Regarding the FFT method in the time-frequency domain, we can achieve better performance by setting appropriate window lengths and strides. Moreover, in reverberant situations, time-frequency domain methods often perform better than time-domain methods. \n\n3. As for the issue of not handling segments with silence, this requires evidence to be claimed. When datasets include similar scenarios, I believe the model can also learn similar separation paradigms.\n\nI also have a concern about the complexity and the number of parameters of the method proposed in the paper. Compared to deterministic models, these kinds of cascaded models often require more parameters and complexity, and I wonder whether this is necessary for enhancing performance.\n\nLastly, I thank the authors for providing additional experiments and analyses to address my concerns. I have decided to raise my score to 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700563110002,
                "cdate": 1700563110002,
                "tmdate": 1700563110002,
                "mdate": 1700563110002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B2E4s4EViS",
            "forum": "UXALv0lJZS",
            "replyto": "UXALv0lJZS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3618/Reviewer_d285"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3618/Reviewer_d285"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new method for the source separation problem. First, a discriminative model is used to produce output sources. Then a generative model is used to refine those signals conditioned on the output of the discriminative model. Finally, a learned mixing coefficient is used to blend between the generative and discriminative outputs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall I appreciate the method and the author's approach to using generative models. Generative models have shown strong performance in many areas and their use in source separation has been somewhat limited. I also appreciate the theoretical analysis which provides solid justification for the choices and results.\n\nThe ablation study shows that the the mixing network is in fact helpful, since the naive approach would be to just use the generative output v_g directly.\n\nIt is also nice that the authors use a variety of discriminative models and compare them, which shows that the method is general. \n\nThe output audio examples provide a good sense to the listener of the model's performance"
                },
                "weaknesses": {
                    "value": "The main issue I have is the usefulness of the theoretical bounds given the underlying assumptions. The paper build heavily on the analysis in Lutati et al. where the bounds were derived by making assumptions on the context used. These assumptions provide a bound that is not realistic, as evidenced by the fact that the bound for WSJ2 mix is 23.1dB but TF-Gridnet achieved 23.4dB gain using purely a discriminative complex valued model. This is something that should be discussed in the paper more."
                },
                "questions": {
                    "value": "I would like to see an ablation where only the generative output is used after conditioning on the discriminative output (not a simple average like the current ablation). Have you done those experiments and how did they perform?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698712522165,
            "cdate": 1698712522165,
            "tmdate": 1699636316998,
            "mdate": 1699636316998,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "men53eyoh3",
                "forum": "UXALv0lJZS",
                "replyto": "B2E4s4EViS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3618/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the detailed feedback.\n \nAs the reviewer correctly stated, our upper bound depends on that of Lutati et. al., which assumes sequential processing of uncorrelated chunks. The bound does not hold for the time-frequency approach of TF-GridNet. While TF-GridNet obtains SOTA results on WSJ2 for two speakers (only), it does not mean that chunking-based methods are inferior: (1) TF-GridNet is unable to deal with non-stationary segments, such as silence, which do not appear in WSJ2 but are frequent in the actual applications, (2) TF-GridNet needs to be retrained for every signal length, and even worse (3) TF-GridNet can only handle relatively short signals. It is also worth noting that (4) time-frequency methods have a sharp decrease in performance as the sample frequency drops since the frequency domain estimation (using FFT) becomes blurred. Finally, (5) TF-GridNet was not shown to be effective for more than two speakers.\n\n> I would like to see an ablation where only the generative output is used after conditioning on the discriminative output (not a simple average like the current ablation). Have you done those experiments and how did they perform?\n\nIn the paper, we demonstrate the need to average the output of the generative model with the deterministic method\u2019s output in Fig. 3, 4. \n\nFollowing the review, we have added to the appendix the results for Libr2iMix and Libri5Mix obtained without mixing the generative model output. These are provided twice:\n\nWithout aligning the phase to the source signal. \nLinearly aligning of the phase.\n\nIn both cases, the performance of the reconstruction with the generative part alone (of course, conditioned on the deterministic part) is worse than the reconstruction using the combination of both the deterministic and generative samples.\n\n| Method            | Libri2Mix | Libri5Mix |\n|-------------------|-----------|-----------|\n|$\\bar v_d$ (baseline)|             |              |\n| (a) $\\bar v_g$          | 19.8      | 10.2      |\n| (b) $\\bar v_g$ phase aligned | 20.0      | 11.3      |\n| Our full method        | 21.9      | 14.2      |\n\n\nEvidently, the generative method\u2019s output is not as effective as the deterministic one (reinforcing figures 3 and 4) but improves the performance when combined with it. We note that the theoretical analysis assumes this combination. $\\bar v_g$ by itself has the same bound as $\\bar v_d$. Only their sum is improved."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561489743,
                "cdate": 1700561489743,
                "tmdate": 1700561489743,
                "mdate": 1700561489743,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qMWjDjkF7i",
            "forum": "UXALv0lJZS",
            "replyto": "UXALv0lJZS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3618/Reviewer_fbeS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3618/Reviewer_fbeS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a diffusion-based post-processing module for single-channel speech enhancement. The authors present a mathematical derivation of the upper-bound of the source-to-distortion for generative methods, proving an improvement over the bound derived for deterministic models in prior work. They also present an architecture that combines the discriminative estimation and the generative estimation in the Fourier domain, which consists of a separation module, a generative module, and a mixing weights prediction module. Empirical results on a number of popular speech separation architectures on two speech separation datasets with multiple speaker numbers demonstrate the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of diffusion-based separation has been applied in speech separation, but the primary novelty of this work lies in the mathematical perspective of improving the SDR upper-bound with the generative approach by combining the output of the discriminative and generative estimations.\n\nThe experiments are conducted across several different separation architectures and for two popular source separation datasets (with several speaker-number settings), and an ablation study of the mixing network is performed. The empirical results demonstrate the improvement of the proposed method with deterministic SOTA on speech separation."
                },
                "weaknesses": {
                    "value": "My major concern about the current version of this manuscript is the clarity of the writing. There are a number of notations (e.g., $v_r, v_{gr}, v_{dr}$) that are used across multiple sections of the paper, but these notations are not easy to follow and the consistency could be improved. In particular:\n\n- Introduction could be clearer with all variables properly defined with types (real vs complex), and dimensions. Additionally, I suggest beginning with some motivation (reiterating parts of Section 2) but focusing on the bottleneck of the existing (deterministic and discriminative) approaches.\n\n- Please make sure to define the acronyms at the first usage (e.g., SDE in Section 2, GM in Section 3.1).\n\n- Please resolve the inconsistency\n  - notations between the opening paragraph ([$\\alpha_i, \\beta_i$] = F($\\bar{v}_d^i, \\bar{v}_g^i$) vs ([$\\alpha_i, \\beta_i$] = F($\\bar{V}_d^i, \\bar{V}_g^i$) in (4).\n  - $I(m_r, v_r)$ --> $I(m_r; v_r)$ in (6).\n  - $p(v_{d}r) --> p(v_{dr})$ in Section 3.1.\n  - The notations of $v_{gr}, \\bar{v}_{gr}$ and $v_{dr}, \\bar{v}_{dr}$ in Section 3.\n\n- In (10): it seems there is overloading of the notation $p(v_{gr})$ on LHS and RHS of the equation.\n\n- The font size of the equations and figure labels could be improved.\n  - In Figure 4 (a), the x-label \"MSE\" is in italics, whereas for (b) it is not.\n  - I would recommend disabling the italics for function names such as \"argmax\", \"ELBO\", \"SDR\", \"log\", etc."
                },
                "questions": {
                    "value": "- I'm uncertain on how the two inequalities in (11) are derived. For the first inequality, how is $I(v_r; v_{gr})$ related to $I(v_r; v_{dr},v_{gr})$ in (9)? The necessary condition for the second inequality is $I(v_r; v_{gr}, v_{dr}) \\leq I(v_r; m_r)$, but this is not implied from (7) or (9). It would be helpful if the authors could clarify the steps. Also, it'll be helpful to explain how the \"3.0\" db is obtained in (21).\n\n- Any reason for transposing the horizontal and vertical axes for the visualizations in Figure 5? It is conventional to display the spectral information in the vertical axis and temporal dimension horizontally.\n\nUpdate after rebuttal: I'd like to thank the authors for addressing the questions. The scores have been updated."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3618/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3618/Reviewer_fbeS",
                        "ICLR.cc/2024/Conference/Submission3618/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3618/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699177281027,
            "cdate": 1699177281027,
            "tmdate": 1700639752116,
            "mdate": 1700639752116,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "igIvbi5eLD",
                "forum": "UXALv0lJZS",
                "replyto": "qMWjDjkF7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3618/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3618/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comprehensive feedback.\n\nThe manuscript has been revised following the requested changes. \n\n$GM$ is defined in the introduction and then again in the beginning of the analysis section. SDR  in definition 3.1.\n\nTo answer the reviewer\u2019s questions:\n\n1. $I(v_r;v_{gr})$ is the mutual information between the source at the r-th chunk and the generative sample in the same chunk. $I(v_r;v_{dr},v_{gr})$ is the mutual information between the source and both the deterministic estimation and the generative sample. The first term is upper bounded by the second part by definition since additional signals can add information and not remove information. \n\n2. In Eq. 9 we show that the mutual information between the generative and source is upper bound by the sum of the mutual information of the source and mixture (first term) and the mutual information between the (1) source, and (2) the tuple of (generative sample, deterministic estimation). Since the generative sample is from approximately the same distribution as the deterministic distribution, the same data-processing theorem bound holds for it. Using Eq. 7 we can upper bound again the second term in Eq. 9  to obtain the second inequality.\nThe 3dB is due to the factor of 2 over the previous bound which translates to a 3dB addition in the logarithmic scale.\n\nIn the final version, we will recreate Fig. 5 of the appendix, replacing the two axes as requested."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3618/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562293728,
                "cdate": 1700562293728,
                "tmdate": 1700562293728,
                "mdate": 1700562293728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]