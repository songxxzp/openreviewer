[
    {
        "title": "Fine-grained Text-to-Image Synthesis with Semantic Refinement"
    },
    {
        "review": {
            "id": "FW7aqV97NR",
            "forum": "RauUgiw7VX",
            "replyto": "RauUgiw7VX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3059/Reviewer_AM6U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3059/Reviewer_AM6U"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces a diffusion-based technique that allows for fine-grained synthesis with semantic refinement. Rather than synthesizing from an entire descriptive sentence, users can highlight specific words. A semantic-induced gradient is incorporated at every denoising phase to assist the model in understanding the given words."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to understand.\n\nThe innovative approach of emphasizing specific words to enhance text-to-image synthesis is novel.\n\nThe inclusion of a user study offers valuable subjective evaluations."
                },
                "weaknesses": {
                    "value": "A primary concern is the limited comparison with recent methods. Considering the rapid advancements in the text-to-image research field (from GAN to Transformer to Diffusion in just three years), a sufficient comparison becomes more important. I commend authors  for leveraging the large-scale LAION dataset, which could reflect the image generation capabilities well. Could authors further include comparisons on this dataset with models like Make-A-Scene, DALL-E 2, CogView 2, Imagen, Parti, or Re-Imagen?  \n\nThere's a notable absence of the FID metric on LAION dataset. Even with potential variations across training datasets, there remains a requirement for an objective quantitative evaluation of all models. I believe this metric should be made accessible to the readers. In addition, incorporating the CLIP score, if possible, would be a commendable step.\n\nThe paper utilizes an additional database for reference image retrieval, enhancing its SEMANTIC REFINEMENT capability. Therefore, it's crucial to provide insights into how this database was constructed. It would also be beneficial to address how the methodology performs with unusual semantics (e.g., Pikachu) or distinct styles (e.g., Picasso-esque).\n\nThe novelty of the paper seems somewhat limited. The method primarily relys on CLIP for condition guidance. This dependence might influence the model's performance, especially in distinguishing subtle differences between similar images."
                },
                "questions": {
                    "value": "During the testing phase, how does this work automatically emphasize specific words within the sentence?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3059/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3059/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3059/Reviewer_AM6U"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697988135264,
            "cdate": 1697988135264,
            "tmdate": 1699636251225,
            "mdate": 1699636251225,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "eTLn9acH0B",
            "forum": "RauUgiw7VX",
            "replyto": "RauUgiw7VX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3059/Reviewer_qacC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3059/Reviewer_qacC"
            ],
            "content": {
                "summary": {
                    "value": "**Summary:** \nThe paper introduces a retrieval-based method for text-to-image generation. The authors argue that this approach can better align semantics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Advantages:** \n1. The writing quality of the paper is acceptable, although the foundational formulation doesn't introduce anything particularly novel.\n2. While the language is clear, the paper doesn't sufficiently distinguish its method from prior similar approaches."
                },
                "weaknesses": {
                    "value": "**Disadvantages:** \n1. The most significant issue with this paper is its lack of innovation.\n2. The results presented are just average. The methods compared are those published more than a year ago, and recent methods are notably absent."
                },
                "questions": {
                    "value": "**Concerns:** \n1. There have already been numerous methods based on retrieval. The core idea of this paper closely mirrors that of \"Re-Imagen\". It's challenging to discern any unique innovation. The authors' arguments on contributions don't point to any significant novelties. Moreover, the paper does not discuss differences from methods like \"Re-Imagen\", emphasizing its lack of substantial innovation.\n\n\n**References:** \n@article{chen2022re,\n  title={Re-Imagen: Retrieval-Augmented Text-to-Image Generator},\n  author={Chen, Wenhu and Hu, Hexiang and Saharia, Chitwan and Cohen, William W},\n  journal={arXiv e-prints},\n  pages={arXiv--2209},\n  year={2022}\n}\n\n@inproceedings{koh2021text,\n  title={Text-to-image generation grounded by fine-grained user attention},\n  author={Koh, Jing Yu and Baldridge, Jason and Lee, Honglak and Yang, Yinfei},\n  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},\n  pages={237--246},\n  year={2021}\n}\n\n@article{sheynin2022knn,\n  title={Knn-diffusion: Image generation via large-scale retrieval},\n  author={Sheynin, Shelly and Ashual, Oron and Polyak, Adam and Singer, Uriel and Gafni, Oran and Nachmani, Eliya and Taigman, Yaniv},\n  journal={arXiv preprint arXiv:2204.02849},\n  year={2022}\n}\n\n@article{blattmann2022semi,\n  title={Semi-parametric neural image synthesis},\n  author={Blattmann, Andreas and Rombach, Robin and Oktay, Kaan and M{\\\"u}ller, Jonas and Ommer, Bj{\\\"o}rn},\n  journal={Advances in Neural Information Processing Systems},\n  volume={11},\n  year={2022}\n}\n\n@inproceedings{liu2023more,\n  title={More control for free! image synthesis with semantic diffusion guidance},\n  author={Liu, Xihui and Park, Dong Huk and Azadi, Samaneh and Zhang, Gong and Chopikyan, Arman and Hu, Yuxiao and Shi, Humphrey and Rohrbach, Anna and Darrell, Trevor},\n  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},\n  pages={289--299},\n  year={2023}\n}"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648604295,
            "cdate": 1698648604295,
            "tmdate": 1699636251154,
            "mdate": 1699636251154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "YUiR8Wnb5t",
            "forum": "RauUgiw7VX",
            "replyto": "RauUgiw7VX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3059/Reviewer_mHH3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3059/Reviewer_mHH3"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the problem of misalignment between the text prompt and the generated image in current t2i models. The authors propose to incorporate a semantic-induced gradient into the denoising process to facilitate the alignment of text details and synthesized images.  Extensive experiments on several datasets verify the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The detailed analysis of why the text-to-image generation model fails to correctly match the text when it is complex is well done. \uff08Figure 2)\n2. The visualizations in Figure 5&6&7  demonstrate good text-image alignment, indicating the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. From the quantitative results in Table 1, we can see that the improvements in text alignment and detail matching are not impressive compared with other methods. For MM-HQ dataset, the proposed method performs similarly to LAFITE on both text alignment (the proposed method: 4.59 vs LAFITE : 4.54) and detail matching (the proposed method: 90.45% vs LAFITE : 89%) metrics. For CUB dataset, the proposed method also performs similarly to AttnGAN. Therefore I have some concerns about the effectiveness of the proposed method despite its qualitative results seeming good.\n2. The idea of using text prompts to retrieve similar images from a database as a condition for image synthesis bears some similarities with \"Retrieval-Augmented Diffusion Models\" and \"KNN-Diffusion: Image Generation via Large-Scale Retrieval\". Although the motivation of the proposed method seems different, this similarity diminishes the originality of the paper.\n3. There are no quantitative results for open-world text-to-image synthesis. It's preferable to randomly sample a validation dataset to assess the effectiveness of the proposed method across a wider spectrum."
                },
                "questions": {
                    "value": "1. The qualitative results (last row) in Figure 7 indicate that the generated images look blurrier than those from stable diffusion. Could the authors give a detailed analysis?\n2. The proposed method needs a database to get the reference image, therefore the diversity of the database is very important. What if there is no similar image that matches the sub-text prompt correctly?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721877336,
            "cdate": 1698721877336,
            "tmdate": 1699636251056,
            "mdate": 1699636251056,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "6rhacHSTOU",
            "forum": "RauUgiw7VX",
            "replyto": "RauUgiw7VX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3059/Reviewer_KP2Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3059/Reviewer_KP2Z"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces SeReDiff, a novel diffusion-based method that overcomes the limitations of existing text-to-image methods in generating fine-grained images that closely match the input text condition, especially lengthy text. SeReDiff also empowers users to emphasize some specific words to guide the generation more accurately. To achieve this purpose, the authors restructure the denoising pipeline of diffusion models by integrating the semantic-induced gradient as a reference input at each denoising step, alongside the image denoised from the previous step. Furthermore, the proposed approach does not rely on text annotations for training but can still achieve fine-grained text-conditioned synthesis in the inference phase."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "_ The authors provide appropriate analyses of the CLIP model, including the imperfect text-image matching as well as the preservation ratio of representation topology.\n\n_ The idea of using gradient to facilitate semantic refinement is interesting and it is complemented by a comprehensive mathematical proof.\n\n_ Extensive experiments are conducted to showcase the robustness of SeReDiff in generating high-quality images that align well with the input text condition when compared to many existing methods."
                },
                "weaknesses": {
                    "value": "_ The paper is quite difficult to follow.\n\n_ The experiment displayed in Table 2 lacks information regarding the generating resolution of each model, whereas this detail can affect the quality of the output images."
                },
                "questions": {
                    "value": "_ Have you considered applying your approach to the latent space instead of pixel space?\n\n_ When incorporating both database retrieval and a two-stage denoising process, does the proposed method significantly extend its runtime in comparison to other methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3059/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3059/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3059/Reviewer_KP2Z"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757158440,
            "cdate": 1698757158440,
            "tmdate": 1700755544618,
            "mdate": 1700755544618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]