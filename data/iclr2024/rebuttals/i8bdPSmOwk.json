[
    {
        "title": "Momentum-driven Noise-free Guided Conditional Sampling for Denoising Diffusion Probabilistic Models"
    },
    {
        "review": {
            "id": "XAfhPJKJKf",
            "forum": "i8bdPSmOwk",
            "replyto": "i8bdPSmOwk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4280/Reviewer_TEdQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4280/Reviewer_TEdQ"
            ],
            "content": {
                "summary": {
                    "value": "1.\tThe authors observe the unstable gradient in conditional sampling process. And the proposed momentum-driven gradient filtering algorithm to stabilize the conditional clean estimation gradient is sound, which uses the historical gradient to constrain the current gradient.\n2.\tThe motivation of the gradient suppression is also reasonable. The prediction error is larger at the early stage of denoising process, so the corresponding gradient should be suppressed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe authors observe the unstable gradient in conditional sampling process. And the proposed momentum-driven gradient filtering algorithm to stabilize the conditional clean estimation gradient is sound, which uses the historical gradient to constrain the current gradient.\n2.\tThe motivation of the gradient suppression is also reasonable. The prediction error is larger at the early stage of denoising process, so the corresponding gradient should be suppressed."
                },
                "weaknesses": {
                    "value": "1.\tAlthough the momentum gradient updating is reasonable, this may not be the first work to introduce momentum gradient updating to diffusion reverse sampling process. The work [1] has designed an adaptive momentum sampler to boost the performance, which also uses first-order and second-order. Since this work is also recent, so it is fine to me. Although that work focuses on unconditional sampling, I think maybe you should discuss the relation and difference between your work and that work.\n\n[1] Boosting Diffusion Models with an Adaptive Momentum Sampler\n\n2.\tFor the gradient suppression, I agree with the motivation that the direction error is larger at the early stage of the denoising process, and the corresponding sampling process should be adjusted. However, according to Algorithm1, the conditional gradient is suppressed. Since the semantic and layout is constructed when t is large, and the gradient compression is larger there, it may destroy the semantic matching between the final generated image and the condition. This may be more serious in some tasks with stricter requirement, such as inverse problems."
                },
                "questions": {
                    "value": "See the weakness part"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4280/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671128343,
            "cdate": 1698671128343,
            "tmdate": 1699636395752,
            "mdate": 1699636395752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p7tmTAVsKb",
                "forum": "i8bdPSmOwk",
                "replyto": "XAfhPJKJKf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4280/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your constructive feedback. We highly value your comments and address them as follows:\n\n> Q1: The relation and difference between our work and the recent work *Boosting Diffusion Models with an Adaptive Momentum Sampler*\n\nThe recent work titled *Boosting Diffusion Models with an Adaptive Momentum Sampler* leverages the momentum algorithm to ascertain the prior denoising direction from preceding timesteps in the context of an unconditional denoising process. Our work, however, takes a different path. We use the momentum algorithm to enhance the consistency and robustness of the guidance gradient during the conditional sampling process. Despite the similar functions of the first- and second-order momentum in smoothing the gradient direction and adaptively assigning the update step size in both studies, our motivation for using momentum algorithm and the problem we seek to solve diverge from the aforementioned work.\n\n\n\n> Q2: Since the semantic and layout is constructed when t is large, and the gradient compression is larger there, it may destroy the semantic matching between the final generated image and the condition. \n\nThanks for this valuable questions. We think for tasks that requires much stricter guidance conditions, such as segmentation map, the problem of semantic and layout mismatching may happen due to the misalignment between the unconditional denoising process and the given conditions. For such kind of tasks, we think excluding the gradient of the intial steps from the suppression scheme, or increasing the number of denoising steps can help to alleviate the problem."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700395550787,
                "cdate": 1700395550787,
                "tmdate": 1700395888336,
                "mdate": 1700395888336,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "422QIJ6TqR",
            "forum": "i8bdPSmOwk",
            "replyto": "i8bdPSmOwk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4280/Reviewer_SuQ1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4280/Reviewer_SuQ1"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an approach for conditional sampling in denoising diffusion probabilistic models (DDPM). It figures out that the performance gap between the clean and noised sample-based methods is mainly caused by the incorporation of estimation deviation in the clean-estimation process, and tries to solve it by implementing momentum-driven gradient filtering and a guidance suppression scheme. The proposed method exhibits superior performance in clean guided conditional image generation, and showcases its state-of-the-art capability in arbitrary style transfer tasks without the requirement of labeled datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The author theoretically analysis the drawbacks of the clean guided methods and provide some results to proof their claims. \n2. The author propose a method to solve the problem, and conduct experiments to its effectiveness.\n3. Except the style transfer part, the motivation is clear and the paper is well-written."
                },
                "weaknesses": {
                    "value": "1. About the style transfer task, I do not think it is appropriate to appear in this paper. According to my understanding, the goal of this paper is to bridging the performance gap between clean and noised sample-based methods. However, the experiment of style transfer task has no relationship with this topic. Therefore, I think the author better give an explanation about why conducting this part of experiments. Besides, according to the above reason, I also recommend the author to remove Section 2.3.\n2. Lacking the results of other works on improving clean-estimation guidance in style transfer task.\n3. The authors claim in Section 4.2 that the clean classifier+guidance function proves to be less reliable than the noise-robust classifier. Could the authors give a more direct proof like Figure 2?\n4. About the experiments in Section 5.1:\n1) the authors do not clarify which table is the results of this experiment\n2) the authors mention FreeDoM and Universal Guidance in the paper, but only provide the results of ADM+FreeDoM. What about the results of DiT+FreeDoM and DiT/ADM+Universal Guidance?\n5. The authors is recommended to prove the results of other methods on image generation task for comparison.\n6. In Section 5.2, there is no description about what is I_cs.\n7. In Section 5.3, the ablation results on style transfer task indicate that \"Raw+Filtering & Suppression\" can achieve the best results on L_content. However, it's results on L_style is only the third best. Does this indicate that \"Raw+Filtering & Suppression\" tends to preserve the original image's features rather than transferring to a new style?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4280/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4280/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4280/Reviewer_SuQ1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4280/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747121621,
            "cdate": 1698747121621,
            "tmdate": 1699636395665,
            "mdate": 1699636395665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PmmrSH8Jqi",
                "forum": "i8bdPSmOwk",
                "replyto": "422QIJ6TqR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SuQ1 (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your thoughtful reviews. We highly value your comments and address them as follows:\n\n> Q1: The experiment of style transfer task has no relationship with this topic. Therefore, I think the author better give an explanation about why conducting this part of experiments. Q2: Lacking the results of other works on improving clean-estimation guidance in style transfer task.\n\nWe appreciate the reviewer's query about the inclusion of the style transfer task in our paper. While we understand the concern, the purpose of incorporating this experiment was two-fold. \nThe first is that previous methods based on noised samples are not suitable for conditional generation tasks on unlabeled datasets, such as arbitrary style transfer. By achieving arbitrary style transfer, we underscore the potential and flexibility offered by our clean sample-based method.\nSecondly, the effectiveness of our proposed method is further demonstrated through comparisons and ablation studies on style-transfer task. For a more comprehensive analysis, we include a comparison with the results from FreeDoM as follows. To ensure a fair comparison, we set the total sampling steps for FreeDoM to be 1110 steps, incorporating its time-travel strategy, while our method uses 1000 ddpm steps for sampling. We find that results from FreeDoM contains dotted noise and shows a inferior sampling quality. As the results show, our method outperforms FreeDoM on both $ L_{content} $ and $ L_{style}$. The qualitative comparison will be updated later.\n\n | Metrics  |  Ours  | FreeDoM  | \n| ------ |:------:|---------:|\n|$\\mathcal{L}_{content}$|  4.70  |  6.30   |\n|$\\mathcal{L}_{style}$  |  1.54  |  2.10   |\n\n> Q3: The authors claim in Section 4.2 that the clean classifier+guidance function proves to be less reliable than the noise-robust classifier. Could the authors give a more direct proof like Figure 2?\n\nIn Appendix.C.2, we have supplemented our discussion with several sample sequnences of clean-estimations obtained during the denoising process in Figure 10. Upon examining these figures, it becomes evident that the clean-estimation is unstable during the denoising process(eg. in the red boxes). This instability subsequently results in an inconsistent guidance gradient throughout the denoising process.\n\n> Q4(1): The authors do not clarify which table is the results of this experiment.\n\nThanks for pointing out this issue. We have made the necessary addition in the PDF, referencing `As shown in Tab. 1' in Section 5.1.\n\n> Q4(2): The authors mention FreeDoM and Universal Guidance in the paper, but only provide the results of ADM+FreeDoM.\n\nWe have add the following comparison with DiT + FreeDoM here. We only provide the DiT + FreeDoM results here because FreeDoM is a follow-up work of Universal Guidance which shares a similar time-travel strategy and shows superior results. To ensure a fair comparison with our method, we have calibrated the total sampling steps for FreeDoM to closely align with ours. Our method yields superior outcomes on ADM model and style-transfer task, while demonstrates comparable results on DiT model. Given that DiT is a latent diffusion model, the guidance gradient $\\frac{\\partial \\mathcal{P}}{\\partial x_t}$ may exhibit a different pattern from ADM model. It may requires a futher tuning of our method's parameters.\n\n| Tables          | FID       |  sFID  | Precision | Recall |\n| --------------- |:---------:|:------:|:---------:|:------:|\n| DiT + raw clean |  3.54     |  5.22  |  0.80     |  0.56  |\n| DiT + FreeDoM   |  3.46     |  5.22  |  0.79     |  0.57  |\n| DiT + Ours      |  3.46     |  5.31  |  0.79     |   0.57 |\n\n> Q5: The authors is recommended to prove the results of other methods on image generation task for comparison.\n\nThank you for your suggestion. To the best of our knowledge, we have compared our method with a variety of clean sample-based guidance approaches. However, further comparisons with noised sample-based methods might not provide a fair evaluation, given the inherent differences between these two kinds of methodologies. Clean sample-based methods normally yield inferior results than the noised sample-based methods, while they offer substantial flexibility in the choice of guidance functions and eliminate the additional training cost for noise-finetuning."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700395458125,
                "cdate": 1700395458125,
                "tmdate": 1700395772431,
                "mdate": 1700395772431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S35RLU61Tp",
                "forum": "i8bdPSmOwk",
                "replyto": "422QIJ6TqR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4280/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SuQ1 (2/2)"
                    },
                    "comment": {
                        "value": "> Q6: In Section 5.2, there is no description about what is $I_{cs}$.\n\nThank you for highlighting this oversight. We have now updated Section 5.2 to include the necessary explanation:  `The differences between the content image $I_c$ and the stylized image $I_{cs}$ is expressed as'.\n\n\n> Q7: In Section 5.3, the ablation results on style transfer task indicate that \"Raw+Filtering \\& Suppression\" can achieve the best results on $ L_{content} $. However, it's results on $ L_{style} $ is only the third best.\n\nIn Section 4.3, we provide an overarching insight that suppressing the early gradient can enhance the sampling quality. We employ the simplest linear scheme to demonstrate this effect. However, we acknowledge that more effective suppression schemes may exist. For instance, we discovered that the suppression scheme $\\tilde{g_t} = \\frac{\\sqrt{\\bar\\alpha_t}}{\\sqrt{1-\\bar\\alpha_t}}\\bar{g_t}$ yields superior results for the style-transfer task, achieving both the lowest content error ($L_{content} = 4.69$) and the most optimal style error ($L_{style} = 1.46$). Moreover, we can readily strike a balance between the content error and style error by modulating the ratio of $\\lambda_c:\\lambda_s$."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700395487780,
                "cdate": 1700395487780,
                "tmdate": 1700456721900,
                "mdate": 1700456721900,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zYKMtEnDcJ",
            "forum": "i8bdPSmOwk",
            "replyto": "i8bdPSmOwk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4280/Reviewer_hNt5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4280/Reviewer_hNt5"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on conditional sampling of denoising diffusion probabilistic models with momentum-driven noise-free guidance. The proposed sampling method utilizes clean guidance functions, eliminating the need for additional training. Most importantly, they find that the primary reason for the worse performance of the previous noise-free guidance is by inaccurate clean estimations, especially during the early denoising stage. Then, they propose several simple yet effective methods for solving this problem. Furthermore, they demonstrate the feasibility and generalization capability on several downstream tasks like conditional image generation and style transfer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The studied research topic DDPMs' conditional sampling is important and interesting in AIGC, which can be easily embedded into multiple DDPMs.\n2. The authors found that the primary reason for the worse performance of the previous noise-free guidance is by inaccurate clean estimations, especially during the early denoising stage, which is also their main motivation.\n3. The authors proposed the momentum-driven gradient filtering to filter noise in the gradient.\n4. The experimental results on two downstream tasks including conditional generation and style transfer show that SOTA performance with many previous latest methods."
                },
                "weaknesses": {
                    "value": "1. Limited Novelty: We know that momentum can obtain a more stable updating process, but as shown in Fig. 2b, the trends of filtered gradient and unprocessed gradient are almost the same rather than a gradual stabilization process. The simple method proposed in this paper is not the most effective solution to prevent noise in the gradient, but only alleviates the problem of early stage noise to some extent\n2. Limited Performance Improvement: In Table 1 in Section 5.1, I note that the proposed method shows the limited improvement compared with 'raw clean guidance', e.g., DiT in sFID, Prec and Rec, and ADM in Prec and Rec.\n3. Missed Experiments: 1) In Table 1 in Section 5.1, I note that the authors only compared clean estimation-based conditional without comparing noise-guidance-based methods. I am curious whether the results generated by the proposed method are comparable to noise-guidance-based methods. 2) In figures 5, 6, and 7 of the qualitative results in the appendix, I can't see any comparisons with other baselines. Please add some experiments to show its ability. 3) what is the specific sampling cost in the experiments? Please add experiments to compare clean-estimation guidance techniques."
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4280/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760874046,
            "cdate": 1698760874046,
            "tmdate": 1699636395580,
            "mdate": 1699636395580,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lRcMmpK5bl",
                "forum": "i8bdPSmOwk",
                "replyto": "zYKMtEnDcJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4280/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4280/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable feedback. We highly value your comments and address them as follows: \n\n> Q1: In Fig. 2b, the trends of filtered gradient and unprocessed gradient are almost the same rather than a gradual stabilization process. The simple method proposed in this paper is not the most effective solution to prevent noise in the gradient.\n\nFigure 2b illustrates the degree of deviation between the current gradient direction and that of the previous timestep. A smaller value on the graph signifies a greater deviation. As depicted in the graph, the filtered gradient presents a substantially more consistent direction. Notably, the significant deviations at certain timesteps (for instance, t=650, t=940) are markedly reduced. Moreover, the experimental results outlined in Table 1 further reinforce the effectiveness of our approach. It is clear that our method significantly enhances the performance of the clean-guided DDPM, surpassing other methods by a considerable margin, thereby establishing itself as the most effective solution to date.\n\n\n> Q2: In Table 1 in Section 5.1, I note that the proposed method shows the limited improvement compared with `raw clean guidance', e.g., DiT in sFID, Prec and Rec, and ADM in Prec and Rec.\n\nFID is a well-acknowledged and widely-used metric for evaluating image generation models, with sFID, Precision, and Recall typically serving as auxiliary metrics for a more comprehensive evaluation (Li et al., 2022b, Peebles & Xie, 2022). We wish to underscore that our method exhibits marked improvements on the ADM model for both FID and sFID metrics. Additionally, it demonstrates substantial enhancement on the DiT model for FID. These results collectively attest to the effectiveness of our proposed method.\n\n[1] Peebles, W. and Xie, S., 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 4195-4205).\n \n[2] Zheng, G., Li, S., Wang, H., Yao, T., Chen, Y., Ding, S. and Li, X., 2022, October. Entropy-driven sampling and training scheme for conditional diffusion generation. In European Conference on Computer Vision (pp. 754-769). Cham: Springer Nature Switzerland.\n\n> Q3(1): In Table 1 in Section 5.1, I note that the authors only compared clean estimation-based conditional without comparing noise-guidance-based methods.\n\nWe did not compare with noise-guidance-based methods because our proposed method is specifically focused on enhancing clean-estimation-based methods. Notably, our approach empowers clean-estimation guidance to surpass the performance of noise classifier guidance in terms of both FID (4.20 vs. 4.59) and sFID (5.17 vs. 5.25), while maintaining comparable levels of precision and recall.\n\n> Q3(2): In figures 5, 6, and 7 of the qualitative results in the appendix, I can't see any comparisons with other baselines. Please add some experiments to show its ability.\n\nWe have add a qualitative comparison in Fig. 5 in Appendix.A.3.\n\n> Q3(3): What is the specific sampling cost in the experiments? Please add experiments to compare clean-estimation guidance techniques.\n\nWe have add the following comparison on the sampling cost in Appendix.A.5.  The inference time is averaged on 10 denoising process running on an NVIDIA A100 GPU.\n\n\n|               Methods                |   FID  |  sFID|  Sampling Cost |\n|  ------------------------------ |:------:|:-------:|:---------------:|\n| ADM + raw clean guidance | 4.99  |  5.58   |  54.16  seconds/sample |\n| ADM + FreeDoM                |  8.66 |  6.84 |  259.95 seconds/sample  |\n| ADM + Plug-and-Play        | 117.01 | 34.17 | 57.12 seconds/sample |\n| ADM + ED-DPM                 | 5.98  |   5.93   |  54.39 seconds/sample  |\n| **ADM + Ours**              | **4.20** | **5.17** |\t 54.20 seconds/sample  |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4280/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700394583433,
                "cdate": 1700394583433,
                "tmdate": 1700395730670,
                "mdate": 1700395730670,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]