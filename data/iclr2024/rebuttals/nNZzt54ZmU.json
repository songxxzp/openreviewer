[
    {
        "title": "Rethink Depth Separation with Intra-layer Links"
    },
    {
        "review": {
            "id": "pY32zNi2vs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_vJDK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_vJDK"
            ],
            "forum": "nNZzt54ZmU",
            "replyto": "nNZzt54ZmU",
            "content": {
                "summary": {
                    "value": "This work studies the depth-separation in the expressive power of neural networks. By allowing intra-layer links, it's shown that such neural networks can represent particular \"sawtooth\" functions efficiently with a small depth."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The results are correct."
                },
                "weaknesses": {
                    "value": "The results are meaningless: a very strong assumption is used to prove a very weak argument.\n\n**Intra-link is unjustified:** the motivation of considering neural networks with intra-layer links needs justification. There should be discussion on practical architectures with intra-layer links, which is crucial for establishing the usefulness of the proposed theory. ResNet and DenseNet are not appropriate for justification: they use across-layer links which are restricted (ResNet uses only identity mapping), while the links considered in this work are intra-layer with little restriction.\n\n**Intra-link is strong:** the network structure with intra-layer links seems too strong. In particular, we can show a much stronger argument that a two-layer intra-layer network can represent any classic neural network. In particular, suppose the target network we would like to represent has depth $k$ with width $w_1...w_k$, then it can be realized by a two-layer intra-layer network with the width of its hidden layer being $\\sum_i w_i$: we partition the neurons into $k$ groups with number $w_i$, then it simulates the target network by using intra-layer links from the $ith$ group to the next one. Effectively, a single layer of an intra-link network can be a classic network.\n\n**Weak results:** it's only proven that such strong network structure can represent particular hard-case functions used in previous depth-separation works. The minimum expectation for the conclusion is like \"this new network structure can represent any deep classic networks\", which I suspect can be easily obtained. However, the result only concerns the particular \"saw-tooth\" functions and is thereby very weak."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2609/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697140544663,
            "cdate": 1697140544663,
            "tmdate": 1699636200318,
            "mdate": 1699636200318,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xpr6DfWTax",
                "forum": "nNZzt54ZmU",
                "replyto": "pY32zNi2vs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to vJDK"
                    },
                    "comment": {
                        "value": "Dear Reviewer vJDK:\n\n**[Q1] Intra-link is unjustified.** \n\nThe depth separation theory suggests that depth is significantly more powerful than width, as reducing the depth will lead to the exponential width in expressing some functions. However, their comparison is always based on the standard fully-connected neural networks. In reality, most deep networks use shortcuts such as ResNet and DenseNet. There is a gap between the depth separation theory and the networks that are often deployed. Our motivation is to fill this gap by comparing the power of depth and width in the context of shortcuts, which is a more realistic setting. Particularly, we would like to investigate the question: Is the width still that weak in the new setting? If we use extra-layer links such as ResNet, critiques may arise that in an unrolled view, ResNet is a simultaneously wide and deep network, which makes it hard to examine the width. Therefore, we choose to add intra-layer links, which does not change the width. Moreover, from a symmetric viewpoint, adding extra-layer links can save the depth, and adding intra-layer links can save the width. \nWe have emphasized our motivation in this revision. \n\n\n**[Q2] Intra-link is strong: the network structure with intra-layer links seems too strong. In particular, we can show a much stronger argument that a two-layer intra-layer network can represent any classic neural network. In particular, suppose the target network we would like to represent has depth $k$ with width $w_1,\\ldots,w_k$, then it can be realized by a two-layer intra-layer network with the width of its hidden layer being $\\sum_i w_{i}$: we partition the neurons into $k$ groups with number $w_i$, then it simulates the target network by using intra-layer links from the $i$-th group to the next one. Effectively, a single layer of an intra-link network can be a classic network.**\n\n\nWhat you propose is unfortunately infeasible. \n\nA two-layer ReLU network can express any $\\mathbb{R} \\to \\mathbb{R}$ ReLU network. But the problem is efficiency. If we use a shallow network to express a deep network, the width we need is exponential, which is the key idea of depth separation. We also know that a two-layer intra-layer network can represent any classic neural network, as long as the width is sufficiently large. However, that is irrelevant to the depth separation theory. Our work is to show this gap can be greatly reduced if we use intra-layer links.\n\nBesides, we are afraid that your assertion is incorrect. Based on the tight bound analysis in our analysis, if the target network has depth $k$ with width $w_1,\\ldots,w_k$, the width we need is approximately $\\prod_{i} w_{i}$ for a two-layer network, instead of  $\\sum_{i} w_{i}$ as you asserted.\n\nFrom the perspective of the number of parameters, adding intra-layer links in a fully-connected network is no more than doubling the number of depth, which means that intra-links are not strong assumptions. But what we have realized is indeed significant. Based on [Telgarsky 2015], doubling the depth of a fully-connected network only reduces the width from $w$ to $w^{1/2}$. However, intra-linking $n$ neurons within a layer can bring an exponential saving for the width based on our dedicated construction.\n\nWe sincerely hope that you can consider raising your score, as our work is not as straightforward as you think.\n\n**[Q3] Weak results: it's only proven that such strong network structure can represent particular hard-case functions used in previous depth-separation works. The minimum expectation for the conclusion is like \"this new network structure can represent any deep classic networks\", which I suspect can be easily obtained. However, the result only concerns the particular \"saw-tooth\" functions and is thereby very weak.**\n\nAs is claimed in  [Q2], our goal is to extend current depth separation theory to shortcut architecture. Our result shows depth separation can be modified with intra-layer links, instead of the ability to represent any networks. The statement of depth separation is that there exists a function representable by a deep network which cannot be represented by a shallow network whose width is lower than a large threshold. So, some specific functions are needed to support depth separation theory. \n\nFurthermore, the sawtooth function is not a hardcase function. Actually, it is a fundamental function in network approximation theory (c.f., [1],[2]). Therefore, most of the relevant approximation results can be easily extended to intra-linked structures, which can greatly save width compared to conventional networks.\n\n\nReference:\n[1]Lu, Jianfeng, et al. \"Deep network approximation for smooth functions.\" SIAM Journal on Mathematical Analysis 53.5 (2021): 5465-5506.\n[2]Yarotsky D. Error bounds for approximations with deep ReLU networks[J]. Neural Networks, 2017, 94: 103-114."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2609/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231724984,
                "cdate": 1700231724984,
                "tmdate": 1700231724984,
                "mdate": 1700231724984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yfr7SK7rgm",
                "forum": "nNZzt54ZmU",
                "replyto": "xpr6DfWTax",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2609/Reviewer_vJDK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2609/Reviewer_vJDK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. \n\nWhen we talk about width/depth/layer, these words have to be put under a specific context for a rigorous definition. For the classic network structures, when we say depth separation, we mean there exists a deep network, when approximated by a shallow one it requires exponentially more parameters.\n\nNow, let me define a new network structure, where I call the whole network a \"layer\", then I claim every network in this structure has one layer and thus magically breaks the depth separation. \n\nClearly this is wrong claim. Why? Because I made the straw man fallacy: the meaning of depth is different in the two network structures.\n\nIntra-link network has a similar issue: a two-layer intra-link network contains classic deep networks as a subset, therefore it makes no sense to compare depth between intra-link network and classic network. The way you compare the depth is misleading.\n\nThe theme of depth separation in classic network is polynomial diversity (function addition brought by width) versus exponential efficiency (function composition brought by width). A shallow network has to use exponentially more parameters by using function addition to imitate function composition which it's not good at. This is the reason depth separation theory only concerns the hardness of approximating certain functions, like the saw-tooth, which naturally fits depth more than width. However, the way intra-link network breaks the depth separation is by including deep classic networks as a subset, this is a tautology."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2609/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663201936,
                "cdate": 1700663201936,
                "tmdate": 1700663201936,
                "mdate": 1700663201936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iK5DYjrq4x",
            "forum": "nNZzt54ZmU",
            "replyto": "nNZzt54ZmU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_b7mN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_b7mN"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a class of models with the goal of increasing the representational capabilities of shallow networks. It does so by introducing \"intra-layer links\", i.e. each neuron is directly linked (through addition) to other neurons in the same layer. In particular, the paper studies the cases where (1) each neuron depends on its proceedings neurons, and (2) every two neurons are linked. They show that this model can approximate the sawtooth function with a significant reduction in width compared to a model of the same depth without intra-layer links. In particular, the reduction in width is exponential. The authors imply that these results should spark reflection in the field of depth separation theory, where depth is often assumed to play a crucial role in efficiently approximating certain classes of functions such as the sawtooth function. The authors also show preliminary results on common datasets (such as CIFAR100 and Tinyimagnet and others)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written, the authors nicely present the background concepts on depth separation required to understand the manuscript. \n2. The differences in terms of extra-linked neural network vs intra-linked nets are nicely explained through Theorem 4 and 6, where it is shown the upper bound on the number of different pieces that each variant can represent (a factor of $w_i + 1$ per layer for extra-linked, and $2^{n_i}-1/n w_i + 1$ for intra-linked. \n3. Theorem 10 nicely explains the effect of intra-linked layers in terms of function approximation. Lemma 11 specializes Theorem 10 to the sawtooth function. In general, I enjoyed the cleanness and preciseness of the exposition of the theorems.\n4. Although I am not entirely familiar with the field of approximation theory, I would imagine this paper's idea of proposing novel architecture with desirable function approximation properties to spark interest in the community."
                },
                "weaknesses": {
                    "value": "1. I appreciate that the authors put forward an extensive explanation of why they think that the intra-linked layer is different from stacking layers. From an information propagation perspective, however, I still struggle to agree with the authors. The quantity $\\tilde{f}_i^{(j)}$ as in the equation of Notation 2 depends on the previous $p < j$ preactivations $g_i^{(0)}, \\dots, g_i^{(p-1)}$. This is similar to add skip connection, where the information is residually added on a neuron-by-neuron way to form the representation of the next neuron. Of course, as it authors point out, it is not the standard way depth is thought in neural networks, and there are differences in terms of the absence of extra weights, and different handling of nonlienarities, which also has an impact on the function class representable and the mechanics of producing linear pieces.  \n2. Related to (1). I am not entirely convinced of why the authors have centered the story around the role of width vs depth. I think the intra-layer mechanism is something out of the scope of depth-vs-width in the way fully connected neural networks are treated in the dept separation theory. In the depth separation theory, the objective is to study the roles of width and depth in the \"natural\" way they are treated in the literature. In the paper, the authors design a new architecture in which each layer has something that resembles depth, and claim that the paper's objective is to \"inspire further contemplation and discussion regarding the depth separation theory\", which in my honest view is misleading. I would like the authors' clarification on this point. \n3. The potential of the architecture for practical usage seems very limited. The computation is inherently sequential and thus cannot make use of existing hardware optimization through parallelization."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2609/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698451453488,
            "cdate": 1698451453488,
            "tmdate": 1699636200212,
            "mdate": 1699636200212,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2votkBIYM1",
                "forum": "nNZzt54ZmU",
                "replyto": "iK5DYjrq4x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to b7mN"
                    },
                    "comment": {
                        "value": "Dear Reviewer b7mN:\n\nWe would like to thank you for your recognition of the presentation and novelty in our work. Here, we address your concerns and answer your questions. We look forward to more discussions with you!\n\n**[Q1] I appreciate that the authors put forward an extensive explanation of why they think that the intra-linked layer is different from stacking layers...which also has an impact on the function class representable and the mechanics of producing linear pieces.**\n\nThanks for these insightful comments. Let us use a simpler example to illustrate this point. Suppose we have a fully-connected network with width=$w$ and depth=$k$ to express a sawtooth function, we have two choices to reduce the width to express the same sawtooth function. The first is to increase the depth of this fully-connected network from $k$ to $nk$, based on [Telgarsky 2015], the width is reduced from $w$ to $w^{1/n}$. The second is to intra-link $n$ neurons in the network, which can bring an exponential saving for the width based on our construction. Therefore, our result is indeed highly non-trivial and provides a new insight into the depth separation theory.\n\n\nDefining the width and depth of a network with shortcuts is indeed tricky. For example, the famous ResNet can be unfolded into a simultaneously wide and deep network whose width and depth are the same. However, few researchers will take the ResNet as a wide network and attribute its success to width instead of depth, thereby vetoing the success of deep learning. To avoid such a conflict,\nin our draft, we define the width and depth as\n\n**[Width and depth of intra-linked networks, (Fan-Lai-Wang, JMLR2023)]** Given an intra-linked network $\\mathbf{\\Pi}$, we delete the intra-layer links layer by layer to make the resultant network $\\mathbf{\\Pi}'$ a standard fully-connected network, which means it has no isolated neurons and shortcuts. Then, we define the width and depth of $\\mathbf{\\Pi}$ to be the same as the width and depth of $\\mathbf{\\Pi}'$.\n\nSuch a definition well aligns with our conventional understanding of width and depth, compared to using the broadest concatenating neurons as the width and the longest path as the depth. \n\n\n**[Q2] Related to (1). I am not entirely convinced of why the authors have centered the story around the role of width vs depth. I think the intra-layer mechanism is something out of the scope of depth-vs-width in the way fully connected neural networks are treated in the dept separation theory...**\n\nThanks for these insightful comments. The depth separation theory suggests that depth is significantly more powerful than width, as reducing the depth will lead to the exponential width in expressing some functions. However, their comparison is always based on the standard fully-connected neural networks. In reality, most deep networks use shortcuts such as ResNet and DenseNet. There is a gap between the depth separation theory and the networks that are often deployed. Our motivation is to fill this gap by comparing the power of depth and width in the context of shortcuts, which is a more realistic setting. Our theoretical results suggest that with intra-linked networks, the width needed is greatly reduced. Thus, we derive a new relationship between width and depth in a different setting, which provides a different perspective than what is suggested in the depth separation theory. \n\nIn this revision, we have clarified the gap between the existing depth separation theory and reality and explained our motivation to provide a new relationship between width and depth in a more realistic setting.\n\n\n**[Q3] The potential of the architecture for practical usage seems very limited. The computation is inherently sequential and thus cannot make use of existing hardware optimization through parallelization.**\n\nThanks for this suggestion. We agree with you that the usage of intra-layer links may hurt the hardware optimization to some extent. However, we still think intra-linked networks are a promising architecture from two aspects:\n\n- Our experiments on 5 synthetic datasets, 15 tabular datasets, and 2 image benchmarks demonstrate that intra-linked networks can achieve better or comparable performance with fewer parameters. In memory-constraint scenarios, the intra-linked networks may be preferred. \n\n- We can design acceleration algorithms for intra-linked networks. Specifically, the acceleration of RNNs and LSTMs has been intensively investigated. We can translate ideas therein such as sequence bucketing [1] to solve the training issues of intra-linked networks.\n\n[1] Khomenko, V., Shyshkov, O., Radyvonenko, O., and Bokhan, K. (2016, August). Accelerating recurrent neural network training using sequence bucketing and multi-gpu data parallelization. In 2016 IEEE First International Conference on Data Stream Mining and Processing (DSMP) (pp. 100-103). IEEE."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2609/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232102034,
                "cdate": 1700232102034,
                "tmdate": 1700232102034,
                "mdate": 1700232102034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mtKSYhSTj7",
                "forum": "nNZzt54ZmU",
                "replyto": "2votkBIYM1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2609/Reviewer_b7mN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2609/Reviewer_b7mN"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for clarifying my questions, especially regarding the difference between depth and intra-layer links. Indeed it can be tricky to define the concept of depth when such modifications are performed. In my view, the fact that each neuron depends on the previous neurons in the same layer is a feature typically attributed to depth. However, I also see the authors' point that in terms of function approximation intra-layer links have different properties from the classical view of depth. I keep my score for now."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2609/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661793003,
                "cdate": 1700661793003,
                "tmdate": 1700661793003,
                "mdate": 1700661793003,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2Nx6nE8U2E",
            "forum": "nNZzt54ZmU",
            "replyto": "nNZzt54ZmU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_45bo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_45bo"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses theoretically about whether width of deep neural networks is always significantly weaker than depth. It introduces intra-layer links, and shows that width can also be powerful when armed with them."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper recognizes an important limitation of depth-width comparison theory - it is always based on the fully-connected networks. It adds interesting and novel points to the power of width.\n\n- The paper includes comprehensive theorems and proofs regarding its points, and also includes both synthesized and real-world experiments."
                },
                "weaknesses": {
                    "value": "- The significance of theoretical contribution should be made more clear in the paper. As a reviewer outside of the learning theory field, I cannot conclude how much the improvement is compared to prior works. \n\n- Based on my understanding, the depth of a neural network is the number of neurons on the longest path from the input to the output, and thus adding an intra-link is equivalent to inserting a new layer, which doubles the depth of the neural network. Hence, I do not agree with the claims in the paper saying that the width can also be powerful when armed with intra-links."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2609/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716081216,
            "cdate": 1698716081216,
            "tmdate": 1699636200041,
            "mdate": 1699636200041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "svRzahF8jo",
                "forum": "nNZzt54ZmU",
                "replyto": "2Nx6nE8U2E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 45bo"
                    },
                    "comment": {
                        "value": "Dear Reviewer 45bo:\n\nWe would like to thank you for your recognition of the strengths in our work. Here, we address your concerns and answer your questions. We look forward to more discussions with you!\n\n**[Q1] The significance of theoretical contribution should be made more clear in the paper. As a reviewer outside of the learning theory field, I cannot conclude how much the improvement is compared to prior works.**\n\nThanks for your advice! The improvement compared to prior works is indeed significant, which can be summarized as follows:\n\n-  With inner links, a network of depth $k$ and width $w$ has $\\mathcal{O}(\\frac{2^{wk}}{w^k})$ times amount of pieces compared to conventional networks.\n\n- When expressing a sawtooth function represented by a fully-connected $2k^2+1$-layer ReLU DNN with $w$ nodes in each layer, a classical network with $(k + 1)$ needs width at least $w^k$ neurons in each layer. However, with inner links, a network with $(k + 1)$ only needs no more than $k\\cdot log_{2} w+2$ neurons in each layer to express such functions. The width is exponentially reduced.\n\n**[Q2] Based on my understanding, the depth of a neural network is the number of neurons on the longest path from the input to the output, and thus adding an intra-link is equivalent to inserting a new layer, which doubles the depth of the neural network. Hence, I do not agree with the claims in the paper saying that the width can also be powerful when armed with intra-links.**\n\nWe agree with you that a network with intra-layer links can be unfolded into a dense network with a special arrangement. Actually, any network with shortcuts can be unfolded, too. However, inserting intra-layer links is intrinsically different from increasing the depth. Following your thoughts, can doubling the depth of a fully-connected network make the width exponentially reduced? The answer is unfortunately negative. From the perspective of the number of parameters, adding intra-layer links in a fully-connected network is no more than doubling the number of depth. Based on [Telgarsky 2015], doubling the depth of a fully-connected network only reduces the width from $w$ to $w^{1/2}$. However, intra-linking $w$ neurons within a layer can bring an exponential saving for the width based on our construction. Therefore, our result is indeed highly non-trivial and provides a new insight into the depth separation theory.\n\nIn fact, allowing increasing depth, the deeper feedforward network has a larger function class than a shallow intra-linked network, and the function class of our intra-linked network is a proper subset of a much deeper fully-connected network. However, given the same width and depth, our intra-linked network has more expressive power (i.e., number of pieces, VC dimension, than a feedforward network per neuron or per parameter. \n\nFor example, we consider the function class represented by a 2-layer ReLU DNN with width 2. The function class of such networks without links has at most 3 pieces, and its VC dimension is 3. However, the function class of inner linked networks has at most 4 pieces, and its VC dimension is 4. This phenomenon can be seen as an analog to the comparison between CNNs and fully-connected NNs. The function classes of CNNs are just subsets of the function classes of fully-connected NNs with some further restrictions on the weights. However, CNNs usually have more expressive power per parameter and achieve better results in practice.\n\nIn addition, defining the width and depth of a network with shortcuts is indeed tricky. For example, the famous ResNet can be unfolded into a simultaneously wide and deep network whose width and depth are the same. However, few researchers will take the ResNet as a wide network and attribute its success to width instead of depth, thereby vetoing the success of deep learning.\nIn our draft, we define the width and depth as\n\n**[Width and depth of intra-linked networks, (Fan-Lai-Wang, JMLR2023)]** Given an intra-linked network $\\mathbf{\\Pi}$, we delete the intra-layer links layer by layer to make the resultant network $\\mathbf{\\Pi}'$ a standard fully-connected network, which means it has no isolated neurons and shortcuts. Then, we define the width and depth of $\\mathbf{\\Pi}$ to be the same as the width and depth of $\\mathbf{\\Pi}'$.\n\nSuch a definition well aligns with our conventional understanding of width and depth, compared to using the broadest concatenating neurons as the width and the longest path as the depth.  \n\nLastly, we agree with you that we might overclaim the power of width since the saving of width is due to the intra-layer links. In this revision, we have lowered our tone from contending the width is also powerful to contending that the width is not that weak. Earlier depth separation theory suggests that an exponential width is comparable to the constant depth, while our result suggests that in the context of intra-layer links, the exponential width is not necessary."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2609/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232273608,
                "cdate": 1700232273608,
                "tmdate": 1700232273608,
                "mdate": 1700232273608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ilHluOIudU",
            "forum": "nNZzt54ZmU",
            "replyto": "nNZzt54ZmU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_Nadz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_Nadz"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies depth-width tradeoffs in neural networks where intralinks within the layers are allowed. The main claim to fame is that intralinks give rise to unexpected behaviour as far as experessivity depth-width bounds are concerned. In particular, by allowing intralinks between neurons of the same layer, the authors show that the network can efficiently represent certain highly oscillatory functions that have been previously used to show lower bounds for the width of shallow neural networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+I believe the paper studies an interesting question regarding the expressivity of neural nets with added intra links.\n\n+The conceptual contribution that intra links significantly help the representation capabilities of neural nets w.r.t. their width requirements is nice."
                },
                "weaknesses": {
                    "value": "- I found most of the results and derived bounds relatively straightforward given the works of Telgarsky, Montufar etc. \n\n- I have trouble understanding why intra links is not effectively the same as adding extra layers to the network. If this is the case, I believe the results are not surprising as they should follow with simple variations from known depth-width bounds.\n\n-the architecture with intralinks as shown in Fig. 1c, I am not sure it has been popular or widely used in empirical studies. So people that are more interested in experimental performance, I am not sure how they will interpret these results for networks they don't really use."
                },
                "questions": {
                    "value": "-Can the authors provide simple examples for why an intralinked network cannot directly be used to \"simulate\" a deep network? Can they elaborate more on their explanation on page 5 top? \n\n-What is the main novelty in the constructions? Let's for now agree that intralinked networks are interesting and there is some conceptual contribution there. The effect they have in representing sawtooth functions should be relatively straightforward so I can't understand exactly the technical novelty of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2609/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698995568956,
            "cdate": 1698995568956,
            "tmdate": 1699636199957,
            "mdate": 1699636199957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aR4kpJHLJ4",
                "forum": "nNZzt54ZmU",
                "replyto": "ilHluOIudU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Nadz (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer Nadz:\n\nWe would like to thank you for your recognition of the conceptual contribution to our work. Here, we address your concerns and answer your questions.\n\n**[Q1] I found most of the results and derived bounds relatively straightforward given the works of Telgarsky, Montufar etc.**\n\nOur results are non-trivial extensions of previous works. The method we estimate the bound of networks is completely different from previous works of Aurora, Montufar etc. On the one hand, we identify conditions for the tightness of the bound, which has been proven to be stronger than existing results. Specifically, in the activation step, we distinguish the existing and newly generated breakpoints to avoid repeated counting, and then in the following pre-activation step, we maximize the oscillation to yield the most pieces after the next activation. On the other hand, the construction of functions in our work, i.e., constructing oscillations by preserving existing breakpoints and splitting each piece into several ones, is generic in analyzing networks. Because of the decoded mechanisms of generating more pieces and oscillations, \n\n- Our bound estimate for conventional networks is much tighter than previous results;\n\n- We also give bound analysis for linked networks, which is applicable for both inner and outer links; \n\n- We also show the tightness of our bound with explicit construction.\n\n- We can construct an intra-linked network whose width needed can be exponentially reduced.\n\n**[Q2] I have trouble understanding why intra links is not effectively the same as adding extra layers....** \n\nSorry for the confusion. Any network with shortcuts can be reshaped into a standard fully-connected network. For example, the famous ResNet can be unfolded into a simultaneously wide and deep network whose width and depth are the same. However, few researchers will take the ResNet as a wide network and attribute its success to width instead of depth, thereby vetoing the success of deep learning. The use of intra-layer links is fundamentally different from adding depth. From the perspective of the number of parameters, adding intra-layer links in a fully-connected network is no more than doubling the number of depth. Based on the depth separation theory [Telgarsky 2015], when doubling the depth of a fully-connected network, the width is only reduced from $w$ to $w^{1/2}$. However, our saving for the width is exponential instead of polynomial, which means that adding intra-layer links has an essentially different mechanism from adding depth. We have illustrated this point in this revision.\n\n\nWe think a much deeper feedforward network has a larger function class than a shallow intra-linked network, and the function class of our intra-linked network is a proper subset of a much deeper feedforward network. However, given the same width and depth, our intra-linked network has more expressive power (i.e., number of pieces, VC dimension, than a feedforward network per neuron or per parameter. Since the function class is different, the result is of course not straightforward.\nFor example, we consider the function class represented by a 2-layer ReLU DNN with width 2. The function class of such networks without links has at most 3 pieces, and its VC dimension is 3. However, the function class of inner linked networks has at most 4 pieces, and its VC dimension is 4. \n\nThis phenomenon can be seen as an analog to the comparison between CNNs and fully-connected NNs. The function classes of CNNs are just subsets of the function classes of fully-connected NNs with some further restrictions on the weights. However, CNNs usually have more expressive power per parameter and achieve better results in practice.\n\n**[Q3] The architecture with intra-links as shown in Fig. 1c, ...**\n\nExploring new and powerful network architectures such as the invention of ResNet has been the mainstream research direction in deep learning in the past decade. Although shortcuts have been widely adopted in network design, to the best of our knowledge, we are the first to consider adding shortcuts within a layer in a fully-connected network. Like the residual connections, we spotlight that no matter how many neurons are linked, the improvement of representation power by intra-layer links increases no trainable parameters for a network. Thus, the intra-layer link is an extremely economical add-on to the model, which has the great potential of enhancing model compactness. Furthermore, our analysis demonstrates that if intra-linking more neurons in a layer, the improvement in the network expressivity can be exponential. Finally, We also empirically confirm the good regression and classification performance of networks with intra-layer links via 5 synthetic datasets, 15 tabular datasets, and 2 image benchmarks in Appendix H). Therefore, we think our comprehensive theoretical analysis and encouraging experimental results can engage people focusing on practical applications."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2609/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232578709,
                "cdate": 1700232578709,
                "tmdate": 1700232578709,
                "mdate": 1700232578709,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cJQYxF1jxq",
            "forum": "nNZzt54ZmU",
            "replyto": "nNZzt54ZmU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_4fqv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2609/Reviewer_4fqv"
            ],
            "content": {
                "summary": {
                    "value": "Authors consider a relatively new architectural tool for neural networks through what is known as intra-layer links, where one can have a link inside a hidden layer between two neurons and then re-visit the depth separation problem in this context. They consider the depth separation between a $k^2$ vs $k$ hidden layer networks as considered by Telgarsky (hard instances/functions) and show that a shallow network with every two neurons in each hidden layer linked via intra-layer links requires at most $\\log(w)k+2$ width, while a shallow network of a standard feedforward network would require at least $w^k$ width, hence showing a possibility of gaining additional expressive power with shallow networks. They also perform experiments on synthetic and real data which seem to agree with their theoretical findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Provides a way to potentially reduce the number of parameters, i.e, use shallower networks with intra-links, since these links do not contain any trainable parameters.\n\n2) They also show that ResNets and DenseNets have (potentially) higher representation power by looking at the maximum number of pieces that can be generated by them and compare to a standard DNN."
                },
                "weaknesses": {
                    "value": "1) These intra-links are not used at all in practice and could end up being quite artificial, although ResNets (which have extra-layer links) are proven to be useful in practice.\n\n2) I feel that the authors could do a better job in explaining possible trade-offs of adding these links (see questions for more details)."
                },
                "questions": {
                    "value": "1) The meaning of zero-points must be defined clearly.\n\n2) Are Bi-directional links of any help?\n\n3) What are the trade-offs for adding as many links as possible for a fixed architecture? Meaning why is one not incentivized to add links between all possible neurons?\n\n4) In the experiments could you indicate how many links were added and is there an effect on optimization/generalization/efficiency on increasing the number of links?\n\n5) When is it better to add extra-layer link vs intra-layer link?\n\n6) Are there are more experiments where authors compare the performance to ResNets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2609/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699480873405,
            "cdate": 1699480873405,
            "tmdate": 1699636199870,
            "mdate": 1699636199870,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YCmjncs27g",
                "forum": "nNZzt54ZmU",
                "replyto": "cJQYxF1jxq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2609/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 4fqv (Part I)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4fqv:\n\nWe would like to thank you for your recognition of the strengths in our work. Here, we address your concerns and answer your questions. We look forward to more discussions with you!\n\n**[Q1] These intra-links are not used at all in practice and could end up being quite artificial, although ResNets (which have extra-layer links) are proven to be useful in practice.**\n\nThe contributions of this draft are mainly theoretical. The depth separation theory suggests that depth is significantly more powerful than width, as reducing the depth will lead to the exponential width in expressing some functions. However, their comparison is always based on the standard fully-connected neural networks. In reality, most deep networks use shortcuts such as ResNet and DenseNet. There is a gap between the depth separation theory and the networks that are often deployed. Our motivation is to fill this gap by comparing the power of depth and width in the context of shortcuts, which is a more realistic setting. Our theoretical results suggest that with intra-linked networks, the width needed is greatly reduced. Thus, we derive a new relationship between width and depth in a different setting, which provides a different perspective than what is suggested in the depth separation theory. \n\nAt the same time, as a side product of our theoretical analysis, although the intra-linked network is not popular now, it has the potential to be a well-performing architecture. First, it has good expressive power. Second, the intra-layer link is an extremely economical add-on to the model, which has the great potential of enhancing model compactness. Finally, We also empirically confirm the good regression and classification performance of networks with intra-layer links via 5 synthetic datasets, 15 tabular datasets, and 2 image benchmarks in Appendix H). Therefore, we believe people focusing on practical applications should also be interested in this novel structure. We will intensively investigate this architecture in the future.\n\n**[Q2] The meaning of zero-points must be defined clearly.**\n\nSorry for the confusion. We should define that the zero-point $x\\in\\mathbb{R}$ of a function $f$: $\\mathbb{R}\\to\\mathbb{R}$ satisfies $f(x)=0$. We have added this definition in this revision.\n\n**[Q3] Are Bi-directional links of any help?**\n\nThanks for your helpful suggestion. Bidirectional links are an interesting direction to explore. In the seminal Hopfield network, bidirectional links are used to construct the memory and query the memory. We notice that the decoded mechanism of producing pieces and oscillations cannot be directly extended into analyzing bidirectional links. We will consider this idea in our future work.\n\n**[Q4] What are the trade-offs for adding as many links as possible for a fixed architecture? Meaning why is one not incentivized to add links between all possible neurons?**\n\nWe notice that there are two kinds of trade-offs arising from adding intra-layer links:\n\n- The computational complexity: let us analyze the characteristics of an intra-lined network. First, it is straightforward to see that using intra-layer links increases a few parameters. But even if only every two neurons are intra-linked in a layer, the improvement is exponentially dependent on depth, \\textit{i.e.}, approximately $\\mathcal{O}(\\frac{3}{2})^{k}$, which is considerable when a network is deep. Therefore, they can serve as an economical yet powerful add-on to the model. Second, the complexity of computing a layer with $W$ neurons in a classical ReLU DNN is $W^2$ multiplications and $W^2$ additions while computing an intra-layer linked ReLU DNN of the same size and with every $n_i$ neurons intra-linked needs $W^2$ multiplications and $W^2+(n_i-1)\\cdot[W/n_i]\\approx W^2+W$ additions, where $[\\cdot]$ is a ceiling function, which is still quadratic. Thus, the computational cost incurred by adding intra-links is minor. When applying intra-layer links in CNNs, the links can be added between different channels. The computational cost is also minor. In brief, intra-layer links are not subjected to a high computational and parametric cost.\n\n- Hardware optimization through parallelization: The usage of intra-layer links may hurt the hardware optimization to some extent. However, we can design acceleration algorithms for intra-linked networks. Specifically, the acceleration of RNNs and LSTMs has been intensively investigated. We can translate ideas therein such as sequence bucketing to solve the training issues of intra-linked networks.\n\nWe have illustrated these trade-offs in this revision."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2609/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272932245,
                "cdate": 1700272932245,
                "tmdate": 1700272932245,
                "mdate": 1700272932245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]