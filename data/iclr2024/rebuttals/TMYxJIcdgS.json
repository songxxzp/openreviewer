[
    {
        "title": "What Makes ImageNet Look Unlike LAION"
    },
    {
        "review": {
            "id": "OR0kjxeKS2",
            "forum": "TMYxJIcdgS",
            "replyto": "TMYxJIcdgS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4388/Reviewer_fH12"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4388/Reviewer_fH12"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the differences between ImageNet and the created LAIONet dataset out of LAION. Through three carefully designed experiments, the authors claim that the information bottleneck explains why ImageNet is less diverse than LAIONet."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The findings about information bottleneck in the paper are very interesting and insightful for future data curation efforts."
                },
                "weaknesses": {
                    "value": "1. The abstract states the \"long-held intuition\" that ImageNet images are \"stereotypical, unnatural, and overly simple representations\". I don't find enough references in Section 1.2 and any other sections.\n2. One important difference between ImageNet and LAION is their data sources - the former is from Flickr and the latter is from CommonCrawl. The two data sources should definitely exhibit different levels of data distribution and diversity. The reviewer think this should also be taken into consideration for analysis.\n  - Both DataComp and LAION come from CommonCrawl. In DataComp [1], the images come from various data sources more than Flickr (Fig 13 in [1]).\n\n[1] Gadre, Samir Yitzhak, et al. \"DataComp: In search of the next generation of multimodal datasets.\" arXiv preprint arXiv:2304.14108 (2023)."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4388/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4388/Reviewer_fH12"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697942055059,
            "cdate": 1697942055059,
            "tmdate": 1700630782418,
            "mdate": 1700630782418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uV7cG6C9PP",
                "forum": "TMYxJIcdgS",
                "replyto": "OR0kjxeKS2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review of our work. \n\nThank you for pointing out the lack of reference about ImageNet's uniqueness. The seminal work of \"Unbiased look at dataset bias.\" (Torralba and Efros) has initiated such studies. We will consider providing further references in the final version.\n\nWe agree that LAION and ImageNet are created from different data sources. However, as long as images on Common Crawl and Flickr are sufficiently rich, we can assume they have the potential to contribute diverse images for each class. Therefore, the difference in intra-class similarity should be explained by identifying where this diversity is lost in the selection mechanism.\n\nThank you for pointing out to DataComp. We believe similar studies including a replication of our work potentially for other benchmarks can be done on DataComp as well and we will suggest this in our updated discussion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328913198,
                "cdate": 1700328913198,
                "tmdate": 1700328913198,
                "mdate": 1700328913198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ygnnkkVciz",
                "forum": "TMYxJIcdgS",
                "replyto": "uV7cG6C9PP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4388/Reviewer_fH12"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4388/Reviewer_fH12"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the replies. I would like to increase my current rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630766695,
                "cdate": 1700630766695,
                "tmdate": 1700630766695,
                "mdate": 1700630766695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pjFsxVOSqp",
            "forum": "TMYxJIcdgS",
            "replyto": "TMYxJIcdgS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4388/Reviewer_fkPT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4388/Reviewer_fkPT"
            ],
            "content": {
                "summary": {
                    "value": "- The paper proposes LAIONet, an ImageNet-like dataset created from LAION-400M\n- The dataset is created by filtering out instances with an image-text CLIP similarity of 0.3. Next, the images are selected based on the ImageNet category synset occurence + a high similariy with the text and the synset definition.\n- The paper then analyzes LAIONet and finds that it is distinctly unlike ImageNet -- the intra-class similarity is lower, and the accuracy of ImageNet trained models drops by 5-12% on LAIONet.\n- The paper then shows that the difference is because ImageNet relied on the image content for the selection process, and that relying on just the text captions creates an information bottleneck which mitigates the selection bias"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper looks into the data creation process and how to mitigate biases in the process, which is important for the community\n- The paper is easy to read and understand, all the experiments are explained very clearly"
                },
                "weaknesses": {
                    "value": "- The paper claims in Section 1.1 \"Choosing an image reveals nothing more about the image than what can be learned from its textual representation. This powerful conditional independence property limits how much selection can bias the distribution of the image. In contrast, in the case of ImageNet (Figure 2b), there is a link from the image to the selection decision.\". This isn't accurate -- choosing an image gives more information than the text representation which is used for LAIONet selection, namely the CLIP image-text similarity, with a high threshold of 0.3. LAIONet doesn't remove image content from the selection criteria, it just uses a CLIP model to do the image-text based selection instead of humans. There is a lot of focus on LAIONet only relying on texts to get closer to the \"true\" distribution and avoiding bias, whereas LAIONet is getting closer in distribution a dataset of concepts which CLIP recognizes and getting biased towards CLIP's understanding of concepts. It is possible though that CLIP has a different and lower bias than human annotators though but there is no discussion of this. \n- The paper claims that ImageNet uses the image content for selection heavily, and for LAIONet there is an information bottleneck. It claims in Section 1.1 that \"Selecting on the basis of the text caption, therefore, retains much of the entropy present in the image distribution\" -- while this statement would be true theoretically for a noise-free dataset, the paper never touches upon or even considers the fact that LAION is a noisy dataset. Using a noisy dataset will produce a higher entropy and diverse dataset on account of mislabeled images as well. The noise also affects the performance of models -- it is not clear how much does the performance of models drop on LAIONet just because the images are mislabeled? The paper has a fundamental flaw that it simply considers one dimension, diversity, and creates LAIONet to be more diverse, without ever considered the label noise dimension -- diversity and noise are inversely correlated.\n- The paper then mentions in Section 2 \"We found CLIP zero-shot top 1 accuracy to only differ by 2% across datasets. Hence, at least from the CLIP view, LAIONet images are not harder to classify. ...\". This discussion also has a flaw -- CLIP was used to filter the images to begin with, so there is an inherent bias here where the test set was created from the same model which is being evaluated.\n- The section about \"A WEAKER IMAGE-TO-SELECTION LINK MAKES IMAGENET MORE LIKE LAIONET\" also comptely ignores noise and just mentions \"weaker image-to-selection link\", wherein lower MTurk selection frequency results in a distribution closer to LAIONet. There is again a confounding factor at play, which is the noise in labels -- if the MTurk selection frequency is lower, it means that the likelihood of mislabeling is higher. \n- There is a discussion on figuring out whether images were used for selection for the creation of ImageNet (section 4.2, section 4.3). \"These observations reject the hypothesis that the graphs of Figure 2 have the same structure and show a potential leak from the image to the selection.\" -- a leak suggests this was unintentional, whereas it is known ImageNet was created by looking at the images' content. I am not sure what the point / contribution of this discussion is? \n- Also, section 4.3 creates a subset which is not like ImageNet, but also not like LAIONet, this is a third setting where the image isn't used at all since this section doesn't use CLIP based filtering."
                },
                "questions": {
                    "value": "- The paper has two limitations which need to be addressed --\n  - There is no discussion of noise at all in the datasets and the paper just talks about diversity. At a bare minimum, all analyses should have shown and compared the prevalence of noise in ImageNet and LAIONet. Only then can any conclusions be drawn which are made in the paper regarding image-to-selection link and / or diversity\n  - The paper ignores the contribution of CLIP thresholding on the creation of LAIONet -- this creates a very strong link to the image content as well in the creation of LAIONet, and also adds a different bias from CLIP. A threshold of 0.3 is very high, and this thresholding is directly connected to the noise and diversity of LAIONet but there isn't any discussion around this either.\n- I am not sure what is the value add of testing the hypothesis whether ImageNet data collection used image content or not when it is known that the image content was used already?\n- The paper also mentions that models perform worse in more frequent classes, but the analysis is only show on LAIONet -- this is a surprising result, given that frequent classes will be seen more often during training, and models are expected to perform on infrequent classes. Does this only happen on LAIONet or does it happen on other datasets as well, specifically ImageNet? It could also be that frequent classes have a different label noise rate on LAIONet?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4388/Reviewer_fkPT",
                        "ICLR.cc/2024/Conference/Submission4388/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698715601705,
            "cdate": 1698715601705,
            "tmdate": 1700674047930,
            "mdate": 1700674047930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mtPyIJOQ6p",
                "forum": "TMYxJIcdgS",
                "replyto": "pjFsxVOSqp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4388/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1"
                    },
                    "comment": {
                        "value": "Thank you for a helpful review of our work. In the following, we discuss two main concerns raised in the review: 1) The contribution of CLIP thresholding in the creation of LAION, and 2) the possibility of noisy labels. We will also address other raised concerns in the last section of the response.\n\n### 1. Contribution of CLIP thresholding in the creation of LAION\n\nThank you for this insightful question. We agree that image content is utilized in the creation of LAION where images with CLIP image-text similarity of 0.3 or higher are selected. **However, in this process, the image content is compared to its caption, limiting the leak of image information to the extent that can be conveyed by the caption.** Therefore, such image-text matching can only distort the distribution of images to the extent that the text can reveal information about the image. Given that images are a far richer modality than text, the text still serves as an information bottleneck. Compare this to the case when a human labeler directly examines the images, with all the potential biases and shortcomings they may have in recognizing the concept. Or when a search engine directly acts on image embeddings or uses popularity metrics to return the results. In these cases, the information leak can be much greater than the leak through matching with the caption.\n\nIn fact, for the specific selection mechanism we employ, the situation is even better. The worst-case scenario when LAION was created would involve a leak of information from the image beyond the class itself, such as the clarity of the image or the difficulty of identifying the object. Even if such information is present in the text and influences the selection of images into LAION, by intentionally avoiding the search for visual descriptions in the caption, we have grounds to believe that our LAIONet selection mechanism is not exploiting this leaked information and is thus less susceptible to selection bias.\n\nLastly, while it's conceivable that a leak of image information occurs when LAION instances are selected based on the 0.3 threshold, the degree to which this leak influences the final images is questionable. Notably, this leak occurs in the initial stage of obtaining a candidate pool; however, our final selection is predominantly influenced by the stringent requirement on textual similarity while deliberately neglecting visual descriptions. Our strict criteria for this similarity significantly reduce the dataset size from over 12M to less than 1M, serving as the primary requirement for image selection.\n\nIn summary, we have never denied the possibility of an information leak from the image through the text. In our depictions of causal graphs in Figure 2, we explicitly illustrate the bidirectional link between the image and the text. For instance, image information naturally leaks into the text when someone writes a caption describing the image, as was also evident when LAION candidates were filtered based on image-text similarity. The essence of our argument is however that this leak and the potential distortion of the final selected images are limited."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328168549,
                "cdate": 1700328168549,
                "tmdate": 1700328168549,
                "mdate": 1700328168549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t7uLzHerfW",
                "forum": "TMYxJIcdgS",
                "replyto": "MpqhkBmQl4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4388/Reviewer_fkPT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4388/Reviewer_fkPT"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author's response"
                    },
                    "comment": {
                        "value": "I appreciate the response by the authors. I believe the paper has value, but with a very different lens. The paper makes the point that an information bottleneck is added wherein only the text is used, whereas in reality what is happening is that a human annotator is replaced with an automated CLIP annotator. I think the paper's point should be more along the lines of \"contrastive models trained on large scale web datasets serve as better annotators than humans for diversity\". There is a huge focus on reducing \"information leakage\" but in reality there is full information still being used, but there is a change (reduction) in bias used for annotations. The paper's focus is in a very different direction, one which I believe has limitations and misrepresents the situation (as shared next), which is why I cannot recommend acceptance. I will bump my rating, but again contingent on the fact that if accepted, the paper should at least dedicate a section discussing this alternative viewpoint.\n\n- I still strongly contend the claim that there is an information bottleneck. This claim does a disservice to the role of the CLIP model for filtering the data -- a reader might not pay as careful attention to this and get the wrong conclusions. Using image content still is vital to create a new dataset. I don't understand what the paper refers to as \"information leak\" -- given that the rebuttal still mentions this, it is worth clarifying -- I believe what is being reduced is not \"information leakage\" but bias. Both humans and CLIP use the full image content to decide whether an image is aligned with a particular class or not (matching happens with different text strings in the two cases) -- i.e. there is full information \"leakage\" -- although the term \"leak\" implies this is unintentional, which it is not! What is improved here is not the reduction in leakage, but an improvement in the quality of the annotators -- CLIP _might_ be a stronger annotator with fewer biases. For instance, CLIP might not care about image quality whereas humans might reject low quality images, or images where the objects are too small, or obscure labels which humans might mislabel but CLIP can easily label correctly (like an exotic dog breed). \n- I do not agree with the statement which conveys that the follow up stage (where the texts are matched with the class name based texts which restricts the images from 12M to 1M) \"serves as the primary requirement for image selection.\". The final images still are based on the 0.3 threshold, there are just more filters applied. In fact, I would wager that you can change the second stage and still produce a reasonable dataset, but without the CLIP thresholding it would not be possible to create a high quality dataset. \n- Re: Section 4.1 I agree that ImageNet v2 is valuable, but just because it is valuable / popular doesn't mean it doesn't have more label noise than ImageNet, and assuming so without data and just considering lower inter-annotator agreement as producing a weaker link without increasing label noise is not warranted. Thus, the paper / response doesn't show that \"A WEAKER IMAGE-TO-SELECTION LINK MAKES IMAGENET MORE LIKE LAIONET\". \"Image-to-selection link\" is an unclear concept used ambiguously in the paper. In this section inter-annotator disagreement is considered a weaker Image-to-selection link, and earlier using a CLIP model instead is used to represent \"image-to-selection link\". Why does CLIP have a weaker image-to-selection link and less information leakage? For instance, if I share a picture of a dog and ask an annotator if it is a dog, whereas use CLIP and match the image with \"an image of a dog\", how is CLIP using less information? In both cases, the systems look at the image and try to see if the image \"matches\" the textual representation (with different definitions of \"matching\" in both situations)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674012200,
                "cdate": 1700674012200,
                "tmdate": 1700674012200,
                "mdate": 1700674012200,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "10aPepdvyg",
            "forum": "TMYxJIcdgS",
            "replyto": "TMYxJIcdgS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4388/Reviewer_j7LU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4388/Reviewer_j7LU"
            ],
            "content": {
                "summary": {
                    "value": "This paper conducts a comparative analysis between the predominant ImageNet dataset in the computer vision field and the recently widely-used LAION dataset. By analyzing their data collection processes, the intrinsic differences between ImageNet and LAION datasets are highlighted. Heuristically, guidelines for selecting data instances based on information bottlenecks are provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Analyzing mainstream datasets helps deepen researchers' understanding of the data. At the same time, it aids the community in designing future datasets with minimal human-induced bias, which in turn helps enhance the generalization performance of models.\n\n- This paper is logically structured, and the conclusions regarding the differences between the ImageNet and LAION datasets are comprehensive. Starting from the inconsistent dataset filtering processes, it further analyzes the differences in intra-class similarity between the two. This leads to the conclusion that the image diversity in the two datasets is inconsistent.\n\n- This paper offers a wealth of visual analysis, which is very helpful in understanding the main conclusions."
                },
                "weaknesses": {
                    "value": "- This paper still lacks a central objective. Although a series of analyses point out the differences between ImageNet and LAIONet, both Figure1 and Figure5 seem to indicate that model performance on ImageNet and LAIONet is positively correlated. This suggests that LAIONet doesn't offer additional indicative value for model performance analysis, which is typically the most important for classification datasets.\n\n- Additionally, the ImageNet dataset and the LAION dataset were created at different times and for different purposes. The former emerged before deep learning became mainstream, aiming to provide a broad object-centric benchmark. In contrast, the latter was prepared for the pre-training of current large-scale models. Given that the paper suggests it can provide guidance for the construction of new datasets, and considering that the current processing methods for the LAION dataset (as well as similar datasets like COYO, mC4, etc.) are already being adopted, what specific new recommendations are included?\n\n- Considering the different collection times of the two datasets as mentioned above, is the gap in intra-class similarity related to the distributional shift of internet data? Also, given that ImageNet-1K was derived from ImageNet-22K, would an analysis of ImageNet-22K be more meaningful?"
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699248855790,
            "cdate": 1699248855790,
            "tmdate": 1699636411922,
            "mdate": 1699636411922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SQ953Eg9DS",
                "forum": "TMYxJIcdgS",
                "replyto": "10aPepdvyg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your helpful review of our work. We give a detailed answer in the following.\n\n---\n\n> This paper still lacks a central objective.\n\nConcerning the objective of our paper, we pose the intriguing question of whether LAION offers images that ImageNet was not capable of and, if so, why. Answering this question comes with new insights about both ImageNet and LAION with potential takeaways for future dataset creation. In summary, our study retrospectively examines ImageNet, pinpointing a selection bias in its data-generating process. This insight was made possible through experimentation with LAION. Additionally, we demystify the distributional advantage of LAION and illustrate how one can leverage an information bottleneck to obtain more diverse representations of a concept.\n\n---\n\n> ... both Figure1 and Figure5 seem to indicate that model performance on ImageNet and LAIONet is positively correlated. This suggests that LAIONet doesn't offer additional indicative value for model performance analysis, which is typically the most important for classification datasets.\n\nRegarding the consistency of performance rankings in ImageNet and LAIONet, we do not believe such consistency undermines the value of evaluation on LAIONet. For example, the accuracy of ImageNet-trained models assessed on LAIONet provides insights into the extent of generalization lost due to reduced diversity. Additionally, beyond accuracy, the comparison of ImageNet with LAIONet enabled the identification of lower within-class diversity. Therefore, evaluating on datasets like LAIONet, while it may not significantly alter model rankings, can still offer valuable insights into model performance in a new domain and illuminate dataset intricacies. It's also worth noting this consistency is not surprising as the widely-used ImageNetV2 also showed consistent ranking of the models but this does not undermine the new insights it offered.\n\n---\n\n> ... the ImageNet dataset and the LAION dataset were created at different times and for different purposes.\n\nConcerning different purposes in the creation of ImageNet and LAION, we agree that these datasets are obtained at different times for different learning objectives. **However, this comparison has been insightful about both datasets:** First, using LAION as an image search engine enabled us to recreate ImageNet in a controlled way and get new insights about ImageNet by contrasting it with the new dataset. This is how we were able to identify a selection bias. Second, large-scale language image models trained on LAION demonstrate unprecedented robustness in image classification. This raises the question of what makes these models so robust. Previous works have conjectured that the LAION image distribution is the primary cause [Fang et al.]. Our work provides explicit evidence that even for the same task and classes, LAION can provide more diverse images than those selected into the classic ImageNet. Thus, such a comparison has been insightful in understanding LAION's capabilities as well.\n\n---\n\n> ... what specific new recommendations are included?\n\nRegarding recommendations for future data collection, we propose that selection based on an information bottleneck, such as concise text, is a promising method to avoid selection bias. Whenever diversity is desired, this mechanism can be employed to acquire diverse instances from the target class. This class may not necessarily belong to ImageNet classes or other well-known benchmarks. As a concrete example, based on the feedback received for our work, a group of industry-based researchers found our selection mechanism promising for obtaining diverse images for a new concept, enabling them to calculate a more representative image embedding for that concept to be later used as part of a search engine. We also received feedback like ``our team spent a long time curating a dataset and then we realized the improvement on it does not translate to our real application. Now I can clearly see what goes wrong.'' These examples have been heartwarming for us.\n\n---\n\n> Considering the different collection times of the two datasets as mentioned above, is the gap in intra-class similarity related to the distributional shift of internet data? Also, given that ImageNet-1K was derived from ImageNet-22K, would an analysis of ImageNet-22K be more meaningful?\n\nConcerning different collection times, we agree this can explain a small part of the significant difference observed between the two datasets. This concern may be more pronounced for technology-related concepts and when images are compared across datasets. However, this is not the case for the majority of ImageNet classes, and we compare image similarity within each dataset rather than across datasets, so, this should be less of a concern. \n\nWe also agree a similar experiment can be done on the larger ImageNet and possibly on the larger LAION 5B version however we do not expect this to change the conclusions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327426232,
                "cdate": 1700327426232,
                "tmdate": 1700327426232,
                "mdate": 1700327426232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qKCYuLxsQ2",
            "forum": "TMYxJIcdgS",
            "replyto": "TMYxJIcdgS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4388/Reviewer_UY2S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4388/Reviewer_UY2S"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the difference between ImageNet and the version of LAION dataset recreated with ImageNet classes. The main finding is that, the image selection of the creation process of ImageNet depends partially on images themselves except for text descriptions, leading to smaller intra-class variances and easier tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The viewpoint of connecting and comparing older and newer datasets is interesting. \n- The writing is generally clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- The only conclusion of this paper is that ImageNet is more of an easy dataset than LAION because the images are curated dependent on image similarities, which makes images of each class less diverse and has smaller intra-class variances. This conclusion is unsurprising since ImageNet is curated very carefully to exclude outlier examples. \n- I do not see much value of the findings. Visual datasets should not be curated only using text descriptions, which leads to a higher probability of getting wrong images inside the dataset. Thus the findings do not reveal a drawback of ImageNet curation process. On the other hand, the datasets nowadays, like LAION, are mostly not curated using names of classes, while the conclusion of this paper only supports curation using the names of classes, and thus has limited values.\n- This paper does not reveal anything related to the different curation processes of Imagenet and LAION, one for image classification and another for vision-text pretraining, but instead create another ImageNet-like dataset from LAION. Thus the title of this paper is inappropriate."
                },
                "questions": {
                    "value": "- Why not pretrain models on both datasets and compare the differences to support your conclusion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4388/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4388/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4388/Reviewer_UY2S"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4388/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699443513208,
            "cdate": 1699443513208,
            "tmdate": 1699636411853,
            "mdate": 1699636411853,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7YZQeuG6dN",
                "forum": "TMYxJIcdgS",
                "replyto": "qKCYuLxsQ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4388/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4388/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review of our work. In the following, we provide a detailed response to the raised concerns.\n\n---\n\n> The only conclusion of this paper is that ImageNet is more of an easy dataset than LAION ... This conclusion is unsurprising since ImageNet is curated very carefully to exclude outlier examples.\n\nOur study stems from the intriguing question of whether for the same task, the recently popular LAION can provide images that ImageNet could not. Indeed, one major finding here is that the ImageNet-like dataset created from LAION, termed LAIONet, demonstrates higher intra-class variability, posing a challenge for ImageNet models to generalize.\n\nWe find this observation to be particularly surprising, considering that we created LAIONet in the same manner as ImageNet, using LAION instead of Flickr as the image search engine. If we assume that LAION and Flickr images closely resemble random images from the web, one might expect the resulting dataset, for appropriately chosen thresholds, to resemble the original ImageNet. However, we show that this is far from the case.\n\nAs you pointed out, we diagnose the cause of this contrast and provide an explanation in terms of a difference in data-generating processes. More precisely, we identify a selection bias resulting from information leakage from the image at the time of selection. Although the existence of selection bias might be trivial, our work shows this mechanism has been significantly at play at the time of ImageNet creation.\n\nWe want to emphasize that the LIONet-ImageNet difference is not caused by noisy labels or outliers within LAIONet. In fact, our conservative approach to creating LAIONet, along with extensive experiments, supports the quality of LAIONet labeling. We have elaborated on this in our response to reviewer fkPT. The difference is precisely due to the selection bias in ImageNet, which has led to a reduction in the intra-class variance of the images.\n\n---\n\n> I do not see much value of the findings.\n\nConcerning the potential implications of our findings, our text-based selection exemplifies how a modality with less information can act as an information bottleneck for selecting instances from a richer modality. Our analysis of LAIONet reveals that such a selection mechanism can offer more diverse examples for a given concept. As a concrete example, based on the feedback received for our work, a group of industry-based researchers found our selection mechanism promising for obtaining diverse images for a new concept, enabling them to calculate a more representative image embedding for that concept to be later used as part of a search engine.\n\nOur study also complements previous empirical observations [Fang et al.] that the robustness of contrastive language image models should be caused only by a diverse training distribution. Our work demystifies this distributional advantage and explains what was missing from a conventional dataset. \n\n---\n\n> Visual datasets should not be curated only using text descriptions, which leads to a higher probability of getting wrong images inside the dataset. \n\nIt's essential to implement safety measures based on specific downstream tasks. In fact, in applications that require applying safety filters based on full image content, as long as the filter is accurate, the distribution of the safe images will remain mainly unaffected and our conclusions remain valid. \n\n---\n\n> ... the conclusion of this paper only supports curation using the names of classes, and thus has limited values.\n\nAs a minor clarification, our text-based selection relies on the unique description of the class as well and the name of the class is not the only identifier. \n\n---\n\n> Why not pretrain models on both datasets and compare the differences to support your conclusion?\n\nWhile training model on LAIONet is feasible, it does not substantially contribute to our conclusion beyond the current results indicating that LAIONet images are more diverse. Consequently, we deemed such an experiment unnecessary and instead directed our focus towards understanding the underlying cause of this difference."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4388/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326353124,
                "cdate": 1700326353124,
                "tmdate": 1700326353124,
                "mdate": 1700326353124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]