[
    {
        "title": "PEMs: Pre-trained Epidemic Time-Series Models"
    },
    {
        "review": {
            "id": "1j8XXQjuj2",
            "forum": "DL7JWbdGr3",
            "replyto": "DL7JWbdGr3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6009/Reviewer_7guK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6009/Reviewer_7guK"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose Pre-trained Epidemic Models (PEMs) that is capable of learning pattern across multiple datasets and multiple diseases using self-supervised learning.  They demonstrate the success of their method in the task of forecasting. Their approach maintains or even\nsurpasses the performance of other strong deep-learning models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-presented and easy to understand.\n1. PEM is a novel model that uses an array of self-supervised learning (SSL) tasks shown to be effective in learning from cross-disease datasets.\n1. PEM demonstrates efficiency in utilizing multiple training datasets while maintaining or even surpassing the performance of other strong deep learning models. This showcases its ability to adapt to downstream tasks with limited data, which is particularly beneficial in real-world\nscenarios where complete datasets may not be readily available, especially in the case of unseen diseases."
                },
                "weaknesses": {
                    "value": "1. __Limitations in Model Compared__: While the paper compares PEM with FUNNEL, a mechanistic model, it does not explore simpler disease-focused mechanistic models or even basic auto-regressive models. Additionally, the absence of testing the performance of simpler models on unseen diseases, like COVID-19, limits the model's generalization claims. Over the years many experts have submitted forecasts for COVID-19 and influenza in real-time (e.g., US COVID-19 Forecast Hub, EU COVID-19 Forecast Hub). The paper already refers to a paper on one of the hubs (Cramer 2022). Without comparison against the models used by the community, it is hard to judge how much value the proposed approach adds. Further, for COVID-19 and epidemiological data in general, there is frequent data revision and backcorrection. The data that is available today is a much cleaner version compared to what was available at the time a forecast was made. Especially for COVID-19 the data revision history is available, therefore, the authors should test the dataset with the appropriate version, not the latest version. It is not clear if the authors have done so.\n\n 2. **Data Preprocessing Details**: The paper lacks information on the data preprocessing steps performed, such as data smoothing and temporal alignment. Preprocessing often has a significant impact on real-time forecasting, especially, when there is a lot of backcorrection.\n\n 3. **Ablation Studies**:   \n    1. Performance compared to simple transfer learning or doing some training on a single disease dataset (such as pre-training on just past flu data if the disease of interest is flu) would have been good. Randmask does not perform nearly as well as the others, so does adding this into the combination of tasks contribute meaningfully?\n    1. The choice of hyperparameters matches the best-performing ones in the \"ablation studies\". Can the authors verify that the hyperparameter selection was done on a validation set and not on the test set?\n\n1. **Additional Comments**:  \n    1. Lower RMSE Values for Flu in the US: The remarkably low RMSE values for Flu in the US  looks unusual. Some more context or explanations for these results would be useful. Was this normalized somehow (like per 100k)?\n\n    1. Consolidation of Appendix Results: While it's useful to have supplementary information in the appendix, the paper might benefit from incorporating key results from Q2, Q3, Q4, and Q6 into the main text to provide a more comprehensive overview of the model's capabilities.\n\n    1. Presentation issues: While easy the writing is easy to understand, the paper has many typos; it will benefit from a pass for presentation."
                },
                "questions": {
                    "value": "Addressing the following, especially the first one, will significantly improve my evaluation:\n\n1. Have the authors compared their results against models that were used by the epidemic forecasting community during COVID-19 and ILI forecasting tasks? (These forecasts are publicly available)\n\n1. Was there any data pre-processing involved? How does that impact the results?\n\n1. Can the authors verify that the hyperparameter selection was done on a validation set and not on the test set?\n\n1. Why is RMSE for the US so low?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717397763,
            "cdate": 1698717397763,
            "tmdate": 1699636644452,
            "mdate": 1699636644452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SNtXPjK064",
                "forum": "DL7JWbdGr3",
                "replyto": "1j8XXQjuj2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6009/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7guK"
                    },
                    "comment": {
                        "value": "**While the paper compares PEM with FUNNEL, a mechanistic model, it does not explore simpler disease-focused mechanistic models or even basic auto-regressive models....COVID-19, limits the model's generalization claims**\n\nWe do compare against state-of-art epidemic forecasting models that have shown to beat past simpler statistical models on epidemic forecasting tasks and\nhave won past epidemic forecasting competition[1,2,3].\nTherefore, we believe this is a fair comparison to provide evidence\nthat PEM indeed provides state-of-art forecasting performance.\n\n\n\n**The paper already refers to a paper on one of the hubs (Cramer 2022). Without comparison against the models used by the community, it is hard to judge how much value the proposed approach adds.**\n\nThere are two main reasons why direct comparisons with the models used\nby forecast hub is not feasible.\nFirst, our model only uses past values of the time-series to forecast future dynamics. It does not ingest additional external variables.\nTherefore, all the benchmarks are univariate time-series forecasting\nor prediction tasks and we evaluate state-of-art baselines designed for such tasks.\nFurther, most models in the forecasting hubs used additional data sources and the models and datasets used are not public in most cases.\n\n\n**The data that is available today is a much cleaner version compared to what was available at the time a forecast was made. Especially for COVID-19 the data revision history is available, therefore, the authors should test the dataset with the appropriate version, not the latest version. It is not clear if the authors have done so.**\n\nAll the baselines and the model are evaluated on the latest available version of\nCovid-19 mortality data for a fair comparison.\nWe agree with the reviewer that data revision is an important\nproblem in epidemic forecasting. Our work doesn't address this problem and instead focuses on modeling and accurately forecasting\nepidemic time-series. However, analyzing and tackling the data revision\nproblem for our pre-training framework is an interesting future direction.\n\n\n**The paper lacks information on the data preprocessing...**\n\nWe do not perform any data pre-processing on raw time-series data provided. Since we used Reversible Instance Normalization  as part of the model, we\ndo not need explicit data normalization before feeding into the model.\n\n\n**Performance compared to simple transfer learning or doing some training on a single disease dataset (such as pre-training on just past flu data if the disease of interest is flu) would have been good.**\n\n\n**Randmask does not perform nearly as well as the others, so does adding this into the combination of tasks contribute meaningfully?**\n\nYes RANDMASK alone is the worse performing SSL task overall. However,\nwe still included it for pre-training because (a) it still outperforms\nthe variant without any pre-training, (b) and using all four SSL tasks\ntogether provided the best performance across all benchmarks.\n\n\n**Can the authors verify that the hyperparameter selection was done on a validation set and not on the test set?**\n\nYes, the hyperparameter selection was done during training on validation data for all benchmarks. \n\n\n**Lower RMSE Values for Flu in the US**\n\nThe influenza incidence time-series extracted from CDC is normalized\nbetween 0 to around 10. We direcltly use these values for training and prediction. Therefore, the RMSE values may appear very low compared to other benchmarks.\n\n\n*References*\n\n[1] Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodr\u00edguez, Chao Zhang, and B Aditya Prakash.\nWhen in doubt: Neural non-parametric uncertainty quantification for epidemic forecasting. Advances in Neural Information Processing Systems\n\n[2] Bijaya Adhikari, Xinfeng Xu, Naren Ramakrishnan, and B Aditya Prakash. Epideep: Exploiting\nembeddings for epidemic forecasting. In Proceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining\n\n[3] Logan C Brooks, David C Farrow, Sangwon Hyun, Ryan J Tibshirani, and Roni Rosenfeld. Flexible\nmodeling of epidemics with an empirical bayes framework. PLoS computational biology,"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635500410,
                "cdate": 1700635500410,
                "tmdate": 1700635500410,
                "mdate": 1700635500410,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KwxSv4JJSY",
            "forum": "DL7JWbdGr3",
            "replyto": "DL7JWbdGr3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6009/Reviewer_A18H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6009/Reviewer_A18H"
            ],
            "content": {
                "summary": {
                    "value": "- novel application of SSL and fine-tuning on broad epidemic data to get PEMs, which they use to forecast various diseases, as well as various other epidemic dynamics such as peak weeks and onset weeks.\n\n- interesting experiments of generalization to novel diseases in appendix D"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- originality: This is a new application domain for SSL as far as I know, but the methodology is not particularly new.\n\n- quality: The submission is well written and formatted, with strong experimental results and ablations.\n\n- clarity: most of the training details are there, though much is relegated to the appendix.\n\n- significance: the work is significant as an application of SSL to a specific domain with greater performance, and finds relevance in the current post-pandemic context."
                },
                "weaknesses": {
                    "value": "- sounds like your segments of time series is basically patching, similar to (Nie et al. 2022). If so, why do you achieve so much better results than PatchTST? Is it just the finetuning?\n\n- I'm not sure I understand why inputting each time step as a single token would lack \"semantic meaning\", seeing as the output representations of a transformer for a given token are influenced by all the other tokens in the input sequence (i.e. \"contextual embeddings\")\n\n- How do you normalize the training process of your method wrt baselines? How can you disentangle the architecture's influence from the influence of the training process and the influence of using additional compute?\n\n- unclear what is being evaluated in appendix D table 6 since it's not known how the baselines are trained. For example, it is clearly remarked that no pre-training is performed for the PT baseline, but not why.\n\n- why are the results around one of the contributions (Significant improvement in data and training efficiency and adaptability to novel epidemics) only in the appendix?"
                },
                "questions": {
                    "value": "- There might be bleed between the influenza data from 2001 to 2010 and the crypto data from 2006 to 2012 given the temporal alignment, but I'm not knowledgeable enough to judge how strong the overlap could be. Is the argument that, since influenza and crypto have different inherent dynamics, they shouldn't influence each other?\n\n- How do you choose the value of T, i.e. the length of the input time series?\n\n- How do you justify the default choice of gamma = 0.1 for lastmask ? Seems like 0.2 performs well too.\n\nOverall, I am rating this a 5 (marginally below acceptance) before discussion, since it is hard to disentangle the impact of the supervised pre-training from the architectural design choices and the increased compute/data. There are no details on how the baselines are trained, so these comparisons do not properly motivate which components of PEM lead to its superior performance. I am very amenable to changing this rating given discussions around how the baselines were implemented for fair comparisons."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698767684435,
            "cdate": 1698767684435,
            "tmdate": 1699636644353,
            "mdate": 1699636644353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lhyTjx27MU",
                "forum": "DL7JWbdGr3",
                "replyto": "KwxSv4JJSY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6009/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer A18H"
                    },
                    "comment": {
                        "value": "**why do you achieve so much better results than PatchTST? Is it just the finetuning?**\n\nIt is due to our SSL pre-training. Indeed performing fine-tuning on heterogenous time-series data by designing specific SSL tasks for extracting useful background knowledge from wide range of disease datasets during pre-training is our most important technical contribution.\nWe note that most other SSL pre-training methods of past works are not effective\non our heterogenous pre-training problem.\nTherefore, our method of effective fine-tuning enables the pre-trained models\nto significantly outperform models without pre-training (such as using PatchTST without pre-training) as well as methods that use\nprevious SSL methods.\n\n**I'm not sure I understand why inputting each time step as a single token would lack \"semantic meaning\", seeing as the output representations of a transformer for a given token are influenced by all the other tokens in the input sequence (i.e. \"contextual embeddings\")**\n\nProviding semantically meaningful input tokens can better enable transformers to model\nrelations between tokens.\nFor example, in the context of language, LLMs use specific tokenization methods to capture semantic meaning instead of individual characters to provide significantly better performance.\nFurther, the inputs of time-series are real-valued numbers rather than specific character.\nTherefore, these individual values of time-series observed in isolation do not provide sufficient context such as local patterns (such as peaks, spikes, etc.)\naround the time-step.\ntherefore, we use segments of time-series that provide semantic information about local patterns.\n\n**How do you normalize the training process of your method wrt baselines? How can you disentangle the architecture's influence from the influence of the training process and the influence of using additional compute?**\n\nFor all deep learning baselines and PEM, we train using early stopping.\nWe use the same compute hardware for training all models and baselines (appendix B) and we measure the training time till convergence\nfor all models.\nPEM uses similar training time to other best baselines with similar compute and provide significantly better performance.\nIn fact, it takes 12-50% less time to match the performance of the baselines (Appendix Table 5).\n\nWe also measure the importance of various training choices and pre-training choices in Table 3 and show the importance of each SSL task\nand linear-probing for optimal performance. We also measure the effect of segmentation in Appendix Table 9.\n\n**unclear what is being evaluated in appendix D table 6 since it's not known how the baselines are trained. For example, it is clearly remarked that no pre-training is performed for the PT baseline, but not why**\n\nAppendix D Table 6 measures the performance of PEM when we remove the\ntraining data for downstream tasks from pre-training.\nThis is denoted by PEM-ExcludeTrain.\nThis measures the performance of fine-tuning PEM to unseen diseases.\nWe observe that even is such cases PEM outperforms the best baseline performance.\n\n**why are the results around one of the contributions (Significant improvement in data and training efficiency and adaptability to novel epidemics) only in the appendix?**\n\nWe summarize the main results of training efficiency and data efficiency in the main paper page 8 and provide additional details in the Appendix: PEM takes 47-88% of the time taken by best baselines and uses 60-80% of total training data to outperform them.\nWe could summarize the training times in Table 4 using the bar graph and add to the main paper.\n\n**There might be bleed between the influenza data from 2001 to 2010 and the crypto data from 2006 to 2012 given the temporal alignment**\n\nThis is an interesting question. However, influenza and crypto show very different disease dynamics. Influenza peaks during winter months and spreads through the air. Crypto mostly occurs during the summer and is spread through food and water contamination.\nWhile influenza is spread through a virus, crypto is spread through a\nmicroscopic parasite.\nWe couldn't find any research linking the incidence of both diseases\n\n**How do you choose the value of T, i.e. the length of the input time series?**\n\nAll the diseases in training and pre-training have a sampling rate of 1 month. Moreover, even seasonal diseases have periodicity within 12 months.\nTherefore, we choose the total input length to be $T=12$.\nWe will add this information in the Appendix.\n\n**How do you justify the default choice of gamma = 0.1 for lastmask ? Seems like 0.2 performs well too.**\n\nYes both 0.2 and 0.1 performs well depends on the downstream tasks.\nHowever, even in cases 0.2 performs better, 0.1 is only slightly worse.\nTherefore, we chose gamma as 0.1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635422098,
                "cdate": 1700635422098,
                "tmdate": 1700635422098,
                "mdate": 1700635422098,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wakUr6WUsU",
            "forum": "DL7JWbdGr3",
            "replyto": "DL7JWbdGr3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6009/Reviewer_ZKwe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6009/Reviewer_ZKwe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a pre-training machnism for using transformers to make time series forecasting for epidemics. The pre-training is self-supervised learning through multiple tasks: randmask, lastmask, peakmask, seasonselect. Similar to large language model pre-training, the proposed pre-training method for time series forecasting learn general patterns from multiple disease datasets. Then fine tune the pre-trained model to a specific disease forecasting task. The method is demonstrated on several real world epidemic forecasting tasks and outperforms the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of pre-train a time-series epidemic forecasting model is very interesting and new. This has not been done before. The method is technically sound and the writing is clear. The experimental design is reasonable and results show superior performance compared with the baselines."
                },
                "weaknesses": {
                    "value": "1. The baselines in this paper do not represent the SOTA performance. Also, the baselines are not \"easy-to-replicate\" methods whose code is not provide. This makes me a bit concern about the effectiveness of the comparison results.\n2. There is no uncertainty quantification analysis of all the methods. It seems the transformer-based models are so large so may have large predicting variance."
                },
                "questions": {
                    "value": "1. In section SEASONDETECT, the paper first mentions that the year is divided into 4 seasons Season 1 (Dec-Feb), but later, it also says the peak season is s_1(d). Could you clarify this in a more consistent way?\n2. Data normalization, will the normalized data be reversed before computing the loss error during training? \n3. Why perform linear probing before fully fine-tuning?\n4. What's the pre-training cost?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The pre-training process involves multiple disease datasets, there may have bias introduced by a disease data due to the nature of disease characteristics, environmental factors, demographic factors, etc. There is no discussion about the possible bias in the pre-training process."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6009/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6009/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6009/Reviewer_ZKwe"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698963460421,
            "cdate": 1698963460421,
            "tmdate": 1699636644257,
            "mdate": 1699636644257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NpvTmu5eZe",
                "forum": "DL7JWbdGr3",
                "replyto": "wakUr6WUsU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6009/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZKwe"
                    },
                    "comment": {
                        "value": "**The baselines in this paper do not represent the SOTA performance.**\n\nThe chosen baselines are state-of-art general forecasting and epidemic forecasting\nmodels at the time of submission. Therefore, we are indeed comparing PEM with the previous best possible performance.\n\nThe baselines used have code released publicly. We used the same code with\nmostly default hyperparameters for evaluation with hyperparameter tuning on learning rate.\nWe can add links to the baseline code in paper to alleviate these concerns.\n\n**There is no uncertainty quantification analysis of all the methods. It seems the transformer-based models are so large that they may have large predicting variance**\n\nOur method and most other baselines only focuses on forecasting and prediction accuracy and does not\nprovide reliable uncertainty estimates.\nHowever, we agree with the reviewer that uncertainty quantification is an important forecasting problem, and extending our methods to produce calibrated probabilistic forecasting is an interesting research direction.\n\n**Clarification on SEASONDETECT**\n\nWe divide the 12 months into four seasons: Dec-Feb, Mar-May, June-Aug and Sept-Nov.\nFor each disease dataset $d$, we assign one of these four seasons as peak season $s_1(d)$. For example, in the case of influenza $s_1(d) =$ Sept-Nov.\nBased on assignment on $s_1(d)$ we assign $s_2(d), s_3(d), s_4(d)$ chronologically.\nIn influenza's case, $s_2(d)=$ Dec-Feb, $s_3(d)=$ Mar-May, $s_4(d)=$ June-Aug.\n\n**Data normalization, will the normalized data be reversed before computing the loss error during training?**\n\nYes. Using Reversible Normalization prevents large gradients during training.\n\n**Why perform linear-probing before fine-tuning?**\n\nKumar et. al [1] showed that doing direct fine-tuning leads to lower accuracy\nas it distorts the learned representations of pre-trained models, especially for downstream tasks with distribution shift, similar to our case where we fine-tune on heterogeneous and unseen disease datasets.\nLinear-probing alleviates this by adapting the final layer of the pre-trained model\nto downstream tasks without distorting the representation learning of the pre-trained model."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635326060,
                "cdate": 1700635326060,
                "tmdate": 1700635326060,
                "mdate": 1700635326060,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "po6GtENy9H",
            "forum": "DL7JWbdGr3",
            "replyto": "DL7JWbdGr3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6009/Reviewer_PcYs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6009/Reviewer_PcYs"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to explore the pre-training of time series models for epidemic data. The proposed model is the first-ever pre-trained time series model for epidemic analysis tasks. Unlike SSL for images or texts, this paper presents four SSL tasks to capture various aspects of epidemic dynamics. Extensive experiments over real-world epidemic datasets verify the effectiveness of the proposed model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper addresses an essential problem in time series domain.\n2. The paper conducts extensive experiments to verify the effectiveness of PEM.\n3. The experiments have verified the effectiveness of PEM, especially in comparison with those without SSL.\n4. The experiments also cover a discussion over data efficiency and generalization to novel diseases."
                },
                "weaknesses": {
                    "value": "1. The related work should be expanded. To the best of my knowledge, a series of papers [1] have been studied on SSL for general time series, also including epidemic analysis. However, they're rarely mentioned or considered as baselines for a fair comparison.\n2. The technical novelty against previous SSL approaches is somewhat limited. \n3. The baselines listed on Page 7 are insufficient. More SSL methods, particularly for general time series, should be included as baselines. For example, it would be interesting to see if PEM can outperform recently developed SSL methods in time series analysis.\n4. It would greatly enhance reproducibility if the authors provide the source code for their approach.\n5. I highly recommend that this paper provide more results on few-shot or zero-shot learning for epidemic data, as it is typically characterized by sparse data.\n\nReference:\n\n[1] Zhang, Kexin, et al. \"Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects.\" arXiv preprint arXiv:2306.10125 (2023)."
                },
                "questions": {
                    "value": "If I have misunderstandings in the weaknesses, please clarify them in the rebuttal phase. Thank you."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6009/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699173603902,
            "cdate": 1699173603902,
            "tmdate": 1699636644139,
            "mdate": 1699636644139,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dBHtKkDa1f",
                "forum": "DL7JWbdGr3",
                "replyto": "po6GtENy9H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6009/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6009/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PcYs"
                    },
                    "comment": {
                        "value": "**The related work should be expanded.**\n\nWe have described recent state-of-art SSL methods and used them as baselines.\nWhile there are many methods that perform SSL on time-series, we focus\non methods that are applicable to any general time-series domain with\nspecific applications to forecasting tasks.\nWe would gladly add additional references to SSL methods including recent methods\ndescribed in the survey referred to by the reviewer.\n\n**The technical novelty against previous SSL approaches is somewhat limited.**\n\n Unlike most previous SSL papers, we are the first to deal with a different broader problem of effectively leveraging multiple heterogeneous epidemic datasets to train a general pre-train model that can be fine-tuned to a wide range of downstream epidemic analysis tasks. \n\nRANDMASK and LASTMASK are indeed widely used pre-training tasks in language domains and are also explored in time-series domain previously. \nHowever, to better extract useful epidemic dynamic information, we also introduce two novel tasks: PEAKMASK and SEASONDETECT. PEAKMASK helps identify peaks and their dynamics which is vital for epidemic forecasting since it is important to model and predict epidemic peaks in advance. Detecting seasonal information SEASONDETECT automatically is also important in successfully modeling epidemic dynamics. These tasks are carefully designed to impart useful background knowledge and patterns from multiple epidemic datasets. \n\nTo further illustrate the importance of designing and choosing the right SSL tasks, we also compared PEM against sophisticated SOTA self-supervised methods for general time-series and show that using generic tasks that perform well in other domains to epidemic datasets leads to poor performance.\n\n**The baselines listed on Page 7 are insufficient.**\n\nWe note that even most recent time-series SSL methods do not deal with multiple\nheterogenous datasets. The consisdered\nSSL baselines (TS2Vec, TNC, TS-TCC) in the paper which provide top performance for specific\nbenchmarks where training data is used for SSL pre-training\nperform much worse than even some baselines which do not use any pre-training in our setting of pre-training on multiple datasets. This shows that even sophisticated general SSL methods cannot simply used\nto pre-train on multiple datasets.\n\n**authors provide the source code for their approach.**\n\nWe have added link to the implementation code in the paper at page 7: [link](https://anonymous.4open.science/r/EmbedTS-3F5D/).\nWe also plan on releasing the weights of pre-trained models on acceptance.\n\n**more results on few-shot or zero-shot learning for epidemic data..**\n\nOur work focuses on the impact of pre-training model weights for more performant training of downstream tasks. Additionally, we evaluated the efficacy of fine-tuning from smaller dataset sizes in page 8 (Q4) and Appendix Figure 4.\nSpecifically we measured the performance of PEM with different fractions\nof training data and found that PEMs in on par or outperform the best baselines\nusing 60-80% of the training data in benchmarks.\n\nIf the reviewer is interested in any specific analysis on few-shot learning,\nwe would be glad to add it to the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6009/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635216340,
                "cdate": 1700635216340,
                "tmdate": 1700635216340,
                "mdate": 1700635216340,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]