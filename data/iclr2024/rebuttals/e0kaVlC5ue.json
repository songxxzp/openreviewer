[
    {
        "title": "Spectral Neural Networks: Approximation Theory and Optimization Landscape"
    },
    {
        "review": {
            "id": "EvXgKUn0MO",
            "forum": "e0kaVlC5ue",
            "replyto": "e0kaVlC5ue",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_sU7q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_sU7q"
            ],
            "content": {
                "summary": {
                    "value": "This paper theoretically studies several questions regarding spectral neural networks and, more generally, neural networks that are trained to approximate the eigenvalues of specific matrices. The authors prove that multi-layer ReLU NNs can approximate normalized Laplacians, with specific bounds on the error, depth of network, and number of neurons. The authors then show that NNs can approximate eigenvectors, up to rotation. Finally, the authors consider the loss landscape of the optimization and show that, in a quotient geometry, they can decompose the loss landscape into 5 regions (3 that are particularly different)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper provides detailed theoretical results for spectral neural networks, among other neural networks that are trained to approximate the spectra of matrices, a field of growing importance. I believe this work will provide greater insight into this area. \n\n2. The authors provide a detailed overview of existing work that is related. \n\n3. Aspects of the paper were well written and well motivated."
                },
                "weaknesses": {
                    "value": "As a note, I was not able to follow the theoretical results, so my review is limited (as reflected by my confidence score). However, I believe there are several ways in which this paper could be made stronger:\n\n1. Q1 is motivated as being of practical importance - how many neurons are needed to achieve an accuracy of X%. While the theoretical results provide this, they are difficult to parse into practical considerations. It would be helpful to have predictions of the theory on how many neurons are needed (and how many layers) for a given accuracy plotted (for at least some choice of parameters described in Theorem 2.1). A comparison with an actual implementation of SNN would make the theoretical results especially convincing (if they match up). \n\n2. Q1, as phrased in the Introduction, suggests that it is unknown whether it is \"possible to approximate eigenvectors of large adjacency matrices with a neural network\". However, as noted by the authors, there has been work showing success in this direction already. Perhaps it would be better to phrase Q1 as \"are there theoretical guarantees that a neural network can approximate the eigenvectors of large adjacency matrices\". \n\n3. Sec. 2.2 is said to be aimed at \"constructive ways to approximate $\\textbf{Y}^*$, but it is unclear how Theorem 2.2 achieves this. There is not mention of optimization in the theorem, and the number of neurons is set to $N = \\infty$. While I understand that the number of neurons can be reduced (in Remark 2.3), this mismatch between aim and result disrupted the flow of the paper. \n\n4. The figures (Fig. 3-5) were mentioned only briefly in the introduction (before many of the details of the paper were introduced and I was under the impression that they would be referenced later in more detail. As it stands, I did not get anything from the figures. Including them later in the text, as the experiments are motivated (e.g., why it is reasonable to consider \"Near optimal\", \"Large gradient\", and \"Near saddle\") would greatly improve their impact. \n\n5. I did not understand what was meant in the Introduction by the \"spectral contrastive loss function $\\mathcal{l}$ is non-convex in the 'ambient' space' \" until the discussion about the need for the quotient geometry in Sec 3. I think making this point more clear earlier in the paper would help the reader understand why this is an interesting and tricky problem."
                },
                "questions": {
                    "value": "My questions can be found in the section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Reviewer_sU7q"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6410/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698710614877,
            "cdate": 1698710614877,
            "tmdate": 1699636713708,
            "mdate": 1699636713708,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IX6p64T0RF",
                "forum": "e0kaVlC5ue",
                "replyto": "EvXgKUn0MO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sU7q"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their interesting questions and suggestions. We are encouraged by their appreciation of our submission regarding its motivation, theoretical soundness, comprehensive literature review, and presentation. Below, we provide answers to the reviewer\u2019s questions. We have added most of these explanatory comments and changes in the updated manuscript in blue text.\n\n> Simulations. \n\nWe thank the reviewer for suggesting further simulations exploring the optimality of our theoretical results. This is a good suggestion that can be presented within the context of the broader research inquiries suggested by Reviewer hkrF. Having said this, we want to highlight the merits of our theoretical contributions in the present paper. Indeed, we bring together, in non-trivial and novel ways, a variety of results from several subfields (manifold learning, neural network approximation, Riemannian optimization, matrix factorization problems) that, in our view, have had limited theoretical interaction. We hope this paper motivates systematic theoretical explorations of important applications of neural networks to eigenvalue problems of differential operators on (unknown) manifolds in high dimensional spaces (see, for example, some suggestions by Reviewer hkrF).\n\n> About presentation. \n\nThe updated manuscript emphasizes the theoretical nature of our inquiries when describing Q1-Q3. The existing empirical results in the literature were indeed important motivators for our work.\n\n> Constructive nature in Theorem 2.2. \n\nWe want to highlight the differences in nature of Theorems 2.1 and 2.2: it is very different from stating that an approximating NN can be found by solving an optimization problem with a **clearly defined objective** (as in Theorem 2.2) than to simply state the existence of an approximating NN with no indication on how even to begin the search for it (as in Theorem 2.1.). While we agree that Theorem 2.2. does not describe an explicit algorithm to find this approximating NN, it is implicit that once the clearly defined target objective has been introduced, one can use any optimization algorithm to search for its minimizers. Because of this, and given our desire to highlight the conceptual differences between Theorems 2.1 and 2.2, we used the word constructive to describe the latter. We would be happy to modify this term if the reviewer believes that there is a more suitable alternative.  \n\n> Figures.\n\nWe thank the reviewer for their suggestion. In the revised manuscript, we refer to our figures after Remark 3.1 when discussing the theoretical results about the landscape of the objective $\\ell$.  \n\n> Highlight non-convexity of the loss function $\\ell$. \n\nWe thank the reviewer for their suggestion. In the revised manuscript, we have highlighted this point in the paragraph mentioned by the reviewer."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086633570,
                "cdate": 1700086633570,
                "tmdate": 1700086633570,
                "mdate": 1700086633570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2lzg6YbPT9",
                "forum": "e0kaVlC5ue",
                "replyto": "IX6p64T0RF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_sU7q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_sU7q"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their time and detailed comments. In reading through their responses, both to my own reviews, as well as those of the other reviewers, I believe the authors have done a good job addresses questions and concerns.\n\nI do have a few remaining comments/questions:\n\n1. Is it correct that the authors have not included any simulations relating number of neurons needed to achieve a given accuracy and any comparisons with any implementations of SNNs? \n\n2. The authors mention that Thm. 2.2 is distinct from Thm. 2.1, because it is in terms of a distinct objective. I think it would be helpful, for understanding this point, as well as seeing the connection between Sec. 3 and Thm. 2.2, if this objective was more clearly mentioned and its role in Thm. 2.2 more clearly stated.\n\n3. Having Figures 1 and 2 be subparts of Figure 3 is confusing (because they are easy to miss and it can suggest to readers there are missing figures)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090233207,
                "cdate": 1700090233207,
                "tmdate": 1700090233207,
                "mdate": 1700090233207,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kSMWWERe4L",
                "forum": "e0kaVlC5ue",
                "replyto": "EvXgKUn0MO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your detailed comments"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful feedback on our manuscript and response.\n\nWe have taken your suggestions into consideration and made the following revisions:\n1. Removed the title of Figure 3.\n2. Reformatted the objective function to be presented in a single line.\n\nWe believe these changes enhance the clarity of our paper.\n\nRegarding the simulation of neuron numbers for accuracy approximation in eigenvector calculations, we acknowledge this as an intriguing area for future research. This aspect, closely tied to the Laplace-Beltrami operator as highlighted in Corollary 1, offers a promising avenue for exploring the relationship between theoretical results and empirical findings. However, our current focus is on the theoretical foundations of Spectral Neural Networks (SNNs). Consequently, our current simulations were confined to the parameterized and ambient optimization landscapes in SNNs.\n\nFuture studies could also fruitfully compare SNNs with other neural network models for spectral embedding tasks. While incorporating a detailed discussion on approximations to Partial Differential Equation (PDE) operators would indeed be valuable, we believe it might overburden the scope of this paper. Such topics merit a thorough examination in subsequent research.\n\nWe appreciate your consideration of our revisions and suggestions for simulation results. We believe the improvements address your initial concerns, enhancing both the clarity and rigor of our work.  If you find that our revisions meet your expectations and improve the overall quality of the manuscript, we would be grateful if you could reflect this in an updated assessment on rating or confidence."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186886119,
                "cdate": 1700186886119,
                "tmdate": 1700187036559,
                "mdate": 1700187036559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KBSqS7DvtG",
                "forum": "e0kaVlC5ue",
                "replyto": "kSMWWERe4L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_sU7q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_sU7q"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for making these changes. I believe they do indeed make the paper stronger. \n\nWhile I understand the focus of the paper is the theoretical foundation of SNNs, I believe that connecting the theory to empirical results would make a clearer paper that strengthens the analytical results. That being said, not having it will not decrease my score (just not increase my score)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656437228,
                "cdate": 1700656437228,
                "tmdate": 1700656437228,
                "mdate": 1700656437228,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7whGOlMwla",
            "forum": "e0kaVlC5ue",
            "replyto": "e0kaVlC5ue",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_BBbU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_BBbU"
            ],
            "content": {
                "summary": {
                    "value": "The paper makes three main contributions:\n\n* It establishes approximation bounds on the depth and the number of neurons needed for a multi-layer neural network to accurately approximate top eigenvectors of a normalized graph Laplacian matrix constructed from data samples lying on a manifold. \n\n* It shows that by globally optimizing the spectral contrastive loss function, one can provably construct a neural network that approximates the eigenvectors up to rotations. \n\n* Motivated by experiments, the paper analyzes the non-convex optimization landscape of the ambient spectral contrastive loss function and shows it to be benign."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Given the amount of recent interest in using neural networks to approximate eigenfunctions, establishing theoretical guarantees for such algorithms is timely and of interest to the community. \n\n* The proofs of results in section 2 looks sensible to me, except the question mentioned below."
                },
                "weaknesses": {
                    "value": "It should be noted that my review did not cover section 3, as I found it difficult to absorb in a reasonable amount of time given the mathematical depth. \n\n* The theory established in this paper seems far from providing any practical insights on training spectral neural networks besides proving the feasibility of such an approach--at least the authors did not attempt to include argument like this in the paper. Not covering data-dependent kernels as those in HaoChen and most SSL work also reduce the significance of the result.\n\n* The proof of the approximation result (Theorem 2.1) is quite straightforward, combining (known) ReLU network approximation results (Chen et al., 2022) with a Lipschizness-like condition for eigenvectors on manifolds (Calder et al., 2022).  It is not clear whether or not the proof could be useful for future theoretical work in this space. \n\n* The presentation of Theorem 2.2 and its proof needs clarification, e.g., I found the paragraph following Corollary 5 difficult to understand: \"Using the fact that bar{U} is invertible, we can easily see that Y_\\theta* ....\". Can you clarify how to get this result? I assume Y_\\theta* is the Y recovered by the optimal neural network that minimizes the spectral contrastive loss. I also find it difficult to see the reasoning behind Remark G.1-G.3 and how they fit into the proof.\n\n* The exposition of the main theorems can be improved. The neural network family constants are never defined in the main text and it makes Theorem 2.1 very hard to read. I strongly encourage the authors to provide intuitive explanations before/after each theorem to aid the understanding of assumptions used, proof ideas, and implications of the result."
                },
                "questions": {
                    "value": "Please see the above question about proof of Theorem 2.2."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6410/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699322775406,
            "cdate": 1699322775406,
            "tmdate": 1699636713575,
            "mdate": 1699636713575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9apHfwEY59",
                "forum": "e0kaVlC5ue",
                "replyto": "7whGOlMwla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BBbU"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and questions, which we address below. We are also encouraged by the reviewer\u2019s appreciation for our theoretical contribution to the ML community.\n\n> Significance of approximation result.\n\nWe start by highlighting that SNNs are useful beyond SSL and the context of the work of HaoChen et al. 2021 Indeed, our results can justify the use of SNNs to solve eigenvalue problems of differential operators on manifolds. The use of SNN in this context is novel, and in particular the induced training procedure is quite different from popular approaches in the literature such as Physics Informed Neural Networks (PINNS). The use of SNN beyond standard tasks in machine learning is thus of interest to the broader scientific community. See, for example, our response to Reviewer hkrF and the Conclusions section in our paper.\n\nOn the other hand, we emphasize that studying the graph constructions in HaoChen et al. 2022 is an interesting research direction and we expect that a similar analysis to the one presented in our paper can be carried out in settings that are more useful for SSL. However, to achieve this, one would first need to generalize some of the theoretical results in Calder et al 2022. This mathematical task seems feasible, but only after a lengthy analysis. This can be explored in future work.\n\n> The proof of Theorem 2.1.\n\nWe refer the reviewer to the general response to reviewers. \n\n> The decomposition of $Y_{\\theta^*}$. \n\nRecall that $\\bar{U}$ is invertible. Hence, let $E = \\bar{U}^{-1}Y_{\\theta^*}$. We let $E^1$ be the first $r$ rows of $E$ with the rest of rows being equal to 0. Similarly, $E^2$ has the rest of the rows of $E$. \n\n> Remarks G.1 - G.3. \n\nMore than being necessary for our proofs, these remarks help convey why our results are not direct consequences of existing results and that careful analysis is needed to deduce them. This is closely connected to the discussion in our general response to reviewers. \n\n> The neural network family that we consider. \n\nWe thank the reviewer for their suggestion. The family of approximating functions was defined in detail in (C.2); this was mentioned in the statement of Theorem 2.1. We do not include its (long) definition in the main text because of space limits. Given all the other important things we needed to include in the main body of the paper, we were forced to move some details to the appendix. We also want to highlight that many remarks in the main body of the paper and in the appendix were included to help convey the relevance of our theoretical results. \n\n\nWe hope our explanations can help convey that our work is well motivated and that the mathematical problems we study are exciting and non-trivial."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086569318,
                "cdate": 1700086569318,
                "tmdate": 1700086569318,
                "mdate": 1700086569318,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nWzY6O4B2w",
            "forum": "e0kaVlC5ue",
            "replyto": "e0kaVlC5ue",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_SXMx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_SXMx"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers minimizing the loss function $\\|Y(\\theta)Y(\\theta)^T-A_n\\|^2_F$, where the low rank matrix $Y$ is estimated by a neural network. The main claims of the paper are that: (C1) the optimal $Y^*$ can be well approximated by a neural network; (C2) the global minima of the loss function is close to the optimal $Y^*$ (up to a rotation); (C3) the loss function $\\|YY^T-A_n\\|^2_F$, as a function of $Y$, has nice geometrical properties."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper explicitly writes out the approximation error bound, requirements on the depth and number of neurons, of the neural network on this spectral approximation problem. \n\nIt also showcase a few nice properties of $\\|YY^T-A_n\\|^2_F$, as a function of $Y$."
                },
                "weaknesses": {
                    "value": "Given the universal approximation theorem of neural networks, it is expected that there exist a neural network that can approximate the optimal matrix $Y^*$. Hence, (C1) should be a natural result. Moreover, most of the techniques seem not new and appeared in prior works, e.g., (Chen et al. 2022). \n\nGiven (C1), the second main claim (C2), e.g., Theorem 2.2, should be quite obvious. (C2) does not discuss the solvability of the optimization problem, i.e., how to find the global minima. Hence, I don\u2019t see a \u201cconstructive way\u201d to find the approximation. To me, this part (C2) is more like a result of the existence of such a neural network, which highly overlaps with the claim in (C1). \n\nWhen analyzing the loss landscape, in Section 3, the paper does not consider the loss function as a function of the network parameters. However, it considers it as a function of the network output. More explicitly, it is basically analyzing $\\|YY^T-A_n\\|^2_F$ as a function of $Y$, not of $\\theta$. First of all, this \u201clandscape\u201d is not the optimization landscape we are mostly interested in. One has to compose it with the network function $Y(\\theta)$ to have the full optimization loss. As we know, the hard part is the network function $Y(\\theta)$. Second, given the simple and symmetrical form of $\\|YY^T-A_n\\|^2_F$, the results presented in Section 3 are not hard to obtain. \n\nThe presentation of the paper can be improved. For example, I had a hard time understanding the notations in the theorems. For example in Theorem 2.1 it is not clear what is $\\epsilon$ and $m$. In Eq.(1.2),  $\\epsilon$ is used for the \u201cbandwidth\u201d for similarity, however, in the proof of Theorems, $\\epsilon$ seems an arbitrarily small positive number. In addition, the meaning of $m$ was not mentioned in the statement of the theorem or its proof."
                },
                "questions": {
                    "value": "no further questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6410/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699383653976,
            "cdate": 1699383653976,
            "tmdate": 1699636713459,
            "mdate": 1699636713459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AgPQvWF0zI",
                "forum": "e0kaVlC5ue",
                "replyto": "nWzY6O4B2w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SXMx"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and questions, which we address in what follows.\n\n> The approximation results in Section 2. \n\nPlease see the general response to reviewers.\n\n> Constructive approximations in Theorem 2.2.\n\nTheorem 2.2. is not a mere existence result. It is very different to state that an approximating NN can be found by solving an optimization problem with a **clearly defined objective** than to simply state the existence of an approximating NN with no indication on how even to begin the search for it (as in Theorem 2.1.). That is, existence just says there are parameters such that something is true, but gives no indication on how to find these parameters. Of course, once one has a clearly defined objective, one can ask the natural follow-up question: what is the behavior of popular optimization algorithms when trying to optimize the target objective? This is what we explore in Q3. \n\n> The landscape results in Section 3.\n\nAs was discussed in paragraphs 2, 3, and 4 on page 4, the optimization landscape is non-trivial because there are two sources of \u201cnon-convexity\u201d: 1) the non-linearity of neural networks and 2) the non-convexity of the ambient loss function. With our numerical illustration (Figures 4 and 5), we suggest that the properties of the landscape for the SNN problem are expected to resemble those of the ambient loss landscape, at least in some form of overparameterized regime. A precise mathematical statement along these lines will be explored in the future and deserves its own careful analysis. As we discussed in paragraphs 2, 3, and 4 on page 4, as well as in the last paragraph of the Conclusions section, this problem is different from other analyses in the literature of training dynamics of overparameterized neural networks because in our case, the ambient loss function $\\ell$ is non-convex, not because neural networks are nonlinear functions. \n\n> Notation.\n\nWe defined $\\epsilon$ in the surroundings of Equation (1.2) and defined $m$ in Assumption 2.1. It is the underlying manifold\u2019s intrinsic dimension. $\\epsilon$ in this work is always used as the length scale that determines the graph connectivity. $\\delta$, on the other hand, is a tunable approximation error. Notice that the size of $\\epsilon$ is implicitly controlled by the number of data points available since our error estimates are only meaningful with a likelihood that depends on $n$ and $\\epsilon$.\n\nWe hope our explanations can help convey that our work is well motivated and that our mathematical contributions are novel, sound, and far from trivial."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086513855,
                "cdate": 1700086513855,
                "tmdate": 1700086513855,
                "mdate": 1700086513855,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XqmWBkOKLa",
                "forum": "e0kaVlC5ue",
                "replyto": "AgPQvWF0zI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_SXMx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_SXMx"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed response. However, my major concerns remain.\n\n1: Given the universal approximation property of neural networks, it is almost obvious to have theorem 2.1. It seems to me that the novelty is to explicitly write out the expressions of requirement and probability, which is not sufficient to meet the ICLR standard.\n\n2: Given the approximation result, I still think it is trivial to translate it into solving the optimization problem. I could not think it as a contribution of the paper.\n\n3: As for the loss landscape, it is actually just analyzing the $||YY^T-A_n||_F$ as a function of $Y$ instead of $\\theta$. Given the simple form of this function, I think it is a course project level problem to analyze. The interesting aspect is still the loss landscape as a function of the model parameters $\\theta$.\n\nOverall, I think the contributions of the paper are minor. I would like to keep my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679431793,
                "cdate": 1700679431793,
                "tmdate": 1700679431793,
                "mdate": 1700679431793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6WUd5KiqKx",
            "forum": "e0kaVlC5ue",
            "replyto": "e0kaVlC5ue",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_hkrF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_hkrF"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the objective of the \"spectral neural networks (SNN)\". \nThe objective is defined by the squared Frobenius norm of the approximation error for the kernel matrix (which is a specific graph Laplacian in the current scope) via its low-rank approximation. \nBy the Eckart--Young--Mirsky theorem, the ambient optimization problem guarantees that its global optimizer recovers the top-$r$ eigenbasis up to an orthogonal transformation. SNN is a neural network that is optimized by this objective, where the neural network outputs parameterize the eigenvectors. \nThe difficulty in the analysis of the SNN mainly comes from the fact that the ambient optimization problem (i.e., when the optimization is done in the nonparametric way without neural network parameterization) is non-convex.\n\nThe paper's contribution is threefold. First, the authors prove that there exists a MLP that can well-approximate the top-$r$ eigen-subspace, with sufficiently large number of neurons, under the manifold assumption (Theorem 2.1). \nSecond, it is shown that, under the same assumption, if the MLP architecture used in the optimization is sufficiently large, then the global optimizer attained by the architecture closely captures the top-$r$ eigenbasis up to a rotation (Theorem 2.2). Roughly speaking, a good MLP can be \"constructed\" by optimizing the SNN objective. \nLastly, the authors analyze the optimization landscape of the \"ambient\" problem, by examining three different regimes (Theorem 3.1-3.4).\n\nThe first two theorems constitutes an approximation theory of neural networks for the graph Laplacian matrix. Theorem 2.1 is a general approximation theory, while Theorem 2.2 is a result that specifically applies to the SNN objective. \nThe results in Section 3 solely cares about the ambient optimization problem being independent of a neural network parametrization, but these results are applicable for any PD matrix with a positive eigengap."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Using neural networks to parameterize eigenfunctions of an operator is a promising approach that has a great potential in many applications for large-scale, high-dimensional data. \nIn particular, the optimization framework of SNN is particularly appealing as an unconstrained problem, in contrast to the existing work such as SpectralNet which considers a constrained optimization problem.\nHence, understanding characteristics of the SNN optimization problem is an important problem.\nThe results in this paper can serve as a good initial attempt in establishing a theory in this context.\n\nThe paper is overall well thought-out, considering the level of technicalities involved in the analysis. \nThe results are well-motivated with illustrations in the introduction.\nI think the paper provides good insights for the subject by carefully putting the recent results in spectral approximation, neural-network approximation, and Riemannian optimization, and thus worth of publication in this venue in general."
                },
                "weaknesses": {
                    "value": "I believe that the manuscript is missing some works in the literature, and adding and discussing these will better guide the reader.\n\nIt is a little bit obscure what can be said beyond the graph Laplacian with MLPs under the manifold assumption, considering that there exist other important operators in different applications. For example, decomposing a Hamiltonian operator with neural networks has shown promising results in quantum chemistry, see, e.g., [A].\nAlso, there exists a recent paper on analyzing the \"generalization error\" of MLPs in solving the Schr\u00f6dinger equation [B]. (It would be nice if a generalization error can be analyzed in the current paper, and if so or not, what could be challenge.)\n\nAnother line of research missing in the current paper is the recent work on generic NN-based eigensolvers [C], [D] that aim to recover the ordered top-$r$ eigenbasis (i.e., without modulo rotation) unlike the SNN and current work. \n\n[A] Hermann, Jan, Zeno Sch\u00e4tzle, and Frank No\u00e9. \"Deep-neural-network solution of the electronic Schr\u00f6dinger equation.\" Nature Chemistry 12.10 (2020): 891-897.\n[B] Lu, Jianfeng, and Yulong Lu. \"A priori generalization error analysis of two-layer neural networks for solving high dimensional Schr\u00f6dinger eigenvalue problems.\" Communications of the American Mathematical Society 2.1 (2022): 1-21.\n[C] Pfau, David, et al. \"Spectral Inference Networks: Unifying Deep and Spectral Learning.\" International Conference on Learning Representations. 2018.\n[D] Deng, Zhijie, Jiaxin Shi, and Jun Zhu. \"Neuralef: Deconstructing kernels by deep neural networks.\" International Conference on Machine Learning. PMLR, 2022."
                },
                "questions": {
                    "value": "- Remark 2.2 is hard to appreciate. Can you explain the reasoning behind this in detail?\n- In the paragraph after (eq. 3.3), \"Finally\" is used twice."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Reviewer_hkrF"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6410/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699611100203,
            "cdate": 1699611100203,
            "tmdate": 1699636713350,
            "mdate": 1699636713350,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lchUxwJEYu",
                "forum": "e0kaVlC5ue",
                "replyto": "6WUd5KiqKx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hkrF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments and their positive feedback. In particular, we thank them for their insightful comments on the use of NNs to solve eigenvalue problems for more general differential operators. Indeed, we agree with the reviewer that it is of interest to provide theoretical guarantees for the use of a SNN-like approach to find the spectrum of other differential operators such as the ones in the Schr\u00f6dinger equation. As the reviewer mentions, this is of relevance for the broader scientific community.\n\nWe hope our edits improve our manuscript\u2019s clarity and also better position our work in the landscape of existing results on similar problems. All references provided by the reviewer were incorporated in the paper. New text has been highlighted in blue. Below we provide answers to the reviewer\u2019s questions.\n\n\n\n> Other differential operators and Generic NN-based eigensolvers.\n\nWe have added references to the papers the reviewer mentioned. In Section A.2 we have added a brief discussion on the eigensolvers mentioned by the reviewer. \n\n>On Remark 2.2.  \n\nRemark 2.2. aims at highlighting that the $\\epsilon^2$ term in our error estimates in Theorem 2.1. is essential for our proof of Theorem 2.2. As discussed in our general response, this $\\epsilon^2$ term could not have been obtained by directly fusing the results in Calder et al 2022 and Chen et al 2022. In order to get the term $\\epsilon^2$, we needed to first obtain a stronger quantitative estimate for the regularity of the (discrete) graph Laplacian eigenvectors (stronger than the ones explicitly stated in Calder et al 2022) and then apply the results in Chen et al 2022 to find an NN approximator of a suitable extrapolation of the discrete eigenvector to the manifold (different from an associated eigenfunction!, for otherwise the error of approximation would not be $\\epsilon^2$ but $\\epsilon$). In order to obtain the better discrete regularity for graph Laplacian eigenvectors, we used the fact that graph Laplacian eigenvectors converge in a stronger semi-norm (almost $C^{0,1}$) toward Laplace-Beltrami eigenfunctions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086452042,
                "cdate": 1700086452042,
                "tmdate": 1700086452042,
                "mdate": 1700086452042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IlZ5jUoq2P",
                "forum": "e0kaVlC5ue",
                "replyto": "lchUxwJEYu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_hkrF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_hkrF"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response. I read all other reviews and discussion, and learned that there exist some pushback on the contribution of the paper. I still think that the paper's contribution should be judged from the entire story around understanding the theoretical properties of the neural-network-based approach for eigensubspace learning, rather than looking at each result separately. In terms of the entire story, I believe that there is a catch in this paper.\nThat said, I think some discussion on the connection from the literature such as Luo and Garcia Trillos (2022) needs be supplemented, as Reviewer JxCW pointed out. I can see that the work is being mentioned throughout in Section 3, but I think it is proper to mention and discuss this paper in Related Work section to indicate readers that while there is a very similar recent analysis in blah, the current paper has a stronger result. \n\nAnother comment, though a bit tangential to the focus of the paper, I wish to make is that though the authors argued in one of the response that the ordered eigenbasis can be postprocessed from the Gram-Schmidt like process, in practice the idea may not work well in practice and finding a structured basis from scratch might be a better solution depending on an application at hand. In this perspective, I think it is also important to make it clear that the paper and SNN focuses on the applications such as spectral clustering that might not require ordered eigenbasis, unlike the traditional eigensolvers.\n\nAll in all, I will keep the score, but decrease my confidence to reflect my agreement on some concerns of other reviewers."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725588645,
                "cdate": 1700725588645,
                "tmdate": 1700725588645,
                "mdate": 1700725588645,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pSXS3h0PpI",
            "forum": "e0kaVlC5ue",
            "replyto": "e0kaVlC5ue",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
            ],
            "content": {
                "summary": {
                    "value": "The paper is a theoretical study of Spectral Neural Networks (SNNs). The problem considered in the paper consists in efficiently approximating an adjacency matrix by a product $\\mathbf Y\\mathbf Y^{\\mathrm T}$. The first main result (Theorem 2.1) gives a bound on the complexity of a neural network that provides an approximate solution of this problem. The second result (Theorem 2.2) shows that a solution provided by the network is close to a global minimizer, up to a rotation. The remaining results (Theorems 3.1-3.4) study the structure of the loss surface by dividing it into several regions with particular properties. The first region is a neighborhood of the optimal solution and the loss is geodesically strongly convex there (Theorems 3.1). Another region is the neighborhood of suboptimal stationary points. These points are described in Theorem 3.2, and Theorem 3.3 shows that near these points there are escape directions so that gradient flow is not trapped. Finally, Theorem 3.4 shows that in the remaining regions the gradient is large. Combined, these results suggest that the considered optimization problem can be efficiently solved by gradient descent."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Contribution, originality, novelty.**  The paper relies very heavily on previous research of matrix factorization and SNNs such as HaoChen et al., 2021 and Luo & Garc\u00b4\u0131a Trillos, 2022. My impression is that the present paper does not bring any fundamentally new ideas compared to previous publications. In particular, the main message expressed in it is that the considered optimization is practically feasible and the loss landscape is benign despite the non-convexity. The same message is found in Luo & Garc\u00b4\u0131a Trillos, 2022 in a similar wording. Moreover, the theorems found in section 3 of the present paper are extremely similar to the theorems in Luo & Garc\u00b4\u0131a Trillos, 2022. The present paper indicates some differences with that earlier paper (e.g., in Remark 3.2), but they are not clearly explained and seem to be rather technical. The paper Luo & Garc\u00b4\u0131a Trillos, 2022 is referred to multiple times in the present paper, but is not mentioned among the related works, which is confusing.  \n\n**Writing and clarity.** On the whole, the paper is clearly written and has a big appendix containing details of its several theorems. At the same time, there are various small issues with the exposition (see below)."
                },
                "weaknesses": {
                    "value": "In addition to the limited novelty mentioned above, the paper suffers from some lack of clarity.\n\n1. The beginning of the introduction sounds like the goal of the paper is to develop and analyze a neural network-based eigensolver.  My understanding of an eigensolver is that this is an algorithm that produces the full list of eigenvalues and eigenvectors. However, the method considered in the paper gives us much less: first, it is restricted to $r$ largest eigenvalues and, second, the produced matrix $\\mathbf Y$ contains eigenvectors only up to an $r\\times r$ rotation, so there is still work to be done to extract the eigenvectors and eigenvalues. These points are not discussed; moreover, in the comparison of SNN with traditional eigensolvers only the advantages of the SNN are mentioned.\n\n2. *\u03b7 is a decreasing, non-negative function* - where do you use that $\\eta$ is decreasing?  \n\n3. *$D_G$ is the degree matrix associated to $G$* - what exactly is the definition of $D_G$?\n\n4. In Theorem 3.2, the notation for matrices with the subscript $S$ seems unexplained. Also, the matrix $\\Lambda$ is not defined (only $\\overline{\\Lambda}$ is defined)."
                },
                "questions": {
                    "value": "I would like the issue with the connection of the present paper to Luo & Garc\u00b4\u0131a Trillos, 2022 to be clarified.\n\nIn general, I think that the Related Work section should clearly indicate the papers that are especially closely connected to the current work, and explain the differences and the added value of the current work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6410/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699643767542,
            "cdate": 1699643767542,
            "tmdate": 1700665164484,
            "mdate": 1700665164484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RzQ1sP8Ll3",
                "forum": "e0kaVlC5ue",
                "replyto": "pSXS3h0PpI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JxCW"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their questions and comments, which we address below. We are encouraged by the reviewer\u2019s appreciation of our presentation and theoretical analysis. *We have added most of these explanatory comments and changes in the updated manuscript in blue text.*\n\n> Only recovering eigenvectors up to rotation. \n\nWe do not believe this is a limitation. First, notice that for many important downstream tasks, such as spectral clustering and spectral contrastive learning, it is sufficient to generate unions of eigenspaces, and thus, recovering individual eigenvectors may not be necessary for those tasks. On the other hand, if one indeed desires to generate the first K individual eigenvectors of $A_n$, it suffices to simply run the SNN algorithm for $r=1, \\ldots , K$ and carry out a Gram-Schmidt like procedure with the outcomes. This pipeline would indeed avoid the use of eigensolvers, as claimed.\n\n> Assumption on $\\eta$.\n\nThe non-increasing assumption on $\\eta$ appears in multiple results in the manifold learning literature. We used some of these results in the proofs of our theorems. Notice that most $\\eta$ used in practice (e.g., Gaussian, 0-1) satisfy this assumption.  \n\n> The definition of $D_G$.\n\n$D_G$ is the diagonal matrix whose $i$-th diagonal entry is the degree of the point $x_i$. $D_G$ was defined in the Appendix in the paragraph below (A.5). In the updated manuscript, we have added an explicit reference to its definition. \n\n> Notations in Theorem 3.2.\n\nThere is a typo in Theorem 3.2. We had not defined $\\Lambda$. We thank the reviewer for pointing this out. We have now incorporated it in the updated manuscript.\n\n> About the related work and novelty.\n\nFirst, we want to point out that the work Luo & Garcia Trillos 2022 that the reviewer mentions makes very different assumptions on the matrix $A_n$ than the ones we make in our paper. The implications of the different assumptions are indicated at multiple points in the paper: in Remark 3.2, the discussion below Theorem 3.2, and Remarks H.1 and H.2. \n\nOur paper studies SNNs comprehensively and combines multiple results from several subfields (such as manifold learning, neural network approximation theory, and Riemannian optimization) in a way that is original and far from trivial. Indeed, as we explain in our general response to reviewers (see also the response to Reviewer hkrF), we had to tackle multiple technical difficulties to bring together existing results in the literature. The paper Chen et al. 2022 that the reviewer mentions is indeed a useful auxiliary result for our work, but by no means are our Theorems 2.1 and 2.2 corollaries of it."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086383332,
                "cdate": 1700086383332,
                "tmdate": 1700086383332,
                "mdate": 1700086383332,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AUzvTga77D",
                "forum": "e0kaVlC5ue",
                "replyto": "RzQ1sP8Ll3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response and clarifying some points that I have raised. \n\nHowever, I have to say that my main issue with the paper - its very high similarity to Luo and Garcia Trillos 2022 and a lack of acknowledgment of this fact - is not resolved by this response (or the general response to reviewers). It is obvious that in section 3 the present paper very closely follows that earlier paper. The idea of \"benign landscape\" and the Riemannian approach are the same; the domain is decomposed into the same subsets $\\mathcal R_1,\\mathcal R_2,\\mathcal R_3',\\mathcal R_3'',\\mathcal R_3'''$; theorems 3.1, 3.3, 3.4 describing the loss landscape in particular subsets are very similar to the respective theorems 3, 4, 5 in Luo and Garcia Trillos 2022. The similarities are much more pronounced than the differences. But (in contrast to the differences) the similarities are not acknowledged in the paper. The Related work section mentions all kinds of works, some only marginally relevant, but does not mention Luo and Garcia Trillos 2022."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567151383,
                "cdate": 1700567151383,
                "tmdate": 1700567151383,
                "mdate": 1700567151383,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P8Z1RhIkf3",
                "forum": "e0kaVlC5ue",
                "replyto": "pSXS3h0PpI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our manuscript and providing your feedback. We have now incorporated a brief discussion on the global optimization landscape from Luo & Garcia Trillos 2022 into the related work section of our paper. Please see our revised manuscript, where this is highlighted in blue.\n\nIn response to your concerns we would like to highlight the following:\n\n1. The reviewer\u2019s main concern is that our paper is very much like Luo and Garc\u00eda Trillos, yet the reviewer has focused entirely on one single aspect of our paper (more on this below) and did not discuss more than half of our paper, which has little to no overlap with the mentioned paper. In particular, there is no mention of the motivation for our problem, nor to the approximation theory results Theorems 2.1 and 2.2, nor to the connection with learning solutions of PDEs, nor to the prospect of using our ideas to provide a theoretical basis for the use of SNNs in other problems in the sciences (as other reviewers have highlighted). In fact, there is not even a mention to our numerical simulations, which try to convey why studying the landscape of the ambient problem may be relevant for the SNN training problem, which is what motivated some of the open problems we formulated in the paper.  \n\n2. We do mention the extent of the similarities between our section 3 and the paper of Luo and Garcia Trillos. Please see our Remark 3.1 in the main text and the multiple remarks and comments in section H. \n\n3. The concepts of benign landscape and the Riemannian approach are very broad ideas in the optimization literature, not just ours or Luo & Garcia Trillos 2022\u2019s. A quick online search reveals this. \n\n4. R1, R2, R3 are indeed similarly defined as in Luo & Garcia Trillos, but, as explained extensively in the paper and in our previous replies to all reviewers,  the challenge is in showing that those regions indeed satisfy the desired properties to describe the landscape of our ambient problem as benign: local strong convexity around global minimizer, escape direction for saddles, large gradient in all other regions. In other words, R1, R2, R3 are ansatz, which are naturally inspired by previous work, but this doesn\u2019t imply the ansatz is trivially correct in our case. There is an enormous difference between 1) guessing what a good answer could be and 2) proving that this guess is indeed a good answer. We thus invite the reviewer to compare the papers in more detail, and not focus exclusively on their superficial similarities. On this last point, we reiterate our discussion in 1. above."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589753639,
                "cdate": 1700589753639,
                "tmdate": 1700589826330,
                "mdate": 1700589826330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nxZddFTTAi",
                "forum": "e0kaVlC5ue",
                "replyto": "P8Z1RhIkf3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response.\n\n1. I keep my opinion that the paper is misleading in its novelty claims. The paper states as one of its main contributions: \"*we begin an exploration of the optimization landscape of SNN and in particular provide a full description of SNN\u2019s associated ambient space optimization landscape*\". I don't see how this can be true, since a significant part of this work has obviously been already done in a different publication. I agree that the paper strengthens a previous analogous optimization landscape result. However, this is far from what the paper claims. I believe that the proper way to handle that would be to explain this previous result in the introduction along with the challenges associated with its improvement, and to claim the improvement. \n2. Regarding the other results of the paper, I agree that it contains new contributions in the form of motivating experiments and the approximation results in section 2. At the same time, I'm not convinced in the importance of these results. My understanding is that Theorem 2.1 is obtained by applying existing general statistical and approximation frameworks to an existing regularity property for the eigenvectors. There is definitely a new technical contribution here. But conceptually the overall approach seems standard, the result is not surprising, and the importance of the specific bounds established in Theorem 2.1 is not clear to me. As for Theorem 2.2, technical aspects aside, I see it as basically saying that under the spectral gap assumption the approximate minimizer of the ambient functional (1.5) is close to an actual minimizer up to a rotation, which is a standard result from linear algebra. There is definitely a new technical contribution in establishing relevant specific bounds for the network size, probability, approximation error etc. as appear in Theorem 2.2. But again, why these particular bounds are important is not very clear to me.  \n\nNevertheless, I admit that I have not studied the proofs and may have missed something, so following this discussion I'm increasing my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665150323,
                "cdate": 1700665150323,
                "tmdate": 1700665150323,
                "mdate": 1700665150323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BmQqOIGVmF",
                "forum": "e0kaVlC5ue",
                "replyto": "lc23lrX2LI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6410/Reviewer_JxCW"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response.\n1. \"*we do initiate the exploration of the SNN problem (i.e., the NN parameterized problem, Luo and Garc\u00eda Trillos do not talk about NN at all!)*\". \\\nAgain, I do not understand this claim. Section 3 on the loss landscape says nothing about neural networks. It only considers the optimization problem (1.5) in the ambient space, analogous to the problem in an earlier paper.\n2. \"*What is a result from linear algebra?*\" \\\nI referred to the uniqueness part of the Eckart\u2013Young\u2013Mirsky theorem, which implies that, under the gap assumption, if $l(\\mathbf Y)$ is close to the minimal value of the functional $l$, then $\\min_{\\mathbf O\\in\\mathbb O_n}\\\\|\\mathbf Y-\\mathbf Y^*\\mathbf O\\\\|_F$ is close to 0. As I wrote, I agree that specific quantitative information regarding the approximation (in particular, in the space of neural networks) found in Theorem 2.2 is a new technical contribution of the paper.\n\nI think that it is by now clear that the authors and I have different opinions on some novelty aspects of the paper, so I thank the authors again and conclude this discussion."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6410/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693319030,
                "cdate": 1700693319030,
                "tmdate": 1700693319030,
                "mdate": 1700693319030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]