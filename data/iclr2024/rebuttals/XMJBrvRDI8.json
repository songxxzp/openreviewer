[
    {
        "title": "Hierarchically branched diffusion models leverage dataset structure for class-conditional generation"
    },
    {
        "review": {
            "id": "DP413PLyJm",
            "forum": "XMJBrvRDI8",
            "replyto": "XMJBrvRDI8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5985/Reviewer_MogQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5985/Reviewer_MogQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel framework for class-conditional generation using diffusion models, which are models that can generate realistic objects by reversing a noisy diffusion process. The framework called hierarchically branched diffusion models, leverages the hierarchical relationship between different classes in the dataset to learn the diffusion process in a branched manner. The framework has several advantages over existing methods, such as being easily extendable to new classes, enabling analogy-based conditional generation (i.e. transmutation), and offering interpretability into the generation process. The paper evaluates the framework on several benchmark and large real-world scientific datasets, spanning different data modalities (images, tabular data, and graphs)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The paper addresses a novel and important problem of class-conditional generation using diffusion models, which can capture the rich information and structure of the data classes.\n* The authors introduce a novel and flexible framework that can exploit the inherent hierarchy between distinct data classes by using branch points and can handle different types of diffusion models and paradigms.\n* Extensive experiments and analysis are conducted to demonstrate the advantages of the proposed framework in continual learning, transmutation, and interpretability."
                },
                "weaknesses": {
                    "value": "* The paper is well-written, but some claims could be improved for clarification. For example, in Section 4 Page 6, it is unclear how the model performs with versus without fine-tuning the upstream branches that also diffuse over the newly added class in the continual learning setting without certain empirical results as support. Another example is that I found the observation in Section 5 Page 6 that letters with a larger feature value tended to transmute to letters with a larger feature value is hard to interpret from Figure 3b with only scatterplots of some feature values given. \n\n* The efficiency of branched models over standard linear models when sampling multiple classes is clear from Table S9 when the number of classes is relatively small. Could the authors provide some insights when the number of classes grows very large with potential comparison regarding complexity analysis?\n\n* In Figure 4, the interpretation at immediate branch points between two classes is shown and aligned with intuition. I am curious about how the visualization is for the branch point from more upstream such as the branch point between class 0 and the immediate branch point between class 4 and 9."
                },
                "questions": {
                    "value": "* Why the FID between true and generated cells are the same for the branched and linear model in Figure S3 c)?\n\n* Could the authors give some insights on why only one baseline of the label-guided (linear) models (Ho et al., 2021) is used for comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Reviewer_MogQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5985/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698230358154,
            "cdate": 1698230358154,
            "tmdate": 1700723327520,
            "mdate": 1700723327520,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jOc6nYmap6",
                "forum": "XMJBrvRDI8",
                "replyto": "DP413PLyJm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review (1)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review and provide valuable feedback! We have answered all the questions and comments in the following.\n\n**Fine-tuning upstream branches (W1)**\n\nIn our experiments of continual learning in branched diffusion models, we found that fine-tuning upstream branches did not help (or hurt) the generative performance of downstream tasks. This is a very expected result, as our branch points are explicitly defined to be the point in diffusion time after which classes diffuse nearly identically. Hence, fine-tuning these upstream branches is not expected to significantly change the model\u2019s performance. We will better clarify this point in the revised version.\n\n**Transmutation of letters (W1)**\n\nIn Figure 3b, we showed the scatterplots for just two examples of features, but the histograms below (still in panel 3b) show that we found positive (often strongly positive) correlations for *all* features in the dataset. Of course, if analogous features were not being retained in the process of transmutation, we would expect these histograms to be tightly centered around 0.\n\n**Efficiency of branched models with more classes (W2)**\n\nThe speedup of branched-diffusion models is only indirectly influenced by the number of classes, and instead is directly determined by the *hierarchy* and how much diffusion time can be shared. The amount of shared diffusion time, in turn, is a function of how similar different classes are (and the diffusion process). For datasets with a lot of very similar classes, we are able to share much more diffusion time, and therefore will enjoy more speedup. However, datasets with very distinct classes will have very late branch points, leading to lower efficiency. It is also worth mentioning that datasets with only very distinct classes are not particularly well-suited to the method of branched diffusion which we propose in this work. Instead, the main contribution of our work is intended for scientific applications, whose datasets are typically characterized by a large amount of internal structure (e.g. cell types, chemical classes, structural folds, etc.).\n\n**Interpretation of branch points between less-related classes (W3)**\n\nBranch points between less-related classes can certainly still be interpreted (and still yield valid interpolations/intermediates). However, between very unrelated classes, the interpretations are naturally less meaningful, as only very high-level (and relatively uninformative) features will be shared between these classes.\n\nTo demonstrate this, we updated our manuscript to show the hybrid resulting from interpreting the branch point between more dissimilar classes (Supplementary Figure S9). For example, on our tabular letters dataset, we quantitatively show that the branch-point hybrids between dissimilar classes still exhibit feature values which are interpolated between the two original classes, and often act as intermediates which exhibit properties of both. We also visually show the branch-point hybrids between dissimilar digits of our MNIST dataset. Our resulting hybrids show the common features between dissimilar digits, which are fairly high-level, including: 1) the digit is in the center of the image; and 2) there are areas which are relatively empty (coinciding with the \u201choles\u201d of how many people draw digits like 6s and 9s). This is a direct consequence of the lower similarity between two unrelated data classes rather than a limitation of branched diffusion models.\n\n**FID of RNA-seq dataset (Q1)**\n\nFor the RNA-seq dataset, below we reproduce the FID values for each class. Due to the high variation in FID values between the classes (likely because some cell types are inherently harder to generate than others), differences in these values were not obviously rendered in Supplementary Figure S3.\n\n|  | Branched | Label-guided |\n| --- | --- | --- |\n| CD16+ NK | 377.254 | 376.300 |\n| Cl. Mono. | 163.470 | 166.414 |\n| Late Eryth. | 150.605 | 153.134 |\n| Macroph. | 151.118 | 156.442 |\n| Megakar. | 64.968 | 65.506 |\n| Mem. B | 1604.477 | 1610.171 |\n| NK | 84.447 | 85.517 |\n| Plasmabl. | 317.073 | 317.584 |\n| Tem/Eff. H. T | 174.465 | 175.692 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096782375,
                "cdate": 1700096782375,
                "tmdate": 1700096782375,
                "mdate": 1700096782375,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7KvV4p2n50",
                "forum": "XMJBrvRDI8",
                "replyto": "YT2Iw2WSbL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_MogQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_MogQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Many thanks to your response and the additional experiments. I have also read the reviews and responses with other reviewers. My concerns are mostly addressed and I have raised my score accordingly."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723300118,
                "cdate": 1700723300118,
                "tmdate": 1700723300118,
                "mdate": 1700723300118,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4N5RohVHmF",
            "forum": "XMJBrvRDI8",
            "replyto": "XMJBrvRDI8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5985/Reviewer_HyFs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5985/Reviewer_HyFs"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new way to do class-conditional generation in diffusion generative models by modelling the formation of class structures as a hierarchical \u2018branching\u2019-type process: The generation starts from a noisy image that contains no class information, and as generation progresses, the partially formed image starts narrowing down to a smaller set of classes. Whenever some classes are ruled, out this is denoted as a \u2018branch point\u2019. In the method, these points are empirically estimated from the noising forward process and the original training data, and during training and generation, a separate conditioning signal is given to each of the branches. Each class has a unique combination of branches. It is shown that this formulation of class-conditional generation helps in avoiding catastrophic forgetting in some scenarios, lends itself to novel image-translation-type conditional generation for MNIST, gene data and molecules. Additionally, the paper shows a method to visualize and interpret the branching points using averages of the noisy data points. There can also be a benefit in generation efficiency if sampling multiple classes by combining the generation of multiple classes at once."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper presents a creative use of the diffusion forward process itself for controlling the generation in a way that is mostly unexplored at the moment. \n+ The observation that training new classes on only subsets of the diffusion forward process avoids catastrophic forgetting, is particularly striking and seems like a genuinely new effect. Possibly this paper could be a first step towards utilizing this property in more realistic continual learning scenarios.\n+ The paper also goes on to come up with different creative use-cases of the explicit branching structure, such as \u2018transmutation\u2019 where data points are transferred from one class to another using the branching structure, and in general finds multiple potentially relevant scientific data sets to experiment on. \n+ Perhaps the ideas here could inspire more research towards creating more structured diffusion generative models in the future."
                },
                "weaknesses": {
                    "value": "- Some of the experiments are not, at the moment, particularly convincing of the usefulness of the effects that they are showcasing. For the analogy-based generation with the RNA-seq data set, some marker genes were indeed changed, but do we have any other way to evalute the success of these generations? Could we formalize a clear objective on what does the conditional generation aim to do in the first place here? A similar issue exists for the molecule data set: Indeed regenerating does allow to generate cycled molecules from acyclic ones, but it is not clear what if any properties of the original molecule are retained this way. Just looking at the results, it seems possible that the generated molecules are a random mixture of the desired property and some atoms and bonds from the original molecules. \n- Continuing on the analogy-based generation, I feel that it would be appropriate to do an ablation where we use a regular diffusion model, noise out the data partially, and regenerate with the changed label. Would this work equally well, or differently somehow?\n- The points about interpretability are also interesting, but it remains a bit unclear what could be the use of these average branching points."
                },
                "questions": {
                    "value": "- What does the high correlation of expression between genes before and after transmutation mean here? That those particular genes often did not change? Is this the property that we want?\n- Since adding uncorrelated noise probably does not result in clean hierarchical class structures in all cases (maybe in more complex image data sets, as pointed out in the paper), do you think it would be possible to induce such structure, e.g., by diffusing in some specifically designed latent spaces or otherwise designing the diffusion process itself to encourage it?\n- I wonder if the example where catastrophic forgetting is avoided in MNIST is possible to extend to multiple steps, to a slightly more realistic continual learning scenario?\n\nOverall, I think the idea is interesting and the paper presents new qualitative effects that emerge from the new formulation, but it is not there yet for publication. In particular, a more thorough experimental validation for continual learning and analogy-based generation would be in place, so that the reader would have clear takeaways. For the analogy-based generation, some kind of formalization of what are we targeting with the conditional generation, would also help with showcasing the potential significance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Reviewer_HyFs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5985/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758450263,
            "cdate": 1698758450263,
            "tmdate": 1699636640483,
            "mdate": 1699636640483,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7jGLVDDMot",
                "forum": "XMJBrvRDI8",
                "replyto": "4N5RohVHmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review (1)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review and provide valuable feedback! We have answered all the questions and comments in the following.\n\n**Formalizing and quantifying the goals of transmutation (W1, Q1)**\n\nIn transmutation (i.e. analogy-based conditional generation), we start with an object $x_0$ of class $c_0$, and transmute it to another object $x_1$ of class $c_1$ (usually $c_0 \\neq c_1$). The goal of transmutation is two-fold: 1) **efficacy**: $x_0$ should have features which define it to be in class $c_0$, and $x_1$ should have features which define it to be in class $c_1$; and 2) **analogy**: features which do not distinguish/define an object\u2019s class to be in $c_0$ or $c_1$ should be retained. This makes analogy-based conditional generation substantially different from standard conditional generation, as we are interested in sampling from $q(c_1, x_0\\in c_0)$ instead of $q(c_1)$. In addition to several qualitative results, we systematically quantified both objectives in our experiments, demonstrating both efficacy and analogy in multiple real-world datasets.\n\nFor the RNA-seq example, we showed **efficacy** by quantifying the marker genes before and after transmutation. The class of a cell (i.e. cell type) is a complex concept, and marker genes are by far the most widely accepted method for determining cell type. As such, we showed key marker genes which demonstrate that a cell was successfully converted from one type to another. To demonstrate **analogy**, we quantified the correlation of non-marker genes, namely genes responsible for COVID-related inflammation (which do not define the cell type). These inflammation genes were explicitly identified by Lee et. al. (2020) (the publication which is the source of the RNA-seq data) as being key upregulated genes in COVID-infected cells (regardless of cell type). The correlation here is quantified over a random sample of cells (each point in the correlation is a single cell). A high correlation indicates that if the starting cell $x_0$ had a high expression of the gene, then the transmuted cell $x_1$ *also* had high expression of that gene (and *vice versa*). For these inflammation genes, this is precisely the desired property, as it indicates that the model is transmuting COVID-infected cells of one type into COVID-infected cells of another type, and healthy cells from one type into healthy cells from another type (i.e. the non-cell-type-defining features are retained).\n\nFor the molecular example, we quantified **efficacy** simply by counting the number of molecules which satisfied the goal structural property. We quantified **analogy** by computing the similarity of functional groups (which, in turn, endow the chemical properties of the molecule) before and after transmutation. Overall, in this experiment, the objective of transmutation is to generate molecules that satisfy the new class (e.g. \u201ccyclic\u201d or \u201chalogenated\u201d), while also resembling the initial molecules. Therefore, our results showing that the generated molecules satisfy the desired target property and are some \u201cmixture\u201d of similar chemical motifs from the original molecule, are certainly an indication that transmutation is working properly. For any starting molecule, it is desirable to see a distribution of similar atoms and functional groups in the generated molecules (i.e. analogy), but still being valid molecules which satisfy the target property (i.e. efficacy)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096531430,
                "cdate": 1700096531430,
                "tmdate": 1700096686715,
                "mdate": 1700096686715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OW3HlbVo3z",
                "forum": "XMJBrvRDI8",
                "replyto": "Svhny1EkGz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_HyFs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_HyFs"
                ],
                "content": {
                    "title": {
                        "value": "Response and additional comments to the authors"
                    },
                    "comment": {
                        "value": "I thank the authors for the new experiments and explanations! I understand some questions better now, but some remain open. Here's some comments / questions that came to mind while reading the response:\n\nTransmutation\n- How do we define which features distinguish/define an objects class to be c_0 or c_1? I still think that a mathematical definition would be better here, so that we can clearly evaluate the sensibility of the definition and the quality of the results. \n- E.g., q(c_1, x_0 \\in c_0): What does this formula mean? By q(c_1), do the authors mean the distribution of the label or the distribution of x_0 given the label?\n- Figure S8 does indeed look promising! But this is still just one rather small example, and a more thorough evaluation against this standard diffusion baseline (with numerical metrics) would make the case more convincing that training with the branching procedure does make the model behave clearly differently from a standard diffusion model. \n\nInterpretability\n- I think I understand slightly better now the interpretability point, and the model understanding part in particular seems useful for working with models like this. \n\nContinual learning\n- How can you make sure that the models generation of previous classes remains identical? Do you train a new neural network? Otherwise this claim seems impossible to me, since finetuning means that you do retrain the weights, but maybe I did not understand properly. \n- I don't understand how do branch models not not have the limitation of adding some new weights for new classes? Is it the case the case that no direct input is given to the model here?\n- For the fine-tuning experiments on continual learning, do you have the details on how long were the models fine-tuned, etc.? This seems like important information here. \n\nFID values\nAn important question I forgot to ask: How is the FID calculated for the cell data? Does FID make sense in this context, given that it uses the InceptionV3 network meant for image data? As a side note, even for MNIST, FID doesn't seem like the best metric, since the InceptionV3 network is trained on ImageNet, which differs quite greatly from the characteristics of MNIST (and as such the inner representations may not accurately reflect the structure of MNIST digits very well). But I'm not demanding any new experiments on the MNIST part. \n\nOverall, my view of the paper overall has not changed as of now. The new results and explanations do give me more confidence that the method is interesting to the community, but the paper in the current stage seems preliminary still, as any of the effects, such as continual learning, have not been explored very thoroughly. In my view, especially the continual learning part seems interesting, due to the clear improvement over the standard diffusion model, and expanding experiments on that to other wider types of data would make for a really interesting paper. I am also open to hearing out further responses from the authors."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700336633919,
                "cdate": 1700336633919,
                "tmdate": 1700336633919,
                "mdate": 1700336633919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yWkajXwmxm",
                "forum": "XMJBrvRDI8",
                "replyto": "4N5RohVHmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to additional comments (1)"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and for clarifying remaining concerns!\n\nFirstly, we would like to note that we do conduct fairly extensive experiments over many different data types and datasets, spanning from **images** to **graphs** to **tabular data**. Two of our datasets were also **large, real-world scientific datasets**, and altogether we showed **global, quantitative** results for each of the benefits brought by our proposed method of branched diffusion (continual learning, transmutation, interpretability, and efficiency). As such, we believe that these experiments (in addition to the additional experiments we have shown throughout the rebuttal period) are sufficient to demonstrate the advantages and disadvantages of our method relative to the current state of the art.\n\n**Transmutation**\n\nUnfortunately, typical probability notation is not very well-suited to capture the process of transmutation. To be more clear, we defined $q(c_{1}, x_{0}\\in c_{0})$ above to be the conditional distribution of objects for class $c_{1}$, but *also* conditioned on an object $x_{0}\\in c_{0}$. If we were to draw an object from $x_{1}\\sim q(c_{1}, x_{0}\\in c_{0})$, we would like both efficacy and analogy to hold (i.e. $x_{1}$ belongs to class $c_{1}$ and features that do not define class identity are shared between $x_{1}$ and $x_{0}$).\n\nAlthough we agree that it would be nice to have more formalization on the process of transmutation, it is unfortunately not possible to mathematically formalize both goals of efficacy and analogy in a straightforward way. In particular, it remains generally infeasible to clearly distinguish and define the class-defining features which should be modified in efficacious transmutation, and the instance-specific features which should be kept analogous. Although class-defining features and instance-specific features certainly exist, they are oftentimes *latent*, *high-level*, and *entangled* with each other. If the class-defining and instance-specific features were disentangled and clearly defined, then a general mathematical formalization would be more accessible, but unfortunately, the realization of these features are highly dependent on the specific application, including the dataset, the definition of classes, and the meaning of the underlying data. Unless we have a more simplistic dataset or we have adequate domain knowledge to perform this disentanglement and latent discovery, it remains infeasible to formally define transmutation at a feature level.\n\nFor this reason, we showed **globally quantitative** results on transmutation for *many datasets of different data types* (Figure 3). For example, we showed transmutation on a tabular dataset, where we could examine feature analogy by inspecting individual features. We also showed globally quantitative results demonstrating both efficacy and analogy on two real-world large scientific datasets: RNA-seq and drug-like molecules. For RNA-seq, to show efficacy and analogy we relied on knowledge of the domain: namely the presence of marker genes which define cell type and analogous COVID-related inflammation genes which were first discovered in Lee et. al. (2020). For the molecules, our quantification of efficacy and analogy relied on high-order structural properties and knowledge of functional groups.\n\nOverall, however, it remains relatively infeasible to clearly mathematically formalize the low- and high-order features which we expect to be retained for analogy but modified for efficacy, particularly when we are attempting to remain general to most scientific applications. Importantly, note that transmutation can be thought of as a generalization of neural style transfer (NST) (i.e. generating images where the subject of the image is given the style of another image). Note that in the seminal works which define NST [1, 2], *there is also no mathematical formalization of the goals of NST*, due to the inability to define ahead of time the features which we expect to be modified or retained.\n\nWe are also happy to hear that our novel ablation study shown in Supplementary Figure S8 was helpful. Importantly, we would like to clarify that *transmutation is a novel technique that we presented, which was **not** a part of traditional linear diffusion models*. Indeed, our main contribution is the discovery of branch points and applying them to a diffusion model, so that we can perform transmutation in the first place. As our results show, attempting to force a conventional linear model to perform transmutation is unnatural and difficult, and transmutation as a technique in linear diffusion models also did not exist prior to our paper. As such, beyond this ablation study showing why branch points are important for transmutation, we do not find it particularly meaningful to perform such an extensive evaluation (as we have done in our other experiments) against such an unnatural and difficult technique which understandably did not exist before."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510751606,
                "cdate": 1700510751606,
                "tmdate": 1700510935223,
                "mdate": 1700510935223,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bynwy6hTL2",
                "forum": "XMJBrvRDI8",
                "replyto": "2hnX37PGKq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_HyFs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_HyFs"
                ],
                "content": {
                    "title": {
                        "value": "Answer to authors"
                    },
                    "comment": {
                        "value": "Thank you for the further elaborations! Couple of last questions (I hope these do not come too late) that came to mind: \n\n\"Contrast this with a linear model, which does not cleanly separate the diffusion of different classes in any manner, and so new classes cannot be introduced efficiently without catastrophic forgetting.\"\n\nDid you try also only fine-tuning the new output heads in a linear diffusion model? I understand that the difference is that one would have to train it for all the diffusion steps instead of just for the last branch, but this would be a good baseline to have. It sounds that the ability to finetune a small amount of parameters for only a part of the diffusion process is the key to making the continual learning scenario to work here. I think that this is quite important information (and could be useful for characterising when could we except this method to be useful). \n\n\"we computed FID by applying the FID metric directly to the feature values\"\n\nSo does this mean that you calculate the 2-Wasserstein distance for the two sets of distances (with a Gaussian assumption)? I am mainly confused how does the calculation work in practice here, since FID itself is defined in a very specific way for image data and involves extracting features from a convolutional network, and I can't see how this could be applied to general vectors."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677700339,
                "cdate": 1700677700339,
                "tmdate": 1700677700339,
                "mdate": 1700677700339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dKx4q7PFEJ",
                "forum": "XMJBrvRDI8",
                "replyto": "4N5RohVHmF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to additional comments (part 2)"
                    },
                    "comment": {
                        "value": "Thank you for clarifying the last few questions!\n\n**Fine-tuning a linear model**\n\nLinear (i.e. traditional, label-guided) diffusion models are generally *single-task* models which are not trained with multiple output heads (Ho et. al. 2021). Instead, class identity is specified to the model as a label embedding which is fed in as an input. It is certainly true that one of the major reasons why branched diffusion models are much easier to extend to new data is the ability to fine-tune a small number of parameters for only a small part of diffusion time, *while guaranteeing that no previously learned tasks/classes are affected*. Indeed, one of the contributions of branched diffusion models\u2014in addition to recognizing that there are natural branch points along the diffusion process\u2014is to turn the neural network into a multi-task network which *optimally* separates diffusion time into distinct branches and tasks to accomplish this separation of parameters.\n\nIn contrast, training a traditional diffusion model to be multi-task (with one output task per data class) is generally not a method which is used today. A lot of the diffusion process between classes (particularly at later times) would be inefficiently learned by multiple tasks, and so a single-task model with label embeddings is the ubiquitously relied-upon approach for class-conditional diffusion today. A branched model solves this problem of inefficient learning by leveraging a hierarchy of class similarities to define branches of shared diffusion. That said, we agree that this method of multi-task diffusion in an otherwise linear model would be far more amenable to continual learning than the ubiquitous single-task linear diffusion models, although it would still be less suitable than branched diffusion models due to the need to train on the entire diffusion timeline for each class (as you suggested).\n\nAdditionally, training a multi-task linear diffusion model would not allow for the many other benefits of branched diffusion models, such as transmutation, interpretability of branch points, or efficient generation.\n\n**FID on feature values**\n\nThat's right! 2-Wasserstein distance is perhaps a clearer term to use here for similarity comparisons on general vectors. We will update the language in our manuscript accordingly."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682797133,
                "cdate": 1700682797133,
                "tmdate": 1700688585362,
                "mdate": 1700688585362,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FuS2PXFuMv",
            "forum": "XMJBrvRDI8",
            "replyto": "XMJBrvRDI8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5985/Reviewer_DhnV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5985/Reviewer_DhnV"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for class-conditional (label guided) sampling from a diffusion model by introducing branching. Analysis on several datasets suggests that the approach can be competitive (or perhaps even superior) in terms of generated sample quality. The proposed approach has several advantages compared to \u201cclassifier-free\u201d guidance. It can readily incorporate new classes and can be used for transmutation (transferring a specific instance from one class to another). The method is considerably more efficient if the aim is multi-class sampling."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "S1. The paper proposes a highly novel and elegant approach to class-conditional (label guided) sampling from a diffusion model. \nS2. Experiments indicate that the proposed method offers competitive (or better) performance to the state-of-the-art \u201cclassifier-free\u201d guidance approach in terms of Frechet inception distance. \nS3. The paper details how the presented method can efficiently incorporate new classes without retraining the entire model and illustrates how the method can be employed for transmutation. Multi-class sampling is considerably more efficient compared to the state-of-the-art approach.  \nS4. The paper is well-written and presents the proposed method clearly."
                },
                "weaknesses": {
                    "value": "W1. Some of the experiments are not particularly compelling and serve more as examples of the potential of the technique rather than providing convincing evidence in support of the claims of the paper. In the image domain, analysis is limited to MNIST and the letter-recognition dataset; this leaves open the question as to how well the approach scales to other (less-structured) types of images and more challenging image classes, where the class hierarchy may not be so clear. \n\nW2. The branching point definition involves a threshold. There does not appear to be a concrete recipe for the selection of this threshold. Based on the paper it seems to be left to the practitioner to determine when branching points are \u201ctoo close\u201d to 0 or T. There is no investigation of how the selection of this threshold impacts performance. \n\nW3. Some of the claims in the paper are not adequately supported by experimental evidence. The claims should be moderated or experimental results provided to support the more general conclusions. \n\nThe paper proposes a novel, intriguing and elegant approach. The major weakness of the paper is that most of the claims in the paper are supported by relatively limited experimentation. For example, the paper claims that the method achieves similar or better generative performance as the state-of-the-art, but does not clearly preface this with the clarification that the outperformance is observed only for two simple character-based image datasets and the similar performance is only established for one other dataset. The paper would be considerably more convincing if there were experiments on more challenging image datasets."
                },
                "questions": {
                    "value": "Q1. \u201cIn general, the branched diffusion models achieved similar or better generative performance compared to the current state-of-the-art label-guided strategy. In many cases, the branched models outperformed the label-guided models, likely due to the multi-tasking architecture which can help limit inappropriate crosstalk between distinct classes.\u201d \u2013 these sentences seem to be strong claims when the experiments are conducted on three datasets (two of which are similar). There seems to be no discernible outperformance for the single-cell RNA-seq data. The outperformance is really only for two character-based image datasets, so \u201cIn many cases\u201d seems to be a stretch.  Considerably more extensive experiments on a variety of datasets are required to support the general claim made in the paper. Alternatively it could be restricted to \u201cFor experiments performed on two character-based datasets and an RNA-seq dataset, \u2026.\u201d \nCan the authors clarify whether they consider the current experiments to be sufficient to demonstrate similar or better generative performance? \n\nQ2. There are concerns that the Frechet inception distance can provide an incomplete or even misleading picture of generative quality (e.g., \u201cThe Role of Imagenet Classes in Frechet Inception Distance\u201d, ICLR 2023; \u201cAssessing Generative Models via Precision and Recall\u201d, NeurIPS 2018). Do the authors consider that there would be value in employing other approaches for investigating sample quality?   \n\nQ3. Why is the new class experiment limited to training on 3 classes? Is this to make the task easier? The new class experiment for the single-cell RNA-seq dataset seems to be similarly limited (just starting with two classes and adding a third). Is there a reason that the more obvious experiment of removing just one class and adding it back is avoided?  What happens if the \u201c1\u201d class is already included (i.e. something that is much closer to the introduced task)? \n\nQ4. \u201cOf course, images and image-like data are the only modalities that suffer from this issue.\u201d \u2013 why are images and image-like data the only modalities? Is there a \u201cnot\u201d missing? Otherwise this seems to be an odd claim. The class-defining subject of a sequence could be at multiple parts of the image. The class of a graph can be defined by two subgraphs that are far from one another. \n\nQ5. \u201cAdditionally, this limitation on images may be avoided by diffusing in latent space.\u201d \u2013 is there evidence for this claim?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Reviewer_DhnV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5985/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802323474,
            "cdate": 1698802323474,
            "tmdate": 1700672897114,
            "mdate": 1700672897114,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5UG1Bxrv5j",
                "forum": "XMJBrvRDI8",
                "replyto": "FuS2PXFuMv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review (1)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review and provide valuable feedback! We have answered all the questions and comments in the following.\n\n**Comparison of generative performance and use of FID (W3, Q1, Q2)**\n\nIn our manuscript, we quantified the generative performance of branched vs linear (i.e. traditional) diffusion models over three datasets. For MNIST, we saw better performance in the branched model for 10/10 classes. For letters, our branched model outperformed in 24/26 classes. For RNA-seq, our branched model outperformed in 8/9 classes. Given these results, we consider it reasonable to claim \u201cin general, the branched diffusion models achieved similar or better generative performance compared to the current state-of-the-art label-guided strategy\u201d, and that \u201cin many cases, the branched models outperformed the label-guided models\u201d (quoted from Section 3.1). Note that our claims here are certainly within the context of our experiments, and we are not attempting to claim that branched diffusion models always offer improved generative performance over their linear counterparts in all situations. For improved clarity, we will modify these sentences as follows: \u201c*In our experiments*, the branched diffusion models generally achieved similar or better generative performance\u2026\u201d\n\nSpecifically for the RNA-seq dataset, below we reproduce the FID values for each class. Due to the high variation in FID values between the classes (likely because some cell types are inherently harder to generate than others), differences in these values were not obviously rendered in Supplementary Figure S3.\n\n|  | Branched | Label-guided |\n| --- | --- | --- |\n| CD16+ NK | 377.254 | 376.300 |\n| Cl. Mono. | 163.470 | 166.414 |\n| Late Eryth. | 150.605 | 153.134 |\n| Macroph. | 151.118 | 156.442 |\n| Megakar. | 64.968 | 65.506 |\n| Mem. B | 1604.477 | 1610.171 |\n| NK | 84.447 | 85.517 |\n| Plasmabl. | 317.073 | 317.584 |\n| Tem/Eff. H. T | 174.465 | 175.692 |\n\nFinally, we agree that every metric for performance (including FID) has its advantages and disadvantages. In our analyses, we believe that FID is sufficient to capture the global trends in generative performance that we wish to understand. In addition to the analysis of FID, we also verified the generative performance of our branched diffusion models using other methods (Supplementary Figures S1\u2014S2).\n\nImportantly, however, we emphasize that the main contributions of our work lie *beyond* generative performance, and we simply wish to ensure that branched diffusion models are not *significantly worse* in performance compared to their linear counterparts. We believe that our analyses in this regard are sufficient to demonstrate this claim.\n\n**Application to scientific domains and datasets outside of images (W1)**\n\nThe main contribution of our work is intended for scientific applications, whose datasets are typically characterized by a large amount of internal structure (e.g. cell types, chemical classes, structural folds, etc.). As such, although we demonstrated our method\u2019s soundness on well-known image benchmarks such as MNIST, we also focused much of our work on real-world large scientific datasets such as single-cell RNA-seq and drug-like molecules, where we showed the promise of branched diffusion to perform real scientific discovery by recovering known biology.\n\nNotably, our proposed method is very much dependent on datasets with internal structure. Datasets which have do not have inherent structure (or only very weak structure) between classes are not suitable for branched diffusion, whose core hierarchical backbone reflects the intrinsic similarities between classes in the dataset. We will modify the language in our manuscript to better clarify this intended use case of our method."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096245237,
                "cdate": 1700096245237,
                "tmdate": 1700096245237,
                "mdate": 1700096245237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J0F15CvmTa",
                "forum": "XMJBrvRDI8",
                "replyto": "9K3elh7s1Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_DhnV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_DhnV"
                ],
                "content": {
                    "title": {
                        "value": "Reply to response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response to my comments. The majority of my questions and identified weaknesses have been addressed by the response. I am currently reflecting on the revised paper, the reviews, and responses, and will potentially raise my recommendation. \n\nW2: Thank you for providing the additional results demonstrating the robustness of the approach for selecting the threshold. The results support the argument that the choice of threshold doesn\u2019t overly matter, provided it does not lead to terminal branches of length 0. The initial concern I expressed in my review was that \u201csimply choosing a value where the branch points are not all too close to t = 0 or t = T\u201d does not make clear how \u201ctoo close\u201d is defined. For reproducibility, it would be helpful to provide a concrete process for the selection of \\epsilon, or at least describe how it was done to produce the results in the paper. \n\nQ3: I am somewhat surprised that the decision to use smaller models to explore continual learning was based on considerations about the time required for training. The image datasets analyzed in the paper are already small. If there is a need to reduce them even further to enable training, then it suggests that the training burden for the approach is very high. The multi-step experiments produce some impressive results and are a good addition to the paper. Was there another reason not to provide results for a larger initial model? The claim \u201cwe expect no difference in difficulty if we were to start with a larger existing model or a smaller one\u201d doesn\u2019t seem to be supported by evidence, and it seems like a simple enough experiment. Does training a 10-class model take a very long amount of time?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618868563,
                "cdate": 1700618868563,
                "tmdate": 1700618868563,
                "mdate": 1700618868563,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9K564qmLht",
                "forum": "XMJBrvRDI8",
                "replyto": "FuS2PXFuMv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to additional comments"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time taken to take our new results and response into consideration and reflection! Here, we will address these remaining questions.\n\n**W2**\n\nOur selection of $\\epsilon$ was done by first examining the similarity between pairs of classes throughout the diffusion timeline from $t = 0$ to $t = T$. Given our rough goal of having branch points not too close to $0$ or $T$, we considered the similarity between pairs of classes at $t = \\frac{T}{2}$. We then simply selected our value of $\\epsilon$ to be the average similarity over pairs of identical classes (i.e. the average similarity between random partitions of data from the same class), minus the average similarity over pairs of different classes, rounded to one significant digit for simplicity. This difference measures the expected gap in similarity between identical versus distinct classes at the midpoint of forward diffusion. We found that this procedure was sufficient to ensure that the branch points were not all too close to $t = 0$ or $t = T$.\n\nOf course, our results in Supplementary Figure S6 also show that the branch points (and downstream performance) are fairly robust to the selection of $\\epsilon$ anyways. We agree that this procedural detail could be very helpful in justifying our selection of $\\epsilon$ (even if performance is not particularly sensitive to this hyperparameter), and we will update our manuscript accordingly.\n\n**Q3**\n\nThe decision to start with a model of fewer classes was motivated by two reasons: 1) sheer convenience of faster training time and data loading (particularly during development and testing); and 2) we needed to hold out never-before-seen classes to introduce for our continual-learning experiments (i.e. if we had trained a model on all available classes, then we would not have new classes to add for our experiments).\n\nOur expectation that there is no increase in difficulty when starting with a larger model versus a smaller model is firmly rooted in the following observations: 1) when we add a new data class to a branched diffusion model, the weights in the existing architecture are frozen so that *catastrophic forgetting is impossible*; and 2) learning to generate a new class effectively reduces to fine-tuning a standard diffusion model, where the early layers of the neural network are pre-trained, and the late diffusion times are transferred from similar data. Note that this process of *freezing* early layers and only training on early diffusion times (for a single class) is also what allows continual learning in a branched model to be *much faster* than in a linear model.\n\nIn order to support our expectation that there is no increase in difficulty for larger models, we note that our experiments in Supplementary Figure S7 showed that the continual-learning benefit of branched diffusion models remained equally effective and performative as more classes were introduced and the size of the model increased. Furthermore, we also showed continual-learning results on branched diffusion models trained on our much larger dataset of single-cell RNA-seq (Figure 2).\n\nFinally, to directly answer the final question, training a 10-class model does not take a prohibitively large amount of time, and this was never a limitation for us. For example, to generate Supplementary Figures S5\u2014S6, we trained many branched diffusion models (on the full 10-class dataset) for 150 epochs, which was the *same as the linear model.*"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672754263,
                "cdate": 1700672754263,
                "tmdate": 1700672863511,
                "mdate": 1700672863511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nnYDCBtSnH",
                "forum": "XMJBrvRDI8",
                "replyto": "9K564qmLht",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_DhnV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_DhnV"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up response"
                    },
                    "comment": {
                        "value": "Thank you for the additional clarification. Since all of my concerns have been addressed, and I think the paper makes a highly novel and promising contribution, I have raised my overall score to 8."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673026029,
                "cdate": 1700673026029,
                "tmdate": 1700673026029,
                "mdate": 1700673026029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZqPxMpHzmh",
            "forum": "XMJBrvRDI8",
            "replyto": "XMJBrvRDI8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5985/Reviewer_EPhc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5985/Reviewer_EPhc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes hierarchically branched diffusion models for class-conditional generation. In a hierarchically branched diffusion model, branched points between all classes is generated based on the similarity between each class pair. The proposed model can be easily extended to continual learning scenarios. The model facilitates analogy-based conditional generation and provides a interpretability into the class-conditional generation process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to understand.\n\n2. The inclusion of well-crafted visualizations greatly enhances the comprehension of key concepts.\n\n3. The proposed method offers meaningful advantages."
                },
                "weaknesses": {
                    "value": "I did not find notable weaknesses of this paper."
                },
                "questions": {
                    "value": "A concern arises regarding the scalability of the proposed method as the number of classes increases. The experiments conducted appear to be limited to datasets with a small number of classes. It would be beneficial if the authors could present results for datasets with a larger number of classes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5985/Reviewer_EPhc"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5985/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834827838,
            "cdate": 1698834827838,
            "tmdate": 1699636640276,
            "mdate": 1699636640276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VLlLX3JHGA",
                "forum": "XMJBrvRDI8",
                "replyto": "ZqPxMpHzmh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review and provide valuable feedback!\n\nTo answer your question about datasets with more classes, we note that one of the datasets we presented had 26 classes, which we consider fairly typical in magnitude (or even larger in magnitude) compared to most scientific applications.\n\nFor example, training a model with cell types for classes would require only 10 - 15 classes for most tissue systems. For drug-discovery applications where the classes are target receptors, most families of receptors only have 5 - 10 members under investigation at any given point."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700096107215,
                "cdate": 1700096107215,
                "tmdate": 1700096107215,
                "mdate": 1700096107215,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "npRIgmixGa",
                "forum": "XMJBrvRDI8",
                "replyto": "VLlLX3JHGA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_EPhc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5985/Reviewer_EPhc"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for answering my questions. However, the large number of classes I mentioned wasn't around 28 but a greater number (e.g., 100). I would like to maintain my positive score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5985/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727772923,
                "cdate": 1700727772923,
                "tmdate": 1700727772923,
                "mdate": 1700727772923,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]