[
    {
        "title": "Node Duplication Improves Cold-start Link Prediction"
    },
    {
        "review": {
            "id": "3vGL3m7rip",
            "forum": "yONJt6nFc3",
            "replyto": "yONJt6nFc3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1580/Reviewer_1iUU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1580/Reviewer_1iUU"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes an augmentation technique to improve the performance of GNNs on low-degree nodes and observes that cold node LP performance typically suffers because they have too few connections resulting in underrepresentation in LP training. This article proposes the NODEDUP method, which utilizes a multi-view approach to improve the fitting of cold node nodes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This method improves the performance of cold nodes without affecting warm nodes, which is a good improvement for the overall results.\n2.  A multi-view method is proposed to enhance the fitting of cold nodes during model training. Through a large number of experiments, it is proved that this method can improve the performance of three kinds of nodes.\n3. NODEDUP can be easily combined with the GNNs model, which is very inspiring for the subsequent work."
                },
                "weaknesses": {
                    "value": "1. There are some spelling and grammar problems in the article, please pay attention to check.\n2. In the analysis of the same type of augmentation methods, the reasons for the performance improvement can be analyzed in depth. \n3. More case studies for low-degree nodes are needed. Why such augmentation is really meaningful but fitting."
                },
                "questions": {
                    "value": "1. Why such augmentation is really meaningful but fitting in this task?\n2. GNNs have a hard time achieving good results on low-degree nodes. This may not always be true. Justify it more clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1580/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1580/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1580/Reviewer_1iUU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723805582,
            "cdate": 1698723805582,
            "tmdate": 1699636086791,
            "mdate": 1699636086791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gqTEiauJjK",
                "forum": "yONJt6nFc3",
                "replyto": "3vGL3m7rip",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1iUU"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1iUU,\n\nThank you for your valuable feedback. Our detailed response is as follows:\n\n>W1: There are some spelling and grammar problems in the article, please pay attention to check.\n\nThank you for pointing this out. We have carefully checked the spelling and grammar in the updated submission.\n\n>W2: In the analysis of the same type of augmentation methods, the reasons for the performance improvement can be analyzed in depth.\n\nAs we demonstrated in Section 3.2, there are two perspectives contributing to the effectiveness of NodeDup. \n1. **From the aggregation perspective,** when the transformation weights for an anchor node and its neighbors are different, messages passed from neighbors will play different roles than those from the anchor node itself. With NodeDup, our model can leverage extra information from an additional \"view.\" The existing view is when a node is regarded as the anchor node during message passing, whereas the additional view is when that node is regarded as one of its neighbors thanks to the duplicated node from NodeDup.\n2. **From the supervision perspective,** NodeDup adds edges that connect low-degree nodes to their duplicates. These additional edges serve as one of the few positive supervision signals for link prediction. More supervision signals for cold nodes lead to better quality embeddings.\n\nFrom the results shown in Figure 2, we can observe that both these two perspectives play important roles in the performance improvements of NodeDup. The experimental results shown in Figure 5 further demonstrate the effectiveness of these two perspectives towards cold nodes because our methods outperform other augmentation methods in this setting. \n\n>W3: More case studies for low-degree nodes are needed. Why such augmentation is really meaningful but fitting.\n\nBased on the results shown in Table 1, we believe our proposed methods are really meaningful but not overfitting to cold node settings because the improvements happen not only on cold nodes but also on warm nodes across all the datasets. \n\n>Q1: GNNs have a hard time achieving good results on low-degree nodes. This may not always be true. Justify it more clearly.\n\nWe acknowledge that this might not always hold true, but current research broadly supports the claim from two aspects:\n1. From the **methodology**, most GNNs follow a message-passing scheme, which updates each node's representations by iteratively aggregating its neighbors' information. Therefore, the performance of GNNs on each node will highly rely on whether it has enough high-quality neighbors.\n2. From the **experimental results**, GNNs tend to perform less effectively on cold nodes than on warm nodes, as illustrated in Figures 1 and 6. This trend is also supported by findings in previous studies [1,2,3].\n\nTo clarify any confusion, we have refined our wording in the revised submission. \n\n\n**Thanks again for your comments and diligence in reviewing our work. We hope our responses have addressed your concerns. If so, we hope that you will consider raising your score. If there are any notable points unaddressed, please let us know and we will be happy to respond.**\n\n[1]Towards locality-aware meta-learning of tail node embeddings on networks. CIKM 2020.   \n[2]Investigating and Mitigating Degree-Related Biases in Graph Convolutional Networks. CIKM 2020.  \n[3]Cold Brew: Distilling graph node representations with incomplete or missing neighborhoods. ICLR 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647601384,
                "cdate": 1700647601384,
                "tmdate": 1700647601384,
                "mdate": 1700647601384,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bBRtqWZBto",
                "forum": "yONJt6nFc3",
                "replyto": "3vGL3m7rip",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Stage Closing Soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer 1iUU,\n\nAs today is the **final day** of our discussion, we anticipate the opportunity to engage with you. Please let us know if our response has successfully addressed all of your concerns. If so, we would be deeply grateful if you consider raising your score. If not, we will happily answer any additional questions you may have. Thank you!\n\nBest regards,  \nNodeDup authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689400626,
                "cdate": 1700689400626,
                "tmdate": 1700689400626,
                "mdate": 1700689400626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mc8bQQBWzm",
                "forum": "yONJt6nFc3",
                "replyto": "bBRtqWZBto",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_1iUU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_1iUU"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response and pardon my late reply. I appreciate the time and efforts you put on rebuttal. Most of my concerns have been addressed in the response."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713660999,
                "cdate": 1700713660999,
                "tmdate": 1700713660999,
                "mdate": 1700713660999,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sbbIyzSBbp",
            "forum": "yONJt6nFc3",
            "replyto": "yONJt6nFc3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose to improve cold-start link prediction performance by duplicating tail nodes in the graph. Besides, they present NodeDup (L) that reduces duplicated nodes by self-loops to speed up the learning process. The experiments on several benchmark datasets demonstrate that the proposed method can serve as a plug-and-play approach to improve conventional GNN methods in link prediction. The authors also conduct several experiments to show that the comparisons in effectiveness and efficiency with other cold-start and augmentation methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* S1: The proposed method is simple and easy to be applied to arbitrary methods.\n\n* S2: Compared to conventional cold-start methods, the proposed method achieves better performance in link prediction.\n\n* S3: The authors conducted several ablation studies to demonstrate the effectiveness and efficiency of the proposed method in different settings and scenarios."
                },
                "weaknesses": {
                    "value": "* W1: The whole idea of the paper is about self-augmentation while the addition of self-loops (i.e., NodeDup (L)) would be a nature form without increasing model parameters. However, augmentation and adding self-loops are not novel in either link prediction [a, b, c, d].\n\n* W2: The experimental settings are problematic. The authors randomly sampled nodes as negative references, but the setup is unrealistic while those references would almost be easy negatives. All of the remaining nodes should be considered as ranking candidates. \n\n* W3: Many performance metrics are inconsistent or contradict with existing studies.\n\n* W4: Large-scale datasets are not included in the experiments. \n\n* W5: Some minor writing flaws exist, such as inconsistent terms without citing each other like inductive and production settings.\n\n---\n\n[a] Cai, L., Yan, B., Mai, G., Janowicz, K., & Zhu, R. (2019, September). TransGCN: Coupling transformation assumptions with graph convolutional networks for link prediction. In Proceedings of the 10th international conference on knowledge capture (pp. 131-138).\n\n[b] Wang, Z., Lei, Y., & Li, W. (2020). Neighborhood attention networks with adversarial learning for link prediction. IEEE Transactions on Neural Networks and Learning Systems, 32(8), 3653-3663.\n\n[c] Yan, Z., Ma, T., Gao, L., Tang, Z., & Chen, C. (2021, July). Link prediction with persistent homology: An interactive view. In International conference on machine learning (pp. 11659-11669). PMLR.\n\n[d] Chien, E., Chang, W. C., Hsieh, C. J., Yu, H. F., Zhang, J., Milenkovic, O., & Dhillon, I. S. Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction, ICLR 2022."
                },
                "questions": {
                    "value": "* Q1: The improvements reported in the paper are astonishing. I wonder if the authors conduct any significance test on the improvements and the corresponding confidence levels, especially for the isolated and low-degree cases.\n\n* Q2: Following Q1, it is surprising that all categories of nodes get benefited a lot after duplicating (or adding self-loops for) cold nodes. \n\n* Q3: Compared to Hits@10, Hits@1 could be more critical in the real-world applications, especially for tail nodes with very few neighbors. I wonder if the authors can also provide the Hits@1 performance.\n\n* Q4: Following W2, the authors should consider conducting a set of experiments using all remaining nodes as candidates for link prediction, thereby alleviating the bias on easy negatives and achieving more fair comparisons.\n\n* Q5: Following W3, I wonder why so many reported metrics are contradict with existing studies. For instance, Cold-brew even underperforms the original GSage across all metrics on Cora/Citeseer and most of metrics on other datasets in Table 1. As the authors do not use the identical setup and data partitions to previous studies, it should be better to have some elaboration on this part.\n\n* Q6: Following W4, in Appendix C, the authors claimed some reasons to not conduct experiments on some large-scale graphs. However, given the improvements on all node categories, I believe it is still worth to verify the performance on the large-scale datasets. Besides, `IGB` is not really *large-scale* while some datasets like `ogbn-products` and `ogbn-papers100M` have millions or handred millions of nodes."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1580/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1580/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829895837,
            "cdate": 1698829895837,
            "tmdate": 1700723964016,
            "mdate": 1700723964016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oMw9h3KZHs",
                "forum": "yONJt6nFc3",
                "replyto": "sbbIyzSBbp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pJyQ [1/3]"
                    },
                    "comment": {
                        "value": "Dear Reviewer pJyQ,\n\nThank you for your valuable feedback. Our detailed response to your concerns is as follows:\n\n>W1: The whole idea of the paper is about self-augmentation while the addition of self-loops (i.e., NodeDup (L)) would be a nature form without increasing model parameters. However, augmentation and adding self-loops are not novel in either link prediction. It is surprising that all categories of nodes get benefited a lot after duplicating (or adding self-loops for) cold nodes.\n\nThank you for directing our attention to these works. Although augmentation and the incorporation of self-loops are not new concepts in link prediction, our contributions to this field remain substantial:\n- **In terms of augmentation methods,** our methods, both NodeDup and NodeDup(L), stand out for their simplicity and efficiency, specifically tailored to enhance cold-start nodes' performance while existing augmentation methods [1,2,3,4,5] all focus on overall link prediction performance. As demonstrated in Figure 5, NodeDup and NodeDup(L) not only significantly boost performance for both Isolated and Low-degree nodes but also offer enhanced run-time efficiency compared to [1,2,3]. Unlike [4], which necessitates extracting topological features for each node pair, and [5], which requires pre-training of the text encoder to improve node features, our methods are more streamlined and less complex. \n- **Regarding the addition of self-loops,** our use in NodeDup(L) diverges from previous implementations [6,7] where self-loops were solely for aggregation. In our framework, these self-loops also provide critical supervision training signals for cold nodes, marking a notable differentiation.\n- **Beyond methodological distinctions,** our work is the first to connect the simple node duplication trick with the cold-start issue, and we have experimentally demonstrated the effectiveness of this approach. In addition, we also emphasize the importance of understanding the underlying mechanics of our approach. In Section 3.2, we conduct a comprehensive analysis to identify the core reasons driving the effectiveness of our methods. Furthermore, in Section 3.3, we investigate the interplay between NodeDup and self-distillation, offering deeper and more interesting insights to the academic community.\n\n\n>W2: The experimental settings are problematic. The authors randomly sampled nodes as negative references, but the setup is unrealistic, while those references would almost be easy negatives. All of the remaining nodes should be considered as ranking candidates. The authors should consider conducting a set of experiments using all remaining nodes as candidates for link prediction, thereby alleviating the bias on easy negatives and achieving more fair comparisons.\n\nThank you for highlighting this aspect. We agree that, ideally, utilizing all remaining nodes as candidates for link prediction would provide a comprehensive evaluation. However, this approach poses significant computational challenges, especially in the context of large-scale datasets. To address this, we follow the official task setting for OGB-Citation2 [1] to randomly sample negatives for each node. This methodology was suggested by Koren et al. [2], which advocates for the \"sampling top-k\" method in recommendation system evaluations. This approach has been widely adopted in numerous related studies [3,4,5,6] due to its balance between thoroughness and computational efficiency."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647947264,
                "cdate": 1700647947264,
                "tmdate": 1700647947264,
                "mdate": 1700647947264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MgF20SBpCu",
                "forum": "yONJt6nFc3",
                "replyto": "sbbIyzSBbp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pJyQ [2/3]"
                    },
                    "comment": {
                        "value": ">W3: Many performance metrics are inconsistent or contradict with existing studies. I wonder why so many reported metrics are contradict with existing studies. For instance, Cold-brew even underperforms the original GSage across all metrics on Cora/Citeseer and most of metrics on other datasets in Table 1. As the authors do not use the identical setup and data partitions to previous studies, it should be better to have some elaboration on this part.\n\nSorry for the confusion. To clarify:\n1. For the **evaluation metrics**, link prediction tasks are also considered recommendation tasks. In this work, we employ Hits and MRR (Mean Reciprocal Rank) as our evaluation metrics, which is consistent with prior literature [14,15].\n2. For the **experimental settings**, we recognize that experimental setups vary across cold-start methodologies [16,17]. In pursuit of a fair comparison and to match real-world scenarios, we have adopted a random split approach for our datasets, splitting edges into training, validation, and testing sets. The degree of nodes is then defined based on this random split, which we believe could provide a realistic evaluation framework.\n3. It's important to note that the setup used in Cold Brew [17], which artificially creates isolated nodes by removing edges from the lowest 10% of nodes in the degree distribution, differs significantly from our experimental setting. We have meticulously fine-tuned Cold Brew for our experimental settings, however, it did not perform as effectively as in its original setting. [18] reports a similar observation in the context of node classification tasks while comparing with [17].\n\n\n>W4: Large-scale datasets are not included in the experiments. In Appendix C, the authors claimed some reasons to not conduct experiments on some large-scale graphs. However, given the improvements on all node categories, I believe it is still worth to verify the performance on the large-scale datasets. \n\nThank you for the suggestion and acknowledgment of the simplicity of our methods. As outlined in Section 3.1, our methods incur a minimal increase in time complexity compared to base GNNs, with the increase being linearly proportional to the number of cold nodes. This ensures scalability. Besides, the effectiveness of our method is also insensitive to dataset size. Given the time constraints, we only extended our experiments to the IGB-1M dataset, featuring 1 million nodes and 12 million edges. The findings, which we detail below, affirm the effectiveness of our methods in handling large-scale datasets, consistent with observations from smaller datasets. Thank you again for your valuable suggestion to enhance our experiments. We have incorporated this result into the revised submission.\n\n|        |            | GSage        | GSage+NodeDup |\n|--------|:------------:|:--------------:|:---------------:|\n| **IGB-1M** | Isolated   | 82.10 \u00b1 0.06 | 87.81 \u00b1 0.40  |\n|        | Low-degree | 84.73 \u00b1 0.06 | 90.84 \u00b1 0.03  |\n|        | Warm       | 89.98 \u00b1 0.02 | 91.31 \u00b1 0.02  |\n|        | Overall    | 89.80 \u00b1 0.02 | 91.29 \u00b1 0.02  |\n\n\n>W5: Some minor writing flaws exist, such as inconsistent terms without citing each other like inductive and production settings.\n\nThank you for pointing it out. We have changed all \"production settings\" to \"inductive settings\" to keep it consistent in the updated submission. We have also carefully checked the writing in the updated one."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648092787,
                "cdate": 1700648092787,
                "tmdate": 1700648092787,
                "mdate": 1700648092787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U6kwtsWG82",
                "forum": "yONJt6nFc3",
                "replyto": "sbbIyzSBbp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pJyQ [3/3]"
                    },
                    "comment": {
                        "value": ">Q1: The improvements reported in the paper are astonishing. I wonder if the authors conduct any significance test on the improvements and the corresponding confidence levels, especially for the isolated and low-degree cases.\n\nWe have conducted the significance evaluation for our results. The analysis yielded p-values less than 0.01 for all node groups, including isolated/low-degree/warm nodes, which indicates a high level of statistical significance. This low p-value suggests that the likelihood of observing such results by chance is very small, thereby reinforcing the validity of our findings. Additionally, our code is available in an [anonymous GitHub repository](https://anonymous.4open.science/r/NodeDup-0241/README.md), along with hyperparameters for reproducing all the reported results. This ensures the reproducibility of our work.\n\n\n>Q2: Compared to Hits@10, Hits@1 could be more critical in the real-world applications, especially for tail nodes with very few neighbors. I wonder if the authors can also provide the Hits@1 performance.\n\nThank you for your valuable suggestions. We have extended our experiments to include analyses on both Cora and Citeseer datasets evaluated by Hits@1, and the results are presented below. These results consistently reflect the relative performance trends observed across different methods, as shown in Table 1. Besides, we believe that evaluating performance using Hits@10 is a realistic and relevant metric, even for tail nodes, because we usually will not only recommend one item/user to each user in real-world applications [19]. Hits@10 is also a commonly-used evaluation metric for link predictions [14,15].\n\n|          |            | |      Hits@1           | |      Hits@10           |\n|----------|:------------:|:------------:|:-------------:|:------------:|:-------------:|\n|          |            |     GSage    | GSage+NodeDup |     GSage    | GSage+NodeDup |\n| **Cora**     | Isolated   | 8.41 \u00b1 1.17  | 13.66 \u00b1 1.81  | 32.20 \u00b1 3.58 | 44.27 \u00b1 3.82  |\n|          | Warm       | 25.87 \u00b1 0.63 | 28.51 \u00b1 0.80  | 59.45 \u00b1 1.09 | 61.98 \u00b1 1.14  |\n|          | Low-degree | 22.28 \u00b1 0.87 | 23.21 \u00b1 0.61  | 61.14 \u00b1 0.78 | 59.07 \u00b1 0.68  |\n|          | Overall    | 22.44 \u00b1 0.63 | 24.29 \u00b1 0.47  | 58.31 \u00b1 0.68 | 58.92 \u00b1 0.82  |\n| **Citeseer** | Isolated   | 20.99 \u00b1 0.67 | 29.77 \u00b1 0.81  | 47.13 \u00b1 2.43 | 57.54 \u00b1 1.04  |\n|          | Low-degree | 34.25 \u00b1 0.63 | 42.34 \u00b1 0.70  | 61.88 \u00b1 0.79 | 75.50 \u00b1 0.39  |\n|          | Warm       | 34.36 \u00b1 1.20 | 35.90 \u00b1 1.13  | 71.45 \u00b1 0.52 | 74.68 \u00b1 0.67  |\n|          | Overall    | 31.81 \u00b1 0.66 | 36.84 \u00b1 0.42  | 63.77 \u00b1 0.83 | 71.73 \u00b1 0.47  |\n\n**We hope that we have addressed your concerns, and that you will consider raising your score. If we have left any notable points of concern unaddressed, please let us know, and we will be happy to respond.**\n\n[1]Dropedge: Towards deep graph convolutional networks on node classification. ICLR 2020.  \n[2]Local augmentation for graph neural networks. ICML 2022.  \n[3]Tuneup: A training strategy for improving generalization of graph neural networks. arXiv 2022.  \n[4]Link prediction with persistent homology: An interactive view. ICML 2021.  \n[5]Node Feature Extraction by Self-Supervised Multi-scale Neighborhood Prediction. ICLR 2022.  \n[6]TransGCN: Coupling transformation assumptions with graph convolutional networks for link prediction. K-CAP 2019.  \n[7]Neighborhood attention networks with adversarial learning for link prediction. IEEE Transactions on Neural Networks and Learning Systems 2021.  \n[8]https://ogb.stanford.edu/docs/linkprop/#ogbl-citation2.   \n[9]Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model. KDD 2008.     \n[10]Performance of Recommender Algorithms on Top-N Recommendation Tasks. RecSys 2010.  \n[11]Leveraging Meta-Path Based Context for Top-N Recommendation with A Neural Co-Attention Model. KDD 2018.  \n[12]Explainable Reasoning over Knowledge Graphs for Recommendation. AAAI 2019.  \n[13]Efficient Training on Very Large Corpora via Gramian Estimation. ICLR 2019.  \n[14]Mlpinit: Embarrassingly simple gnn training acceleration with mlp initialization. ICLR 2023.   \n[15]Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. NeurIPS Benchmark Track 2023.  \n[16]Tail-GNN: Tail-Node Graph Neural Networks. KDD 2021.  \n[17]Cold Brew: Distilling graph node representations with incomplete or missing neighborhoods. ICLR 2022.  \n[18]GRAPHPATCHER: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation. NeurIPS 2023.  \n[19]https://www.amazon.science/the-history-of-amazons-recommendation-algorithm."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648242529,
                "cdate": 1700648242529,
                "tmdate": 1700648242529,
                "mdate": 1700648242529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LAMtouzacm",
                "forum": "yONJt6nFc3",
                "replyto": "sbbIyzSBbp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Stage Closing Soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer pJyQ,\n\nThank you for the time you spent reviewing our paper and the constructive feedback you've provided. We believe we have addressed your questions and concerns in detail. As today is the **final day** of our discussion, we anticipate the opportunity to engage with you. Please consider raising our score if we have addressed all of your questions/concerns. If not, we are happy to elaborate on our responses and answer any additional questions you may have!\n\nThanks again,  \nNodeDup authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689256209,
                "cdate": 1700689256209,
                "tmdate": 1700689256209,
                "mdate": 1700689256209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "69DwnANscL",
                "forum": "yONJt6nFc3",
                "replyto": "oMw9h3KZHs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                ],
                "content": {
                    "comment": {
                        "value": "For W1, unfortunately, almost all of the previous studies adding self-loop will intuitively use them as supervision signals, and intrinsically would solve the cold-start issues. \n\nFor W2, it would be an issue on the topic of cold-start tasks, especially for the self-loop based methods. With easy negatives and training with self-loops of cold-start nodes, it will have much higher tendency to make those negative easier. Besides the performance evaluation, the authors should also elaborate how the proposed method can be applied into real-world applications because the real-world candidates cannot be randomly-sampled."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722554213,
                "cdate": 1700722554213,
                "tmdate": 1700722554213,
                "mdate": 1700722554213,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UqlvVpVyOK",
                "forum": "yONJt6nFc3",
                "replyto": "MgF20SBpCu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                ],
                "content": {
                    "comment": {
                        "value": "For W3, in this case, I would encourage the authors to compare methods in the same experimental setup, to verify there is no biases favoring the proposed method as mentioned in W2.\n\nFor W4, thanks the authors for spending the time on conducting the experiments on a larger scale dataset."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723276438,
                "cdate": 1700723276438,
                "tmdate": 1700723276438,
                "mdate": 1700723276438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HTiOuviVZm",
                "forum": "yONJt6nFc3",
                "replyto": "U6kwtsWG82",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                ],
                "content": {
                    "comment": {
                        "value": "For Q1 and Q2, thanks the authors for conducting additional experiments to address the concerns.\n\nI also notice that the citations numbers listed in the author comments cannot map to the description."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723943275,
                "cdate": 1700723943275,
                "tmdate": 1700723943275,
                "mdate": 1700723943275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4vmwd84kVh",
                "forum": "yONJt6nFc3",
                "replyto": "HTiOuviVZm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Reviewer_pJyQ"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledged that I have read all of the author responses. As some concerns have been addressed, especially about extra experiments, I have increased my score to 5 based on some remaining flaws in the novelty and experimental settings."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724047544,
                "cdate": 1700724047544,
                "tmdate": 1700724047544,
                "mdate": 1700724047544,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ATmu94ISE5",
            "forum": "yONJt6nFc3",
            "replyto": "yONJt6nFc3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1580/Reviewer_exPD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1580/Reviewer_exPD"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an augmentation-based method (i.e., NodeDup) tailored for the isolated and low-degree nodes in the link prediction task. The proposed method can not only significantly improve the prediction performance on cold nodes, but also can maintain the overall prediction capability. By simply duplicating the cold nodes, the proposed method exhibits faster running speed than the previous methods based on augmentation. Comparing with the vanilla GNN models, the cold-start models, and the graph data augmentation models, the experimental results are sufficient to demonstrate the superiority of NodeDup. Additionally, NodeDup is a light plug-and-play method which is able to collaborate with almost all decoders and encoders for link prediction task."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "S1: The cold-start problem towards graph data is meaningful and challenging, whose difficulty mainly lies on the isolated and low-degree nodes. The proposed method NodeDup succeeds in improving the prediction capability on the cold node, without damage to the overall performance at meantime.\nS2: Extensive experiments are conducted. The ablation study on duplication times and nodes facilitates the understanding of when NodeDup is effective.\nS3: The compatibility of NodeDup is exceptional. The prediction capability consistently outperforms different types of baselines, when combines with GraphSAGE, GAT, and JKNet encoders."
                },
                "weaknesses": {
                    "value": "W1: The major concern is the definition of cold nodes or long-tailed nodes. (1) After checking the paper several times, I am quite surprised to see that the threshold lambda is set to 2 over all datasets. Considering the difference of number of nodes/edges/communities/densities in different graphs, it would be much better if authors could carefully define and investigate the definition of code nodes. (2) I am wondering whether it is proper to define code nodes based on the 1-st order degree instead of high-order connectivity. For example, given two nodes: m has 2 connected neighbors but has 200 two-hop neighbors while n has 10 connected neighbors but only has 10 two-hop neighbors. Based on hop-wise message passing used in GNNs, I think node m would attend to more neighborhood information, while node n might be more cold. Hence, it would be much better if authors could explain more on the definition of cold nodes.     \nW2: Based on the claim in the introduction: they are under-represented in standard supervised LP training due to their few (if any) connections, a straightforward strategy is to change the distribution of node sampling to ensure more tail nodes could join the training process. In my humble opinion, as NODEDUP only duplicate code nodes and connected them to the original ones, it essentially only changes the distribution of batch sampling. I notice that in E.3, authors only present the performance compared with upsampling, while the discussions of experimental results are missing. It would be much better to clarify the reasons why NODEDUP is effective instead of numbers.  \nW3: Another concern is the baselines. First, the old-fashioned link prediction baselines are missing, including but not limited to common neighbors (CN), Jaccard, preferential attachment (PA), Adamic-Adar (AA), resource allocation (RA), Katz, PageRank (PR), and SimRank (SR). (2) Some SOTA models are also needed such as SEAL (Link Prediction Based on Graph Neural Networks). (3) The studied problem is similar to the degree-bias in GNNs. Please consider to add baselines such as `` On Generalized Degree Fairness in Graph Neural Networks\u2019\u2019."
                },
                "questions": {
                    "value": "Q1: What is the key innovation of NodeDup? According to the manuscript, it seems that NodeDup is almost the standard node duplicate technique. More discussions about the existing node duplicate works are recommended.\nQ2: What is the main difference between NodeDup(L) and self-loop in GNN? During the experiment, does the GraphSAGE (and GAT, JKNet) baseline introduce self-loop process? If no, the authors may want to add more experimental explanations. If yes, why NodeDup(L) is more effective than vanilla GNN models?\nQ3: Why NodeDup(L) is more effective than NodeDup towards warm nodes in almost all scenarios? From my perspective, the two methods conduct the same operation on the warm nodes. The authors may want to add more discussions.\nQ4: In light of the similarity to self-distillation mentioned in section 3.3, the comparison between NodeDup and self-distillation technique is recommended."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836909382,
            "cdate": 1698836909382,
            "tmdate": 1699636086631,
            "mdate": 1699636086631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rbtQGXNdgs",
                "forum": "yONJt6nFc3",
                "replyto": "ATmu94ISE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer exPD [1/6]"
                    },
                    "comment": {
                        "value": "Dear Reviewer exPD,\n\nThank you for your valuable feedback. Our detailed response to your concerns is as follows:\n\n> W1: Concerns about the definition of cold nodes or long-tailed nodes. (1) The threshold set to 2 over all datasets. \n\nWe apologize for any confusion regarding the definition of cold nodes. \n1. Our decision to set the threshold $\\delta$ at 2 is grounded in data-driven analysis, as illustrated in Figures 1 and 6. These figures reveal that nodes with degrees not exceeding 2 consistently perform below the average Hits@10 across all datasets, and higher than 2 will outperform the average results.\n2. Additionally, our choice aligns with methodologies in previous studies [1,2], where cold or long-tailed nodes are identified using a fixed threshold. \n3. We conduct further experiments with different threshold $\\delta$ on Cora and Citeseer datasets. The results are shown below. Our findings were consistent across different thresholds, with similar observations at $\\delta$ = 1, $\\delta$ = 2, and $\\delta$ = 3. This indicates that our method's effectiveness is not significantly impacted by changes in this threshold.\n\nThank you for your suggestion. We have incorporated this clarification into our paper to ensure greater clarity.\n\n|          |            ||   $\\delta$ = 1         ||     $\\delta$ =  2           ||     $\\delta$ =    3                  |\n|----------|:------------:|:--------------:|:---------------:|:------------:|:-------------:|:------------:|:-------------:|\n|          |            | GSage        | GSage+NodeDup |     GSage    | GSage+NodeDup |     GSage    | GSage+NodeDup |\n| Cora     | Isolated   | 31.34 \u00b1 5.60 | 42.20 \u00b1 2.30  | 32.20 \u00b1 3.58 | 44.27 \u00b1 3.82  | 31.95 \u00b1 1.26 | 43.17 \u00b1 2.94  |\n|          | Low-degree | 53.98 \u00b1 1.20 | 57.99 \u00b1 1.34  | 59.45 \u00b1 1.09 | 61.98 \u00b1 1.14  | 59.64 \u00b1 1.01 | 62.68 \u00b1 0.63  |\n|          | Warm       | 61.68 \u00b1 0.29 | 61.17 \u00b1 0.43  | 61.14 \u00b1 0.78 | 59.07 \u00b1 0.68  | 61.03 \u00b1 0.79 | 59.91 \u00b1 0.44  |\n|          | Overall    | 58.01 \u00b1 0.57 | 59.16 \u00b1 0.44  | 58.31 \u00b1 0.68 | 58.92 \u00b1 0.82  | 58.08 \u00b1 0.74 | 59.99 \u00b1 0.53  |\n| Citeseer | Isolated   | 47.25 \u00b1 1.82 | 56.49 \u00b1 1.72  | 47.13 \u00b1 2.43 | 57.54 \u00b1 1.04  | 47.31 \u00b1 2.17 | 56.90 \u00b1 1.12  |\n|          | Low-degree | 54.10 \u00b1 0.85 | 71.09 \u00b1 0.47  | 61.88 \u00b1 0.79 | 75.50 \u00b1 0.39  | 62.97 \u00b1 0.83 | 75.45 \u00b1 0.40  |\n|          | Warm       | 72.41 \u00b1 0.35 | 74.57 \u00b1 1.04  | 71.45 \u00b1 0.52 | 74.68 \u00b1 0.67  | 73.57 \u00b1 0.46 | 75.02 \u00b1 0.84  |\n|          | Overall    | 64.27 \u00b1 0.45 | 70.53 \u00b1 0.91  | 63.77 \u00b1 0.83 | 71.73 \u00b1 0.47  | 64.05 \u00b1 0.42 | 71.80 \u00b1 0.40  |\n\n\n> (2) Whether it is proper to define cold node based on the 1st order degree.\n\nThank you for asking about the definition of \"cold nodes\". We believe that defining cold nodes based on 1st-order degree is justified for several reasons:\n\n1. This approach aligns with established methodologies [1,2,3] in current research, where they define long-tail nodes based on their 1st-order degree.\n2. As evidenced in Figures 1 and 6, our experimental results highlight a performance gap among nodes when categorized based on their 1st-order degree. This observation provides a sound experimental setting for evaluation, reinforcing the validity of our approach.\n3. In the scenario provided by the reviewer, a node like $m$, with few direct (1st-order) neighbors but a large number of two-hop (2nd-order) neighbors, may have its representation disproportionately influenced by the super-stars (high-degree neighbors). This could adversely affect its performance, despite it being ostensibly less 'cold' than a node like $n$, which has a few 1st-order neighbors and also a few 2nd-order neighbors. This phenomenon further validates our choice to prioritize 1st-order connections in defining cold nodes. \n\nThat being said, we agree that exploring cold-start scenarios based on 2-hop neighbors can be a good followup work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646545486,
                "cdate": 1700646545486,
                "tmdate": 1700646545486,
                "mdate": 1700646545486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JBPZxMlsYY",
                "forum": "yONJt6nFc3",
                "replyto": "ATmu94ISE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer exPD [2/6]"
                    },
                    "comment": {
                        "value": ">W2: Why NODEDUP is effective instead of numbers?\n\nAs we demonstrated in Section 3.2, there are two perspectives contributing to the effectiveness of NodeDup. \n1. **From the aggregation perspective,** when the transformation weights for an anchor node and its neighbors are different, messages passed from neighbors will play different roles than those from the anchor node itself. With NodeDup, our model can leverage extra information from an additional \"view\". The existing view is when a node is regarded as the anchor node during message passing, whereas the additional view is when that node is regarded as one of its neighbors thanks to the duplicated node from NodeDup.\n2. **From the supervision perspective,** NodeDup adds edges that connect low-degree nodes to their duplicates. These additional edges serve as one of the few positive supervision signals for link prediction. More supervision signals for cold nodes lead to better quality embeddings.\n\nFrom the results shown in Figure 2, we can observe that both these two perspectives play important roles in the performance improvements of NodeDup."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646632283,
                "cdate": 1700646632283,
                "tmdate": 1700646632283,
                "mdate": 1700646632283,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7r09DOlCY1",
                "forum": "yONJt6nFc3",
                "replyto": "ATmu94ISE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer exPD [3/6]"
                    },
                    "comment": {
                        "value": ">W3: Another concern is the baselines. First the old-fashined link prediction baselines are missing. Some SOTA models are also needed such as SEAL. The studied problem is similar to the degree-bias in GNNs. Consider \"On Generalized Degree Fairness in Graph Neural Networks\".\n\nWe sincerely appreciate your suggestions. We have added extra baselines to make our analysis more comprehensive. \n1. We further compare our methods with **traditional link prediction baselines**, such as common neighbors (CN), Adamic-Adar(AA), Resource allocation (RA). The results are shown below. We observe that GSage+NodeDup can consistently outperform these heuristic methods across all the datasets, with particularly significant improvements observed on isolated nodes. \n2. We have conducted the experiments to compare with the existing methods [2,3], which also tackle **degree bias in GNNs**. The results, presented in Table 1, consistently show our methods outperforming these approaches. We appreciate the suggestion of work, DegFairGNN [4], and have thoroughly investigated its application in the link prediction context. The results are shown below. Unfortunately, we've found that this approach is not well-suited for link prediction tasks for several reasons:\n    - **Designed for Node Classification:** The method in [4] introduces a learnable debiasing function in the GNN architecture to produce fair representations for nodes, aiming for similar predictions for nodes within the same class, regardless of their degrees. However, this is designed specifically for node classification tasks. For example, the fairness loss, which ensures prediction distribution uniformity among low and high degree node groups, is not suitable for link prediction, because there is no defined node class in link prediction tasks.\n    - **Importance of Degree Trait in Link Prediction:** The approach in [4] achieves significant performance in node classification tasks by effectively mitigating degree bias. However, in the context of link prediction, the degree trait is crucial. Applying [4] would compromise the model's ability to learn from structural information, such as isomorphism and common neighbors. This, in turn, would negatively impact link prediction performance, as evidenced by references [5,6]."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646712892,
                "cdate": 1700646712892,
                "tmdate": 1700646712892,
                "mdate": 1700646712892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qt5xTkvq3T",
                "forum": "yONJt6nFc3",
                "replyto": "ATmu94ISE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer exPD [4/6]"
                    },
                    "comment": {
                        "value": "|               |            | **CN** | **AA** | **RA** | **DegFairGNN** | **Gsage**    | **Gsage+NodeDup** |\n|---------------|:------------:|:--------:|:--------:|:--------:|:----------------:|:--------------:|:-------------------:|\n| **Cora**      | Isolated   | 0.00   | 0.00   | 0.00   | 18.70 \u00b1 1.53   | 32.20 \u00b1 3.58 | 44.27 \u00b1 3.82      |\n|               | Low-degree | 20.30  | 20.14  | 20.14  | 38.43 \u00b1 0.14   | 59.45 \u00b1 1.09 | 61.98 \u00b1 1.14      |\n|               | Warm       | 38.33  | 38.90  | 38.90  | 42.49 \u00b1 1.82   | 61.14 \u00b1 0.78 | 59.07 \u00b1 0.68      |\n|               | Overall    | 25.27  | 25.49  | 25.49  | 39.24 \u00b1 1.10   | 58.31 \u00b1 0.68 | 58.92 \u00b1 0.82      |\n| **Citeseer**  | Isolated   | 0.00   | 0.00   | 0.00   | 15.50 \u00b1 1.27   | 47.13 \u00b1 2.43 | 57.54 \u00b1 1.04      |\n|               | Low-degree | 26.86  | 27.00  | 27.00  | 45.06 \u00b1 0.96   | 61.88 \u00b1 0.79 | 75.50 \u00b1 0.39      |\n|               | Warm       | 37.30  | 39.02  | 39.02  | 55.47 \u00b1 1.08   | 71.45 \u00b1 0.52 | 74.68 \u00b1 0.67      |\n|               | Overall    | 30.81  | 31.85  | 31.85  | 44.58 \u00b1 1.03   | 63.77 \u00b1 0.83 | 71.73 \u00b1 0.47      |\n| **CS**        | Isolated   | 0.00   | 0.00   | 0.00   | 17.93 \u00b1 1.35   | 56.41 \u00b1 1.61 | 65.87 \u00b1 1.70      |\n|               | Low-degree | 39.60  | 39.60  | 39.60  | 49.83 \u00b1 0.68   | 75.95 \u00b1 0.25 | 81.12 \u00b1 0.36      |\n|               | Warm       | 72.73  | 72.74  | 72.72  | 61.72 \u00b1 0.37   | 84.37 \u00b1 0.46 | 84.76 \u00b1 0.41      |\n|               | Overall    | 69.10  | 69.11  | 69.10  | 60.20 \u00b1 0.37   | 83.33 \u00b1 0.42 | 84.23 \u00b1 0.39      |\n| **Physics**   | Isolated   | 0.00   | 0.00   | 0.00   | 19.48 \u00b1 2.94   | 47.41 \u00b1 1.38 | 66.65 \u00b1 0.95      |\n|               | Low-degree | 46.08  | 46.08  | 46.08  | 47.63 \u00b1 0.52   | 79.31 \u00b1 0.28 | 84.04 \u00b1 0.22      |\n|               | Warm       | 85.48  | 85.74  | 85.70  | 62.79 \u00b1 0.82   | 90.28 \u00b1 0.23 | 90.33 \u00b1 0.05      |\n|               | Overall    | 83.87  | 84.12  | 84.09  | 62.13 \u00b1 0.76   | 89.76 \u00b1 0.22 | 90.03 \u00b1 0.05      |\n| **Computers** | Isolated   | 0.00   | 0.00   | 0.00   | 9.36 \u00b1 1.81    | 9.32 \u00b1 1.44  | 19.62 \u00b1 2.63      |\n|               | Low-degree | 28.31  | 28.31  | 28.31  | 18.90 \u00b1 0.81   | 57.91 \u00b1 0.97 | 61.16 \u00b1 0.92      |\n|               | Warm       | 59.67  | 63.50  | 62.84  | 31.44 \u00b1 2.25   | 66.87 \u00b1 0.47 | 68.10 \u00b1 0.25      |\n|               | Overall    | 59.24  | 63.03  | 62.37  | 31.27 \u00b1 2.22   | 66.67 \u00b1 0.47 | 67.94 \u00b1 0.25      |\n| **Photos**    | Isolated   | 0.00   | 0.00   | 0.00   | 12.99 \u00b1 1.51   | 9.25 \u00b1 2.31  | 17.84 \u00b1 3.53      |\n|               | Low-degree | 28.44  | 28.78  | 28.78  | 20.18 \u00b1 0.21   | 52.61 \u00b1 0.88 | 54.13 \u00b1 1.58      |\n|               | Warm       | 64.53  | 67.26  | 66.88  | 42.72 \u00b1 0.89   | 67.64 \u00b1 0.55 | 68.68 \u00b1 0.49      |\n|               | Overall    | 63.94  | 66.64  | 66.26  | 42.37 \u00b1 0.87   | 67.32 \u00b1 0.54 | 68.39 \u00b1 0.48      |\n| **IGB-100K**  | Isolated   | 0.00   | 0.00   | 0.00   | 57.09 \u00b1 21.08  | 75.92 \u00b1 0.52 | 88.04 \u00b1 0.20      |\n|               | Low-degree | 12.26  | 12.26  | 12.26  | 59.45 \u00b1 21.84  | 79.38 \u00b1 0.23 | 88.98 \u00b1 0.17      |\n|               | Warm       | 30.65  | 30.65  | 30.65  | 65.57 \u00b1 20.43  | 86.42 \u00b1 0.24 | 88.28 \u00b1 0.20      |\n|               | Overall    | 26.22  | 26.22  | 26.22  | 64.16 \u00b1 20.70  | 84.77 \u00b1 0.21 | 88.39 \u00b1 0.18      |\n\n3. Considering our methods are flexible to integrate with GNN-based link prediction structures, we conduct the experiments on top of **SEAL**[5] on the Cora and Citeseer datasets. The results are shown in the following table. We can observe that adding NodeDup on top of SEAL can consistently improve link prediction performance in the Isolated and Low-degree node settings on these two datasets.\n\n|               |            |     **SEAL**     | **SEAL + NodeDup** |\n|---------------|:------------:|:------------:|:--------------:|\n| **Cora**          | Isolated   | 62.20 \u00b1 1.06 |  70.73 \u00b1 0.61  |\n|               | Low-degree | 66.80 \u00b1 2.83 |  67.70 \u00b1 4.11  |\n|               | Warm       | 56.69 \u00b1 2.36 |  54.87 \u00b1 1.61  |\n|               | Overall    | 60.60 \u00b1 2.38 |  60.89 \u00b1 2.36  |\n| **Citeseer**      | Isolated   | 56.92 \u00b1 5.53 |  66.37 \u00b1 1.01  |\n|               | Low-degree | 64.13 \u00b1 2.56 |  65.54 \u00b1 1.69  |\n|               | Warm       | 58.81 \u00b1 3.22 |  60.73 \u00b1 2.75  |\n|               | Overall    | 60.18 \u00b1 2.98 |  63.35 \u00b1 1.43  |\n\nThank you again for pointing these baselines to strengthen our analysis. We have added all of these analysis in the updated submission."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646762281,
                "cdate": 1700646762281,
                "tmdate": 1700646762281,
                "mdate": 1700646762281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r2PLUOnj3r",
                "forum": "yONJt6nFc3",
                "replyto": "ATmu94ISE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer exPD [5/6]"
                    },
                    "comment": {
                        "value": ">Q1: What is the key innovation of NodeDup? According to the manuscript, it seems that NodeDup is almost the standard node duplicate technique. More discussions about the existing node duplicate works are recommended.\n\nOur work, to the best of our knowledge, is the first to employ node duplication for link prediction tasks. This innovation manifests in two key aspects:\n\n1. **Methodological Innovation:** NodeDup is not conventional duplication - it selectively duplicates only the cold nodes, rather than all nodes. In addition to this duplication, NodeDup establishes connections between each cold node and its duplicate. This approach is crucial for two reasons: firstly, it offers additional viewpoint information during the aggregation process; secondly, it provides crucial supervision training signals for link prediction tasks. \n2. **Analytical Contribution:** Beyond the design of the method, our work is the first to connect the simple node duplication trick with the cold-start issue, and we have experimentally demonstrated the effectiveness of this approach. In addition, we offer an extensive analysis in Section 3.2, where we delve into two potential reasons underpinning the effectiveness of our proposed method. Moreover, we explore the relationship between NodeDup and self-distillation in Section 3.3, providing deeper insights to the research community.\n\n>Q2: What is the main difference between NodeDup(L) and self-loop in GNN? During the experiment, does the GraphSAGE (and GAT, JKNet) baseline introduce self-loop process? If no, the authors may want to add more experimental explanations. If yes, why NodeDup(L) is more effective than vanilla GNN models?\n\nApologies for the confusion. To clarify, GraphSAGE, GAT, and JKNet inherently incorporate self-loops as part of their foundational methodologies. Accordingly, in our experiments, we have also integrated self-loops into these baseline models. However, as we demonstrated at the end of Section 3.2, it's important to note that NodeDup(L) differs from traditional self-loops in two significant ways:\n1. **For the aggregation,** we take GraphSAGE as an illustrative example. In the PyG's official implementation, GraphSAGE would have separate weights $\\text{W}_1$ and $\\text{W}_2$ for self-representation and neighbor representations. NodeDup enhances this by adding an extra self representation into the neighbor information. This additional representation is assimilated through the weight matrix $\\text{W}_2$, effectively providing an additional \"view\" or perspective of self representation, distinct from the conventional self-loop.\n2. **For the supervision signal,** different from the normal self-loops, the edges added by NodeDup(L) also serve as additional positive training samples for the cold nodes."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646912070,
                "cdate": 1700646912070,
                "tmdate": 1700646912070,
                "mdate": 1700646912070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qFGWmNQVjq",
                "forum": "yONJt6nFc3",
                "replyto": "ATmu94ISE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer exPD [6/6]"
                    },
                    "comment": {
                        "value": ">Q3: Why NodeDup(L) is more effective than NodeDup towards warm nodes in almost all scenarios? From my perspective, the two methods conduct the same operation on the warm nodes. The authors may want to add more discussions. \n\nThank you for pointing this out. We also observed the performance differences between NodeDup and NodeDup(L) upon cold/warm nodes and provided the analysis in Section 4.2. The biggest difference between NodeDup and NodeDup(L) is that NodeDup has the node duplication step. From our understanding, the impact of node duplication of NodeDup on the original graph structure likely affects the performance of warm nodes, which explains the better perfromance of NodeDup(L) in this setting. Besides, the node duplication of NodeDup also provides an extra view during the aggregation for the isolated nodes, which leads NodeDup is more effective than NodeDup(L) towards isolated nodes. \n\n>Q4: In light of the similarity to self-distillation mentioned in section 3.3, the comparison between NodeDup and self-distillation technique is recommended.\n\nThank you for the recommendation. We have conducted the experiments to compare the NodeDup and self-distillation on Citeseer. The results are shown in Figure 4. We can observe that NodeDup consistently outperforms self-distillation in all scenarios. As demonstrated in Section 3.3, unlike self-distillation, NodeDup offers valuable positive supervision training signals for cold nodes. Furthermore, the multi-view information aggregated through NodeDup potentially offers more valuable insights than the supplementary information from self-distillation, especially for cold nodes. This could also be a contributing factor to the observed performance difference.\n\n**In light of our answers to your concerns, we hope you consider raising your score. If you have any more concerns, please do not hesitate to ask and we'll be happy to respond.**\n\n[1]Towards locality-aware meta-learning of tail node embeddings on networks. CIKM 2020.  \n[2]Tail-GNN: Tail-Node Graph Neural Networks. KDD 2021.  \n[3]Cold Brew: Distilling graph node representations with incomplete or missing neighborhoods. ICLR 2022.  \n[4]On Generalized Degree Fairness in Graph Neural Networks. AAAI 2023.  \n[5]Link prediction based on graph neural networks. NeurIPS 2018.  \n[6]Graph Neural Networks for Link Prediction with Subgraph Sketching. ICLR 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646980491,
                "cdate": 1700646980491,
                "tmdate": 1700646980491,
                "mdate": 1700646980491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HeFTjKkEpM",
                "forum": "yONJt6nFc3",
                "replyto": "ATmu94ISE5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Stage Closing Soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer exPD, \n\nAs today is the **final day** of our discussion, we anticipate the opportunity to engage with you. If you have any remaining questions or concerns, please don't hesitate to share them with us. We will be happy to respond. If we have sufficiently alleviated your concerns, we hope you'll consider raising your score. Thank you!\n\nBest regards,   \nNodeDup authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688988163,
                "cdate": 1700688988163,
                "tmdate": 1700689217579,
                "mdate": 1700689217579,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]