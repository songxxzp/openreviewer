[
    {
        "title": "Closing the Gap between TD Learning and Supervised Learning - A Generalisation Point of View."
    },
    {
        "review": {
            "id": "ToCH09j43m",
            "forum": "qg5JENs0N4",
            "replyto": "qg5JENs0N4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2040/Reviewer_6ajj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2040/Reviewer_6ajj"
            ],
            "content": {
                "summary": {
                    "value": "* This work discusses a form of generalization in reinforcement learning (RL) called \"stitching generalization.\" According to the paper, the stitching generalization cannot be achieved with existing supervised-learning (SL) based RL algorithms because it is fundamentally different from the generalization on independent & identically distributed (i.i.d.) dataset. This is explained via a concrete example of an MDP depicted in Figure 1.\n* Also, this work propose \"temporal data augmentation\" technique for goal-conditioned RL to actually implement stitching generalization inside the SL-based RL algorithms. This technique is based on careful sampling of augmented goal, which is determined by *closeness\" of states based on a clustering algorithm.\n* Moreover, this work provides a novel benchmark to evaluate the stitching capability of RL algorithm.\n* Lastly, this work empirically show the efficacy of the temporal data augmentation + outcome conditional behavior cloning (OCBC) algorithms on their own benchmark."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The motivation is clear and the main message is well presented. I enjoyed reading the paper.\n* Their main method of data augmentation is novel and intuitive.\n* The empirical results are promising, even after making the task more difficult to generalize.\n* Lastly, the limitations of their work is clearly mentioned. I also look forward to scalable version of their algorithm."
                },
                "weaknesses": {
                    "value": "* **Theoretical contribution is a bit marginal.**\n  - The authors claim that they \u201cprovide a theoretical framework for studying stitching.\u201d They indeed provided a formal definition of stitching generalization in Definition 1. However, it is not actually used anywhere. If it is not applicable to deduce any theoretical guarantee, what is the purpose of proposing such definition? At least for the counterexample proposed in Lemma 4.1 and depicted in Figure 1, the authors should be able to provide any kinds of implications on the (lower bound of) stitching generalization. This will definitely strengthen the theoretical contribution of the paper. If it is not possible, please kindly explain the reason/context.\n* **Isn\u2019t it a problem of trajectory-based sampling rather than transition-based sampling that it is impossible to implement stitching with SL-based methods?**\n  - At first glance, I thought that the statement \u201cstitching generalization is not the same as i.i.d. generalization\u201d is not quite the problem of supervised learning itself. Let\u2019s take a look at the counterexample in Figure 1. If we break down the trajectories collected by policies $\\beta_{h=1}$ and $\\beta_{h=2}$ into transitions, can\u2019t even a supervised method learn something from a virtual trajectory, say, 2-3-4, which was not exactly in the dataset, by only i.i.d. sampling the transitions? To rephrase my question, can\u2019t a SL-based method learn stitched behaviors by transition-based sampling or so-called *bootstrapping*? I don\u2019t think this would be exactly the same as the proposed temporal augmentation method. Please correct me if I\u2019m wrong.\n* **Weakness on Algorithm 1 (OCBC + Temporal Data Augmentation) regarding additional hyperparameter**\n  - It uses clustering algorithm, so it has an additional hyperparameter, $k$ for k-means algorithm. To the best of my knowledge, the main paper does not discuss on the effect of the choice of such hyperparameter, which might be difficult to properly tune.\n* **It seems necessary to conduct additional Experiments on DT + temporal augmentation.**\n  - Although the paper empirically proves that it can enhance the data utilization of *RvS* algorithm with their proposed augmentation technique, it does not provide any results on \u2018\u2019DT + temporal augmentation\u2019\u2019 combination. This experiment will strengthen the empirical contribution of the paper. If it is not applicable, please kindly explain why.\n* **There seems to be some minor typos.**\n  - pg 1. Kaelbling (1993) $\\rightarrow$ (Kaelbling, 1993)\n  - pg 3., Equation (1). $\\mathbb{E}\\_{s\\sim p_0(s_0)} [p_t^{\\pi}(s_{t+} =g \\mid s_0=s)]$ $\\rightarrow$  $\\mathbb{E}\\_{s\\sim p_0(s_0)}[p_+^{\\pi}(s_{t+} =g \\mid s_0=s)]$\n  - pg 3. \u201c\u2026 and then sampling a trajectory from the corresponding policy $\\\\{\\beta(a\\mid s, h)\\\\}$.\u201d $\\rightarrow$ $\\beta(a\\mid s, h)$"
                },
                "questions": {
                    "value": "* In Definition 1, does the performance function $f$ have any relavance to the objective $J$ defined in Equation (3) or the maximum likelihood objective defined in Equation (6) and (7)?\n* In page 5, what does it mean by \u2018combinatorial generalization\u2019? Do you have a definition or any reference for that?\n* Just curious: regarding Lemma 4.1, do you have any comments or implications on the (sort of) bias term $\\E_{p(h)} \\left[ p^{\\beta_h}_+ (s_{t+} \\mid s) p^{\\beta_h}_+ (s) \\right] - p^{\\beta}_+ (s_{t+} \\mid s) p^{\\beta}_+ (s)$ ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Reviewer_6ajj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2040/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698664106495,
            "cdate": 1698664106495,
            "tmdate": 1699636135846,
            "mdate": 1699636135846,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j7CgwdeFZa",
                "forum": "qg5JENs0N4",
                "replyto": "ToCH09j43m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 6ajj (1 / 2)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\nWe thank the reviewer for their detailed feedback. It seems like the reviewer's main concerns are the importance of theoretical contributions, introduction of a new hyperparameter and some missing experiments. We have revised Sections 5, 6 and:\n\n1) Added Figure 8, in appendix B, to show that data augmentation works for a range of K values {10 25 40 50 75 100}.\n\n2) Added Figure 11, in Appendix B which shows preliminary results for scalable SL algorithms built on top of our work can also achieve stitching generalization.\n\n3) Added Appendix D.1, which shows that uses our definition of stitching, to prove that the optimal OCBC performs policy improvement over the behavior policy. \n\n**Do these clarification and new experiments and proofs address all the reviewer's concerns?**\n\n> They indeed provided a formal definition of stitching generalization in Definition 1. However, it is not actually used anywhere.\n\nWe believe a formal definition of stitching generalization is useful because we use the definition to generate the datasets used to evaluate stitching. We revise section 5, para \u201cPopular offline datasets do not evaluate stitching.\u201d (see green text and equation), to clarify this. In Appendix D.1, we add a section which shows that the Bayes optimal OCBC policy that is learnt on a \u201cstitched dataset\u201d, where states-goals are sampled using the test distribution, is an improvement over the BC policy. In offline RL, policy improvement guarantees over the behavior policy are important because optimality guarantees are not generally realisable [1,2].\n\n> Weakness on Algorithm 1 (OCBC + Temporal Data Augmentation) regarding additional hyperparameter\n\nIn appendix B, Figure 8, we add a new experiment which ablates the value of the number of centroids on two environments (ant maze and point maze medium). We run experiments with K ranging between {10 25 40 50 75 100}. We can see that all values significantly improve the stitching generalization over the baseline. \n\n\n> Isn\u2019t it a problem of trajectory-based sampling rather than transition-based sampling\n\nOCBC algorithms also learn from individual transitions. For example in Figure 1, they will be trained on transitions (2,up,3) and (3,right,4). But they will never sample (2,up,4), which is out of distribution (has 0 support in the training distribution). There is no guarantee that a SL method will perform on points which are strictly out of distribution. But our paper\u2019s main goal is to take initial steps to endow purely SL methods with these desirable properties of stitching-based algorithms like TD, while still retaining their simplicity and scalability. \n\n> It seems necessary to conduct additional Experiments on DT + temporal augmentation.\n\nThe reason we do not add data augmentation with DT is that because DT takes in a context of previous states and actions, the data augmentation method should cluster a history of states and actions, which is difficult to do computationally. We have added this reason in our updated paper (see Caption of Figure 4, text in green).\n\n> I also look forward to scalable version of their algorithm.\n\nWe believe that purely SL algorithms which learn state representations containing information about future outcomes, can possess desirable properties associated with TD learning while still retaining their simplicity and scalability. In Figure 11, in Appendix B, we include an experiment that shows using such representations as inputs for the OCBC policy, improves stitching generalization without the need of a distance metric.\n\n> There seems to be some minor typos.\n\nThank you for pointing these out, we have fixed these typos."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163471657,
                "cdate": 1700163471657,
                "tmdate": 1700163471657,
                "mdate": 1700163471657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lu2YAfFZwa",
            "forum": "qg5JENs0N4",
            "replyto": "qg5JENs0N4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2040/Reviewer_4eRX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2040/Reviewer_4eRX"
            ],
            "content": {
                "summary": {
                    "value": "This work analyzes SL-based RL approaches for (goal-conditioned) offline RL, where data are collected using various policies and one hopes to \"stitch\" existing experience/trajectories to generalize to unseen (start, goal) pairs. It shows that common outcome conditional behavioral cloning (OCBC) methods can fail this task and shows a lemma (Lemma 4.1) that learning from training experience with different contexts may not lead to generalization (in terms of the mixed behavior policy). Then the paper proposes to use temporal augmentation that essentially stitches two trajectories together to have better coverage for the (start, goal) pair. Experiments show that the augmentation can be effective for some offline RL tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Studies an important problem of generalization from offline data\n- Shows a counterexample of the training and test discrepancy (Lemma 4.1)\n- Clear writing in most places"
                },
                "weaknesses": {
                    "value": "1. Some key concepts and results are not clearly explained.\n\n    \n\n    1.1. The core concept, \"stitch property\", is poorly introduced and readers have to refer to the reference to know what it means. There is no consistent nor clear definition for this \"stitch property\", making it difficult to understand what the paper is trying to achieve.\n\n    1.2. \"Our paper focuses on the problem of stitching generalization, not finding optimal policies; however, stitching generalization should be a necessary component for finding optimal policies.\" Proof needed.\n\n2. The proposed heuristic of using clustering to stitch close states together lacks theoretical guarantees. What distance metric is suitable here? In the RL context, similar states do not mean they have similar outcomes. For example, two states can be close to each other in Euclidean norm but blocked by a walk and thus not reachable; or two states can be far apart (e.g., top and bottom of a cliff) but one can easily reach the bottom by falling but not the other way around.\n\n3. For the experiment, the paragraph on \"Does more data remove the need for augmentation\" is not very surprising given the construction of the dataset. In SL, the fact that more data helps generalization is based on the IID assumption, which does not hold in the current setting since the (state, goal) pairs are not seen during training (also Lemma 4.1). \n\nMinor\n- $p_+^{\\beta_h}(s,a)$ in Eq.(7) is undefined.\n- Both generalization and generalisation are used.\n- point-maze large taks -> task"
                },
                "questions": {
                    "value": "Q1: Any suggestions for a reasonable distance metric for the states? \n\nQ2: Are there any theoretical justification for the temporal augmentation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2040/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813284798,
            "cdate": 1698813284798,
            "tmdate": 1699636135761,
            "mdate": 1699636135761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AZpGvDTjoH",
                "forum": "qg5JENs0N4",
                "replyto": "Lu2YAfFZwa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 4eRX"
                    },
                    "comment": {
                        "value": "Dear reviewer,\nWe thank the reviewer for their detailed feedback. It seems like the reviewer's main concerns are regarding a clear definition of stitching, a theoretical justification for the data-augmentation, and the difficulty of obtaining a good distance metric. We have incorporated these concerns using new updates to the papers and new image based experiments: \n\n1) We add a new paragraph at the start of Section 4 (green text) to explain how stitching is colloquially associated with different properties of TD learning. We add that we focus on one of these properties as a clear difference between TD and OCBC methods and call it stitching generalization, which we concretely define in Definition 1.\n\n2) In Figures 9 and 10, in Appendix B, we perform additional experiments with an image-based version of the point mazes. We show that performing data augmentation works with images as well, where a good distance metric is not available.\n\n3) In section 5, we added a paragraph \u201cTheoretical intuition on temporal data augmentation\u201d, to provide some theoretical justification for data augmentation.\n\n4) In Figure 11, in Appendix B, we include a preliminary experiment that removes the use of a distance metric, by learning representations. We believe that this approach is a promising direction for the future work that builds on our paper.  \n\nBelow, we will describe these updates and experiments in more detail and answer the specific reviewer questions **Do these answers, revisions and experiments address all the reviewer's concerns?**\n\n> In the RL context, similar states do not mean they have similar outcomes\n\nWe agree with the reviewer that having a good distance metric is challenging and have already acknowledged the limitation in the main paper. But empirically, we show that this limitation might not always matter. We add two sets of experiments ( appendix B.2, Figures 9 and 10 ) on image-based pointmaze tasks and show that K-means using L2 distances between images can perform well, significantly improving over the OCBC baselines on all tasks. We would like to add that for all of our maze experiments, if the cut-off distances for K-means clustering was too large, then it would group together states on different sides of the walls like you suggest. But our empirical results (Figures 4,9, and 10) show that using K-means works well.\n\n> \"Does more data remove the need for augmentation\" is not very surprising given the construction of the dataset\n\nIn this work, we aim to understand and evaluate the stitching properties of OCBC algorithms. The experiments are designed to evaluate this question and support the claim that data augmentation isn't just about inflating the size of the dataset. This claim (and verification) is important because in today's world of large models, many researchers might otherwise be tempted to throw large amounts of data at this problem, without avail.\n\n> Q1: Any suggestions for a reasonable distance metric for the states?\n\nWe believe that learning state representations such that the distance between these representations contains information about the similarity of their outcomes, is a scalable and promising direction of future work [1,2]. This will solve the exact problem that you correctly pointed out. In Figure 11, in Appendix B, we include an experiment that shows using such representations as inputs for the OCBC policy, improves stitching generalization without the need of a distance metric.\n\n> Q2: Are there any theoretical justification for the temporal augmentation?\n\nWhile exact theoretical guarantees for temporal data augmentation will depend on how good the distance metric is, we can think of the temporal data augmentation as trying to diversify the dataset by sampling from $\\pi^{\\beta}_+(g \\mid s)$. Since this is the test distribution for stitching, data augmentation should improve stitching generalization. In section 5, we add a paragraph, \u201cTheoretical intuition on temporal data augmentation\u201d to intuitively explain how data augmentation leads to a better policy.\n\n> Minor\nThank you for spotting these errors, we have fixed them in our updated paper.\n\n[1] DATA-EFFICIENT REINFORCEMENT LEARNING WITH SELF-PREDICTIVE REPRESENTATIONS\n[2] Contrastive Learning As a Reinforcement Learning Algorithm"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163219202,
                "cdate": 1700163219202,
                "tmdate": 1700163219202,
                "mdate": 1700163219202,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "idcWAScwws",
                "forum": "qg5JENs0N4",
                "replyto": "EbNT8G7ybI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Reviewer_4eRX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Reviewer_4eRX"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for providing additional explanations and experiments. \n\nWhile I do believe the paper studies an important problem, the additional explanation (first paragraph of Section 4) of the stitching property still lacks mathematical rigor. Additionally, the experiment based on images can still be favorable to L2 distance because similar images do have similar meanings in that context.\n\nOverall, I think the paper has merit in a better understanding of the problem, but the discussion is not mathematically rigorous and the algorithm development lacks theoretical justifications. As a result, I keep my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603292913,
                "cdate": 1700603292913,
                "tmdate": 1700603292913,
                "mdate": 1700603292913,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aMjeUnIvUX",
                "forum": "qg5JENs0N4",
                "replyto": "Lu2YAfFZwa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 4eRX"
                    },
                    "comment": {
                        "value": "> the additional explanation (first paragraph of Section 4)\n\nThis is meant to be an intuitive introduction on how stitching is colloquially associated with various properties in the literature. In Definition 1, we give the precise definition of stitching generalization. If you think there is something about the definition that isn't rigorous, please let us know. We'd be happy to look into it.\n\n> the algorithm development lacks theoretical justifications\n\nThe data augmentation we propose does have theoretical backing. In Figure 2, in the limit of finest clusters, i.e, as the size of the blue circle tends to zero, temporal data augmentation is equivalent to sampling from the Markov chain s -> w -> g. This shows that in the limit of the finest clustering, temporal data augmentation will sample cross trajectory state-goal pairs. In practice, we show that K-means works well across various values of K (See Figure 8).\n\n>  images can still be favorable to L2 distance because similar images\n\nWe agree that there can be image based tasks where it will be difficult to make data augmentation work. But the image based maze tasks are challenging, and adding the temporal data-augmentation (5 lines of code) improves the performance of baselines by more than 2 times on all the 3 image based tasks.\n\n**We kindly ask the reviewer to reconsider the paper in light of the revisions and clarifications.**"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614112098,
                "cdate": 1700614112098,
                "tmdate": 1700659999645,
                "mdate": 1700659999645,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PBJebqXKe7",
            "forum": "qg5JENs0N4",
            "replyto": "qg5JENs0N4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2040/Reviewer_DGi6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2040/Reviewer_DGi6"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the differences between dynamic programming-based reinforcement learning (RL) and supervised learning (SL)-based RL. A key property of the former is \"stitching\" past experiences to address new tasks. The study relates this ability to a unique form of generalization. Through experiments, the authors reveal that SL-based RL methods might lack this stitching capability. To address this, they introduce temporal data augmentation, enabling SL-based RL to handle unseen (state, goal) pairs effectively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provides a comprehensive analysis of the differences between dynamic programming-based RL and SL-based RL through the lens of generalization, offering insights into their inherent properties.\n2. A novel augmentation method is proposed to make SL more generalizable."
                },
                "weaknesses": {
                    "value": "1. The proposed stitching generalization describes how \"far away\" different goals are in the sense of transitions, and the proposed method relies on a heuristic to describe that. This is basically what we aim to learn in TD learning and in most cases we cannot just get this kind of information for free. See Q1.\n2. The experiments are limited to Antmaze where the proposed clustering would probably work with raw state inputs, but if we imagine pixel inputs, it would be very hard to find some heuristic to make it work, as mentioned in 1.\n3. Missing baselines: Contrastive-based learning is also studied for goal-reaching problems as one kind of supervised learning problem, and it might also learn some representation more useful than the baseline included in the paper."
                },
                "questions": {
                    "value": "Q1: The optimal augmentation would be some metric that describes \"how \"far away\" different states are in the sense of transitions\" almost perfectly. Then such a metric will basically provide us with the optimal value function. In this sense, getting such a metric could be as hard as solving it with TD, so why does the proposed method necessarily \"close the gap\" between TD and SL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Reviewer_DGi6"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2040/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823741187,
            "cdate": 1698823741187,
            "tmdate": 1699636135676,
            "mdate": 1699636135676,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dSQvgy4DXU",
                "forum": "qg5JENs0N4",
                "replyto": "PBJebqXKe7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to DGi6"
                    },
                    "comment": {
                        "value": "Dear reviewer,\nWe thank the reviewer for their detailed feedback. It seems like the reviewer's main concerns regarding the assumption of a distance metric, especially in high dimensional states, and the inclusion of contrastive representation-based baselines. We address these concerns by adding two new experiments and updates:\n\n1) In Figures 9 and 10, in Appendix B, we perform an additional experiment with an image-based version of the point mazes. We show that performing data augmentation using k means on images performs much better than the baseline. Although we agree that finding a good distance metric is difficult, which we have already mentioned in the paper, our new results using images and the existing ant maze tasks with 29 dimensional state spaces, suggests that K-means with simple L2 distance can empirically still perform great.\n\n2) In Appendix D.2, we prove that contrastive RL will not learn Q values for states, goals that are used to test for stitching. \n\n3) In Figure 11, in Appendix B, we include a novel version of RvS which takes in input the contrastive representations of states [1]. Just like the reviewer suggested, we see that these representations are indeed able to improve the stitching abilities of OCBC methods. We believe that this approach is a promising direction for the future work that builds on our paper.  \n\nBelow, we will describe these updates and experiments in more detail and answer the specific reviewer questions **Do these answers, revisions and experiments address all the reviewer's concerns?**\n\n> The proposed stitching generalization describes how \"far away\" different goals\n\nThe information of how far away goals are from a particular state is not directly available in the distance metric. We do not make this assumption, and it is not even required. We only need a distance metric to make sense on very small length scales, that is it should identify states which are very close together. This is easier than describing how \u201cfar away\u201d goals are from a given state (Q value of reaching a goal). To actually learn to actually select actions that help you reach unseen goals, the temporal data augmentation we propose is important. \n\n> The experiments are limited to Antmaze\n\nTo incorporate your suggestion, we include two sets of experiments with the image based version of point maze tasks (umaze, medium and large). In figure 9 and 10, in appendix B, we see that applying clustering on images and using it for our data augmentation outperforms the baselines on all tasks. It also matches the performance of the state based augmentation in the large maze (Figure 10). \n\n> Missing baselines: Contrastive-based learning is also studied for goal-reaching problems as one kind of supervised learning problem,\n\nIn Appendix D.2, we add a proof (based on Lemma 4.1)  which shows that contrastive RL [1] will not estimate the Q function for state-goal pairs used to test for stitching. We will include the actual empirical results in the camera ready version, as we need more time to adapt their code (which is written in a different framework) to our settings.\n\n> it might also learn some representation more useful than the baseline included in the paper.\n\nAccording to your suggestions, we performed an ablation experiment which uses contrastive representations [1] as inputs to the OCBC policy on point maze tasks (umaze, medium and large). In Figure 11, in Appendix B,  we see that these representations are indeed able to improve the stitching abilities of OCBC algorithms. We have added Appendix B.3, which notes that learning contrastive representations for OCBC algorithms is a promising direction for future work.\n\n> The optimal augmentation would be some metric that describes \"how \"far away\" different states are in the sense of transitions\" almost perfectly. \n\nThis proposed metric could work well without our augmentation method, however it is more than what is needed to improve performance. We do need a distance metric to cluster close-by states. Intuitively, we want a distance metric that can classify states as very close or not. See for example, figure 2 where the distance metric does not tell us how far $\\tilde{g}$ is from $s$. It only tells us that $w$ and $g$ are close-by states.\n\n[1] Contrastive Learning As a Reinforcement Learning Algorithm"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163094907,
                "cdate": 1700163094907,
                "tmdate": 1700163094907,
                "mdate": 1700163094907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ikbO8vSpuB",
                "forum": "qg5JENs0N4",
                "replyto": "n8emlArOtU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Reviewer_DGi6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Reviewer_DGi6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the extra experiments and rebuttals! However I still have concerns about the distance metric: To do clustering as the paper suggests, it is required that the number of steps needed to transit from one state to another in the cluster is small, which means what metric should be used really depends on the underlying dynamics (i.e. the states could be close in the state space but hard to transit in between). Generally speaking, getting such information is not easy and it is hard to imagine some universal way to do it without any stitching, given that how to cluster those nearby states depends on the dynamics. So I will keep my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712352340,
                "cdate": 1700712352340,
                "tmdate": 1700712352340,
                "mdate": 1700712352340,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vRvjgb56WZ",
            "forum": "qg5JENs0N4",
            "replyto": "qg5JENs0N4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2040/Reviewer_dPJE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2040/Reviewer_dPJE"
            ],
            "content": {
                "summary": {
                    "value": "First, the paper identifies one important capability present in dynamic programming but lacking in supervised learning approaches to RL: the stitching property, which generalizes between different trajectories. - The second conceptual leap connects the stitching property to a certain kind of generalization. Finally, the authors propose to outfit data augmentation from supervised learning for reinforcement learning as a simple method to improve generalization. Experiments on two enviornments show that RvS with state and goal augmentation improve over RvS and DT. It is also claimed that more data did not help the DT improve, nor did increasing the number of layers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The theoretical foundation developed in this paper is interesting. There is high value in formalizing stitching, and hypothesizing on fundamental limits of certain algorithmic approaches that apply supervised learning to reinforcement learning problems. I also like the ethos of the approach, bringing data augmentation to RL for a specific purpose"
                },
                "weaknesses": {
                    "value": "- Some of the theoretical arguments fail to convince. I think there are a few details that seem inconsistent throughout the paper, such as the fact that data is not the fundamental limitation vs popular datasets not evaluating stitching because they include most (s,g) pairs.\n- The experimental evaluation seems limited overall, and somewhat preliminary. I usually do not put much weight on this kind of weakness, but I also am not comfortable with the theoretical positioning of the paper. Hence, I must put more weight on the experiments in my decision for this paper."
                },
                "questions": {
                    "value": "- Section 1 (Stitching as goal-state pairs): My understanding of the stitching property is that it is more general: given two partially disjoint trajectories that partially optimal, stitching via dynamic programming can combine the two partially optimal trajectories towards an optimal trajectory. Is your definition equivalent, is it a generalization or is it specific to the goal-directed setting?\n- Section 1 (Limited data vs generalization): You state that this is not a problem of limited data, because \" there can be (state, goal) pairs that are never visited in the same trajectory, despite being frequented in separate trajectories.\" In a way this is a source of limited data, if the \"data\" is a trajectory rather than an experience tuple. I think this is implicit in your argument, but it can be made explicit for clarity.\n- Section 4: (More formal \"Limited data vs generalization\"): I am still not sure how this is different from iid generalization, or how the lemma 4.1 makes this point. My understanding of lemma 4.1 is that there exists a context distribution for which the distribution of states induced by a collection of policies will never be equal to the BC policy. But this is because the policies collecting the data are conditioned on information not available to the BC policy. This induces something like partial observability, which is the source of the problem. Without the context, I do not think this would hold and thus stitching generaliation would be equivalent to iid generalzation. But the problem is, in a sense, limited data: the context is never accessible and hence the data (or information) is fundamentally limited.\n- Section 4 (stitching is not finding an optimal policy): I agree that stitching can help find an optimal policy, but this is besides the point in the context of your setting (offline RL). The question is two-fold, under what conditions: 1) can stitching help find a better policy and 2) does fidning a better policy necessitate stitching.\n- Section 5 (Nearby states) : how important is it that the states are mapped exactly onto the states in the set of experience? For example, you could imagine adding noise to all states and goals, which would therefore naturally stitch several nearby trajectories. Given the fact that you are assuming the space to have a distance metric, it is probably well-behaved so that this is effective.\n- Section 5 (conditions for evaluating stitching): While the first condition is easy to engineer into the problem, the second condition seems problematic. How can you know whether the BC policy has a non-zero probability besides running the experiment?\n- Section 6 (DT + aug?): One interesting approach that is missing is combining DT with data augmentation, seeing as DT is more performant than RvS. Is there any reason why this was left out? Is it computational concerns or a more fundamental limitation.\n- Section 6 (More data): This seems at odds with condition 1 under \"popular datasets do not evalute stitching\". Surely, if every state and goal pair were included in the dataset, then DT would improve?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2040/Reviewer_dPJE"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2040/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826616731,
            "cdate": 1698826616731,
            "tmdate": 1700535716674,
            "mdate": 1700535716674,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DSM1D6aai5",
                "forum": "qg5JENs0N4",
                "replyto": "vRvjgb56WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to dPJE (1 / 2)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\nWe thank the reviewer for their detailed feedback. It seems like the reviewer's main concerns are about the correctness of a theoretical claim (stitching vs generalization) and the limited experiments. We believe the theoretical results are correct (see below), and have revised Section 4, 5, and added a new proof in Appendix D.1 to clarify the results; we have also added new image-based experiments to bolster the empirical contributions of the paper. Below, we will describe these experiments in more detail and answer the specific reviewer questions **Do these answers, revisions and experiments address all the reviewer's concerns?**\n\n> Why is stitching a problem of generalizing to a different test distribution, and not a problem that occurs due to limited data from the training distribution?\n\nConsider two datasets collected via two different sources:\n\nSource 1 : Different data collecting policies $\\beta_h$, with probability $p(h)$.\n\nSource 2 : The BC policy $\\beta$.\n\nBoth these datasets induce the same discounted distribution over states (Lemma 3.1), but the induced distribution over trajectories (state - goal pairs) is different in general, even after collecting infinite data from both sources (Lemma 4.1). This can be observed from Figure 1, where there are 4 possible trajectories. The BC policy $\\beta$ induces a distribution that samples all 4 trajectories (Figure 1.c),. But the data collecting policies $\\beta_h$ induce a distribution over only 2 trajectories (Figure 1.b). This holds true even if  infinite data is collected from both sources. We agree that if we had access to the other 2 trajectories, then OCBC algorithms would be able to perform stitching generalization. Our claim is that these trajectories are out of training distribution, because we assume that the data is collected using Source 1. If we have access to all possible out-of-distribution trajectories, then of course generalization wouldn\u2019t be an issue. Moreover, the perspective that, \u201cif only you had data from the test distribution the problem would be solved, and hence it is a limited data problem\u201d is incorrect when the train and test distribution are different. Because you will never have data from the test distribution by definition. And since the test distribution for stitching is different from the train distribution (Lemma 4.1), stitching is a generalization problem.\n\n> I think there are a few details that seem inconsistent throughout the paper, such as the fact that data is not the fundamental limitation vs popular datasets not evaluating stitching because they include most (s,g) pairs.\n\nWe want to clarify that this is not an inconsistency.  In section 5, in para \u201cPopular offline datasets do not evaluate stitching\u201d we have added one sentence (in green color) to clarify this. The main claim of our paper is that:  In offline RL settings, when you have multiple data-collecting policies, even if you collect infinite data from these policies, generally there will be state-goal pairs that would never be sampled together. At test time, we need to evaluate the algorithm on exactly these state-goal pairs to evaluate stitching. The problem with the original D4RL datasets is that they evaluate on the same state-goal pairs, which are visited by the same data collection policy. See Figure 10 (D4RL) vs Figure 3 (ours) to see this difference. \n\n> experimental evaluation seems limited overall, and somewhat preliminary.\n\nWe have added an image-based version of the point maze environment which uses a top-down image of the maze instead of the low-level x,y coordinates (see Figure 9). Figure 9 reaffirms that OCBC algorithms are unable to stitch. And despite the high dimensionality, our data augmentation does perform stitching. We would like to add that our experiments on Ant locomotion tasks, which are very popular in offline RL literature [1,2,3], and are considered to be challenging (much harder than the D4RL\u2019s locomotion tasks).\n\n> Stitching as goal-state pairs : My understanding of the stitching property is that it is more general\n\nWe agree that the term stitching is used more generally. At the start of section 4, we have added a paragraph (green text) that talks about different properties which are associated with stitching. Our definition indeed covers all the existing trajectories that can be re-combined using dynamic programming. In the offline RL setting, dynamic programming methods compare all actions (be it from different trajectories) that were taken in a particular state. By definition, these actions come from the distribution of the BC policy. This is exactly the distribution used to test for stitching. Our definition can be trivially extended to the reward-case, by substituting state-returns pairs (similar to the Decision Transformer paper) instead of the state-goal pairs."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700162232909,
                "cdate": 1700162232909,
                "tmdate": 1700162897266,
                "mdate": 1700162897266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zxbkMKbL4l",
                "forum": "qg5JENs0N4",
                "replyto": "vRvjgb56WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Reviewer_dPJE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Reviewer_dPJE"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarification"
                    },
                    "comment": {
                        "value": "Both the changes to the paper and the answers to my questions clarify my misunderstandings. My new understanding is that stitching generalization is different from iid generalization because there is a latent structure in the (state,goal) pairs seen during training and those that are relevant for testing. Thus, the distribution is not identical nor independently distributed. The edge-case that I described, where all states and goals are present in the dataset, would not require stitiching generalization (and presumably, the supervised learning approaches would work here).\n\nI appreciate your reply. A few questions remain around the relevance of this problem setting, because the stitching property is not one that has been investigated before given the state of the very common offline RL benchmark (D4RL). These is value, however, in pointing out this property and I think this work need not answer that question. I will update my score (5 -> 6) to reflect this exchange."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535693270,
                "cdate": 1700535693270,
                "tmdate": 1700535732350,
                "mdate": 1700535732350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QvUvB3wtSH",
                "forum": "qg5JENs0N4",
                "replyto": "vRvjgb56WZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2040/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to dPJE"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and further discussion on the work.\n\nWe believe the \"relevance of this problem setting\" is very important to the RL, supervised learning, and planning communities.\n\n> A few questions remain around the relevance of this problem setting, because the stitching property is not one that has been investigated before given the state of the very common offline RL benchmark (D4RL). \n\nThe D4RL paper [1] has stated multiple times that testing for stitching is important (stitching is mentioned 6 times, See section 4,5 of [1] ). This raises an important question of when and whether stitching occurs. The first important contribution of our paper is to give a concrete definition of stitching as a form of generalization. Given this relation, we build on D4RL datasets to explicitly test for stitching. Indeed, our experiments reveal that OCBC algorithms which perform well on the original D4RL datasets [8], cannot perform stitching. We open source our code and the datasets needed to test for stitching. Arguably, stitching is the key property that helps offline RL algorithms find better solutions than the behavior policy. See appendix D.1, where we prove that the stitching is required to show one-step policy improvement over the behavior policy.\n\nGiven the recent advent of using large supervised learning transformer models for many problems in RL, it is important to ground these approaches. This has become a very popular and relevant topic in RL [2,3,4,5,6,7,8,9] as well as in SL [10,11,12,13]. Our work shows both theoretically and empirically, why OCBC algorithms, even with larger transformers and larger datasets will not perform stitching. Our concrete definition of stitching, and open source datasets will give future researchers a chance to improve OCBC algorithms. While our work shows that current OCBC algorithms do not stitch,  we also propose a simple data augmentation which outperforms the baselines on all tasks on state-based (Fig 4) as well as image-based tasks (Figure 9,10). **In the light of these contributions, we believe that many RL researchers at ICLR 2024 would find our work useful and interesting. We kindly ask the reviewer to reconsider the paper in light of the revisions and clarifications.**\n\n[1] D4RL: Datasets for Deep Data-Driven Reinforcement Learning\n\n[2] Imitating Past Successes can be Very Suboptimal\n\n[3] When does return-conditioned supervised learning work for offline reinforcement learning?\n\n[4] You can\u2019t count on luck: Why decision transformers and rvs fail in stochastic environments\n\n[5] Dichotomy of Control: Separating What You Can Control from What You Cannot\n\n[6] Upside-Down Reinforcement Learning Can Diverge in Stochastic Environments With Episodic Reset\n\n[7] Free from Bellman Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning\n\n[8] RvS: What is Essential for Offline RL via Supervised Learning?\n\n[9] Is Conditional Generative Modeling all you need for Decision-Making?\n\n[10] Faith and Fate: Limits of Transformers on Compositionality\n\n[11] Location attention for extrapolation to longer sequences\n\n[12] Unveiling transformers with LEGO: a synthetic reasoning task\n\n[13] NaturalProver: Grounded mathematical proof generation with language models."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2040/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579529840,
                "cdate": 1700579529840,
                "tmdate": 1700580163535,
                "mdate": 1700580163535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]