[
    {
        "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis"
    },
    {
        "review": {
            "id": "tE1mdCrUjm",
            "forum": "9JQtrumvg8",
            "replyto": "9JQtrumvg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1265/Reviewer_Jsxu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1265/Reviewer_Jsxu"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduced 1) a web-agent model that manipulates the web objects by human natural language instructions 2) a newly pretrained HTML-T5 model as a component in web-agent. \n\nThe experimental results show that 1) the web-agent, compared to solely using it's component Flan-U-Plam, is significantly better in a benchmark; and 2) the newly introduced HTML-T5 itself is outperforming existing HTML LLMs on web understanding tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall the reviewer found the experiments are well designed in supporting their claims of 1) the overall methods is much better than using a single LLM and 2) the HTML-T5 is an advance by itself. The most recent models are included in the experiments, and the evaluation datasets (mind2web and miniwob++) are used in training of both the proposed HTML-T5 and the baseline model long-T5. Therefore, the reviewer has no concerns of unfair comparisons."
                },
                "weaknesses": {
                    "value": "The presentation can be improved. Please consider revise the writing to avoid the questions below."
                },
                "questions": {
                    "value": "1. How are \"open ended actions\" (Figure 1), \"canonical sub-instructions\"(abstract), and \"pre-defined action space\" defined? Does the author promote the open-ended action space or pre-defined action space?\n\n2. In section 3.3, does \"given a few canonical examples for program generation\" describe the step of \"few-shot prompt' in Figure 3? Where do these examples come from?\n\n3. Could the author elaborate on the difference between two tasks in 4.1 and 4.2, except that they have different baseline models and datasets. What's the difference between input/output, etc. \n\n4. What's the definition of \"planning\" in Section 3.2. Is summarization referring to \"localizing\" the relavant snippet of the current sub-instruction?\n\n5. Could the author summarize the WebAgent workflow end-to-end. i.e. describe Figure 3 and explain the user input, system's knowledge/DB if exists, and the HTML-T5 to Flan-U-Plam is one-time action or interactive process. \n\n6. Could the author summarize the newly curated dataset that is used to pretrain/finetune part or entire WebAgent? E.g. template, sub-instruction, action examples\n\n7. In Table1, for example, the real-estate case, does the WebAgent see the same searching page but different instructions in the 20 tests?\n\n8. Is it correct that HTML-T5 is trained only for summarization, while the other models compared in Table 4 are multi-tasking.\n\n\nGlad to raise the score if the clarity will be significantly improved."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1265/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697679540214,
            "cdate": 1697679540214,
            "tmdate": 1699636053387,
            "mdate": 1699636053387,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EkGDd5t8Sj",
                "forum": "9JQtrumvg8",
                "replyto": "tE1mdCrUjm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Jsxu (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate your careful reading and detailed discussion of our paper. We address your concerns below and please let us know if there are remaining questions or unclear points.\n\n\n**> Questions 1**\n\nPre-defined action spaces are a-priori defined sets of all effective actions that an agent can perform. Our goal is to show that programs offer superior alternatives to pre-defined action spaces: They are more flexible (open-ended), and adaptable, allowing them to benefit from advancements in program synthesis. By \u201ccanonical sub-instructions\u201d, we refer to the rule-based parsing of instructions to sub-instructions. We revised the caption of Figure 1, deleted the \u201ccanonical\u201d word from the abstract to reduce ambiguity, and revised Section 3 to clarify.\n\n**> Questions 2**\n\nYes, few-shot prompting. We used generic examples, including selecting checkboxes, entering text into inputs, clicking on options, and scrolling, for few-shot prompting. These examples are independent of any domain in our study to ensure simplicity, robust generalization to unseen websites, and prevent any information leakage. We also clarified few-shot prompting in Section 3.3.\n\n\n**> Questions 3**\n\nInput is common across all tasks, encompassing user instructions, navigation history, and raw HTML documents. However, outputs vary depending on the specific task. In real-world website navigation (Section 4.1), outputs consist of paired sub-instructions and corresponding `data-ref` attributes associated with the relevant elements. Within MiniWoB, outputs are defined as pre-defined actions (e.g., \"click(ref=X)\") as outlined by the simulator. In Mind2Web, outputs manifest as multiple-choice questions (e.g., \u201cB\u201d) and operational labels (e.g., \u201cClick XXX\u201d). Section 4.1 focuses on interactive real-world evaluation, while Section 4.2 explores evaluation on the simulator and offline evaluation. We revised section 4.2 to clarify.\n\n\n\n\n**> Questions 4**\n\n*Planning* is predicting the next sub-instruction to be executed based on the current state. Rather than predicting all the sub-instructions in advance, HTML-T5 iteratively generates the next sub-instruction per step (i.e. closed-loop planning). *Summarization* is retrieving all the relevant HTML snippets by predicting their `data-ref` attributes, similar to extractive summarization. The combined sub-instruction and HTML snippets serve as the input for Flan-U-PaLM. We revised Section 3 to clarify.\n\n\n\n\n**> Questions 5**\n\nWe added the following paragraph to the beginning of Section 3 to explain the high-level workflow of WebAgent:\n\n\u201cUsers initiate natural language interactions with a clear intent, such as apartment searching. Upon receiving the initial request, HTML-T5 formulates a \u201cgo to<URL>\u201d sub-instruction, triggering Flan-U-PaLM to generate a corresponding Python program that navigates to the specified website.The raw HTML content of the newly opened page is extracted and fed into HTML-T5 along with the user\u2019s instruction and previous planning steps. This information is utilized to predict the next sub-instruction and identify relevant reference IDs for extractive HTML summarization. Flan-U-PaLM,in turn, generates a Python program based on these sub-instructions and the combined HTML snippet.This iterative process of planning, summarization, and program synthesis continues until a designated end-of-episode sub-instruction is predicted or the maximum number of iterations is reached.\u201d"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534999878,
                "cdate": 1700534999878,
                "tmdate": 1700551292428,
                "mdate": 1700551292428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9DdNoSu4Jl",
                "forum": "9JQtrumvg8",
                "replyto": "tE1mdCrUjm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Jsxu (2/2)"
                    },
                    "comment": {
                        "value": "**> Questions 6**\n\nWe revised Section 3 to clarify our pre-training (`Pre-Training with Mixture of Long-Span Denoising` in Section 3.1) and fine-tuning (Section 3.2) corpora. We also summarize our revision below.\n\nFor the **pre-training** dataset, we collect 100 WARC files (April 2019) from the CommonCrawl corpus and remove the non-Unicode or alphanumeric-only HTML documents. We then extract subtrees around <label> elements that have a special attribute called \u201cfor\u201d that associates the corresponding label with a unique input element in the same HTML document. This pre-processing step improves the quality of the pre-training corpus by focusing only on HTML that is relevant for instruction following and grounding. Our final dataset has 3.41M examples.\n\nFor the **finetuning** dataset, we sample instructions by randomly assigning values to placeholders in manually-curated templates. We employ a rule-based parser to decompose instructions into sequences of sub-instructions;  corresponding reference IDs are retrieved from HTML using regular expressions. At each step of the process, Flan-U-PaLM is provided with the sub-instruction and the associated HTML snippets to generate navigation programs that are executed through Selenium WebDriver. HTML-T5 is fine-tuned using self-experience demonstrations gathered through instruction sampling, scripted planning, and prompted program synthesis, as detailed earlier. It utilizes task instructions (e.g. please search 2 bedroom and 2+ bathroom houses in new york, ny with a max price of $7500 on real estate website), sub-instruction histories (e.g. go to real estate website,type in new york into search,click on search, click on price, click on max rent), and raw HTML as inputs. Subsequently, it generates the next sub-instruction (e.g. type in 7500 into max rent) and extracts the relevant `data-ref` attributes used for retrieving HTML snippets.\n\n\n\n\n**> Questions 7**\n\nThe agent navigates to the same home page but the underlying HTML document could be different. For example, home pages can show example listings that change between visits to the website.\n\n\n\n\n**> Questions 8**\n\nHTML-T5 is first pre-trained with the new HTML corpus that we curated from CommonCrawl (Section 3.1). Subsequently, it is finetuned on the entirety of MiniWoB's available demonstrations.\nFigure 4 compares the success rates on MiniWoB among finetuned methods based on different pre-trained models (LongT5, Flan-LongT5, HTML-T5, and Flan-T5). All the models in Table 4 are exclusively **finetuned** with 12K/347K MiniWoB demonstrations, rather than with multi-task corpora including other NLP tasks. We compare the efficacy of pre-trained models for simulated web automation tasks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535119395,
                "cdate": 1700535119395,
                "tmdate": 1700551224533,
                "mdate": 1700551224533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ml5WwcHPHG",
                "forum": "9JQtrumvg8",
                "replyto": "tE1mdCrUjm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please let us know if you have any further questions."
                    },
                    "comment": {
                        "value": "Dear Reviewer Jsxu,\n\nThank you again for your taking the time to review our paper. Do you have any further questions about the paper?\n\nPlease let us know if you have any further questions. We will try to address them before the discussion period ends.\n\nThank you!\n\nThe authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677343030,
                "cdate": 1700677343030,
                "tmdate": 1700677343030,
                "mdate": 1700677343030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tBq0fKvsxm",
            "forum": "9JQtrumvg8",
            "replyto": "9JQtrumvg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1265/Reviewer_eCh6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1265/Reviewer_eCh6"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a Web Agent that (1) decomposes natural language instructions into sub-instructions plan, (2) summarizes long HTML pages into task-relevant snippets (based on sub-instructions), and (3) acts on web pages by writing and executing Python programs with the Selenium WebDriver.\n\nWebAgent is based on two neural networks: HTML-T5 (introduced in this work) and Flan-U-PaLM.\nHTML-T5 is an encoder-decoder transformer trained on HTML documents from CommonCrawl with various long-range denoising objectives. The model is then fine-tuned on specific downstream tasks to predict a sub-instruction and a summary of the HTML page (data-ref HTML attributes?) given the natural language instruction, previous sub-instructions, and the raw HTML page.\n\nGiven the predicted sub-instruction and HTML snippet from HTML-T5, Flan-U-PaLM is then prompted to predict executable Python code that will perform the sub-instruction on a given web page.\n\nHTML-T5 is evaluated on MiniWoB++ and Mind2Web. Results show better performance than previous baselines.\nWebAgent is evaluated on WebSRC and instructions following on real websites based on task attributes successfully covered. Experiments show that the modular approach of WebAgent is beneficial compared to using only 1 language model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This work is making a significant contribution to the field by providing two models: one encoder-decoder that reads, understand and summarizes HTML pages: HTML-T5; and one WebAgent that combines the previous model with a code generation model (Flan-U-PaLM) to act and follow instructions on synthetic and real websites. \n\nSome notable strengths of the proposed architecture are:\n- To capture long-range dependencies in long documents, HTML-T5 uses both local and transient global attention similar to Long-T5. In addition it is pre-trained on various long-range denoising objectives.\n- To be able to execute actions on real websites, WebAgent produces executable Python code instead of discrete and non-generalizable HTML actions. This allows the agent to handle any action space present in real HTML pages instead of being limited to a set of fixed actions.\n\nExperimental results show that WebAgent is able to solve tasks in real websites."
                },
                "weaknesses": {
                    "value": "Overall, this is a strong paper, however, one weakness of this work is the lack of baselines to compare results against in real-world tasks. Table 1 provides good ablation study insights into the proposed WebAgent but there are no other Agents to compare to. Similarly in Table 3, HTML-T5 is only compared against MindAct on Mind2Web. Are there any other agents that could be used on this benchmark? \n\n---\n\nAnother weakness of this work is its clarity and ease of comprehension. Some aspects of the paper were not entirely clear, in particular how was HTML-T5 trained to predict sub-instruction plans and HTML summaries? What data supervision was used for that?\n\nSimilarly, it is not entirely clear what HTML-T5 produces: Figure-3 indicates \"HTML-snippets\", but the paper mentions multiple times that it \"summarizes\" HTML pages (so it should produce a summary?), and in Section 3.2 the paper states that it predicts ``_the corresponding data-ref attributes_''. If the model outputs only data reference IDs (like suggested also with Figure 6) then this is not summarization but more like information retrieval and the paper should reflect this. In addition, if object references are what is really being predicted, then it is not clear how Flan-U-PaLM make use of that information without having access to the raw HTML containing these objects.\n\nAnother confusion is the window size of HTML-T5: in Section 3.1 it is mentioned that the input sequence length of HTML-T5 is 4096, but in section 4.2 it uses 16k tokens for the context window. Which one is it? 16k tokens seems more likely overall since the model is supposed to take as input instruction, previous sub-instructions, and raw HTML. Just the raw HTML would overflow the 4096 context size as mentioned in the paper and illustrated by Figure 2. After reading 4096 in Sections 3.1, it was hard to understand how all inputs of HTML-T5 would fit in such a small window (especially after seeing Figure 2).\n\n---\n\nEventually, one important thing that the paper should discuss is the difference between train and test settings. It seems like WebAgent was trained on all domains individually. What precautions were made to ensure that the testing tasks do not overlap with the ones used during training?\n\n---\n\nMinor: some syntactic mistakes make the paper hard to read sometimes."
                },
                "questions": {
                    "value": "Mostly clarification questions related to weaknesses above:\n\n- What data was used to train HTML-T5 to predict sub-instruction plans and HTML summaries?\n\n- What is defined as a \"HTML summary\" and how is it used by Flan-U-PaLM?\n\n- How did the HTML-T5 inputs (instruction, previous sub-instructions, and raw HTML) fit into a window size of only 4098? The raw HTML would take up all the space.\n\n- How was the train/test split done to ensure no task (or even sub-task) overlap?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1265/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698287261951,
            "cdate": 1698287261951,
            "tmdate": 1699636053321,
            "mdate": 1699636053321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K7QQ2jXDR3",
                "forum": "9JQtrumvg8",
                "replyto": "tBq0fKvsxm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer eCh6"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful review and comments. Please let us know any remaining questions or concerns if you have.\n\n**> Weaknesses 1**\n\nWe have added new results to Appendix K (Figure 9) that compare Flan-U-PaLM-540B, publicly available gpt-3.5-turbo, and smaller model-size variants of the Flan-PaLM family of models (8B and 62B sized models). These models were tested on the map website using the same set of instructions. The results demonstrate that:\n\n- Flan-U-PaLM-540B and gpt-3.5-turbo exhibit comparable performance (80% success, 93.8% score).\n- Flan-PaLM-62B (60% success, 86.3% score) falls short of Flan-U-PaLM-540B due to inferior program synthesis capabilities.\n- Flan-PaLM-8B was unable to generate valid programs.\n\nOur findings indicate that increasing model size enhances WebAgent performance, and that any LLM capable of generating python/selenium code can be integrated into WebAgent. \n\nFor Mind2Web, we included the results from a recent concurrent work (Synapse [1]) in Table 3. HTML-T5 still compares favorably and achieves the best result.\n\n**> Weaknesses 2 & Questions 1**\n\nWe have made the following revisions to Section 3 to improve clarity:\n\n**Explanation of the WebAgent Workflow:** We introduced a new paragraph at the beginning of Section 3 to explain the high-level workflow of WebAgent, including user-WebAgent interaction and planning, summarization, and program synthesis components.\n\n\n**Revised Section 3.1:** We revised Section 3.1 to clarify architectural details of HTML-T5 as well as the pre-training corpus and objectives.\n\n\n**Revised Section 3.2:** We elaborated on the self-experience supervision approach, which involves sampling new instructions using templates, curating navigation scripts to gather planning (sequence of sub-instructions) and summarization data (corresponding reference IDs), prompting Flan-U-PaLM to generate Python programs, and utilizing execution feedback to eliminate incorrect trajectories. Furthermore, we provided a detailed explanation of HTML-T5 fine-tuning, which employs demonstrations collected through scripted planning and prompted programming to train HTML-T5 for automated planning and summarization tasks.\n\nWe collect 260 episodes on real-estate, 230 episodes on social-media, and 410 episodes on map websites (explained in `Evaluation Methodology` in Section 4.1). Examples for different tasks are illustrated in **Appendix D**. Please let us know if you have further unclear points.\n\n\n**> Weaknesses 3 & Questions 2**\n\nWe revised Section 3.2 to clarify fine-tuning of HTML-T5 and prompting of Flan-U-PaLM. HTML-T5 is fine-tuned to predict sub-instructions and corresponding `data-ref` attributes directly from raw HTML documents. HTML snippets that correspond to these \u201cdata-ref\u201d attributes are extracted and merged, similar to how extractive summarization [2] or retrieval works. We feed merged HTML snippets to prompt Flan-U-PaLM to generate navigation programs as depicted in Figure 3. Please let us know if you have further unclear points.\n\n\n**> Weaknesses 4 & Questions 3**\n\nOur preliminary analysis has shown that approximately 90% of our pre-training HTML corpus has around 4K context length (see Figure 5). While raw HTML documents can be longer, our pre-processing methodology extracts useful subtrees for pre-training; substantially reducing context length (see Appendix C) while improving effectiveness (see Table 7 for an ablation). That is why we **pre-train** HTML-T5 using 4096 window size (`Pre-Training with Mixture of Long-Span Denoising` in Section 3.1), but finetune it with 16K window size (Section 4.2) to generalize to real-world HTML documents. \n\n\n\n\n**> Weaknesses 5 & Questions 4**\n\nIn the context of real-world navigation, we constructed a single dataset of instructions. This dataset was carefully partitioned into training and testing sets to guarantee no overlap. All instructions included in the testing set are provided in Appendix F. For both MiniWoB and Mind2Web, we adopted the experimental setup established by their respective authors. In MiniWoB, instructions are randomly generated from a vast pool of instructions, and environments accept an argument for either \"training\" or \"testing.\" For Min2Web, training and testing sets are maintained separately.\n\n\n**> Weaknesses 6**\n\nThank you for pointing out the syntactic mistakes. We updated our manuscripts and fixed the issues we found.\n\n```\n[1] Deng et al., (2023) Mind2Web: Towards a Generalist Agent for the Web (https://arxiv.org/abs/2306.06070)\n[2] Xiao and Carenini (2019) Extractive Summarization of Long Documents by Combining Global and Local Context  (https://arxiv.org/abs/1909.08089)\n[3] Guo et al., (2021) LongT5: Efficient Text-To-Text Transformer for Long Sequences (https://arxiv.org/abs/2112.07916)\n```"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534725995,
                "cdate": 1700534725995,
                "tmdate": 1700551330777,
                "mdate": 1700551330777,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ayVZJb7tLE",
                "forum": "9JQtrumvg8",
                "replyto": "K7QQ2jXDR3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Reviewer_eCh6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Reviewer_eCh6"
                ],
                "content": {
                    "title": {
                        "value": "thanks for the clarifications"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to clarify my questions and to update the manuscript, it is more clear now."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684018082,
                "cdate": 1700684018082,
                "tmdate": 1700684018082,
                "mdate": 1700684018082,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hzzmAausZD",
            "forum": "9JQtrumvg8",
            "replyto": "9JQtrumvg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1265/Reviewer_ASiH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1265/Reviewer_ASiH"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new LLM-based agent for web-based tasks which achieves state of the art on Mind2Web.\nThe proposed method combines two LLMs into one agent, HTML-T5 which is a new pretrained model and is further finetuned for planning and summarization, and Flan-U-PaLM which is a frozen model and generates programs to allow the model to interact with web environments."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The model's usage of HTML-T5 for planning and summarization is effective and novel, and the overall performance is good. Especially on Mind2Web, it significantly pushes the upper bound of performance."
                },
                "weaknesses": {
                    "value": "Because the model relies on Flan-U-PaLM with 540B parameters, it's difficult to judge how reliant the method is on the ability of this particular model to generate executable code.\n\nThe organization of the paper could be improved, including more details about how feedback was acquired and finetuning was done to enable planning and summarization (i.e. Fig 6 in appendix)"
                },
                "questions": {
                    "value": "- There are some missing recent baselines for miniwob++ [1]. These methods report that the task performance is near human (93%). Could you provide more information about the performance of the proposed method (which is a bit lower) in this context?\n\n- Is it possible to report results using models other than Flan-U-PaLM with 540B parameters?\n\n- Will HTML-T5 be released?\n\n[1] SYNAPSE: Trajectory-as-Exemplar Prompting with Memory for Computer Control. Zheng et al., arxiv 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1265/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1265/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1265/Reviewer_ASiH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1265/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698868587174,
            "cdate": 1698868587174,
            "tmdate": 1700645155477,
            "mdate": 1700645155477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4izZR6GcZn",
                "forum": "9JQtrumvg8",
                "replyto": "hzzmAausZD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer ASiH"
                    },
                    "comment": {
                        "value": "We appreciate the careful reading and thoughtful comments. We address your concerns below, and please let us know if there are remaining questions or unclear points.\n\n\n**> Weaknesses 1 & Questions 2**\n\nWe have added new results to Appendix K (Figure 9) that compare Flan-U-PaLM-540B, publicly available gpt-3.5-turbo, and smaller model-size variants of the Flan-PaLM family of models (8B and 62B sized models). These models were tested on the map website using the same set of instructions. The results demonstrate that:\n\n- Flan-U-PaLM-540B and gpt-3.5-turbo exhibit comparable performance (80% success, 93.8% score).\n- Flan-PaLM-62B (60% success, 86.3% score) falls short of Flan-U-PaLM-540B due to inferior program synthesis capabilities.\n- Flan-PaLM-8B was unable to generate valid programs.\n\nOur findings indicate that increasing model size enhances WebAgent performance, and that any LLM capable of generating python/selenium code can be integrated into WebAgent. \n\n\n**> Weaknesses 2**\n\nWe have made the following revisions to Section 3 to improve clarity:\n\n**Explanation of the WebAgent Workflow:** We introduced a new paragraph at the beginning of Section 3 to explain the high-level workflow of WebAgent, including user-WebAgent interaction and planning, summarization, and program synthesis components.\n\n\n**Revised Section 3.1:** We revised Section 3.1 to clarify architectural details of HTML-T5 as well as the pre-training corpus and objectives.\n\n\n**Revised Section 3.2:** We elaborated on the self-experience supervision approach, which involves sampling new instructions using templates, curating navigation scripts to gather planning (sequence of sub-instructions) and summarization data (corresponding reference IDs), prompting Flan-U-PaLM to generate Python programs, and utilizing execution feedback to eliminate incorrect trajectories. Furthermore, we provided a detailed explanation of HTML-T5 fine-tuning, which employs demonstrations collected through scripted planning and prompted programming to train HTML-T5 for automated planning and summarization tasks.\n\n\n**Revised Section 3.3:** We clarified the few-shot prompting.\n\nWe collect 260 episodes on real-estate, 230 episodes on social-media, and 410 episodes on map websites (explained in `Evaluation Methodology` in Section 4.1). Examples for different tasks are illustrated in **Appendix D**. Please let us know if you have further unclear points.\n\n\n**> Questions 1**\n\nTo provide a more comprehensive comparison, we have added a state-of-the-art (SoTA) baseline utilizing GPT-3.5 (Synapse [2]) in Table 4. However, we emphasize that our primary focus is on comparing models that are readily accessible, disclose their training corpus, and adhere to the same training dataset size (12K or 347K). While GPT variants demonstrate impressive performance on MiniWoB, direct comparisons are hindered by the lack of transparency regarding their training corpora. In this context, HTML-T5 emerges as a promising step towards on-device and privacy-preserving deployment.\n\n\n**> Questions 3**\n\nWe have explained the details of HTML-T5, including its architecture, training process, and data preprocessing steps. HTML-T5 utilizes publicly available T5 models and is trained on the CommonCrawl corpus. While PaLM models are not open-sourced, we intend to release HTML-T5 after thoroughly evaluating its potential societal implications.\n\n```\n[1] OpenAI (2023) GPT-4 Technical Report (https://arxiv.org/abs/2303.08774)\n[2] Zheng et al., (2023) Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control (https://arxiv.org/abs/2306.07863)\n```"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534435708,
                "cdate": 1700534435708,
                "tmdate": 1700534514091,
                "mdate": 1700534514091,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9NJ0BD8PL5",
                "forum": "9JQtrumvg8",
                "replyto": "4izZR6GcZn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Reviewer_ASiH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Reviewer_ASiH"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' detailed response and revisions. The revision addresses most of my concerns, and I have raised my rating to accept."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645116096,
                "cdate": 1700645116096,
                "tmdate": 1700645116096,
                "mdate": 1700645116096,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MujrL4V6CP",
            "forum": "9JQtrumvg8",
            "replyto": "9JQtrumvg8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1265/Reviewer_Gbpj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1265/Reviewer_Gbpj"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"WebAgent,\" an autonomous agent driven by large language models (LLMs) that completes navigation tasks on real websites by following user instructions and combining canonical web actions in a program space. WebAgent's capabilities are outlined as follows:\n\n---\n\nPlanning Sub-Instructions Per Step: It decomposes natural language instructions into sub-instructions, planning out the steps needed to complete a task.\n\nSummarizing Long HTML Pages: It can summarize lengthy HTML pages into snippets that are relevant to the task at hand, based on the sub-instructions derived from the user's commands.\n\nActing via Programming: It grounds sub-instructions and HTML snippets into executable Python codes, allowing it to interact with real websites programmatically.\n\n\n---\nTo form WebAgent, two LLMs are combined:\n\nFlan-U-PaLM: Used for grounded code generation. This model provides the agent with the ability to generate code snippets that can interact with web pages.\n\n\nHTML-T5: Used for task planning and conditional HTML summarization. This model has an encoder-decoder architecture and is specialized in capturing the structure, syntax, and semantics of long HTML pages. It  incorporates local and possibly global attention mechanisms to better process the structure of HTML documents.\n\n---"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper has several strengths:\n\n----\n1. Unlike prior works, there is a focus on real world application. Demonstrating success in real-world web navigation tasks provides a strong case for the practical application of this research. This has implications for the usability and deployment of AI systems in everyday tasks.\n\n----\n\n\n2. The collaborative approach, where different models work together to complete tasks, showcases a novel use of ensemble techniques in a practical setting, which encourages more research in model collaboration. There is also additional benefits of such a modular approach, in that scalability and error analysis becomes easier. The use of an ensemble of specialized models to address specific aspects of the problem space, is a departure from the trend of using a single generalist model for all tasks.This specialization can lead to performance improvements and more efficient computation."
                },
                "weaknesses": {
                    "value": "1. Especially for this kind of work, the broader impacts section should be in the main text and should be fully fleshed out. This is a significant weakness in this work.\n\n-----\n\n2. It would be good to have a baseline comparison comparing what performance looks like with model scale. Flan-U-PaLM is a 540B parameter model which puts it at a scale inaccessible to many researchers.. it would be good to benchmark how this approach scales from small accessible open source models, to the large ones used in this work.\n\n----"
                },
                "questions": {
                    "value": "Does Webagent replan after failures? How does it handle failures?\n\nRelated to a mentioned weakness, how does this approach scale? would it just perform better with more data, parameters and compute?\n\nAre all the components of web-agent available open-source and will web-agent be open-sourced?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This work builds automated bots to interact on the web. This is important work but it should have a fully fleshed out broader impacts section."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1265/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699034230973,
            "cdate": 1699034230973,
            "tmdate": 1699636053149,
            "mdate": 1699636053149,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c2WhjeQYlC",
                "forum": "9JQtrumvg8",
                "replyto": "MujrL4V6CP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Gbpj"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the careful reading and constructive feedback. We address your concerns below. Please let us know if you have further questions.\n\n\n**> Weaknesses 1**\n\nFollowing ICLR 2024 Author Guide (https://iclr.cc/Conferences/2024/AuthorGuide), we added the ethics statement section between the main text and reference. We discussed that a careful treatment of the subject from the researchers, policymakers and industries to form guidelines and regulations to prevent misuse of web automation from negatively impacting real users is needed. Please check page 10 in the revised paper.\n\n\n\n\n**> Weaknesses 2**\n\nWe have added new results to Appendix K (Figure 9) that compare Flan-U-PaLM-540B, publicly available gpt-3.5-turbo, and smaller model-size variants of the Flan-PaLM family of models (8B and 62B sized models). These models were tested on the map website using the same set of instructions. The results demonstrate that:\n\n\n- Flan-U-PaLM-540B and gpt-3.5-turbo exhibit comparable performance (80% success, 93.8% score).\n- Flan-PaLM-62B (60% success, 86.3% score) falls short of Flan-U-PaLM-540B due to inferior program synthesis capabilities.\n- Flan-PaLM-8B was unable to generate valid programs.\n\n\nOur findings indicate that increasing model size enhances WebAgent performance, and that any LLM capable of generating python/selenium code can be integrated into WebAgent.\n\n\n\n\n**> Questions 1**\n\nOur planner, HTML-T5, is trained to automatically replan whenever a new state is observed. By doing so, the agent will react more quickly when an unusual transition is encountered. For example, in case an error causes the state to go to the home screen before finishing apartment search, the agent will immediately start replanning from the first step.\n\n\n\n\n**> Questions 2**\n\nOur results show that the performance of WebAgent would improve with scale. Model-size ablation of HTML-T5 shows that if we increase the number of parameters from 220M to 3B, the performance gets better (Table 8 in Appendix H). In Table 4, we also show that increasing the data size from 12K to 347K improves the performance. We also briefly explain these points in Section 5 (`Broad Generalization across the Internet`). \n\n\n**> Questions 3**\n\nWe have explained the details of HTML-T5, including its architecture, training process, and data preprocessing steps. HTML-T5 utilizes publicly available T5 models and is trained on the CommonCrawl corpus. While PaLM models are not open-sourced, we intend to release HTML-T5 after thoroughly evaluating its potential societal implications."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534235761,
                "cdate": 1700534235761,
                "tmdate": 1700534235761,
                "mdate": 1700534235761,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ChGEjGUSrt",
                "forum": "9JQtrumvg8",
                "replyto": "MujrL4V6CP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1265/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please let us know if you have any further questions."
                    },
                    "comment": {
                        "value": "Dear Reviewer Gbpj,\n\nThank you again for your taking the time to review our paper. Do you have any further questions about the paper?\n\nPlease let us know if you have any further questions. We will try to address them before the discussion period ends.\n\nThank you!\n\nThe authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1265/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677384078,
                "cdate": 1700677384078,
                "tmdate": 1700677384078,
                "mdate": 1700677384078,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]