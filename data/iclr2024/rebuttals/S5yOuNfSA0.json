[
    {
        "title": "Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP"
    },
    {
        "review": {
            "id": "vZm0tdcxd2",
            "forum": "S5yOuNfSA0",
            "replyto": "S5yOuNfSA0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5970/Reviewer_nFDE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5970/Reviewer_nFDE"
            ],
            "content": {
                "summary": {
                    "value": "This paper theoretically examines transferable representation learning of CLIP. The analysis reveals that with a near-optimal network trained on the data, features from different modalities align, allowing for zero-shot learning when appropriate prompts are used. The paper also demonstrates that contrastive learning with sparse features can lead to unexpected positive pairs, emphasizing the need for careful consideration. Building on these general theoretical findings, the authors provide deeper insights into specific cases, illustrating how multi-modal learning aligns different features and how CLIP's learned features outperform those obtained through naive square loss. To validate their theoretical predictions, the authors conduct experiments on real data. Additionally, inspired by their theoretical findings, they propose a novel regularization technique for CLIP, effectively improving zero-shot performance across various tasks, as confirmed by empirical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and flows smoothly, making it relatively easy for readers to understand. \n2. This article focuses on what's behind the explosive effectiveness of CLIP, and the paper attempts to delve into the principles underlying CLIP, demonstrating a certain level of originality and innovation."
                },
                "weaknesses": {
                    "value": "1. More relevant experimental results are expected, such as results from a wider range of downstream tasks.\n2. The article exclusively analyzes and experiments with CLIP, without thoroughly exploring the applicability of this new methods to other relevant contrastive learning approaches."
                },
                "questions": {
                    "value": "Can one simply replace the CLIP component in all CLIP-related work with CLIP+Reg to achieve a better performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723724058,
            "cdate": 1698723724058,
            "tmdate": 1699636638150,
            "mdate": 1699636638150,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5AU8igySNy",
                "forum": "S5yOuNfSA0",
                "replyto": "vZm0tdcxd2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nFDE"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments!\n\n**Q1**: More relevant experimental results are expected, such as results from a wider range of downstream tasks.\n  \n**A1**: Thank you for your suggestion. To add more experiments on downstream tasks, we have added experiments on retrieval accordingly and summarized the zero-shot results in table below, including Flickr-30k (1k) and MSCOCO (5k) following the CyCLIP paper [1]. As also noted in [1], text retrieval per image is easier than image retrieval per caption, due to the nature of both datasets containing 5 captions for each image. The trend of our results align with that reported in [1]. Lastly, while CyCLIP does not provide consistent improvement over CLIP (in our experiment setting as well as in their reported results), our regularization term provides consistent improvement to both CLIP objective and CyCLIP objective. This has also been updated in our Appendix C.1.\n\nFlickr-30k (1k)\n|  | Text R@1 | Text R@5 | Text R@10 | Image R@1 | Image R@5 | Image R@10 | Average\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CLIP | 87.36 | 93.0 | 95.18 | 26.88 | 54.18 | 66.22 | 70.47 |\n| CLIP+Reg | **87.42** | **93.42** | **95.82** | **29.94** | **58.00** | **69.82** | **72.40** |\n| CyCLIP | 87.34 | 93.12 | 95.04 | 29.00 | 56.50 | 67.62 | 71.44 |\n| CyCLIP+Reg | 87.20 | 93.20 | 95.56 | 29.14 | 56.94 | 68.64 | 71.78 | \n\nMSCOCO (5k)\n|  | Text R@1 | Text R@5 | Text R@10 | Image R@1 | Image R@5 | Image R@10 | Average\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CLIP | 81.19 | 83.21 | 84.42 | 4.73 | 11.66 | 15.93 | 46.86 |\n| CLIP+Reg | **81.25** | **83.31** | 84.49 | 4.98 | 12.14 | 16.66 | 47.14 |\n| CyCLIP | 81.06 | 82.92 | 84.28 | 4.70 | 11.66 | 15.93 | 46.86 |\n| CyCLIP+Reg | 81.31 | 83.28 | **84.65** | **5.27** | **12.17** | **16.70** |**47.23** | \n\n---\n\n**Q2**: The article exclusively analyzes and experiments with CLIP, without thoroughly exploring the applicability of this new methods to other relevant contrastive learning approaches.\n\n**A2**: Since we are the first paper that theoretically studies the transferable representation learning of CLIP,  the primary focus of our paper was an in-depth analysis of the CLIP model. Due to the space limit, we only consider the vanilla CLIP model in our theory and, additionally, the CyCLIP [1] model in the experiment. We acknowledge the importance of exploring other contrastive learning approaches and consider this an important direction for future research.\n\n---\n\n**Q3**: Can one simply replace the CLIP component in all CLIP-related work with CLIP+Reg to achieve a better performance?\n\n**A3**: We think so. Replacing the CLIP component with CLIP+Reg in various CLIP-related projects is feasible and can potentially lead to improved performance. In principle, our regularization is quite simple and can improve the alignment between image-text pairs, as demonstrated by the above experiment results. However, achieving a successful replacement might necessitate additional effort in exploring various related works to fully leverage the benefits of our regularization.\n\n---\n\n[1] Goel et al. \"Cyclip: Cyclic contrastive language-image pretraining.\" Advances in Neural Information Processing Systems 35 (2022): 6704-6719."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434154530,
                "cdate": 1700434154530,
                "tmdate": 1700434154530,
                "mdate": 1700434154530,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pwVO1xjP6U",
            "forum": "S5yOuNfSA0",
            "replyto": "S5yOuNfSA0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5970/Reviewer_CWhP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5970/Reviewer_CWhP"
            ],
            "content": {
                "summary": {
                    "value": "This paper offers theoretical analysis of the underlying principles of CLIP, shedding light on why CLIP exhibits robust transferability. Additionally, the paper introduces a novel regularization technique designed to enhance the performance of CLIP."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written and novel. This paper offers a robust analysis, including mathematical proofs, of CLIP. These contributions greatly contribute to our understanding of CLIP."
                },
                "weaknesses": {
                    "value": "1. The paper's primary focus appears to be on CLIP's zero-shot transferability. While this is undoubtedly a significant aspect, it's worth considering that CLIP's robust zero-shot performance results from a strong semantic space based on extensive vision-semantic data. Therefore, an exploration of the visual-semantic alignment aspect could be an intriguing avenue for further investigation.\n\n2. In introduction section, the author cites \"blue sky\" and \"white cloud\" as examples of unique features. However, these instances might be seen as special cases.  As CLIP is based on a large amount of vision-semantic data, it's possible that the missing elements could appear in various other captions. Therefore, I question the significance of this problem. To address this concern, the author may need to conduct overall statistics on the data. Furthermore, the term 'unique features' could benefit from a more precise definition or explanation.\n\n3.  Some notations and definitions in the paper can be challenging to follow. For instance, the terms 'one-to-one mapping' or 'one-to-one matching' could benefit from clearer explanations for readers.\n\n4. Expanding the range of experiments to include various downstream tasks, rather than solely focusing on zero-shot and Linear probing, would provide a more comprehensive assessment of the paper's proposed methods and their practical applications."
                },
                "questions": {
                    "value": "See `Weakness' above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761045763,
            "cdate": 1698761045763,
            "tmdate": 1699636638043,
            "mdate": 1699636638043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c5IniYNDJf",
                "forum": "S5yOuNfSA0",
                "replyto": "pwVO1xjP6U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CWhP (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your helpful and detailed comments! \n\n**Q1**: The paper's primary focus appears to be on CLIP's zero-shot transferability. While this is undoubtedly a significant aspect, it's worth considering that CLIP's robust zero-shot performance results from a strong semantic space based on extensive vision-semantic data. Therefore, an exploration of the visual-semantic alignment aspect could be an intriguing avenue for further investigation.\n\n**A1**: Thank you for your valuable suggestion.  While our primary focus is CLIP\u2019s zero-shot transferability, we indeed considered the visual-semantic alignment in our analysis. As noted in Remark 4.3, image-text pairs sharing the same topic tend to score higher, indicating a visual-semantic alignment in CLIP's learned representations. Furthermore, the margin discussed in our paper can be considered a useful metric for studying this alignment. We've expanded on this in Appendix A for further clarification.\n\n---\n\n**Q2**: In introduction section, the author cites \"blue sky\" and \"white cloud\" as examples of unique features. However, these instances might be seen as special cases. As CLIP is based on a large amount of vision-semantic data, it's possible that the missing elements could appear in various other captions. Therefore, I question the significance of this problem. \n\n**A2**: In the introduction section, we presented an example to illustrate the existence of such an issue. While CLIP is trained on extensive image-caption datasets, each image is paired with only a single, brief caption. It is crucial to note that CLIP's pre-training data are web-sourced image-caption pairs, characterized by their succinct and often non-elaborate nature, as these captions are not meticulously crafted by annotators. Consequently, instances where captions do not comprehensively describe the corresponding image are common in such datasets. \n\nWhile the ground truth object list is not available for CC3M, we can first consider the MSCOCO dataset that has a ground truth object list. Analyzing the 410,600 image-caption pairs from its training data, we identified 330,843 pairs wherein the caption failed to include at least one object based on exact matching criteria. This amounts to **80.58%** of the dataset. We have included the distribution bar plot in Appendix C.3. \n\nWe also provide several examples of image captions, along with the ground truth objects in the image.  We highlight the objects that are missed from the caption.\n- Caption: \u201cA restaurant has modern wooden tables and chairs.\u201d\n- Objects: [**potted plant**, dining table, **book**, **vase**, chair] \n\n- Caption: \u201cA bicycle parked in a kitchen with a stove and cabinets.\u201d\n- Objects: [bicycle, **bottle**, **cup**, **toaster**, **sink**, **spoon**, **bowl**, **oven**]\n\n- Caption: \u201cA person is cutting a roast with a fork and knife.\u201d\n- Objects: [person, **oven**, knife]\n\nAs shown above, humans do not extraneously identify all conceivable objects in an image when trying to describe it, let alone the background information such as the scene. If the reviewer browses through some examples of MSCOCO and CC3M, it will be a frequent case that the captions cannot fully depict the image details. In Appendix C.3, we further provide several image-caption examples from CC3M."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433987286,
                "cdate": 1700433987286,
                "tmdate": 1700433987286,
                "mdate": 1700433987286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tYTx2iPtoA",
                "forum": "S5yOuNfSA0",
                "replyto": "pwVO1xjP6U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CWhP (2/2)"
                    },
                    "comment": {
                        "value": "**Q3**: Some notations and definitions in the paper can be challenging to follow. ('one-to-one mapping' or 'one-to-one matching') \n\n**A3**: Thank you for the valuable suggestion, we have unified the terminology as one-to-one mapping.\n\n---\n\n**Q4**: Expanding the range of experiments to include various downstream tasks, rather than solely focusing on zero-shot and Linear probing\u2026\n\n**A4**: Our research concentrates on the zero-shot transfer capabilities in the CLIP. This focus aligns with the core tasks (zero-shot and linear probing) emphasized in the original CLIP paper and its subsequent developments, such as FILIP [1] and CyCLIP [4]. To add more experiments on downstream tasks, we have added experiments on retrieval accordingly and summarized the zero-shot results in table below, including Flickr-30k (1k) and MSCOCO (5k) following [4]. As also noted in [4], text retrieval per image is easier than image retrieval per caption, due to the nature of both datasets containing 5 captions for each image. The trend of our results align with that reported in [4]. Lastly, while CyCLIP does not provide consistent improvement over CLIP (in our experiment setting as well as in their reported results), our regularization term provides consistent improvement to both CLIP objective and CyCLIP objective. This has also been updated in our Appendix C.1.\n\nFlickr-30k (1k)\n|  | Text R@1 | Text R@5 | Text R@10 | Image R@1 | Image R@5 | Image R@10 | Average\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CLIP | 87.36 | 93.0 | 95.18 | 26.88 | 54.18 | 66.22 | 70.47 |\n| CLIP+Reg | **87.42** | **93.42** | **95.82** | **29.94** | **58.00** | **69.82** | **72.40** |\n| CyCLIP | 87.34 | 93.12 | 95.04 | 29.00 | 56.50 | 67.62 | 71.44 |\n| CyCLIP+Reg | 87.20 | 93.20 | 95.56 | 29.14 | 56.94 | 68.64 | 71.78 | \n\nMSCOCO (5k)\n|  | Text R@1 | Text R@5 | Text R@10 | Image R@1 | Image R@5 | Image R@10 | Average\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CLIP | 81.19 | 83.21 | 84.42 | 4.73 | 11.66 | 15.93 | 46.86 |\n| CLIP+Reg | **81.25** | **83.31** | 84.49 | 4.98 | 12.14 | 16.66 | 47.14 |\n| CyCLIP | 81.06 | 82.92 | 84.28 | 4.70 | 11.66 | 15.93 | 46.86 |\n| CyCLIP+Reg | 81.31 | 83.28 | **84.65** | **5.27** | **12.17** | **16.70** |**47.23** | \n\n---\n\n[1] Yao, Lewei, et al. \"FILIP: Fine-grained Interactive Language-Image Pretraining.\" International Conference on Learning Representations. 2021.\n\n[2] Li et al. \"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pretraining Paradigm.\" International Conference on Learning Representations. 2021. \n\n[3] Mu et al. \"Slip: Self-supervision meets language-image pre-training.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. \n\n[4] Goel et al. \"Cyclip: Cyclic contrastive language-image pretraining.\" Advances in Neural Information Processing Systems 35 (2022): 6704-6719."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434051030,
                "cdate": 1700434051030,
                "tmdate": 1700434051030,
                "mdate": 1700434051030,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vjDwBUQype",
                "forum": "S5yOuNfSA0",
                "replyto": "pwVO1xjP6U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up with Reviewer CWhP"
                    },
                    "comment": {
                        "value": "Dear Reviewer CWhP, \n\nThank you again for your insightful comments. We are following up to engage in further discussion and address any outstanding questions you might still have. If our rebuttal and the revisions to our paper have satisfactorily addressed your concerns, we respectfully hope that you could reconsider your assessment of our work."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605035965,
                "cdate": 1700605035965,
                "tmdate": 1700605035965,
                "mdate": 1700605035965,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iegnthFBIe",
            "forum": "S5yOuNfSA0",
            "replyto": "S5yOuNfSA0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5970/Reviewer_7qr5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5970/Reviewer_7qr5"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on providing theoretical support for the CLIP training and its zero-shot transferability. The main claim is that the contrastive learning objective in CLIP may not cover all the positive pairs, e.g., some features in an image may not be present in its corresponding captions. In section 3, the authors show that the empirical loss converges to the true loss when the number of batches is large enough. In section 4, they show that the learned similarity score f_hat between negative pairs is smaller than the score between positive pairs given that there exists a score function f* such that this relation holds. Based on such assumption, in section 5, they conclude that a trained CLIP model can achieve small top-r error and this generalizes to different distributions as the distribution shift is bounded. Based on the prior assumption and derivations, they have three claims: 1) Margin depends on the temperature tau, 2) We should only regularize positive pairs instead of both positive and negative pairs, 3) With sufficiently small tau, we can find a f_hat with large margin. They test these claims with experiments on CC3M."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper provides theoretical bounds on CLIP training and its zero-shot transferability."
                },
                "weaknesses": {
                    "value": "1. Contrastive learning has been widely studied in the community with several variants of NCE loss, with different ways to regularize positive and negative pairs to improve the margins (e.g., [a]). The behavior of temperature in contrastive learning was also studied (e.g., [b]), and so was regularization (e.g., [c]). It is not surprising that adding the regularization of the distance between positive pairs can improve the performance. Also, how does the proposed solution compare to those methods?  \n\n2. The authors propose only to regularize the distance between positive pairs, but there is no ablation comparison to the variants that regularize both or negative pairs only.\n\n3. The introduction states that the contrastive learning objective in CLIP may not cover all the positive pairs, which makes sense. However, \n it is unclear how the proposed solution addresses this issue.\n\n4. The experiments are conducted on a relatively small dataset compared to CLIP. The training behavior and the generalization ability of the representation may be different.  \n\n5. The derivations make sense but they are under several assumptions.\n\n[a] Unified Contrastive Learning in Image-Text-Label Space, CVPR'22  \n[b] Understanding the Behaviour of Contrastive Loss, CVPR'21  \n[c] Large-Margin Contrastive Learning with Distance Polarization Regularizer, ICML\u201921"
                },
                "questions": {
                    "value": "My questions are listed in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5970/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5970/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5970/Reviewer_7qr5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790579405,
            "cdate": 1698790579405,
            "tmdate": 1699636637932,
            "mdate": 1699636637932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jLpYLIIBil",
                "forum": "S5yOuNfSA0",
                "replyto": "iegnthFBIe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7qr5 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive comments!\n\n**Q1**: Contrastive learning has been widely studied in the community. Also, how does the proposed solution compare to those methods?\n\n**A1**:  Thank you for pointing out these related works. We have added discussions on those papers. Similar to CLIP and ALIGN, paper (a) proposed a contrastive learning scheme UniCL for multi-modal learning. Unlike CLIP that primarily rely on image-text data, UniCL uniquely integrates image-label data (ImageNet-1K). This integration enables them to group data with identical labels, allowing for a broader range of positive pair identifications within the dataset. There is no regularization term proposed in this paper; rather, it\u2019s a different contrastive learning method. We have included this paper in discussion as an improvement over CLIP, alongside other methods we discussed such as SLIP, FILIP, and CyCLIP. Moreover, for the paper that actually proposed regularization terms for CLIP (CyCLIP), we indeed made a thorough discussion.  \n\nThe (b,c) focuses on **unimodal contrastive learning**, and thus cannot adequately explain the zero-shot transfer capability of CLIP. As we emphasized in our paper, we focus on **multi-modal contrastive learning**, which has not been studied from a theoretical perspective and has its special challenges resulting from multi-modality. Ours is the first paper to provide a formal understanding of transferable representation learning and zero-shot transfer in CLIP. We have added (b,c) to the related work section and as well as the detailed discussion in Appendix A.\n\n---\n\n**Q2**: The authors propose only to regularize the distance between positive pairs, but there is no ablation comparison to the variants that regularize both or negative pairs only.\n\n**A2**: In our paper, we did clarify that accurately identifying truly negative pairs in image-text data is challenging, as different pairs within the same batch may share common features. Since web-scale image-text data are inherently unlabeled, it is different from the unimodal case where negative pairs can be easily determined by the class labels. As shown in Figure 1, the image-text pairs containing similar features could appear in the same training batch and be mistakenly considered as negative pairs. Consequently, our proposed regularization term only incorporates positive pairs. \n\nWe have added an ablation study by taking all off-diagonal pairs as negative pairs and adding them as a regularization term. The table presented below compares the effects of regularizing positive pairs (Pos) against off-diagonal pairs (Neg) in terms of zero-shot accuracy. While the regularization on positive pairs markedly improves performance, regularization on off-diagonal pairs yields only marginal enhancements on some datasets and no improvement on others. This unstable performance may be attributed to the presence of positive pairs among the off-diagonal elements in the unlabeled image-text data. We have added a discussion on this in Appendix C.2. \n\n|  | CIFAR10 | CIFAR100 | STL10 | Food101 | ImageNetV2 |  DTD | Average |\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CLIP | 63.85 | 31.17 | 90.35 | 8.39 | 20.24 | **21.22** |  39.20 |\n| CLIP+Pos | **67.47** | **33.33** | **92.64** | **12.14** | **22.36** | 19.63 | **41.26** |\n| CLIP+Neg | 64.36 | 31.01 | 91.25 | 9.59 | 20.17 | 20.74 | 39.52 |\n\n---\n\n**Q3**: The introduction states that the contrastive learning objective in CLIP may not cover all the positive pairs, which makes sense. However, it is unclear how the proposed solution addresses this issue.\n\n**A3**: In our paper, we acknowledge the challenge posed by the fact that \"Contrastive learning objectives in CLIP may not cover all positive pairs,\" both from theoretical and empirical standpoints. Our primary goal is to rigorously explore transferable representation learning in CLIP, and to introduce a theoretically-guided regularization that enhances the original CLIP, particularly focusing on the concept of \u201cmargin\u201d. Therefore, our approach primarily addresses this issue from a theoretical angle by incorporating this mismatch into our analysis. We recognize the significance of empirically addressing this issue, but it goes beyond the scope of our current work. The literature [a] you mentioned is a milestone work in this area, which introduces a new loss function. Analyzing why this improved loss [a] outperforms CLIP from a theoretical perspective can be an interesting future direction."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433479131,
                "cdate": 1700433479131,
                "tmdate": 1700433479131,
                "mdate": 1700433479131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jLNVHNSTGE",
                "forum": "S5yOuNfSA0",
                "replyto": "iegnthFBIe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7qr5 (2/2)"
                    },
                    "comment": {
                        "value": "**Q4**: The experiments are conducted on a relatively small dataset compared to CLIP. \n\n**A4**: Unfortunately, the dataset used by Open AI is not open-source. Therefore we utilize the CC3M dataset which is a standard and widely used dataset in prior research on CLIP [1-4]. This dataset is also used in UniCL as the reviewer mentioned, referred to as GCC3M. Many of their experiments (ImageNet-1K, GCC3M, GCC3M+ImageNet-1K) are done at smaller or the same scale.\n\n---\n\n**Q5**: The derivations make sense but they are under several assumptions.\n\n**A5**: In order to theoretically analyze CLIP, we have to make certain assumptions, like all theory papers do. The assumptions streamline the analysis, rendering the problem more amenable. More importantly, the assumptions made in our paper are quite standard and reasonable. In detail, in Assumption 3.1, we make a conditionally independent assumption which is commonly used in the theoretical literature of self-supervised learning (as we mentioned in Remark 3.2). In Assumption 4.1, we make a completeness assumption to allow the existence of a good representation of image-text pairs. We acknowledge that the relaxation of our assumptions would be a interesting future direction. \n\n[1] Yao, Lewei, et al. \"FILIP: Fine-grained Interactive Language-Image Pretraining.\" International Conference on Learning Representations. 2021.\n\n[2] Li et al. \"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pretraining Paradigm.\" International Conference on Learning Representations. 2021.\n\n[3] Mu et al. \"Slip: Self-supervision meets language-image pre-training.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[4] Goel et al. \"Cyclip: Cyclic contrastive language-image pretraining.\" Advances in Neural Information Processing Systems 35 (2022): 6704-6719."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433533804,
                "cdate": 1700433533804,
                "tmdate": 1700433533804,
                "mdate": 1700433533804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CsZjjwffrg",
                "forum": "S5yOuNfSA0",
                "replyto": "iegnthFBIe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up with Reviewer 7qr5"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7qr5,\n\n\nThank you for taking the time to review our paper and providing valuable feedback. We would like to follow up with you and provide additional experiment results regarding Q2 on regularization of negative pairs. In our previous rebuttal, we have explored regularizing the negative (off-diagonal) pairs as compared to positive pairs. Here, we have provided additional experiment results on regularizing both positive and negative (off-diagonal) pairs, which are summarized in the table below. These results demonstrate that regularization of both pairs leads to improvements over only regularizing negative (off-diagonal) pairs across most datasets. However, it still underperforms our proposed method CLIP+Pos, which only regularizes positive pairs. This experiment further supports our initial observation that some off-diagonal image-text pairs may share common features, making their categorization as negative pairs potentially detrimental. Identifying true negative pairs in unlabeled image-text data could be an interesting direction for future research.\n\n\n|  | CIFAR10 | CIFAR100 | STL10 | Food101 | ImageNetV2 |  DTD | Average |\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CLIP | 63.85 | 31.17 | 90.35 | 8.39 | 20.24 | **21.22** |  39.20 |\n| CLIP+Pos | **67.47** | **33.33** | **92.64** | **12.14** | **22.36** | 19.63 | **41.26** |\n| CLIP+Neg | 64.36 | 31.01 | 91.25 | 9.59 | 20.17 | 20.74 | 39.52 |\n| CLIP+Both | 65.12 | 32.63 | 91.68 | 9.67 | 21.07 | 20.26 | 39.91 |\n\n\nWe look forward to your further feedback and are more than happy to discuss any remaining questions you may have. If our rebuttal and the revised paper have effectively addressed your questions and concerns, we sincerely hope you could reconsider your evaluation of our paper. Thank you!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604505122,
                "cdate": 1700604505122,
                "tmdate": 1700604505122,
                "mdate": 1700604505122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rJpoz4lihX",
            "forum": "S5yOuNfSA0",
            "replyto": "S5yOuNfSA0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5970/Reviewer_P1Cv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5970/Reviewer_P1Cv"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates transferable representation learning underlying CLIP and demonstrates how features from different modalities can be aligned. Then a new CLIP-type method is proposed, the effectiveness of the proposed method is proved through experiments on multiple benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-written and easy to follow. \n- This paper theoretically examines the transferable representation learning in CLIP. The theory seems sound.\n- This paper proposes an easy regularization technique for CLIP that can effectively improve its zero-shot performance."
                },
                "weaknesses": {
                    "value": "My major concerns lie in the empirical studies.\n- The current pre-training experiments are all based on the CC3M, which is much smaller than the full 400M dataset used by the CLIP. It is unclear whether the proposed regularization technique holds when extended to a larger dataset. It is recommended to conduct experiments on datasets with different sizes.\n- In Table 1, why incorporating the regularization term into the contrastive objective is harmful to DTD? \n- It seems that the results of CyCLIP in Table 1 and Table 2 are inconsistent with the original CyCLIP paper.\n- Can the proposed regularization technology work in CyCLIP?\n- Is the method equally effective for other downstream tasks such as retrieval?"
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5970/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797845669,
            "cdate": 1698797845669,
            "tmdate": 1699636637831,
            "mdate": 1699636637831,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Xe6lsvRyDt",
                "forum": "S5yOuNfSA0",
                "replyto": "rJpoz4lihX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P1Cv (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your strong support and valuable feedback\uff01We answer your major comments and questions as follows. \n\n**Q1**:  Current pre-training experiments are all based on the CC3M\u2026 it is recommended to conduct experiments on datasets with different sizes.\n\n**A1**: Thank you for your valuable suggestion. CLIP serves as a strategy for learning features that can be transferred between text and images. Unfortunately, the dataset used by Open AI is not open-source. Therefore we utilize the CC3M dataset which is a standard and widely used dataset used in prior research on CLIP [1-4].\n\n---\n\n**Q2**: In Table 1, why incorporating the regularization term into the contrastive objective is harmful to DTD?\n\n**A2**: Thank you for raising this point. While our method can improve on 5 out of 6 downstream tasks, Describable Textures Dataset (DTD) presents a special case where the regularization term can get a worse zero shot performance. We conjecture that the unique properties of texture data in DTD require a more fine-grained approach of regularization. Therefore, we conduct more experiments on the DTD dataset and find that the combination of our regularization and CyCLIP can improve the original CLIP on the DTD data set. \n\n| CLIP | CLIP+Reg | CyCLIP | CyCLIP+Reg |\n| :--------: | :-------: |  :--------: | :-------: |\n| 21.22 | 19.63 | 20.21 | **21.49** |\n\n---\n\n**Q3**: It seems that the results of CyCLIP in Table 1 and Table 2 are inconsistent with the original CyCLIP paper.\n\n**A3**: Indeed, our results for CyCLIP are better than the results reported in the original CyCLIP paper on most tasks. This is due to the different experiment setting of our work, as we train from pre-trained image and text encoders while they trained from scratch. Similar settings can be found in LiT [5] and Flamingo [6] that train from pre-trained single-modality models. \n\n---\n\n**Q4**: Can the proposed regularization technology work in CyCLIP?\n\n**A4**: Yes, it indeed can be used for CyCLIP and leads to improvements on most tasks. We have added the results for CyCLIP+Reg as follows.\n\nZero-shot\n|  | CIFAR10 | CIFAR100 | STL10 | Food101 | ImageNetV2 |  DTD | Average |\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CyCLIP | 60.71 | 28.87 | 89.98 | 9.72 | 19.66 | 20.21 | 38.19 |\n| CyCLIP+Reg | 58.73 | **30.15** | **90.79** | **10.87** | **19.68** | **21.49** | **38.62** |\n\nLinear probing\n|  | CIFAR10 | CIFAR100 | STL10 | Food101 | DTD |  Flowers | OxfordPets | Average |\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CyCLIP | 86.31 | 63.93 | 93.69 | 61.57 | 56.86 | 70.56 | 70.46 | 71.91|\n| CyCLIP+Reg | 85.08 | 62.60 | **93.83** | **62.40** | 54.95 | **72.32** | **77.24** | **72.63** |\n\n---\n\n**Q5**: Is the method equally effective for other downstream tasks such as retrieval?\n\n**A5**: Thank you for pointing us to the retrieval task. We have added experiments on retrieval accordingly and summarized the zero-shot results in the table below, including Flickr-30k (1k) and MSCOCO (5k) following the original CyCLIP paper [4]. As also noted in [4], text retrieval per image is easier than image retrieval per caption, due to the nature of both datasets containing 5 captions for each image. The trend of our results align with that reported in [4]. Lastly, while CyCLIP does not provide consistent improvement over CLIP (in our experiment as well as in their reported results), our regularization term provides consistent improvement to both CLIP objective and CyCLIP objective. This has also been updated in our Appendix C.1.\n\nFlickr-30k (1k)\n|  | Text R@1 | Text R@5 | Text R@10 | Image R@1 | Image R@5 | Image R@10 | Average\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CLIP | 87.36 | 93.0 | 95.18 | 26.88 | 54.18 | 66.22 | 70.47 |\n| CLIP+Reg | **87.42** | **93.42** | **95.82** | **29.94** | **58.00** | **69.82** | **72.40** |\n| CyCLIP | 87.34 | 93.12 | 95.04 | 29.00 | 56.50 | 67.62 | 71.44 |\n| CyCLIP+Reg | 87.20 | 93.20 | 95.56 | 29.14 | 56.94 | 68.64 | 71.78 | \n\nMSCOCO (5k)\n|  | Text R@1 | Text R@5 | Text R@10 | Image R@1 | Image R@5 | Image R@10 | Average\n| :--------: | :-------: |  :--------: | :-------: | :-------: | :-------: | :-------: | :-------: |\n| CLIP | 81.19 | 83.21 | 84.42 | 4.73 | 11.66 | 15.93 | 46.86 |\n| CLIP+Reg | **81.25** | **83.31** | 84.49 | 4.98 | 12.14 | 16.66 | 47.14 |\n| CyCLIP | 81.06 | 82.92 | 84.28 | 4.70 | 11.66 | 15.93 | 46.86 |\n| CyCLIP+Reg | 81.31 | 83.28 | **84.65** | **5.27** | **12.17** | **16.70** |**47.23** |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433155648,
                "cdate": 1700433155648,
                "tmdate": 1700433155648,
                "mdate": 1700433155648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7ctwBRWdZi",
                "forum": "S5yOuNfSA0",
                "replyto": "rJpoz4lihX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5970/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P1Cv (2/2)"
                    },
                    "comment": {
                        "value": "[1] Yao, Lewei, et al. \"FILIP: Fine-grained Interactive Language-Image Pretraining.\" International Conference on Learning Representations. 2021.\n\n[2] Li et al. \"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pretraining Paradigm.\" International Conference on Learning Representations. 2021.\n\n[3] Mu et al. \"Slip: Self-supervision meets language-image pre-training.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[4] Goel et al. \"Cyclip: Cyclic contrastive language-image pretraining.\" Advances in Neural Information Processing Systems 35 (2022): 6704-6719.\n\n[5] Xiaohua Zhai et al. \"Lit: Zero-shot transfer with locked-image text tuning.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[6] Jean-Baptiste Alayrac et al. \"Flamingo: a visual language model for few-shot learning.\" Advances in Neural Information Processing Systems 35 (2022): 23716-23736."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5970/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433271802,
                "cdate": 1700433271802,
                "tmdate": 1700433271802,
                "mdate": 1700433271802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]