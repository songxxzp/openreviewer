[
    {
        "title": "CResT: Cross-Query Residual Transformer for Object Goal Navigation"
    },
    {
        "review": {
            "id": "rz4eFU7ySi",
            "forum": "f9Djqso1p1",
            "replyto": "f9Djqso1p1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission377/Reviewer_divX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission377/Reviewer_divX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an end-to-end Transformer-based architecture for object goal navigation. The main contribution is the inclusion of additional residual connections in the transformer, which helps in training a deeper architecture. The method is evaluated in the AI2-THOR synthetic environment."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed residual connections might actually be helpful, but further investigation is required."
                },
                "weaknesses": {
                    "value": "Limited novelty, writing needs improvement, experiments are insufficient. See comments underneath for details."
                },
                "questions": {
                    "value": "The intro reads more as a related work section (particularly paragraph 3). It is not properly explained what is the limitation of the prior work that this paper tries to address. There is no reason to explain why the state representation is important for end-to-end methods since that is already kinda obvious. Rather give examples of what existing state representations are not able to encode well, in order to build your narrative towards the paper's contribution. Similarly, the contributions of the paper are not clear in the intro. For example, in what ways is the visual information from the proposed CResT better?\nThe writing needs substantial improvements. Statements such as \"we propose this because it is better\" (without further explanation) are uninformative and should be avoided. Also there are minor language issues throughout the paper. One example: first sentence of the intro \"navigating\" -> \"navigation\".\n\nThe paper presents the Residual Transformer as one of its contributions which is confusing, especially for an object-goal navigation paper. The authors propose three residual connections to the vanilla transformer architecture. But various designs on residual connections for transformers have already been explored in the literature which are not cited in the paper:\n[A] He et al, RealFormer: Transformer Likes Residual Attention, ACL 2021\n[B] Xie et al, ResiDual: Transformer with Dual Residual Connections, arXiv 2023\nIf the authors are proposing a new design of their own in order to address the gradient vanishing problem (which they keep mentioning) then they should follow the methodologies of [A,B] on how to present and justify their proposal. In the context of object goal navigation, the proposed design is a straightforward extension of transformers without proper justification. This is exacerbated by the trivial description of the overall architecture. It seems to me that the inspiration for this work is the VTNet paper (which is cited and compared to in the paper). However, in VTNet there is a clear description of the thought process behind the architecture and visual demonstration of how the resulting representation is useful for object-goal navigation.\n\nRandom mask for data augmentation is pretty common and should not be claimed as novelty.\n\nThere are some discrepancies in the experimental results. First, the performance reported in the VTNet paper is higher than what is shown here. If the authors followed a different experimental setup than the baselines then it should be clearly noted in the paper. Second, in the ablation experiment of No connections / All connections, the vanilla transformer is shown to fail (14.6% SR), which should encourage more investigation on the experiment itself. Vanilla transformers are already very powerful in extracting visual representations and have been used with relative success in obj-nav including the VTNet paper so I used expect at least decent performance.\n\nFinally, if the main motivation of the paper is to enhance visual features, then perhaps a more realistic environment (e.g., Habitat) is more suitable to demonstrate the work, rather than a fully synthetic environment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission377/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission377/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission377/Reviewer_divX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549527613,
            "cdate": 1698549527613,
            "tmdate": 1699635964864,
            "mdate": 1699635964864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "MadeKIi8XN",
            "forum": "f9Djqso1p1",
            "replyto": "f9Djqso1p1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission377/Reviewer_oDQU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission377/Reviewer_oDQU"
            ],
            "content": {
                "summary": {
                    "value": "This submission introduces the Cross-Query Residual Transformer for Object Goal Navigation. The proposed method mainly consists of Residual Transformer and Random Mask. The proposed method is evaluated on the AI2-THOR dataset. The submission claims to achieve state-of-the-art performance. However, the methods in comparison seem quite outdated."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- This submission clearly presents the ObjectNav task.\n- This submission attempts to utilize information at different scales (global and local)."
                },
                "weaknesses": {
                    "value": "- The technical contributions are somewhat weak; several claimed innovations in the paper, such as the Transformer and Residual connections, are well-established methods that have been previously introduced and proven effective.\n- The work appears more akin to a general visual extractor and does not seem particularly tailored to the ObjectNav task.\n- The submission lacks comparison with many recent works conducted on the AI2-THOR simulator; the following is a subset.\n\t[1] Object-Goal Visual Navigation via Effective Exploration of Relations among Historical Navigation States. CVPR 2023\n\t[2] Layout-Based Causal Inference for Object Navigation. CVPR 2023\n\t[3] Search for or Navigate to? Dual Adaptive Thinking for Object Navigation. ICCV 2023\n- The idea lacks novelty. The method overall seems to merely merge the global and object features provided by VTNet in a slightly modified form, and the performance improvement compared to VTNet is not significant.\n- The experimental section is quite insufficient, lacking adequate demonstration of the motivation, making it difficult to substantiate the method's effectiveness and necessity."
                },
                "questions": {
                    "value": "- The submission declares that \"The Residual Transformer is proposed to solve the gradient vanishing problem,\" It's unclear how this gradient vanishing problem' manifests uniquely in the ObjectNav task. Are there experimental results or analyses that demonstrate this phenomenon in the ObjectNav task? Evidence of gradient vanishing specifically impacting the performance or training stability in ObjectNav would be crucial to justify the adoption of the Residual Transformer as a solution.\n- In the submission, the motivation behind adopting a cross-query approach is not clear. Why is it implemented only in the decoder and not in the encoder? The submission should provide evidence or theoretical justification for these design choices.\n- The one of contribution of the paper, \"Random Mask\", seems to be an operation similar to dropout?\n- Are there specific examples or evidence in the context of \"Overfitting means wonderful training results yet unsatisfying testing results, and it becomes obvious when a relatively larger network trains on a relatively limited dataset.\"? For instance, are there experimental results or visualizations demonstrating this phenomenon in the ObjectNav task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757530678,
            "cdate": 1698757530678,
            "tmdate": 1699635964799,
            "mdate": 1699635964799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "jtTcONLhtn",
            "forum": "f9Djqso1p1",
            "replyto": "f9Djqso1p1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission377/Reviewer_9zLV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission377/Reviewer_9zLV"
            ],
            "content": {
                "summary": {
                    "value": "The document introduces the Cross-Query Residual Transformer (CResT) method for object goal navigation. CResT enhances navigation by improving visual feature extraction from RGB images. The architecture incorporates an overall network with Global and Local Features, a Residual Transformer for deeper training, and a Random Mask for data augmentation. The method surpasses other models on the AI2-THOR dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors introduce the Cross-Query Residual Transformer (CResT), which excels in the extraction of intricate visual features from RGB camera images. As a result, CResT provides a more holistic and informative visual representation. This innovation allows CResT to achieve state-of-the-art performance on the AI2-THOR dataset in the realm of object goal navigation.\n2. The proposed CResT can be trained in one stage instead of the two-stage training in the previous method. This makes the training process more concise and efficient.\n3. The authors also adopt a Random Mask module to augment the Global Feature and Local Feature. This data augmentation technique augments both the Global and Local Features of the network, thereby improving its generalization capabilities and effectively mitigating the risk of overfitting."
                },
                "weaknesses": {
                    "value": "1. Lack of in-depth explanation of the contributions: The authors do not provide sufficient insights into the introduction of residual connections in the Transformer. It would be beneficial to have a more detailed explanation of how these connections address the gradient vanishing problem and improve the backpropagation in the network.\n2. Limited novelty in the proposed random mask module: The proposed random mask module is a commonly used data augmentation technique in deep learning. While it is applied in the context of visual navigation, the paper does not provide enough justification or explanation for its effectiveness specifically for this task. Further analysis or comparison with alternative data augmentation methods would strengthen the novelty of this module."
                },
                "questions": {
                    "value": "The primary question concerning the novelty of the proposed methods has been articulated in the weaknesses section. If the authors successfully address the concerns specified in the weaknesses section, I would reconsider my recommendation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission377/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission377/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission377/Reviewer_9zLV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759342082,
            "cdate": 1698759342082,
            "tmdate": 1699635964707,
            "mdate": 1699635964707,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "EgsCwXJ9QF",
            "forum": "f9Djqso1p1",
            "replyto": "f9Djqso1p1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission377/Reviewer_4dQB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission377/Reviewer_4dQB"
            ],
            "content": {
                "summary": {
                    "value": "This article aims to improve the end-to-end object navigation system. The author focuses on improving the visual state representation of the navigator. Specifically, the author introduces additional shortcut connections to the transformer architecture to alleviate the gradient vanishing problem and applies random masking to the visual features as data augmentation. The proposed method is validated on the AI2-THOR dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The research problem is interesting and holds broad potential value due to the significance of visual representations in embodied tasks."
                },
                "weaknesses": {
                    "value": "1. The technique contribution seems limited and appears to be an incremental improvement to VTNet by introducing more shortcut connections and random masking. I am not convinced by the author's statement regarding the gradient vanishing problem in the original transformer. The reason is as follows:\na) The author's argument is solely based on the improved performance of object navigation after adding more shortcuts. It lacks supporting theoretical, visualization, or metric analysis. \nb) In Table 2, the performance of a 1-layer transformer is already good (0.7312 SR), and the improvement with a 6-layer transformer is limited (0.7322 SR). How does this example demonstrate the effectiveness of adding more shortcuts to address the gradient vanishing problem? \n2. The experiments are insufficient as they are only validated on the AI2-THOR dataset. Why weren't other object navigation datasets, such as Habitat [1] or ProcTHOR [2], used for validation? \n3. The comparative methods are insufficient, and I hardly see any recent object navigation works from 2022 and 2023.\n\n[1] Habitat-web: Learning embodied object-search strategies from human demonstrations at scale, CVPR\u201922\n[2] ProcTHOR: Large-Scale Embodied AI Using Procedural Generation, NeurIPS\u201922"
                },
                "questions": {
                    "value": "What are the advantages of studying end-to-end object navigation when recent articles [3] have shown that it fails when transferred to the real world, compared to the counterpart modular approaches? \n\n[3] Navigating to objects in the real world, Science Robotics\u201923"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission377/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699140740248,
            "cdate": 1699140740248,
            "tmdate": 1699635964633,
            "mdate": 1699635964633,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]