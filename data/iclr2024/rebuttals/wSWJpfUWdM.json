[
    {
        "title": "FedLAP-DP: Federated Learning by Sharing Differentially Private Loss Approximations"
    },
    {
        "review": {
            "id": "1Vub0NGPTr",
            "forum": "wSWJpfUWdM",
            "replyto": "wSWJpfUWdM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2109/Reviewer_vGpK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2109/Reviewer_vGpK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new federated learning training algorithm. The key idea of the algorithms is to share synthetic datasets instead of sharing the model weights. The authors argue that the method is superior to traditional model-averaging because the server can recover an approximation of the global loss landscape. The authors also demonstrate that the proposed method can be adapted to satisfy differential privacy (DP) by replacing the clean gradients with clipped noisy gradients. Some experiments show that the new method can outperform the traditional one in both private and non-private settings and may have better communication cost tradeoffs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors identify the problem of the existing weight-averaging FL training methods.\n* The proposed method can overcome the limitations of the existing algorithms.\n* The proposed algorithm is shown to have better empirical performances than the existing ones."
                },
                "weaknesses": {
                    "value": "1. Some operations in the proposed algorithm may need better motivation or explanation. Please refer to Questions.\n2. The privacy description part of the algorithm can be improved. The description in Section 4.4 is not clear enough and self-contained for a main contribution in the main text. Some descriptions may need to be more precise (e.g., \"sanitize\" should be explicitly referred to the clipping operation). Since the privacy composition part mainly relies on existing tools, it may be better to provide a brief conclusion about the composition results. Also, the notations in Appendix (T?) are not consistent with Algorithm 1 (R_b?), which requires extra conjecture to parse the result.\n3. Some hyper-parameters may need to show how to tune ($R_b$ and $R_l$), because they are so different for private and non-private settings.\n4. The experiment setting may need to be more convincing. Please refer to Questions."
                },
                "questions": {
                    "value": "1. It is mentioned that the synthetic labels are initialized to be a fixed, balanced set, but the experiments have heterogeneous data. This sounds controversial and may deserve more explanation. When a class does not appear in a local dataset, what should we expect the synthetic data of that class to look like? How do those affect global training compared with relatively iid data?\n2. How do you decide the synthetic dataset size of each client? Besides the factors mentioned (local dataset size and bandwidth), it seems the data complexity and local data distribution should also be considered when deciding the synthetic dataset size.\n3. Section 5.1 mentions that the learning rate is 100. Is it a typo? Otherwise, why we need such a large learning rate may need to be explained.\n4. What do the \"Fixed, Max, Median, and Min\" radius selection mean? Do they matter for whether private or non-private settings? The description needs to be more specific and consistent (i.e., it says r=1.5 or 10 in Section 5.1, but a different wording in Section 5.4).\n5. In the experiment setting of comparing the communication cost and epochs, there may be more reasonable settings. For example, when comparing the communication cost, shouldn't we compare the best end-to-end performance with fixed communication costs (e.g., 500MB) by varying the communication rounds and with the best hyper-parameters? \n6. Another interesting aspect not explored in the experiments FedAvg v.s. FedLAP with different sizes of models. Does a model with more parameters benefit from or loss advantage with the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Reviewer_vGpK"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698255430829,
            "cdate": 1698255430829,
            "tmdate": 1699636143563,
            "mdate": 1699636143563,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YeIC122kGn",
                "forum": "wSWJpfUWdM",
                "replyto": "1Vub0NGPTr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer vGpK"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the insightful comments and will polish the paper accordingly. We now address the specific questions and concerns below.\n\n----\n**Q1: How to tune ($R_b$ and $R_l$)**\n\nThe parameter $R_b$ is not contingent on privacy budgets and generally enhances performance when assigned a higher value. We chose different values to limit computation and execution time. Regarding $R_l$, we conduct a line search in {0, 1, 2, 3, 5}. We found that a large $R_l$ does not always translate to better performance, which could be attributed to the capacity of the synthetic images.\n\n----\n**Q2: When a class does not appear in a local dataset, what should we expect the synthetic data of that class to look like?**\n\nWe would like to clarify that the initialized label set contains only the classes that appear on a client, which are typically not regarded as sensitive to privacy. The images may look random and could be not interpretable for humans. However, we expect that the synthetic images associated with a specific class will also carry information of other classes, thereby making the information class-agnostic.\n\n----\n**Q3: How do the authors decide the synthetic dataset size of each client? The data complexity and local data distribution should also be considered when deciding the synthetic dataset size.**\n\nOur work considers replacing the plain gradients with synthetic images and explores how to carry more training information given the same amount of communication costs. Therefore, we set the size roughly the same as the model parameters (in Table 1, ours with 0.96$\\times$ vs. the baselines with 1$\\times$ costs) and evenly assign the size to each class.\n\nOn the other hand, we agree with the reviewer that the size could be set according to the distributions and probably even to different classes. However, defining a privacy-preserving (should potentially be data-independent; otherwise, a DP mechanism is required for the chosen size) and reliable metric is non-trivial. We will leave it for future work.\n\n----\n**Q4: Section 5.1 mentions that the learning rate is 100. Is it a typo?**\n\nNo. We notice that the gradients that arrive at the synthetic images are typically small. Therefore, we set it to 100 to approximate the training information in a reasonable time (e.g., due to limited computation on edge devices). We also notice such settings in existing dataset distillation works. For instance (implemented by [1] and the other official codes), gradient matching [2] uses 1, distribution matching [3] uses 10, and MTT [4] uses 10000.\n\n----\n**Q5: What do the \"Fixed, Max, Median, and Min\" radius selection mean? Do they matter for private or non-private settings?**\n\n**Recap.** Our method considers two kinds of effective approximation regions, namely $r$ and $r_k$, respectively. The first is a predefined radius that the server expects the client to approximate. However, local data may introduce different difficulties in approximation. Therefore, the clients assess and report the effective approximation region ($r_k$), defined as the radius where the loss values of synthetic data deviate from the real loss (Figure 2).\n\n**The strategies.** After client training, the server will receive a set of radii $\\{r_k\\}$ suggested by the K clients. The server can decide how to leverage the information. **Fixed** ignores the information and uses the predefined $r$; **Max**, **Median**, and **Min** operate based on how much the server trusts the clients. Our experiments mainly use min, the most conservative strategy. The detailed ablation study is provided in Sec. D.2.\n\n**Strategy in DP settings.** Since the client-suggested radius $r_k$ is data-dependent and could introduce additional privacy concerns, we only use the predefined $r$ in our DP experiments.\n\n----\n**Q6: When comparing the communication cost, shouldn't we compare the best end-to-end performance with fixed communication costs (e.g., 500MB) by varying the communication rounds and with the best hyper-parameters?**\n\nWe have precisely adhered to the setup in this work. We begin with tuning the gradient-sharing baselines and set the size of synthetic sets to match the communication costs, thus creating a constant cost benchmark as suggested by the reviewer. Based on the size, we searched the other hyper-parameters (except for #communication rounds) for our FedLAP.\n\n----\n**Q7: Does a model with more parameters benefit from or lose advantage with the proposed method?**\n\nThe benefits will hold if the approximation quality does not decrease.\n\n----\n**Q8:  the notations in Appendix (T?) are not consistent with Algorithm 1 (R_b?)**\n\nOur algorithm accesses the data more than $R_b$ times. It requires \"$R_b \\times R_i \\times$ *the number of iterations to achieve the radius $r$*\" times. Thus, we use $T$ for conciseness."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174200044,
                "cdate": 1700174200044,
                "tmdate": 1700442034513,
                "mdate": 1700442034513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "S6ii7EdgCH",
                "forum": "wSWJpfUWdM",
                "replyto": "1Vub0NGPTr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for the end of the discussion phase"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nSince we are approaching the end of the discussion phase, we kindly remind you that you can re-evaluate and adjust your score if our response solves your concerns. We will also be more than happy to answer any further questions and comments.\n\nIn addition, we have revised the manuscript according to the reviewers' comments. All revision has been marked in red. Specifically,\n\n1. (Reviewer SbQA) Algorithm 1 and the corresponding descriptions have been revised for better readability\n2. (Reviewer XtEq, vGpK) We replace the word \"sanitize\" with \"clip\" to explicitly refer to DP operations and rephrase the corresponding sentences.\n3. (Reviewer vGpK) We add a brief conclusion to Sec. 4.4 to summarize the result of our privacy analysis."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656558687,
                "cdate": 1700656558687,
                "tmdate": 1700656558687,
                "mdate": 1700656558687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o5ORRnShQO",
                "forum": "wSWJpfUWdM",
                "replyto": "1Vub0NGPTr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Extension to larger architectures"
                    },
                    "comment": {
                        "value": "To further support our claim in **Q7**, we conducted an additional experiment on CIFAR-10 with VGG11 of size 9231114 floating points. For a fair comparison, we assign 300 synthetic images to each class and keep the other hyper-parameters the same as in our paper. The size of synthetic sets comes from $9231114/(32 \\times 32 \\times 3) \\approx 3000$, where $(32 \\times 32 \\times3)$ is the size of a synthetic image. The quota of 3000 synthetic images is then evenly assigned to 10 classes, resulting in 300 images per class. Thus, both methods consume the same communication cost for one federated round.\n\nDue to the time limit, we provide a preliminary result against FedAvg and FedProx.\n\n| Epoch  | 1     | 2     | 3     | 4     | 5     | Best |\n|--------|-------|-------|-------|-------|-------|-------|\n| FedAvg | 10.00 | 11.92 | 18.76 | 22.33 | 26.75 | 35.45 |\n| FedProx | 10.00 | 10.00 | 9.43 | 10.00 | 9.97 | 54.89 |\n| Ours   | 12.13 | 28.42 | 36.69 | 40.23 | 46.35 | 58.12 |\n\nThe preliminary result shows that our method improves much faster than the plain gradient-sharing method (FedAvg), confirming the effectiveness of our method against non-IID distributions. The faster improvement implies better utility and communication efficiency, making our method favorable concerning limited bandwidth. It is worth noting that larger architectures suffer from non-IID distributions more than the simple ConvNet and waste most of the iterations at the beginning stage."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705554718,
                "cdate": 1700705554718,
                "tmdate": 1700734113407,
                "mdate": 1700734113407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yzN7m9qECG",
            "forum": "wSWJpfUWdM",
            "replyto": "wSWJpfUWdM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2109/Reviewer_QU6w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2109/Reviewer_QU6w"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach called FedLAP-DP for federated learning. In contrast to previous methods that share point-wise local gradients for global model update, the new approach proposes to perform global optimization at the server by leveraing synthetic samples received from clients. Experiments are conducted to show the performance of the proposed method as well as some baseline methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is interesting to borrow the idea of Dataset Distillation into federated model training to tackel the non-iid data and privacy issues.\n\nThe paper is overall clearly structured and easy to follow."
                },
                "weaknesses": {
                    "value": "Though the idea is interesting, it seems to lack formal guarantees on the approximation achieved for each local synthetic dataset and how approximation level affects the learning performance in theory.\n\nAs for the DP side, the explicit trade-off between the privacy loss and the learning utility is also unclear for the proposed method.\n\nIn experiment, the performance on different settings with different heterogeneity can be further explored. And since you consider the record-level DP, the setting of privacy budget $\\epsilon$ is relatively large even for high privacy regime where $\\epsilon$ is set to be 2.79."
                },
                "questions": {
                    "value": "1. Can you provide some formal theoretical guarantees for the proposed algorithm, e.g., approximation of the synthetic data learnt compared with the optimal one, and the learning performance, privacy-utility tradeoffs, so on.\n\n2. Is there any possiblity to extend the proposed method to client-level DP, which is very important in cross-device scenarios such as collaboration between massive IoT devices. If not, then what is the barrier for doing this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751101108,
            "cdate": 1698751101108,
            "tmdate": 1699636143492,
            "mdate": 1699636143492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "spFZGQwcZO",
                "forum": "wSWJpfUWdM",
                "replyto": "yzN7m9qECG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QU6w"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s insightful comment and concerns. We acknowledge that incorporating theoretical analysis could enhance understanding; however, this does not diminish the contributions our work makes to effective non-IID federated optimization with differential privacy guarantees, which is also supported by the feedback from the other reviewers. \n\nWe hope the reviewer will re-evaluate the score in light of the following discussion and prospective extensions of our work.\n\n----\n**Q1: Lack of formal guarantees on gradient approximation and learning performance**\n\nTheoretical understanding of deep learning algorithms is limited in general, and this applies to dataset distillation as well. We agree that more foundational works along this line is needed, but this is beyond the scope of our contribution.\n\n----\n**Q2: The explicit trade-off between the privacy loss and the learning utility is also unclear for the proposed method.**\n\nIt has been analyzed in Sec. 5-3 and Figure 4.\n\n----\n**Q3: The privacy budget $\\varepsilon = 2.79$ is relatively large even for a high privacy regime.**\n\nDetermining an appropriate value for $\\varepsilon$ to define \"a high privacy regime\" is subject to debate. Nonetheless, the range of 2 to 8 for $\\varepsilon$ is widely accepted in the existing literature, such as in DP-SGD [1] (but in centralized frameworks). In real-world applications, companies like Apple [2] implement values of 4 and 8 in their Safari browser. Therefore, our setup of $\\varepsilon$ is a justified choice for maintaining a high privacy regime while also preserving practical utility.\n\nMoreover, the chosen value does not diminish our contribution, as the proposed FedLAP demonstrates consistent superiority over its counterparts when evaluated with smaller $\\varepsilon$.\n\n----\n**Q4: Is there any possibility to extend the proposed method to client-level DP?**\n\nYes. Consider a scenario where we have a trusted server, a common assumption in client-level differential privacy. One possible extension is the server groups the synthetic images based on client IDs before server training. During each server training iteration, the server draws batches from every group and calculates the average gradient for each batch. Then, it proceeds to clip the gradients and add noise to the average of the clipped gradients. It\u2019s important to note that the noise scale introduced here is based on the rate of client sampling, which is different from the data-record sampling rate considered in our work.\n\nLastly, the server uses the processed gradients to update the server model. This step completes a single training cycle. The server continues the process until finishing one communication round, thus ensuring client-level DP.\n\n----\n**Reference**\n\n[1] Abadi, Martin, et al. \"Deep learning with differential privacy.\" Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.\n\n[2] https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173879832,
                "cdate": 1700173879832,
                "tmdate": 1700174603276,
                "mdate": 1700174603276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jB9fgnPqa9",
                "forum": "wSWJpfUWdM",
                "replyto": "yzN7m9qECG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for the end of the discussion phase"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nSince we are approaching the end of the discussion phase, we kindly remind you that you can re-evaluate and adjust your score if our response solves your concerns. We will also be more than happy to answer any further questions and comments.\n\nIn addition, we have revised the manuscript according to the reviewers' comments. All revision has been marked in red. Specifically,\n\n1. (Reviewer SbQA) Algorithm 1 and the corresponding descriptions have been revised for better readability\n2. (Reviewer XtEq, vGpK) We replace the word \"sanitize\" with \"clip\" to explicitly refer to DP operations and rephrase the corresponding sentences.\n3. (Reviewer vGpK) We add a brief conclusion to Sec. 4.4 to summarize the result of our privacy analysis."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656539895,
                "cdate": 1700656539895,
                "tmdate": 1700656539895,
                "mdate": 1700656539895,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "29AQuRtFab",
                "forum": "wSWJpfUWdM",
                "replyto": "jB9fgnPqa9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Reviewer_QU6w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Reviewer_QU6w"
                ],
                "content": {
                    "comment": {
                        "value": "Q1: Lack of formal guarantees on gradient approximation and learning performance\n\nNo, you can see much previous work on DP+FL or DP+Deep Learning, although there are no theoretical results on DP+Deep Neural Networks, they also contain theoretical guarantees on convex loss or linear loss. So in my opinion, no theoretical explanation is unacceptable for your problem. Also the theory of DP guarantee also follows the previous paper directly and less interesting. \n\nFor Large epsilon, yes previous paper uses epsilon=8. However, they are for client level rather than sample level of privacy. \n\nThere are also many problems and issues with the experiments. The datasets you use are very small. As a experimental paper, you need to use real-world scale data. To the best of my knowledge, the previous paper such as DP-Follow the leader, it uses large scale of data. Thus, there is a lack of experiments. Of course the number of clients is only 5, which is also unreasonable. \n\nSo, I will not change my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658041139,
                "cdate": 1700658041139,
                "tmdate": 1700658041139,
                "mdate": 1700658041139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TT4Y4TJndx",
                "forum": "wSWJpfUWdM",
                "replyto": "yzN7m9qECG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the follow-up questions"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the prompt reply! The reviewer seems to be raising new issues that are slightly different from the original review. To address these, we offer the following clarification.\n\n----\n> you can see much previous work on DP+FL or DP+Deep Learning, although there are no theoretical results on DP+Deep Neural Networks, they also contain theoretical guarantees on convex loss or linear loss. So, in my opinion, no theoretical explanation is unacceptable for your problem.\n\nThe original review asked how approximation affects performance in our setting, namely deep learning models. As noted by the reviewer and in our previous response, **there are no (or limited) theoretical results on DP+Deep Neural Networks**.\n\nThe reviewer now suggests analyzing simplified models with linear or convex functions, which could be done as those studied in approximation theory. However, to our best knowledge, the efficacy of such methods heavily relies on assumptions and characteristics, which are not comprehensively understood in the context of deep learning. Given the entirely different characteristics, such analysis may not provide practically relevant insights. In contrast, the field typically quantifies approximation errors of unknown functions using empirical metrics, which is precisely what we adopt to learn synthetic images, namely, the cosine distance (and a more common metric MSE in the appendix).\n\nOverall, we still acknowledge the importance of theoretical insight in deep learning but believe that pursuing this direction is non-trivial and beyond our current scope. As an initiative exploration, our work has demonstrated promising results, as acknowledged by other reviewers. We welcome any additional references the reviewer might suggest.\n\n----\n> For Large epsilon, yes previous paper uses epsilon=8. However, they are for client level rather than sample level of privacy.\n\nThe value we used is **a common choice in existing works on record-level DP**. For instance, DP-SGD, the reference we provided in the previous response, considers **record-level DP** and **a range of 2-8**. \n\nIn addition, though we acknowledge that a tighter privacy constraint is worth investigating, the usable utility of models is equally important. As reported in Table 2, prior works with $\\varepsilon=2.79$ and non-IID data splits deliver around 18% accuracy only, which has been far behind useful (compared to random chance 10% and non-DP settings $\\approx$ 75%). Therefore, we believe our setup ($\\varepsilon \\in [2,10] $) is a reasonable choice, which strikes a good balance between privacy and utility.\n\n----\n> the experiment settings\n\nWe consider a **cross-silo** scenario [1], where relatively few but powerful participants attend. **We invite the reviewer to point out the specific work they refer to for a more direct comparison.**\n\n[1] Kairouz, Peter, et al. \"Advances and open problems in federated learning.\" Foundations and Trends\u00ae in Machine Learning 14.1\u20132 (2021): 1-210.\n\n----\nWe are grateful for the continued dialogue and the opportunity to discuss our work further. We hope these points adequately solve the concerns and clarify the rationale behind our methodological choices."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695883523,
                "cdate": 1700695883523,
                "tmdate": 1700696103335,
                "mdate": 1700696103335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0mUHRo9hOt",
            "forum": "wSWJpfUWdM",
            "replyto": "wSWJpfUWdM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2109/Reviewer_XtEq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2109/Reviewer_XtEq"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies federated learning with data condensation. In order to handle data heterogeneity, instead of sending local updates as in FedAvg, this paper proposes a method of sending synthetic data samples. Experiments show that the proposed method can improve model performance as well as reduce communication. To protect privacy, this paper use Gaussian mechanism to enforce record-level differential privacy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper is well written and easy to follow. Extensive experiments are conducted and the results look promising."
                },
                "weaknesses": {
                    "value": "My biggest concern is that the novelty may be limited. Data condensation + FL has been well studied, e.g. [1,2]. In particular, using condensed datasets in FL has been discussed in [1]. The novelty of this work looks limited. It would be highly appreciated if the authors can show improvements over existing works in terms of communication, performance, etc. \n\n\n[1] Liu, Ping, Xin Yu, and Joey Tianyi Zhou. \"Meta knowledge condensation for federated learning.\" arXiv preprint arXiv:2209.14851 (2022).\n[2] Behera, Monik Raj, et al. \"Fedsyn: Synthetic data generation using federated learning.\" arXiv preprint arXiv:2203.05931 (2022)."
                },
                "questions": {
                    "value": "For DP, the paper says \"we sanitize the gradients derived from real data with the Gaussian mechanism\"\u3002 Do you clip the gradients or do something to bound the sensitivity? How will this affect the model utility?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Reviewer_XtEq"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824918759,
            "cdate": 1698824918759,
            "tmdate": 1699636143413,
            "mdate": 1699636143413,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HrFURePtxT",
                "forum": "wSWJpfUWdM",
                "replyto": "0mUHRo9hOt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XtEq"
                    },
                    "comment": {
                        "value": "We are encouraged that the reviewer find our work easy to follow and promising. We hope our response below can mitigate the reviewer's concern about novelty.\n\n----\n**Q1: My biggest concern is that the novelty may be limited. Data condensation + FL has been well studied, e.g. [1,2].**\n\nNone of the mentioned works investigate privacy-preserving federated learning with DP guarantees. In addition, our FedLAP explicitly leverages approximation quality, i.e., in what region the synthetic data resemble the gradients best, to further boost the performance. These differences outline our novelty and contributions.\n\n----\n**Q2: It would be highly appreciated if the authors can show improvements over existing works in terms of communication, performance, etc.**\n\nIn addition to the non-archival works mentioned by the reviewer, we have compared FedLAP to FedDM, a concurrent work that shares a similar motivation and was published in CVPR 2023. Our method demonstrates superior performance (Sec. 5-2 and Table 1) and provides stronger privacy guarantees (discussed in Sec. 2) by explicitly considering effective approximation quality.\n\n----\n**Q3: Do you clip the gradients or do something to bound the sensitivity? How will this affect the model utility? \"we sanitize the gradients derived from real data with the Gaussian mechanism\" looks vague.**\n\nYes, we clip and add noise to achieve an algorithm with DP guarantees. The process introduces noise and could hurt the model utility. Table 2 and Figure 4 show that our method achieves a better utility-privacy trade-off with clipping and Gaussian noise, i.e., in a DP setting. We will revise the sentence to avoid confusion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173569663,
                "cdate": 1700173569663,
                "tmdate": 1700173569663,
                "mdate": 1700173569663,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9gj2dh0o46",
                "forum": "wSWJpfUWdM",
                "replyto": "0mUHRo9hOt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for the end of the discussion phase"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nSince we are approaching the end of the discussion phase, we kindly remind you that you can re-evaluate and adjust your score if our response solves your concerns. We will also be more than happy to answer any further questions and comments.\n\nIn addition, we have revised the manuscript according to the reviewers' comments. All revision has been marked in red. Specifically,\n\n1. (Reviewer SbQA) Algorithm 1 and the corresponding descriptions have been revised for better readability\n2. (Reviewer XtEq, vGpK) We replace the word \"sanitize\" with \"clip\" to explicitly refer to DP operations and rephrase the corresponding sentences.\n3. (Reviewer vGpK) We add a brief conclusion to Sec. 4.4 to summarize the result of our privacy analysis."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656522895,
                "cdate": 1700656522895,
                "tmdate": 1700656522895,
                "mdate": 1700656522895,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hog7AiH7Aw",
                "forum": "wSWJpfUWdM",
                "replyto": "HrFURePtxT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Reviewer_XtEq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Reviewer_XtEq"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your reply"
                    },
                    "comment": {
                        "value": "Thank authors for the response. I still would like to keep my evaluations. It looks like DP seems to be enforced via plaintext adaptation of Gaussian mechanism and post-processing. FedLAP may introduce a better data generation algorithm, but this requires more experiment validations. I believe it would be helpful to compare with other data condensation/distillation methods as listed in [1,2]. By the way, [1] actually comes from ICLR 2023."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713094869,
                "cdate": 1700713094869,
                "tmdate": 1700713094869,
                "mdate": 1700713094869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7KeBx62u3e",
                "forum": "wSWJpfUWdM",
                "replyto": "0mUHRo9hOt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up response on the contribution and novelty"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the reply and the time spent reading our response. We would like to follow up on the comments below. While we are approaching the end of the discussion, we remain eager to hear from you and hope you can re-evaluate our work in light of the following contributions.\n\n----\n- **The contribution of the DP part is non-trivial.** \n\nChen et al. have shown that a trivial extension of using DP-SGD to learn synthetic images leads to sub-optimal performance. Our work extends it to federated settings and associates it with non-IID scenarios for the first time, providing the same privacy protection as FedAvg with DP guarantees while achieving better utility. This aspect is crucial and has been overlooked by prior works, e.g., the ones suggested by the reviewer.\n\n[1] Dingfan Chen, Raouf Kerkouche, and Mario Fritz. Private set generation with discriminative information. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n\n----\n- **Theoretical and empirical results**\n\nIn addition to the formulation above, we have provided formal privacy analysis in Sec. A and extensive empirical results in Sec. 5. The former analysis builds a starting point for further fundamental understanding of DP+sample synthesis in FL. The latter can serve as a solid baseline for future non-IID federated optimization and could be of practitioners' interest."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729594472,
                "cdate": 1700729594472,
                "tmdate": 1700732322520,
                "mdate": 1700732322520,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gLJyISR9CJ",
                "forum": "wSWJpfUWdM",
                "replyto": "0mUHRo9hOt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update on additional related work"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer again for referring us to the related work published in ICLR 2023 (our first version was released roughly at the same as its arXiv version). \n\nWe particularly find it interesting, and many proposed components could be beneficial when integrated into our **non-DP** settings, such as conditional initialization and dynamic weight assignment. However, **any data-dependent operations introduce additional privacy risks, and the privacy protection aspect is missing in the paper**. It remains unclear how to incorporate DP into Liu et al. while our method provides rigorous DP guarantees with proper theoretical analysis, clearly outlining our contribution. We will discuss and empirically compare FedLAP to it in our paper."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733660755,
                "cdate": 1700733660755,
                "tmdate": 1700733739110,
                "mdate": 1700733739110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E5RFBAduIg",
            "forum": "wSWJpfUWdM",
            "replyto": "wSWJpfUWdM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2109/Reviewer_SbQA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2109/Reviewer_SbQA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel federated learning (FL) algorithm, namely FedLAP-DP, to address the drawback of bias in global optimization in traditional FL. The approach involves generating synthetic data resembling real data on the client side and substituting local gradients with these synthetic samples during transmission to the central server to approximate the global loss landscape. The central server then iterates using these synthetic samples, thus mitigating the bias in global optimization. Additionally, differential privacy (DP) is employed to protect the privacy of synthetic data of clients. This idea is innovative, and the writing quality is also acceptable."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The approach proposed in this paper involves generating synthetic data resembling real data on the client side and then leverages the synthetic data to update the local model,  thereby reducing the negative impact of DP on the training.\n2. To control the communication cost, the size of the synthetic dataset is much smaller than the real client dataset. Thus, in the local training, the proposed approach leverages a small dataset to update a local model with a satisfactory performance.\n3. In addition to applying this approach to the DP setting, it can also address the issue of data heterogeneity in FL.\n4. Extensive experimental results demonstrate that FedLAP-DP outperforms the traditional approaches with faster convergence under the different DP settings."
                },
                "weaknesses": {
                    "value": "1. I have a question regarding the generation of synthetic data and the iterations performed by the central server. In the case of non-iid data distribution, are the synthetic samples submitted by the clients to the central server consistent with the original data distribution? If so, referring to Algorithm 1, would there still be bias in the iterations conducted by the central server? Could you please provide a detailed explanation of the algorithm design on how the synthetic data is generated?\n2. Please further explain the parameters set that appeared in Sec.5 EXPERIMENT Part 5.1.\n3. It would be better to remark on each curve in the experimental figures.  Some notations are not clear.\n4. The baselines used in this paper are too simple. Some advanced DP-FL baselines should be included in this paper, such as [1], [2], [3] and etc.\n\nThe mentioned references are as follows: \n[1] Skellam mixture mechanism: a novel approach to federated learning with differential privacy\n[2] Dpis: An enhanced mechanism for differentially private SGD with importance sampling\n[3] PrivateFL: Accurate, Differentially Private Federated Learning via Personalized Data Transformation"
                },
                "questions": {
                    "value": "The following comments should be addressed.\n1. I have a question regarding the generation of synthetic data and the iterations performed by the central server. In the case of non-iid data distribution, are the synthetic samples submitted by the clients to the central server consistent with the original data distribution? If so, referring to Algorithm 1, would there still be bias in the iterations conducted by the central server? Could you please provide a detailed explanation of the algorithm design on how the synthetic data is generated?\n2. Some notations are not clear. For example, I cannot see the effect of indexes j and l in Algorithm 1.\n3. Please further explain the parameters set that appeared in Sec.5 EXPERIMENT Part 5.1.\n4. It would be better to remark on each curve in the experimental figures. \n5. Some advanced DP-FL baselines should be included in this paper, such as [1], [2], [3] and etc. \n\nThe mentioned references are as follows: \n[1] Skellam mixture mechanism: a novel approach to federated learning with differential privacy\n[2] Dpis: An enhanced mechanism for differentially private SGD with importance sampling\n[3] PrivateFL: Accurate, Differentially Private Federated Learning via Personalized Data Transformation"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2109/Reviewer_SbQA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831095412,
            "cdate": 1698831095412,
            "tmdate": 1699636143320,
            "mdate": 1699636143320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h1xBwRK6JE",
                "forum": "wSWJpfUWdM",
                "replyto": "E5RFBAduIg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SbQA"
                    },
                    "comment": {
                        "value": "We are thankful for the reviewer\u2019s feedback and the raised concerns. We now address the concerns below and also hope our response will encourage the reviewer to re-evaluate the score in light of the responses provided.\n\n----\n**Q1: Are the synthetic samples submitted by the clients to the central server consistent with the original data distribution?**\n\nNo, we do not target realistic data generation but the critical information necessary for training (i.e., the loss landscape approximation). As the provided visualization in Figure 12 and 13 of the Appendix, the generated images are notably different in appearance from the real data.\n\n----\n**Q2: If so, referring to Algorithm 1, would there still be bias in the iterations conducted by the central server?**\n\nNo. As discussed in the previous response and Sec. 3-1, the bias comes from the biased local optimization and naive weight average. Instead, our method communicates the critical training information and enables unbiased optimization on the server side (Sec. 4-3). However, we also admit that the approximation could affect the performance, which leaves space to improve, as discussed in Sec. 6.\n\n----\n**Q3: Could you please provide a detailed explanation of the algorithm design on how the synthetic data is generated?**\n\nAs shown in Algorithm 1, the process begins with sampling a batch of real data and calculating the mean gradients $g^\\mathcal{D}$. These gradients serve as a static objective for training synthetic images, as specified in Eq. 8 and 17 (see Appendix). Subsequently, synthetic images are processed through the identical network to compute their mean gradients. Based on Eq. 8, the synthetic images are then adjusted to align their gradients with $g^\\mathcal{D}$. This procedure repeats \u200b$R_b$ times for every batch of real data. We are readily available to provide further clarification if needed.\n\n----\n**Q4: The parameter set that appeared in Sec.5 EXPERIMENT Part 5.1.**\n\nIs the reviewer referring to the set ($R_i$ , $R_l$ , $R_b$, $r$)? If so, we offer additional explanations to the descriptions in Algorithm 1 and Sec. 4 as follows.\n\n$R_i$ denotes the number of sampled training trajectories, i.e., the empirical approximation of the expectation in Eq. 7. Within each sampling, $r$ and $R_l$ control the length of the trajectory and step size, respectively. Lastly, $R_b$ controls how many times gradients generated by a real batch will be used to update the synthetic images to deliver the same gradients.\n\n----\n**Q5: It would be better to remark on each curve in the experimental figures. Some notations are not clear.**\n\nWe kindly request that the reviewer provide additional details regarding any specific notations found to be unclear. We believed we had ensured all figures included clear legends and that font sizes were adjusted for easy readability. We would welcome more specific feedback to improve our presentation further.\n\n----\n**Q6: The baselines used in this paper are too simple. The reviewer suggested three related works to compare.**\n\n[1] and [3] aim at a different problem, making them incomparable to our FedLAP. [1] solves the performance degradation caused by the discretization of multi-party computing (MPC), while our work does not consider MPC. [3] targets personalized federated learning and assumes individual objectives for different clients, while we consider general federated learning and a unified global objective. We have discussed the difference in Sec. 3-1. Concerning the settings, they are incomparable to our FedLAP.\n\n[2] mitigates DP performance degradation by importance sampling. Since it directly works on data, it may be complementary and could be advantageous when integrated into our methodology. However, it does not diminish our contribution to non-IID DP federated optimization based on synthetic image generation.  We will discuss the relevant works in our paper.\n\n**Edit**: We updated the response to clarify the difference and explain why [1] and [3] are incomparable to our method.\n\n----\n**Q7: The effect of indexes $j$ and $l$ in Algorithm 1.**\n\nWe apologize for the confusion and will clarify it in the paper. \n\nThe indices were omitted for conciseness. In particular, i and j are related to the update status of synthetic images. It can be denoted as $S^{i, j}_k$ with $i$ being the number of training trajectories that the synthetic images have observed, $j$ the number of updates given a real batch, and $k$ the index of local clients. For $l$, it denotes the number of steps an inner loop will run."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173259193,
                "cdate": 1700173259193,
                "tmdate": 1700517605289,
                "mdate": 1700517605289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Me1YbVMMNw",
                "forum": "wSWJpfUWdM",
                "replyto": "E5RFBAduIg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2109/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nSince we are approaching the end of the discussion phase, we kindly remind you that you can re-evaluate and adjust your score if our response solves your concerns. We will also be more than happy to answer any further questions and comments.\n\nIn addition, we have revised the manuscript according to the reviewers' comments. All revision has been marked in red. Specifically,\n\n1. (Reviewer SbQA) Algorithm 1 and the corresponding descriptions have been revised for better readability\n2. (Reviewer XtEq, vGpK) We replace the word \"sanitize\" with \"clip\" to explicitly refer to DP operations and rephrase the corresponding sentences.\n3. (Reviewer vGpK) We add a brief conclusion to Sec. 4.4 to summarize the result of our privacy analysis."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656449265,
                "cdate": 1700656449265,
                "tmdate": 1700656449265,
                "mdate": 1700656449265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]