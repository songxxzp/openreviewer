[
    {
        "title": "Training Unbiased Diffusion Models From Biased Dataset"
    },
    {
        "review": {
            "id": "JopdKhC6R4",
            "forum": "39cPKijBed",
            "replyto": "39cPKijBed",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5163/Reviewer_irEP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5163/Reviewer_irEP"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of dataset bias in diffusion models. Authors introduces a novel method called Time-dependent Importance reWeighting (TIW). This method, leveraging a time-dependent density ratio, offers improvements in both reweighting and score correction processes. One of its major contributions is making the weighting objective more manageable and drawing a theoretical link to traditional score-matching objectives that aim for unbiased distributions. Experiments shows TIW outperforms other baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem this paper tries to address is important.\n\n2. The methodology introduced is innovative.\n\n3. This research creates a theoretical bridge between the proposed objective and the conventional score-matching objectives, offering a robust guarantee.\n\n4. Extensive testing of the method across diverse datasets and under varied conditions underscores its robustness and versatility.\n\n5. Preliminary results from the experiments are encouraging."
                },
                "weaknesses": {
                    "value": "1. Introducing time-dependent methods might lead to heightened computational complexities in contrast to their time-independent counterparts. It would be beneficial to include a comparative analysis or discussion addressing this aspect.\n\n2. One pivotal query that emerges is whether the efficiency and overall success of the proposed method are intrinsically tied to the performance of the time-dependent discriminator. It would be enlightening to elucidate this relationship.\n\n3. There's a pertinent concern regarding the model's robustness. If the model isn't aptly regularized or subjected to limited training data, it might inadvertently heighten the risk of overfitting.\n\n4. The research paper titled \"Fair Diffusion\" by Friedrich et al. [1] is notably aligned with the theme of fairness in diffusion models. However, its conspicuous absence in the current work, either as a benchmark or a referenced study, is intriguing. The authors might want to consider explicating their rationale behind not incorporating it as a baseline or offering a comparative analysis. \n\n\n[1] Friedrich et al. \"Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness.\""
                },
                "questions": {
                    "value": "Can this method be transferred to other models like GAN-based model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5163/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5163/Reviewer_irEP",
                        "ICLR.cc/2024/Conference/Submission5163/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498131007,
            "cdate": 1698498131007,
            "tmdate": 1700622557142,
            "mdate": 1700622557142,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qxzuazgOLy",
                "forum": "39cPKijBed",
                "replyto": "JopdKhC6R4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the questions and feedback"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's sincere and helpful feedback. We answer for reviewer\u2019s comments below.\n\n**Q1. [Computation Cost]** *Introducing time-dependent methods might lead to heightened computational complexities in contrast to their time-independent counterparts. It would be beneficial to include a comparative analysis or discussion addressing this aspect.*\n\n**Response to Q1.**\n\nThank you for addressing important issues. We compare the computation cost of time-independent importance reweighting (IW-DSM), and time-dependent importance reweighting (TIW-DSM) in terms of 1) training time, 2) training memory, 3) sampling time, and 4) sampling memory.\n\nBoth methods require the evaluation of the discriminator for diffusion model training. However, TIW demands 34% more training time and 13% more training memory compared to IW-DSM. This is because we need to compute a new term $\\nabla \\log{w^t_{\\phi}}(x_t)$ using the automatic gradient module in the PyTorch, while IW-DSM only requires a feed-forward value. However, once the training is complete, the discriminator is not used for sampling, so the sampling time & memory remain the same. We conducted tests on RTX 4090 * 4 Cores on CIFAR-10 experiments. We also reflect it in Appendix D.5. Note that the training of time-dependent discriminator is negligibly cheap, converging around 10 minutes with 1 RTX 4090.\n\n||IW-DSM|TIW-DSM|\n|---|---|---|\n|Training time|0.26 Second/Batch |0.34 Second/Batch|\n|Training memory|13,258 MiB*4Core|15,031 MiB*4Core|\n|Sampling time| 7.5 Minute/50k |7.5 Minute/50k|\n|Sampling memory| 4,928 MiB*4Core |4,928 MiB*4Core |"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230285667,
                "cdate": 1700230285667,
                "tmdate": 1700230285667,
                "mdate": 1700230285667,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YkG9e5OlUZ",
                "forum": "39cPKijBed",
                "replyto": "JopdKhC6R4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5163/Reviewer_irEP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5163/Reviewer_irEP"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for your detailed rebuttals. After reading all the rebuttals, including those from other reviewers, I would like to adjust my initial score from 'borderline accept' (6) to 'weak accept' (7). Unfortunately, there isn't a specific selection for 'weak accept,' as the next available option is 'accept' with a score of 8. Therefore, I have decided to increase my confidence in my assessment to 4. \n\nTo all ACs and SACs, please consider my suggestions as a 'weak accept.' Thank you for your understanding."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700622818235,
                "cdate": 1700622818235,
                "tmdate": 1700623239056,
                "mdate": 1700623239056,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NWOQDnZXti",
            "forum": "39cPKijBed",
            "replyto": "39cPKijBed",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5163/Reviewer_WcK7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5163/Reviewer_WcK7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method to train an unbiased diffusion model with a weak supervision setting based on a sampling process that they called time-independent importance reweighting. They first analyzed why they should use the time-independent importance reweighting, then how to apply score matching to the reweighting process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper builds on successful recent advances in diffusion models, which excel in high-fidelity image generation within generative learning frameworks. Thus, the proposed TIW method shows promise for diverse applications, including text-to-image generation, image-to-image translation, and video generation.\n2. The paper addresses the understudied issue of dataset bias in generative modeling, introducing a method to enhance the fairness and reliability of ML systems.\n3.The paper establishes a theoretical equivalence between the proposed method and existing score-matching objectives from unbiased distributions, which could provide a strong foundation for the validity of their approach."
                },
                "weaknesses": {
                    "value": "1. The paper builds upon a large body of previous work, which while showing the paper\u2019s relevance, also means that any limitations or issues in those previous works could potentially affect the validity and effectiveness of TIW. The actually implementation process is similar to previous work on GAN.\n2. The paper shows that it mitigates the bias in the dataset, but I am not sure whether that method would influence the overall generation quality. It is important to see detailed qualitative and quantitative analysis."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698538972252,
            "cdate": 1698538972252,
            "tmdate": 1699636511231,
            "mdate": 1699636511231,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IRYXmOgwOg",
                "forum": "39cPKijBed",
                "replyto": "NWOQDnZXti",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the questions and feedback"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's feedback. We answer for reviewer's comments below.\n\n**[Clarification on miscommunicated point]**\n\nIn response to the reviewer's summary, we would like to kindly point out that there are some miscommunicated points. The proposed method, TIW-DSM in eq. (10) is based on time-dependent importance reweighting, which is distinctly different from IW-DSM which leverages time-independent importance reweighting in eq. (7). The reviewer seems to think of our method as IW-DSM, but IW-DSM is the baseline method. We will consider distinguishing the two methods with more easily understandable names for the final draft."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229403996,
                "cdate": 1700229403996,
                "tmdate": 1700229403996,
                "mdate": 1700229403996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4AyrnThbdg",
            "forum": "39cPKijBed",
            "replyto": "39cPKijBed",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5163/Reviewer_6Mt9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5163/Reviewer_6Mt9"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes time-dependent importance reweighting to mitigate the dataset bias for diffusion training. The proposed method utilizes time-dependent density ratio estimation, which is more precise than the previous time-independent one used in GAN. The authors provide both theoretical understanding and empirical demonstrations of the proposed method's benefits."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is very well-written and easy to follow. I really enjoy the presentation flow and the thought process from Eq.7 to Eq.9 to Eq.10, with clear reasoning behind each change.\n2. The method is intuitive and simple, yet clever and effective. Utilization of importance sampling by time-dependent density ratio estimation (DRE) in diffusion models is novel and natural. DRE becomes more precise as $t$ becomes larger.\n3. I also appreciate the authors' efforts in providing toy examples and figures, which further makes the paper easy to understand.\n4. The problem the paper proposes to study is an important one, as real-world datasets are never short of biases, especially the unannotated ones.\n5. The proposed method's empirical performance is convincing. I also appreciate the inclusion of Eq.7 training numbers for the ablation study."
                },
                "weaknesses": {
                    "value": "I do not have any major complaints. I list my minor questions and suggestions in the Questions section below."
                },
                "questions": {
                    "value": "1. I can see the arguments for time-dependent density ratio estimation the same as the ones for [1] (especially), [2], etc. Thus I suggest the authors make the citations and discussions accordingly. Nonetheless, I agree that the use case of DRE is different.\n2. In theory, the method works if $E_{x_0 \\sim p_{\\textrm{bias}}(x_0|x_t)} [\\nabla\\log p(x_t|x_0)] = \\nabla\\log p_{\\textrm{bias}}^t(x_t)$ and DRE's gradient actually contains $\\nabla\\log p_{\\textrm{data}}^t(x_t)$ information. Both require DRE to be really accurate, which, as the authors demonstrate, is reasonably true only for large $t$. Thus, might I suggest that the authors only train with Eq.10 for large $t$, and revert back (gradually) to standard diffusion training on the biased dataset for small $t$? The intuition is that for small $t$, the model only does denoising, which does not care about dataset biases (the model has already decided on the content it wants to generate). For an intuitive understanding, see Fig.2 in [3]. For a more theoretical explanation see [4], where small $t$'s score field is dominated by one sample, making the score for unbiased the same as biased here.\n3. In Fig.4, every method's performance peaks early and gets worse. This is unexpected for me. Could the authors provide an explanation?\n\n\n[1] Wang et al. \"Diffusion-gan: Training gans with diffusion.\" ICLR 2023.\n\n[2] Arjovsky et al. \"Towards principled methods for training generative adversarial networks.\" ICLR 2017.\n\n[3] Rombach et al. \"High-resolution image synthesis with latent diffusion models.\" CVPR 2022.\n\n[4] Xu et al. \"Stable target field for reduced variance score estimation in diffusion models.\" ICLR 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5163/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5163/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5163/Reviewer_6Mt9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836630490,
            "cdate": 1698836630490,
            "tmdate": 1699636511132,
            "mdate": 1699636511132,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x68nLemY2D",
                "forum": "39cPKijBed",
                "replyto": "4AyrnThbdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the questions and feedback"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for a deep understanding of our work and highly constructive suggestions. We answer the reviewer\u2019s comments below.\n\n**Q1. [Time-dependent density ratio in GANs]** *I can see the arguments for time-dependent density ratio estimation the same as the ones for [1] (especially), [2], etc. Thus, I suggest the authors make the citations and discussions accordingly. Nonetheless, I agree that the use case of DRE is different.*\n\n*[1] Wang et al. \"Diffusion-gan: Training gans with diffusion.\" ICLR 2023.*\n\n*[2] Arjovsky et al. \"Towards principled methods for training generative adversarial networks.\" ICLR 2017.*\n\n**Response to Q1.**\n\nWe appreciate your valuable advice. We acknowledge the existence of several works that employ noise injection tricks in the discriminator input of GANs. We have thoroughly examined the papers you provided guidance on, along with additional relevant papers, and have summarized our findings in Appendix B.4."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227469830,
                "cdate": 1700227469830,
                "tmdate": 1700227469830,
                "mdate": 1700227469830,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zFiibyzik3",
                "forum": "39cPKijBed",
                "replyto": "4AyrnThbdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continued"
                    },
                    "comment": {
                        "value": "**Q2. [Objective function interpolation]** *In theory, the method works if $E_{x_{0}\\sim p_{bias}(x_{0}|x_{t})} [\\nabla \\log p(x_{t}|x_{0})] = \\nabla \\log p_{bias}^{t}(x_t)$ and DRE's gradient actually contains $\\nabla \\log p_{data}^{t}(x_t)$ information. Both require DRE to be really accurate, which, as the authors demonstrate, is reasonably true only for large $t$. Thus, might I suggest that the authors only train with Eq.10 for large $t$, and revert back (gradually) to standard diffusion training on the biased dataset for small $t$? The intuition is that for small $t$, the model only does denoising, which does not care about dataset biases (the model has already decided on the content it wants to generate). For an intuitive understanding, see Fig.2 in [3]. For a more theoretical explanation see [4], where small $t$'s score field is dominated by one sample, making the score for unbiased the same as biased here.*\n\n*[3] Rombach et al. \"High-resolution image synthesis with latent diffusion models.\" CVPR 2022.*\n\n*[4] Xu et al. \"Stable target field for reduced variance score estimation in diffusion models.\" ICLR 2023.*\n\n**Response to Q2.**\n\nWe appreciate your sincere suggestions. We acknowledge your suggestion makes sense and have conducted experiments in Appendix E.7. We attempted hard truncation of two objectives at a specific time $\\tau$ as described in eq. (54), which utilize $(0,\\tau)$ for $L_{DSM}$ objective, and $(\\tau, T)$ for $L_{TIW-DSM}$ objective. Contrary to the intuition, the performance in Figure 27 is just interpolated. One possible reason is the negative effects of hard truncation on the objectives. Giving a sharp difference at the boundary of $\\tau$ might not have been favorable for the generation path. As you suggested, we are considering experiments involving the gradual interpolation of the two objectives and will include the results in the final draft."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228124894,
                "cdate": 1700228124894,
                "tmdate": 1700229162285,
                "mdate": 1700229162285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zKCkyH3974",
                "forum": "39cPKijBed",
                "replyto": "4AyrnThbdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continued"
                    },
                    "comment": {
                        "value": "**Q3. [Overfitting on CIFAR-10 experiments]** *In Fig.4, every method's performance peaks early and gets worse. This is unexpected for me. Could the authors provide an explanation?*\n\n**Response to Q3.**\n\nAs we described in Appendix C, some literature reports that overfitting is correlated with training configurations in network architecture, EMA, and diffusion noise scheduling. One of our own observations is that overfitting becomes more pronounced as the number of data decreases, as shown in Figure 10. \n\nFigure 4-(e) indicates that TIW-DSM shows less overfitting compared to IW-DSM. IW-DSM can not utilize the information from $D_{bias}$ effectively because time-independent density ratio provides extremely small reweighting value as shown in Figure 8-(a). On the other hand, TIW-DSM utilizes the information from $D_{bias}$ more effectively as shown in Figure 8-(b-d)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228335989,
                "cdate": 1700228335989,
                "tmdate": 1700228359694,
                "mdate": 1700228359694,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w0Z8oGFXKt",
                "forum": "39cPKijBed",
                "replyto": "4AyrnThbdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5163/Reviewer_6Mt9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5163/Reviewer_6Mt9"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for addressing my questions, especially in Appendix E.7. The results are indeed surprising to me, and perhaps they suggest that the quality of DRE is actually good when heavily sharing parameters despite the difficulty of the task, or the existing understanding of the score field is incomplete.\n\nI have read other reviewers' comments and the corresponding rebuttal, and I thank the authors for providing detailed responses. I still lean towards acceptance and thus keep my rating."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688163792,
                "cdate": 1700688163792,
                "tmdate": 1700688163792,
                "mdate": 1700688163792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KkNN5wMCuW",
            "forum": "39cPKijBed",
            "replyto": "39cPKijBed",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5163/Reviewer_EYEW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5163/Reviewer_EYEW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a time-dependent importance-sampling objective for diffusion models, to mitigate the bias in the training set. The idea is to estimate the time-dependent importance weights between the bias distribution and the true (unbiased) distribution. The importance-weighted score-matching objective is then translated into a corresponding denoising score-matching objective. Controlled experiments showcase the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper proposes a new denoising score-matching objective to tackle the biases in the training set.\n\n- In contrast to the previous method (Eq.7), the proposed method estimates the time-dependent density ratio, which is arguably easier to estimate.\n\n- Experiments on CIFAR-10/CIFAR-100/FFHQ demonstrate that the proposed method (TIW-DSM) not only outperforms baselines with solely reference unbiased data but also beats the vanilla go-to method IW-DSM."
                },
                "weaknesses": {
                    "value": "- The biggest concern I have is that the proposed methods lack practical significance, and seem tangent to practical settings. The biases problem studies in this paper go away when considering conditional generation (e.g., text-to-image generation). For example, we can generate a batch of male images by conditioning on the \"male\" label, even though the female images are the majority. \n\n\n- Regarding comparison to baseline (IW-DSM), some of the claimed advantages are not rigorous\uff1a(1) On page 4, \"the perturbation provides an infinite number of samples from each distribution.\": note that the perturbed distribution is $p_{data} * N(0, \\sigma_t^2)$, one would still need large amount of data in $p_{data}$ (or $D_{ref}$) in order to estimate quantities related to $p_{data} * N(0, \\sigma_t^2)$. (2) It's true that the accumulated MSE error in the density ratio estimation is smaller in the proposed method, but it introduces an additional erroneous term $\\nabla \\log w$ in the new objective. It could result in an even larger error compared to the baseline.\n\n- Fig. 8.(a): I think a better density ratio estimator can get away from this over-confidence issue. A simple method is to regularize the classifier such that it cannot perfectly distinguish the two distributions.\n\n\n- A natural baseline missing in the paper is asking generative models (e.g. SDXL) to generate more unbiased data for the considered small datasets and train the model in the vanilla DSM way."
                },
                "questions": {
                    "value": "- For the experiments, do the authors generate 50k images to calculate the FID score? What's the architecture and noisy schedule used in this paper? The current FID score seems quite worse off.\n\n- How can this method be used in practical text-to-image generation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699021642544,
            "cdate": 1699021642544,
            "tmdate": 1699636511020,
            "mdate": 1699636511020,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G5YqgTocdt",
                "forum": "39cPKijBed",
                "replyto": "KkNN5wMCuW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the questions and feedback"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for the constructive and thoughtful feedback. We answer for reviewer\u2019s comments below.\n\n**Q1. [Connection to text-to-image model]** *The biggest concern I have is that the proposed methods lack practical significance, and seem tangent to practical settings. The biases problem studies in this paper go away when considering conditional generation (e.g., text-to-image generation). For example, we can generate a batch of male images by conditioning on the \"male\" label, even though the female images are the majority*.\n  \n \n**Response to Q1.**\n\n**[Mitigate bias in text-to-image models]** Thank you for your feedback. From the perspective of providing a text-to-image foundation model for practical use, addressing bias becomes a more crucial consideration. For instance, providing \u201cnurse\u201d as a condition results in the generation of only female nurses as shown in [a1] and Figure 28 in our paper. The proposed method introduces an approach to mitigate latent bias, so the proposed method could be applicable in this case. We constructed a reference dataset and fine-tuned the Stable Diffusion on the \u201cnurse\u201d prompt to mitigate bias on gender. The fine-tuned Stable Diffusion with the TIW-DSM objective successfully generates a certain proportion of male nurses as shown in Figure 29 (Please refer to Appendix E.8 for detailed explanations). This experiment demonstrates that our method works well in text-to-image diffusion models. In addition to fine-tuning, this approach can be applied to training a text-to-image model from scratch. Constructing a reference set for (prompt, bias) pairs deemed important by society and applying our objective during training, should enable a relatively fair generation.\n\n**[Dataset setting]** Indeed, text-to-image models are trained based on explicit supervision of (text, image) pairs. On the other hand, the weak supervision setting we employ is cost-effective in terms of dataset collection. While we acknowledge the role of general-purpose models like Stable Diffusion, it is also crucial to develop task-specific models starting from the dataset collection phase. It\u2019s worth noting that dataset collection with weak supervision is prevalent in companies [a2]. \n\n[a1] Maggio et al. The Bias problem: Stable Diffusion from https://vittoriomaggio.medium.com/the-bias-problem-stable-diffusion-607aebe63a37\n\n[a2] 23 & me. The Real Issue: Diversity in Genetics Research retrieved from https://blog.23andme.com/articles/the-real-issue-diversity-in-genetics-research."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700223858620,
                "cdate": 1700223858620,
                "tmdate": 1700223858620,
                "mdate": 1700223858620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]