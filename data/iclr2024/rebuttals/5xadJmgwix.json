[
    {
        "title": "Scale-Adaptive Diffusion Model for Complex Sketch Synthesis"
    },
    {
        "review": {
            "id": "GJcFhE7KNw",
            "forum": "5xadJmgwix",
            "replyto": "5xadJmgwix",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission870/Reviewer_SPRZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission870/Reviewer_SPRZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors proposed a strategy to dynamically change the scale of classifier-guidance during the reverse diffusion process for the generation of pixel-based sketches. For this purpose, they empirically designed a scaling indicator and a residual sketch to achieve the optimal guidance scale needed at the current diffusion stage. The scaling indicator is to assess the recognizability and complexity of current generative results. The residual sketch x_rs (x_t, s) is to evaluate the extent to which the guidance scale s will impact the generative process. The guidance scale s is optimized to enforce the residual sketch to be synchronized with the scale indicator. Moreover, they adopted a three-phase sampling strategy to enhance the sketch diversity and quality. Experiments demonstrated the superiority of their approach in the realm of sketch generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposal of scaling indicator and residual sketch is novel and can conduct an effective scale adaptive classifier-guided diffusion process.\n2. The experimental part is convincing, and includes both quantitative and qualitative results. And the ablation study shows the effectiveness of their methods.\n3. The organization of this paper is clear and easy to understand."
                },
                "weaknesses": {
                    "value": "1. The scale adaptive classifier-guided diffusion process seems useful and attractive, however, the scaling indicator and residual sketch are proposed mainly based on their experience in the field of sketch generation. In my opinion, it may rely on the simple structure of sketch. I wonder if similar methods can be applied to other data modalities, such as natural images.\n2. Equation 4 appears to be dubious. I understand the point that the residual sketch should be synchronized with the scale indicator. However, are they on the same numerical scale and is it appropriate to directly apply mean squared error for optimization? I believe that a more rigorous discussion or proof is needed based on their definitions (Formula 2 and 3).\n3. The contribution \u201cthree-phase sampling strategy\u201d has been proven to be effective but it is merely a simple technique for diffusion process and the innovation may be limited.\n4. Using the fraction of stroke pixels to the whole canvas to evaluate the sketch complexity is oversimplified."
                },
                "questions": {
                    "value": "1. The authors mentioned that vector-base approaches are inherently limited when tackling intricate and complex sketches. I would like to understand why pixel-based methods have an advantage over vector-based methods? I hope the authors can provide some intuitive explanations.\n2. In Section 2, the authors claimed that an additional property of pixel-based diffusion modeling is classifier gradient can be used as guidance. So are there any obstacles to using classifier-guidance for vector-based diffusion? It also seems feasible in principle.\n3. In Equ 2, are the settings of the three hyperparameters obtained through experiments or are there some empirical principles?\n4. In Equ. 4, why do different images in the same batch share the same scale?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698545623987,
            "cdate": 1698545623987,
            "tmdate": 1699636013427,
            "mdate": 1699636013427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nj0SUbRiOO",
                "forum": "5xadJmgwix",
                "replyto": "GJcFhE7KNw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SPRZ"
                    },
                    "comment": {
                        "value": "Thank you for your time and critical feedback!\n>The scale adaptive classifier-guided diffusion process seems useful and attractive, however, the scaling indicator and residual sketch are proposed mainly based on their experience in the field of sketch generation. In my opinion, it may rely on the simple structure of sketch. I wonder if similar methods can be applied to other data modalities, such as natural images.\n\nInteresting point! Unfortunately, the model proposed in this work is sketch-specific as one of the key components, i.e., complexity, is measured by the fraction of the *binary* stroke pixels on the whole canvas. Such an assumption is no longer held for natural images. However, perhaps the take-home message is that imposing some explicit quality control over the generation is possible by leveraging scale adaptive classifier guidance. \n\n>Equation 4 appears to be dubious. I understand the point that the residual sketch should be synchronized with the scale indicator. However, are they on the same numerical scale and is it appropriate to directly apply mean squared error for optimization? I believe that a more rigorous discussion or proof is needed based on their definitions (Formula 2 and 3).\n\nThanks for the suggestion. We have now provided the scores of these two terms in Equation 4 at different time steps during optimization in Appendix C in the revised paper. It shows that the numerical scales are on the same level, which is achievable because the parameters (i.e., $\\alpha$, $\\beta$ and $\\gamma$) can adapt the scale indicator $\\varsigma(x_t^{(i)})$ to be on par with the residual sketch term, i.e., ${1\\over HW}\\sum_{HW} x_{rs}(x_t^{(i)}, s)$.\n\n> The contribution \u201cthree-phase sampling strategy\u201d has been proven to be effective but it is merely a simple technique for diffusion process and the innovation may be limited.\n\nThanks. Yes, simple as it might be, this does align with our vision of coming up with a more explainable approach, and we hope you would agree that, the results are quite decent (i.e., it does work).\n\n> Using the fraction of stroke pixels to the whole canvas to evaluate the sketch complexity is oversimplified.\n\nThanks, we were also surprised at how effective this simple strategy is \u2013 this is however sketch-specific (black and white pixels), and chance is it will not generalize to natural images. \n\n>The authors mentioned that vector-base approaches are inherently limited when tackling intricate and complex sketches. I would like to understand why pixel-based methods have an advantage over vector-based methods? I hope the authors can provide some intuitive explanations.\n\nOf course! Intuitively, we seek to sidestep the cost-expensive process of rasterization, i.e., neural line rendering [A], which is a common practice for existing works on vector-based sketch generation [A, B]. Rasterization is often required since the classifier is trained on pixel-format sketches. We will further clarify it in the revised version. \n\n[A] \"Sketch-R2CNN: an RNN-rasterization-CNN architecture for vector sketch recognition.\" TVCG 2020.\n[B] \"SketchKnitter: Vectorized Sketch Generation with Diffusion Models.\" ICLR 2023.\n\n> In Section 2, the authors claimed that an additional property of pixel-based diffusion modeling is classifier gradient can be used as guidance. So are there any obstacles to using classifier-guidance for vector-based diffusion? It also seems feasible in principle.\n\nThanks. The trouble here is that the classifier is implemented by CNN, which is obtained by training on (rasterized) sketch images. Hence, the generated sketches (in vector format) will have to be converted back to rasterized images before applying the classifier-guidance, which is costly. This process can be even more expensive since it demands multiple classifier-guided sampling during our scale optimization at each time step. \n\n> In Equ 2, are the settings of the three hyperparameters obtained through experiments or are there some empirical principles?\n\nWe perform a greedy search strategy to determine the optimal choice of $\\alpha$, $\\beta$ and $\\gamma$. We have added more details in the implementation and further provided ablative results when varying these hyperparameters in Appendix B.\n\n> In Equ. 4, why do different images in the same batch share the same scale?\n\nThanks! In our implementation, a batch version scale is utilized in order to reduce the computational cost, due to limited GPU resources. Ideally, each sketch generation should have its own scale schedule. However, this can be readily achieved by setting the batch size to one, but at a higher cost as noted. We have provided more discussions on this in the implementation details of the paper revision."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644627429,
                "cdate": 1700644627429,
                "tmdate": 1700644627429,
                "mdate": 1700644627429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GuYu77CW60",
            "forum": "5xadJmgwix",
            "replyto": "5xadJmgwix",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission870/Reviewer_t4As"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission870/Reviewer_t4As"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel sketch generation method based on classifier-guided diffusion models. Specifically, the authors propose a scale-adaptive classifier-guided diffusion model, which achieves a delicate balance between recognizability and complexity in generated sketches. In addition, the authors also propose a three-phase sampling strategy to enhance sketch diversity and quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The authors analyze the impact of guidance scale on diffusion-based sketch generation tasks and propose a scale adaptive classifier-guided sampling method to achieve a delicate balance between recognizability and complexity in generated sketches.\n+ The authors point out the impact of unconditional guidance and classifier guidance on generating sketches in diffusion models and propose a three-phase sampling strategy.\n+ Quantitative and qualitative experiments have shown that the proposed method outperforms existing sketch generation methods."
                },
                "weaknesses": {
                    "value": "+ From Figure 2 and Section 4.1, we can see that the input of the sketch classifier is a clean sketch estimated from noisy images. However, the author mentioned in Section 4.1 that the classifier is trained by using noisy sketches, which is obviously contradictory.\n\n+ From Table 2, we can see that using unconditional guidance can increase the diversity of generated sketches. However, from Figure 5, it can be seen that under the same random seeds, the images generated by different categories during the warm-up sampling stage are the same, and some strokes will remain in the final generation result. Therefore, it remains to be discussed whether the warm-up sampling is truly effective for sketch generation tasks."
                },
                "questions": {
                    "value": "The default size of the produced sketches is set to 64\u00d764. Can this method still generate sketches well at higher resolutions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission870/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission870/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission870/Reviewer_t4As"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654003390,
            "cdate": 1698654003390,
            "tmdate": 1699636013351,
            "mdate": 1699636013351,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eqKPaSSJmR",
                "forum": "5xadJmgwix",
                "replyto": "GuYu77CW60",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission870/Reviewer_t4As"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission870/Reviewer_t4As"
                ],
                "content": {
                    "title": {
                        "value": "The comment during rebuttal"
                    },
                    "comment": {
                        "value": "Since I don't receive a timely response from the authors during the rebuttal period, I will maintain my original rating score."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640982613,
                "cdate": 1700640982613,
                "tmdate": 1700640982613,
                "mdate": 1700640982613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y4vtC3J1WH",
                "forum": "5xadJmgwix",
                "replyto": "GuYu77CW60",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t4As"
                    },
                    "comment": {
                        "value": "Thank you for the critical feedback. Below are our replies to each of your questions. \n> From Figure 2 and Section 4.1, we can see that the input of the sketch classifier is a clean sketch estimated from noisy images. However, the author mentioned in Section 4.1 that the classifier is trained by using noisy sketches, which is obviously contradictory.\n\nThanks! To clarify, the estimated sketch $x_{0|t}$ at each time step $t$ is not a clean one (notice the blur regions of $x_{0|t}$ in Figure 2), especially in the early stage $x_{0|t}$ can be rather noisy. \n\nTo gain more insights into this matter, we have provided some quantitative results of using the classifier to recognize $x_{0|t}$ at different time steps in Appendix D. For comparison, we also report the classification results on $x_t$ (i.e., noisy sketches). We can see that the classifier can achieve the same level of top-1 accuracy both on $x_t$ and $x_{0|t}$.\n\n\n\n> From Table 2, we can see that using unconditional guidance can increase the diversity of generated sketches. However, from Figure 5, it can be seen that under the same random seeds, the images generated by different categories during the warm-up sampling stage are the same, and some strokes will remain in the final generation result. Therefore, it remains to be discussed whether the warm-up sampling is truly effective for sketch generation tasks.\n\nGood point! Actually, warm-up sampling can indeed increase diversity of the holistic structure of the final generated sketches, when compared to the alternative of applying classifier guidance at the beginning, as shown in Table 2(b). \n\nIn Figure 5, we meant to show that classifier guidance can be applied starting at the second stage, i.e., scale adaptive sampling, where the model can adapt to whatever the overall shape (diversity enlarged) formed at the first warm-up stage. \n\n> The default size of the produced sketches is set to 64\u00d764. Can this method still generate sketches well at higher resolutions?\n\nSure, we can change the default resolution to a higher one, and we have conducted additional experiments of generating sketches in resolution of $256\\times 256$ on the same subset of 30 categories. Results show that the quality of the generated sketch remains at the same level (FID 3.76)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644076442,
                "cdate": 1700644076442,
                "tmdate": 1700644076442,
                "mdate": 1700644076442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IioQ1DVaTA",
            "forum": "5xadJmgwix",
            "replyto": "5xadJmgwix",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission870/Reviewer_bpbS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission870/Reviewer_bpbS"
            ],
            "content": {
                "summary": {
                    "value": "This paper divides the diffusion denoising process into three phases. The first and the last phases are unconditional generation, the middle phase is conditioned on the classifier guidance. For the middle phase, this paper proposes an extension block for classifier-guided diffusion models to perform scale adaptive classifier-guided sampling on the task of pixel-level sketch generation, addressing the challenge of dynamically optimizing the guidance scale for classifier-guided diffusion. This method achieves a balance between \"recognizability\" and \"complexity\" in the generated sketches. Experiments are on the QuickDraw dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The three-phase sampling strategy can maintain sketch diversity and quality.\n\n2. The idea of dynamically optimizing the guidance scale is reasonable and prospective.\n\n3. Overall, the presentation and writing are easy to follow, and the experiment is detailed."
                },
                "weaknesses": {
                    "value": "1. The main weakness is that the proposed block is specialized for classifier-guided diffusion. However, classifier-guided diffusion models are not as popular and powerful as those multimodal diffusion models using cross-attention. This limits the impact and universality of the proposed block.\n\n2. The idea of scale adaptive classifier-guided sampling is sound and this method achieves a balance between \"recognizability\" and \"complexity\". However, this also fixes the \"recognizability\" and \"complexity\" of the results. I mean, in classifier-guided diffusion models, users can adjust the guidance scale to trade off diversity for fidelity. But the proposed adaptive method can not.\n\n3. The implementation details are naive such as the complexity c(x0|t) using $L_0$ norm, and the determination equotion for $t_w$ and $t_d$."
                },
                "questions": {
                    "value": "1. I wonder whether the SGD process employed to obtain the optimal value of guidance scale s at each timestep t by minimizing Lt(s) is convergent. There is no further exploration.\n\n2. Also about the SGD process. In equation 4,\"N is the number of sketches generated within a sampling batch\". But why should the sketches in the same batch share the same guidance scale $s$? I mean, $s$ should be independent for each sketch in one batch.\n\n3. About the optimization objective Lt(s), which is \"intuitive\" but not from mathematical proof. This optimization objective seems questionable. \n\n4. How to you determine the parameters \u03b1 = 1.0, \u03b2 = 0.2, and $\\gamma$ = 0.02?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission870/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission870/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission870/Reviewer_bpbS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678546916,
            "cdate": 1698678546916,
            "tmdate": 1699636013261,
            "mdate": 1699636013261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rbmDux4X4t",
                "forum": "5xadJmgwix",
                "replyto": "IioQ1DVaTA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bpbS"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments! \n> The main weakness is that the proposed block is specialized for classifier-guided diffusion. However, classifier-guided diffusion models are not as popular and powerful as those multimodal diffusion models using cross-attention. This limits the impact and universality of the proposed block.\n\nThanks! Indeed, cross-modal diffusion models, such as stable diffusion, are more appealing than classifier-guided models for controllable generation. However, our work enables a fine-tuned control of the sketch generation to achieve a complexity and recognizability balance by adaptive scaling and the residual sketch, which is currently under-explored by existing diffusion models. To explore if such a finer tuning strategy is applicable to cross-modal diffusion models will be our future endeavor.\n\n> The idea of scale adaptive classifier-guided sampling is sound and this method achieves a balance between \"recognizability\" and \"complexity\". However, this also fixes the \"recognizability\" and \"complexity\" of the results. I mean, in classifier-guided diffusion models, users can adjust the guidance scale to trade off diversity for fidelity. But the proposed adaptive method can not.\n\nGreat point! Although the vanilla classifier-guided diffusion model offers users the \"flexibility\" to trade off diversity for fidelity, this process is cumbersome or even impossible in practice as revealed in Figure 1. Users have to try numerous choices for each different class. And there is no way to identify if the optimal or even nearby optimal is achieved. In contrast, our proposed method automates scale searching. Importantly, the whole process is explainable due to the interpretability of the scale indicator and the visibility of the devised residual sketch, as shown in Figure 6.\n\n> The implementation details are naive such as the complexity $c(x_{0|t})$ using $L_0$ norm, and the determination equotion for $t_w$ and $t_d$.\n\nThanks, we think this is rather neat, and most importantly matches our vision of enabling a more explainable approach. Results too are fairly convincing, we hope. \n\n> I wonder whether the SGD process employed to obtain the optimal value of guidance scale s at each timestep t by minimizing Lt(s) is convergent. There is no further exploration.\n\nGreat suggestion! We have now provided the loss values at different time steps during scale adaptive sampling in Appendix C. We can see that, given a scaling indicator $\\varsigma(x_t^{(i)})$ as target, the fraction of pixel changes, i.e., ${1\\over HW}\\sum_{HW} x_{rs}(x_t^{(i)}, s)$, is forced to approach the target by optimizing the scale $s$, hence the loss $L_{t}(s)$ is gradually converged to zero.\n\n\n> Also about the SGD process. In equation 4,\"N is the number of sketches generated within a sampling batch\". But why should the sketches in the same batch share the same guidance scale  $s$? I mean,  $s$  should be independent for each sketch in one batch.\n\nGood spot! It is correct that $s$ should ideally be independent for each sketch. However, in order to speed up training and the later generation process (given the limited computation resource), we then sought to optimize $L_t(s)$ in a batch, as we described in the implementation details. Even with this compromised choice, our model can still achieve superior generation results over the baseline methods.\n\n> About the optimization objective Lt(s), which is \"intuitive\" but not from mathematical proof. This optimization objective seems questionable.\n\nThanks. This objective function could be interpreted as a regression problem, which synchronizes the residual sketch to the scaling indicator, thus determining the optimal scale $s$. Intuitively, minimizing $L_{t}(s)$ is to enforce the fraction of pixel changes reflected by the residual sketch, i.e., ${1\\over HW}\\sum_{HW} x_{rs}(x_t^{(i)}, s)$, can conform to the scaling indicator, i.e., $\\varsigma(x_t^{(i)})$, which sets a goal of balanced complexity and recognizability for each sampling step.\n\n> How to you determine the parameters $\\alpha$ = 1.0, $\\beta$ = 0.2, and  $\\gamma$  = 0.02?\n\nThe optimal parameters are determined by performing greedy search on a small validation set, which is composed of sketches generated by our model. Specifically, we generate 1k sketches per class by conducting classifier guidance on the obtained unconditional DDPM under different configurations. We have further clarified this in Appendix B in our revised version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644482671,
                "cdate": 1700644482671,
                "tmdate": 1700644482671,
                "mdate": 1700644482671,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DlpEoH5CKL",
                "forum": "5xadJmgwix",
                "replyto": "rbmDux4X4t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission870/Reviewer_bpbS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission870/Reviewer_bpbS"
                ],
                "content": {
                    "title": {
                        "value": "Q2. Also about the SGD process."
                    },
                    "comment": {
                        "value": "Q2. Also about the SGD process. In equation 4,\"N is the number of sketches generated within a sampling batch\". But why should the sketches in the same batch share the same guidance scale s? \n\nFuther question: Your answer of speeding up training seems questionable. So have you tried to use batchsize=1? And and how about the result, would it improve? Does the batchsize N affect the results?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645740993,
                "cdate": 1700645740993,
                "tmdate": 1700645740993,
                "mdate": 1700645740993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "71RO5p92c0",
            "forum": "5xadJmgwix",
            "replyto": "5xadJmgwix",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission870/Reviewer_st3k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission870/Reviewer_st3k"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for class-guided sketch synthesis. The base model is an unconditional DDIM model operating in pixel space. During testing, the method uses classifier guidance to generate class-guided sketches. Naively using the same scale for all the time steps often produces low-fidelity or over-sketching samples. To address this issue, the paper proposes the scaling indicator which is computed based on stroke complexity and recognizability. At each sampling step, the classifier guidance scale is adaptively optimized to match the residual sketch with the scaling indicator. All the experiments are done on the QuickDraw dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The results are good qualitatively.\n- The idea of adaptive scale optimization is interesting.\n- The paper reads well and is easy to follow."
                },
                "weaknesses": {
                    "value": "The validation of the idea is lacking:\n- The comparison with classifier-free guidance is missing.\n- In Table 1, what's the classifier guidance scale of DDIM? To validate the idea, it will be good to sweep over all possible classifier guidance scales and show that the proposed method works better than any constant guidance scale.\n- Why not just replace the classifier score in Equation-1 with the scaling indicator? In this case, the residual sketch and scale optimization are not needed anymore."
                },
                "questions": {
                    "value": "What's the effect of \\alpha and \\beta?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission870/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813576868,
            "cdate": 1698813576868,
            "tmdate": 1699636013167,
            "mdate": 1699636013167,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FHTMZZoXKE",
                "forum": "5xadJmgwix",
                "replyto": "71RO5p92c0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission870/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission870/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer st3k"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments! We respond to your specific questions below. Hope our replies and revisions have addressed your concerns.\n> The comparison with classifier-free guidance is missing.\n\nThanks! We have now included the classifier-free diffusion guidance (CFDG) results in Table 1 in the revision. We can see that our model can also achieve better sample quality than CFDG, i.e., FID score 3.08 vs 3.75 (this is the best result with the optimal guidance strength $\\omega = 2$). Note: due to limited time and compute, we can only provide results on 30 categories for now. Results of the full 345 categories will be added in the final version.\n\n\n> In Table 1, what's the classifier guidance scale of DDIM? To validate the idea, it will be good to sweep over all possible classifier guidance scales and show that the proposed method works better than any constant guidance scale.\n\nGot it. In Table 1, the scale of classifier guidance used for DDIM was set to 0.4, which is the optimal constant scale determined by greedy search. We here further show DDIM results with some alternative choices of scale. We can see that our model can consistently outperform DDIM with various constant scales. We have also clarified how the DDIM guidance scale is selected in the revised paper.\n\n||  FID | Prec | Rec |\n|--|:--:|:--:|--|\n|DDIM (s=0.3)| 4.21 | 0.66  | 0.35 |\n|DDIM (s=0.4, *our default*)| 4.08 | 0.71 | 0.30 |\n|DDIM (s=0.5)| 4.13  |  0.68 | 0.27 |\n|DDIM (s=0.6)| 4.37 | 0.68  | 0.21 |\n|Ours| 3.08 | 0.68  | 0.35 |\n\n\n> Why not just replace the classifier score in Equation-1 with the scaling indicator? In this case, the residual sketch and scale optimization are not needed anymore.\n\nInteresting point. We tried, however this did not work. Results obtained mostly contain sparse dots or completely blank. The reason, we think, is that when using only the scaling indicator but no classifier gradient as a part of the guidance, there will be no control towards the desired category. Moreover,  the *new* guidance will corrupt the estimated noise at each step.\n\n> What's the effect of \\alpha and \\beta?\n\nAs described in the manuscript (penultimate line page 4) , $\\alpha$ and $\\beta$ are used to balance the impacts between the complexity $c(x_{0|t})$ and recognizability $f(x_{0|t})$. To gain more insights into how $\\alpha$ and $\\beta$ impact the generation results, we provide ablative results with different settings in Figure 7(a) of Appendix B."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission870/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644345083,
                "cdate": 1700644345083,
                "tmdate": 1700644345083,
                "mdate": 1700644345083,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]