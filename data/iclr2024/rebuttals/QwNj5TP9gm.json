[
    {
        "title": "Evidential Conservative Q-Learning for Dynamic Recommendations"
    },
    {
        "review": {
            "id": "yL1Cvq4kpm",
            "forum": "QwNj5TP9gm",
            "replyto": "QwNj5TP9gm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4158/Reviewer_gFnt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4158/Reviewer_gFnt"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the Evidential Conservative Q-Learning (ECQL) framework designed for dynamic recommendations. ECQL employs evidence-based exploration, guided by uncertainty measures, to uncover items that cater to the users' long-term preferences. The proposed framework is characterized by its Sequential state encoder, which keeps track of a dynamic state space via a sliding window over user history, and the Conservative Evidential Actor-Critic (CEAC) module that facilitates uncertainty-informed exploration and conservative policy learning. The framework's distinctive elements include an evidential reward system that leans towards exploring items with uncertain outcomes to maximize information gain, and a conservative Q-learning mechanism that guards against overestimating the policy value, thus ensuring quality recommendations. The paper further supports the efficacy of ECQL with a theoretical analysis that underscores its convergence behavior and recommendation quality. Empirical evaluations on real-world datasets reveal that ECQL surpasses benchmark methods. Additionally, ablation studies emphasize the significant contributions of vacuity-driven exploration and conservative learning. The major contributions of the paper encompass the innovation of an evidential RL approach for recommendations, an emphasis on uncertainty-driven exploration, the integration of conservative off-policy learning, theoretical assurances, and the creation of a holistic end-to-end framework."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper stands out due to its innovative combination of evidential learning and reinforcement learning tailored for recommendation systems. The inception of leveraging evidence-based uncertainty for exploration, paired with conservative off-policy learning to dodge overestimations, showcases an inventive approach. \n\n2. On the technical front, the paper exhibits robustness. It provides theoretical backing, ensuring that the methods are grounded in solid logic. The empirical evaluations, especially those conducted on real-world datasets, further validate the methodology by demonstrating its state-of-the-art prowess. \n\n3. Structurally, the paper is well-organized, offering a lucid exposition of the principal concepts and the constituents of the framework."
                },
                "weaknesses": {
                    "value": "My main concern about the paper is the experimental parts.\n\n1. The datasets used are very small in this paper. Ml-1M and ML-100K are both very small. For Net\ufb02ix, the authors only sampled 6,042 users. For Yahoo!, only 54,000 ratings were sampled. Larger datasets are required for validation.\n\n2. I'm concerned about the effect-boosting effect of the proposed method on different datasets. Since the improvment is not large. Did the authors perform the hypothesis testing to prove the significance of improvements? Also, the effect enhancement of the proposed method is much smaller inthe  largest dataset  Yahoo! than in other datasets. Is there any specific research on this phenomenon?\n\n3. In the experimental part, the RL-based models lack some more recent work (the latest method is in 2018), which cuts down the validity of the proposed method.\n\n4. Some diagrams can bedemonstrated in a more expressive way. For example, Figure 2 could insert some images to illustrate the composition of themodules in the network. \n\n5. Some formulas are not written very rigorously, for example: all mathematical formulas are not followed by a punctuation mark. In addition, some expressions are not sufficiently formal, such as 'score' and 'rating'."
                },
                "questions": {
                    "value": "In the QUALITATIVE STUDY in Section 5.2, regarding the random selection of the User ID, is there a discussion of the reason for selecting this user? I think it is more persuasive to describe the reason for the selection first and select multiple users for the QUALITATIVE STUDY."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675121920,
            "cdate": 1698675121920,
            "tmdate": 1699636381694,
            "mdate": 1699636381694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zQMPYBrWVI",
                "forum": "QwNj5TP9gm",
                "replyto": "yL1Cvq4kpm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gFnt"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and providing valuable comments. We summarize our response as follows:\n\n**Q1. The datasets used are very small in this paper. Ml-1M and ML-100K are both very small. For Net\ufb02ix, the authors only sampled 6,042 users. For Yahoo!, only 54,000 ratings were sampled. Larger datasets are required for validation.**\n\nThanks for the suggestion. Please refer to the answer to Q2 in the general response.\n\n**Q2. I'm concerned about the effect-boosting effect of the proposed method on different datasets. Since the improvement is not large. Did the authors perform the hypothesis testing to prove the significance of improvements? Also, the effect enhancement of the proposed method is much smaller in the largest dataset Yahoo! than in other datasets. Is there any specific research on this phenomenon?**\n\nThank you for this comment. We have provided the standard deviation of the proposed ECQL model along with the most competitive baselines from each category in Table 7 of Appendix E.2. The small standard deviation clearly the performance advantage over other methods. We further conduct a significance test to compare ECQL with the second best performing baseline CL4SRec. We obtain a p-value of 0.04, which confirms the performance advantage of ECQL over CL4SRec is statistically significant. To better understand the impact on the size of the datasets, we have conducted additional experiments on two larger datasets as suggested by the reviewer. As can be seen from the answer to Q2 in the general response, our model achieves much better P@5 and nDCG@5 as compared with two most recent RL based baselines. \n\n**Q3. In the experimental part, the RL-based models lack some more recent work (the latest method is in 2018), which cuts down the validity of the proposed method.**\n\nThanks for the suggestion. Please refer to the answer to Q1 in the general response.\n\n\n\n**Q4. Some diagrams can be demonstrated in a more expressive way. For example, Figure 2 could insert some images to illustrate the composition of the modules in the network.**\n\nThanks for the suggestion. We updated Figure 2 in the revised paper to better illustrate the modules composition.\n\n**Q5.  Some formulas are not written very rigorously, for example: all mathematical formulas are not followed by a punctuation mark. In addition, some expressions are not sufficiently formal, such as 'score' and 'rating'.**\n\nFollowing the reviewer's suggestion, we have added punctuation marks to all the equations. The mathematical definitions of score and rating are given in Equation 2 and the following text, respectively. We are happy to make them more formal if the reviewer has more specific suggestions. \n\n\n**Q6. Describe the reason for the selection of the given user and then select multiple users for the qualitative study.**\n\nThanks for the comment. We would like to clarify that the users in Tables 1, 3, and 12 of the revised paper are all different users randomly selected from the test set. They are representatives from many testing users where we provide a more detailed analysis on the recommendation behavior of ECQL and the competitive baselines. It is clear that ECQL needs to perform better on those individual users in order to achieve an overall better recommendation performance on evaluation metrics, including precision and nDCG. Following the reviewer's suggestion, we conduct a more comprehensive analysis on multiple users to complement the qualitative study presented in Appendix E.4. We calculate the Gini index of the diversity for all the recommended results. First, we test 1,800 users from Netflix test set and then collect 16 (number of time steps) $\\times$ 1,800 total recommendations. For each recommendation, we calculate its Gini index by separating 5 results into different categories and then calculate \n\\begin{align}\nGini=1-\\sum_{c=1}^{C}P(c)^2    \n\\end{align}\nwhere $C$ is the number of categories and $P(c), c\\in [1,C]$ is the probability for each category. Finally, we average the calculated Gini index for 1,800 recommendations and get the averaged Gini index as 0.71, which is close to 1. This indicates that the recommendation of our model is quite diverse and contains objects from different categories. Comparing to SAC and CoLin which have averaged Gini indexes 0.65 and 0.68 respectively, our ECQL achieves the most diverse recommendation thanks to the novel evidence-based exploration, which is also verified by Table 12 of the revised paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407552504,
                "cdate": 1700407552504,
                "tmdate": 1700409294867,
                "mdate": 1700409294867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9bx63QGGYa",
            "forum": "QwNj5TP9gm",
            "replyto": "QwNj5TP9gm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4158/Reviewer_Yout"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4158/Reviewer_Yout"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new variant of Q-learning called ECQL for dynamic recommendations problems. The ECQL aims to improve the exploration efficiency of \\epsilon-greedy strategy, which is commonly used in many reinforcement learning algorithms. Specifically, the exploration strategy of ECQL integrates evidence-based uncertainty and conservative learning. The purpose is to discover items that are located beyond current observation but reflect users\u2019 long-term interests. Extensive experiments are conducted to validate the performance of ECQL."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper studies an important problem of dynamic recommendation, i.e., discover items that are located beyond current observation but reflect users\u2019 long-term interests.  \n\nThe proposed ECQL is shown to have nice empirical performance."
                },
                "weaknesses": {
                    "value": "The motivation of this work is not convincing. It is claimed that epsilon-greedy may not be able to learn the optimal policy that captures effective user preferences and achieves the maximum expected reward over the long term. This claim is not supported by any evidences. To the best of my knowledge, the \\epsilon-greedy strategy is a simple approach to balance the exploration vs. exploitation tradeoff of RL. As long as the RL model is tailored to RS in a right manner and the exploration vs. exploitation tradeoff is well balanced, the learned policy may not have the claimed weakness. Also, one can tune epsilon to attain different strength of exploration. There are other exploration strategies developed in RL literature. Can you claim all of them are not learn the optimal policy that captures effective user preferences and achieves the maximum expected reward over the long term? I am also confused by Figure 1. Little details are provided. It is hard for me to judge whether Figure 1 is the outcome of improper applying of RL to RS or some other factors. For example, is it a consequence of that the optimal policy of the RL has such weakness or the epsilon-greedy strategy leads to a sub-optimal strategy that has such weakness. \n\nThe challenge analysis is confusing. It is mentioned in the Introduction \u201cto address the above challenge\u201d. I do not get what is the above challenge. Technically, what is it? \n\nThis paper is not placed clearly. It aims to address the limitation of epsilon-greedy strategy. In the related work, it does not discuss previous works on exploration strategies and place this paper properly in this research line. In the related work, I am not convinced by the claim that randomize exploration strategies are less effective at capturing users\u2019 long-term preferences. Could provide any evidence?\n\nIt is not clear how the proposed exploration strategy connects to balancing exploration vs. exploitation tradeoff. In terms of the balancing exploration vs. exploitation tradeoff, it is unclear why it is better than the epsilon-greedy or fine tuned epsilon-greedy."
                },
                "questions": {
                    "value": "Please refer to the comments on the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4158/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4158/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4158/Reviewer_Yout"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830741294,
            "cdate": 1698830741294,
            "tmdate": 1699636381617,
            "mdate": 1699636381617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "afCfhsxVla",
                "forum": "QwNj5TP9gm",
                "replyto": "9bx63QGGYa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Yout [Part I]"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and providing valuable comments. We first summarize the proposed model (ECQL), its important concept, and its implication in RS. Second, we provide our responses to the questions.\n\n**Motivation and novelty of ECQL**: In general, ECQL learns an effective and cautious recommendation policy by integrating evidence-based uncertainty and conservative learning systematically. ECQL conducts evidence-aware explorations to discover items that are located beyond current observation but potentially aligned with users\u2019 long-term interests. Effective exploration of ECQL is guaranteed through the novel use of vacuity, which is an evidence-based second-order uncertainty, derived in the subjective logic framework (see Equation 1 for the definition of vacuity).  The vacuity guided exploration can identify uncertain and informative items (from large item space), indicative of users' long-term interest. As a result, the proposed evidential reward (see Equation 3) encourages the RL agent to recommend items that the model has the least knowledge (as indicated by a high vacuity). After collecting the user feedback, the RL agent can most effectively gain knowledge of the user preference to make better recommendations in the long run. Furthermore, by integrating uncertainty-aware exploration with conservative learning, ECQL is able to make more diverse recommendations that may reflect a long-term interest while keeping a conservative view that does not deviate too much from current interests. Please refer to  Table 12 of the Appendix for more illustrative examples and supporting evidence. We also provide a comprehensive quantitative comparison among different exploration strategies in our answer to Q3 of the general response. \n\n\n**Q1. It is claimed that $\\epsilon$-greedy may not be able to learn the optimal policy that captures effective user preferences and achieves the maximum expected reward over the long term. This claim is not supported by any evidence.**\n\nSince $\\epsilon$-greedy performs completely random exploration, it is very likely that the explored items may contain the ones that the user is already familiar with or those that completely deviate from users' interest. In contrast, as mentioned above, the vacuity-guided exploration encourages the RL agent to recommend items that the model has the least knowledge. After collecting the user feedback, the RL agent can most effectively gain knowledge of the user preference to make better recommendations in the long run. Besides $\\epsilon$-greedy, we also compared with another commonly used exploration strategy, soft-actor-critic (SAC), which leverages entropy to perform exploration. However, as we made clear in the paper, a high entropy may imply either high vacuity (lack of evidence) or high dissonance (conflict of strong evidence). However, dissonance is not effective for exploration in RS due to its focus on confusing items mostly derived based on the users' current interests. Figure 8 of the Appendix shows that ECQL achieves a much higher cumulative reward than SAC and $\\epsilon$-greedy, which confirms its effectiveness in capturing users' long-term interest. Table 12 of Appendix E.4 presents a qualitative study to demonstrate that ECQL is able to recommend more diverse items than SAC and $\\epsilon$-greedy relevant to user's preference. Finally, Table 11 of Appendix E.3 provides a comparison with a collaborative contextual bandit based method CoLin to further demonstrate the effective exploration of ECQL. \n\n\n**Q2.  For Figure 1. Little details are provided.  It is hard for me to judge whether Figure 1 is the outcome of improper applying of RL to RS or some other factors.**\n\nIn Figure 1, we illustrate that the existing RL methods based on $\\epsilon$-greedy do not provide effective exploration. Their recommendations usually concentrate on a narrower items space, primarily focusing on highly rated items but only reflecting short-term interest. In contrast, the proposed ECQL method provides more informative items to help models learn the user's long-term interest.\n\n\n**Q3. It is mentioned in the Introduction \"to address the above challenge\". I do not get what is the above challenge. Technically, what is it?**\n\nAs mentioned in the paper and the response to Q2, the major challenge with the existing RL-based methods is the lack of a systematic exploration to discover users' long-term interests. Hence, we need a better exploration strategy to capture user's diverse interests and maximize the long-term reward.\n\n\n**Q4. I am not convinced by the claim that randomize exploration strategies are less effective at capturing users' long-term preferences. Could provide any evidence?**\n\nPlease refer to our detailed response to Q1 that mentions multiple concrete evidences provided in the paper to explain why random exploration is less effective."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407465167,
                "cdate": 1700407465167,
                "tmdate": 1700578143684,
                "mdate": 1700578143684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "po9dvcYtyx",
                "forum": "QwNj5TP9gm",
                "replyto": "9bx63QGGYa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Yout [Part II]"
                    },
                    "comment": {
                        "value": "**Q5.  In terms of the balancing exploration vs. exploitation tradeoff, it is unclear why it is better than the epsilon-greedy or fine-tuned $\\epsilon$-greedy.**\n\nTo achieve optimal policy in the guided exploration system, it is always important to balance exploration and exploitation. By leveraging second-order uncertainty (vacuity) guided exploration, we can effectively locate items that the model lacks the knowledge for now but may reflect the model's long-term interest to build a more precise user profile. Please refer to our discussion on the motivation and novelty of ECQL for more details."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407491548,
                "cdate": 1700407491548,
                "tmdate": 1700407491548,
                "mdate": 1700407491548,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZUIxUQmUaC",
            "forum": "QwNj5TP9gm",
            "replyto": "QwNj5TP9gm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4158/Reviewer_GVwA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4158/Reviewer_GVwA"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a novel evidential conservative Q-learning framework (ECQL) that learns an effective and conservative recommendation policy by integrating evidence-based uncertainty and conservative learning. Specifically, ECQL includes two main components, i.e., a uniquely designed sequential state encoder and a novel conservative evidential-actor-critic module. Extensive experiments on real datasets demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper introduces a novel recommendation framework named ECQL, which uses evidential conservative Q-learning for dynamic recommendation.\n\n2. ECQL is a new model that integrates reinforcement learning with evidential learning to provide uncertainty-aware diverse recommendation.\n\n3. The authors also provide theoretical analysis to justify the desired convergence behavior and recommendation quality that guarantees to avoid risky recommendations.\n\n4. The proposed framework integrates a sequential encoder, an actor-critic network, and an evidence network to provide end-to-end integrated training process.\n\n5. The authors have performed extensive experiments on real datasets and compare the proposed model with SOTA baseline methods."
                },
                "weaknesses": {
                    "value": "1. In the Section 2, the authors do no include the recent studies about sequential recommendation. The most recent dynamic/sequential recommendation methods mentioned by them are proposed in 2019. The sequential recommendation methods developed in recent 4 year are not discussed in Section 2. Similarly, the authors are suggested to include some discussions about the RL methods developed in recent 3 years.\n\n2. One advantage of the proposed method is to provide diverse recommendations. However, in Table 2, the authors do not analyze the diversity of the generated recommendation results.\n\n3. The authors are suggested to include more sequential recommendation methods proposed in 2021 and 2022 as baselines. TimeSVD++ and CKF are not needed to be used as baseline methods. \n\n4. The Movielens-100K dataset is too small for experimental evaluation. The authors are suggested to include some larger datasets for experiments, for example Amazon review datasets. \n\n5. In the proposed ECQL, there are two main modules. However, there is no experiments studying the importance of these two parts. The authors are suggested to perform an ablation study."
                },
                "questions": {
                    "value": "1. The proposed framework integrates a sequential encoder, an actor-critic network, and an evidence network to provide end-to-end integrated training process. For the actor-critic network and evidence network, which one is more important? Moreover, whether the model performance is dominated by the base sequential encoder? Whether the proposed framework can help improve the performance of different base sequential encoders?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4158/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698938822328,
            "cdate": 1698938822328,
            "tmdate": 1699636381546,
            "mdate": 1699636381546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0fEGb7vsqa",
                "forum": "QwNj5TP9gm",
                "replyto": "ZUIxUQmUaC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GVwA"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and providing valuable comments. We summarize our response as follows.\n\n**Q1. In the Section 2, the authors do not include the recent studies about sequential recommendation... Similarly, the authors are suggested to include some discussions about the RL methods developed in recent 3 years.**\n\nThanks for the suggestion. Please refer to the answer to Q1 in the general response. \n\n\n**Q2.  One advantage of the proposed method is to provide diverse recommendations. However, in Table 2, the authors do not analyze the diversity of the generated recommendation results.**\n\nThank you for the suggestion. We have provided the qualitative analysis of diversity in Table 12 of the Appendix. To further provide the quantitative evidence, we calculate the Gini index of the diversity for all the recommended results. First, we test 1,800 users from Netflix test set and then collect 16 (number of time steps) $\\times$ 1,800 total recommendations. For each recommendation, we calculate its Gini index by separating 5 results into different categories and then calculate \n\\begin{align}\nGini=1-\\sum_{c=1}^{C}P(c)^2    \n\\end{align}\nwhere $C$ is the number of categories and $P(c), c\\in [1,C]$ is the probability for each category. Finally, we average the calculated Gini index for 1,800 recommendations and get the averaged Gini index as 0.71, which is close to 1. This indicates that the recommendation of our model is quite diverse and contains objects from different categories. Comparing to SAC and CoLin which have averaged Gini indexes 0.65 and 0.68 respectively, our ECQL achieves the most diverse recommendation thanks to the novel evidence-based exploration, which is also verified by Table 12 of the revised paper.\n\n\n\n\n**Q3. The authors are suggested to include more sequential recommendation methods proposed in 2021 and 2022 as baselines. TimeSVD++ and CKF are not needed to be used as baseline methods.**\n\nThank you for the suggestion. We have removed TimeSVD++ and CKF from the main comparison table and moved them to Appendix E.2 where we compare with some other baseline models. For the newer sequential recommendation models, in addition to CL4SRec (Xie et al., 2022), we have added two additional baselines: ResAct (Xue et al., 2023) and SAR (Antaris et al., 2021), and compared them with our approach on two larger datasets. Please refer to our response to Q4 below for details. \n\n\n**Q4. The Movielens-100K dataset is too small for experimental evaluation. The authors are suggested to include some larger datasets for experiments, for example, Amazon review datasets.**\n\nThanks for the suggestion. Please refer to the answer to Q2 in the general response. \n\n\n**Q5. In the proposed ECQL, there are two main modules. However, there is no experiments studying the importance of these two parts. The authors are suggested to perform an ablation study.**\n\nThanks for the suggestion. Please refer to the answer to Q3 in the general response for the detailed results. \n\n\n**Q6. The proposed framework integrates a sequential encoder, an actor-critic network, and an evidence network to provide end-to-end integrated training process. For the actor-critic network and evidence network, which one is more important? Moreover, whether the model performance is dominated by the base sequential encoder? Whether the proposed framework can help improve the performance of different base sequential encoders?**\n\nThanks for the great suggestion. First, we emphasize that the actor-critic network and evidence network work seamlessly to form our ECQL recommender system. Actor-critic provides the RL-base mechanism, while the evidence network provides the evidential prediction and the corresponding vacuity used for effective exploration. Please refer to the answer to Q3 in the general response for the detailed results for impacts of exploration strategies and base sequential encoders. \n\n\n\n**References**\n\n-  Xue, Wanqi, et al. \"ResAct: Reinforcing long-term engagement in sequential recommendation with residual actor.\" ICLR, 2023.\n\n- Antaris, Stefanos, and Dimitrios Rafailidis. \"Sequence adaptation via reinforcement learning in recommender systems.\" Proceedings of the 15th ACM Conference on Recommender Systems. 2021.\n\n- Wang, Xiang, et al. \"Neural graph collaborative filtering.\" Proceedings of the 42nd international ACM SIGIR conference on Research and development in Information Retrieval. 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407208925,
                "cdate": 1700407208925,
                "tmdate": 1700407208925,
                "mdate": 1700407208925,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aIu75RNH9d",
                "forum": "QwNj5TP9gm",
                "replyto": "ZUIxUQmUaC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer GVwA, \n\nThanks for your insightful suggestions to help us further improve the paper! We summarize our update as below:\n\n1. As suggested, we include two recent RL-based sequential recommendation baselines: : ResAct (Xue et al., 2023) and SAR (Antaris et al., 2021). And we compare them with our approach on two larger datasets, MovieLens-10M and Amazon Book. Please refer to our answers to Q1 and Q2 in the general response for details.\n\n2. We study the importance of these two modules SSE and CEAC in the proposed framework and conduct ablation studies on both of them. Please refer to the answer to Q3 in the general response for the detailed results.\n\n3. We have provided the qualitative analysis of diversity in Table 12 of the Appendix. To further provide the quantitative evidence, we calculate the Gini index of the diversity for all the recommended results and conduct an analysis by comparing to other RL-based exploration strategies SAC and CoLin.\n\nWe hope you find our answers satisfactory, and consider updating your assessment accordingly! As always, we are happy to provide any additional clarifications if needed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619477130,
                "cdate": 1700619477130,
                "tmdate": 1700621253235,
                "mdate": 1700621253235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ElriCjN6es",
            "forum": "QwNj5TP9gm",
            "replyto": "QwNj5TP9gm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4158/Reviewer_jjM8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4158/Reviewer_jjM8"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel recommendation model that integrates reinforcement learning with evidential learning to provide uncertainty-aware diverse recommendations that may reflect users\u2019 long-term interests. The proposed model, called Evidential Conservative Q-learning (ECQL), conducts evidence-aware explorations to discover items that are located beyond current observation but reflect users\u2019 long-term interests. Additionally, it provides an uncertainty-aware conservative view on policy evaluation to discourage deviating too much from users\u2019 current interests. The paper presents a theoretical analysis to justify the desired convergence behavior and recommendation quality that guarantees to avoid risky (or overly optimistic) recommendations. The paper proposes a framework that includes a sequential state encoder and a Conservative Evidential Actor-Critic (CEAC) module. The former generates the current state of the environment by aggregating historical information and a sliding window that contains the current user interactions as well as newly recommended items from RL exploration that may represent future interests. The latter performs an evidence-based rating prediction by maximizing the conservative evidential Q-value and leverages an uncertainty-aware ranking score to explore the item space for a more diverse and valuable recommendation. The paper conducts experiments over four real-world datasets and compares with state-of-the-art baselines to demonstrate the effectiveness of the proposed model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a novel recommendation model that integrates reinforcement learning with evidential learning to provide uncertainty-aware diverse recommendations that may reflect users\u2019 long-term interests.\n    The paper presents a theoretical analysis to justify the desired convergence behavior and recommendation quality that guarantees to avoid risky (or overly optimistic) recommendations.\n    The paper conducts extensive experiments over four real-world datasets and compares with state-of-the-art baselines to demonstrate the effectiveness of the proposed model."
                },
                "weaknesses": {
                    "value": "Many of the design choices of the paper are not well justified, it is also quite difficult to understand how some of the components of the model stick together. \n    The evaluation of the paper is done on data that is inherently not sequential, (with the exception of the Music data), in particular the Movielens dataset is a survey dataset and has many issues with regards to sequential evaluation. \n    Most of the increases in IR scores are rather marginal and it is unclear how much of that can be attributed to the particular elements of the method. \nUsing ratings as rewards signals is rather difficult to justify as currently ratings are not available for almost all industry related systems"
                },
                "questions": {
                    "value": "How does the method compare on real sequential data where reward signals are much more subtle?\nWhat element of the model provides the increase in performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4158/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4158/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4158/Reviewer_jjM8"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4158/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699993607694,
            "cdate": 1699993607694,
            "tmdate": 1699993607694,
            "mdate": 1699993607694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4xEFRCxcNx",
                "forum": "QwNj5TP9gm",
                "replyto": "ElriCjN6es",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jjM8 [Part I]"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our paper and providing valuable comments. We summarize our response as follows.\n\n**Q1. Many of the design choices of the paper are not well justified, it is also quite difficult to understand how some of the components of the model stick together.**\n\nThank you for the comment. The question consists of two parts and we address them separately in what follows.\n\n- *Justification of design choices:* The proposed ECQL seamlessly integrates two major components: a **sequential state encoder** and a **Conservative Evidential Actor-Critic (CEAC)** module. The former primarily focuses on generating the current state of the RL environment by aggregating the previous state, the current items captured by a sliding window, and the future items from the recommendation. This provides an effective means of dynamic state representation for better future recommendations. Meanwhile, the CEAC module leverages evidential uncertainty to effectively explore the item space to recommend items that potentially align with the user's long-term interest. It encourages learning the optimal policy by maximizing a novel conservative evidential Q-value to make more diverse recommendations that may reflect a long-term interest while keeping a conservative view that does not deviate too much from current interests.\n\n- *How the components of the model work together:* As shown in Figure 2 in the paper, the key components of the model include the **sequential state encoder (SSE), Action network, Evidence network**, and **Critic network**. The **SSE** maintains a dynamic state space with a sliding window $W_t$ which moves along the user's interaction history $H_u$ over time to input new data into the SSE, and a conservative evidential-actor-critic (CEAC) module which functions as an RL agent to explore the item space by introducing the evidence-based uncertainty (i.e., vacuity) into a new off-policy evidential RL setting. By incorporating previous state information, recent items captured by a sliding window, and the recommended items from the RL agent, the sequential encoder generates the current state ${\\bf s}_t$. This state is further passed to the **Action network** that predicts the mean and variance to form a Gaussian policy distribution. We sample a current action ${\\bf a}_t$ from the policy distribution that corresponds to the latent preference of the user that simultaneously captures the past (via a previous state), current (through a sliding window) and future interest (through RL exploration). By leveraging the current action and total item embeddings from the Item Pool ($\\mathcal{I}$), the **Evidence network** provides the evidence that can be used to form the rating prediction for exploitation while estimating the uncertainty for better exploration. Finally, the **Critic network** generates a conservative evidential Q-value for conservative policy updates of the action network. \n\n**Q2.  The evaluation of the paper is done on data (except Music) that is inherently not sequential**\n\nWe would like to clarify that, broadly speaking, sequential data refers to the data in which current time data is dependent upon its previous time data. For example, the Movielens dataset includes the user interaction sequence in timestamps where the current movie interaction is dependent upon its previous time interactions. In this sense, all the datasets used in our evaluation are sequential as they record how the user interactions with items sequentially changed over a period of time. Furthermore, most of the standard sequential recommendation models like CASER (Tang et al.,2018), SASRec (Kang et al., 2018), and BERT4Rec (Sun et al.,2019) have used these datasets for evaluation. Therefore, we leveraged the chosen datasets to compare with those baselines. In addition, we include two other datasets, including the Amazon book-rating dataset, and conducted additional experiments and comparisons with some recent sequential recommendation methods. Please refer to the answer to Q2 in the general response."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406703442,
                "cdate": 1700406703442,
                "tmdate": 1700408273168,
                "mdate": 1700408273168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fxGxsTKYle",
                "forum": "QwNj5TP9gm",
                "replyto": "ElriCjN6es",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4158/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jjM8 [Part II]"
                    },
                    "comment": {
                        "value": "**Q3. Most of the increases in IR scores are rather marginal and it is unclear how much of that can be attributed to the particular elements of the method.**\n\nThank you for this comment. We have provided the standard deviation of the proposed ECQL model along with the most competitive baselines from each category in Table 7 of Appendix E.2. The small standard deviation clearly shows the performance advantage over other methods. We further conduct a significance test to compare ECQL with the second best performing baseline CL4SRec. We obtain a p-value of 0.04, which confirms that the performance advantage of ECQL over CL4SRec is statistically significant. We have also improved our ablation study to investigate the impact of the sequential state encoder and the conservative evidential-actor-critic (CEAC) module. The results are reported in our answer to Q3 in the general response. Furthermore, we also our vacuity-guided exploration, we compare with other commonly exploration strategies. The results can be in our answer to Q3 in the general response. All the new ablation study results are also included in Appendix E.3 of the revised paper. \n\n**Q4. Using ratings as rewards signals is rather difficult to justify as currently ratings are not available for almost all industry related systems**\n\nWe follow the standard way of applying RL method in RS, where ratings are leveraged to compute reward as did in recent methods ResAct (Xue et al., 2023) and SAR (Antaris et al., 2021). Our vacuity-based exploration is a general strategy that can be used in other types of user feedback other than ratings. Please refer to our answer to Q5 as well. \n\n**Q5. How does the method compare on real sequential data where reward signals are much more subtle?**\n\nWe would like to clarify that our evidential reward definition is a general one that can be extended to other types of user feedback beyond ratings. For example, for click based feedback, the ratings can be changed to a binary signal while the vacuity term (which is the key novelty of our approach) in Equation 3 remains unchanged. \n\n**Q6. What element of the model provides the increase in performance?**\n\nPlease refer to the answer to Q3 in the general response for the detailed ablation study results. \n\n**References**\n\n-  Xue, Wanqi, et al. \"ResAct: Reinforcing long-term engagement in sequential recommendation with residual actor.\" ICLR, 2023.\n\n- Antaris, Stefanos, and Dimitrios Rafailidis. \"Sequence adaptation via reinforcement learning in recommender systems.\" Proceedings of the 15th ACM Conference on Recommender Systems. 2021. \n\n- Sun, Fei, et al. \"BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer.\" Proceedings of the 28th ACM international conference on information and knowledge management. 2019.\n\n- Kang, Wang-Cheng, and Julian McAuley. \"Self-attentive sequential recommendation.\" 2018 IEEE international conference on data mining (ICDM). IEEE, 2018.\n\n- Tang, Jiaxi, and Ke Wang. \"Personalized top-n sequential recommendation via convolutional sequence embedding.\" Proceedings of the eleventh ACM international conference on web search and data mining. 2018."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4158/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407010651,
                "cdate": 1700407010651,
                "tmdate": 1700408694373,
                "mdate": 1700408694373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]