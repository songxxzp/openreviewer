[
    {
        "title": "Hessian-Aware Bayesian Optimization for Decision Making Systems"
    },
    {
        "review": {
            "id": "0QdY0OxDS2",
            "forum": "KKBZzMLGvH",
            "replyto": "KKBZzMLGvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_hYdE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_hYdE"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of learning good decision-making policies when multiple agents are involved.\nThis problem traditionally presents several challenges, the biggest of which include reasoning about sparse feedback and unreliable gradient information, and accounting for interactions between agents.\nTo perform effective optimization with sparse feedback and unreliable gradient information, the authors employ Bayesian optimization (BayesOpt).\nHowever, BayesOpt doesn't scale well to high dimensions, so the paper proposes using an abstraction of role and role interaction to model the agents on a high level, thus significantly simplifying the policy space to search over.\nTo further aid scaling BayesOpt to the high-dimensional search space, the authors use a surrogate Hessian model to learn an additive structure of the space, which ultimately helps identify good regions within the space.\nExperimental results show promising performance of the proposed algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem studied is interesting and important.\nThe paper motivates the use of the role and role interaction abstraction well, and the proposed affinity score seems like a good way to quantify how suitable each agent is for each role.\nSome experiments show really strong performance from the proposed algorithm against a wide range of baselines."
                },
                "weaknesses": {
                    "value": "There seem to be many components to the algorithm: role abstraction, BayesOpt with the Upper Confidence Bound policy, the Hessian surrogate.\nAs far as I call tell, the ablation study doesn't give me insights into which components are useful for the final algorithm.\nFor example, could we use a different optimization algorithm that doesn't rely on gradient information such as DIRECT or evolutionary algorithms?\nHow much does information about the Hessian help BayesOpt?\n\nI would have liked to see more discussion on the quality of the Hessian surrogate.\nMy understanding is that Hessian information can be accessed via the JIT compilation of $v(\\theta)$.\nHow good is this surrogate?\nWhat are the situations where this surrogate doesn't offer reliable information?"
                },
                "questions": {
                    "value": "Please see my questions in the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8961/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8961/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8961/Reviewer_hYdE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8961/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698596968091,
            "cdate": 1698596968091,
            "tmdate": 1699637127920,
            "mdate": 1699637127920,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hilBfbtvqn",
                "forum": "KKBZzMLGvH",
                "replyto": "0QdY0OxDS2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hYdE"
                    },
                    "comment": {
                        "value": "Dear reviewer, here are the answers to your questions.\n\n- On the ablation study.\n- - As is typical in reinforcement learning or deep learning style papers, we ablated the higher-order model extensively (Algorithm 1,2,3). We did not ablate Algorithm 4 (Bayesian optimization) as we're not familiar of this as a technique in Bayesian optimization papers.\n- - This paper evolved from an earlier work where we explored other techniques for empirically optimizing the acquisition function in BO which is the significant bottleneck since the work of Kandasamy et. al. We have several unreleased results regarding research into optimizing or improving the DIRECT algorithm.\n- - We did not pursue and abandoned the above research work as it was deemed no longer interesting or valuable to the AI community.\n- - We are not sure how evolutionary algorithms would fare at this problem. We wanted to have an algorithm backed by theoretical regret guarantees which is what Algorithm 4 provides.\n- - To determine whether the Hessian helps in BayesOpt. We compared against TreeBO which is the exact same additive structure as our work, however, our approach uses the surrogate Hessian to learn the dependency structure. This validation is presented in Fig. 4 and Fig. 9 in Appendix C.5.\n\n- How good is this surrogate? What are the situations where this surrogate doesn't offer reliable information?\n- - In our validation, we found the surrogate to be always helpful. We believe this has to do with the \"constructed nature\" of the benchmarks, which allows for significant learnability."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699654669796,
                "cdate": 1699654669796,
                "tmdate": 1700314757277,
                "mdate": 1700314757277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p90ZPuDEjW",
                "forum": "KKBZzMLGvH",
                "replyto": "0QdY0OxDS2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hYdE (continued)"
                    },
                    "comment": {
                        "value": "We want to give a very small addendum to our response. We finally understood the reviewer's comments about ablation of Algorithm 4. We want to point out that there are only 2 hyperparameters in Algorithm 4 (Bayesian optimization), which were completely unchanged throughout the validation and found through simple grid search.\n\nThe hyperparameters for Algorithm 4 are as follows:\n\n- HA-GP-UCB\n- - Size of the largest maximal clique: Note that in the theoretical analysis, the regret scales with the size of the largest clique. We observed that having larger maximal cliques would demonstrate slow improvement in the objective function due to an overly complex additive model. Having smaller maximal cliques would cause the objective function to asymptotically converge to a subpar value due to the limited expressiveness of the additive model.\n- - Total number of additive kernels: Due to computational reasons, we limited the total number of additive kernels generated. We encountered out-of-memory and computational issues on commodity GPUs if this value was set too high.\n\nThe hyperparameters for Algorithm 1, 2, 3 are as follows:\n- Higher-order model\n- - Neural network sizes: Internally, our higher-order model is constructed using neural networks forming the computational components. Each neural network component consisted of 2 hidden layers, with 4 neurons per layer. We found that having overly large networks would be too large for HA-GP-UCB to optimize, and likely would be too large for memory-constrained devices."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741487928,
                "cdate": 1700741487928,
                "tmdate": 1700742362910,
                "mdate": 1700742362910,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "A9fybXmcaQ",
            "forum": "KKBZzMLGvH",
            "replyto": "KKBZzMLGvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_MZim"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_MZim"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Hessian-aware Bayesian optimization for the multi-agent policy optimization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The multi-agent policy optimization is interesting setting.\n\nThe paper seemingly technically sound, but I currently cannot follow the technical detail because of reason below."
                },
                "weaknesses": {
                    "value": "Although I'm not familiar with the topic (reinforcement learning with multi-agent), I currently think the paper is not easy to follow for many readers. Even the basic problem setting is difficult to fully understand because of unclear descriptions. The meanings of several key words (abstraction, immutable, compile, and so on) are not clarified in the text, because of which technical description is difficult to understand for those who are not familiar with the topic."
                },
                "questions": {
                    "value": "Overall, I feel severe difficulty to understand the paper. I'd appreciate if the authors could provide more elaborated explanation.\n\nWhat is the definition of the role affinity function \\Lambda^\\theta_r,i ? What does it indicate?\n\nWhat is h without subscript in line 4 of Algorithm 4?\n\nWhat is a surrogate of Hessian? How is it constructed ? \n\nThe authors claim the cumulative regret of the proposed method scale with O(log D). What assumption makes it possible? The implication behind assumption 1 is difficult to find for me. Does it assume some low dimensional essential dependency in the underlying true function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8961/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699011904298,
            "cdate": 1699011904298,
            "tmdate": 1699637127805,
            "mdate": 1699637127805,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C5TukDOGTq",
                "forum": "KKBZzMLGvH",
                "replyto": "A9fybXmcaQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MZim"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe are looking into making changes to the draft to aid in readability. Here are immediate answers to your questions.\n\n- Basic problem setting\n- - The basic problem setting is that of multiple agents interacting cooperatively to accomplish a common goal. This is also in a memory constrained setting (such as on IoT devices or drone delivery tasks). Within this problem setting, we propose a variant of high dimensional Bayesian optimization and show how it can optimize a compact policy space (memory constrained setting). Specifically we show in the case of malformed reward scenarios or complex multi-agent interactions, globally optimizing a small policy using our approach outperforms related work.\n\n- Definitions of key words\n- - Abstraction: We solve several problems in our work by using abstraction. Abstraction, is separating a complex system into many components which interact in a well defined manner. In our work we use abstraction to separate decision making into role assignment, and role interaction,\n- - Immutable: Immutable here means something that does not change. As we are optimizing over a parameter space, the immutable part of the policy (the instructions Alg. 1,2,3) are immutable as they are not searched over or optimized.\n- - Compile: Here compilation means the immutable and mutable (parameters) part of the policy are joined together to create the actual policy.\n\n- What is the definition of the role affinity function \\Lambda^\\theta_r,i ? What does it indicate?\n- - The role affinity function is trying to quantify the affinity for an agent to take on a specific role. It indicates the affinity for that agent to take on that specific role.\n\n- What is h without subscript in line 4 of Algorithm 4?\n- - h is a tensor (a stack of matrices) formed by collecting many Hessian matrices from line 3 in Algorithm 4. \n\n- What is a surrogate of Hessian? How is it constructed ?\n- - The surrogate Hessian is a surrogate quantity used in the paper as access to the true Hessian is either not available or too expensive to approximate. We discuss this further in Appendix H. The construction of the surrogate is as follows (found in Appendix A)\n- - To estimate the Hessian, we used the Hessian-Vector product approximation. We relaxed the discrete portions of our metamodel policy into differentiable continuous approximation for this phase using the Sinkhorn-Knopp algorithm for the Role Assignment phase. For role interaction network connectivity, we used a sigmoid to create differentiable \u201csoft\u201d edges between each role.\n\n- The authors claim the cumulative regret of the proposed method scale with O(log D). What assumption makes it possible? The implication behind assumption 1 is difficult to find for me. Does it assume some low dimensional essential dependency in the underlying true function?\n- - The approximate high level sketch of Assumption 1 is that the dependency graph drawn from the Erd\u0151s-R\u00e9nyi model is of low degree/connectivity. The base of the logarithm is related to the quantity p_g we discuss in Assumption 1. This is elaborated on in the proof of Theorem 1."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699653278862,
                "cdate": 1699653278862,
                "tmdate": 1700631828390,
                "mdate": 1700631828390,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b1hSoXR2k3",
            "forum": "KKBZzMLGvH",
            "replyto": "KKBZzMLGvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_jHt7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_jHt7"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a multi-agent reinforcement learning algorithm based on Bayesian optimization. The main contribution of this paper is the idea to incorporate the Hessian information to solve the additive structure of the high dimensional problem as an additive decomposition. Because the value Hessian is unavailable, the system relies on the policy Hessian as a surrogate. The paper includes extensive evaluations and regret bounds."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The main contribution of the paper in terms of the Hessian aware BO is very interesting and could have a very large impact on the community. In fact, I believe that the MARL application could be completely removed and still have a valuable paper, maybe extended to other high dimensional problems with additive structure. I have minimal experience with MARL, but it seems that the approach is competitive in that area as well.\nThe paper provides an excellent theoretical and empirical analysis."
                },
                "weaknesses": {
                    "value": "The main weakness of the paper (and it is understandable) is the amount of clutter in the paper. Most figures and tables are impossible to read without zoom. The amount of information provided should be more appropriate to a journal article, although I can see the motivation behind a submission to ICLR instead. However, the paper is clear enough.\nDespite reading appendix G, regarding the method presented, it is still unclear to me if H_pi can always be used as a reliable surrogate for H_v, as g might have some 0 (or small) components. In fact, for the optimal policy, shouldn\u2019t it be 0?"
                },
                "questions": {
                    "value": "-Why do you need JIT-compilation? Isn\u2019t it just instantiation with certain parameter values?\n-In algorithm 4, do you initialise the parameters with a uniform distribution? In BO literature, it is common to use low discrepancy sequences or sampling procedures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8961/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699063624787,
            "cdate": 1699063624787,
            "tmdate": 1699637127695,
            "mdate": 1699637127695,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9GOL39LnT5",
                "forum": "KKBZzMLGvH",
                "replyto": "b1hSoXR2k3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jHt7"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe are looking into making changes to the draft to aid in readability. Here are immediate answers to your questions.\n\n- Why MARL and BO?\n- - In order to make Bayesian Optimization valuable and competetive theoretically and empirically with a clear use case, we focused on decision making systems for memory constrained devices.\n\n- Why conference and not journal?\n- - We believe that many excellent works in Computer Science come from conference publications. Which is why we hoped to have this work be accepted at a conference.\n\n- H_pi can always be used as a reliable surrogate for H_v?\n- - This is a good question. In all of our validation scenarios, H_pi always served well in learning the dependency structure. We believe that this is due to the \"constructed nature\" of the reinforcement learning benchmarks. The tasks themselves are constructed in such a way that learning is possible. We're thinking about whether it is possible to make an argument along this direction in the Appendix.\n\n- Need for JIT-compilation?\n- - The JIT-compilation was a design decision in order to have a compact policy space. We are looking for citations and references which show that this approach is a reasonable way to simplify a parameter space.\n\n- Algorithm 4 initialization procedure\n- - We use Sobol sequences in order to quasi-randomly sample the parameter space."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699646666120,
                "cdate": 1699646666120,
                "tmdate": 1699647205969,
                "mdate": 1699647205969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5K6LdMXR2A",
                "forum": "KKBZzMLGvH",
                "replyto": "NaHynE4nAx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_jHt7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_jHt7"
                ],
                "content": {
                    "comment": {
                        "value": "Just to clarify, I don't know what is happening in openreview. I also see the review history empty but the PDF I can access now is different from the one submitted originally. It has been updated.\n\nI've seen the same behavior in other papers."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700972083,
                "cdate": 1700700972083,
                "tmdate": 1700700972083,
                "mdate": 1700700972083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2Q3JSs8E0t",
                "forum": "KKBZzMLGvH",
                "replyto": "9GOL39LnT5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_jHt7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_jHt7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for comments.\n\nCorrect me if I'm wrong, but the JIT-compilation is a way to deal with a parametric model. You have an algorithm/model and you evaluate it with different parameters. In fact, I don't agree that it is a meta-model. I think the explanation with a parametric model would simplify the overall explanation.\n\nThe JIT-compilation, while it could be interesting, it is just a matter of experimentation/implementation (and should be discussed in that Section). It could be implemented as a Python function, a C++ template or something else. But I don't think it changes the method."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737149060,
                "cdate": 1700737149060,
                "tmdate": 1700737149060,
                "mdate": 1700737149060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nS2OkPQJcu",
                "forum": "KKBZzMLGvH",
                "replyto": "b1hSoXR2k3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "We apologize for the last minute and quick response. Due to this issue which we noticed ourselves, we have changed the \"metamodel\" to a HOM (higher order model), and we have also changed \"JIT-compilation\" to GEN (generation) or GEN process.\n\nThis is more in line with the literature that we are able to find on the topic (as shown in footnote 4)."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741145014,
                "cdate": 1700741145014,
                "tmdate": 1700741199614,
                "mdate": 1700741199614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jvykYvQYOq",
            "forum": "KKBZzMLGvH",
            "replyto": "KKBZzMLGvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_22Fs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_22Fs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a metamodel based on message-passing neural network  for multi-agent policy search. The MPNN represents a dependency graph between agents, which is claimed to be expressive and compact. A Bayesian optimization algorithm is proposed to learn the graph structure. Using direct queries of Hessian, we can learn the dependency graph using a GP-UCB variant. Experimental results have shown strong performance compared to MARL, HDBO and ablations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of using Bayesian optimization to learn the dependency graph structure is interesting and novel.\n2. The multi-agent policy representation is significant.\n3. Combination of role assignment and role interaction looks powerful in empirical results.\n4. Figures help to understand the contributions."
                },
                "weaknesses": {
                    "value": "1. The presentation of the paper is poor.\n- More background knowledge should be introduced to further clarify the motivations, like the necessity of the JIT compilation.\n- The notation is extensively abused and many algorithms and notations have no formal definitions (See questions for some of them).\n2. The motivation of one of the main contributions, using Hessian in Bayesian optimization is not very clear. As Bayesian optimization is a black-box function, directly querying Hessian might be very hard.\n3. There are no proofs or lemmas for the theorems.\n4. Should add related works of following topics:\n- Learning the dependency graph;\n- Modeling dependency graph as a MPNN;\n- Bayesian optimization to learn the graph structure;\n\nOverall, it is a very interesting paper with possibly strong contributions, but the writing and presentation is very poor and confusing. I suggest the authors to do a very thorough revision during the rebuttal."
                },
                "questions": {
                    "value": "### Questions regarding the Hessian-award BO:\n1. How to observe the surrogate Hessian of the black-box function?\n2. What is the motivations of using Hessian in BO? If you already observed Hessian, why not using the gradient descent?\n### Questions overall:\n3. what do you mean by JIT compilations? The meaning is not clear in the context of multi-agent policy structure. \n4. Why do you use MPNN rather than graph convolution, graph attention or other transformer vi rants? These models should be more expressive.\n### Questions about the notations:\n- What does the subscripts of $\\theta$ means? For example, sometimes you use $\\theta_{r,i}$ and some times $\\theta_{g,e}$ without any explanations on what are $r,g,i,e$.\n- What is super script D in section 4.4? I only see the definition of ``some dimensionality\u2019\u2019. $D$ is a very important dependency terms of regret in the theorem.\n- What does the super script ${}^(i)$ mean in section 4.4?\n- What is the mathematical expression of regret $r(t)$ of querying Hessian?\n- What is $\\mathcal U$ in Algorithm 4?\n- What is the Max-Cliques operation in Algorithm 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8961/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699304018328,
            "cdate": 1699304018328,
            "tmdate": 1699637127581,
            "mdate": 1699637127581,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NbVbRmzdM2",
                "forum": "KKBZzMLGvH",
                "replyto": "jvykYvQYOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 22Fs"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe are looking into making changes to the draft to aid in readability. Here are immediate answers to your questions.\n\n\n- The presentation of the paper is poor.\n- - We are looking into improving the readability with another draft.\n\n- More background knowledge should be introduced to further clarify the motivations, like the necessity of the JIT compilation.\n- - This is an interesting point, our approach with JIT compilation was to save on resources as we were considering the specific use case of memory constrained devices. We will see if we can find other related work which does the same to justify this design choice.\n\n- The notation is extensively abused and many algorithms and notations have no formal definitions (See questions for some of them).\n- - We attempted our best to not abuse notation. Unfortunately, to make contributions that would be interesting and valuable to the decision making systems community, as well as have theoretical results, some of our notation was slightly abused.\n\n- The motivation of one of the main contributions, using Hessian in Bayesian optimization is not very clear. As Bayesian optimization is a black-box function, directly querying Hessian might be very hard.\n- - We discuss this in detail in Appendix H.\n\n\n- There are no proofs or lemmas for the theorems.\n- - We are not sure about what the reviewer means? Our proofs and lemmas were included in the supplementary materials. Could it be the case that this was not uploaded in OpenReview? If this is the case then we will reach out to the PCs in order to make sure that this will be fixed.\n\n- RW on Learning the dependency graph;\n- - We are not aware of other works that learn the dependency graph other than Gibbs sampling or evolutionary algorithms, which we have included.\n\n- RW on Modeling dependency graph as a MPNN;\n- - We are not aware of any additional work which models the dependency graph in Bayesian optimization as a MPNN. If the reviewer knows of such a work, we are happy to add it. The closest such related work is that of Rolland et. al.\n\n- RW on Bayesian optimization to learn the graph structure;\n- - We do not use Bayesian optimization to learn the graph structure. We use a random sampling strategy over the domain. This is described in Algorithm 4.\n\n- How to observe the surrogate Hessian of the black-box function?\n- - We discuss this in Appendix A: To estimate the Hessian, we used the Hessian-Vector product approximation. We relaxed the discrete portions of our metamodel policy into differentiable continuous approximation for this phase using the Sinkhorn-Knopp algorithm for the Role Assignment phase. For role interaction network connectivity, we used a sigmoid to create differentiable \u201csoft\u201d edges between each role.\n\n- What is the motivations of using Hessian in BO? If you already observed Hessian, why not using the gradient descent?\n- - The motivation for using the Hessian in BO is to generalize the theoretical arguments of Srinivas et. al., Duvenaud et. al., Kandasamy et. al. and Rolland et. al. We believed that by using the Hessian a tighter regret bound in the high-dimensional Bayesian optimization could be obtained. We're not sure what the second part of the question means. Could the reviewer clarify?\n\n- what do you mean by JIT compilations? The meaning is not clear in the context of multi-agent policy structure.\n- - JIT compilation is taking the immutable (unchanging) part of the metamodel policy (Alg. 1,2,3) and combining it with the mutable (changing) part of the metamodel policy (the parameters) and constructing the actual model which takes decisions.\n\n- Why do you use MPNN rather than graph convolution, graph attention or other transformer vi rants? These models should be more expressive.\n- - As we were interested in making contributions to the decision making systems field, we were not interested in using these techniques from the deep learning literature. It could certainly be the case that using these architectures could be more expressive. We are not able to comment on this as we are not in the deep learning field.\n\n- Regarding $\\theta_{r,i}$ and other portions of the metamodel parameter space.\n- - We defined these terms implicitly when we used them. Unfortunately due to the space constraints of the conference format, this was the only way we could submit this paper to a conference.\n- - These terms are defined, implicitly, in Section 4.1, 4.2, as well as in the graphical illustration of Fig. 2.\n- - We also invite the reviewer to read the code included in the supplementary materials, which we have tried to make very understandable and readable.\n\nWe continue with further answers in the next comment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699645328549,
                "cdate": 1699645328549,
                "tmdate": 1700314606227,
                "mdate": 1700314606227,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tvEaFRBYFY",
                "forum": "KKBZzMLGvH",
                "replyto": "jvykYvQYOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 22Fs (continued)"
                    },
                    "comment": {
                        "value": "- Superscript $(i)$ in Section 4.4\n- - Superscript $(i)$ represents the maximal clique within the additive decomposition. Here a complex high dimensional function space being search over is simplified by assuming it is constructed through addition of functional subspaces. This allows us to use the nice properties of linearity of addition. We touch upon this further in the proof of Proposition 1 in Appendix E.\n\n- What is the mathematical expression of regret $r(t)$ of querying Hessian?\n- - The regret mathematical expression is related to the model of computation we are analyzing in. For example, often time or space complexity is analyzed in order to understand and evaluate the quality of algorithms. Communication complexity is another complexity term that is used in distributed computing environments. In decision making algorithms, typically the key complexity is the regret, which compares the decision made to the best possible decision that could have been made with perfect information.\n\n- What is $\\mathcal{U}$ in Algorithm 4?\n- - $\\mathcal{U}$ represents the uniform random distribution that we sample from in order to learn the dependency structure.\n\n- What is the Max-Cliques operation in Algorithm 4?\n- - The Max-Cliques operation in Algorithm 4 computes the maximal cliques that was discussed earlier in our response. This is related to finding the Max-Cliques that allow us to simplify the functional space which we discuss further in the proof of Proposition 1. We note that typically, this specific operation is NP-hard in time complexity, however, this is typically thought not to be a problem within the model of computation we are analyzing in (decision making).\n- - This specific operation was approximated quickly using the NetworKit library."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699676579405,
                "cdate": 1699676579405,
                "tmdate": 1700314637672,
                "mdate": 1700314637672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rgz59BKsE3",
                "forum": "KKBZzMLGvH",
                "replyto": "jvykYvQYOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 22Fs (continued)"
                    },
                    "comment": {
                        "value": "We want to highlight our defined term, \"JIT-compilation.\"\n\nIn the introduction we describe, \"To achieve this, we use a metamodel which generates a policy model. The metamodel is divided into immutable instructions (i.e., algorithms) corresponding to the abstractions of the role and role interaction and mutable parameters that are just-in-time (JIT) compiled into a policy model during evaluation.\"\n\nIn section 4.3, we describe, \"To overcome these challenges, we propose a metamodel which JIT-compiles into a graphical model. The JIT-compilation is conditional on the agents\u2019 state thus capturing dynamic role interactions; in addition the compilation allows for a more compact policy space with far fewer parameters. The resultant compiled graphical model captures the state-dependent interaction between roles and yields the resultant actions for each role. After compilation, the interaction between roles are captured by the resultant conditional random field.\"\n\nAlso in section 4.3, we describe, \"The graphical model compilation procedure is presented in Algorithm 2. Finally, Algorithm 3 drives the JIT-compilation.\"\n\nWe're working on adding a table of notations in order to aid readability which we can include in the Appendix. We will be uploading this shortly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699879478358,
                "cdate": 1699879478358,
                "tmdate": 1699879589690,
                "mdate": 1699879589690,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NaHynE4nAx",
                "forum": "KKBZzMLGvH",
                "replyto": "jvykYvQYOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_22Fs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_22Fs"
                ],
                "content": {
                    "title": {
                        "value": "Re authors' response"
                    },
                    "comment": {
                        "value": "I apologize that I didn't see the proofs in the appendix in the initial review.\n\nHowever, I don't find the authors' response satisfactory. It remains unclear of the motivation of doing Hessian aware algorithm for black-box optimization and how costly it will be. Hessian type of methods are like double-sword but the author didn't make it clear what's the disadvantages and limitations. \nThe authors said that they would do their best to improve the paper and would update the paper. However, till now, there has been no revision uploaded (I checked the revision history and found it empty). Because the paper was poorly written, the paper remains poorly written since there is no update. Thus, it is still difficult for me to understand and assess the contribution of the paper. \n\nTherefore, I would keep my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700696068248,
                "cdate": 1700696068248,
                "tmdate": 1700696068248,
                "mdate": 1700696068248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "69IJ6Jo7nE",
                "forum": "KKBZzMLGvH",
                "replyto": "jvykYvQYOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "- We will immediately reach out to the PC as this should not be the case. We have uploaded many revisions for the paper.\n- We elaborate on the costs and benefits of the Hessian several times within the paper, and show that due to the expensive nature of the Hessian, we focus our specific use case to only decision making systems where a useful and valuable surrogate Hessian can be extracted from observing the policy generation process.\n- - Introduction: In our work, we overcome this shortcoming by observing the GEN process of the HOM. In particular, we can measure a surrogate Hessian during the GEN process which significantly simplifies the task of learning the additive structure. We term this approach Hessian-Aware GP-UCB (HA-GP-UCB) and visualize our approach in Fig. 1. \n- - Section 4.4: In practice, observing the hessian $H_v$ is not possible due to v being a black box function. However, during the GEN process we can observe a surrogate Hessian, $H_\\pi$. This surrogate Hessian is closely related to the $H_v$ as $v(\\theta)$ is determined through interaction of the policy with an unknown environment. Because the value of a policy is a function of the policy; it follows by the chain rule $H_\\pi$ is an important sub-component of $H_v$. We utilize the surrogate Hessian in our work and demonstrate its strong empirical performance in validation.\n- - Appendix H: In Section 4.5 we remarked that although we cannot observe $H_v$, we can observe a surrogate hessian, $H_\\pi$ which is related to $H_v$ by the chain rule. We justify our choice here with showing how $H_\\pi$ is an important sub-component of $H_v$ (Skorski, 2019).\n- - Appendix H: A possible avenue of overcoming this limitation is considering Hessian estimation through zero\u2019th order queries. Several works along this direction have recently appeared using Finite Differences (Cheng et al., 2021), as well as Gaussian Processes (M\u00fcller et al., 2021). We consider removing this dependency on the surrogate Hessian for future work.\n\n\n\nWe want to especially highlight, our approach is not a general purpose approach to Bayesian Optimization. Our approach is an approach to solving difficult problems in decision making systems *using* Bayesian Optimization. General purpose HA-GP-UCB is outside the scope of this work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700468009,
                "cdate": 1700700468009,
                "tmdate": 1700724861409,
                "mdate": 1700724861409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HpSzkiAIvG",
                "forum": "KKBZzMLGvH",
                "replyto": "69IJ6Jo7nE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_22Fs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_22Fs"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the quick response. \nTo make it easier to see the difference, would you mind highlighting the revised part? Since it seems that open review didn't have a revision history, there is only one PDF, so difficult to compare."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702423447,
                "cdate": 1700702423447,
                "tmdate": 1700702423447,
                "mdate": 1700702423447,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rNhGJIJwOn",
                "forum": "KKBZzMLGvH",
                "replyto": "Zo30RaOSnW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_22Fs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_22Fs"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for highlighting the revision. \n\nI understand that the rebuttal period is time-limited so it is difficult to do a major revision. However, because the major concern is the paper is hard to follow, the paper remains hard to follow (though I worked on multiagent Bayesian optimization and marl before). I am afraid that I am unable to change my score given the current status of the paper."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709247824,
                "cdate": 1700709247824,
                "tmdate": 1700709247824,
                "mdate": 1700709247824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pleSGnJQ2E",
                "forum": "KKBZzMLGvH",
                "replyto": "jvykYvQYOq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for continuing to engage with us. I empathize with the reviewer, and I can confidently state that this work attempts to have the most content within the conference format. This comparison is with the other works I am reviewing right now across major conferences associated with machine learning.\n\nWe want to highlight the key points that went into the research and construction of this paper which make it a valuable contribution:\n- Our work needed to be theoretically grounded requiring a regret bound.\n- Our work was in the subfield of high-dimensional Bayesian optimization requiring us to fix the incorrect proof by Kandasamy et. al. and give a non-trivial and complete regret bound when compared to the work of Rolland et. al.\n- Our work needed to have strong empirical results in at least one subfield of machine learning to justify our approach.\n- Our work needed to have strong empirical results in a currently valuable field of machine learning to have a chance of acceptance to the conference format.\n\nGiven the above constraints, our paper attempted to satisfy all these requirements which made it hard to follow. We thank the reviewer for their work in reviewing this paper."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712490512,
                "cdate": 1700712490512,
                "tmdate": 1700712574297,
                "mdate": 1700712574297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6sF26WDsle",
            "forum": "KKBZzMLGvH",
            "replyto": "KKBZzMLGvH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_z1BB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8961/Reviewer_z1BB"
            ],
            "content": {
                "summary": {
                    "value": "A decision making system determine a sequence of actions to achieve a desired goal.  To solve such a problem, the authors use Bayesian optimization where gradient information is not actively utilized.  In particular, the authors propose Hessian-aware Bayesian optimization to optimize a multi-layer architecture.  Finally, some theoretical and numerical results are reported to show the validity of the proposed methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* It solves an interesting problem."
                },
                "weaknesses": {
                    "value": "* This paper is hard to follow.  There are too many components but they are not described appropriately.\n* Writing and presentation should be improved.  For example, the Introduction section is started by the following sentences: \"Decision Making Systems choose sequences of actions to accomplish a goal. Multi-Agent Decision Making Systems choose actions for multiple actors working together towards a shared goal. Multi-Agent Reinforcement Learning (MARL) has emerged as a competitive approach for optimizing Decision Making Systems in the multi-agent settings\"  These are three independent sentences.  This part should be re-written.  In addition, a period is missing.  Also, this sentence \"We propose the usage of Bayesian Optimization (BO)\" is somewhat unnatural.  Please revise it.  There are other cases, but I would not enumerate all of them.  Please revise an article carefully.\n* Figures are too small and legends are overlapped with graphs; please see Figures 4 and 5.\n* Theoretical results heavily rely on the previous work, in particular (Srinivas et al., 2010).\n* I would like to ask about the results in Figure 5.  First off, why do BO results only show current maxima until a particular iteration, i.e., monotonically increasing?  Why are the other results fluctuated?  Also, why do the results at initial iterations differ across algorithms?  It seems unfair to the baseline methods.  Moreover, some baseline methods are better than the proposed methods.  These results seem reasonable.  Why do the authors use Bayesian optimization instead of reinforcement learning in these problems?"
                },
                "questions": {
                    "value": "* Does Table 1 show the superior performance of your algorithm?  To my understanding, HA-GP-UCB does not outperform some algorithms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8961/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699582759037,
            "cdate": 1699582759037,
            "tmdate": 1699637127452,
            "mdate": 1699637127452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AUF9HFJzRc",
                "forum": "KKBZzMLGvH",
                "replyto": "6sF26WDsle",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer z1BB"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe are looking into making changes to the draft to aid in readability. Here are immediate answers for your questions:\n\n- This paper is hard to follow. There are too many components but they are not described appropriately.\n- - We are looking into how best to edit the main paper text and the appendix in order to improve the readability of the paper.\n\n- Writing and presentation should be improved. For example, the Introduction section is started by the following sentences: \"Decision Making Systems choose sequences of actions to accomplish a goal. Multi-Agent Decision Making Systems choose actions for multiple actors working together towards a shared goal. Multi-Agent Reinforcement Learning (MARL) has emerged as a competitive approach for optimizing Decision Making Systems in the multi-agent settings\" These are three independent sentences. This part should be re-written. In addition, a period is missing. Also, this sentence \"We propose the usage of Bayesian Optimization (BO)\" is somewhat unnatural. Please revise it. There are other cases, but I would not enumerate all of them. Please revise an article carefully.\n- - We are looking into how best to edit this part of the paper. We do want to highlight that the purpose of these three sentences is to highlight and show that, our specific contribution is to the field of multi-agent decision making systems, not reinforcement learning, or Bayesian optimization. As this is the case, we needed to frame and introduce all of these areas and how they're related within the introduction and how our work fits in the context of these areas.\n\n- Figures are too small and legends are overlapped with graphs; please see Figures 4 and 5.\n- - We can include larger figures which are the exact same as Figure 4 and 5 in the Appendix. As our contribution requires significant validation, we had to make the figures a bit smaller to present all of the important validation in the main text.\n\n- Theoretical results heavily rely on the previous work, in particular (Srinivas et al., 2010).\n- - We respectfully disagree. We believe that our work builds on top of Srinivas et. al., as well as Duvenaud et. al., Kandasamy et. al. and Rolland et. al. We do want to highlight that our theoretical results are novel as they provide a regret bound that scales with O(log D) with the dimension, which is significantly better than other bounds in high dimensional Bayesian optimization.\n\n- I would like to ask about the results in Figure 5. First off, why do BO results only show current maxima until a particular iteration, i.e., monotonically increasing? Why are the other results fluctuated? Also, why do the results at initial iterations differ across algorithms? It seems unfair to the baseline methods. Moreover, some baseline methods are better than the proposed methods. These results seem reasonable. Why do the authors use Bayesian optimization instead of reinforcement learning in these problems?\n- - This is intimately related to how (in the area of decision making systems) different approaches are evaluated. In reinforcement learning based decision making systems, the policy that is evaluated is the singular policy that is being trained using reinforcement. In Bayesian optimization, the policy that is evaluated is the best found policy so far. Hence the difference between policy search (Bayesian optimization) and policy reinforcement (reinforcement learning). Our validation and our results show in resource constrained environments, with malformed or sparse reward, our approach of policy search using Bayesian optimization outperforms policy reinforcement methods such as various reinforcement learning methods we validated against.\n\n- Results in Table 1\n- - In Table 1 we show the conditions under which HA-GP-UCB outperforms related work. This is related to the motivation and problem statement of the paper where we are interested in sparse or malformed rewards in cooperative multi-agent decision making scenarios."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699644161701,
                "cdate": 1699644161701,
                "tmdate": 1699674750461,
                "mdate": 1699674750461,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XE9vzAenIk",
                "forum": "KKBZzMLGvH",
                "replyto": "AUF9HFJzRc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_z1BB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Reviewer_z1BB"
                ],
                "content": {
                    "comment": {
                        "value": "Most of my concerns still remain.\n\nIn particular, I do not agree with the statement on the difference of assessment criteria between BO and RL. They should be compared with the same criteria.  If they are different, how do we understand and interpret the comparisons between the results?\n\nTherefore, I would like to keep my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591660018,
                "cdate": 1700591660018,
                "tmdate": 1700591660018,
                "mdate": 1700591660018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bQfz3CbLpg",
                "forum": "KKBZzMLGvH",
                "replyto": "6sF26WDsle",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8961/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for engaging in discussion with us.\n\nAlthough it is highly unusual, and we have not seen results in reinforcement learning papers displayed in such a manner. We have created a new Appendix, Appendix K, where we replot the results from reinforcement learning approaches using the \"best policy found so far\" approach similar to BO. We want to point out that in practice, this is not how reinforcement learning works. However, in the spirit of giving reinforcement learning the best benefit of the doubt, we have created Appendix K with these results.\n\nIn this Appendix, we see that all our conclusions from the paper still remain and stand.\n\nIn addition, we repeat the same procedure for Table 5, and Table 6 which investigate RL and MARL under sparse or malformed reward. This is presented in Appendix L. In Appendix L, we see that all our conclusions from the paper still remain and stand as shown by Table 7 and Table 8.\n\nWe have also expanded footnote 10 to refer to Appendix J, Appendix K, and Appendix L showing these alternate presentations of data.\n\nWe hope that this helps the reviewer compare the two approaches of policy search and policy reinforcement. We also note that in all these cases, the data presented is overly favorable to RL and MARL, yet still our conclusions and observations hold.\n\nWe are also happy to help clarify any other concerns and engage with the reviewer further for this paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8961/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625456334,
                "cdate": 1700625456334,
                "tmdate": 1700638555118,
                "mdate": 1700638555118,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]