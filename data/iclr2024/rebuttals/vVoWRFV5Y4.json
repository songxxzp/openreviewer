[
    {
        "title": "Solving the Quadratic Assignment Problem With Deep Reinforcement Learning"
    },
    {
        "review": {
            "id": "hASFBkaYh1",
            "forum": "vVoWRFV5Y4",
            "replyto": "vVoWRFV5Y4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6125/Reviewer_Gzbd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6125/Reviewer_Gzbd"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors investigated the Quadratic Assignment Problem (QAP) solving using deep reinforcement learning with double pointer networks, where an upper pointer network selects locations and a lower pointer network selects facilities. And the proposed method is evaluated on synthetic QAP instances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1- This paper is well motivated as the QAP has been rarely studied using DRL;\n2- It seems the proposed method works on the self-generated dataset."
                },
                "weaknesses": {
                    "value": "1- It seems that from the DRL based heuristic perspective, the objective function in QAP does not make difference compared with the ones in ILP, both of which are just captured by a reward in DRL. In this sense, the QAP in this paper is almost the same as the vehicle and customer assignment in VRPs with linear objectives, such as\n[a] Deep Reinforcement Learning for Solving the Heterogeneous Capacitated Vehicle Routing Problem. IEEE T Cybernetics;\n[b] Learning to Solve Vehicle Routing Problems with Time Windows through Joint Attention. Arxiv;\n[c] Solving NP-hard Min-max Routing Problems as Sequential Generation with Equity Context. Arxiv.\nThose works also involve sequentially or parallelly selecting vehicles and customers, while using (advanced) Transformers rather than pointer network.\n\n2- Besides, the Transformers in 'Learning Improvement Heuristics for Solving Routing Problems. TNNLS' outputs a probability matrix, which could also be tailored to the probability of facility and location pair with proper masking.\n\n3- The evaluation is quite simple, which solely focuses on one single problem with randomly generated instances. And the baselines are also insufficient and not strong enough.\n\nOverall, the method falls short of novelty, and the evaluation is inadequate, which is below the standard of an ICLR publication."
                },
                "questions": {
                    "value": "Please see the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697968101163,
            "cdate": 1697968101163,
            "tmdate": 1699636663235,
            "mdate": 1699636663235,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JlR14QD7hX",
                "forum": "vVoWRFV5Y4",
                "replyto": "hASFBkaYh1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6125/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the feedback. While we focus on a facility location motivation, it is true that the quadratic assignment problem can be viewed as a generalization of several other combinatorial optimization problems such as the traveling salesman problem and some vehicle routing problems \u2013 indeed, we feel this is motivation for developing a DRL methodology to solve it. We have added the suggested references to our literature review, and we are working to implement these suggestions (as well as those made by other referees) to improve performance and narrow the gap with our baseline methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667880702,
                "cdate": 1700667880702,
                "tmdate": 1700667880702,
                "mdate": 1700667880702,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cVlXMNEbZK",
            "forum": "vVoWRFV5Y4",
            "replyto": "vVoWRFV5Y4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6125/Reviewer_DVdt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6125/Reviewer_DVdt"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the end-to-end application of Deep RL for solving the QAP. Because the QAP necessitates assignments between a set of facilities and locations, the authors introduce a novel autoregressive model that sequentially selects a location and then a facility (to make a pair), and then repeats this process until the assignment is done. The results are promising. The derived solutions are, on average, within 7.5% of those generated by a heuristic method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper represents the pioneering effort to tackle the QAP using a neural network-based approach, marking an important milestone in the field. The paper is well-structured, with contents presented in a manner that's easy to follow and understand."
                },
                "weaknesses": {
                    "value": "The results are too weak for ICLR publication. While one could argue that the considerable optimality gap might be attributed to the intrinsic complexity of the QAP, it's evident that a notable portion arises from the authors' dependence on a neural net model and the training methods that closely mirror the early contributions of Bello et al. that might not be the best fit.\n\nTo draw a parallel, when the pointer network was initially applied to solve the TSP in an end-to-end manner, there existed an optimality gap of around 7% for 100-node TSPs. However, with subsequent refinements in models and training techniques, that gap has been narrowed to almost 0% now. \n\nIt's plausible that similar methodological advancements could significantly benefit the QAP approach presented here. I'd recommend reconsidering the use of the Critic network for such combinatorial optimization tasks, especially given the challenges of predicting the final outcome midway through solution construction.\n\nA few references that could be helpful are:\n[model]\nMatrix Encoding Networks for Neural Combinatorial Optimization (Kwon, et al.)\nLearning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer (Ma, et al.)\n[RL method]\nPOMO: Policy Optimization with Multiple Optima for Reinforcement Learning (Kwon, et al.)"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6125/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6125/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6125/Reviewer_DVdt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698550339473,
            "cdate": 1698550339473,
            "tmdate": 1699636663126,
            "mdate": 1699636663126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gJqjRKlfTk",
                "forum": "vVoWRFV5Y4",
                "replyto": "cVlXMNEbZK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6125/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We really appreciate the constructive suggestions as to how we can improve model performance and narrow the gap with existing methods. We have added references to the suggested works, and are working to integrate some of these ideas into our model. In particular, we are thinking about how to improve the performance of our critic network, as well as how to select a more problem-specific attention mechanism."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667866072,
                "cdate": 1700667866072,
                "tmdate": 1700667866072,
                "mdate": 1700667866072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H1CtCCjgvq",
            "forum": "vVoWRFV5Y4",
            "replyto": "vVoWRFV5Y4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6125/Reviewer_tFs4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6125/Reviewer_tFs4"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an RL method to solve quadratic assignment problems. The authors reformulate the original QAP as a seq2seq problem. However, the paper is not easy to follow. The motivations and contributions of the work are not clear.  The details of the method are missing. The experimental evaluation is not good enough. Therefore, I think the paper is under the bar of ICLR in its current form."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The topic of using RL to solve the quadratic assignment problem is interesting.\n\nThe authors reformulate the original QAP as a seq2seq problem."
                },
                "weaknesses": {
                    "value": "The motivation and contribution of the work is not clear.\n\nThe paper is not easy to follow.\n\nThe literature review is not good enough. A lot of classic literature is missing.\n\nThe experimental evaluation is not enough. Only Gurobi and swap-based local search heuristic is compared. Which makes the paper less convincing. \n\nThe size of QAP used in the experiment is quite small. \n\nThe performance is quite poor, which cannot outperform simple heuristics such as SWAP."
                },
                "questions": {
                    "value": "Please see the weeknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552197709,
            "cdate": 1698552197709,
            "tmdate": 1699636663006,
            "mdate": 1699636663006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D8jf8Gekkm",
                "forum": "vVoWRFV5Y4",
                "replyto": "H1CtCCjgvq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6125/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. We have expanded the literature review to include some additional relevant references suggested by the referees, as well as clarified the motivation and contribution in the introduction. Regarding the evaluation, we feel that adding further baselines is difficult, since to our knowledge there are no other RL frameworks for QAP. We are working on improving performance relative to the swap heuristic by making tweaks to our model architecture, following the suggestions of other reviewers. We are also working to improve scalability by reducing model size where it is possible to do so without compromising performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667845748,
                "cdate": 1700667845748,
                "tmdate": 1700667845748,
                "mdate": 1700667845748,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CpW57f0z9B",
            "forum": "vVoWRFV5Y4",
            "replyto": "vVoWRFV5Y4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6125/Reviewer_jXD8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6125/Reviewer_jXD8"
            ],
            "content": {
                "summary": {
                    "value": "The Quadratic Assignment Problem (QAP) is an NP-hard problem with significant challenges, especially for larger instances. The paper proposes using deep reinforcement learning (DRL) to address the QAP, introducing a novel double pointer network to tackle the Koopmans-Beckman formulation of QAP. The method is trained using A2C on synthetic datasets, and its performance is benchmarked against a swap-based local search heuristic."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Using deep reinforcement learning to address the QAP is an innovative method, setting it apart from traditional optimization techniques. The double pointer network alternates between selecting locations and facilities, providing a dynamic solution approach. The model is trained on a large dataset of synthetic instances, making it robust and generalizable."
                },
                "weaknesses": {
                    "value": "The method is tested primarily for QAP instances up to size 20, highlighting a potential scalability concern. While the DRL approach shows promise, there is still a performance gap when compared to the swap heuristic and the Gurobi solver. Unlike traditional optimization methods which provide a solution and its optimality certificate, this DRL approach only gives a solution."
                },
                "questions": {
                    "value": "Why was the swap-based local search heuristic chosen as the baseline? \nIs the performance gap mentioned equivalent to the standard duality or MIP gap commonly used in optimization? If not, how do they differ? The obscurity in the definition makes it difficult to assess the performance of the new method. \nHow does the DRL method scale with larger problem sizes, especially when compared with traditional algorithms?\nWhy is training limited to 20 minutes and 1 hour for n = 10 and n = 20? How will performance improve if the training time is increased?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6125/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6125/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6125/Reviewer_jXD8"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6125/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812914691,
            "cdate": 1698812914691,
            "tmdate": 1699636662898,
            "mdate": 1699636662898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yFdRO05X7i",
                "forum": "vVoWRFV5Y4",
                "replyto": "CpW57f0z9B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6125/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6125/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the feedback. Regarding baselines: we chose the swap heuristic because it provides near-optimal solutions (within about 1% of the provable optimum found by Gurobi) in a very short time. The gap provided is therefore a (nearly tight) lower bound and good approximation of the standard MIP gap. Regarding scalability, we are working on reducing the number of model parameters to generalize to larger sizes; while scalability is indeed a concern, this is even more true for Gurobi, which is not able to solve the problem in hours for sizes larger than 20. Regarding training time, we clarify that the reported training times of 20 minutes and 1 hour were per training epoch."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6125/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667826370,
                "cdate": 1700667826370,
                "tmdate": 1700667826370,
                "mdate": 1700667826370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]