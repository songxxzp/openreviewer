[
    {
        "title": "Long-distance Targeted Poisoning Attacks on Graph Neural Networks"
    },
    {
        "review": {
            "id": "GVIMEOH8ag",
            "forum": "QHfIe4chR5",
            "replyto": "QHfIe4chR5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3789/Reviewer_eKgy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3789/Reviewer_eKgy"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the vulnerability of Graph Neural Networks (GNNs) to targeted poisoning attacks, where an attacker manipulates the graph to misclassify a specific node. Most existing attacks focus on manipulating nodes within the node's neighborhood, but this paper explores \"long-distance\" attacks, where the manipulated nodes are outside this neighborhood. The paper presents a principled optimization-based approach for small graphs but also offers a more cost-effective heuristic-based approach for larger graphs. The findings indicate that long-distance targeted poisoning is effective and challenging to detect by existing GNN defense mechanisms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is well-written, offering a clear and easily understandable presentation of the research.\n+ The approach and contributions made by the paper are noteworthy, particularly the exploration of long-distance targeted poisoning attacks in GNNs, even though the proposed method is primarily heuristic in nature."
                },
                "weaknesses": {
                    "value": "- The MetaLDT method, while promising, appears to demand significant time and computational resources, which may limit its practicality for larger graphs.\n- The MimicLDT approach, while addressing the cost concerns, seems to compromise on the effectiveness of the attack. \nThis trade-off between efficiency and success rate should be discussed better.\nSome aspects of the paper's approach require further clarification. Additional details and explanations could help the reader better understand the methodology and its intricacies, enhancing the overall quality of the paper."
                },
                "questions": {
                    "value": "- The definition of \"long distance\" and the specific distance of the injected malicious nodes remain unclear in the current version of the paper.\n\n- Have you taken into account the influence of robust methods during the poisoning process? If so, what are the results regarding the method's effectiveness when attackers lack knowledge of defense mechanisms, which is more practical in real-world scenarios?\n\n- The rationale behind why MimicLDT is more efficient is not clearly articulated. Further elaboration on this aspect would be beneficial. Is it possible to discuss the trade-off between efficiency and effectiveness, such as exploring adjustments to hyperparameters to strike a balance between MimicLDT and MetaLDT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3789/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3789/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3789/Reviewer_eKgy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801570381,
            "cdate": 1698801570381,
            "tmdate": 1699636335897,
            "mdate": 1699636335897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WPsczsh2zj",
                "forum": "QHfIe4chR5",
                "replyto": "GVIMEOH8ag",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed review and valuable feedback. We address your comments and questions one by one below.\n\n**[W.1.]** *This trade-off between efficiency and success rate should be discussed better.* \\\n**[A.1.]** We clarify the tradeoff in our answer to your Question.3.  Thank you for the suggestion.\n\n----- \n\n**[Q.1.]** *The definition of \"long distance\" and the specific distance of the injected malicious nodes remain unclear in the current version of the paper.* \\\n**[A.1.]** We define long distance as following, for any attack point $v_a$, the shortest path length from $v_a$ to the target node $v_t$ should be larger than the number of layers used by the victim GNN model. In our experiments, we set $k=3$. We will emphasize the definition more clearly in revision. \n\n----- \n\n**[Q.2.]** *Have you taken into account the influence of robust methods during the poisoning process? If so, what are the results regarding the method's effectiveness when attackers lack knowledge of defense mechanisms.* \\\n**[A.2.]** In the experiments reported in the paper, MetaLDT\u2019s inner training loops and MimicLDT\u2019s surrogate models account for what GNN defenses are used. In the table below, we include results in a setting where MimicLDT\u2019s surrogate model does not account for GNN defenses, and we instead use vanilla GCN as the surrogate model. \n|  | Surrogate | GCN | GraphSAGE | GAT | GNNGuard | SoftMedianGDC | JaccardGCN | SVDGCN | ProGNN |\n| ----- | ----- | -----| ----- | ----- | ----- | ----- |----- |  ----- |  ----- |\n| Cora | Aware the defense | 0.67 | 0.63 | 0.60 | 0.70 | 0.55 | 0.66 | 0.74 | 0.59 |\n|    | Vanilla GCN | 0.67 | 0.63 | 0.59 | 0.68 | 0.29 | 0.65 | 0.35 | 0.53 |\n| arXiv | Aware the defense | 0.74 | 0.73 | 0.70 | 0.64 | 0.59 | 0.63 | 0.62 | 0.58 |\n|    | Vanilla GCN | 0.74 | 0.72 | 0.67 | 0.64 | 0.42 | 0.61 | 0.49 | 0.55 |\n\nWe observe that for some defenses (e.g., SAGE, GNNGuard), not knowing about the defenses has little or no impact on poison success rate, but for others (e.g., SVDGCN and SoftMedian) the effects are more noticeable.  Table 9 in the paper\u2019s appendix evaluates MetaLDT under a similar scenario, i.e., in a case where its inner-training loop does not account for defenses, and leads to similar conclusions. \n\n----- \n\n**[Q.3.]** *The rationale behind why MimicLDT is more efficient is not clearly articulated. Further elaboration on this aspect would be beneficial. Is it possible to discuss the trade-off between efficiency and effectiveness, such as exploring adjustments to hyperparameters to strike a balance between MimicLDT and MetaLDT?* \\\n**[A.3.]** MimicLDT uses certain design heuristics to restrict the optimization to a much smaller search space than MetaLDT, and is thus more efficient. In particular, MimicLDT does not optimize for which attack points to use nor the graph structure among injected nodes. Furthermore, MimicLDT uses a surrogate optimization goal (trying to use injected nodes to cause an attack point and the target to collide in the embedding space) to determine injected node features. This is in contrast to MetaLDT whose optimization considers the entire space of attack node choices and injected node/graph structures and uses meta-learning (which requires very expensive inner training) to achieve direct end-to-end optimization of the label flipping probability.\n\nIn terms of the trade-off between MetaLDT and MimicLDT, we note that MetaLDT always has significantly higher computational cost than MimicLDT in order to achieve any decent poisoning rate, e.g., in Figure 5 of the Appendix, we show that, for Cora and GCN, MetaLDT requires a minimum of 8 inner training rounds and 9670.46s to reach the same poison success rate that MimicLDT can reach in about 43.17 seconds. On the other hand, MetaLDT\u2019s larger search space means that it can often achieve higher poison success rates than MimicLDT can if time is not a concern.\n\n-----"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339900961,
                "cdate": 1700339900961,
                "tmdate": 1700339900961,
                "mdate": 1700339900961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "arXHYz7DgQ",
                "forum": "QHfIe4chR5",
                "replyto": "WPsczsh2zj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_eKgy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_eKgy"
                ],
                "content": {
                    "title": {
                        "value": "Reply for Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for clarifying my concerns in your rebuttal. I will maintain my original score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621590280,
                "cdate": 1700621590280,
                "tmdate": 1700621590280,
                "mdate": 1700621590280,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FYeL6X1a83",
            "forum": "QHfIe4chR5",
            "replyto": "QHfIe4chR5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3789/Reviewer_E2SS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3789/Reviewer_E2SS"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates targeted poisoning attacks on GNNs, in which an attacker injects nodes in a graph to cause a target node to be incorrectly classified to a label of the attacker\u2019s choosing and considers that the attacked nodes are not within the targeted node\u2019s k-neighborhood.  The proposed attack is then evaluated and tested against some empirical defenses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+The studied problem is interesting"
                },
                "weaknesses": {
                    "value": "-Threat model is strong\n- Novelty is limited\n-Missing many references"
                },
                "questions": {
                    "value": "My major concern is that the problem has been extensively studied, and the novelty is not sufficient. \n\nThe authors claim that most of existing attacks on GNNs modify the target node\u2019s k-hop neighborhood, but this is not accurate. For instance, most of the cited poisoning attacks focus on global structure attack, where the entire graph structure can be modified. \n \nThe threat model assumes that the attacker has access to the training data, (including the original graph G, node features, and labels, and also knows the training procedure), which is a rather strong assumption. There exist (restricted) black-box attacks to GNNs, while the authors do not compare and discuss with them \n\nThe evaluated empirical defenses are easy to be broken by stronger attacks, as demonstrated in [a]. Hence, it is not surprising that these defense cannot defend against the proposed attack. \n\n[a] Felix Mujkanovic, Simon Geisler, Stephan G\u00fcnnemann, and Aleksandar Bojchevski. Are defenses for graph neural networks robust? Advances in Neural Information Processing Systems 35 (NeurIPS2022), 2022.\n\nIn fact, there exist many certified defenses against graph structure attacks, but the authors do not test them against the proposed attack."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807217292,
            "cdate": 1698807217292,
            "tmdate": 1699636335824,
            "mdate": 1699636335824,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SSTQrV8QvC",
                "forum": "QHfIe4chR5",
                "replyto": "FYeL6X1a83",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the valuable feedback. We address your concerns and questions below.\n\n**[Q.1.]** *The authors claim that most of existing attacks on GNNs modify the target node\u2019s k-hop neighborhood, but this is not accurate. For instance, most of the cited poisoning attacks focus on global structure attack, where the entire graph structure can be modified.* \\\n**[A.1.]** Although existing poisoning attacks can modify any part of the entire graph structure, the results of their optimization end up exclusively changing the target\u2019s k-hop neighborhood.  Furthermore, existing attacks based on meta learning can not scale to large graphs and thus are not practical.\n\n----- \n\n**[Q.2.]** *There exist (restricted) black-box attacks to GNNs, while the authors do not compare and discuss with them* \\\n**[A.2.]** To the best of our knowledge, no existing black-box GNN attacks can be applied to our setting, i.e., long-distance node-injection targeted poisoning. While we do not discount the possibility that potential black box attacks for this setting could be developed, doing so would require significant further research. We will add a discussion about black box attacks to a revised version of our paper.\n\n----- \n\n**[Q.3.]** *There exist many certified defenses against graph structure attacks, but the authors do not test them against the proposed attack.* \\\n**[A.3.]** We did not evaluate certified defenses because we do not know of any existing implementations that can be applied to our setting. Many of the certified defenses for GNNs, such as Wang et. al KDD\u201921 [1] and Tao et. al WSDM\u201921 [2], cannot handle node injection attacks. The only work that handles node injection is by Tao et al [3], however, no public implementation is available. \n \n[1] Wang, B., Jia, J., Cao, X., & Gong, N. Z. (2021, August). Certified robustness of graph neural networks against adversarial structural perturbation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (pp. 1645-1653). \\\n[2] Tao, S., Shen, H., Cao, Q., Hou, L., & Cheng, X. (2021, March). Adversarial immunization for certifiable robustness on graphs. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining (pp. 698-706). \\\n[3] Tao, S., Cao, Q., Shen, H., Wu, Y., Hou, L., & Cheng, X. (2023). Graph adversarial immunization for certifiable robustness. IEEE Transactions on Knowledge and Data Engineering.\n\n-----"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700339137748,
                "cdate": 1700339137748,
                "tmdate": 1700339137748,
                "mdate": 1700339137748,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v8vDYmR46y",
            "forum": "QHfIe4chR5",
            "replyto": "QHfIe4chR5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies targeted poisoning attacks on graph neural networks, which aims to cause misclassification of a single victim node. In order to increase the stealthiness of the attack, the injected poisoning points do not belong to the top-k neighbors of the target victim. The edges and node features of the fake injected nodes are optimized through meta-learning for small graphs and through feature-collision for larger graphs. Empirically, the proposed attack performs better compared to existing short-distance attacks, when the manipulatable nodes are far from the target nodes."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The attack performance is good compared to other baselines when the attackers can only manipulate nodes that are far from the target victim.\n2. The approach of summarizing the attack patterns from expensive attacks (on small graphs) to design efficient attacks scalable for larger graphs is good."
                },
                "weaknesses": {
                    "value": "1. The motivation of considering nodes that are outside the top-k neighbors of the target victim is unclear. The authors argued in the appendix that, using some graph explanation tools, the attached nodes can be retried relatively well in some settings. However, the authors made an implicit assumption that such a tool can be directly treated as a detection method, without distinguishing the differences between the influential nodes for the target victim and other nodes. Can we use some threshold to filter out suspicious looking influence nodes for the node under examination? Will this filtering step can be evaded by some adaptive attacks so that short-distance attacks can still survive without sacrificing the effective much? \n2. From the technical perspective, I did not find a significant (inherent) difference from the previously proposed Meta-Attack, as the major the differences are on optimizing a different loss function to encode the targeted attack objective and also to avoid making connections with nodes of top-k neighbors of the target victim during optimization."
                },
                "questions": {
                    "value": "1. What is the value of $k$ to determine if an attack is short-distance or long distance. \n2. This is not a question, but rather a comment for the authors on proposing potentially stronger attacks. There is some interesting analogy between poisoning attacks on graphs and on images. The meta-learning approach is used to design poisoning attacks for both graphs (cited in the paper) and the images [1], the common drawback is the lack of scalability. The feature collusion attack is similar to the Shafahi et al.'s PoisonFrog paper cited in the paper. There might be some chance to rely on using the gradient alignment [2] technique to design stronger attacks in graph domains. \n\n[1] Huang et al., \"MetaPoison: Practical General-purpose Clean-label Data Poisoning\", ICML 2019.\n[2] Geiping et al., \"Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching\", ICLR 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699043618347,
            "cdate": 1699043618347,
            "tmdate": 1699636335706,
            "mdate": 1699636335706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "peZccJMyAi",
                "forum": "QHfIe4chR5",
                "replyto": "v8vDYmR46y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the valuable feedback and insightful comments. We address your concerns and questions below.\n\n**[W.1.]** *The motivation of considering nodes that are outside the top-k neighbors of the target victim is unclear. Can we use some threshold to filter out suspicious looking influence nodes for the node under examination? Will this filtering step can be evaded by some adaptive attacks so that short-distance attacks can still survive without sacrificing the effective much?* \\\n**[A.1.]** You made a good point that attackers could try to strengthen their attacks if they knew that GNN explanation tools are being used to detect short distance attacks. In addition to being less conspicuous, we think the bigger advantage of a long distance attack like MimicLDT is that it allows the attacker to choose from a much larger population of potential attack points. In particular, any node whose label is the same as the target poison label can function as an attack point, and there are many more of those at further distance away from the target node.\n\n----- \n\n**[W.2.]** *From the technical perspective, I did not find a significant (inherent) difference from the previously proposed Meta-Attack, as the major the differences are on optimizing a different loss function to encode the targeted attack objective and also to avoid making connections with nodes of top-k neighbors of the target victim during optimization.* \\\n**[A.2.]**  Our main contribution is the practical long distance poisoning attack MimicLDT, whose design heuristics are derived from our observations of MetaLDT\u2019s behavior.  Compared to existing attacks, MimicLDT can scale to much larger graphs and is easier for the attacker to launch because it is flexible with which attack points to use.  We design and evaluate MetaLDT to motivate the heuristics of MimicLDT and to quantify how much MimicLDT gives up in terms of poison effectiveness to its design heuristics.\n\nWe also want to clarify that the differences between MetaLDT and Meta-Attack extend beyond changes to the loss function, and require changing the optimization procedure. Specifically, MetaLDT (a) requires the optimization procedure to alternate between optimizing the adjacency matrix and optimizing node features; (b) uses a gradient descent based feature optimizer, rather than one based on meta-scores, thus allowing us to change the feature of all injected nodes in a single feature-optimization iteration (rather than requiring changes to a single node at a time). These changes to the optimization procedure are crucial to making MetaAttack work in our setting.  \n\n-----\n \n**[Q.1.]** *What is the value of k to determine if an attack is short-distance or long distance.* \\\n**[A.1.]** $k$ is the number of GNN layers used by the victim model. A long distance attack is one in which injected nodes are further away from the target node than the number of GNN layers. \n\n----- \n\n**[Q.2.]** *There might be some chance to rely on using the gradient alignment technique to design stronger attacks in graph domains.* \\\n**[A.2.]** Thank you very much for pointing out the connection to gradient alignment. We agree that it might be feasible to apply gradient alignment for attacks on GNNs and will investigate this further. \n\n-----"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338067369,
                "cdate": 1700338067369,
                "tmdate": 1700338067369,
                "mdate": 1700338067369,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r0AWqdaOFd",
                "forum": "QHfIe4chR5",
                "replyto": "v8vDYmR46y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the feedback from the authors. However, the concerns still remain. In particular:\nW1. The argument of many potential attack points for the attacker to choose is not a very well-motivated. Ideally, if the authors can show that a complete pipeline where the short distance nodes are infeasible to achieve attack success empirically (with concrete numbers), then the argument of using nodes outside top-k neighbors are strongly motivated. \nW2. Thanks for clarifying on the difference to the Meta-Attack details. \nQ1. Thanks for the clarification. Is there a reason to justify why such a way set the value of $k$ captures the threshold between short and long distance? Providing some explanation in the paper will be helpful."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620495237,
                "cdate": 1700620495237,
                "tmdate": 1700620495237,
                "mdate": 1700620495237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RE8sO02Ebz",
                "forum": "QHfIe4chR5",
                "replyto": "v8vDYmR46y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the quick response. For W1, I think this is on a right path to provide better motivation for the problem. However, befriending with the target is a special case of k=1 and I think it might still be fairly easy to connect to the target as $k$-hop neighbors. In the original comment, what I meant by a complete pipeline is to show that, with some detectors (e.g., graph explanation tools) with reported TPR and FPR, indeed can detect the short distance attacks. This way, it will make the paper more convincing. If all state-of-the-art short-distance attacks are detected in such a way, designing long distance attack will be very intriguing."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670929055,
                "cdate": 1700670929055,
                "tmdate": 1700670947245,
                "mdate": 1700670947245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "49ttxD6N1X",
                "forum": "QHfIe4chR5",
                "replyto": "KDe3XuPvS3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_2Mxw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing additional results on the attack success rates. As mentioned in my first review, if the graph explanation tool is to be used as a complete detector, it should report the detection performance when facing perturbed graphs and also the corresponding benign graphs. A detector is only useful if it can work well against both perturbed and benign graphs. That is what I meant by a complete pipeline, and I should have made this clearer in the last comment."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698694536,
                "cdate": 1700698694536,
                "tmdate": 1700698694536,
                "mdate": 1700698694536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EdAGsNkMnr",
            "forum": "QHfIe4chR5",
            "replyto": "QHfIe4chR5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3789/Reviewer_xnBR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3789/Reviewer_xnBR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes and studies a new type of attack on GNNs that does not modify the target node\u2019s k-hop neighborhood, which is called long-distance poisoning attack. To solve the problem, both a bilevel optimization-based approach inspired by meta-learning and an approximate heuristic-based approach are proposed. Extensive experiments are conducted on both small and large-scale graphs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Exploring the attack performance of long-range targeted poisoning attacks is valuable and important.\n2. The proposed MimicLDT is well-motivated based on the observation from MetaLDT.\n3. The paper is easy-to-follow."
                },
                "weaknesses": {
                    "value": "1. The comparison with short-distance attacks is valuable. However, the compared baselines lack more recent node injection attack methods.\n2. Some claims lack further empirical or theoretical support. For example, the authors claim that 'there are many more potential attack points beyond the target\u2019s K-hop neighborhood\u2019. It would be better if authors could offer detailed support data analysis.\n3. Some minor errors:\ndesigne-> design\nheuristicsc)Finally -> heuristics. c) Finally"
                },
                "questions": {
                    "value": "1. Whether the proposed method be generalized to unknown victim models?\n2. Is there any data analysis supporting the claim that 'there are many more potential attack points beyond the target\u2019s K-hop neighborhood\u2019?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3789/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3789/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3789/Reviewer_xnBR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3789/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699336999739,
            "cdate": 1699336999739,
            "tmdate": 1699636335641,
            "mdate": 1699636335641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CAzyElT5t0",
                "forum": "QHfIe4chR5",
                "replyto": "EdAGsNkMnr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the detailed review and valuable feedback. We will address each comment and question below.\n\n**[W.1.]** *The comparison with short-distance attacks is valuable. However, the compared baselines lack more recent node injection attack methods.* \\\n**[A.1.]** We evaluated the efficacy of AFGSM (Wang et al., DMKD\u201920)[1], a well cited recent node-injection poisoning attack. We used the author\u2019s implementation, but modified it to perform targeted label-flipping poisoning instead of misclassifying the target to be any arbitrary class. The table below shows the Cora results of direct attacks (injected nodes can be direct neighbors of the target node) as well as indirect attacks (injected nodes cannot be direct neighbors but can be k-hop neighbors). The experiments use the same setting as that in our paper (Sec 6, Table 1).  \n|  |  | GCN | GraphSAGE | GAT | GNNGuard | SoftMedianGDC | JaccardGCN | SVDGCN | ProGNN |\n| ----- | ----- | -----| ----- | ----- | ----- | ----- |----- |  ----- |  ----- |\n| Cora | Direct | 0.52 | 0.42 | 0.43 | 0.50 | 0.39 | 0.49 | 0.43 | 0.46 |\n|    | Indirect | 0.37 | 0.24 | 0.31 | 0.33 | 0.25 | 0.32 | 0.23 | 0.34 |\n\nWe can see that both the direct and indirect-attacks of AFGSM have a poison success rate ranging from 23\u201352%, which is lower than what we achieved (55-74% for MimicLDT, 53-96% for MetaLDT). We will add this comparison to a revised version of the paper.\n\nWe would also like to note that most recent injection attacks, including, TDGIA(Zou et al., KDD\u201921), GIA-HAO (Chen et al., ICLR\u201922), CANA(Tao et al., 2023), G2A2C(Ju et al., AAAI\u201923), are test-time evasion attacks, so we cannot directly compare against them.\n\n[1] Jihong Wang, Minnan Luo, Fnu Suya, Jundong Li, Zijiang Yang, and Qinghua Zheng. 2020. Scalable attack on graph data by injecting vicious nodes. Data Min. Knowl. Discov. 34, 5 (Sep 2020), 1363\u20131389. https://doi.org/10.1007/s10618-020-00696-7\n\n----- \n\n**[W.2.]** *Some claims lack further empirical or theoretical support. For example, the authors claim that 'there are many more potential attack points beyond the target\u2019s K-hop neighborhood\u2019.* \\\n**[A.2.]** Please refer to our answer to Q.2.\n\n-----\n \n**[W.3.]** *Some minor errors: designe-> design heuristicsc)Finally -> heuristics. c) Finally* \\\n**[A.3.]** Thank you for pointing out the typos. We will fix these. \n\n----- \n\n**[Q.1.]** *Whether the proposed method be generalized to unknown victim models?* \\\n**[A.1.]** Our proposed attack assumes knowledge of the victim model. We do not believe it can be generalized to unknown victim models in theory. However, empirically, the situation is a bit more complex: we ran experiments where the attacker who uses a GCN as the surrogate model either did not know what model was being used (e.g. GAT) or was unaware of what defenses were in use, and measured the efficacy of MimicLDT. We show results in the table below (the rows corresponding to \u201cvanilla GCN\u201d):\n|  | Surrogate | GCN | GraphSAGE | GAT | GNNGuard | SoftMedianGDC | JaccardGCN | SVDGCN | ProGNN |\n| ----- | ----- | -----| ----- | ----- | ----- | ----- |----- |  ----- |  ----- |\n| Cora | Aware the defense | 0.67 | 0.63 | 0.60 | 0.70 | 0.55 | 0.66 | 0.74 | 0.59 |\n|    | Vanilla GCN | 0.67 | 0.63 | 0.59 | 0.68 | 0.29 | 0.65 | 0.35 | 0.53 |\n| arXiv | Aware the defense | 0.74 | 0.73 | 0.70 | 0.64 | 0.59 | 0.63 | 0.62 | 0.58 |\n|    | Vanilla GCN | 0.74 | 0.72 | 0.67 | 0.64 | 0.42 | 0.61 | 0.49 | 0.55 |\n\nOur results show that in many cases (e.g. GAT) using a vanilla GCN based surrogate model can poison as successfully as using a surrogate based on the correct victim model. However, we also see a significant drop in poison success rate in other cases (e.g., SoftMedian, SVDGCN).\n\n----- \n\n**[Q.2.]** *Is there any data analysis supporting the claim that 'there are many more potential attack points beyond the target\u2019s K-hop neighborhood\u2019?* \\\n**[A.2.]** We evaluated this empirically for the Cora dataset. Figure 1 of [supplementary material](https://drive.google.com/file/d/1l11Ko6MdRqI0-ok3dN_BDi2pTxumK6CY/view?usp=sharing) shows the distribution of the distance of potential attack points from a target.  We can see that about 95% of the attack points are more than 3-hops away (thus qualifying as long-distance attacks). In Figures 2 and 3, we show the attack points picked by MimicLDT across various datasets and MetaLDT across various models in our experiments are between 4 and 19 hops away.\n\n-----"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700337582365,
                "cdate": 1700337582365,
                "tmdate": 1700337582365,
                "mdate": 1700337582365,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "We8Dn1RqXP",
                "forum": "QHfIe4chR5",
                "replyto": "CAzyElT5t0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_xnBR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3789/Reviewer_xnBR"
                ],
                "content": {
                    "title": {
                        "value": "Replys"
                    },
                    "comment": {
                        "value": "Thank the authors for responding and answering my questions. I will keep my current evaluation rating for this paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3789/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663056940,
                "cdate": 1700663056940,
                "tmdate": 1700663056940,
                "mdate": 1700663056940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]