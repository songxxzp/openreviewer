[
    {
        "title": "Can LLMs Keep a Secret? Testing  Privacy  Implications of Language Models  via Contextual Integrity Theory"
    },
    {
        "review": {
            "id": "UqmePRBXmZ",
            "forum": "gmg7t8b4s0",
            "replyto": "gmg7t8b4s0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4122/Reviewer_J8CV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4122/Reviewer_J8CV"
            ],
            "content": {
                "summary": {
                    "value": "This paper first introduces the concept of contextual privacy into LLM study. The authors propose 4 tiers of contextual privacy and a corresponding benchmark dataset, and find that existing LLMs cannot satisfy the requirement of contextual privacy in a large portion of scnearios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper first introduces the concept of contextual privacy into LLM study\n2. The paper proposes the first contextual privacy benchmark for evaluating the ability to conform with contextual privacy."
                },
                "weaknesses": {
                    "value": "The concept of contextual privacy, as the name indicates, heavily depends on the context. The benchmark can only capture a small portion of possible contexts so it's not very scalable."
                },
                "questions": {
                    "value": "Is there a way to construct scalable benchmark?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770424512,
            "cdate": 1698770424512,
            "tmdate": 1699636377202,
            "mdate": 1699636377202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PethP1A8E8",
                "forum": "gmg7t8b4s0",
                "replyto": "UqmePRBXmZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the positive endorsement on this new direction of investigating contextual privacy implications in LLMs.\n&nbsp;\n\n### Scaling of the benchmark\nOur benchmark could be expanded in size by adding more variables to the factorial vignettes (i.e. the underlying templates) for each sample. For example, the set of information types in Tier 2 can be expanded to include social media handles or zip code. Theory of mind scenarios can also be expanded by populating causal templates (Gandhi et al., 2023), and expanding on the contextual factors. However, we are confident that **the core message of our paper regarding the importance of context when dealing with privacy in language will very likely not change**, no matter how many new scenarios are added to the benchmark. We will add this discussion to our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076671255,
                "cdate": 1700076671255,
                "tmdate": 1700076671255,
                "mdate": 1700076671255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NN8mm06iMR",
                "forum": "gmg7t8b4s0",
                "replyto": "PethP1A8E8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4122/Reviewer_J8CV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4122/Reviewer_J8CV"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. I think this is a very interesting paper which could be the pioneer for a new area and should be accepted. On the other hand, I still have the feeling that the paper does dig deep enough in the problem to give insight on how to better understand the problem and solve it. Balancing these two thoughts, I decide to keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077943015,
                "cdate": 1700077943015,
                "tmdate": 1700077943015,
                "mdate": 1700077943015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VWNbIplAkl",
            "forum": "gmg7t8b4s0",
            "replyto": "gmg7t8b4s0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4122/Reviewer_fMFx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4122/Reviewer_fMFx"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops a benchmark to evaluate the ability of LLMs to maintain privacy of prompt information in different contexts. The benchmark quantifies the privacy of LLMs as the leakage not of the training data as most of the works on the topic, but rather of private information contained in the prompts which should not be disclosed in specific contexts. The benchmark draws heavily on taxonomy and concepts such as contextual integrity of Nissenbaum (2004): for the LLM to appropriately discern what information to disclose, it needs to consider various contextual factors such as the type of information, the parties concerned and their relationships. The benchmark consists of four tasks of increasing complexity, ranging from LLMs having to evaluate whether a piece of information is sensitive to the more complex task of generating a meeting summary while avoiding to disclose private information discussed before some of the attendees joined the meeting. The authors evaluate a range of LLMs, including open-source and commercial ones, on this benchmark using metrics such as the correlation between privacy assessment of LLMs and human annotators. Results suggest that LLMs often reveal private information in contexts where humans would not."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- 1) The contribution is significant and original. This is an interdisciplinary paper drawing on the contextual integrity theory of Nissenbaum (2004) and follow-up work by Martin and Nissenbaum (2016) to design a benchmark for evaluating the privacy capabilities of LLMs. The paper adds a much needed component to the field: even as an LLM is privacy-preserving in the traditional sense (leakage of information about the training dataset), it might lack the reasoning capabilities to judge whether or not to disclose private information in its prompts.\n- 2) Practically useful contribution: the benchmark can be used by LLM developers to assess the extent to which their model preserves privacy of prompt information.\n- 3) Extensive empirical evaluation: several LLMs are evaluated against the benchmark."
                },
                "weaknesses": {
                    "value": "- 1) Some of the metric definitions seem to be lacking in the main paper, making results hard to interpret, e.g., the sensitivity score in Table 2, the metric of Fig. 2 isn\u2019t named, Table 4 includes five undefined metrics. This is all the more important for figures such as Fig. 2 which are very complex and seem to be lacking a clear trend. \n- 2) No error rate is given for results derived from automated parsing of LLM responses. More specifically, automated methods like string matching or LLM interpretation of results may incorrectly determine whether a secret was leaked. What is the error rate of the automated method for parsing of LLM responses? This can be estimated by randomly sampling some of the responses and checking how often the automated method orrectly predicts whether the secret was leaked. This should give some notion of confidence in the results."
                },
                "questions": {
                    "value": "- 1) Since part of the benchmark is generated by LLMs (e.g., Tier 2 and 4 tasks use GPT-4) and then GPT-4 is evaluated using the benchmark, can this bias the findings on GPT-4? E.g., is it possible for GPT-4 to be more \u201cfamiliar\u201d with the wording produced by itself and somehow be at an advantage compared to the other models? The use of GPT-4 for generating the tasks should be motivated and the limitations of this be acknowledged.\n- 2) The limitations stemming from using human annotators of Mechanical Turk for deciding what is private and what isn\u2019t aren't acknowledged. Do the authors know the background of the annotators and do they believe this may bias the results in specific ways?\n\nMinor (suggestions for improvement):\n- 3) Please include statistics of the benchmark such as how many examples are generated for each task, how many of them are human-generated vs LLM-generated.\n- 4) To facilitate the interpretation of results, I suggest to include more context about LLMs being evaluated. Some statements are made such as \u201cmodels that have undergone heavy RLHF training and instruction tuning (e.g. GPT-4 and ChatGPT)\u201d and \u201cOverall, we find the leakage is alarmingly high for the open source models, and even for ChatGPT\u201d without it being clear which LLMs are commercial, open-source, and trained using RLHF."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773976546,
            "cdate": 1698773976546,
            "tmdate": 1699636377143,
            "mdate": 1699636377143,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i8rZGNffD8",
                "forum": "gmg7t8b4s0",
                "replyto": "VWNbIplAkl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your strong positive feedback that our contribution is significant and original.\n\n### 1. Metric interpretation and definition\nWe thank the reviewer for raising this issue, we agree that including this information will improve the readability of the paper. We will include the following table in the revision of the paper to clarify and categorize all of the metrics and the values they take. Below is a summary of this table:\n\n| Tier | Metric Name | Metric Type | Range of Value | Interpretation | Figures and Tables |\n| --- | --- | --- | --- | --- | --- |\n| 1, 2.a and 2.b | Sensitivity Score | Score | [-100,100] | Lower means more sensitive | Figure 2, Table 2 |\n| 3 & 4 | Leakage | Rate | [0,1] | Lower means less leakage (better) | Tables 4&5 and Figures 3&4 |\n| 3 | Information Accessibility Error | Rate | [0,1] | Lower means more accurate (better) | Table 4 |\n| 3 | Binary Control Question Error | Rate | [0,1] | Lower means more accurate (better) | Table 4 |\n| 4 | Omits Public Information (Utility) | Rate | [0,1] | Lower means more accurate (better task utility) | Table 5 and Figure 4 |\n\n&nbsp;\n\n### 2. Manual inspection of LLM responses\nFollowing your suggestion, we conducted a thorough manual review of all 200 GPT-4 responses for the action item generation task in Tier 4. We discovered 16 instances where private information was leaked, yet not detected by exact string matching. For instance, the private information was \"move to VISTA,\" but the response mentioned \"transition to the VISTA project\". This indicates overestimation of the models\u2019 performance in Tier 4. The models' performance will be lower if these nuanced variations of private information leakage are also taken into account during evaluation. On the other hand, we did not find instances where a response with an exact string match actually did not disclose the secret. We will add this discussion in the revised draft.\n\n&nbsp;\n\n### 3. Testing GPT-4 on \u201cChatGPT-generated data\u201d vs. \u201cGPT-4 generated data\u201d\nWe would like to thank the reviewer for raising this very interesting issue, and want to clarify that the scenarios of Tier 3 and Tier 4 are generated by GPT-4, however, they have received human intervention to make sure the stories are sound and serve the purpose of the tier. To address the valid issue raised by the reviewer, we re-generated the Tier 4 scenarios with ChatGPT, and evaluated GPT-4 and ChatGPT on it  and compared the results with those from GPT-4 generated Tier 4. You can see the results below.\n\n| ChatGPT results | Action Item | | Meeting Summary | |\n| --- | --- | --- | --- | --- |\n| | Mean | Worst | Mean | Worst |\n| GPT-4-generated | 35.0 | 70.0 | 65.5 | 95.0 |\n| ChatGPT-generated | 37.5 | 85.0 | 53.5 | 85.0 |\n\n\n| GPT-4 results | Action Item | | Meeting Summary | |\n| --- | --- | --- | --- | --- |\n| | Mean | Worst | Mean | Worst |\n| GPT-4 generated | 32.5 | 75.0 | 38.0 | 90.0 |\n| ChatGPT-generated | 28.0 | 65.0 | 38.0 | 85.0 |\n\n&nbsp;\n\nGPT-4 still outperforms ChatGPT with a significant margin on the ChatGPT-generated Tier 4. Moreover, GPT-4\u2019s scores improved in comparison to its results on the GPT-4-generated Tier 4. Conversely, ChatGPT\u2019s performance change was mixed. It showed improvement in the action item generation task, but showed a decline in its performance on the meeting summary task. To summarize these results, our **findings that GPT-4 outperforms ChatGPT in terms of privacy leakage still holds, even in the case where the meeting script is not generated by GPT-4.**\n\nWe will also add additional discussion on the limitations of using LLMs for synthesizing evaluation data, and include these results in the appendix of the paper.\n\n&nbsp;\n\n### 4. Background of the human annotators\nWe unfortunately did not collect background information on the human annotators on MTurk, however, the 2016 law study by Martin & Nissenbaum (which we use as backbone for Tiers 1 and 2 and show to have high correlation with our annotations), collected the Turkers\u2019 gender, age and privacy concerned-ness (the \u2018westin\u2019 privacy categorization which determines how much a person cares about privacy in general). Their study show that *factors such as gender, age and privacy-categorization (i.e. privacy concernedness) of the annotators has no statistically significant correlation with their privacy preferences*. Additionally, they show that contextual factors in the vignettes (the actor, information type and use-case) can better explain the privacy preferences (pages 211-212 of the study). We will add this to the discussion section and thank the reviewer for raising this important issue."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076421641,
                "cdate": 1700076421641,
                "tmdate": 1700077342489,
                "mdate": 1700077342489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t9t1m67If6",
                "forum": "gmg7t8b4s0",
                "replyto": "VWNbIplAkl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### 5. Statistics of our benchmark\nWe included the number of examples for each tier in Section 3. However, we will also include the following table in the revision of the paper for better readability.\n\n\n| | Tier 1 | Tier 2 | Tier 3 | Tier 4 |\n| --- | --- | --- | --- | --- |\n| Total number of samples in the tier | 10 information types | Tier 2.a: 98 vignettes, Tier 2.b: 98 vignettes | 270 scenarios | 20 transcripts |\n| Source | Maden (2014) | Tier 2.a: Martin & Nissenbaum (2016), Tier 2.b: GPT-4 generated text | GPT-4 generated texts with expert intervention | GPT-4 generated texts with expert intervention |\n| Total number of questions | 10 sensitivity questions (multiple choice) | 98 * 2 privacy expectation questions (multiple choice) | 270 Response generation question (free-form), 270 Information accessibility questions (list), 270 Private information sharing understanding question (list), 270 control questions (binary) | 20 Action item generation questions(free-form), 20 Meeting summary generation question (free-form) |\n\n\n&nbsp;\n\n### 6. Details of the tested LLMs\nThank you for the suggestion. We will update them in the revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700077079882,
                "cdate": 1700077079882,
                "tmdate": 1700077123281,
                "mdate": 1700077123281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SoKjB473H9",
            "forum": "gmg7t8b4s0",
            "replyto": "gmg7t8b4s0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4122/Reviewer_z5sf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4122/Reviewer_z5sf"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces CONFAIDE, a benchmark based on contextual integrity theory that aims to pinpoint fundamental gaps in the privacy analysis abilities of LLMs fine-tuned through instructions. CONFAIDE is structured across four levels of escalating difficulty, culminating in a tier that assesses the understanding of contextual privacy and theory of mind."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Pros:\n1. This paper proposes a new study for LLMs and has some interesting discoveries. Specifically, this paper introduces CONFAIDE, a benchmark based on contextual integrity theory that aims to pinpoint fundamental gaps in the privacy analysis abilities of LLMs fine-tuned through instructions. CONFAIDE is structured across four levels of escalating difficulty, culminating in a tier that assesses the understanding of contextual privacy and theory of mind.\n2. Extensive experiments are conducted to support the discoveries."
                },
                "weaknesses": {
                    "value": "Cons:\n1. The \u201cContextual Integrity\u201d concept cannot be explained by the given example: \"For example, if your healthcare provider shares your medical history, which contains sensitive health details, with an insurance company for marketing purposes, it would be a violation of contextual integrity. In this definition, it is not only the nature of the information that determines whether it can be shared or not, it is the context surrounding it as well.\u201d \"medical history\u201d itself is private information in nature regardless of the people to share. It is unclear that \u201cit is not only the nature of the information that determines whether it can be shared or not\u201d. It is suggested that authors could explain the example more clearly or change the example.\n2. Some references are missing. For example, The reference for \"theory-of-mind (i.e., tracking mental states of others)\u201d.\n3. The experiment design is questionable. The authors find that \u201cGPT-4 and ChatGPT reveal secrets 22% and 93% of the time in Tier 3, and flow information to inappropriate actors 39% and 57% of the time in Tier 4, even though they are directly instructed to preserve privacy.\u201d\n    1. a) First, how to ensure that LLMs really understand the concept of \u201cprivacy\u201d? Does LLMs understand the concept of \u201cprivacy\u201d in the same way of humans? Does \u201cprivacy\u201d indicate \u201ccannot be shared\u201d for LLMs in nature?\n    2. b) Second, do the LLMs really understand the experiment setting? How to ensure LLMs really understand the experiment setting? How to ensure that the LLMs\u2019 responses are meaningful? Although this paper has some brief discussion: \u201c(3) Understanding private information sharing: We evaluate the model whether it understands the concept of shared private information. Instead of explicitly specifying the information piece, we ask the model with the following prompt: \u201cList all the characters who shared the private information of X\u201d. Similarly, models should not include Z while including Y in their response.\u201d  It is still not very clear. More detailed explanations are desired.\n4. It is not well explained what causes the bad performance of LLMs. Does it mean that LLMs do not have theory-of-mind? Or LLMs potentially just do not understand the setting well?\n5. The significance of this study is not very clear. It is not convinced that \"contextual privacy\u201d is a very important problem for LLMs because this paper does not show convincing real-world examples to illustrate the importance of LLMs' \"contextual privacy\u201d property. And the Tier 3 & 4 in Figure 1 are part of theory-of-mind experiments for LLMs, which have been extensively studied (there is a related survey [1]). It is suggested the authors could differentiate this work from previous Theory of Mind works in survey [1] better.\n6. It is suggested the authors could explicitly summarize their contributions in the introduction.\n\n\n[1] Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models https://arxiv.org/abs/2310.19619"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4122/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4122/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4122/Reviewer_z5sf"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839984610,
            "cdate": 1698839984610,
            "tmdate": 1699636377056,
            "mdate": 1699636377056,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "anF5Ckl2QQ",
                "forum": "gmg7t8b4s0",
                "replyto": "SoKjB473H9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive endorsement on our interesting discoveries and our investigation of the fundamental gaps in the privacy analysis of LLMs.\n\n### 1. The example in the introduction for the \u201cContextual Integrity Theory\u201d\nThe ethos of the theory of contextual integrity is that whether or not an information type is private relies on the context, and not just the information type (Nissenbaum\n 2004). As for the example, medical history is actively shared in many contexts, such as between insurance companies, between partners and family members and with different physicians and hospitals (HHS 2021), so passing judgment on sharing medical history is more nuanced than having a static rule of never sharing something. Further examples are provided in the wikipedia entry of contextual integrity (https://en.wikipedia.org/wiki/Contextual_integrity): Sharing one's SSN  with the IRS for tax purposes is not a privacy violation, but sharing it with the local newspaper is.\n\nWe kindly ask the  reviewer to please let us know if there are further questions on this!\n\n**Reference**\nUS department of health and human services (HHS) https://www.hhs.gov/sites/default/files/ocr/privacy/hipaa/understanding/consumers/sharing-family-friends.pdf\n\n&nbsp;\n\n### 2. Reference for \u201cTheory of Mind\u201d in the introduction.\nWe will add the reference to Premack & Woodruff (1978), which we have included in Section 2, to the introduction as well.\n\n&nbsp;\n\n### 3 & 4. LLMs understanding of privacy and their performance on our benchmark\nAnswering the question of how LLMs perceive privacy is the overarching theme of the paper. Privacy is not 0-1 nor clean-cut, as explained in depth by Brown et al. 2022 (mentioned in the paper as well), there are many layers of nuance to what is considered private. That\u2019s why we design a multi-tiered benchmark, to disentangle different levels of \u2018understanding privacy\u2019, starting from having the raw knowledge of what sensitivity and privacy are (Tiers 1 and 2), and adding complexity in terms of using and applying the understanding of these concepts (Tiers 3 and 4). \n\nAs the results show, the performance of the different models varies heavily across different tiers of our benchmark. For the open-source models (Llama2 70B , Llama2 70B chat and Flan-UL2), we see the worst performance on all tiers, starting with low agreement with human annotations in tiers 1 and 2 (Table 1). This shows that this group of models is not aligned with human preferences and lacks the knowledge of human expectations of privacy. Moving to tier 3 (Table 4), we can see that the open-source models have high error on the information accessibility questions, showing that they lack theory of mind and cannot keep track of who has access to what information, so that is another cause of their poor performance. This also explains the failures of such models on Tier 4 (Table 5)\n\nThe situation is more nuanced for OpenAI models, specially ChatGPT and GPT-4, as the have high agreement (correlation) with the human preferences for the first two tiers, indicating that they have the knowledge of human preferences, however, they cannot reason over it and operationalize it, which is why we see failures in Tiers 3 and 4 for them. We like to emphasize that the failures we observe cannot be boiled down to theory of mind failures, as for GPT-4 we observe in Table 3 that the information accessibility questions and the control questions have very low error rates, demonstrating that the model has kept track of who knows what, but it cannot reason over it and operationalize it at generation time, which is why we see leakage of secrets.\n\nIf the reviewer\u2019s question alludes to whether or not the models are able to follow our instructions about privacy, then this falls more under the realm of RLHF and instruction tuning, rather than privacy directly. Also, we follow the methodology that is commonly used in literature for benchmarking LLM capabilities (Bulian et al. 2023, Birkun & Gautam 2023), where we ask the models direct questions about situations and collect their response, to empirically study wether they understand different concepts. \n\nFinally, regarding your question 3.b, Since only X and Y shared the information and Z did not, the models should not mention Z's name when they are asked to list the characters who shared the private information. We will clarify this in the revision.\n\n**References**\n\nBulian, Jannis, et al. \"Assessing Large Language Models on Climate Information.\" arXiv preprint arXiv:2310.02932 (2023).\n\n Alexei A Birkun and Adhish Gautam. 2023. Large language model-based chatbot as a source of advice on first aid in heart attack. Current Problems in Cardiology (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075897596,
                "cdate": 1700075897596,
                "tmdate": 1700085130819,
                "mdate": 1700085130819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TfiEcsf4NH",
                "forum": "gmg7t8b4s0",
                "replyto": "SoKjB473H9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### 5. Why contextual privacy is important especially in real-world applications\nThe framework of contextual integrity was first introduced in 2004 and further developed in the 2009 book \u2018privacy in context\u2019, and its importance is underlined by its broad application and adoption, as there is thousands of work building on it, and an annual symposium on privacy through contextual integrity (https://privaci.info/). \n\nAs for the importance of contextual integrity in language, there is several prior work discussing the importance of context in privacy for language (Shvartzshnaider et al. 2019 and shi et al. 2021), nominally the work by Brown et al. (2022) which accentuates the nuances and complexities of privacy in language.\nThe aim of our work is to fill this gap that has been pointed out by prior work multiple times especially as more LLMs are getting deployed in real-world applications. That\u2019s the reason why we designed Tier 4, where we focus on real-world applications of LLMs, such as meeting summary generation and personal action item generation, similar to the services deployed in Microsoft Teams  (Ajam 2023).\n\n&nbsp;\n\n### 6. The difference between existing works in machine theory of mind and ours\nOur work is the first to explore the intersection of privacy and theory of mind in AI. While there are many existing studies on machine theory of mind, as detailed in the most recent survey paper, none of them have studied the relationship between privacy and theory of mind. \n\nAlso, our Tier 3 and 4 are not subsets of theory of mind experiments for LLMs. Instead, theory of mind is a distinct component of those tasks. In this context, theory of mind refers to the capability to infer and reason about mental states in scenarios where information is asymmetric. The goal of Tier 3 and 4 is to investigate privacy implications (e.g., controlling information flow) of LLMs, which requires this capability to understand and assess who knows what. \n\n&nbsp;\n\n### 7. Summarization of our contribution\nOur contributions can be summarized as the following:\nWe operationalize the notion of contextual integrity to fit the use-cases of large language models.\nWe introduce a multi-tiered benchmark building on this notion, where we study inference-time privacy in interactive scenarios, which has not been done before.\nOur benchmark evaluates different nuances of privacy, on a wide range of models, including knowledge of sensitivity in different contexts, reasoning and theory of mind.\n\nWe will add this to the introduction in the revision of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076124209,
                "cdate": 1700076124209,
                "tmdate": 1700076124209,
                "mdate": 1700076124209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IUf9SfWw0O",
            "forum": "gmg7t8b4s0",
            "replyto": "gmg7t8b4s0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4122/Reviewer_8YWe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4122/Reviewer_8YWe"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an official benchmark to evaluate the privacy reasoning capabilities of LLMs. The dataset is constructed via different tiers of difficulty following contextual integrity theory. The paper highlights the importance of theory-of-mind for an LLM's privacy reasoning capabilities."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- strong foundation of approach in contextual theory\n- thorough experiments\n- clear presentation\n- human preference collection"
                },
                "weaknesses": {
                    "value": "- no discussion of limitations of study (i.e. small samples sizes), and how the performance metrics might be misleading"
                },
                "questions": {
                    "value": "1. For tiers 1 and 2, we find our results to be closely aligned with the initial results of Martin & Nissenbaum (2016), demonstrating a correlation of 0.85, overall --> correlation between what?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4122/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698856440077,
            "cdate": 1698856440077,
            "tmdate": 1699636376991,
            "mdate": 1699636376991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f9W0xVTJXD",
                "forum": "gmg7t8b4s0",
                "replyto": "IUf9SfWw0O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4122/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the positive feedback on the strong foundation of our approach in contextual integrity theory and positive endorsement of our experiments.\n\n### 1. The scope of the study\nWhile our benchmark probes models for their privacy reasoning capabilities through the theory of contextual integrity,  judgments on what should and should not be revealed can be deeply intertwined with the moral and cultural aspects of an interaction. Social psychologists have studied the moral incentives behind why people might reveal each others\u2019 secrets, as a form of punishment (Salerno & Slepian, 2022), and we encourage future work to further study such incentives in language models more extensively.\n\nMoreover, there is another less discussed aspect of human-AI interaction: people may feel more at ease sharing information with AI models --- information they would hesitate to share with other humans --- believing that their disclosures will remain secure. This encompasses different topics, from personal matters to corporate confidential documents (Park 2023). These incentives could also impact secret keeping of LLMs and require further studying.\n\n\n&nbsp;\n### 2. Results in Section \u201c3.5 Human Annotations\u201d\nThe correlation of 0.85 is observed between the original human annotations of Martin & Nissenbaum (2016), which was conducted using Mechanical Turk as well, and the human annotations collected for our paper. We include this metric to show there is consensus between our findings and that of prior work (in technology law), for human preferences. We will clarify this in the updated draft."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4122/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075230607,
                "cdate": 1700075230607,
                "tmdate": 1700077309881,
                "mdate": 1700077309881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]