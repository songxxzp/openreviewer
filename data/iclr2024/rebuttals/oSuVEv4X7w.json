[
    {
        "title": "Clover: Closed-Loop Verifiable Code Generation"
    },
    {
        "review": {
            "id": "dVgswPZW7G",
            "forum": "oSuVEv4X7w",
            "replyto": "oSuVEv4X7w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission113/Reviewer_L24Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission113/Reviewer_L24Y"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a technique for \"verifiable code generation\" in which rather than the generated artifact being code alone, one should think about an artifact that consists of a triplet (code, docstring, annotation).\n\nThe core idea is to use a consistency checker to verify that the triplet (c,d,a) is consistent. To do that, Clover performs a 6-way check:\n1. code \u2192 docstring\n2. docstring \u2192 code\n3. code \u2192 annotation\n4. annotation \u2192 code\n5. docstring \u2192 annotation\n6. annotation \u2192 docstring\n\nThe paper also presents the CloverBench, a set of 60 small textbook-style programs (e.g., binary_search, bubble_sort)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- An interesting approach for making generated code carry supporting evidence for its correctness.\n- A benchmark suite that can enable further experimentation on (code,docstring,annotation) triplets or subsets thereof.\n- Clear and compelling presentation."
                },
                "weaknesses": {
                    "value": "- I like the approach, but I'm afraid that the evaluation really falls short. The main weakness is that CloverBench contains classical examples for which GPT4 is clearly able to generate docstring, invariants, and code quite easily from each component of the triplet. In fact, I am guessing that it has seen different versions of the loop-invariant for each of these examples multiple times in different styles and languages.\n- The weakest link in the Clover approach itself is the natural language description in the docString. Would realistic docStrings contain the level of details required for correspondence with the annotation? What happens when they do not? The under-specification gap between a docString and the annotation may be hard to bridge?\n- Equivalence checking is always a challenge. This challenge is even more pronounced for checking \"equivalence\" between natural language descriptions.\n- The Dafny syntax gets in the way of getting a clear reading of where are the problems. Given the simple examples, I would hypothesize that the Clover would have seen 100% success with GPT4 and a mainstream language for annotations."
                },
                "questions": {
                    "value": "- Your docStrings (e.g., as shown in Listing 2) are quite elaborate and are much closer to the formal spec than to a natural language docString that you'd expect to find in real programs. The distance between realistic NL docStrings and the formal annotation can be a real challenge for Clover. Do you have any thoughts on how to address that?\n- Maybe you can consider Clover with a set of input/output examples instead of or in addition to the docString. Maybe it makes sense to consider (code,annotation, docString+examples) either as a triplet in which examples make the docString more robust, or as a quadruple in which examples provide yet another aspect, making it a four-leaf clover.\n- When all three artifacts (code,docstring, annotation) are generated by the same source, they may be consistent but incorrect. Did you consider using a combination of different models for generating the triplet?\n- Why don't you repeat the experiments with an annotation language embedded into a mainstream language as a library / helper functions? Something that would be then easy to translate back to Dafny. Alternatively, use the prompt to \"teach\" GPT4 some basic Dafny syntax and its meaning."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776976728,
            "cdate": 1698776976728,
            "tmdate": 1699635936345,
            "mdate": 1699635936345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VZRIamevTT",
                "forum": "oSuVEv4X7w",
                "replyto": "dVgswPZW7G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> I like the approach, but I'm afraid that the evaluation really falls short. The main weakness is that CloverBench contains classical examples for which GPT4 is clearly able to generate docstring, invariants, and code quite easily from each component of the triplet. In fact, I am guessing that it has seen different versions of the loop-invariant for each of these examples multiple times in different styles and languages.\n\nLLMs generalize based on examples they have seen.  In order for Clover to work, the LLMs have to be able to understand the code, annotations, and docstrings.  Since this is a new direction, it is unclear how far LLMs can generalize.  We argue that our evaluation is the right first step.  Note that Dafny is not a mainstream language, and all of our examples were hand-written.  This means that the LLM has never seen these specific examples.  The fact that it works at all answers a first set of questions about whether LLMs can reason well enough to add specifications in a new language to code that they have never seen (but which may be similar to code that they have seen). The fact that LLMs can do this is a new result and is not at all obvious.  Clearly, the next step will be to scale up to more difficult examples, but we believe it would be a mistake to jump directly to complicated code without tackling smaller milestones first.\n\nAs for the use of other languages, please refer to our general response to Question 3. \n\n> The weakest link in the Clover approach itself is the natural language description in the docString. Would realistic docStrings contain the level of details required for correspondence with the annotation? What happens when they do not? The under-specification gap between a docString and the annotation may be hard to bridge?\n\nThis is a great question that we did encounter when we conducted the experiments. The docstring in our context is not simply informal natural language.  It must be a bit more than that: specifically it should be  a complete natural language description of the functional properties of the code. The annotation is treated as a formal certificate, and the natural language description bridges the formal properties and human intention. We will make this point clearer in the revised version.\n\n\n> Equivalence checking is always a challenge. This challenge is even more pronounced for checking \"equivalence\" between natural language descriptions.\n\nFor our thoughts on docstring equivalence check, please refer to our general response in Question 2. \n\nThis is true, and especially the \u201cequivalence\u201d between natural language descriptions is imprecise, as we also mention in the paper. The result from GPT4 is skewed towards accept, which means it is easy to say a pair of docstrings are equivalent if there is no contradiction. Therefore, the rejection rate for incorrect examples on the edges that rely on docstring equivalence checks is not ideal. However, the equivalence check for annotations is strict, and the equivalence check for code is okay, so the aggregated rejection rate still looks good (i.e., 100% in our experiments). \n\n\n> The Dafny syntax gets in the way of getting a clear reading of where are the problems. Given the simple examples, I would hypothesize that the Clover would have seen 100% success with GPT4 and a mainstream language for annotations.\n\nPlease refer to our general response, Question 5. We do observe Dafny syntax errors as a significant cause of errors, but we also observe cases where the model failed to understand the program correctly. Moreover, since we allow feedback from the Dafny compiler to GPT4, more than half of the time, GPT4 is able to fix the syntax mistakes if it has the correct understanding of the program functionality.  So Dafny syntax is part of the problem, but not all of it.  We had to balance the ability of GPT4 to reason about Dafny with the need for a strong verification back-end.  Dafny was a good compromise.  If we had used a mainstream language, there are several other failure modes we would likely have seen.  First of all, the annotation language may not be strong enough (e.g., Dafny supports quantifiers, but C-style assertions do not).  Second, the verification tools may have failed.  Dafny is SoTA for deductive verification.  Tools for other languages fall short in this respect."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118680199,
                "cdate": 1700118680199,
                "tmdate": 1700118680199,
                "mdate": 1700118680199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EnrdNm1mjC",
                "forum": "oSuVEv4X7w",
                "replyto": "dVgswPZW7G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Your docStrings (e.g., as shown in Listing 2) are quite elaborate and are much closer to the formal spec than to a natural language docString that you'd expect to find in real programs. The distance between realistic NL docStrings and the formal annotation can be a real challenge for Clover. Do you have any thoughts on how to address that?\n\nIn our hypothetical setting of an ideal codebase, the docstring is interpreted as a natural language description of the formal specification (annotation). We do expect that the docstring must contain all the information needed for a human developer to recreate the correct code. We\u2019ll elaborate on this in a revised version.   Note that one interesting direction for future work would be to explore whether LLMs can bridge this gap between informal docstrings and the more formal ones we require.  \n\n> Maybe you can consider Clover with a set of input/output examples instead of or in addition to the docString. Maybe it makes sense to consider (code,annotation, docString+examples) either as a triplet in which examples make the docString more robust, or as a quadruple in which examples provide yet another aspect, making it a four-leaf clover.\n\nThanks for the great suggestion! In Appendix A.2.2 (page 15), we have briefly discussed the option of adding unit tests in the loop (four-leaf clover!). In the case of the user entering the docstring, it makes a lot of sense to add input/output examples, which can hugely improve the accuracy of the docstring, which is also common in other datasets for code (MBPP, HumanEval). In the case of machine-generated input/output examples, it may not be easier than generating the annotations. We will seriously consider this as a next step.\n\n> When all three artifacts (code,docstring, annotation) are generated by the same source, they may be consistent but incorrect. Did you consider using a combination of different models for generating the triplet?\n\nThis is a good point. Using a portfolio of models is a great suggestion for reducing consistent model hallucinations We have also experimented with Claude. Our hypothesis is that an incorrect triplet is hard to get consistent among each artifact accidentally, from the intuition that the possibility of being incorrect is many. But theoretically, such a case could happen, which we also discussed in Appendix A.2.1 (page 15). The reduction from correctness to consistency is not mathematically complete, which is an inherent limitation of the Clover framework. A combination of different models may help with the problem, though it further complicates the process. Also, as we discussed in Appendix A.2.2, there could be many Clover variants using the principle of consistency test. Ensembling different models could be an interesting future research.\n\n> Why don't you repeat the experiments with an annotation language embedded into a mainstream language as a library / helper functions? Something that would be then easy to translate back to Dafny. Alternatively, use the prompt to \"teach\" GPT4 some basic Dafny syntax and its meaning.\n\nPlease refer to our general response to Question 5. We have also tried to \u201cteach\u201d GPT4 some basic Dafny syntax as we showed in the system prompt but they tend to not work effectively. However, given our evidence in Question 5, we think that Dafny syntax is not a significant issue given our current use. \n\nIt is a great suggestion to translate a mainstream annotation language to Dafny. But there does not exist an automatic translator. If we were to use LLM for translation, the outcome would not be better than our current use. We will provide experimental evidence to show that translation is not that easy. We tried to translate the 18 typical loop invariants in table 2 (Pei 2023) and only 8 succeded out of 18 in the first try and 13 succeeded after three tries with Dafny compiler feedback. This shows that automatic translation using LLM to Dafny syntax is not easier than directly generating invariants in Dafny syntax.\n\n\nKexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2023. Can large language models reason about program invariants? In Proceedings of the 40th International Conference on Machine Learning (ICML'23), Vol. 202. JMLR.org, Article 1144, 27496\u201327520."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118807827,
                "cdate": 1700118807827,
                "tmdate": 1700715859197,
                "mdate": 1700715859197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7xncn2xFoX",
            "forum": "oSuVEv4X7w",
            "replyto": "oSuVEv4X7w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission113/Reviewer_29hf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission113/Reviewer_29hf"
            ],
            "content": {
                "summary": {
                    "value": "Summary:\n-------------\nThis paper introduces the Clover paradigm, which aims to ensure the correctness of code generated by large language models (LLMs), through a closed-loop verification process. As the process of formal verification of code is tedious when done manually by human experts, the authors propose an LLM-based automated approach for the same. The paper provides an interesting insight that AI-generated codes (i.e. C) should inherently also include formal specifications (annotation i.e. A which includes precondition, postcondition, loop-invariants) and natural language descriptions (docstring i.e. D). As such, the six ordered pairs of these three components (code, annotation, docstring) viz. A-D, D-A, C-D, D-C, A-C, and C-A can be mutually verified for consistency through either a LLM or a SAT/SMT solver. Therefore, a code can be considered formally verified when all these six consistency checks are passed. Essentially, the Clover checks act as a conservative filter that approves of codes that are formally verified, well-documented, and internally consistent.\n\nThe paper also puts forward a miniature CloverBench dataset of 60 codes written in the Dafny programming language. The codes are small-sized and contain at most one method or helper function (termed as those 'found in standard CS textbooks'). Each code has a ground-truth version (where the code, annotation, and docstring are manually verified to be correct and consistent), three incorrect versions, and five unit tests.  \n\nAn input program is assumed to contain annotation (A), documentation (D), and code (C). As part of the Clover paradigm, the paper proposes how each of the six ordered pairs among A, D, and C can be mutually verified. For instance, the ordered pair A-D can be verified using an LLM. Given an annotation (A), an LLM is asked to generate a docstring (D_gen). Subsequently, it is checked whether D and D_gen are equivalent. Such LLM-based generation and equivalence checking is carried out for A-D, D-A, C-D, D-C, and A-C ordered pairs. However, for the C-A ordered pair, Dafny's deductive verifier is used to check whether the code satisfies the generated annotation. The paper also elaborates on how equivalence can be tested between each input component and its corresponding generated version viz. (C, C_gen), (A, A_gen), and (D, D_gen). (C, C_gen) equivalence is tested using the unit tests included as part of the Clover dataset. (A, A_gen) equivalence is tested by writing the equivalence of the two annotations as a formal lemma and asking Dafny's formal tool to prove the lemma. (D, D_gen) equivalence is tested by asking an LLM whether the two docstrings are semantically equivalent. The LLM used for all these checks is GPT-4. \n\nFor evaluation, the authors first check whether GPT-4 can generate code with annotations and docstrings included. Using a zero-shot approach, given the annotation the LLM can produce the correct code for 41/60 in the dataset. This accuracy iteratively increases when the LLM is provided with the Dafny compiler and verifier's output as feedback and the LLM is repeatedly asked to correct itself. A similar pattern is observed when the LLM is asked to generate an annotation given a Dafny code. Secondly, the author evaluates their Clover paradigm using the six consistency checks. The Clover paradigm accepts 45 of the 60 correct codes in the CloverBench dataset and rejects all the incorrect versions. The major reason for non-acceptance is incorrect syntax in the LLM-generated Dafny codes. The paper also puts forward a fairly elaborate ablation study on the different aspects of the Clover paradigm. \n\nThe paper also discusses the limitations of the current implementation of the Clover paradigm, including the need for more research to improve the formal verification tools and to expand the coverage to more mainstream languages. However, the authors argue that the Clover paradigm has the potential to significantly reduce the time and expertise required for formal verification and to improve the trustworthiness of code generated by LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is very nicely written and proposes a well-principled approach to verify AI-generated codes.\n\n- Particularly, I found the insight interesting where the authors propose that AI-generated codes should inherently also include formal specifications (annotation viz. precondition, postcondition, loop-invariants) and natural language descriptions (docstring). This is a principled strategy that essentially asks an LLM to provide some form of certification justifying what it generated.\n\n- The proposed set of six checks in the Clover paradigm has the potential towards a good foundation for verifying AI-generated codes, especially because LLM-generated codes are typically erroneous when the complexity of codes increases."
                },
                "weaknesses": {
                    "value": "- Although the paper proposes a good concept for program verification, it's still in a nascent stage and has a long way to go for real-world deployment. I understand that this is a first step towards a bigger objective when the author mentions that \"we predict that Clover, coupled with steadily improving generative AI and formal tools, will enable a future in which fully automatic, scalable generation of formally verified code is feasible. This paper charts the first steps towards realizing this vision\". However, a few aspects of the paper concern me. For example, the Clover paradigm is tested on a simple and very small dataset of 60 programs that contain at most one helper function (or even none). This is a significantly trivial benchmark to test on, even for a proof of concept. In my opinion, for the readers to be convinced of the applicability of the Clover paradigm, it should be tested on relatively larger benchmarks. \n\n- Even for a proof of concept, a paper should argue how it can suffice for challenging cases. When a paper aims to propose a promising program verification approach, it is not sufficient to say that \"This test is, of course, imprecise (complete but not sound), but our evaluation suggests that it suffices for the level of complexity in CloverBench. More advanced equivalence checking techniques might be required for more complex examples.\" or \"This method is conservative in the sense that it succeeds only if the two annotations are indeed equivalent, but it may fail on equivalent annotations due to limitations of the verification tool being used.\". It would be nice to get an insight into how the authors aim to check (or at least approximately) code equivalence for difficult benchmarks. \n\n- The paper provides no insight into the performance of Clover on other mainstream programming languages like C, C++, Java, and Python. Although I agree with the fact that AI-generated codes should include formal specifications and natural language descriptions which is inherent in a language like Dafny that supports formal specification, there should be at least some insight about how this should be tackled in or extended to the mainstream languages of today. Otherwise, this would essentially mean that to get the benefit of a closed-loop verifier for AI-generated codes, the whole community should shift towards a verifier-friendly language like Dafny, which is not possible. \n\n- The related work of the paper is not elaborate enough and fails to describe in which line of work the present paper is closest to. Is the paper the first to propose a verifier for (AI/LLM)-based code generation from docstrings and annotations? If not, what were the limitations in the other papers that motivated the authors to come up with this approach? The related work will also look better if structured by organizing it into comprehensive categories that encompass various code verification methods in use.\n\n- The paper does not compare their proposed approach against other similar works in program verification. The input requirements for Clover are quite strong (in the sense that it requires annotation and documentation along with the code). So, an AI-generated program that has all these three can be verified by Clover and any other similar SoTA program verification approaches that do not have such strong requirements. As such, it makes sense to provide a comprehensive comparison with SoTA approaches and justify empirically whether there is a significant improvement in verification filtering after imposing the strong input requirements. If not, why will the strong input requirements make sense for an automated verifier?\n\n- Although the paper is well-written, I believe the structure could be made more reader-friendly. It needed two passes for me to understand the concept. The insights given in the later part of the introduction felt too abstract on the initial read. It required me to go through the whole proposed approach and come back to the introduction again to grasp the insights given in the introduction. Algorithm-1 is helpful in this context, but I presume that was pushed to the appendix due to lack of space.\n\n- Consider a situation: A user provides a prompt to the LLM to generate a code according to some specification (e.g. sort the input array in ascending order). The LLM generates a code that sorts the input array in descending order. Along with the code, the LLM provides annotation and docstring that are fully consistent with the generated code. In this case, the Clover checks will be successful because the code-annotation-docstring is mutually consistent with the six checks. But, the code does not match the user specifications. Because the paper proposes a closed-loop approach of code generation + verification, it would be insightful to tackle such odd but common cases where the generated annotation, docstring, and code are mutually consistent, but do not tally with the prompt given by the user to generate the code (original functionality intended by the user).\n\n- (Minor) There are a few spelling and syntactic mistakes in the current version e.g. \"tranlations\" on Page 6, \"COLVER\" on Page 22, and space missing before full-stops in quite a few places.\n\n- Reproducibility: I did not see any URL to access the code or dataset."
                },
                "questions": {
                    "value": "Is the paper the first to propose a verifier for (AI/LLM)-based code generation from docstrings and annotations? If not, what were the limitations in the other papers that motivated the authors to come up with this approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Reviewer_29hf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815024873,
            "cdate": 1698815024873,
            "tmdate": 1699635936234,
            "mdate": 1699635936234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i5Rp0ecvOX",
                "forum": "oSuVEv4X7w",
                "replyto": "7xncn2xFoX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review. Some common questions are answered in the response overview above. Here we provide answers to the specific questions you raised.\n\n> Although the paper proposes a good concept for program verification, it's still in a nascent stage and has a long way to go for real-world deployment. I understand that this is a first step towards a bigger objective when the author mentions that \"we predict that Clover, coupled with steadily improving generative AI and formal tools, will enable a future in which fully automatic, scalable generation of formally verified code is feasible. This paper charts the first steps towards realizing this vision\". However, a few aspects of the paper concern me. For example, the Clover paradigm is tested on a simple and very small dataset of 60 programs that contain at most one helper function (or even none). This is a significantly trivial benchmark to test on, even for a proof of concept. In my opinion, for the readers to be convinced of the applicability of the Clover paradigm, it should be tested on relatively larger benchmarks.\n\nThank you for your interest in our direction of work and valid suggestions regarding how we might proceed next.  Please refer to our general response to Question1. \n\nRegarding your concerns on CloverBench dataset, it is under active development and currently has 60+ examples.\nWhat you see in the 60 examples is the necessary and unavoidable atomic components of larger benchmarks. Most frameworks start from intraprocedural analysis and then extend to interprocedural analysis. We do have plans to expand the benchmarks to be interprocedural. Please refer to our general response on Verus results. It shows that Clover can be multilingual and still has good performance.\n\nMay we ask which aspects of the dataset would you be most interested in seeing an extension of?\n\n> Even for a proof of concept, a paper should argue how it can suffice for challenging cases. When a paper aims to propose a promising program verification approach, it is not sufficient to say that \"This test is, of course, imprecise (complete but not sound), but our evaluation suggests that it suffices for the level of complexity in CloverBench. More advanced equivalence checking techniques might be required for more complex examples.\" or \"This method is conservative in the sense that it succeeds only if the two annotations are indeed equivalent, but it may fail on equivalent annotations due to limitations of the verification tool being used.\". It would be nice to get an insight into how the authors aim to check (or at least approximately) code equivalence for difficult benchmarks.\n\nThere are well-established formal techniques for checking that two pieces of code are equivalent.  For example, the literature on translation validation contains algorithms for checking the equivalence of two pieces of code before and after optimization.  We expect that such techniques can be leveraged for Clover\u2019s equivalence checking.  We will clarify this roadmap in the final version.\n\n> The paper provides no insight into the performance of Clover on other mainstream programming languages like C, C++, Java, and Python. Although I agree with the fact that AI-generated codes should include formal specifications and natural language descriptions which is inherent in a language like Dafny that supports formal specification, there should be at least some insight about how this should be tackled in or extended to the mainstream languages of today. Otherwise, this would essentially mean that to get the benefit of a closed-loop verifier for AI-generated codes, the whole community should shift towards a verifier-friendly language like Dafny, which is not possible.\n\nPlease refer to our general response to Question 3.\n\nThe ability to use Clover on other languages depends, not on modifications to Clover, but on the availability of verification tools.  Thus, the pivot we are hoping to help inspire is not for everyone to write their code in Dafny, but for the verification community to provide stronger tools for mainstream languages.  That said, there are some tools already, and a natural direction for future work is to explore instantiations of Clover using those tools."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118347482,
                "cdate": 1700118347482,
                "tmdate": 1700713335685,
                "mdate": 1700713335685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v1ApBbwMxt",
                "forum": "oSuVEv4X7w",
                "replyto": "7xncn2xFoX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The related work of the paper is not elaborate enough and fails to describe in which line of work the present paper is closest to. Is the paper the first to propose a verifier for (AI/LLM)-based code generation from docstrings and annotations? If not, what were the limitations in the other papers that motivated the authors to come up with this approach? The related work will also look better if structured by organizing it into comprehensive categories that encompass various code verification methods in use.\n\nWe will revise the section about related work as you suggest. However, Clover does propose a new paradigm that differs significantly from any previous related work. In particular, to answer your question, yes - we believe this is the first work to suggest using consistency between docstrings, annotations, and code to ensure correctness. Some previous works explore the possibility of adding assertions to the generated code (Wu 2023), which correspond to a partial  but not a complete annotation. Other works use dynamic analyzers as the ground truth (Pei 2023) and train LLMs to generate program invariants to partially specify pre/post conditions (they do not explicitly support quantifiers). Clover is the first attempt to propose a systematic mechanism of adding verification in the loop under the complete definition of deductive program verification. \n\nKexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2023. Can large language models reason about program invariants? In Proceedings of the 40th International Conference on Machine Learning (ICML'23), Vol. 202. JMLR.org, Article 1144, 27496\u201327520.\n\nWu, Haoze et al. \u201cLemur: Integrating Large Language Models in Automated Program Verification.\u201d ArXiv abs/2310.04870 (2023): n. pag.\n\n> The paper does not compare their proposed approach against other similar works in program verification. The input requirements for Clover are quite strong (in the sense that it requires annotation and documentation along with the code). So, an AI-generated program that has all these three can be verified by Clover and any other similar SoTA program verification approaches that do not have such strong requirements. As such, it makes sense to provide a comprehensive comparison with SoTA approaches and justify empirically whether there is a significant improvement in verification filtering after imposing the strong input requirements. If not, why will the strong input requirements make sense for an automated verifier?\n\nClover is the first to propose a consistency checking algorithm between docstrings, annotations, and code.  Thus, there are no good targets for direct comparison.  The suggestion is to look at other SoTA program verification approaches.  Program verification typically requires the code and the specification only.  Here, Dafny is SoTA and we are using it in the loop in Clover, so I\u2019m not sure it makes sense to compare Clover with Dafny.  What we can do is compare how the 6 checks compare with using only the Dafny check.  In addition, we have added a new dataset of Verus (Lattuada 2023), which is a rust-like language and verifier.  We could also potentially compare Dafny with other program verification systems, but since our paper is not about verification systems, this doesn\u2019t seem to make a lot of sense. \n\nWe have added End2End experiment to show that even with the strong Clover filter, we have a good acceptance rate for correct generations while maintaining the perfect rejection rate. We believe this empirically justifies there is improvement in verification filtering after imposing the strong input requirements.\n\nAs mentioned, we believe that there are no fully comparable SoTA approaches out there that can do exactly what we do. But if you have a specific ablation study in mind that makes sense, we would be willing to add it. May we ask what exactly you are thinking about? \n\nAndrea Lattuada, Travis Hance, Chanhee Cho, Matthias Brun, Isitha Subasinghe, Yi Zhou, Jon Howell, Bryan Parno, and Chris Hawblitzel. 2023. Verus: Verifying Rust Programs using Linear Ghost Types. Proc. ACM Program. Lang. 7, OOPSLA1, Article 85 (April 2023), 30 pages. https://doi.org/10.1145/3586037\n\n> Although the paper is well-written, I believe the structure could be made more reader-friendly. It needed two passes for me to understand the concept. The insights given in the later part of the introduction felt too abstract on the initial read. It required me to go through the whole proposed approach and come back to the introduction again to grasp the insights given in the introduction. Algorithm-1 is helpful in this context, but I presume that was pushed to the appendix due to lack of space.\n\nThanks for the feedback. We\u2019ll reorganize the writing and move concrete descriptions earlier in a revised version."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118476446,
                "cdate": 1700118476446,
                "tmdate": 1700714095001,
                "mdate": 1700714095001,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v6sFoCsq3Q",
                "forum": "oSuVEv4X7w",
                "replyto": "7xncn2xFoX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Consider a situation: A user provides a prompt to the LLM to generate a code according to some specification (e.g. sort the input array in ascending order). The LLM generates a code that sorts the input array in descending order. Along with the code, the LLM provides annotation and docstring that are fully consistent with the generated code. In this case, the Clover checks will be successful because the code-annotation-docstring is mutually consistent with the six checks. But, the code does not match the user specifications. Because the paper proposes a closed-loop approach of code generation + verification, it would be insightful to tackle such odd but common cases where the generated annotation, docstring, and code are mutually consistent, but do not tally with the prompt given by the user to generate the code (original functionality intended by the user).\n\nPlease refer to our general response to Question 4.\n\n> (Minor) There are a few spelling and syntactic mistakes in the current version e.g. \"tranlations\" on Page 6, \"COLVER\" on Page 22, and space missing before full-stops in quite a few places.\n\nThanks! We\u2019ll fix them in an updated version.\n\n> Reproducibility: I did not see any URL to access the code or dataset.\n\nWe have attached the code/dataset in this rebuttal.\n\n> Is the paper the first to propose a verifier for (AI/LLM)-based code generation from docstrings and annotations? If not, what were the limitations in the other papers that motivated the authors to come up with this approach?\n\nClover does propose a new paradigm that differs significantly from any previous related work. In particular, to answer your question, yes - we believe this is the first to propose a verifier for (AI/LLM)-based code generation from docstrings and annotations. Some previous works explore the possibility of adding assertions to the generated code (Wu 2023), which correspond to a partial but not a complete annotation. Other works use dynamic analyzers as the ground truth (Pei 2023) and train LLMs to generate program invariants to partially specify pre/post conditions (they do not explicitly support quantifiers). Clover is the first attempt to propose a systematic mechanism of adding verification in the loop under the complete definition of deductive program verification.\n\nKexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2023. Can large language models reason about program invariants? In Proceedings of the 40th International Conference on Machine Learning (ICML'23), Vol. 202. JMLR.org, Article 1144, 27496\u201327520.\n\nWu, Haoze et al. \u201cLemur: Integrating Large Language Models in Automated Program Verification.\u201d ArXiv abs/2310.04870 (2023): n. pag."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700118558764,
                "cdate": 1700118558764,
                "tmdate": 1700714528543,
                "mdate": 1700714528543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ACKG8Kka4D",
            "forum": "oSuVEv4X7w",
            "replyto": "oSuVEv4X7w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission113/Reviewer_kgwt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission113/Reviewer_kgwt"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Clover, a method to generate safe and formally verified code using LLMs.\nIn the first step, Clover can utilize LLMs like GPT-4 to generate (i) Code (ii) DocStrings in Natural Language (iii) Formal Specifications (e.g. loop-invariants, pre and post conditions).\nIn the second step, Clover applies pairwise consistency checks on the LLM generated Code, Docstrings, and Specifications. \nMore specifically, consistency checks are performed via reconstruction testing (for Specifications <=> Docstring and Code <=> Docstring checks) and deductive verification tools (for Code <=> Specification checks). Reconstruction testing utilizes LLMs to reconstruct one component (e.g. Code ) from another (e.g. Docstring), and further uses an LLM to verify the equivalence between the reconstructed and the original output.\n\nThe paper makes reasonable contributions overall, and its strengths and contributions include choosing and formalizing an important problem, and dataset (CloverBench) construction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper explores a method to rigourously verify the artifacts generated by Code-LLMs. This problem should be of wide interest, but there seem to be little efforts in this direction.\n- Clover exhibits high acceptance rates for correctly generated artifacts while precisely rejecting the incorrect ones.\n- The paper contributes CloverBench, which could be a useful benchmark in the future."
                },
                "weaknesses": {
                    "value": "- It seems Clover will need to depend on very strong LLMs (e.g., to generate code/docstring from just the annotations). Thus, consistency checks may often fail if the LLM is weak.\n- Dependence on a verification tool. (Can it check any given java file or just short program snippets?)\n- A very small evaluation set of just 60 examples."
                },
                "questions": {
                    "value": "- How much do code LLMs (e.g., CodeLlama, StarCoder, WizardCoder) benifit from the Clover approach? What fraction of the code generated by these LLMs is typically unacceptable in the first place?\n- It would be interesting to see experiments with open-source LLMs as well.\n- How does Clover ensure the security aspects of the generated code? Is there an actual example in the eval set (CloverBench) to demonstrate it?\n- What is the domain of dafny examples (competitive programming / ... ?)\n- In Section 4.2, how do you determine whether GPT-4 passes or fails for the task of generating Code/Annotations? Do you use Clover to determine pass/fail?\n- Can Clover be extended to repository level code verification?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Reviewer_kgwt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699113130862,
            "cdate": 1699113130862,
            "tmdate": 1699635936164,
            "mdate": 1699635936164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xZMD8fix80",
                "forum": "oSuVEv4X7w",
                "replyto": "ACKG8Kka4D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your questions and comments! We will try to answer all of them. Feel free to add more questions/comments.\n\n> It seems Clover will need to depend on very strong LLMs (e.g., to generate code/docstring from just the annotations). Thus, consistency checks may often fail if the LLM is weak.\n\nIt is true that the overall Clover success rate depends on the specific tools in each component, especially LLMs used for generation, as do most frameworks. Arguably this is not a Clover weakness. We are not aiming to train/fine-tune a generation model, although this could be a future direction. One interesting idea we have in mind is to fine-tune an LLM to achieve annotation domain knowledge transfer. In this way, with a fine-tuned model for generating pre/post conditions and loop invariants, even less mainstream annotation languages can be good candidates for the Clover framework.\nOur experimental results support the hypothesis that LLMs are strong enough for this task.  With k=10, we only fail 13% of the time.  And future efforts can improve this rate further.\n\n> Dependence on a verification tool. (Can it check any given java file or just short program snippets?)\n\nPlease refer to our general response to Question 3 for information about why we use Dafny and its capabilities  Formal verification is a rich field with a long history.  Over time, the capabilities of formal tools have improved dramatically.  Today, tools like Dafny can be used to verify very large programs, as long as they are annotated with formal specifications.\n\nFor Java code specifically, there are a limited number of formal tools.  Perhaps the most prominent is the Java Pathfinder (JPF) system, a tool developed by NASA to verify executable Java bytecode programs. It employs primarily symbolic execution techniques. The main application has been Model checking of concurrent programs, to find defects such as data races and deadlocks.  Clover requires a specific kind of verification tool, one that supports deductive verification.  JPF is not designed as a deductive verification framework though it could be used to help construct one.  We hope that our work will help motivate such efforts.\n\n> A very small evaluation set of just 60 examples.\n\nPlease refer to our general response to Question 1. \n\n> How much do code LLMs (e.g., CodeLlama, StarCoder, WizardCoder) benifit from the Clover approach? What fraction of the code generated by these LLMs is typically unacceptable in the first place?\n\nWe have experimented with CodeLlama and the results are shown in the general response. To answer the second question, please see our general comment on End2End experiment.\n\n> It would be interesting to see experiments with open-source LLMs as well.\n\nPlease see our general comment on CodeLlama.\n\n> How does Clover ensure the security aspects of the generated code? Is there an actual example in the eval set (CloverBench) to demonstrate it?\n\nWhen security properties can be formulated as functional properties of the code, Clover can check them.  Moreover, examples written in Dafny can ensure no side effects (no illegal memory access, heap safety, etc.) happen during memory read/write operations. \n\nThe ```BubbleSort``` example in CloverBench is an example of this. Not only can we ensure the functional correctness (ascending order and all elements remain the same), we are able to make sure that the memory operations are confined to the input array a with ```modifies a```. Code can only access memory locations specified after ```modifies``` clause.\n```\nmethod BubbleSort(a: array<int>)\n modifies a\n ensures forall i,j::0<= i < j < a.Length ==> a[i] <= a[j]\n ensures multiset(a[..])==multiset(old(a[..]))\n{\n var i := a.Length - 1;\n while (i > 0)\n   invariant i < 0 ==> a.Length == 0\n   invariant -1 <= i < a.Length\n   invariant forall ii,jj::i <= ii< jj <a.Length ==> a[ii] <= a[jj]\n   invariant forall k,k'::0<=k<=i<k'<a.Length==>a[k]<=a[k']\n   invariant multiset(a[..])==multiset(old(a[..]))\n {\n   var j := 0;\n   while (j < i)\n     invariant 0 < i < a.Length && 0 <= j <= i\n     invariant forall ii,jj::i<= ii <= jj <a.Length ==> a[ii] <= a[jj]\n     invariant forall k, k'::0<=k<=i<k'<a.Length==>a[k]<=a[k']\n     invariant forall k :: 0 <= k <= j ==> a[k] <= a[j]\n     invariant multiset(a[..])==multiset(old(a[..]))\n   {\n     if (a[j] > a[j + 1])\n     {\n       a[j], a[j + 1] := a[j + 1], a[j];\n     }\n     j := j + 1;\n   }\n   i := i - 1;\n }\n}\n```"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110989014,
                "cdate": 1700110989014,
                "tmdate": 1700713022146,
                "mdate": 1700713022146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2DztOQSeWc",
                "forum": "oSuVEv4X7w",
                "replyto": "ACKG8Kka4D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> What is the domain of dafny examples (competitive programming / ... ?)\n\nTextbook algorithms that you may find in undergraduate introductory algorithm/data structures classes.\n\n> In Section 4.2, how do you determine whether GPT-4 passes or fails for the task of generating Code/Annotations? Do you use Clover to determine pass/fail?\n\nWe did not use Clover in Section 4.2 as doing so would make it unclear whether problems are in the generation phase or the checking phase, and we want to focus on only the generation phase.  Thus, checking for Section 4.2 was done manually. \n\n> Can Clover be extended to repository level code verification?\n\nPlease refer to our general response to Question 1.\n\nYes. That is our vision and we will move in that direction in our next project. For larger code bases, we will need to consider:\n1. Interprocedural verification protocols.  We have an idea for this that we call CloverChain.  As mentioned in our general response, we will sketch this idea in the future work section.\n2. Support for complex language syntax and large LLM context windows\n3. Advanced equivalence checking methods (e.g., formal equivalence checking for code)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111175969,
                "cdate": 1700111175969,
                "tmdate": 1700111175969,
                "mdate": 1700111175969,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z3Rn8CpWy8",
            "forum": "oSuVEv4X7w",
            "replyto": "oSuVEv4X7w",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission113/Reviewer_M5ic"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission113/Reviewer_M5ic"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Clover to check the consistency among code, docstrings and formal annotations. Specifically, it conducts a six-way check: code to docstring, docstring to code, code to annotation, annotation to code, docstring to annotation, and annotation to code. Clovers does experiment with a benchmark of 60 small programs.\n\nClover uses a verification tool to check whether code satisfies the annotation, which I agree. But I have a concern. The programs used in the paper are simple ones. As far as I know, verification tools suffer from scalability issues. I would like to know the insights of the authors regarding this.\n\nTo check whether an annotation is consistent with code, Clover uses LLM to generate new code for a given annotation and checks whether the generated code and the original are equivalent. Firstly, in many cases, there would be multiple annotations for a given piece of code. For example, for a sorting algorithms, an annotation can specify all the elements in a given set are in ascending order and another annotation specifies all the elements in the given set remain the same. How Clover deal with such cases? Secondly, the generated code from LLM may not be consistent with the given annotation. So if the generated code and the original code are inconsistent, it does not mean the annotation does not align with the original code. This second concern also holds for other checking. Basically, Clover assumes that LLM must have a very high accuracy in translating one artifact to another. I admit that LLMs are powerful, but LLMs do have limitations like hallucinations. I would suggest to discuss the threats to validity. Moreover, the paper claims that Clover has the potential to improve the trustworthiness of code generated by LLMs. It is kind of the chicken-egg thing. \n\nWhen checking whether two pieces of code are consistent, Clover leverages a set of input-output pairs which makes a lot of sense. But such pairs are not always availble. What would be the solution without the pairs? Sometimes, we do have such data availble, but not complete. It is possible that the two pieces of code pass the test, but actually they are inconsistent. I would suggest the paper includes some disscussion on this. \n\nThe paper employs LLM to check the semantic equivalence between two natural language sentenses. Natural languages are ambigious. Even human may not fully understand the actual semantic in a natural language sentence. I have concerns on the acccuracy of the result from LLM. Also, the paper does not have details regarding how to use LLMs to do this task.\n\nClover is used on a very small dataset of 60 textbook-style programs. I would suggest the authors to test on relatively larger benchmarks to better demonstrate the potencial of the proposed work."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ An important problem\n+ Easy follow paper"
                },
                "weaknesses": {
                    "value": "- Unsound design\n- Small-scale experiment\n- Lack important details"
                },
                "questions": {
                    "value": "How LLMs are used to check whether two docstrings are semantically equivalent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission113/Reviewer_M5ic"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission113/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699646262234,
            "cdate": 1699646262234,
            "tmdate": 1699646262234,
            "mdate": 1699646262234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8mThodG6JL",
                "forum": "oSuVEv4X7w",
                "replyto": "Z3Rn8CpWy8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your questions and comments! We will try to answer all of them. Feel free to add more questions/comments.\n\n> The programs used in the paper are simple ones. As far as I know, verification tools suffer from scalability issues. I would like to know the insights of the authors regarding this.\n\nAnswered in General Response 1.\n\n> Firstly, in many cases, there would be multiple annotations for a given piece of code. For example, for a sorting algorithms, an annotation can specify all the elements in a given set are in ascending order and another annotation specifies all the elements in the given set remain the same. How Clover deal with such cases?\n\nWhen we say \u201cannotation,\u201d we mean the union of all of the individual logical specification formulas, including preconditions, postconditions, and loop invariants. It is quite possible for an \u201cannotation\u201d to include more than one postcondition.  We will clarify this in the final version.\nNote that the key requirement is that the annotation is sufficiently precise to uniquely determine the output for any given input.  It does not matter how many pre-/post-conditions are required to achieve this. We have specified in the paper that the Clover input constraints restrict the input docstring, code, and annotation triplets to define one and only one equivalent class of output (formally defined in Section 3.2). In other words, it expects that each of the three components provides sufficient detail to unambiguously determine a unique result of running the code on any given input. \n\n> Secondly, the generated code from LLM may not be consistent with the given annotation. So if the generated code and the original code are inconsistent, it does not mean the annotation does not align with the original code. This second concern also holds for other checking. \n\nWe are assuming you are referring to the reconstruction step when verifying the consistency between code and annotation.  And you are asking whether it could happen that the code and annotation are in fact consistent, but we get a false negative because the LLM generates code from the annotation that is not consistent with it.\n\nYes, this can happen and could lead to false negatives (we reject a consistent triple as inconsistent).  Note that we in fact bias our entire framework towards false negatives, because we want to avoid a false positive at all costs.  Our empirical results show that in this we were quite successful, as we never had a single false positive.  However, even with all of the possibilities for false negatives, we still only get false negatives 25% of the time.  If we use k=10, this goes down to 13%.  One of the promising and perhaps surprising results of the paper is that despite the potential for such false negatives, the scheme works pretty well for a first attempt.  And there are many avenues available to improve things going forward.\n\n> Basically, Clover assumes that LLM must have a very high accuracy in translating one artifact to another. I admit that LLMs are powerful, but LLMs do have limitations like hallucinations. I would suggest to discuss the threats to validity. \n\nClover relies on LLM\u2019s being good at translation, but does not assume that they are.  This is a subtle difference.  The consistency checking is designed in such a way that if any LLM fails because of hallucinations, then the entire consistency check fails.  As mentioned above, this biases the system towards false negatives while providing strong protection against false positives.\n\nThe community will not stop using LLMs because they can hallucinate, nor do we believe there exists the need to. But when it comes to security-critical high-stakes code generation, it would be reassuring to have strong guarantees about code correctness. This is what Clover is designed to do. If hallucinations appear in the generation phase, the generated artifacts have a high probability of being rejected by Clover\u2019s six consistency checks. The presence of a formal tool (the Dafny verifier) ensures that certain kinds of inconsistencies are always rejected, protecting against those kinds of hallucinations. Hallucinations can also happen during reconstruction steps, which could affect the accuracy of the Clover consistency checking. However, we believe that the chances of a false positive are very low.  Our experiments show a high acceptance rate for ground-truth examples and a 100% reject rate for incorrect examples, providing evidence for this claim. The intuition behind this goes as follows: since we have 6 separate checks, we believe that an inconsistent triple has a low probability of succeeding on all six checks accidentally. At the same time, with several independent tries, a reasonable though not perfect success rate for reconstruction delivers a good pass rate for correct ones."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110293498,
                "cdate": 1700110293498,
                "tmdate": 1700712638448,
                "mdate": 1700712638448,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VvwH2VP1dk",
                "forum": "oSuVEv4X7w",
                "replyto": "Z3Rn8CpWy8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission113/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Moreover, the paper claims that Clover has the potential to improve the trustworthiness of code generated by LLMs. It is kind of the chicken-egg thing.\n\nThis paper is not about improving  LLM code generation capabilities. It is about automatically determining whether generated code is correct and should be trusted. While it is true that we rely on some generation capabilities for this check, we bias these towards false negatives and we also rely on formal methods for some checks.  The result is that even though LLMs themselves are not trustworthy, our consistency check is highly trustworthy.  In fact, it never accepts incorrect triples in our experiments.  It is similar to how redundant or fault-tolerant computing can produce highly reliable systems even though individual pieces are not reliable.\n\n> When checking whether two pieces of code are consistent, Clover leverages a set of input-output pairs which makes a lot of sense. But such pairs are not always availble. What would be the solution without the pairs? Sometimes, we do have such data availble, but not complete. It is possible that the two pieces of code pass the test, but actually they are inconsistent. \n\nThis is a valid point. Sometimes unit tests do not exist in the wild. And unit tests are inherently incomplete. There are other more advanced alternatives, including concolic/symbolic testing and formal equivalence checking, but these are overkill for our simple examples.\nIn future work, as we tackle more challenging examples, we plan to bring in these more advanced alternatives.  These can also be used in the case when no unit tests exist.\n\n\n> The paper employs LLM to check the semantic equivalence between two natural language sentenses. Natural languages are ambigious. Even human may not fully understand the actual semantic in a natural language sentence. I have concerns on the acccuracy of the result from LLM. Also, the paper does not have details regarding how to use LLMs to do this task.\n\nPlease refer to our general response to docstring equivalence checking in Q2.\n\n\n> Clover is used on a very small dataset of 60 textbook-style programs. I would suggest the authors to test on relatively larger benchmarks to better demonstrate the potencial of the proposed work.\n\nThis is definitely a great suggestion. Please refer to our general response to CloverBench dataset and how we plan to scale.  As we argue there, it is important to start simple in order to test the first few hypotheses and not try to jump too far too soon.  We believe this work validates our initial hypotheses and provides a good baseline for scaling up to large interprocedural analysis.\nCloverBench is a long-term project, and we will continue to improve it over time.  Note that it is designed manually, and each benchmark requires hand-written formal annotations and  multiple variants, so it is some work to scale up the benchmark set.\n\n> How LLMs are used to check whether two docstrings are semantically equivalent?\n\nAfter playing with different embedding models, we finally choose to ask GPT4 if two docstrings are semantically equivalent. The concrete prompt is in the appendix (page 20)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110316263,
                "cdate": 1700110316263,
                "tmdate": 1700110316263,
                "mdate": 1700110316263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]