[
    {
        "title": "FedEBA+: Towards Fair and Effective Federated Learning via Entropy-based Model"
    },
    {
        "review": {
            "id": "OFMxnNXKln",
            "forum": "UJeIujVxMn",
            "replyto": "UJeIujVxMn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5023/Reviewer_4hVe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5023/Reviewer_4hVe"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to formulate a concurrently fair and efficient protocol in federated learning. Even though fairness has been a widely studied topic in federated learning and many earlier works have discussed that, it remains a major challenge to design an FL algorithm that both improves the global (final) model performance and guarantees fairness. The authors tackled the challenge by modifying the aggregation rule via a method inspired by entropy, a central notion in information theory."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The source of inspiration is fundamental (information in the learning protocol).\n\nThe objective function is novel to the best of my knowledge, and many novel hyper-parameters (e.g. temperature) are intuitive and well-explained.\n\nAnalyses on convergence and fairness preservation are provided and comprehensive."
                },
                "weaknesses": {
                    "value": "Section 5.2 carries significant importance as it assesses the fairness performance of the protocol. However, the analysis is based on regression only.\n\nAlthough the authors' choice of definition of fairness (Definition 3.1, fairness via variance) is a common one, there are many more definitions available in the literature. Even in the cited original source of this definition (Li, et al 2019), they introduced a few other fairness metrics (cosine similarity and entropy). The theoretical analyses and experiments in this paper prove the FedEBA+'s efficiency for the chosen fairness definition for the regression task, but does not show the efficiency nor limitations for other definitions. It would also be beneficial if the authors could summarize some other commonly used definitions and comment on their advantages and disadvantages."
                },
                "questions": {
                    "value": "As mentioned in the weakness section, there are some other fairness metrics, in the cited reference and some other ones. Would you provide any justifications to use this metric instead of others?\n\nDid you try FedEBA+ on other tasks beyond regression?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5023/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698693594324,
            "cdate": 1698693594324,
            "tmdate": 1699636491032,
            "mdate": 1699636491032,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EhqQWSDno9",
                "forum": "UJeIujVxMn",
                "replyto": "OFMxnNXKln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4hVe (1/3)"
                    },
                    "comment": {
                        "value": "Dear reviewer 4hVe, thank you for providing constructive feedback. We have fully revised our manuscript and have addressed all of the comments, as well as added new experiments to further strengthen our work. Please find our responses to your raised questions below:\n\n> As mentioned in the weakness section, there are some other fairness metrics, in the cited reference (cosine similarity and entropy) and some other ones. Would you provide any justifications to use this metric instead of others? It would also be beneficial if the authors could summarize some other commonly used definitions and comment on their advantages and disadvantages.\n\nThank you for your suggestion; we would like to clarify that:\n\n- Variance as fairness can be used to measure the difference between clients, designed to serve the interest of FL clients while aiming to enhance FL model performance during the process, ensuring each party has similar performance. Otherwise, this will hinder the clients\u2019 participation interest. **In this paper, we use fairness because it is straightforward to follow, easy to compute, and is the most widely used in fairness FL works~[1,2,3,4,5], leading to a fair comparison.**\n- **We provide results under other fairness metrics, i.e., cosine similarity and entropy metrics**, as used in [1]. The geometric angle corresponds to the cosine similarity metric, and the KL divergence between the normalized accuracy vector $\\mathbf{a}$ and the uniform distribution $\\mathbf{u}$ can be directly translated to the entropy of $\\mathbf{a}$. In the table below, it shows that FedEBA+ maintains its superiority over others on FashionMNIST in these fairness metrics.\n\n| Algorithms                       | Global Acc.  | Fairness Metric 1:Var. | Fairness Metric 2: Angle ($\\circ$) - cosine similarity | Fairness Metric 3: KL ($a\\|\\|u$) -entropy |\n| -------------------------------- | ------------ | ---------------------- | ------------------------------------------------------ | --------------------------------------- |\n| FedAvg                           | 86.49 \u00b1 0.09 | 62.44\u00b14.55             | 8.70\u00b11.71                                              | 0.0145\u00b10.002                            |\n| q-FFL $q=0.001$                  | 87.05\u00b1 0.25  | 66.67\u00b1 1.39            | 7.97\u00b10.06                                              | 0.0127\u00b10.001                            |\n| FedMGDA+ $\\epsilon=0.0$          | 84.64\u00b10.25   | 57.89\u00b16.21             | 8.21\u00b11.71                                              | 0.0132\u00b10.0004                           |\n| AFL $\\lambda=0.7$                | 85.14\u00b10.18   | 57.39\u00b16.13             | 7.28\u00b10.45                                              | 0.0124\u00b10.0002                           |\n| PropFair $M=0.2,thres=0.2$       | 85.51\u00b10.28   | 75.27\u00b15.38             | 8.61\u00b12.29                                              | 0.0139\u00b10.002                            |\n| TERM $T=0.1$                     | 84.31\u00b10.38   | 73.46\u00b12.06             | 9.04\u00b10.45                                              | 0.0137\u00b10.004                            |\n| FedFV $\\alpha=0.1, \\tau_{FV}=10$ | 86.98\u00b10.45   | 56.63\u00b11.85             | 8.01\u00b11.14                                              | 0.0111\u00b10.0002                           |\n| FedEBA+ $\\alpha=0.9 \\tau=0.1$    | 87.50\u00b10.19   | 43.41\u00b14.34             | 6.46\u00b10.65                                              | 0.0063\u00b10.0009                           |\n\n\n\n- **We have summarized the commonly used definitions of fairness metrics and commented on their advantages and disadvantages in the table below.** Euclidean Distance and Pearson correlation coefficient are usually used for contribution fairness, and risk difference and Jain\u2019s fairness Index are typically used for group fairness, which is a different target from performance fairness in this paper.\n\n  In particular, cosine similarity and entropy have similar roles to variance, used to measure the performance distribution among clients. The more uniform the distribution, the smaller the variance, the more similar to vector $1$, and the larger the entropy of the normalized performance. Thus, we only need one of them for performance fairness, and we use variance, which is the most widely used metric in related works."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108140951,
                "cdate": 1700108140951,
                "tmdate": 1700110254500,
                "mdate": 1700110254500,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vJMoR8pV57",
                "forum": "UJeIujVxMn",
                "replyto": "OFMxnNXKln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4hVe (2/3)"
                    },
                    "comment": {
                        "value": "| Fairness metrics | Variance [1,2,5,6]                                           | cosine similarity [2,7,8]                                    | entropy [2,7,8]                                              | Euclidean Distance [9,10,11]                                 | Pearson Correlation Coefficient [12, 13]                     | Risk Difference [14,15,16]                         | Jain\u2019s Fairness Index [17,18,19    ]                         |\n| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------- | ------------------------------------------------------------ |\n| Application      | Accuracy parity; performance fairness                        | same as variance                                             | same as variance                                             | Contribution fairness                                        | Contribution fairness                                        | Group fairness                                     | Group fairness; selection fairness; performance fairness; contribution fairness; |\n| Advantage        | Simplicity and straightforward to implement, as it focuses on a common performance metric. | similar to variance                                          | similar to variance                                          | Straightforward Interpretation; Sensitivity to Magnitude Differences | Scale Invariance; Captures Linear Relationships              | Sensitivity to Group Disparities; interpretability | Normalization Across Groups; Applicability to Various Metrics; Flexibility |\n| Disadvantage     | It only measures the relative fairness; Sensitive to outliers. | similar to variance; Does Not Capture Magnitude Differences; Sensitivity to Zero Vectors | similar to variance; Dependence on Normalization;Sensitivity to Client Number | Sensitivity to Scale; doesn\u2019t account for direction          | Sensitive to Outliers; May Not Capture Magnitude Differences; the result may be inaccurate since it assumes that the two data variables have a linear relationship | Lack of Normalization                              | Sensitivity to Metric Choice; Complexity in interpretability |\n\nWe have included the above discussion in Section 16 of our revised manuscript.\n\n[1]Zafar M B, Valera I, Gomez Rodriguez M, et al. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment[C] WWW 2017.\n\n[2]Li T, Sanjabi M, Beirami A, et al. Fair resource allocation in federated learning[J]. ICLR 2020.\n\n[3]Li T, Hu S, Beirami A, et al. Ditto: Fair and robust federated learning through personalization[C] ICML 2021.\n\n[4]Wang Z, Fan X, Qi J, et al. Federated learning with fair averaging[J]. Ijcai 2021\n\n[5]Hu Z, Shaloudegi K, Zhang G, et al. Federated learning meets multi-objective optimization[J]. TNSE 2022.\n\n[6]Shi Y, Yu H, Leung C. Towards fairness-aware federated learning[J]. TNNLS 2023.\n\n[7]Selbst A D, Boyd D, Friedler S A, et al. Fairness and abstraction in sociotechnical systems[C]//Proceedings of the conference on fairness, accountability, and transparency. 2019: 59-68.\n\n[8]Hardt M, Price E, Srebro N. Equality of opportunity in supervised learning[J]. NeurIPS 2016.\n\n[9]Aggarwal C C, Hinneburg A, Keim D A. On the surprising behavior of distance metrics in high dimensional space[C] Database Theory\u2014ICDT 2001\n\n[10]Wei S, Tong Y, Zhou Z, et al. Efficient and fair data valuation for horizontal federated learning[J]. Federated Learning: Privacy and Incentive, 2020\n\n[11]Song T, Tong Y, Wei S. Profit allocation for federated learning[C] 2019 IEEE International Conference on Big Data (Big Data)\n[12]Jia R, Dao D, Wang B, et al. Towards efficient data valuation based on the shapley value[C] AISTATS 2019.\n\n[13]Wang G, Dang C X, Zhou Z. Measure contribution of participants in federated learning[C] 2019 IEEE international conference on big data (Big Data).\n\n[14]Du W, Xu D, Wu X, et al. Fairness-aware agnostic federated learning[C] SIAM on Data Mining (SDM) 2021.\n\n[15]Dwork C, Hardt M, Pitassi T, et al. Fairness through awareness[C] ITCS 2012.\n\n[16]Hardt M, Price E, Srebro N. Equality of opportunity in supervised learning[J]. NeurIPS 2016.\n\n[17]Chiu D M. A quantitative measure of fairness and discrimination for resource allocation in shared computer systems[R]. Digital Equipment Corporation, 1984.\n\n[18]Liu H, Zhang X, Shen X, et al. A fair and efficient hybrid federated learning framework based on XGBoost for distributed power prediction[J]. arXiv:2201.02783, 2022. \n\n[19]Cho Y J, Gupta S, Joshi G, et al. Bandit-based communication-efficient client selection strategies for federated learning[C] ACSSC 2020."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108558627,
                "cdate": 1700108558627,
                "tmdate": 1700110240484,
                "mdate": 1700110240484,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s28VxJpiX1",
                "forum": "UJeIujVxMn",
                "replyto": "OFMxnNXKln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4hVe (3/3)"
                    },
                    "comment": {
                        "value": "> **Did you try FedEBA+ on other tasks beyond regression?**\n> \n\nThank you for your valuable suggestion. \n\n**In theoretical analysis, beyond regression, we have tried to analyze FedEBA+ on smooth and strongly convex loss functions.** We have added this in Section 12.3 of the revised manuscript and we would like to state the result and idea here:\n\n- We show the performance variance of FedEBA+ is smaller than that of FedAvg:\n- We start by considering two client cases with smooth and strongly convex functions:\n    - one of them is considered to be the outlier, which means the loss value is much larger than others in the parameter space and the optimal model parameter is far away from others.\n    - For ease of computing and expression, let current $x_t=0$. Consider each client\u2019s model after local update lies on its optimal parameter, w.l.o.g, assume Client 1 with optimal value $x_1^*$  and $F_1(x_1^*)=0$. Similarly, Client 2 (outlier) with optimal value $x_2^*$ that is far away from $x_1^*$ (w.l.o.g, assume $x_2^*>x_1^*$) and $F_2(x_2^*)=a>0$ (relative position, which does not affect the analysis).\n    - By consider the entropy-based aggregation probability (FedEBA+) and average aggregation (FedAvg), we obtain $\\tilde{x}=p_1x_1^* +p_2x_2^*$ and $\\overline{x}=\\frac{1}{2}(x_1^*+x_2^*)$.\n    - By some derivation, we show that comparing variance is equal to comparing $\\|\\|F_1(\\tilde{x})-F_2(\\tilde{x})\\|\\|^2$ and $\\|\\|F_1(\\overline{x})-F_2(\\overline{x})\\|\\|^2$.\n    - Due to the property of strongly convex and smooth, the outlier $F_2(x)$ monotonically decreases on $(x_1^*,x_2^*)$. Combining the condition that $F_2(x)$ is an outlier, we can conclude that the smaller $\\|\\|x-x_2\\|\\|^2$ is, the smaller variance.\n    - By the defination of entropy-based aggregation, we know $p_1 = \\frac{1}{1+e^a},p_2 = 1-p_1$, leading to $\\|\\|\\tilde{x}-x_2\\|\\|^2 <\\|\\|\\overline{x}-x_2\\|\\|^2$, and $\\|\\|F_1(\\tilde{x})-F_2(\\tilde{x})\\|\\|^2<\\|\\|F_1(\\overline{x})-F_2(\\overline{x})\\|\\|^2$. Thus in this case, we prove FedEBA+ achieves a smaller variance than FedAvg.\n- Extending the conclusion to any number of clients, we use mathematical induction to show that\uff1a\n    - Assume for $N$ client, the variance of FedBEA+ is smaller than that of FedAvg\n    - For $N+1$  client, we derive the variance as $\\mathbf{Var}^{N+1}=\\frac{N}{N+1}\\mathbf{Var}^{N}+\\frac{\\sum_{i=1}^N(F_i-F_{N+1})^2}{(N+1)^2}$.\n    - Thus, to prove $\\mathbf{Var_{EBA+}}^{N+1}<\\mathbf{Var_{Avg}}^{N+1}$, it is equal to prove $\\sum_{i=1}^N(F_i-F_{N+1})^2$. Since Client $N+1$ is an outlier, by the same analysis as we used in two clients case, we can prove for each client $i\\in [N]$, $\\|\\|F_i(\\tilde{x})-F_{N+1}(\\tilde{x})\\|\\|^2<\\|\\|F_i(\\overline{x})-F_{N+1}(\\overline{x})\\|\\|^2$.\n- Thus, by combining the above two cases, we prove in strongly convex and smooth cases, the variance of FedEBA+ is smaller than FedAvg.\n\nIn addition, the general analysis of performance fairness in FL remains an open challenge in this field. We would like to point out that the majority of existing fairness FL research employs simplified regression models to analyze fairness [1,2] or even lacks a fairness analysis altogether [3,4,5,6,7,8]. Only very limited work analyzes performance fairness (variance) [9] due to the natural property of the designed objective function $f(x)=(\\frac{1}{2}\\sum_{i=1}^mF_i(x)^2)^{\\frac{1}{2}}$.\n\nWe have applied FedEBA+ to many different models, including MLP, CNN, Resnet-18, and Mobilenet-v2, beyond regression.\n\n[1]Lin S, Han Y, Li X, et al. Personalized Federated Learning towards Communication Efficiency, Robustness and Fairness[J]. NeurIPS, 2022.\n\n[2]Chu W, Xie C, Wang B, et al. FOCUS: Fairness via Agent-Awareness for Federated Learning on Heterogeneous Data[J]. arXiv preprint arXiv:2207.10265, 2022.\n\n[3]Wang Z, Fan X, Qi J, et al. Federated learning with fair averaging[J]. arXiv preprint arXiv:2104.14937, 2021.\n\n[4]Hu Z, Shaloudegi K, Zhang G, et al. Federated learning meets multi-objective optimization[J]. IEEE Transactions on Network Science and Engineering, 2022, 9(4): 2039-2051.\n\n[5]Li T, Hu S, Beirami A, et al. Ditto: Fair and robust federated learning through personalization[C]//International Conference on Machine Learning. PMLR, 2021: 6357-6368.\n\n[6]Du W, Xu D, Wu X, et al. Fairness-aware agnostic federated learning[C]//Proceedings of the 2021 SIAM International Conference on Data Mining (SDM). Society for Industrial and Applied Mathematics, 2021: 181-189.\n\n[7]Kanaparthy S, Padala M, Damle S, et al. Fair Federated Learning for Heterogeneous Data[C]//5th Joint International Conference on Data Science & Management of Data (9th ACM IKDD CODS and 27th COMAD). 2022: 298-299.\n\n[8]Zhao Z, Joshi G. A dynamic reweighting strategy for fair federated learning[C]//ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022: 8772-8776.\n\n[9]Li T, Sanjabi M, Beirami A, et al. Fair resource allocation in federated learning[J]. ICLR 2020."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109009220,
                "cdate": 1700109009220,
                "tmdate": 1700110263588,
                "mdate": 1700110263588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I3ouqHRWS4",
                "forum": "UJeIujVxMn",
                "replyto": "OFMxnNXKln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer 4hVe,\n\nThank you very much for your valuable review, which significantly enhanced our manuscript's quality.\n\nAs the discussion period is ending soon, we would like to know if our responses have addressed your concerns. Please let us know if there are any additional clarifications or experimental evaluations that you believe would contribute to the improvement of our manuscript.\n\nYour expertise and the time you've dedicated to reviewing our manuscript is deeply appreciated.\n\nWarmest regards,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461114779,
                "cdate": 1700461114779,
                "tmdate": 1700461114779,
                "mdate": 1700461114779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yoTCdtoJrN",
                "forum": "UJeIujVxMn",
                "replyto": "I3ouqHRWS4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Reviewer_4hVe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Reviewer_4hVe"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I appreciate the authors' feedback. My concern for Q1 (choice of fairness metrics) has been well-addressed. However, the additional analysis on cases beyond regression is still fairly limited, since strong convexity must be assumed. Therefore, I will keep my original assessment."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708455372,
                "cdate": 1700708455372,
                "tmdate": 1700708455372,
                "mdate": 1700708455372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WlQYIN4GKm",
            "forum": "UJeIujVxMn",
            "replyto": "UJeIujVxMn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5023/Reviewer_ABP1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5023/Reviewer_ABP1"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces FedEBA+, an algorithm designed to increase fairness without sacrificing overall performance. It begins by formulating a constrained optimization problem to optimize the entropy of the aggregation probability while constraining the model to achieve ideal performance.\n\nThe algorithm includes convergence analysis, variance analysis, and pareto-optimality analysis. Additionally, the authors evaluate FedEBA+ on several datasets against various baseline algorithms, demonstrating its superior performance in terms of global accuracy and fairness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper addresses an important question in federated learning systems: how to achieve fairness without sacrificing accuracy.\n2. The proposed approach is supported by convergence analysis and fairness guarantees.\n3. The proposed approach's effectiveness is demonstrated by comparing it with a significant number of baselines and presenting experimental results."
                },
                "weaknesses": {
                    "value": "1. [Major] It is unclear which contributes to the improved performance, the optimization algorithm, or the entropy-based fairness regularization. \n2. [Medium] The performance of the proposed FedEBA+ doesn\u2019t seem to be significant, especially in terms of worst 5% accuracy."
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5023/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711587384,
            "cdate": 1698711587384,
            "tmdate": 1699636490939,
            "mdate": 1699636490939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6GyFPwjaUv",
                "forum": "UJeIujVxMn",
                "replyto": "WlQYIN4GKm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ABP1 (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer ABP1, thanks for your time in reviewing our paper, please find our responses to your raised questions:\n\n> [Major] It is unclear which contributes to the improved performance, the optimization algorithm, or the entropy-based fairness regularization.\n> \n\nThank you for your comment.\n\nWe would like to clarify that \n\n- FedEBA+ is an optimization algorithm comprising two parts: the proposed entropy-based aggregation (derived from Eq 2) and the proposed alignment update (derived from Eq 6). The alignment update includes both model alignment (Sec 4.2.1, Line 45 of Algorithm 1) and gradient alignment (Sec 4.2.1, Line 45 of Algorithm 1). Although these are distinct components, they collectively form one part of our optimization algorithm. Thus, the optimization algorithm, integrating a new aggregation and a new alignment update process, contributes to improved performance.\n- The ablation results of FedEBA+ demonstrate that each step, i.e., aggregation and alignment update, is beneficial to improved performance, as shown in Table 1 of the original submission. We present the ablation study here to illustrate that both aggregation and alignment updates contribute to the performance:\n    - FedEBA corresponds to the algorithm that only uses entropy-based aggregation compared with FedAvg; FedEBA+ indicates that the algorithm uses both entropy-based aggregation and alignment update.\n    - Comparing FedEBA with FedAvg, FedEBA improves accuracy by about 1.6 and fairness by about 14.3 on CIFAR-10, highlighting the proposed aggregation approach\u2019s advantages.\n    - comparing FedEBA+ and FedEBA, FedEBA+ improves accuracy by about 3.4 and fairness by about 20.8 on CIFAR-10, showing the effectiveness of the proposed alignment update approach.\n    - Similar comparisons can be observed in FashionMNIST, CIFAR-100, and Tiny-ImageNet.\n\n|  | CIFAR-10 |  |  |  | FashionMNIST |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Algorithm | Global Acc. | Var. | Worst 5% | Best 5% | Global Acc. | Var. | Worst 5% | Best 5% |\n| FedAvg | 67.79\u00b10.35 | 103.83\u00b110.46 | 45.00\u00b12.83 | 85.13\u00b10.82 | 86.49 \u00b1 0.09 | 62.44\u00b14.55 | 71.27\u00b11.14 | 95.84\u00b1 0.35 |\n| FedEBA | 69.38\u00b10.52   | 89.49\u00b110.95 | 50.40\u00b11.72  | 86.07\u00b10.90 | 86.70\u00b10.11 | 50.27\u00b15.60 | 71.13\u00b10.69 | 95.47\u00b10.27 |\n| FedEBA+ | 72.75\u00b10.25  | 68.71\u00b14.39  | 55.80\u00b11.28  | 86.93\u00b10.52 | 87.50\u00b10.19 | 43.41\u00b14.34 | 72.07\u00b11.47 | 95.91\u00b10.19 |\n|  | **CIFAR-100** |  |  |  | **Tiny-ImageNet** |  |  |  |\n| Algorithm | Global Acc. | Var. | Worst 5% | Best 5% | Global Acc. | Var. | Worst 5% | Best 5% |\n| FedAvg | 30.94\u00b10.04 | 17.24\u00b10.08 | 0.20\u00b10.00 | 65.90\u00b11.48 | 61.99\u00b10.17 | 19.62\u00b11.12 | 53.60\u00b10.06 | 71.18\u00b10.13 |\n| FedEBA | 32.38\u00b10.13 | 17.09\u00b10.06 | 0.75\u00b10.22 | 66.40\u00b10.47 | 63.34\u00b10.25 | 15.29\u00b11.36 | 54.17\u00b10.04 | 70.98\u00b10.10 |\n| FedEBA+ | 33.39\u00b10.22 | 16.92\u00b10.04 | 0.95\u00b10.15 | 68.51\u00b10.21 | 64.05\u00b10.09 | 14.91\u00b11.85 | 54.32\u00b10.09 | 71.27\u00b10.04 |\n\nThe above Table has been added as Table 11 of the revised version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106967668,
                "cdate": 1700106967668,
                "tmdate": 1700110199700,
                "mdate": 1700110199700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kiImyCzdqF",
                "forum": "UJeIujVxMn",
                "replyto": "WlQYIN4GKm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ABP1 (2/2)"
                    },
                    "comment": {
                        "value": "> [Medium] The performance of the proposed FedEBA+ doesn\u2019t seem to be significant, especially in terms of worst 5% accuracy.\n> \n\nThank you for your suggestion. \n\nWe would like to clarify that FedEBA+ aims to solve the challenge of improving fairness and the global model\u2019s accuracy simultaneously. Thus, the interpretation of experimental results should consider both.\n\n- Figure 3 visually demonstrates that when considering both fairness and accuracy, FedEBA+ outperforms others significantly, positioning itself in the lower right corner while other algorithms cluster away.\n- To better understand the simultaneous improvement in fairness and accuracy, we introduce the *coefficient of variation (CV)* as a metric to measure the relative fairness level, as used in [1,2]. This relative statistic, defined as the ratio of the standard deviation to the mean ($C_v=\\frac{std}{acc}$), captures both fairness and accuracy simultaneously. The table below presents the $C_v$ of algorithms on FashionMNIST and CIFAR-10 (Table 1 of our original submission), showing a substantial improvement with FedEBA+.\n    - From the table below, we can observe that when considering both fairness and accuracy simultaneously, FedBEA+ improves the performance to 21% in FashionMNIST, while the others achieve at most 4.8% (4.4x), and in CIFAR-10, FedEBA+ improves by 27.8%, while others achieve at most 13.3% (2.1x).\n\n|  | FashionMNIST |  | CIFAR-10 |  |\n| --- | --- | --- | --- | --- |\n| Algorithm | $C_v=\\frac{std}{acc}$ | $C_v$ improvement over FedAvg | $C_v=\\frac{std}{acc}$ | $C_v$ improvement over FedAvg |\n| FedAvg | 0.09136199 | 0% | 0.150312741 | 0% |\n| qffl | 0.112432356 | -23% | 0.144026806 | 4.2% |\n| FedMGDA+ | 0.089893051 | 1.3% | 0.146896915 | 2.4% |\n| AFL | 0.088978374 | 2.6% | 0.134878199 | 10.1% |\n| PropFair | 0.101459812 | -11.3% | 0.135671155 | 10.9% |\n| TERM | 0.101659126 | -10.1% | 0.146631123 | 2.7% |\n| FedFV | 0.086517483 | 4.8% | 0.130809249 | 13.3% |\n| FedEBA+ | 0.072539115 | 21.8% | 0.1139402 | 27.8% |\n- Even when looking at the performance boosts in accuracy and fairness alone, from the results in Table 1, we can conclude that:\n    - In FashionMNIST, the accuracy improvement of our algorithm is twice as large (**2\u00d7**) as the state-of-the-art (SOTA) algorithm FedFV, while the fairness improvement is **3\u00d7** larger than the SOTA algorithm FedFV.\n    - In CIFAR-10, the accuracy improvement of our algorithm is about **1.5\u00d7** than the SOTA algorithm FedFV, while the fairness improvement is **1.45\u00d7** larger than SOTA PropFair.\n\nRegarding the worst 5% accuracy:\n\n- While our primary focus is not solely on improving the worst clients' performance, the enhancement of fairness inherently improves the under-performing clients. FedEBA+ consistently delivers the best worst 5% performance in most cases, including CIFAR-10 and Tiny-Imagenet, as demonstrated in Tables 1 and 2 of our original submission.\n- AFL outperforms FedEBA+ in CIFAR-100 by focusing on the worst clients. However, this approach sacrifices overall performance, resulting in poor global accuracy, variance, and best 5% accuracy. A similar outcome is seen with FedMGDA+ on FashionMNIST.\n\nThe above results have been added in Table 13 of Appendix 14 of the revised manuscript.\n\n[1]Jain R K, Chiu D M W, Hawe W R. A quantitative measure of fairness and discrimination[J]. Eastern Research Laboratory, Digital Equipment Corporation, Hudson, MA, 1984, 21.\n\n[2]Pitoura T, Triantafillou P. Distribution fairness in Internet-scale networks[J]. ACM Transactions on Internet Technology (TOIT), 2009, 9(4): 1-36."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107060350,
                "cdate": 1700107060350,
                "tmdate": 1700110209116,
                "mdate": 1700110209116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vrufSAnJHO",
                "forum": "UJeIujVxMn",
                "replyto": "WlQYIN4GKm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear reviewer ABP1,\n\nWe greatly appreciate your insightful review, which plays a crucial role in enhancing the quality of our manuscript. Furthermore, we've incorporated comments from other reviewers to further enhance our paper. We summarize the revisions in the \"****Summary of Revision****\" global comment.\n\nAs the discussion period is ending soon, we want to confirm that our responses, including the clarification of the algorithm\u2019s contribution and showcasing the improvement of FedEBA+, have adequately addressed your concerns. We are happy to take any follow-up questions and look forward to discussing with you.\n\nWe sincerely appreciate your expertise and dedicated time in reviewing our manuscript.\n\nWarmest regards,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461065139,
                "cdate": 1700461065139,
                "tmdate": 1700461065139,
                "mdate": 1700461065139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bE5AZtO6Yg",
                "forum": "UJeIujVxMn",
                "replyto": "vrufSAnJHO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Reviewer_ABP1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Reviewer_ABP1"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response and new experiments"
                    },
                    "comment": {
                        "value": "Q1: Thanks for the new experiments. However, if the key idea of this paper is to use the proposed two components together to improve the results, then I think a more thorough empirical study should be conducted to understand each component. \n\nQ2: The response cleared my question. \n\nSince my concern regarding Q1 is not fully addressed, I will keep my original evaluation."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638488334,
                "cdate": 1700638488334,
                "tmdate": 1700638488334,
                "mdate": 1700638488334,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zQi1poXGGC",
                "forum": "UJeIujVxMn",
                "replyto": "WlQYIN4GKm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Dear Reviewer ABP1, \n\nThank you for your response and patience.\n\n- As motivated by your suggestion, we provide a thorough ablation study of FedEBA+ in terms of two components, i.e., the proposed entropy-based **aggregation** and the proposed **alignment** **update**. In particular:\n    1. **FedAvg** serves as the backbone.\n    2. **FedAvg + aggregation** is employed to demonstrate the individual effectiveness of our proposed aggregation step, which is effective in reducing variance and has the ability to increase accuracy compared with FedAvg.\n    3. `[New results]` **FedAvg + alignment** is utilized to showcase the individual effectiveness of our proposed alignment step, which is effective in improving accuracy and has the ability to reduce variance compared with FedAvg.\n    4. **FedAvg + aggregation + alignment** is used to show the effectiveness of our proposed algorithm, FedEBA+. FedEBA+ incorporates these two steps to effectively enhance fairness and accuracy.\n\n|  | CIFAR-10 |  |  |  | FashionMNIST |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Algorithm | Global Acc. | Var. | Worst 5% | Best 5% | Global Acc. | Var. | Worst 5% | Best 5% |\n| FedAvg | 67.79\u00b10.35 | 103.83\u00b110.46 | 45.00\u00b12.83 | 85.13\u00b10.82 | 86.49 \u00b1 0.09 | 62.44\u00b14.55 | 71.27\u00b11.14 | 95.84\u00b1 0.35 |\n| FedAvg+aggregation | 69.38\u00b10.52 | 89.49\u00b110.95 | 50.40\u00b11.72 | 86.07\u00b10.90 | 86.70\u00b10.11 | 50.27\u00b15.60 | 71.13\u00b10.69 | 95.47\u00b10.27 |\n| FedAvg+alignment  | 72.04\u00b10.51 | 75.73\u00b14.27 | 53.45\u00b11.25 | 87.33\u00b10.23 | 87.42\u00b1 0.09 | 60.08\u00b17.30 | 69.12\u00b11.23 | 97.8\u00b10.19 |\n| FedAvg+aggregation+alignment | 72.75\u00b10.25 | 68.71\u00b14.39 | 55.80\u00b11.28 | 86.93\u00b10.52 | 87.50\u00b10.19 | 43.41\u00b14.34 | 72.07\u00b11.47 | 95.91\u00b10.19 |\n- In addition, we conducted an ablation study to demonstrate the effectiveness of each update method within the proposed alignment step, including fair gradient alignment ($\\theta=0$) and global model alignment ($\\theta=90^\\circ)$, as shown in [the ablation study of $\\theta$](https://openreview.net/forum?id=UJeIujVxMn&noteId=oQd18SUjPP).\n\nThe new ablation results for FedEBA+ have been incorporated into Table 11 of our revised manuscript. The ablation study for $\\theta$, which reveals the results of the alignment step (i.e., fair gradient alignment and global model alignment), is available in Table 5 of both the original and revised versions.\n\nWe sincerely appreciate your thoughtful consideration of the comprehensive ablation studies, which were conducted with the aim of addressing your concerns. It would be highly appreciated if you could reconsider your score in light of our responses to the issues raised.\n\nThank you once again for your valuable comments, which have significantly contributed to the improvement of our paper."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653921154,
                "cdate": 1700653921154,
                "tmdate": 1700653934218,
                "mdate": 1700653934218,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7wYdvyigpu",
            "forum": "UJeIujVxMn",
            "replyto": "UJeIujVxMn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5023/Reviewer_yc23"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5023/Reviewer_yc23"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of achieving fairness in FL without compromising the performance of the global model. The proposed FedEBA+ algorithm employs an entropy-based aggregation strategy to give higher weights to underperforming clients and an alignment update method to improve fairness and global model performance. The paper provides convergence analysis and fairness proof for FedEBA+, and shows that it outperforms existing state-of-the-art methods on several datasets in terms of both fairness and global model performance. The paper also conducts ablation studies to evaluate the impact of hyperparameters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The FedEBA+ algorithm addresses the accuracy-fairness trade-off issue. The extensive results on four datasets show that FedEBA+ reduces variance without compromising the performance of clients.\n- An ablation study highlights the advantages of the proposed aggregation approach of FedEBA+. Specifically, it shows that the aggregation strategy alone can differ from FedAvg, while the addition of the aligned update ($\\alpha$ >0) further improves performance, demonstrating the effectiveness of each component of the algorithm.\n- Theoretically, the authors provide convergence analysis and fairness analysis."
                },
                "weaknesses": {
                    "value": "The algorithm may have flaws: \n- The communication overhead appears to be much higher than FedAvg in Algorithm 1.  According to Line 3  and Line 9,  clients need to communicate with the server twice at each round t. This contradicts the traditional FedAvg that only communicates once at each round. \n- In Line 9, $\\tilde{g}^t$ is the fair gradient of the selected clients obtained using one local update. However, there is no step before line 9 that collects the one local update from clients. The authors should clarify this process.  \n-  In algorithm 1, Line 10  indicates that Eq 3 will use the local model $x_{t,K}^j$ as $x$ to calculate the $p_j$. However, using local model $x_{t,K}^j$ for Eq 3 leads to different denominators $\\sum_{i=1}^N \\exp[F_i(x_{t,K}^j)/\\tau]$ for different client $j$. Then $p_j$ for different clients is not normalized by the same denominator.  I am not sure how it satisfies the constraint  $\\sum_j p_j=1$ in Eq 2. An explanation of how this approach satisfies the constraint would be helpful.\n\n\n\nWriting: \n- Although the high-level idea is communicated well, the mathematical presentation could be improved for better accessibility. The paper's current use of a complex and inconsistent notation system hampers understanding.   For example, it seems that the notations, $\\nabla \\tilde{f}(x)$, $\\tilde{\\Delta}_t$,  and $\\tilde{g}^t$ are interchangeably used to represent the FedSGD global gradient,  (e.g., the aggregation of one-step local updates) in different sections, causing confusion. \n- It would be clearer if the authors could connect Eq 8 and Eq 10 to Eq 6, given that they share a similar form and intuition. For example, Eq 6 is not referenced in Section 4.2.2, thus making the purpose of Equation 10 unclear.\n\n- The motivation of maximum entropy for fairness may need further clarification.  Though the authors discussed fairness in the introduction and mentioned the Shannon entropy in Section 4.1, the connection between entropy and fairness is not intuitively clear. More explicit reasoning or examples illustrating why higher entropy equates to greater fairness would be beneficial, especially in federated learning settings.\n\n- From equation (2),  the term \u201cideal loss\u201d $\\tilde{f}(x)$ is introduced without a clear definition or expression, leading to confusion that is not resolved until later sections (4.2.1 and 4.2.2).  It would be beneficial if the authors could provide more discussion or examples for  \u201cideal loss\u201d here. \n\n\nClarification: \n\n- The fairness of FedEBA+ is partly achieved based on the assumption that the FedSGD global gradient is an ideal fair gradient. Specifically, according to the end of Section 4.2, the authors view a FedSGD global gradient as the ideal global gradient and the ideal fair gradient. Clarity is needed on why this assumption is made and whether the FedSGD gradient truly encapsulates ideal fairness.  Also, it would be helpful to consider FedSGD as one fairness baseline in the experiments, to demonstrate the effect of another component in FedEBA+, maximum entropy aggregation, for fairness. \n\n\n- The following statement is not clear: \u201cThe specific challenge lies in finding a way to introduce entropy into the FL training process while incorporating FL\u2019s uniform performance as a constraint within the framework.\u201d  Isn\u2019t maximum entropy encouraging uniform performance? (Intuitively,  the outcomes with uniform probability give the highest entropy, i.e., uncertainty, based on the definition of entropy)  Does maximum entropy go against the goal of uniform performance? The reason why these concepts are treated distinctly in this statement requires clarification. \n\nTheory: \n- The claimed convergence rate is not appropriate because there is a non-vanishing constant error term, which means that the gradient of the algorithm will never approach zero. The convergence rate is O(1) instead. This finding is inconsistent with previous studies, such as FedAvg convergence under the non-iid setting [1], which does not have the constant error term. \n\n- The Theorem 5.3 claims that FedEBA+ achieves a smaller variance than FedAvg. However, the effect of FL data distributions is not reflected in Theorem 5.3. It would be clearer if the authors could justify how non-iid degree will affect the fairness results. Moreover, could the authors provide definitions for T() and A() in the variance analysis?\n\nExperiments: \n- The algorithms introduce a few additional hyperparameters: temperature $\\tau$, angle threshold $\\theta$, and $\\alpha$. From Figure 4, it appears that the performance of the algorithm is quite sensitive to the hyperparameters, and different datasets have different optimal hyperparameters. It indicates a need for extensive hyperparameter tuning across different datasets. This requirement potentially reduces the algorithm's practicality compared to more straightforward federated learning algorithms like FedAvg.\n- The influence of the angle threshold $\\theta$ on FedEBA+'s performance is not demonstrated. It would be beneficial to provide an ablation study on  $\\theta$.\n\n\nReference:\n\n[1] On the Convergence of FedAvg on Non-IID Data, ICLR 2020"
                },
                "questions": {
                    "value": "Please see my questions in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5023/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5023/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5023/Reviewer_yc23"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5023/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699511440443,
            "cdate": 1699511440443,
            "tmdate": 1699636490830,
            "mdate": 1699636490830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OLDGjHXqIe",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc23 (1/7)"
                    },
                    "comment": {
                        "value": "Dear reviewer yc23, we would like to thank you for your time spent reviewing our paper and for providing constructive comments. Please kindly find our responses to your raised questions below:\n\n### For algorithm:\n\n> According to Line 3 and Line 9, clients need to communicate with the server twice at each round t. This contradicts the traditional FedAvg that only communicates once at each round.\n> \n\nThank you for your suggestions. We'd like to clarify the communication issue in Algorithm 1:\n\n- **Communication time is less than twice that of FedAvg.** Apologies for the unclear description of Algorithm 1 in our original submission, which caused confusion that clients need to communicate with the server twice. We have carefully revised descriptions of Algorithm 1 in the revised manuscript. Actually, as shown in Algorithm 1 of the revised version, only the communication capacity is sufficient (Line 3) and the arccos between clients performance and $\\mathbf{1}$ is larger than the threshold (Line 5), the server needs one additional communication (Line 6) compared with FedAvg, otherwise the communication cost is not doubled.\n    - **Communication cost is limited and acceptable:** (1) In Line 3, the client updates only the loss, with negligible cost (for example, in our CIFAR-10 experiment, the communication cost of a loss is $e^{-6}$MB) compared to the model communication. (2) In Line 11, additional cost involves uploading one-step gradients, but, as per our $\\theta$ ablation study, this cost is minimal and leads to satisfactory results. For instance, as shown in Appendix 11, Table 5, in the original submission, on FashionMNIST, when the additional communication ratio (additional communication as a percentage of total communications) is 0 ($\\theta=90^\\circ$) or 2.1% ($\\theta=45^\\circ$), FedEBA+ achieves the accuracy 87% comparable with best baselines, and improves the variance to minimal 48.91, far outperforming others.\n- **FedEBA+ differs from FedAvg for a reason:** FedEBA+ aims to improve accuracy and ensure fairness simultaneously. FedEBA+ can be reverted to the FedAvg framework by setting $\\theta$ large enough, making Line 3 unnecessary. Even so, FedEBA+ outperforms FedAvg in both accuracy and fairness, as demonstrated in our $\\theta$ ablation study in Table 5 of Appendix 15, in the revised manuscript.\n\nWe've updated Algorithm 1 in our revised version.\n\n> In Line 9,\u00a0\u00a0$\\tilde{g}$ is the fair gradient of the selected clients obtained using one local update. However, there is no step before line 9 that collects the one local update from clients. The authors should clarify this process.\n> \n\nThank you for your thoughtful reminder. Sorry for the confusion caused by our abbreviated description of Algorithm 1 in the original submission. In the revised manuscript, we have amended Line 6  \n\u201cCalculate $\\tilde{g}^t$ by (9)\u201d \ninto \n\n\u201cSever collects the gradient $\\nabla F_i(x_t)$, calculates the ideal fair gradient $\\tilde{g}^{b,t}$ by (9) and sends $\\tilde{g}^{b,t}$ to selected clients.\u201d\n\n> In algorithm 1, Line 10 indicates that Eq 3 will use the local model\u00a0\u00a0$x_{t,K}^i$ as\u00a0$x$\u00a0to calculate the $p_j$\u00a0. However, using local model\u00a0$x_{t,K}^i$\u00a0for Eq 3 leads to different denominators\u00a0$\\sum_{i=1}^N\\exp({F_i(x_{t,K}^i)/\\tau})$\u00a0for different client\u00a0. Then\u00a0$p_j$\u00a0for different clients is not normalized by the same denominator. I am not sure how it satisfies the constraint\u00a0$\\sum_jp_j=1$\u00a0in Eq 2. An explanation of how this approach satisfies the constraint would be helpful.\n> \n\nThank you for your suggestions. We would like to explain how this approach satisfies the constraint:\n\n1. In Line 10 of Algorithm 1, Eq 3 corresponds to $p_i = \\frac{\\exp({F_i(x_{t,K}^i)/\\tau})}{\\sum_{j=1}^N\\exp({F_j(x_{t,K}^i)/\\tau})}$. Despite different clients having different $F_i(x_{t,K}^i)$, the denominators among clients are the same, i.e., $\\sum_{j=1}^N\\exp({F_j(x_{t,K}^i)/\\tau})$.\n2. We verify $\\sum_i p_i = 1$ by $\\sum_{i}p_i=p_1+p_2+\\dots+p_N = \\frac{\\exp({F_1(x_{t,K}^1)/\\tau})}{\\exp({F_1(x_{t,K}^1)/\\tau})+\\exp({F_2(x_{t,K}^2)/\\tau})+\\dots+\\exp({F_N(x_{t,K}^N)/\\tau})}+\\frac{\\exp({F_2(x_{t,K}^2)/\\tau})}{\\exp({F_1(x_{t,K}^1)/\\tau})+\\exp({F_2(x_{t,K}^2)/\\tau})+\\dots+\\exp({F_N(x_{t,K}^N)/\\tau})}+\\cdots+\\frac{\\exp({F_N(x_{t,K}^N)/\\tau})}{\\exp({F_1(x_{t,K}^1)/\\tau})+\\exp({F_2(x_{t,K}^2)/\\tau})+\\dots+\\exp({F_N(x_{t,K}^N)/\\tau})} $\n$= \\frac{\\exp({F_1(x_{t,K}^1)/\\tau})+\\exp({F_2(x_{t,K}^2)/\\tau})+\\dots+\\exp({F_N(x_{t,K}^N)/\\tau})}{\\exp({F_1(x_{t,K}^1)/\\tau})+\\exp({F_2(x_{t,K}^2)/\\tau})+\\dots+\\exp({F_N(x_{t,K}^N)/\\tau})}=1$.\n3. Moreover, using the same reasoning for $i\\in S_t$ instead of $i \\in [N]$, the result remains unchanged, i.e., $\\sum_{i\\in S_t}p_i = 1$."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104098925,
                "cdate": 1700104098925,
                "tmdate": 1700110012209,
                "mdate": 1700110012209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vVGyhyDR1c",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc23 (2/7)"
                    },
                    "comment": {
                        "value": "### Writing:\n\n> The paper's current use of a complex and inconsistent notation system hampers understanding. For example, it seems that the notations,\u00a0$\\nabla\\tilde{f}$,\u00a0$\\tilde{\\Delta}_t$, and\u00a0$\\tilde{g}^t$\u00a0are interchangeably used to represent the FedSGD global gradient, (e.g., the aggregation of one-step local updates) in different sections, causing confusion.\n> \n\nApologies for any confusion caused by the notations. We'd like to clarify:\n\n- Only $\\tilde{\\Delta}_t$ represents the average aggregation of one-step local updates, i.e., FedSGD. In contrast, $\\tilde{g}_t$ represents the reweighted aggregation of the current global model\u2019s gradient on each local dataset.\n- To prevent confusion with the interchangeability of $\\nabla \\tilde{f}(x_t),\\tilde{\\Delta}_t,\\tilde{g}_t$ in different sections, we will use:\n    - $\\nabla \\tilde{f}^{a}(x_t)=\\tilde{\\Delta}t^a=\\frac{1}{|S_t|}\\sum_{i\\in S_t}(x_{t,1}^i-x_{t,0}^i)$ for Section 4.2.1 to represent the ideal global gradient (the FedSGD global gradient).\n    - $\\nabla \\tilde{f}^{b}(x_t)=\\tilde{g}^{b,t}=\\sum_{i\\in S_t}\\tilde{p}_i g_i^t$ for Section 4.2.2 to represent the ideal fair gradient.\n\nThe above description has been added to our revised manuscript, indicated by a red line in Section 4.2.\n\n> It would be clearer if the authors could connect Eq 8 and Eq 10 to Eq 6, given that they share a similar form and intuition. For example, Eq 6 is not referenced in Section 4.2.2, thus making the purpose of Equation 10 unclear.\n> \n\nThank you for your suggestions. We would like to connect (1) Eq 8 to Eq 6 and (2) Eq 10 to Eq 6.\n\n- Eq 6 is  $\\frac{\\partial L\\left(x,p_i, \\lambda_0, \\lambda_1\\right)}{\\partial x} = (1-\\alpha)\\sum_{i=1}^m p_i \\nabla F_i(x) + \\alpha \\nabla \\tilde{f}(x)$.\n    \n    According to model update and local SGD, $x_{t+1}= x_t-\\eta\\frac{\\partial L\\left(x,p_i, \\lambda_0, \\lambda_1\\right)}{\\partial x}$ and $\\eta\\Delta_t=x_{t+1}-x_{t}$, the final global model update can be written as:\n    \n    $\\Delta_t=-\\frac{\\partial L\\left(x,p_i, \\lambda_0, \\lambda_1\\right)}{\\partial x}=-(1-\\alpha)\\sum_{i=1}^m p_i \\nabla F_i(x) - \\alpha \\nabla \\tilde{f}(x)$.\n    \n- Eq 8 is  $\\tilde{\\Delta}^a_t = \\frac{1}{|S_t|}\\sum_{i\\in S_t} \\tilde{\\Delta_{t}}^{a,i}= \\frac{1}{|S_t|}\\sum_{i\\in S_t}(x_{t,1}^i - x_{t,0}^i)$ . \nHere, the ideal gradient $\\nabla \\tilde{f}(x)$ of Eq 6 is expressed by $\\nabla \\tilde{f}^a(x_t)=\\tilde{\\Delta}t^a$*,* and thus the final model update is expressed as \\\n$\\Delta_t = (1-\\alpha) \\sum{i\\in S_t}p_i\\Delta_t^i +\\alpha \\tilde{\\Delta}^a_t$, \\\ncompleting the connection to Eq 6.\n- Eq 10 is $h_{t,k}^i \\gets (1-\\alpha)g_{t,k}^i + \\alpha \\tilde{g}^{b,t}$.\nHere, the ideal gradient $\\nabla \\tilde{f}(x)$ of Eq 6 is expressed by $\\nabla \\tilde{f}^b(x_t)=\\tilde{g}^{b,t}=\\sum\\nolimits_{i\\in S_t}\\tilde{p_i} g_i^t$, and thus the final model update is expressed as \\\n $\\Delta_t=-(1-\\alpha)\\sum_{i\\in S_t}p_i\\eta_L\\sum_{k=0}^{K-1}g_{t,k}^i-\\alpha K \\eta_L\\sum_{i\\in S_t}\\tilde{p}ig_i^t$ \\\nby $\\Delta_t=\\sum{i\\in S_t}p_i\\Delta_t^i,\\Delta_t^i=x_{t,K}^i-x_{t,0}^i=-\\eta_L\\sum_{k=0}^{K-1} h_{t,k}^i$, finishing the connection to Eq 6.\n\nWe have added these connections in the revised manuscript of Sec 4.2.1 and Sec 4.2.2, indicated by red text.\n\n> The motivation of maximum entropy for fairness may need further clarification. Though the authors discussed fairness in the introduction and mentioned the Shannon entropy in Section 4.1, the connection between entropy and fairness is not intuitively clear. More explicit reasoning or examples illustrating why higher entropy equates to greater fairness would be beneficial, especially in federated learning settings.\n> \n\nThank you for your suggestion, we would like to elaborate on why higher entropy equates to greater fairness would be beneficial in FL setting and give example to illustrate it:\n\n- **Elaboration:** In FL, without constraints on the aggregation probability, maximum entropy results in a uniform distribution of aggregation probabilities, treating each client equally. However, when constraints are introduced, such as ensuring the aggregated loss is close to the ideal loss, maximum entropy is achieved by not making overly confident predictions while adhering to the constraints. Consequently, the aggregation becomes proportional to each client\u2019s loss. The rationale is that a larger loss indicates poor model performance on a client, prompting the updated model to pay more attention to that client, resulting in equal performance across all clients."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104440475,
                "cdate": 1700104440475,
                "tmdate": 1700110036613,
                "mdate": 1700110036613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o0lyMoZTXY",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc23 (3/7)"
                    },
                    "comment": {
                        "value": "- **Example:** We present a toy case in our original submission of Appendix 11.1, comparing the fairness behavior between FedAvg, q-FedAvg, and FedEBA+. The example illustrates that our aggregation probability, with maximum entropy, leads to better fairness than FedAvg and q-FedAvg[1]. We would like to restate it here to illustrate the benefit that higher entropy leads to greater fairness:\n\nIn particular, we consider two clients participating in training, each with a regression model:\n$f_1(x_t) = 2(x-2)^2,\nf_2(x_t) = \\frac{1}{2}(x+4)^2.$\nCorresponding,$\\nabla f_1(x_t) = 4(x-2),\\nabla f_2(x_t) = (x+4).$ \n\nWhen the global model parameter $x_t = 0$ is sent to each client, each client will update the model by running gradient decent, here w.l.o.g, we consider one single-step gradient decent, and stepsize $\\lambda = \\frac{1}{4}$:\n$x_1^{t+1} = x_t - \\lambda \\nabla f_1(x_t) = 2,\nx_2^{t+1} = x_t - \\lambda \\nabla f_2(x_t) = -1.$ \n\nThus, for uniform aggregation: $x_{uniform}^{t+1} = \\frac{1}{2}(x_1^{t+1}+x_2^{t+1})=\\frac{1}{2}.$\n\nWhile for FedEBA+: $x_{EBA+}^{t+1}=\\frac{e^{f_1(x_1^{t+1})}}{e^{f_1(x_1^{t+1})}+e^{f_2(x_2^{t+1})}}x_1^{t+1} +\\frac{e^{f_2(x_2^{t+1})}}{e^{f_1(x_1^{t+1})}+e^{f_2(x_2^{t+1})}}x_2^{t+1} \\approx -0.1$\n\nTherefore, \n$\\mathbb{Var_{uniform}} = \\frac{1}{2}\\sum_{i=1}^2\\left(f_i(x_{uniform}^{t+1})-\\frac{1}{2}\\sum_{i=1}^2(f_i(x_{uniform}^{t+1})\\right)^2 = 2*(2.81)^2 ,\n\\mathbb{Var_{EBA+}} = \\frac{1}{2}\\sum_{i=1}^2\\left(f_i(x_{EBA+}^{t+1})-\\frac{1}{2}\\sum_{i=1}^2(f_i(x_{EBA+}^{t+1})\\right)^2 = 2*(0.6)^2 .$\n\nThus, we prove that FedEBA+ achieves a much smaller variance than uniform aggregation.\n\nFurthermore, for q-FedAvg, we consider $q=2$ which is also used in the proof of [1]:\n$\\nabla x_1^t = L(x^t - x_1^{t+1})=-2, \\nabla x_2^t = L(x^t - x_2^{t+1})=1 .$\n\nThus, we have:\n\n$\\Delta_1^t = f_1^q(x_t)\\nabla x_1^t = 8*(-2) = -16,\nh_1^t = qf_1^{q-1}(x_t)\\|\\nabla x_1^t\\|^2 + Lf_1^q(x_t) = 1\\times 1\\times 2^2+8 = 12.$\n\n$\\Delta_2^t = f_2^q(x_t)\\nabla x_2^t = 8*(1) = 8,\nh_2^t = qf_2^{q-1}(x_t)\\|\\nabla x_2^t\\|^2 + Lf_2^q(x_t) = 1\\times 1\\times 1^2+8 = 9.$\n\nFinally, we can update the global parameter as:\n$x^{t+1}_{qfedavg} = x^t - \\frac{\\sum_i \\Delta_i^t}{\\sum_i h_i^t} \\approx -0.4.$\n\nThen we can easily get:\n$\\mathbb{Var_{qfedavg}} =\\frac{1}{2}\\sum_{i=1}^2\\left(f_i(x_{qfedavg}^{t+1})-\\frac{1}{2}\\sum_{i=1}^m(f_i(x_{qfedavg}^{t+1})\\right)^2 = 2*(2.52)^2$ \n\nIn conclusion, we prove that $\\mathbb{Var_{EBA+}} \\leq \\mathbb{Var_{qfedavg} }\\leq \\mathbb{Var_{uniform}}.$\n\nIn this case, the normalized performance\u2019s entropy, after maxing the constrained entropy of aggregation probability, exhibits a relationship akin to variance (greater entropy corresponds to improved fairness).\n\n$Entropy(f(x_{EBA+}^{t+1}))=-\\sum_{i=1}^2\\frac{f_i(x_{EBA+}^{t+1})}{\\sum_{j=1}^2f_j(x_{EBA+}^{t+1})}\\log(\\frac{f_j(x_{EBA+}^{t+1})}{\\sum_{i=j}^2f_i(x_{EBA+}^{t+1})})\\approx  0.996$, where $f_1(x_{EBA+}^{t+1})=2*(2.1)^2,f_2(x_{EBA+}^{t+1})=\\frac{1}{2}*(3.9)^2.$\n\n$Entropy(f(x_{qfedavg}^{t+1}))=-\\sum_{i=1}^2\\frac{f_i(x_{qfedavg}^{t+1})}{\\sum_{j=1}^2f_j(x_{qfedavg}^{t+1})}\\log(\\frac{f_j(x_{qfedavg}^{t+1})}{\\sum_{i=j}^2f_i(x_{qfedavg}^{t+1})})\\approx 0.942$, where$f_1(x_{qfedavg}^{t+1})=2*(2.4)^2,f_2(x_{qfedavg}^{t+1})=\\frac{1}{2}*(3.6)^2.$\n\n  $Entropy(f(x_{uniform}^{t+1}))=-\\sum_{i=1}^2\\frac{f_i(x_{uniform}^{t+1})}{\\sum_{j=1}^2f_j(x_{uniform}^{t+1})}\\log(\\frac{f_j(x_{uniform}^{t+1})}{\\sum_{i=j}^2f_i(x_{uniform}^{t+1})})\\approx 0.890$, where $f_1(x_{uniform}^{t+1})=2*(1.5)^2,f_2(x_{uniform}^{t+1})=\\frac{1}{2}*(4.5)^2$.\n\nTherefore, \n $Entropy(f(x_{EBA+}^{t+1}))\\geq  Entropy(f(x_{qfedavg}^{t+1}))\\geq Entropy(f(x_{uniform}^{t+1}))$, and $\\mathbb{Var_{EBA+}} \\leq \\mathbb{Var_{qfedavg}} \\leq \\mathbb{Var_{uniform}} .$\n\nWe have mentioned this in Section 4.1 of the revised manuscript.\n\n[1]Li T, Sanjabi M, Beirami A, et al. Fair resource allocation in federated learning[J]. ICLR 2020.\n\n\n> From equation (2), the term \u201cideal loss\u201d\u00a0$\\tilde{f}(x)$\u00a0is introduced without a clear definition or expression, leading to confusion that is not resolved until later sections (4.2.1 and 4.2.2). It would be beneficial if the authors could provide more discussion or examples for \u201cideal loss\u201d here.\n> \n\nThank you for your suggestion. We would like to add an explanation of the ideal loss:\n\nThe ideal loss $\\tilde{f}(x)$ serves as the target or objective for the aggregated losses. Its specific expression depends on the goal of the FL system. In Eq (2), as a robust constraint, $\\tilde{f}(x)$ aims to ensure that the aggregated clients' losses are as close to the desired target loss as possible.\n\nIn this paper, for example, when the ideal loss acts as the ideal fair loss, the gradient of the ideal loss should be a reweighted aggregation of clients\u2019 unbiased local updates, $\\nabla \\tilde{f_{fair}}(x)=\\sum_{i\\in S_t}p_i \\nabla F_i(x)$, where $\\nabla F_i(x)$ is the unbiased local update of the model $x$.\n\nMotivated by your comments, we have added this discussion in our revised manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105863882,
                "cdate": 1700105863882,
                "tmdate": 1700110055973,
                "mdate": 1700110055973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3rrHo0BuL8",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc23 (4/7)"
                    },
                    "comment": {
                        "value": "### Clarification\n\n> The fairness of FedEBA+ is partly achieved based on the assumption that the FedSGD global gradient is an ideal fair gradient. Specifically, according to the end of Section 4.2, the authors view a FedSGD global gradient as the ideal global gradient and the ideal fair gradient.\n> \n\nWe would like to clarify that we employ the one-step gradient to obtain the ideal local gradient.\n\nFor the ideal global gradient, we use the average of the ideal local update as the ideal global gradient, which aligns with the FedSGD global gradient. However, it's crucial to note that even for the ideal global gradient, the FedSGD gradient is only utilized to correct the model update rather than being used as the updated model itself.\n\nIn contrast, for the ideal fair gradient, it differs from the FedSGD gradient. The fair gradient is the reweighted average of the ideal local update based on our fair aggregation probability $\\sum_{i\\in S_t}\\tilde{p_i }\\nabla F_i(x_t)$, instead of a direct average $\\frac{1}{|S_t|}\\sum_{i\\in S_t}\\nabla F_i(x_t)$. \n\n> Clarity is needed on why this assumption is made and whether the FedSGD gradient truly encapsulates ideal fairness.\n> \n\nClarification: **It is not FedSGD that encapsulates the ideal fairness, but the reweighted aggregated one-step local updates represent the ideal fairness.**\n\n- [Why reweight] Based on the local loss with the global model on each client ($F_i(x_t)$), the server knows the global model\u2019s performance on each client, allowing it to adjust the attention it pays to different clients: assigning higher weights to clients with larger loss (poor performance) for fairness. According to the analysis of entropy, to make the aggregated loss to be fairer, the aggregation should be reweighted based on Eq 3.\n- [Why one-step update] The choice of using one-step local updates stems from the observation that multiple local updates can introduce local bias in FL[1,2,3]. Therefore, using the one-step gradient can better reflect the true global model's update on the clients without local bias. Combining this with the fair reweight aggregation strategy ensures that the model represents a more accurate fair gradient.\n\n[1]Karimireddy S P, Kale S, Mohri M, et al. Scaffold: Stochastic controlled averaging for federated learning[C]//International conference on machine learning. PMLR, 2020: 5132-5143.\n\n[2]Wang J, Liu Q, Liang H, et al. Tackling the objective inconsistency problem in heterogeneous federated optimization[J]. Advances in neural information processing systems, 2020, 33: 7611-7623.\n\n[3]Mendieta M, Yang T, Wang P, et al. Local learning matters: Rethinking data heterogeneity in federated learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 8397-8406.\n\n> Also, it would be helpful to consider FedSGD as one fairness baseline in the experiments, to demonstrate the effect of another component in FedEBA+, maximum entropy aggregation, for fairness.\n>\n\nThank you for your suggestion.\n\n- We present the results of FedSGD in the table below, demonstrating that FedSGD [1] performs even worse than FedAvg. This aligns with findings in other works [2], and the suboptimal performance of FedSGD can be attributed to the inefficiency introduced by the one-step local update. While the one-step update reduces local bias in a non-iid setting, other FL algorithms, including FedEBA+, with multiple updates achieve higher accuracy in significantly fewer communication rounds than FedSGD. Our approach leverages the advantages of both multiple and one-step updates by using FedSGD gradient solely for alignment in model updates.\n\n|  | CIFAR-10 |  |  |  | FashionMNIST |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Algorithm | Global Acc. | Var. | Worst 5% | Best 5% | Global Acc. | Var. | Worst 5% | Best 5% |\n| FedAvg | 67.79\u00b10.35 | 103.83\u00b110.46 | 45.00\u00b12.83 | 85.13\u00b10.82 | 86.49 \u00b1 0.09 | 62.44\u00b14.55 | 71.27\u00b11.14 | 95.84\u00b1 0.35 |\n| FedSGD | 67.48\u00b10.37 | 95.79\u00b14.03 | 48.70\u00b10.9 | 84.20\u00b10.40 | 83.79\u00b10.28 | 81.72\u00b10.26 | 61.19\u00b10.30 | 96.60\u00b10.20 |\n| FedEBA+ | 72.75\u00b10.25  | 68.71\u00b14.39  | 55.80\u00b11.28  | 86.93\u00b10.52 | 87.50\u00b10.19 | 43.41\u00b14.34 | 72.07\u00b11.47 | 95.91\u00b10.19 |\n\n- Additionally, we want to emphasize that we have conducted an ablation study for each step of FedEBA+, demonstrating the effectiveness of the maximal entropy-based aggregation. The detailed results are provided in the restated table here:\n    - FedEBA corresponds to the algorithm that only uses entropy-based aggregation compared with FedAvg; FedEBA+ indicates the algorithm uses both entropy-based aggregation and alignment update\n    - Comparing FedEBA with FedAvg, FedEBA improves accuracy by about 1.6 and fairness by about 14.3 on CIFAR-10, highlighting the proposed entropy-based aggregation approach\u2019s effectiveness. Similar observations can be found in FashionMNIST, CIFAR-100, and Tiny-ImageNet."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106034554,
                "cdate": 1700106034554,
                "tmdate": 1700110080686,
                "mdate": 1700110080686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HmA3lKnhDo",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc23 (5/7)"
                    },
                    "comment": {
                        "value": "|  | CIFAR-10 |  |  |  | FashionMNIST |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| Algorithm | Global Acc. | Var. | Worst 5% | Best 5% | Global Acc. | Var. | Worst 5% | Best 5% |\n| FedAvg | 67.79\u00b10.35 | 103.83\u00b110.46 | 45.00\u00b12.83 | 85.13\u00b10.82 | 86.49 \u00b1 0.09 | 62.44\u00b14.55 | 71.27\u00b11.14 | 95.84\u00b1 0.35 |\n| FedEBA | 69.38\u00b10.52   | 89.49\u00b110.95 | 50.40\u00b11.72  | 86.07\u00b10.90 | 86.70\u00b10.11 | 50.27\u00b15.60 | 71.13\u00b10.69 | 95.47\u00b10.27 |\n| FedEBA+ | 72.75\u00b10.25  | 68.71\u00b14.39  | 55.80\u00b11.28  | 86.93\u00b10.52 | 87.50\u00b10.19 | 43.41\u00b14.34 | 72.07\u00b11.47 | 95.91\u00b10.19 |\n|  | **CIFAR-100** |  |  |  | **Tiny-ImageNet** |  |  |  |\n| Algorithm | Global Acc. | Var. | Worst 5% | Best 5% | Global Acc. | Var. | Worst 5% | Best 5% |\n| FedAvg | 30.94\u00b10.04 | 17.24\u00b10.08 | 0.20\u00b10.00 | 65.90\u00b11.48 | 61.99\u00b10.17 | 19.62\u00b11.12 | 53.60\u00b10.06 | 71.18\u00b10.13 |\n| FedEBA | 32.38\u00b10.13 | 17.09\u00b10.06 | 0.75\u00b10.22 | 66.40\u00b10.47 | 63.34\u00b10.25 | 15.29\u00b11.36 | 54.17\u00b10.04 | 70.98\u00b10.10 |\n| FedEBA+ | 33.39\u00b10.22 | 16.92\u00b10.04 | 0.95\u00b10.15 | 68.51\u00b10.21 | 64.05\u00b10.09 | 14.91\u00b11.85 | 54.32\u00b10.09 | 71.27\u00b10.04 |\n\nWe have included the new experimental results in Table 11 of the revised manuscript.\n\n[1]McMahan H B, Moore E, Ramage D, et al. Federated learning of deep networks using model averaging[J]. CoRR, 2016.\n\n[2]McMahan B, Moore E, Ramage D, et al. Communication-efficient learning of deep networks from decentralized data[C]//Artificial intelligence and statistics. PMLR, 2017\n\n> The following statement is not clear: \u201cThe specific challenge lies in finding a way to introduce entropy into the FL training process while incorporating FL\u2019s uniform performance as a constraint within the framework.\u201d Isn\u2019t maximum entropy encouraging uniform performance? (Intuitively, the outcomes with uniform probability give the highest entropy, i.e., uncertainty, based on the definition of entropy) Does maximum entropy go against the goal of uniform performance? The reason why these concepts are treated distinctly in this statement requires clarification.\n> \n\nSorry for the confusion by our previous expression. Thank you for pointing out the inaccurate statement, we have revised it into  \u201cThe challenge lies in modeling FL aggregation as entropy and incorporating that using aggregation method to make the performance distribution more uniform.\u201d\n\nIndeed, maximizing the entropy of performance aligns with the goal of achieving uniform performance. However, in our paper, the entropy is not over performance but aggregation probability $\\mathbb{H}(p) := -\\sum_{i=1}^m p_i \\log(p_i)$. Simply maximizing the entropy $\\mathbb{H}(p)$ corresponds to making each client have a uniform aggregation distribution, this is FedAvg. \n\nRecall that our aim is to make the distribution of performance uniform, thus it\u2019s necessary to link the aggregation probability and model performance, and link them as a constraint to make the performance uniform. Thus, we introduce the ideal fair loss $\\tilde{f}(x)$ as a constraint $\\sum_{i}p_iF_i=\\tilde{f}(x)$ of the constrained entropy as shown in Eq (2), this forces the loss of aggregation to be close to the loss under truly fair model performance to ensure that the aggregated model (global model) obtained with such an aggregation probability is fair. Consequently, the aggregation probability $p_i$ becomes proportional to each client\u2019s loss $F_i(x)$. The rationale is that a larger loss indicates poor model performance on a client, prompting the updated model to pay more attention to that client, resulting in equal performance across all clients. \n\nThe constrained entropy also yields the highest entropy after satisfying the condition, ensuring that the aggregation does not take other confidential conditions into consideration and giving the highest entropy.\n\n### Theory:\n\n> The claimed convergence rate is not appropriate because there is a non-vanishing constant error term, which means that the gradient of the algorithm will never approach zero. The convergence rate is O(1) instead. This finding is inconsistent with previous studies, such as FedAvg convergence under the non-iid setting [1], which does not have the constant error term.\n> \n\nThank you for your feedback. We'd like to clarify:\n\n- **No-vanishing term is common in biased sampling/aggregation FL:** The inclusion of a no-vanishing term is a common requirement in biased sampling/aggregation methods, as demonstrated in various works[1,2,3,4,5]. The proposed entropy-based aggregation is a biased method, aiming to improve fairness, thus there is a no-vanishing term. Besides FL, this phenomenon is also commonly observed in optimization[6,7].\n- **Biased sampling/aggregation has advantages in FL:** Previous research has shown that biased sampling can expedite convergence rates and enhance accuracy in FL [1], as well as mitigate local bias issues [2]. In our work, we employ biased aggregation methods to enhance the fairness of FL."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106205333,
                "cdate": 1700106205333,
                "tmdate": 1700110097719,
                "mdate": 1700110097719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tYYOxzMzne",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc23 (6/7)"
                    },
                    "comment": {
                        "value": "- **Our convergence can degrade to FedAvg:** Our approach can revert to the state-of-the-art FedAvg's convergence rate, particularly with partial client participation [8], by setting the aggregation probability to a uniform distribution $f(x_t)=\\sum_{i\\in S_t}p_iF_i(x_t)=\\sum_{i\\in S_t}\\frac{1}{|S_t|}F_i(x_t)$, causing the no-vanishing terms to vanish.\n\n[1]Cho Y J, Wang J, Joshi G. Towards understanding biased client selection in federated learning[C] AISTATS 2022.\n[2]Wang J, Liu Q, Liang H, et al. Tackling the objective inconsistency problem in heterogeneous federated optimization[J]. NeurIPS 2021.\n\n[3]Balakrishnan R, Li T, Zhou T, et al. Diverse client selection for federated learning via submodular maximization[C] ICLR. 2022.\n\n[4]Cho Y J, Wang J, Joshi G. Client selection in federated learning: Convergence analysis and power-of-choice selection strategies[J]. arXiv preprint arXiv:2010.01243, 2020.\n\n[5]Guo Y, Lin T, Tang X. Towards federated learning on time-evolving heterogeneous data[J]. ICLR federated learning workshop 2021.\n\n[6]Generalized-Smooth Nonconvex Optimization is As Efficient As Smooth Nonconvex Optimization, ICML 2023\n\n[7]Loizou N, Vaswani S, Laradji I H, et al. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence[C] AISTATS 2021.\n\n[8]Yang H, Fang M, Liu J. Achieving linear speedup with partial worker participation in non-iid federated learning[J]. ICLR 2021.\n\n> The Theorem 5.3 claims that FedEBA+ achieves a smaller variance than FedAvg. However, the effect of FL data distributions is not reflected in Theorem 5.3. It would be clearer if the authors could justify how non-iid degree will affect the fairness results.\n> \n\nThank you for your suggestions. We'd like to justify the non-iid degree in Theorem 5.3:\n\nAs discussed in Appendix 11.2, specifically in the *Test Loss* paragraph, \u201cthe dataset on client $i$ is denoted as $\\left(\\boldsymbol{\\Xi}_i, \\mathbf{y}_i\\right)$, where $\\boldsymbol{\\Xi}_i$ is fixed, and $\\mathbf{y}_i$ follows a Gaussian distribution $\\mathcal{N}\\left(\\boldsymbol{\\Xi}_i \\mathbf{w}_i, \\sigma_2^2 \\boldsymbol{I}_n\\right)$. The heterogeneity across clients arises solely from the variability in $\\mathbf{w}_i$\u201c, where $\\textbf{w}_i$ is the true parameter on client $i$.\n\nIn Theorem 5.3, we have demonstrated that $V^{\\text{Avg}}=\\operatorname{var}\\left(f_i^{t e}\\left(\\mathbf{w}^{A v g}\\right)\\right)=\\frac{b^2}{4} \\operatorname{var}\\left(\\left\\|\\overline{\\mathbf{w}}-\\mathbf{w}_i\\right\\|_2^2\\right)$ and $V^{\\text{EBA+}}=\\operatorname{var}\\left(f_i^{t e}\\left(\\mathbf{w}^{EBA+}\\right)\\right)=\\frac{b^2}{4} \\operatorname{var}\\left(\\left\\|\\tilde{\\mathbf{w}}-\\mathbf{w}_i\\right\\|_2^2\\right)$. This implies that a larger non-iid degree, i.e., greater data heterogeneity across clients, corresponds to a larger heterogeneity of $\\mathbf{w}_i$. Consequently, larger heterogeneity of $\\textbf{w}_i$ results in larger variance for both FedAvg and FedEBA+.\n\nMoreover, Theorem 5.3 establishes that under the same heterogeneity degree, FedEBA+ consistently achieves smaller variance than FedAvg. \n\nMotivated by your comments, we have mentioned this explicitly in Theorem 5.3 of the revised manuscript. \n\n> Moreover, could the authors provide definitions for T() and A() in the variance analysis?\n> \n\nSorry for the confusion by the notations. We would like to clarify that \n\n- $T(\\xi_{i,k})$ is the generalized regression coefficient, representing a generalized expression of the variable $\\xi_{i,k}$.\n- $A(\\xi_{i,k})$ denotes the noise term distributed by $N(\\mu_{\\xi},\\sigma)$.\n\nThese clarifications have been incorporated into the notations section before Theorem 5.3 in our revised manuscript.\n\n### Experiments:\n\n> The algorithms introduce a few additional hyperparameters: temperature\u00a0, angle threshold\u00a0, and\u00a0. From Figure 4, it appears that the performance of the algorithm is quite sensitive to the hyperparameters, and different datasets have different optimal hyperparameters. It indicates a need for extensive hyperparameter tuning across different datasets. This requirement potentially reduces the algorithm's practicality compared to more straightforward federated learning algorithms like FedAvg.\n> \n\nThank you for your comments. We would like to clarify:\n\n- **No extensive hyperparameter tuning is needed:** Our algorithm consistently **achieves the best performance across all four datasets (FashionMNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet) under the same set of parameters** (temperature $\\tau=0.1$, $\\alpha=0.9$), as reported in Table 1 and Table 2 of our original submission.\n- **Fine-tuning hyperparameters can further improve algorithms, but common choices suffice:** Figure 4 demonstrates that different hyperparameter choices can enhance performance, even surpassing the best result in Table 1 and Table 2. However, no specific finetuned hyperparameter choice outperforms baselines, as indicated in the table below."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106586482,
                "cdate": 1700106586482,
                "tmdate": 1700110119856,
                "mdate": 1700110119856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oQd18SUjPP",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yc23 (7/7)"
                    },
                    "comment": {
                        "value": "|  | FashionMNIST |  | CIFAR-10 |  |\n| --- | --- | --- | --- | --- |\n| Algorithm | Global Acc. | Var. | Global Acc. | Var. |\n| FedAvg | 86.49 \u00b1 0.09 | 62.44\u00b14.55 | 67.79\u00b10.35  | 103.83\u00b110.46 |\n| q-FFL$_{q=0.001}$ | 87.05\u00b1 0.25   | 66.67\u00b1 1.39  | 68.53\u00b1 0.18  | 97.42\u00b1 0.79 |\n| q-FFL$_{q=0.5}$ | 86.57\u00b1 0.19  | 54.91\u00b1 2.82 | 68.76\u00b1 0.22  | 97.81\u00b1 2.18 |\n| q-FFL$_{q=10.0}$ | 77.29\u00b1 0.20  | 47.20\u00b1 0.82 | 40.78\u00b1 0.06  | 85.93\u00b1 1.48 |\n| PropFair$_{M=0.2, thres=0.2}$ | 85.51\u00b10.28  | 75.27\u00b15.38  | 65.79\u00b10.53  | 79.67\u00b15.71 |\n| PropFair$_{M=5.0, thres=0.2}$ | 84.59\u00b11.01 | 85.31\u00b18.62 | 66.91\u00b11.43  | 78.90\u00b16.48 |\n| FedFV$_{\\alpha=0.1, \\tau{fv}=10} $ | 86.98\u00b10.45  | 56.63\u00b11.85  | 71.10\u00b10.44  | 86.50\u00b17.36 |\n| FedFV$_{\\alpha=0.2, \\tau{fv}=0}$ | 86.42\u00b10.38   | 52.41\u00b15.94 | 68.89\u00b10.15 |  82.99\u00b13.10 |\n| FedEBA+$_{\\alpha=0.1, \\tau=0.1}$ | 86.98\u00b10.10   | 53.26\u00b11.00 | 71.82\u00b10.54 | 83.18\u00b13.44 |\n| FedEBA+$_{\\alpha=0.3, \\tau=0.1}$ | 87.01\u00b10.06 | 51.878\u00b11.56 | 71.79\u00b10.35 | 77.74\u00b16.54 |\n| FedEBA+$_{\\alpha=0.5, \\tau=0.1}$ | 87.21\u00b10.06 | 40.02\u00b11.58 | 72.44\u00b10.33 | 69.71\u00b13.09 |\n| FedEBA+$_{\\alpha=0.7, \\tau=0.1}$ | 87.23\u00b10.07 | 40.456\u00b11.45 | 72.36\u00b10.15 | 77.61\u00b16.31 |\n| FedEBA+$_{\\alpha=0.9, \\tau=0.1}$ | 87.50\u00b10.19 | 43.41\u00b14.34 | 72.57\u00b10.33 | 69.88\u00b15.86 |\n| FedEBA+$_{\\alpha=0.9, \\tau=0.05}$ | 87.42\u00b10.10 | 50.46\u00b12.37 | 72.19\u00b10.16 | 71.79\u00b16.37 |\n| FedEBA+$_{\\alpha=0.9, \\tau=0.5}$ | 87.26\u00b10.06 | 52.65\u00b14.03 | 71.89\u00b10.39 | 75.29\u00b19.01 |\n| FedEBA+$_{\\alpha=0.9, \\tau=1.0}$ | 87.14\u00b10.07 | 52.71\u00b11.45 | 72.30\u00b10.26 | 73.79\u00b19.11 |\n| FedEBA+$_{\\alpha=0.9, \\tau=5.0}$ | 87.10\u00b10.14 | 55.52\u00b12.15 | 72.43\u00b10.11 | 82.08\u00b18.31 |\n\n- **Angle threshold** $\\theta$ **requires no tuning as it is a choice, not a fine-tuned hyperparameter:** For $\\theta$, a smaller value corresponds to better performance, as shown in the ablation study. The choice of $\\theta$ depends solely on the user's needs based on communication capabilities and does not necessitate fine-tuning. Even with a large $\\theta$ and no additional communication, FedEBA+ outperforms baselines, notably FedAvg. The ablation study for $\\theta$ is provided in the reply to the next answer.\n- **Hyperparameter finetuning is common in fair FL algorithms:** Table 3 in our submission illustrates that all fair algorithms require some hyperparameter tuning. These algorithms aim to improve fairness, not just the basic communication function between the server and clients. Unlike FedAvg, which requires no hyperparameter tuning, fair algorithms can significantly enhance fairness and global accuracy through fine-tuning.\n\nThe results for different parameter choices of ($\\tau$ and $\\alpha$) have been added to Table 12 in the revised manuscript.\n\n> The influence of the angle threshold\u00a0\u00a0on FedEBA+'s performance is not demonstrated. It would be beneficial to provide an ablation study on\u00a0.\n> \n\nThank you for your suggestion. \n\n- We would like to clarify that due to page limits, we provided the ablation study of $\\theta$ in Appendix 14 (Table 5 and Figures 9-12) on FashionMNIST in our original submission.\n- Furthermore, we have presented new ablation results for $\\theta$ on CIFAR-10. The table below illustrates that:\n    - With sparse additional communication, like $\\theta=40^\\circ$, FedEBA+ achieves significantly better performance than baselines (Accuracy improves by 2.5, fairness improves by 24.3 compared to FedAvg on CIFAR-10).\n    - Even with no additional communication ($\\theta=90^\\circ$), FedEBA+ promises the best fairness and comparable accuracy on both datasets.\n\n|  | FashionMNIST (MLP) |  | Additional communication as a percentage of total communications (7.8MB/round) | CIFAR-10 (CNN) |  | Additional communication as a percentage of total communications (30.4MB/round) |\n| --- | --- | --- | --- | --- | --- | --- |\n| Algorithm | Global Acc. | Var. |  | Global Acc. | Var. |  |\n| FedAvg | 86.49 \u00b1 0.09 | 62.44\u00b14.55 | - | 67.79\u00b10.35  | 103.83\u00b110.46 | - |\n| q-FFL | 87.05\u00b1 0.25   | 66.67\u00b1 1.39  | - | 68.53\u00b1 0.18  | 97.42\u00b1 0.79 | - |\n| FedMGDA+ | 84.64\u00b10.25  | 57.89\u00b16.21 | - | 67.16\u00b10.33  | 97.33\u00b11.68 | - |\n| AFL | 85.14\u00b10.18  | 57.39\u00b16.13 | - | 66.21\u00b11.21  | 79.75\u00b11.25 | - |\n| PropFair | 85.51\u00b10.28  | 75.27\u00b15.38 | - | 65.79\u00b10.53  | 79.67\u00b15.71 | - |\n| TERM | 84.31\u00b10.38  | 73.46\u00b12.06 | - | 65.41\u00b10.37  | 91.99\u00b12.69 | - |\n| FedFV | 86.98\u00b10.45  | 56.63\u00b11.85  | - | 71.10\u00b10.44  | 86.50\u00b17.36 | - |\n| FedEBA+$_{\\tau=0.1, \\alpha=0.9}$ |  |  |  |  |  |  |\n|     $\\theta=0\u00b0$ | 87.50\u00b10.19 | 43.41\u00b14.34 | 50.0% | 72.75\u00b10.25 | 68.71\u00b14.39 | 50.0% |\n|     $\\theta=15\u00b0$ | 87.14\u00b10.12 | 43.95\u00b15.12 | 48.6% | 71.92\u00b10.33 | 75.95\u00b14.72 | 26.2% |\n|     $\\theta=30\u00b0$ | 86.96\u00b10.06 | 46.82\u00b11.21 | 37.7% | 70.91\u00b10.46 | 70.97\u00b14.88 | 12.7% |\n|     $\\theta=45\u00b0$ | 86.94\u00b10.26 | 46.63\u00b14.38 | 4.2% | 70.24\u00b10.08 | 79.51\u00b12.88 | 0.2% |\n|     $\\theta=90\u00b0$ | 86.78\u00b10.47 | 48.91\u00b13.62 | 0 | 70.14\u00b10.27 | 79.43\u00b11.45 | 0 |\n\nWe add the results of CIFAR-10 to Table 5 of the original paper as a richer Table 5 in the revised manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700106768719,
                "cdate": 1700106768719,
                "tmdate": 1700110152848,
                "mdate": 1700110152848,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0844GivnNU",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer yc23,\n\nI hope this message finds you well.\n\nWe would like to first express our profound gratitude for the time and expertise you've dedicated to the assessment of our submission. We appreciate your insights and have found your comments to be extremely beneficial in refining our work.\n\nRegarding your concerns, we hope that our rebuttal was able to shed light on them. To summarize briefly, we have clearly described the algorithm by adding explanations, improved the writing based on your suggestions, clarified the misunderstanding regarding relationships between FedSGD and FedEBA+, and between maximum entropy and performance uniformity. We have also explained and justified the soundness of the theory and provided additional experimental results of hyperparameters.\n\nYour constructive feedback has recommended that we refine our expression and pinpoint the limitations of our work. We deeply appreciate your expertise and we have refined our work based on your feedback. Thank you once again for your time and consideration.\n\nWarmest regards,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461014337,
                "cdate": 1700461014337,
                "tmdate": 1700461014337,
                "mdate": 1700461014337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QRm4zLpi22",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Reviewer_yc23"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Reviewer_yc23"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the responses. After reading your responses, I have some comments on it:\n\n> 1: High communication costs remain. \n\nThanks for fixing the algorithm. However, the algorithm still requires two communication exchanges between the server and clients in each round $t$, as suggested by  Line 6 and Line 11. This seems inherent to the proposed method since each client needs the global gradient to regularize local training. In contrast, other fairness-focused algorithms like q-FFL and AFL manage with a single communication per round.\n\n> 2: The notations are not consistent in Algorithm 1. \n\n$\\tilde{g}^t$ in line 9 is not defined.  How does this differ from the $\\tilde{g}^{b,t}$ in line 6?\n\n> 3:  Eq 3 is still wrong. \n\nIn your response, step 1 and step 2 are not consistent: In step 1, the same $x^i_{t,K}$ is used for both denominator and numerator, aligning with what Equation 3 suggests. However, in the step 2, take $p_1$ as an example, a different approach is used: $x^1_{t,K}$ for numerator, but different $x^1_{t,K}, x^2_{t,K}, \u2026 $ for denominator. This inconsistency needs to be addressed for a clearer understanding. \n\n> 4: Interchangeability of $\\tilde{f}(x)$, $\\tilde{\\Delta}_t$,  and $\\tilde{g}^t$. \n\nThank the author for providing the justifications for introducing the unweighted aggregation (i.e., ideal global gradient) and reweighted aggregation for FedSGD update  (ideal fair gradient). However, considering the heterogeneity of local data, it seems that weighted aggregation could also enhance global model performance and should be used as the ideal global gradient. The original FedSGD algorithm [1] uses a weighted average based on the number of local samples. The design choice for unweighted aggregation needs further clarification. \n\n> 5: Theory. \n\nThank you for clarifying the issue of biased sampling. However, the paper's claim about a convergence rate of O(1/T) as mentioned above Remark 5.2, appears problematic due to the constant error term O(1). This suggests that the accumulated gradient will not converge to zero.\n\n\nThe authors well answer other questions. I would like to keep my original score.\n\n\n[1] Communication-Efficient Learning of Deep Networks from Decentralized Data, AISTATS 2017."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678277187,
                "cdate": 1700678277187,
                "tmdate": 1700693674801,
                "mdate": 1700693674801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TgTuPMX5kf",
                "forum": "UJeIujVxMn",
                "replyto": "7wYdvyigpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5023/Reviewer_yc23"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5023/Reviewer_yc23"
                ],
                "content": {
                    "comment": {
                        "value": "Q3: Eq 3 defines $p_i = \\frac{\\exp (F_i(x)/\\tau)}{\\sum_j \\exp (F_j (x)/\\tau)}$.  The denominator and numerator use the *same* $x$, which I believe is incorrect. Your response in step 3 presents $p_i = \\frac{\\exp (F_i(x)/\\tau)}{\\sum_j \\exp (F_j (x^j)/\\tau)}$, differing from Equation 3. You are using different $x$ for the denominator and numerator.\n\nQ4:  Why do the authors use weighted average $n_i/n$ for all FL algorithms, but use unweighted average $1/|S_i|$ for the proposed ideal global gradient? In the current design, \"the ideal global gradient\" does not take the heterogeneity into account, which is not ideal. \n\nQ5: [1] proves that  $F (w^T) - F (w*) \\leq O(1/T) $ + constant error. This indicates that the algorithm in [1] converges to a neighborhood of optimality (e.g., the achieved loss is not optimal).  [3] proves a similar bound but for $w^T - w^*$.  \nHowever, the authors try to bound the gradient, i.e.,  $\\nabla F( w^T) \\leq O(1/T) $ + constant error.  The constant error term for gradient indicates that the algorithm does not converge to a stationary point.  Theorem 1 in [2] and  [A] actually do not involve the constant error term.\n\n[A] Adaptive Federated Optimization, ICLR 2021"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5023/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716694905,
                "cdate": 1700716694905,
                "tmdate": 1700718063487,
                "mdate": 1700718063487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]