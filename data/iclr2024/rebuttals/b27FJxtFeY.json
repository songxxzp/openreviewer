[
    {
        "title": "Quantum AdaBoost with Supervised Learning Guarantee"
    },
    {
        "review": {
            "id": "epdBanNupT",
            "forum": "b27FJxtFeY",
            "replyto": "b27FJxtFeY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1646/Reviewer_Vvq1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1646/Reviewer_Vvq1"
            ],
            "content": {
                "summary": {
                    "value": "This work exploits a quantum Adaboost method to enhance the generalization capability of quantum neural networks, where related theoretical analysis is provided. The work also demonstrates the ensemble architecture of quantum neural networks is promising on NISQ devices."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Using the Adaboost method for Quantum Neural Networks is interesting. \n\n2. The theoretical bounds for the quantum Adaboost algorithm are a necessary contribution."
                },
                "weaknesses": {
                    "value": "1. In section 2.1, using multi-qubit quantum gates for the parametric quantum circuits is not optimal, as real multi-qubit quantum gates have to suffer from more serious quantum noise and are harder to deal with the Barren Plateau problem. Why not use single quantum parametric gates? \n\n2. The Eq. (5) associated with the analysis of empirical risk minimizer is incorrect for the quantum neural network (QNN). The output of  QNN relies on the measurement, resulting in an additional optimization bias that is related to the expectation of observables. The authors can refer to the Reference as:\n\nRef. Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen*, Min-Hsiu Hsieh*, \"Theoretical Error Performance Analysis for Variational Quantum Circuit Based Functional Regression,\" Nature Publishing Group, npj Quantum Information, Vol. 9, no. 4, 2023\n\n3. Accordingly, the theoretical upper bound on Eq. (9) is not complete. An additional optimization bias corresponding to the optimization bias needs to be considered. \n\n4. The quantum Adaboost algorithm in Algorithm 2 is basically identical to the classical Adaboost one. So, what are the quantum advantages of quantum neural networks against classical neural networks? Since the performance of the classical neural networks can be also boosted to better performance, it is not clear why the authors highlight the quantum Adaboost counterpart. \n\n5. The authors do not provide a deeper discussion for the simulations as shown in Figure 4 and 5, why more Rounds are beneficial to the performance boost for the quantum Adaboost system? and why does the quantum Adboost method even attains worse results at the very beginning?"
                },
                "questions": {
                    "value": "1. Why more rounds T can be beneficial to the proposed Adaboost method? \n\n2. Why not provide the classical neural networks to compare the Adaboost performance? \n\n3. If using the same Adaboost algorithm, what are the quantum advantages of using quantum neural networks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698571774614,
            "cdate": 1698571774614,
            "tmdate": 1699636092931,
            "mdate": 1699636092931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WgFulOa6WR",
                "forum": "b27FJxtFeY",
                "replyto": "epdBanNupT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Vvq1 (Weaknesses)"
                    },
                    "comment": {
                        "value": "Thank you very much for your insightful comments. We have thoroughly revised the manuscript to make our contributions clearer.  \n\nWe utilize multi-qubit quantum gates for the following reasons. 1) Parametric quantum circuits constructed by using only single quantum parametric gates are classical in essence and cannot provide sufficient expressibility for variational quantum algorithms to ensure a good approximation to the solution. 2) We agree with you that multi-qubit quantum gates may suffer from more noise and lead to Barren Plateau issues. There have been many schemes to mitigate the training issues. 3) In our experiments, we only employ 2-qubit quantum gates. 4) 2-qubit quantum gates can be realized in a native way to minimize the hardware noise. 5) 2-qubit quantum gates are necessary components in most of the existing parametric quantum circuits. \n\nThank you for pointing out the reference. We have read it in detail. First of all, Eq. (5) in our paper is a widely-adopted figure of merit to depict the generalization error, see e.g., \n\n  1)\tMatthias C. Caro, et.al., Generalization in quantum machine learning from few training data, Nature Communications, 13:4919, 2022. \n  2)\tMatthias C. Caro, et.al., Out-of-distribution generalization for learning quantum dynamics, Nature Communications, 14:3751, 2023.\n  3)\tHsin-Yuan Huang, et.al., Power of data in quantum machine learning, Nature Communications, 12:2631, 2021.\n\nIn quantum machine learning, the prediction accuracy is what we really concern, but it cannot be directly accessed as both the label of unseen data and the distribution is unknown. Nevertheless, we do have access to the empirical or training error. The difference between the prediction error and the training error is referred to as the generalization error. It is unknown but we can bound it as we have done in our work. \n\nIn your mentioned reference, they provided another way for error decomposition, but the so-called training error (whose meaning is different from ours) that results from the optimization bias of gradient-based algorithms is hard to bound. In fact, they did not give such a bound. In addition, to obtain their main results, they assumed a crucial PL condition which assumes that the norm of the cost function gradient is bounded below by the cost function multiplied by a parameter $\\mu$, namely, $\\frac{1}{2}\\\\|\\nabla\\mathcal{L}_S(\\boldsymbol{\\theta})\\\\|^2_2\\geq\\mu\\mathcal{L}_S(\\boldsymbol{\\theta})$. This is a very strong assumption, as it is well-known that training quantum parameterized circuits often suffers from the Barren Plateau issue, where the cost gradient vanishes exponentially fast as the scale of the problem increases. Thus, the strong PL constraint may limit the applicability of their main result. Our work provides the first theoretical proof that for binary classification the prediction error of variational quantum AdaBoost can converge to 0 as the increase of the number of boosting rounds and sample size."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363496640,
                "cdate": 1700363496640,
                "tmdate": 1700363496640,
                "mdate": 1700363496640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ibMCVKzW1t",
                "forum": "b27FJxtFeY",
                "replyto": "epdBanNupT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Vvq1 (Questions)"
                    },
                    "comment": {
                        "value": "- Why more rounds T can be beneficial to the proposed Adaboost method? \n\nThe fact that more rounds are beneficial to the performance of quantum AdaBoost is owing to the characteristic of AdaBoost. Take Algorithm 2 for an illustration. When $t=1$, it starts from a selected base classifier whose error is less than 1/2. Then at each round ($t\\in[T]$), a new base classifier, whose error is less than 1/2, is selected and the distribution is updated to have the effect of focusing more on the points incorrectly classified at the next round. After $T$ rounds, the classifier returned by AdaBoost is a non-negative linear combination of the base classifiers selected at each round. In Figure 4 and Figure 5, when $T=1$, namely, at the beginning stage of AdaBoost, the classifier is only a base classifier, which is weak. Thus, its performance is relatively bad. We have made this clearer in the current manuscript. \n\n- Why not provide the classical neural networks to compare the Adaboost performance? \n- If using the same Adaboost algorithm, what are the quantum advantages of using quantum neural networks?\n\nAccording to your suggestion, we have performed new experiments to compare our quantum AdaBoost with its classical counterpart. For classical AdaBoost, weak classifiers can be boosted to obtain a strong classifier which has a high level of training accuracy. However, it is generally hard to theoretically guarantee that the generalization error of the strong classifier is small. In contrast, for quantum machine learning, the generalization of quantum classifiers can be guaranteed, while the training is often difficult. In our paper we combine the advantages of quantum machine learning in generalization and classical ensemble methods in training to obtain our main result, namely, variational quantum AdaBoost with supervised learning guarantee for binary classification. In the revised manuscript, as demonstrated in Table 1, we numerically validate that while the training accuracy of quantum AdaBoost is lower than that of classical AdaBoost, the prediction accuracy on new data of quantum AdaBoost is better as compared with classical AdaBoost.\n\n||QCNN+AdaBoost|QCNN+Bagging|CNN+AdaBoost|CNN+Bagging|\n|--|--|--|--|--|\n|Training Acc. |$0.975 \\pm 0.002$| $0.898 \\pm 0.006$|$0.980 \\pm 0.004$| $0.982\\pm0.004$|\n|Prediction Acc. |$\\boldsymbol{0.973\\pm0.001}$|$0.888\\pm0.005$|$0.967\\pm0.003$|$0.965\\pm0.002$|\n|Base Classifier |$0.861\\pm0.019$|$0.851\\pm0.020$|$0.876\\pm0.051$ |$0.872\\pm0.045$|"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363686660,
                "cdate": 1700363686660,
                "tmdate": 1700364231822,
                "mdate": 1700364231822,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JFdSewpSrH",
                "forum": "b27FJxtFeY",
                "replyto": "WgFulOa6WR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1646/Reviewer_Vvq1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1646/Reviewer_Vvq1"
                ],
                "content": {
                    "title": {
                        "value": "A follow-up response to the authors' feedback"
                    },
                    "comment": {
                        "value": "I thank the authors' response to my comments on the paper. Although the authors tried to clarify the theoretical contribution to the quantum neural network, however, their results are not novel as AdaBoost is a commonly used method to aggregate weaker classifiers into a strong one, making the training error eventually lowered to 0. The AdaBoost method is not restricted to QNN, it can be used in all classical machine learning methods, and the authors do not demonstrate the quantum advantages over the classical ones in terms of generalization capability or efficient training process. \n\nAs for the reference I recommended to the authors, the setup of the PL condition for QNN can indeed guarantee a low training error, and the PL condition can be relaxed by using a pre-training method that also ensures a low optimization bias, and the pre-trained model can be much more easily set up. The authors can refer to the following paper below:\n\nJun Qi, et al., \"Pre-Training Tensor-Train Networks Facilitate Machine Learning with Variational Quantum Circuits,\" arXiv:2306.03741v1. \n\nOverall, the theoretical contribution of this paper is limited, and I cannot raise my evaluation score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707595951,
                "cdate": 1700707595951,
                "tmdate": 1700707595951,
                "mdate": 1700707595951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zPyMFyX9gS",
            "forum": "b27FJxtFeY",
            "replyto": "b27FJxtFeY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1646/Reviewer_Pgtb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1646/Reviewer_Pgtb"
            ],
            "content": {
                "summary": {
                    "value": "This work mainly proposes a quantum counterpart of AdaBoost algorithm, giving the theoretical analysis of prediction error on the binary classification problem and numerically providing the proof-of-principle experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The manuscript is well-written and clearly introduces the quantum AdaBoost under the framework of the variational quantum algorithm. From theoretical and numerical perspectives, it demonstrates the feasibility of improving the performance of a quantum learning model by combining a few weaker ones."
                },
                "weaknesses": {
                    "value": "1. With the limitation of the system size and circuit depth, the study aims to combine a few weaker quantum classifiers to improve the performance. The manuscript does not show the quantum advantages of the proposed model compared to classical ones from neither theoretical nor numerical. For instance, giving some tasks that are challenging for classical algorithms but can be surpassed by quantum learning models. \n2. The technical and conceptual contributions are not significant enough.\n3. In numerics, it only provides a single run which is insufficient."
                },
                "questions": {
                    "value": "1. The algorithm 2 points out the error of the base classifier should be small. However, under the limitation of circuit depth, how to guarantee the classifier $h_t$ has a small test error meanwhile with shallow circuit depth.\n2. In theorem 3.1, it points out that the generalization error is bounded by the number of training samples $n$ and independent trainable gates $K$. Since we can increase the number of $K$ to improve the model and decrease the error, however, why do we increase $K$, the bound is getting worse, which is not reasonable. \n3. The quantum advantages are not quite clear, is there any evidence that the proposed method gives quantum advantages?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Reviewer_Pgtb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653129338,
            "cdate": 1698653129338,
            "tmdate": 1699636092861,
            "mdate": 1699636092861,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kn5LuoYL9f",
                "forum": "b27FJxtFeY",
                "replyto": "zPyMFyX9gS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Pgtb"
                    },
                    "comment": {
                        "value": "Thank you very much for your insightful comments. We have thoroughly revised the manuscript to make our contributions clearer. \n\nFinding various practical tasks that can demonstrate quantum superiority remains an open question. Here, we combine the advantages of quantum machine learning in generalization and classical AdaBoost in training to provide the first theoretical proof that for binary classification the prediction error of variational quantum AdaBoost can converge to 0 as the increase of the number of boosting rounds and sample size. In the revised manuscript, we also numerically validate that while the training accuracy of quantum AdaBoost is lower than that of classical AdaBoost, the prediction accuracy on new data of quantum AdaBoost is better as compared with classical AdaBoost. We have demonstrated the new numerical results in Table 1 in the revised manuscript. And we have performed more runs of experiments and all the results support our conclusion.\n\n||QCNN+AdaBoost|QCNN+Bagging|CNN+AdaBoost|CNN+Bagging|\n|--|--|--|--|--|\n|Training Acc. |$0.975 \\pm 0.002$| $0.898 \\pm 0.006$|$0.980 \\pm 0.004$| $0.982\\pm0.004$|\n|Prediction Acc. |$\\boldsymbol{0.973\\pm0.001}$|$0.888\\pm0.005$|$0.967\\pm0.003$|$0.965\\pm0.002$|\n|Base Classifier |$0.861\\pm0.019$|$0.851\\pm0.020$|$0.876\\pm0.051$ |$0.872\\pm0.045$|\n\n- The algorithm 2 points out the error of the base classifier should be small. However, under the limitation of circuit depth, how to guarantee the classifier $h_t$ has a small test error meanwhile with shallow circuit depth.\n\nAlgorithm 2 only requires that quantum weak classifiers are slightly better than random guess, which is easy to be satisfied. We have made this clearer in the revised version.\n\n-  In theorem 3.1, it points out that the generalization error is bounded by the number of training samples $n$ and independent trainable gates $K$. Since we can increase the number of $K$ to improve the model and decrease the error, however, why do we increase $K$, the bound is getting worse, which is not reasonable.\n\nWhen increasing $K$, the bound is getting worse. This is owing to the Occam\u2019s Razor principle: *Plurality should not be posited without necessity*. Specifically, there is a trade-off between reducing the training error versus controlling the size of the hypothesis set: a larger hypothesis set ($K$ is larger) could help reduce the training error, but is penalized by the second and third terms in Eq. (9). For a similar training error, it suggests using a smaller hypothesis set. \n\n- The quantum advantages are not quite clear, is there any evidence that the proposed method gives quantum advantages?\n\nFinding various practical tasks that can demonstrate quantum superiority remains an open question. Here, we utilize the advantage of quantum machine learning in generalization to provide the first theoretical proof that for binary classification the prediction error of variational quantum AdaBoost can converge to 0 as the increase of the number of boosting rounds and sample size."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363394224,
                "cdate": 1700363394224,
                "tmdate": 1700363394224,
                "mdate": 1700363394224,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ujp8XnC3mn",
            "forum": "b27FJxtFeY",
            "replyto": "b27FJxtFeY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1646/Reviewer_5yFC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1646/Reviewer_5yFC"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers quantum ensemble learning when the quantum classification models are used as weaker learners. The generalization error bound is given for this type of ensemble learning. The authors also give empirical evidence of improved accuracy by quantum ensemble learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "*Quality*\n\n- The analysis of ensemble learning when the weak learners are quantum models are provided with rigor.  This analysis follows the standard routine for ensemble learning, and is technically sound to the best of my knowledge.\n\n- The experimental settings make sense to me. \n\n*Clarity*\n\n- This paper is in general well written."
                },
                "weaknesses": {
                    "value": "*Novelty*\n\n- Limited novelty in theoretical analysis. The proof seems a straightforward combination of the standard analysis for ensemble learning and well-established lemmas for quantum models. Thus, the novelty of the theoretical analysis in this paper is limited. \n\n- Limited novelty in the findings. The finding that ensemble learning can improve upon weak learners is not new to most ML audience. Thus, the key findings in this paper are not novel. \n\n*Significance*\n\n- Due to the limited novelty, this work seems of limited significance in both theoretical machine learning and quantum machine learning. \n\n*Reproductivity*\n\n- As there is no code for this work, it is unclear whether the empirical results are reproductible."
                },
                "questions": {
                    "value": "*Question 1: Theoretical novelty*\n\nThe analysis in this paper seems a direct combination of existing tools for ensemble learning and quantum classifiers. What are the non-trivial theoretical points in this work?\n\n*Question 2: Difference with related work*\n\nWhat are the main differences between this work and existing works for ensemble learning and quantum machine learning? Please respond precisely.\n\n*Question 3: New findings*\n\nWhat are the main differences in experiments between the quantum weak learners and weak (classical) classifiers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Reviewer_5yFC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699533937470,
            "cdate": 1699533937470,
            "tmdate": 1699636092799,
            "mdate": 1699636092799,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h8K8mDQqBq",
                "forum": "b27FJxtFeY",
                "replyto": "Ujp8XnC3mn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5yFC"
                    },
                    "comment": {
                        "value": "Thank you very much for your insightful comments. We have thoroughly revised the manuscript to present our contributions and the differences from existing works more clearly.\n\n- Theoretical novelty\n\nTo the best of our knowledge, our work provides the first theoretical proof that for binary classification the prediction error of variational quantum AdaBoost can converge to 0 as the increase of the number of boosting rounds and sample size. To be specific, for classical AdaBoost, weak classifiers can be boosted to obtain a strong classifier which has a high level of training accuracy. However, it is generally hard to theoretically guarantee that the generalization error of the strong classifier is small. In contrast, for variational quantum machine learning, the generalization of quantum classifiers can be guaranteed, while the training is often difficult. In this paper we combine the advantages of quantum machine learning in generalization and classical ensemble methods in training, and utilize their analytical tools to obtain our main result, namely, variational quantum AdaBoost with supervised learning guarantee for binary classification. \n\n- Difference with related work\n\nWe have revised Sec. 1.2 Related Work to make it clearer. For your convenience, we list the differences between our work and existing works in the following.\n\n   1)\tAs compared with classical AdaBoost, the training accuracy of quantum AdaBoost is lower, since it is often harder to train a quantum classifier than to train a classical classifier. However, as the generalization is easy to be guaranteed for quantum classifiers, the prediction accuracy on new data of quantum AdaBoost is better as compared with classical AdaBoost. We have performed new experiments to demonstrate this in Table 1 in the revised manuscript.\n   2)\tVarious quantum versions of AdaBoost have been proposed, such as Arunachalam & Maity, 2020; Wang et al., 2021b; Ohno, 2022. In their works, they employed quantum subroutines, e.g., mean estimation and amplitude amplification, to update quantum weak classifiers and estimate the weighted errors. Therefore, the realizations of these quantum versions of AdaBoost are beyond the scope of current noisy intermediate-scale quantum (NISQ) devices. In contrast, in our work we utilize variational quantum classifiers realized on the current NISQ circuits, which are obtained through a quantum-classical hybrid way.\n   3)\tAlthough the variational quantum AdaBoost has been investigated, e.g., in Li et al., 2023, its performance is validated only through numerical simulations. In our work, we theoretically guarantee the performance of quantum AdaBoost.\n   4)\tIn the current NISQ era, quantum machine leaning is generally weak owing to limited scale of quantum circuits and inevitable influences of noise. Thus it is better to utilize the ensemble methods, e.g., AdaBoost, to combine weak learners to obtain a strong leaner. \n\n- New findings\n\nThe training accuracy of quantum weak learners is lower than that of classical ones, as it is often harder to train quantum learners than to train classical learners. In our new experiments (illustrated in Table 1), under the same number of optimization iterations, the average training accuracy of quantum AdaBoost (having 120 parameters) is 0.975 with standard deviation 0.002, while the classical AdaBoost (having 787 parameters) has a higher level of average training accuracy which is 0.980 with standard deviation 0.004. In contrast, as the generalization is easy to be guaranteed for quantum classifiers, the average prediction accuracy on new data of quantum AdaBoost is 0.973 with standard deviation 0.001, which is better than that of the classical AdaBoost being 0.967 with standard deviation 0.003. We have demonstrated these results in Table 1 in the revised manuscript.\n\n||QCNN+AdaBoost|QCNN+Bagging|CNN+AdaBoost|CNN+Bagging|\n|--|--|--|--|--|\n|Training Acc. |$0.975 \\pm 0.002$| $0.898 \\pm 0.006$|$0.980 \\pm 0.004$| $0.982\\pm0.004$|\n|Prediction Acc. |$\\boldsymbol{0.973\\pm0.001}$|$0.888\\pm0.005$|$0.967\\pm0.003$|$0.965\\pm0.002$|\n|Base Classifier |$0.861\\pm0.019$|$0.851\\pm0.020$|$0.876\\pm0.051$ |$0.872\\pm0.045$|\n\n- As there is no code for this work, it is unclear whether the empirical results are reproductible.\n\nWe will release the codes when preparing the camera-ready version. If you are interested in reproducing our results, we are also ready to share these codes with you."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363332047,
                "cdate": 1700363332047,
                "tmdate": 1700375010487,
                "mdate": 1700375010487,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D0ynCoJxVn",
                "forum": "b27FJxtFeY",
                "replyto": "h8K8mDQqBq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1646/Reviewer_5yFC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1646/Reviewer_5yFC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I do not have further questions."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632174717,
                "cdate": 1700632174717,
                "tmdate": 1700632174717,
                "mdate": 1700632174717,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HTwPTWxgLZ",
            "forum": "b27FJxtFeY",
            "replyto": "b27FJxtFeY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1646/Reviewer_9hGs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1646/Reviewer_9hGs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to apply AdaBoost to ensemble models of parameterized quantum circuits for binary or multi-class classification of quantum states. An additional variant of the AdaBoost algorithm is proposed which is tailored for binary classification. Theoretical bounds and numerical demonstrations are presented in the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well-presented. The algorithms and theorems are easy to follow."
                },
                "weaknesses": {
                    "value": "The proposed method, named quantum AdaBoost, does not exploit the underlying quantum information. Effectively, it treats the PQCs as arbitrary weak classifiers that can take quantum states as input, and use AdaBoost to ensemble them. The novelty of this work is not thoroughly justified by the evidence presented. Besides, it is confusing to claim that multiple quantum AdaBoost algorithms have already been proposed in the literature while naming the proposed framework as quantum AdaBoost. The comparison between these algorithms and frameworks is also not explained. \n\nThe main theorem of the paper seems to be a combination of previously known results, with an additional bound on the Rademacher complexity of PQCs. The ideas justifying its novelty are not illustrated.\n\nThe experiments have not considered other ensemble methods and only compare the proposed framework with the base models. The experiments where noises are present do not suffice to justify that the proposed method is robust to the noises."
                },
                "questions": {
                    "value": "What are the differences between your quantum AdaBoost and other previously proposed quantum AdaBoost algorithms?\n\nHow does your framework compare to other ensemble methods for quantum classifiers?\n\nWhy does your framework mitigate the effects of noises in PQCs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1646/Reviewer_9hGs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1646/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699614152093,
            "cdate": 1699614152093,
            "tmdate": 1699636092731,
            "mdate": 1699636092731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tglN5dNOOn",
                "forum": "b27FJxtFeY",
                "replyto": "HTwPTWxgLZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1646/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9hGs"
                    },
                    "comment": {
                        "value": "Thank you very much for your insightful comments.\n\n- What are the differences between your quantum AdaBoost and other previously proposed quantum AdaBoost algorithms?\n\nWe have thoroughly revised the manuscript to more clearly present our contributions and the differences from other previously proposed algorithms. For example, we have changed the title to \u201cVariational Quantum AdaBoost with Supervised Learning Guarantee\u201d and rewritten Sec. 1.2 Related Work and Sec. 1.3 Our Contributions to make them clearer.\n\nTo the best of our knowledge, our work provides the first theoretical proof that for binary classification the prediction error of variational quantum AdaBoost can converge to 0 as the increase of the number of boosting rounds and sample size. To be specific, for classical AdaBoost, weak classifiers can be boosted to obtain a strong classifier which has a high level of training accuracy. However, it is generally hard to theoretically guarantee that the generalization error of the strong classifier is small. In contrast, for variational quantum machine learning, the generalization of quantum classifiers can be guaranteed, while the training is often difficult. In this paper we combine the advantages of quantum machine learning in generalization and classical ensemble methods in training to obtain our main result, namely, variational quantum AdaBoost with supervised learning guarantee for binary classification. \n\nVarious quantum versions of AdaBoost have been proposed, such as Arunachalam & Maity, 2020; Wang et al., 2021b; Ohno, 2022. In their works, they employed quantum subroutines, e.g., mean estimation and amplitude amplification, to update quantum weak classifiers and estimate the weighted errors. Therefore, the realizations of these quantum versions of AdaBoost are beyond the scope of current noisy intermediate-scale quantum (NISQ) devices. In contrast, in our work we utilize variational quantum classifiers realized on the current NISQ circuits, which are obtained through a quantum-classical hybrid way.\n\n- How does your framework compare to other ensemble methods for quantum classifiers?\n\nFollowing your suggestion, we have performed new experiments comparing our quantum AdaBoost with Quantum Bagging, classical neural networks+AdaBoost, and classical neural networks+Bagging. We find that our quantum AdaBoost has the best prediction accuracy on new data among all these results. The results are demonstrated in Table 1 in the revised manuscript.\n\n||QCNN+AdaBoost|QCNN+Bagging|CNN+AdaBoost|CNN+Bagging|\n|--|--|--|--|--|\n|Training Acc. |$0.975 \\pm 0.002$| $0.898 \\pm 0.006$|$0.980 \\pm 0.004$| $0.982\\pm0.004$|\n|Prediction Acc. |$\\boldsymbol{0.973\\pm0.001}$|$0.888\\pm0.005$|$0.967\\pm0.003$|$0.965\\pm0.002$|\n|Base Classifier |$0.861\\pm0.019$|$0.851\\pm0.020$|$0.876\\pm0.051$ |$0.872\\pm0.045$|\n\n\n- Why does your framework mitigate the effects of noises in PQCs?\n\nIn addition to the depolarizing noise, we also perform new experiments under the amplitude damping noise and phase damping noise. All experimental results demonstrate that quantum AdaBoost can help mitigate the influence of these noises. The reasons are twofold. First, in our variational quantum AdaBoost, weak learners can be boosted to obtain a strong learner as long as the weak learners are slightly better than random guess. Noise may degrade the weak leaners, however, as long as they are still better than random guess, they can be boosted to obtain a strong learner. Second, as the quantum parameterized circuit is shallow, quantum learners are weak, but also, the classifiers are less affected by noise due to shallow circuits."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700363187965,
                "cdate": 1700363187965,
                "tmdate": 1700363187965,
                "mdate": 1700363187965,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EhCVS1eXZ2",
                "forum": "b27FJxtFeY",
                "replyto": "tglN5dNOOn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1646/Reviewer_9hGs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1646/Reviewer_9hGs"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the author's response.\n\n> However, it is generally hard to theoretically guarantee that the generalization error of the strong classifier is small. \n\nThere exists well-established research on the theoretical guarantees of the generalization error of ensemble methods, which is essentially one of the motivations of ensemble methods. For example, check chapter 4 of [1].\n\n[1] Schapire R E, Freund Y. Boosting: Foundations and algorithms[J]. Kybernetes, 2013, 42(1): 164-166."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1646/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577355297,
                "cdate": 1700577355297,
                "tmdate": 1700577355297,
                "mdate": 1700577355297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]