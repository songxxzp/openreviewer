[
    {
        "title": "Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem"
    },
    {
        "review": {
            "id": "Na49L87kmP",
            "forum": "nxnbPPVvOG",
            "replyto": "nxnbPPVvOG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3582/Reviewer_v15j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3582/Reviewer_v15j"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the trade-off between the variance (MSE) and bias (bias norm) for the regularized linear regression model, where the authors consider various Schatten norms for the regularized term. Then, the authors derive the expected MSE for the Gaussian and diagonal ensembles and compare it with the ridge regression model's one and show that the ridge regression model is not always the best in term of the trade-off by using some experiments on sythetic datasets.  There exit some similar works such as Bayatti and Montanari (2011), Samet et al. (2013) for Lasso, however the authors limit their work to the class of Schatten norms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+  The authors can obtain an exact expression for the average test error (MSE) for the spherical Gaussian model, and this bound is nearly matched to the experiment results (cf. Figure 2).\n+ Experiments show that Ridge regression, which used Frobenius norm, is not always the best option for the linear model in term of the trade-off between the variance (MSE) and the bias (the bias norm). More specifically, the authors show that the Frobenius norm and the nuclear one are likely to have the same average MSE on the spherical Gaussian ensemble (model) or the diagonal matrix model, but using the nuclear norm usually achieves better MSE than the Frobenius norm. This fact also holds when mapping the dataset via a random fourier transform (RFF)."
                },
                "weaknesses": {
                    "value": "+ The theoretical results only hold in the thermodynamic limit $N\\rightarrow \\infty, d \\rightarrow \\infty$ and $d/N\\rightarrow \\lambda$. This means that the results only hold when the number of observations is linear to the signal dimension. However, in common high-dimensional settings, the number of observations is usually sub-linear to the signal dimension. \n+ The result looks not an extension of the Gauss-Markov theorem since it only holds under expectation over $X$ when $X$ is an Gaussian ensemble or a diagonal ensemble. The Gauss-Markov theorem works for any $X$. \n+ It looks more interesting to compare your experiment results with other norms (outside the class of  Schatten norms) such as between the nuclear norm and Lasso (norm-$1$). \n+ Too many typos. Please check and correct them."
                },
                "questions": {
                    "value": "How do your results in comparison with other norms which don't belong to Schatten class of norms such as Lasso (norm-$1$)?\n\nBesides, please check and correct the following typos and unprecise.\n\n+ $L \\in \\mathbb{R}^{k \\times N} \\rightarrow L \\in \\mathbb{R}^{d \\times N}$\n+ p.2, line 21 from the top: $\\mbox{var}\\_{\\epsilon}=L^T L$ should be changed to $\\mbox{var}\\_{\\epsilon}=\\sigma^2 L L^T$. To keep the later, you may change the definition of $\\hat{\\beta}$ to $L^T(X)Y$ throughout your paper.\n+ In the definition 1, you aim to minimize the variance subject to a constraint on bias by $C$. But, in the later (Theorem 2, Figure 1, etc.), it seems to me that you don't mention $C$ again, but only mention $\\alpha$. At least you should mention what is $\\alpha$ as a function of $C$ or vice versa in Theorem 2.\n+ In Figure 1, you only plot for the very special case when $X$ is diagonal. You should also plot for other cases of $X$. \n+ Typo in Theorem 2.1, the expectation should be over $\\epsilon$ only since you already take expectation over $X$ in MSE, and $Y$ is a function of $X$ and $\\epsilon$. \n+ Right below Figure 2: Figure 3.1 $\\rightarrow$ Figure 3. \n+ Similarly, Figure 3.2.1 $\\rightarrow$ Figure 5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3582/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3582/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3582/Reviewer_v15j"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3582/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697725876575,
            "cdate": 1697725876575,
            "tmdate": 1699636313168,
            "mdate": 1699636313168,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5DCL6jEKd8",
                "forum": "nxnbPPVvOG",
                "replyto": "Na49L87kmP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v15j"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments. Below we provide responses to specific points. \n\n>The theoretical results only hold in the thermodynamic limit . This means that the results only hold when the number of observations is linear to the signal dimension. However, in common high-dimensional settings, the number of observations is usually sub-linear to the signal dimension.\n\nThis is an interesting observation. If you have a specific paper or reference in mind here, we would be happy to cite it and discuss this point as a limitation in the Discussion section. \n\n>The result looks not an extension of the Gauss-Markov theorem since it only holds under expectation over $X$ when X is an Gaussian ensemble or a diagonal ensemble. The Gauss-Markov theorem works for any X.\n\nWe apologize for the lack of clarity here. What we refer to as the Extended Gauss-Markov Theorem is actually Theorem 2 (and we have now clearly labeled this in the text in the statement of the theorem). You are correct that the results about the test error in sections 2.2.1 and 2.2.2 only hold under specific distributional hypotheses. In contrast, Theorem 2 does not make any distributional assumptions, or indeed have any probabalistic component whatsoever- it is purely a statement about an optimization problem which holds for basically arbitrary X (technically X must be full rank). However, if we assume that errors epsilon_i are uncorrelated, zero mean, and have finite variance, then we have the probabilistic interpretation that LL^T is the covariance matrix and LX-I is the bias operator. So while this interpretation does impose some distributional assumptions, it is not any more than found in the classical Gauss-Markov theorem (Note that, in the original submission  we required Gaussian error distributions, but as a consequence of this discussion have realized this is not actually necessary). \n\n>It looks more interesting to compare your experiment results with other norms (outside the class of Schatten norms) such as between the nuclear norm and Lasso (norm-1)\n\nWe have done so and included results in the appendix (section A.10). The summary is that the spectral and nuclear models are competitive with lasso even when the ground truth data has sparse structure.\n\nIt is also worth noting here that the Lasso is not manifestly a \u201cLinear estimator\u201d in the sense we define it in Section 2, even allowing for the use of norms outside the Schatten family. As a reminder, in order to qualify as a Linear estimator in our sense, the coefficients must take the form LY, where L depends only on X. It is not at all obvious that this is the case for Lasso, although we do not have a proof to the contrary.\n\n*Edit:*Actually,it is easy to prove this about Lasso. Indeed, in the case of $d=1$ (i.e. one predictor variable),the Lasso with regularization $\\alpha$ has the well-known solution $w=sign(\\langle X,Y\\rangle)max(\\langle X,Y\\rangle/|X|^2-\\alpha,0)$,which is clearly not a linear function of $Y$. \n\n>Too many typos. Please check and correct them.\n\nWe have corrected a large number of small typos, see the general response to reviewers as well as the individual responses to the other reviewers.\n\n>How do your results in comparison with other norms which don't belong to Schatten class of norms such as Lasso (norm-1)?\n\nSee above\n\n>k\\times N\\to d\\times N\n\nCorrected\n\n>p.2, line 21 from the top: var =L^TL should be changed to sigma^2 LL^T . To keep the later, you may change the definition of beta throughout your paper.\n\nWe have corrected the variance from L^TL to LL^T in both the main text and supplement. As far as the presence of sigma^2, we have in fact assumed the noise variance sigma^2=1  (see second paragraph of section 2.1), which is why we omitted it from the expression. \n\n>In the definition 1, you aim to minimize the variance subject to a constraint on bias. But, in the later (Theorem 2, Figure 1, etc.), it seems to me that you don't mention again, but only mention . At least you should mention what is as a function of or vice versa in Theorem 2.\n\nWe have included an explicit description of the relationship between alpha and C in the supplement (A.8), see also our response to reviewer ACbc.\n\n>In Figure 1, you only plot for the very special case when X is diagonal. You should also plot for other cases of X. \n\nThank you for pointing this out. However, the diagonal case is actually no loss of generality here. For consider any matrix X with svd UDV^T. Given some matrix L that satisfies |LX-I|<C, consider L\u2019=VLU. It is easy to see that |L\u2019D-I|<C and Tr(LL^t)=Tr(L\u2019L\u2019^T). This implies that if we know the optimum L(D) we can just rotate it to obtain L(X).\n\n>Typo in Theorem 2.1, the expectation should be over epsilon\n\nCorrected\n\nRight below Figure 2: Figure 3.1 -> Figure 3.\n\nCorrected \n\nSimilarly, Figure 3.2.1-> Figure 5\n\nCorrected"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889982870,
                "cdate": 1699889982870,
                "tmdate": 1700495880353,
                "mdate": 1700495880353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LmpRxghZNp",
                "forum": "nxnbPPVvOG",
                "replyto": "5DCL6jEKd8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3582/Reviewer_v15j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3582/Reviewer_v15j"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors' rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for your answering to my questions and comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700305748787,
                "cdate": 1700305748787,
                "tmdate": 1700305748787,
                "mdate": 1700305748787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TDtw25ZBrf",
            "forum": "nxnbPPVvOG",
            "replyto": "nxnbPPVvOG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3582/Reviewer_ACbc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3582/Reviewer_ACbc"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers a variant of linear regression with constraints placed on a \"bias operator\". Under this framework, the paper discusses an extension of the Gaussian-Markov theorem, showing empirical and theoretical evidence for its main result, Theorem 2. Later discussions in the paper surround \"flatness\" and \"deepness\" of various losses considered within the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tackles an interesting class of linear regression models and delivers a thorough presentation of various aspects of the problem, from the problem definition, main theorem statement, to several case studies, all of which help to paint the overall picture of the problem. The constrained setup considered in the paper is also interesting and intuitive. Considering that linear models are a core concept of machine learning, the paper is of sufficient interest to ICLR."
                },
                "weaknesses": {
                    "value": "There are several dimensions of weaknesses presented in the paper:\n\na. Clarity and overall quality of presentation. The paper does not appear to be carefully edited and revised, with multiple typographical errors in the first paragraph of the introduction alone (examples: \"somewht\" in line 3, lack of period at end of sentence in line 4, reverted quotation marks on line 5, etc.). The graphs, equations, and tables in latter parts of the paper can also benefit from detailed revisions. These issues surrounding clarity and presentation are not constrained to the first paragraph and can be found throughout the paper and also the supplementary material.\n\nb. Discussion of main theorem. While the problem setting itself is interesting, the discussions of the main theorem (Theorem 2) leave an impression that it can be further discussed. For example, what are the values of alpha? Although it is shown in the supplementary that alpha is a consequence of solving Equation 1 using Lagrange multipliers, it is also unclear how large (or small) the value of alpha is and how it impacts the interpretations of the main result. \n\nc. Considering that one of the paper's main claims is a \"flat minima\" phenomenon, the paper would benefit from stronger theoretical results (apart from simulation-based arguments) surrounding this claim. \n\nd. Possible typo in main theorem. In the main theorem's statement for the nuclear norm, the result relies on $\\max(\\Sigma,\\alpha)$: should $\\Sigma$ be replaced with something like the maximum eigenvalue?"
                },
                "questions": {
                    "value": "Several questions were listed in the above \"weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns were found."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3582/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671653571,
            "cdate": 1698671653571,
            "tmdate": 1699636313055,
            "mdate": 1699636313055,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5R2er6Qixb",
                "forum": "nxnbPPVvOG",
                "replyto": "TDtw25ZBrf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ACbc"
                    },
                    "comment": {
                        "value": "Thank you for your helpful comments. Below, find our responses to specific points. \n\n>The paper does not appear to be carefully edited and revised, with multiple typographical errors in the first paragraph of the introduction alone (examples: \"somewht\" in line 3, lack of period at end of sentence in line 4, reverted quotation marks on line 5, etc.). The graphs, equations, and tables in latter parts of the paper can also benefit from detailed revisions. These issues surrounding clarity and presentation are not constrained to the first paragraph and can be found throughout the paper and also the supplementary material.\n\nThank you for pointing this out. We have corrected the specific typos mentioned here, as well as a large number of other small typos, see the general response to reviewers as well as the individual responses to the other reviewers.\n\n>While the problem setting itself is interesting, the discussions of the main theorem (Theorem 2) leave an impression that it can be further discussed. For example, what are the values of alpha? Although it is shown in the supplementary that alpha is a consequence of solving Equation 1 using Lagrange multipliers, it is also unclear how large (or small) the value of alpha is and how it impacts the interpretations of the main result.\n\nWe have added a discussion of the explicit relation between alpha and C in the appendix (section A.8), with a pointer following the main theorem.\n\n>Considering that one of the paper's main claims is a \"flat minima\" phenomenon, the paper would benefit from stronger theoretical results (apart from simulation-based arguments) surrounding this claim\n\nWe appreciate this concern. While the review period is unfortunately likely too short for us to develop substantial new theoretical results, we can at least make an interesting theoretical observation about our results which bears on the \"flat minima\" phenomenon, and  was not mentioned in the main paper.  Namely, starting from our integral-based expressions for the test error (Prop. 2.1 and 2.4), we can obtain explicit expressions for the curvature of the error curve simply by differentiating under the integral. The resulting analytic expressions are complicated and not particularly illuminating, which is why we did not include them in the paper, and opted instead to explore the properties of the minima through simulations. \n\n>In the main theorem's statement for the nuclear norm, the result relies on \\Sigma should be replaced with something like the maximum eigenvalue?\n\nThe result is correct as stated, but we see how the notation could potentially be confusing here. We have clarified that the max(...) refers to an elementwise maximum of the singular values. (Note that we have also changed the notation from \\Sigma to \\sigma here)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889580059,
                "cdate": 1699889580059,
                "tmdate": 1699889580059,
                "mdate": 1699889580059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dohISJ0ASE",
            "forum": "nxnbPPVvOG",
            "replyto": "nxnbPPVvOG",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3582/Reviewer_cHcu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3582/Reviewer_cHcu"
            ],
            "content": {
                "summary": {
                    "value": "The Gauss-Markov theorem says that the best unbiased linear estimator is the pseudoinverse of the data matrix (in the paper denoted by $X$), which can be obtained by minimizing the Frobenius norm of the estimator $L$ subject to the constraint that $L$ is the left inverse of the data matrix, $LX=I$. The paper generalizes this formulation by relaxing this constraint to $||LX-I||_p\\le C$ where $||\\cdot||_p$ is the Schatten $p$-norm (vector $p$-norm on the singular values), thus allowing some bias $C$. A special case is the ridge/Tichonov regression, obtained for $p=2$. The parameter $C$ (or a monotonically related parameter $\\alpha$) is determined by validation, by minimizing the test error. The advantage is that for $p\\neq2$, the minimum of the test error over $C$ may be flatter than for $p=2$. \n\nThe authors derive a closed form solution for the optimal estimator $L$ for $p=1$ (nuclear norm) and $p=\\infty$ (spectral norm). Next, they derive explicit form (as integrals) of for the test error in thermodynamical limit for two special distributions of the data: spherical Gaussians (elements of $X$ and noise in right-hand sides are i.i.d. normal) and diagonal data (when the Gramian $X^TX$ is diagonal). The integrals are solved in closed form for $p=1$ and $p=\\infty$. This theoretical formula is shown to agree with test error on synthetically generated data.\n\nThe cases $p=1,2,\\infty$ are compared in a simulated experiment in which $\\alpha$ with smallest test error is estimated by x-validation, where best $\\alpha$ is selected by \"grid search\" from 9 log-spaced values. This showed that the nuclear-norm regression is comparable to or better than (depending on methodology) ridge regression. A similar result is obtained for the similar experiments for nonlinear regressors (random Fourier features)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The observation that a wider minima of $\\alpha$ can be achieved at the cost of a little bias is interesting.\nThe theoretical results (test errors) are non-trivial to derive.\nThe text is clear enough, though clarity could be improved by more effort."
                },
                "weaknesses": {
                    "value": "I cannot assess novelty reliably because my expertise is mainly in optimisation rather than estimation (however, I understand all parts of the main paper well). In fact, rather than extending Gauss-Markov theorem, the paper generalizes ridge regression (please, consider changing the title accordingly). It is well-known that ridge regression has non-zero bias but a smaller variance than pseudoinverse.\nSo the main novelty seems to be that of flatter minima of test error, rather than the generalization of ridge regression.\n\nThough the minima for the nuclear-norm regression are indeed flatter than for the ridge regression, the difference is sometimes only minor, as seen in Figure 2. It is true that the experiments show that in estimating $\\alpha$ by x-correlation, the nuclear norm most often wins. However, this might be due to the experimental methodology. E.g., if there were more than 9 values of $\\alpha$, the deeper minima of the ridge regression might have been hit much more often.\n\nA major weakness, in my opinion, is that the experiments are done only on synthetic data. The applications of linear regression are abundant, so it should be possible to find many suitable real datasets for this.\n\nMinor/fixable issues:\n\n1st formula in section 2.1: symbol $L(X)$ is used here but then never more. Change to $L$.\n\nThm 2: Letter $\\Sigma$ is usually used to denote the diagonal matrix with singular values. For vector of singular values, better use $\\sigma$ or $s$.\n\nThere are many small mistakes/typos in the text. E.g., references to figures in sections 2.2.4 and 3 refer to non-existent figures (e.g., figure 2.2.4 in section 2.2.4).\n\nIn the 5th line of section 2.2.1, can the formula for MSE be simplified (i.e., calculate the mean value in closed form)? It is confusing that the MSE has quite different form in sections 2.2.1 and 2.2.2 (this lets the reader wonder if this difference is substantial or just due to little care for text clarity). This might deserve a comment.\n\nMost displayed equations are unnumbered, which is not friendly for reviewers (given that lines are not numbered in the ICLR style).\n\nThe asterisk symbol in Proposition 2.1 and in the first displayed formula in section 2.2.2 has not been defined.\n\nThe integrals for $Err(\\alpha)$ in sections 2.2.1 and 2.2.2 are almost the same, up to the integrating measures. I wonder if they are correct or there are typos in them..?\n\nPOST REBUTTAL: The authors have clarified my objections, to certain extent. I am therefore raising my evaluation. However, please note that I cannot assess novelty reliably."
                },
                "questions": {
                    "value": "It would be helpful to vary the number of values of $\\alpha$ in the grid method (currently this value is 9) in the experiments, i.e., to make it a hyperparameter. Pls see my remark on this in \"weaknesses\".\n\nIt would be helpful in Figure 5 to report not only winners but also MSE for different models (as in Figure 6 left) - because a winner can win by only a small margin.\n\nWhy are any experiments on real data not included? Is there a theoretical obstacle? Or, perhaps, you believe that the results of such experiments would not be informative enough? Please comment (also in the paper, if accepted)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3582/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3582/Reviewer_cHcu",
                        "ICLR.cc/2024/Conference/Submission3582/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3582/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698704767588,
            "cdate": 1698704767588,
            "tmdate": 1700820913182,
            "mdate": 1700820913182,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ehuAi0WMvp",
                "forum": "nxnbPPVvOG",
                "replyto": "dohISJ0ASE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cHcu (pt 1)"
                    },
                    "comment": {
                        "value": "Thank you for the careful reading of our paper and helpful comments. Below, find our responses to specific points. \n\n>In fact, rather than extending Gauss-Markov theorem, the paper generalizes ridge regression (please, consider changing the title accordingly). It is well-known that ridge regression has non-zero bias but a smaller variance than pseudoinverse. So the main novelty seems to be that of flatter minima of test error, rather than the generalization of ridge regression.\n\nWe appreciate and understand the suggestion to change the title. However, we believe that the characterization of the result in terms of generalizing the GM theorem is accurate and that changing the title would come at the expense of clarity. As we explain in Section 2: \u201cThe classic Gauss-Markov theorem tells us that if we want to impose exactly zero bias, then the minimal variance estimator is the ordinary least squares estimator. But what if we want to relax this, to allow non-zero but bounded amount of bias?\u201d. From a presentational standpoint, moreover, we believe that the emphasis of the GM theorem viewpoint is clearer than a framing in terms of generalizing Ridge regression, for the reason that there are many potential ways one could think to generalize Ridge regression, while the GM theorem provides a very specific and principled avenue of generalization.   \n\nAs per your observation that ridge regression has non-zero bias but smaller variance than OLS, this is certainly true and widely known. But our Theorem 2 in fact implies a significantly stronger (and, we think, more interesting) statement, namely that ridge regression attains the best possible tradeoff between bias and variance, assuming that \u201cbias\u201d is being measured using the Frobenius norm of LX-I (cf. middle panel of figure 1). This particular fact, while easy to prove on its own, does not appear to be well known (at least, we could not find a result to exactly this effect in the literature). \n\n>Though the minima for the nuclear-norm regression are indeed flatter than for the ridge regression, the difference is sometimes only minor, as seen in Figure 2. It is true that the experiments show that in estimating by x-correlation, the nuclear norm most often wins. However, this might be due to the experimental methodology. E.g., if there were more than 9 values of, the deeper minima of the ridge regression might have been hit much more often.\n\nYou are certainly correct in observing that the number of alpha values will influence the relative performance of the models in cross-validation. We acknowledged this point, both in section 2.2.3, and in the Discussion: \u201cIt should be noted that the particulars of the cross-validation results depend on the number of \u03b1 values used in the grid search, among other factors; and as noted before, the effect of the curvature at the minimum decreases as more values of \u03b1 are used.\u201d  That said, we have also included additional results in the Appendix (section A.9) showing the effect of adding more values of alpha. Not surprisingly, the Ridge model often benefits from extra values of alpha in these experiments. \n\n>A major weakness, in my opinion, is that the experiments are done only on synthetic data. The applications of linear regression are abundant, so it should be possible to find many suitable real datasets for this.\n\nWe appreciate this concern. When writing the paper, we considered including such experiments, but opted not to for the following reason. Our rationale in designing the experiments was to (1) understand the factors that can influence the performance of the three considered models, and (2) to illustrate the practical consequences of the \u201cflat minima\u201d phenomenon vis a vis cross-validated accuracy. We thus opted for exclusively synthetic data because it is much easier to control the underlying properties of such data compared with real datasets (as per (1)), and such data are already sufficient to illustrate (2). \n\nThat said, there is no issue in principle or in practice with applying either the Spectral or Nuclear regressions to real datasets. In order to address your concern, we have done exactly this, and included the results in the Appendix (section A.11). To summarize, we consider the well-known Diabetes dataset and California housing dataset, both available from sklearn. For each dataset, we generated random splits with a training set size of 300, and the remaining samples used for testing. Otherwise, we followed the same methodology as in the synthetic Gaussian simulations. We find results that are qualitatively similar to the table5 from the Gaussian data. Namely, the Nuclear has a small advantage in terms of average MSE, and a fairly substantial one in terms of the probability of winning on a given split. \n\n(cont...)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888868705,
                "cdate": 1699888868705,
                "tmdate": 1699978938886,
                "mdate": 1699978938886,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mcWyn4cfev",
                "forum": "nxnbPPVvOG",
                "replyto": "dohISJ0ASE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cHcu (pt. 2)"
                    },
                    "comment": {
                        "value": ">1st formula in section 2.1: symbol L(X) is used here but then never more. Change to L\n\nCorrected\n\n>Thm 2: Letter \\Sigma is usually used to denote the diagonal matrix with singular values. For vector of singular values, better use \\sigma or s\n\nChanged to \\sigma\n\n>There are many small mistakes/typos in the text. E.g., references to figures in sections 2.2.4 and 3 refer to non-existent figures (e.g., figure 2.2.4 in section 2.2.4).\n\nCorrected. We have also corrected a large number of other small typos, see the general response to reviewers as well as the individual responses to the other reviewers.\n\n>In the 5th line of section 2.2.1, can the formula for MSE be simplified (i.e., calculate the mean value in closed form)? It is confusing that the MSE has quite different form in sections 2.2.1 and 2.2.2 (this lets the reader wonder if this difference is substantial or just due to little care for text clarity). This might deserve a comment.\n\nThank you for pointing this out. We have clarified this point in the text- we note that the expression for MSE can be algebraically simplified, but that we have defined it as we did in order to make clear that it is an average over the test distribution. We have also taken the opportunity to further explain why there is a difference between the MSE expressions between the spherical Gaussian and diagonal cases in section 2.2.2. We note there that the difference arises from the fact that the test observations (i.e. rows of Xtest) are not independent in the diagonal case.  \n\n>Most displayed equations are unnumbered, which is not friendly for reviewers (given that lines are not numbered in the ICLR style).\n\nWe have inserted numbers in all displayed equations in both the main text and supplement \n\n>The asterisk symbol in Proposition 2.1 and in the first displayed formula in section 2.2.2 has not been defined.\n\nThis symbol denotes the multiplication of two numbers- however we have removed it for the sake of clarity \n\n>The integrals for ERR in sections 2.2.1 and 2.2.2 are almost the same, up to the integrating measures. I wonder if they are correct or there are typos in them..?\n\nYou are correct that these two integrals are nearly the same. The basic reason for this is that both formulas are derived in a similar fashion of exploiting rotational symmetry of the matrix ensemble to reduce the MSE to a sum over the spectrum of Xtr. Besides the integrating measures, the only difference is that the integral for the Diagonal case has an extra factor of x, which intuitively corresponds to the fact that errors in estimating the coefficient for a direction of high variance contributes more to the overall error than for a direction of low variance. We have clarified this point in the main text. \n\n>It would be helpful to vary the number of values of alpha in the grid method (currently this value is 9) in the experiments, i.e., to make it a hyperparameter. Pls see my remark on this in \"weaknesses\".\n\nWe have done this and included the results in the appendix, see above. \n\n>It would be helpful in Figure 5 to report not only winners but also MSE for different models (as in Figure 6 left) - because a winner can win by only a small margin.\n\nIn the new results with varied number of alpha (A.9), we also report the individual MSE and win probability for each model. In these results, we considered only a relatively small number of the cells shown in Figure 5, however, we think they give a representative idea of the size of the margin of victory. Indeed, we see that the margin of victory in terms of MSE can be slim, but the win probabilities are usually more decisive (i.e. significantly >33%). \n\n>Why are any experiments on real data not included? Is there a theoretical obstacle? Or, perhaps, you believe that the results of such experiments would not be informative enough? Please comment (also in the paper, if accepted).\n\nSee above."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699888898907,
                "cdate": 1699888898907,
                "tmdate": 1699888898907,
                "mdate": 1699888898907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6BwiPnRcSn",
                "forum": "nxnbPPVvOG",
                "replyto": "dohISJ0ASE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3582/Reviewer_cHcu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3582/Reviewer_cHcu"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your response to my review. Let me react to some of them.\n\nAd the title: This is a minor thing and I agree with the original title. Let me just remark that when initially reading the title, I expected something more. The crucial thing about GM is that the variance is the least, wrt semidefinite the partial order on covariance matrices. We do not have this here (as you mention in the paper). We sacrifice this, together with zero, just like in ridge regression.\n\nAd larger numbers of $\\alpha$: Thanks for appendix A.9. It shows that (expectedly) larger numbers of $\\alpha$ makes ridge much better -- but *not always*. Namely, in top-right subfigure in Figure 7 in A.9,  ridge does not win even for 50 values of $\\alpha$. This means that either the ridge minimum is really really narrow (so that we would need more values of $\\alpha$ to find it -- please consider such experiment!) or that the global minimum of test error is smaller for spectral than for ridge. Which of these two options is true?\n\nThanks for including the experiment on real data. They confirm the theoretical results.\n\nLet me have one more comment: another reviewer asked about the relation between $\\alpha$ and $C$. I wonder, if problem (3) (in the revised manuscript) were reformulated to a penalty form, i.e., we minimize $\\\\|LX-I\\\\|_p + {\\alpha\\over 2}\\\\|L\\\\|_F^2$ unconstrained, would this be the same $\\alpha$ as in the paper? If so, please consider to include this for better clarity (rather than just section A.8)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700049427189,
                "cdate": 1700049427189,
                "tmdate": 1700049620889,
                "mdate": 1700049620889,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H24n7ETJo7",
                "forum": "nxnbPPVvOG",
                "replyto": "dohISJ0ASE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3582/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for responding to our rebuttal. To address your new points:\n\n>Namely, in top-right subfigure in Figure 7 in A.9, ridge does not win even for 50 values of alpha . This means that either the ridge minimum is really really narrow (so that we would need more values of to find it -- please consider such experiment!) or that the global minimum of test error is smaller for spectral than for ridge. Which of these two options is true?\n\nThe short answer is that the ridge minimum is smaller than for the spectral, but is comparatively very narrow. The panel you point out has rho=0 (i.e. uncorrelated features), so we can use some of the analytic results from the paper to understand this phenomenon. As we remarked in section 2.2.3, the global minimum of Ridge will always be smaller than for the Nuclear or Spectral in this case. Looking at the bottom rows of Figure 3, on the other hand, we see that the Spectral (but not Nuclear) has much flatter minima than Ridge for large sigma and intermediate lambda (for reference, the panel in question has sigma=3.5 and lambda=.5). This is entirely consistent with what we see in the panel from A.9-where the Spectral, but not Nuclear, outperforms Ridge in this case even for large numbers of alpha.  \n\nIt is also important to note that, even though the ground truth error curve for Ridge may have the lowest global minimum, we never actually directly observe this curve (since it is an average over all datasets, and taken in the thermodynamic limit). Rather, we only estimate the value for any fixed alpha by using cross-validation. The selection of random CV splits introduces noise into our MSE estimates. If the scale of this noise is comparable to the difference between the Ridge minimum and Spectral minimum, then adding more alpha values can result in \"overfitting to the validation set\", where we select an alpha value that happened to get a lucky CV split over one that is actually close to the minimum.  So it is possible that even with large numbers of alpha we will not find the global minimum with this methodology. \n\n\n>Let me have one more comment: another reviewer asked about the relation between and . I wonder, if problem (3) (in the revised manuscript) were reformulated to a penalty form, i.e., we minimize ${\\frac {\\alpha}2}\\||L\\||_2^2+\\||LX-I\\||_p$ unconstrained, would this be the same as in the paper? If so, please consider to include this for better clarity (rather than just section A.8).\n\nUnfortunately, the alpha in this constrained problem would not exactly correspond to the alpha in Theorem 2. We can illustrate this most easily in the ridge case. There, we can use matrix calculus to see that the solution L to the penalty problem must satisfy $\\alpha*L+{\\frac {(LX-I)X^T}{ 2||LX-I||}}=0$.After some algebra, we see that this corresponds to Ridge regression with a regularization strength of $2||LX-I||\\alpha$"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3582/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700160397215,
                "cdate": 1700160397215,
                "tmdate": 1700160397215,
                "mdate": 1700160397215,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]