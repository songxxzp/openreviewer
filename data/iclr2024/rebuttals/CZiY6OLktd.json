[
    {
        "title": "MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided Learning Process"
    },
    {
        "review": {
            "id": "5xrmqQHlQT",
            "forum": "CZiY6OLktd",
            "replyto": "CZiY6OLktd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3041/Reviewer_FBCv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3041/Reviewer_FBCv"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a Multi-Granularity Time Series Diffusion (MG-TSD) model for time series prediction. In general, MG-TSD controls the learning process of the diffusion model by leveraging the temporal signals in time series data at different granularity levels. In particular, the authors link the forward process of the diffusion model to the data smoothing process. Motivated by this, this work develops a multi-granularity guidance diffusion loss function, such that the inherent features within data can be preserved and a regularized sampling path can be achieved. Experiments on real-world time series datasets demonstrate the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. In the context of time series forecasting, it is a good idea to stabilize the diffusion model with the help of coarse-grained temporal signals in time series data. \n2. The derivation of the multi-granularity guidance loss function is solid. Besides, the learning procedure devised in this work is applicable."
                },
                "weaknesses": {
                    "value": "1. The assumption that \"... forward process of the diffusion model ... intuitively aligns with the process of smoothing fine-grained data into a coarser-grained representation ...\" is not verified through theoretical analysis or empirical study. \n\n2. Experimental settings for the time series forecasting task are unclear. For example, how the context interval and prediction interval are constructed in a time series dataset? Do you utilize a sliding window to roll the time series to build the context and predict intervals? Do the consecutive context/predict intervals overlap or not?\n\n3. More experimental results are expected. For instance, the authors only evaluate the time series forecasting methods under one length setting in each dataset, like the setting of context-24-predict-24 is utilized in Solar, Electricity, Traffic, Taxi, and KDD-Cup datasets.\n\n4. The reproductivity of this work is a concern.\n\n\n\\\\======================\\\\\n\nAfter rebuttal.\n\nMost of my concerns are clarified."
                },
                "questions": {
                    "value": "According to the inference procedure, the proposed time series forecasting approach can predict one future horizon time step at a time. How to apply this predictive approach for the long-term time series forecasting task effectively?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3041/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3041/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3041/Reviewer_FBCv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3041/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756122886,
            "cdate": 1698756122886,
            "tmdate": 1700794985073,
            "mdate": 1700794985073,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uz4T1tHaNX",
                "forum": "CZiY6OLktd",
                "replyto": "5xrmqQHlQT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FBCv (1/3)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your thorough review and valuable feedback. Enclosed are our responses to your insightful questions and concerns.\n\n**Q1. According to the inference procedure, the proposed time series forecasting approach can predict one future horizon time step at a time. How to apply this predictive approach for the long-term time series forecasting task effectively?**\n\nWe are grateful to the reviewer for the insights. \n\n- In our Temporal Process Module, we utilize the RNN as the backbone model to encode the temporal dependence sequentially. Therefore, predicting m prediction length involves taking n-to-1 (n timesteps predicts one timestep) for m times autoregressively, where each inference step incorporates previous predictions into the context. Based on the extra experiment concerning long-term forecasting, our method experiences only a moderate decrease in performance while extending prediction length.\n- It is worth noting that our Temporal Process Module is adaptable and can accommodate other encoders capable of handling variable-length time steps. This includes, but is not limited to, autoregressive RNNs in an n-to-1 manner or non-autoregressive methods of an n-to-m manner such as transformers.\n- In certain scenarios, a Transformer-based TPM might be a more efficient solution, taking m timesteps predictions with one step. However, Transformers have limitations regarding the maximum length of the time series, which means the context length and prediction length cannot be increased infinitely. Additionally, for high-dimensional time series data (such as Wikipedia dataset with 2000 dimensions), the attention mechanism might suffer from huge training and inference overhead.\n\nSince the primary focus of this paper is on multi-granularity guidance, we did not test the eligibility of transformer variant for TPM and we will take it as an important future work.\n\n**W1. The assumption that \"... forward process of the diffusion model ... intuitively aligns with the process of smoothing fine-grained data into a coarser-grained representation ...\" is not verified through theoretical analysis or empirical study.**\n\nThanks to the reviewer for the valuable insights. We would like to point out that the results depicted in Figure 3 of our manuscript provide empirical support for our assumption. Please note that, in Figure 3, the x-axis denotes the denoising diffusion steps, and the y-axis signifies the $\\\\text{CRPS}\\_{\\\\text{sum}}$ values. \n\nFigure 3 illustrated an investigation into the effect of share ratio, where we evaluate the performance of MG-TSD using various share ratios across different coarse granularities. \n\nIn Figure 3(a)-(d), the dashed blue curve in each plot represent $\\\\text{CRPS}\\_{\\\\text{sum}}$ values between the coarse-grained targets and 1-hour samples come from 1-gran (finest-gran) model at each intermediate denoising step; each point on the orange polylines represents the $\\\\text{CRPS}\\_{\\\\text{sum}}$ value of 1-hour predictions by 2-gran MG-TSD models (where one coarse granularity is utilized to guide the learning process for the finest-grained data), with different share ratios ranging from [0.2, 0.4, 0.6, 0.8, 1.0], and the lowest point of the line segment can be used to characterize the most suitable share ratio for the corresponding granularity.\n\nThe four subplots from (a) to (d) illustrate a gradual smoothing transformation of the distribution of increasingly coarser targets. A key observation is that from the left to the right panel, the distribution of coarser targets gradually aligns with the distribution of intermediate samples at larger diffusion steps. More specifically, as granularity transitions from fine to coarse (4h\u21926h\u219212h\u219224h), the denoising steps at which the distribution most resembles the coarse-grained targets increase (approximately at steps 20\u219240\u219260\u219260). This comparison underscores the similarity between the diffusion process and the smoothing process from the finest-grained data to coarse-grained data, both of which involve a gradual loss of finer characteristics from the finest-grained data through a smooth transformation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330213128,
                "cdate": 1700330213128,
                "tmdate": 1700330213128,
                "mdate": 1700330213128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uak7PYCWUe",
                "forum": "CZiY6OLktd",
                "replyto": "5xrmqQHlQT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FBCv (2/3)"
                    },
                    "comment": {
                        "value": "**W2. Experimental settings for the time series forecasting task are unclear.**\n\nThanks the reviewer for the question. To clarify, we generate coarse-grained data from the finest-grained time series data over the entire timeline. We rely on the GluonTS[1] library for data splitting and creating the training and testing instances. The GluonTS library is widely used in time series forecasting. Further explanations are provided below:\n\n- Generation of the coarse-grained data: Section 3.1 has the details to generate multi-granularity data. A sliding window with a pre-defined size $s^g$ for granularity $g$ is applied to the finest-grained data across the entire timeline, where $g=1,2,...,G$. These\u00a0**sliding windows**\u00a0are intentionally\u00a0**non-overlapping**. Within each window, we smooth the finest-grained data by averaging and replicate the average $s^g$ times to align with the timeline $[1, T]$.\n- Dataset splitting: All the datasets we used in the benchmark experiments are public and available in GluonTS[1]. The library has these datasets pre-split into training and testing sets, and we follow this splitting method to obtain our training and testing datasets.\n- Creation of training and testing instances: we randomly sample the context window, followed by the prediction window, from the complete training data. This process can be viewed as applying a moving window to auto-regressively roll through the entire timeline, with consecutive time intervals\u00a0**overlapping.**\u00a0Furthermore, for different datasets, the context length and prediction length vary, such as 24-hour-24-hour for Electricity, and 30-Day-30-Day for Wikipedia, as detailed in Appendix C.1.\n\n**W3. More experimental results about different prediction lengths are expected.**\n\nThanks the reviewer for insightful suggestion. We have conducted more experiments to test the performance of MG-TSD method with different prediction length. \n\n**Experiment setting**: In the current experiment, we assess the long-term prediction performance of MG-TSD by comparing it with the baseline TimeGrad, chosen for its competitive performance in the main experiment (Table 1) and its shared characteristic of forecasting in an autoregressive manner.  For the solar and electricity datasets (with the frequency of 1 hour), we set the context length to 24 hours and evaluate the performance of methods with prediction lengths of 24 hours, 48 hours, 96 hours and 144 hours. The average $\\\\text{CRPS}\\_{\\\\text{sum}}$, $\\\\text{NRMSE}\\_{\\\\text{sum}}$, and $\\\\text{NMAE}\\_{\\\\text{sum}}$ metrics are computed for both MG-TSD and the baseline over 10 independent runs, with error bars indicating the corresponding standard deviations. (We would include additional baselines in the plots later to strengthen our conclusions for updated paper. )\n\n**Experiment results and findings**: The preliminary results indicate that MG-TSD performs well for long-time forecasting. The results indicate that as the prediction length increases, the performance of our proposed method stays robust, exhibiting no sudden decline. Furthermore, our method consistently outperforms the competitive baseline. This performance advantage is anticipated to persist in future trends, with no indication of convergence between the approaches. \n\n**Table G: Results for Solar Dataset**\n\n|Prediction Length|Method|$\\\\text{CRPS}\\_{\\\\text{sum}}$|$\\\\text{NRMSE}\\_{\\\\text{sum}}$|$\\\\text{NMAE}\\_{\\\\text{sum}}$|\n|-|-|-|-|-|\n|24h|TimeGrad|0.3335\u00b10.0653|0.6952\u00b10.0644|0.3637\u00b10.0665|\n|48h|TimeGrad|0.3615\u00b10.0298|0.7392\u00b10.0566|0.4070\u00b10.0298|\n|96h|TimeGrad|0.3737\u00b10.0213|0.7905\u00b10.0481|0.4113\u00b10.0238|\n|144h|TimeGrad|0.4301\u00b10.0140|0.9285\u00b10.0219|0.4768\u00b10.0109|\n|24h|MG-TSD|0.3178\u00b10.0342|0.6591\u00b10.0503|0.3480\u00b10.0356|\n|48h|MG-TSD|0.3401\u00b10.0271|0.7234\u00b10.0398|0.3862\u00b10.0217|\n|96h|MG-TSD|0.3500\u00b10.0270|0.7395\u00b10.0439|0.3909\u00b10.0264|\n|144h|MG-TSD|0.3659\u00b10.0311|0.8179\u00b10.0600|0.4226\u00b10.0292|\n\n**Table H: Results for Electricity Dataset**\n\n|Prediction Length|Method|$\\\\text{CRPS}\\_{\\\\text{sum}}$|$\\\\text{NRMSE}\\_{\\\\text{sum}}$|$\\\\text{NMAE}\\_{\\\\text{sum}}$|\n|-|-|-|-|-|\n|24h|TimeGrad|0.0205\u00b10.0033|0.0348\u00b10.0057|0.0266\u00b10.0049|\n|48h|TimeGrad|0.0264\u00b10.0020|0.0474\u00b10.0034|0.0343\u00b10.0026|\n|96h|TimeGrad|0.0304\u00b10.0048|0.0558\u00b10.0092|0.0407\u00b10.0065|\n|144h|TimeGrad|0.0532\u00b10.0090|0.0953\u00b10.0153|0.0674\u00b10.0096|\n|24h|MG-TSD|0.0174\u00b10.0042|0.0296\u00b10.0086|0.0226\u00b10.0071|\n|48h|MG-TSD|0.0212\u00b10.0028|0.0334\u00b10.0045|0.0279\u00b10.0042|\n|96h|MG-TSD|0.0224\u00b10.0069|0.0376\u00b10.0103|0.0286\u00b10.0086|\n|144h|MG-TSD|0.0341\u00b10.0091|0.0609\u00b10.0142|0.0473\u00b10.0116|\n\n**Action taken:** We have included the experiments and results in Appendix B.2. Figure 5 in the appendix visualizes the results in Table G and Table H provided here, providing a clearer presentation of our conclusions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330267279,
                "cdate": 1700330267279,
                "tmdate": 1700330267279,
                "mdate": 1700330267279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eB3Q1MYlpt",
                "forum": "CZiY6OLktd",
                "replyto": "5xrmqQHlQT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FBCv (3/3)"
                    },
                    "comment": {
                        "value": "**W4. Reproducibility**\n\nWe plan to release our code soon. We are intensively working on cleaning up our code to ensure it is well-documented, thoroughly tested, and user-friendly. Additionally, to guarantee the reproducibility of our experiments, we performed 10 independent runs for each setting and reported both the average values and the standard deviation (std).\n\nReferences:\n\n[1]Alexandrov A, Benidis K, Bohlke-Schneider M, et al. Gluonts: Probabilistic and neural time series modeling in python[J]. The Journal of Machine Learning Research, 2020, 21(1): 4629-4634."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330299729,
                "cdate": 1700330299729,
                "tmdate": 1700330299729,
                "mdate": 1700330299729,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RvogO7Lpub",
                "forum": "CZiY6OLktd",
                "replyto": "uz4T1tHaNX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Reviewer_FBCv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Reviewer_FBCv"
                ],
                "content": {
                    "title": {
                        "value": "Question on response to W1"
                    },
                    "comment": {
                        "value": "I am confused that, as the four subplots from (a) to (d) in Figure 3 suggested, the denoising diffusion steps at which the distribution most resembles the coarse-grained targets decrease, i.e., 80 $\\rightarrow$ 60 $\\rightarrow$ 40 $\\rightarrow$ 40, which is opposite to your statement. \nPlease correct me if I missed any key points."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640367325,
                "cdate": 1700640367325,
                "tmdate": 1700640367325,
                "mdate": 1700640367325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wEhjfgFBxg",
                "forum": "CZiY6OLktd",
                "replyto": "VPItzAv8xg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Reviewer_FBCv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Reviewer_FBCv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your clarification.\n\nAgain, I am still not well convinced that, regarding a time series, the connection between the diffusion process (forward process) and the smoothing process does hold. Could you provide more convincing evidence? \n\nFrom the point of view of time series decomposition, a time series $X$ can be approximated as $X = T + S + \\epsilon$ where $T$ and $S$ denote trend and seasonality, respectively. And $\\epsilon$ corresponds to the noisy part, which can also be deemed as details of $X$. As the diffusion steps increase, $X$ will be corrupted by $X^{\\prime} = T + S + \\epsilon + \\epsilon^{\\prime}$, how to prove that $X^{\\prime}$ represents the coarser version of $X$?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671241490,
                "cdate": 1700671241490,
                "tmdate": 1700671241490,
                "mdate": 1700671241490,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CTkQ6yHjXJ",
            "forum": "CZiY6OLktd",
            "replyto": "CZiY6OLktd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3041/Reviewer_Zqn4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3041/Reviewer_Zqn4"
            ],
            "content": {
                "summary": {
                    "value": "Diffusion probabilistic models which can generate high-fidelity samples reserve stochastic nature. This characteristic makes it less effective in probabilistic time series forecasting tasks. To improve the efficiency of Diffusion probabilistic models, this paper introduces a novel MG-TSD model with an innovatively designed multi-granularity guidance loss function that efficiently guides the diffusion learning process. To effectively utilize coarse-grained data across various granularity levels, this paper propose a concise implementation method. What\u2019s more, this approach does not rely on additional external data, making it versatile and applicable across various domains. Extensive experiments conducted on real-world datasets demonstrate the superiority of the proposed model, achieving the best performance compared to the state-of-the-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. In the context of the time series forecasting, where fixed observations exclusively serve as objectives, diffusion probabilistic models would result in forecasting instability and inferior prediction performance. Unlike constraining the intermediate states during the sampling process, this paper creatively leverages multiple granularity levels within data to guide the learning process of diffusion models.\n2. This paper provides a series of ablation experiments to test the effect of share ratio and the number of granularities, it evaluate the performance of MG-TSD using various share ratios across different coarse granularities and the number of granularities.\n3. Clarity:  The paper offers a clear presentation to the model architecture with a good explanation of the methodology."
                },
                "weaknesses": {
                    "value": "1. This paper lacks an evaluation of the time complexity of the model. It may be more sufficient to add experiments that consume memory and time.\n2. MG-TSD is consisting of Multi-granularity Data Generator, Temporal Process Module (TPM), and Guided Diffusion Process Module. However, the ablation experiment part of this paper lacks performance testing of each module, especially Multi-granularity"
                },
                "questions": {
                    "value": "1. In Equation 7 of Section 3.2.1, is the distribution of \u2208 consistent with the distribution of x_N? Does the distribution of \u2208 obey the normal distribution? The distribution and meaning of \u2208 are not pointed out.\n2. Compared with the existing diffusion probabilistic models, does the MG-TSD model framework differs only in the Guided Diffusion Process Module ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics review needed."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3041/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847207274,
            "cdate": 1698847207274,
            "tmdate": 1699636249423,
            "mdate": 1699636249423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uMcT4TYTxe",
                "forum": "CZiY6OLktd",
                "replyto": "CTkQ6yHjXJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zqn4 (1/2)"
                    },
                    "comment": {
                        "value": "We are grateful for your constructive comments. Below we address each question and concern.\n\n**Q1: In Equation 7 of Section 3.2.1, is the distribution of $\\\\epsilon$ consistent with the distribution of $x\\_N$? Does the distribution of $\\\\epsilon$ obey the normal distribution? The distribution and meaning of $\\\\epsilon$ are not pointed out.**\n\nThank the reviewer for bringing attention to this. The $\\\\epsilon$ in equation (7) adheres to the standard normal distribution, consistent with the backbone denoising diffusion models. The dimension of the  $\\\\epsilon$ is the same with $x^{g}$.\n\n**Action taken**: We have incorporated this clarification into our manuscript.\n\n**Q2: Compared with the existing diffusion probabilistic models, does the MG-TSD model framework differs only in the Guided Diffusion Process Module?**\n\nThanks for this question. We would like to further clarify on our module design.\n\n- The core difference between MG-TSD and previous work principally lies in the approach of incorporating information from data. We creatively propose a multi-granularity guidance approach to naturally exploit intrinsic coarse-to-grain features within data to stabilize the samples from the diffusion model. This is facilitated by the Guided Diffusion Process Module.\n- The multi-granularity data generator is unique in our work for the preprocessing of original data into multiple alternatives with different granularities. This module collaborates synergistically with the Guided Diffusion Process Module, enabling it to adapt to multi-granularity requirements.\n- Although the temporal process module is similar to previous work[1][2] in its function for extracting historical context, it was dedicatedly designed to adapt for the multi-granularity cases. To be more specific, temporal process module employs separate RNN submodule for each granularity, without parameters sharing across different granularities.\n\n**W1. Concern about the time complexity and memory of the model.**\n\nExperiments have been conducted to evaluate the time and memory usage of the MG-TSD model during training across various granularities. \n\n**Experiment setting:** These experiments were executed using a single A6000 card with 48G memory capacity. The Solar dataset was utilized in this context, with a batch size of 128, input size of 552, 100 diffusion steps, and 30 epochs.\n\n**Experiment results and findings:** As illustrated in the corresponding graph and table, there is a linear increase in memory consumption with an increase in granularity. A slight addition in training time is also observed. These findings are coherent with the architecture of our model. In particular, each additional granularity results in the introduction of an extra RNN in the Temporal Process Module and an increase in computation within the Guided Diffusion Process Module. As per theoretical expectations, these resource consumptions should exhibit linear growth. Moreover, it is pertinent to mention that an excessive increase in granularity may not notably boost the final prediction results, hence the granularity should be kept within a certain range. Therefore, the consumption of memory will not rise indefinitely.\n\n**Table F: Comparison of Time and Memory Consumption at Different Granularity Levels in MG-TSD Model Training**\n\n|Granularity|Memory(GB)|RunTime(Minute)|Relative Memory Occupancy|Relative Run Time|\n|-|-|-|-|-|\n|2|6.06|25.93|100%|100%|\n|3|13.45|27.48|222%|106%|\n|4|20.03|33.33|331%|129%|\n|5|29.09|33.52|480%|129%|\n\n\n**Summary:** In summary, while it is evident that an increase in granularity escalates the consumption of both time and memory, these increases are reasonable and fall within acceptable boundaries.\n\n**Action taken:** We have included the experiments and results in Appendix B.2. Figure 6 in the appendix visualizes the results in Table F provided here, providing a clearer presentation of our conclusions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330038747,
                "cdate": 1700330038747,
                "tmdate": 1700330038747,
                "mdate": 1700330038747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Anrcz90Gko",
                "forum": "CZiY6OLktd",
                "replyto": "CTkQ6yHjXJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zqn4 (2/2)"
                    },
                    "comment": {
                        "value": "**W2. Concern about the ablation study for the Multi-granularity Data Generator module and TPM module**\n\nThanks to the reviewer for the comment. \n\nWe would like to highlight that Table 3 in our manuscript serves as an ablation study of the Multi-granularity component in the MG-TSD model. The experiment, presented in Table 3, explores the effect of varying the number of granularity levels on the model's performance. Testing the performance of the model with an additional granularity level inherently assesses the effectiveness of all three modules. More specifically, the Multi-granularity Data Generator Module is executed to generate the data at a new granularity level. The Temporal Process Module (TPM) incorporates an additional RNN cell to encode the temporal pattern within the added granularity data, while the Guided Diffusion Process Module adapts the learning and generation process with the additional coarse-grained data. The results in Table 3 reveal that increasing the number of granularity levels typically improves the performance of the MG-TSD model. This finding confirms that the mechanism currently designed to generate coarse-grained data is effective in enhancing performance. Theoretically, the MG-TSD model can handle an unlimited increase in granularity levels. However, in practical scenarios, we observe that the marginal benefit tends to diminish as the number of granularity levels increases. Our findings suggest that employing four to five granularity levels typically suffices to achieve optimal performance.\n\nTo further clarify, the Multi-granularity Generator Module functions as a predefined data pre-processing step and is utilized to generate coarse-grained data at various granularity levels. It does not involve parameter optimization during the training stage. The Temporal Process Module serves as an encoder in the model, which captures and compresses the temporal dependencies in time series data up to a certain timestep. The RNN is adopted in our model as it presents a feasible and convenient method of implementation for this module.  The choice of RNN as the backbone encoder in TPM aligns with various previous works, including TimeGrad[1] and GP-copula[2]. These studies utilize RNN for modeling temporal dependencies, which attests to the effectiveness of RNN. Since the primary focus of this paper is on multi-granularity guidance, we did not conduct extra ablation studies on the designs of these components.\n\nReferences: \n\n[1] Rasul K, Seward C, Schuster I, et al. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting[C]//International Conference on Machine Learning. PMLR, 2021: 8857-8868.\n\n[2] Salinas D, Bohlke-Schneider M, Callot L, et al. High-dimensional multivariate forecasting with low-rank Gaussian copula processes[J]. Advances in neural information processing systems, 2019, 32."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330084978,
                "cdate": 1700330084978,
                "tmdate": 1700330084978,
                "mdate": 1700330084978,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vLrkBmOWU5",
            "forum": "CZiY6OLktd",
            "replyto": "CZiY6OLktd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3041/Reviewer_Zq3E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3041/Reviewer_Zq3E"
            ],
            "content": {
                "summary": {
                    "value": "This paper employs the diffusion model for time series forecasting and introduces the Multi-Granularity Time Series Diffusion model, which comprises three key components: 1). The Multi-Granularity Data Generator, responsible for generating multi-granularity data. 2). The Temporal Process Module, which utilizes an RNN architecture to capture temporal dynamics. 3). The Guided Diffusion Process Module is aimed at generating stable time-series predictions. This model leverages various levels of granularity within the data to guide the forward process of the diffusion model. Additionally, the paper designs a multi-granularity guidance loss function and explores optimal configurations for different granularity levels, proposing a practical rule of thumb. Extensive experiments are conducted to showcase its precision and effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The research problem addressed in this study is of paramount significance and holds great interest. Accurate time prediction has broad applications, including tasks like anomaly detection and energy consumption control.\n2. The paper introduces a novel and intriguing approach by linking various granularities in the time series with the forward process in the diffusion model.\n3. The paper is excellently written and presented in a clear and comprehensible manner."
                },
                "weaknesses": {
                    "value": "1. Some related works can be further discussed.\n2. There is only one metric in the main experiment, which is not enough.\n3. Compared with the baseline, the performance improvement is not obvious.\n4. The use of the RNN architecture requires further explanation"
                },
                "questions": {
                    "value": "1.\tThere have been some similar works, such as TimeDiff[1] and D3VAE[2], which also applies the diffusion model. What are the technical advantages of these studies?\n2.\tThe paper designed MG-TSD based on the diffusion model. Why is it not compared with the diffusion-based models in the baseline? Besides, There are some newer Transformer-based models, such as PatchTST[3], and Autoformer[4] should be compared in your experiments.\n- [1]Shen L, Kwok J. Non-autoregressive Conditional Diffusion Models for Time Series Prediction[J]. arXiv preprint arXiv:2306.05043, 2023.\n- [2] Li Y, Lu X, Wang Y, et al. Generative time series forecasting with diffusion, denoise, and disentanglement[J]. Advances in Neural Information Processing Systems, 2022, 35: 23009-23022.\n- [3] Nie Y, Nguyen N H, Sinthong P, et al. A Time Series is Worth 64 Words: Long-term Forecasting with Transformers[C]//The Eleventh International Conference on Learning Representations. 2022.\n- [4] Wu H, Xu J, Wang J, et al. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting[J]. Advances in Neural Information Processing Systems, 2021, 34: 22419-22430."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3041/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698947802393,
            "cdate": 1698947802393,
            "tmdate": 1699636249364,
            "mdate": 1699636249364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8N1dzMszXh",
                "forum": "CZiY6OLktd",
                "replyto": "vLrkBmOWU5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zq3E (1/3)"
                    },
                    "comment": {
                        "value": "Thank you very much for your careful review and constructive suggestions! Please find our response to your questions and concerns.\n\n**Q1&W1. Some related works can be further discussed. There have been some similar works, such as TimeDiff[1] and D3VAE[2], which also apply the diffusion model. What are the technical advantages of these studies?**\n\nDiffusion-based methods that use generations from conditional distribution as predictions are typically from probabilistic methods. In contrast, sequence-based (including transformer-based) models are mostly deterministic methods.\n\nAutoformer and PatchTST are deterministic time-series forecasting models based on dedicated designed Transformers, which are different from MG-TSD in the scope of prediction. D3VAE and TimeDiff are probabilistic time-series forecasting methods, which are closer to the scope of our work.\n\nBoth D3VAE and TimeDiff involve diffusion probabilistic models. D3VAE utilizes only the forward diffusion process and the prediction stage is majorly taken over by a VAE architecture, which is fundamentally different from MG-TSD in the way of generating forecasts.\n\nIn contrast, both TimeDiff and MG-TSD utilize a conditional denoising process over time intervals to perform forecasting. The major difference between TimeDiff and MG-TSD lies in the way of incorporating information within data, TimeDiff is trained with the mixed up of hidden contexts and future ground truths as sample conditionings, while MG-TSD utilizes multi-granularity guidance, leveraging intrinsic coarse-to-grain features within data.\n\nThe technical advantages of diffusion-based time series forecasting are summarized as below:\n\n- **Uncertainty quantification.** A primary advantage of probabilistic models is their capability to accurately capture data distribution instead of just point estimates. This allows for the convenient construction of prediction intervals using multiple outputs from the diffusion model. By knowing the data distribution at a specific timestamp, one can quantify the prediction uncertainty and provide more reliable forecasts. It also enables the convenient evaluation of the probability of extreme events occurring. For instance, in the wind power sector, reliable forecasts are essential. An unexpected extreme event causing a wind farm to shut down can wipe out months of revenue. This underscores the importance of probabilistic forecasting, which considers both the expected power output and the uncertainty of the forecast, thereby aiding in the minimization of such risks [1].\n\n- **The capability to model arbitrary distributions without parametric assumptions.** This capacity of diffusion models to characterize distributions is also applicable to modeling the distribution of time series. A flaw in other methods of distribution modeling is that they are strictly constrained by the functional structure of their target distributions. For example, the previous choice, Transformer-MAF[2], models multivariate time series with an autoregressive deep learning model, in which the data distribution is expressed by a conditional normalizing flow. Diffusion-based methods, conversely, can offer a less restrictive solution, as indicated in reference [3].\n\n**Action Taken**: We have included additional experimental results with respect to these mentioned works, on datasets of different dimensions and predicting lengths. Overall, the performance of MG-TSD is superior. Please refer to the reply to **Q2&W2** below for numerical results. We have also included these results in the updated manuscript.\n\n**Q2&W2. The paper designed MG-TSD based on the diffusion model. Why is it not compared with the diffusion-based models in the baseline? Besides, there are some newer Transformer-based models, such as PatchTST[6], and Autoformer[7] should be compared in your experiments. There is only one metric in the main experiment, which is not enough.**\n\nThanks for suggesting additional references. In our original benchmark experiment, the TimeGrad method included in our baseline is a diffusion-based model. To ensure comprehensive comparisons, we have now incorporated all the mentioned works, TimeDiff[4], D3VAE[5],  PatchTST[6], and AutoForemer[7], into our baselines.\n\nFurthermore, we have broadened our evaluation metrics to include $\\\\text{NMAE}\\_{\\\\text{sum}}$ (Normalized MAE) and $\\\\text{NRMSE}\\_{\\\\text{sum}}$ (Normalized RMSE). Detailed results for these additional metrics can now be found in the updated appendix, particularly in Tables 4 and 5.\n\nHere, we attach the benchmark experiment results of three metrics $\\\\text{CRPS}\\_{\\\\text{sum}}$, $\\\\text{NMAE}\\_{\\\\text{sum}}$, and $\\\\text{NRMSE}\\_{\\\\text{sum}}$ below. These additional experiments validate that our method consistently delivers state-of-the-art performance."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329823998,
                "cdate": 1700329823998,
                "tmdate": 1700329823998,
                "mdate": 1700329823998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3KnBS46VTm",
                "forum": "CZiY6OLktd",
                "replyto": "vLrkBmOWU5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zq3E (2/3)"
                    },
                    "comment": {
                        "value": "**Table A: Comparison of $\\\\text{CRPS}\\_{\\\\text{sum}}$ (smaller is better) of models on six real-world datasets. The\nreported mean and standard error are obtained from 10 re-training and evaluation independent runs.**\n|Method|Solar|Electricity|Traffic|KDD-cup|Taxi|Wikipedia|Avg Rank|\n|-|-|-|-|-|-|-|-|\n|Vec-LSTM ind-scaling|0.4825\u00b10.0027|0.0949\u00b10.0175|0.0915\u00b10.0197|0.3560\u00b10.1667|0.4794\u00b10.0343|0.1254\u00b10.0174|8.8|\n|GP-Scaling|0.3802\u00b10.0052|0.0499\u00b10.0031|0.0753\u00b10.0152|0.2983\u00b10.0448|0.2265\u00b10.0210|0.1351\u00b10.0612|7.3|\n|GP-Copula|0.3612\u00b10.0035|0.0287\u00b10.0005|0.0618\u00b10.0018|0.3157\u00b10.0462|0.1894\u00b10.0087|0.0669\u00b10.0009|5.7|\n|LSTM-MAF|0.3427\u00b10.0082|0.0312\u00b10.0046|0.0526\u00b10.0021|0.2919\u00b10.1486|0.2295\u00b10.0082|0.0763\u00b10.0051|5.3|\n|Transformer-MAF|0.3532\u00b10.0053|0.0272\u00b10.0017|0.0499\u00b10.0011|0.2951\u00b10.0504|0.1531\u00b10.0038|0.0644\u00b10.0037|4.0|\n|TimeGrad|0.3335\u00b10.0653|0.0232\u00b10.0035|0.0414\u00b10.0112|0.2902\u00b10.2178|0.1255\u00b10.0207|0.0555\u00b10.0088|2.2|\n|D3VAE|0.4449\u00b10.0375|0.1424\u00b10.0883|0.3967\u00b10.1165|0.4861\u00b10.0517|0.5909\u00b10.4645|1.9950\u00b11.9874|10.0|\n|TimeDiff|1.3323\u00b10.0305|0.3505\u00b10.0075|0.4778\u00b10.0058|0.3622\u00b10.0127|0.4517\u00b10.0101|0.1140\u00b10.0105|9.7|\n|TACTiS|0.4209\u00b10.0330|0.0259\u00b10.0019|0.1093\u00b10.0076|0.5406\u00b10.1584|0.2070\u00b10.0159|-|8.2|\n|MG-Input|0.3239\u00b10.0427|0.0238\u00b10.0035|0.0658\u00b10.0065|0.2977\u00b10.1163|0.1592\u00b10.0087|0.0567\u00b10.0091|3.8|\n|MG-TSD|**0.3081\u00b10.0099**|**0.0149\u00b10.0017**|**0.0323\u00b10.0125**|**0.1837\u00b10.0865**|**0.1159\u00b10.0132**|**0.0529\u00b10.0054**|**1.0**|\n\n**Table B: Comparison of $\\\\text{NRMSE}\\_{\\\\text{sum}}$ (smaller is better) of models on six real-world datasets. The\nreported mean and standard error are obtained from 10 re-training and evaluation independent runs.**\n|Method|Solar|Electricity|Traffic|KDD-cup|Taxi|Wikipedia|Avg Rank|\n|-|-|-|-|-|-|-|-|\n|Vec-LSTM ind-scaling|0.9952\u00b10.0077|0.1439\u00b10.0228|0.1451\u00b10.0248|0.4461\u00b10.1833|0.6398\u00b10.0390|0.1618\u00b10.0162|7.2|\n|GP-Scaling|0.9004\u00b10.0095|0.0811\u00b10.0062|0.1469\u00b10.0181|0.3445\u00b10.0621|0.3598\u00b10.0285|0.1710\u00b10.1006|6.3|\n|GP-Copula|0.8279\u00b10.0053|0.0512\u00b10.0009|0.1282\u00b10.0033|**0.2605\u00b10.0227**|0.3125\u00b10.0113|0.0930\u00b10.0076|4.0|\n|Autoformer|0.7046\u00b10.0000|0.0475\u00b10.0000|0.0951\u00b10.0000|0.8984\u00b10.0000|0.3498\u00b10.0000|0.1052\u00b10.0000|5.2|\n|PatchTST|0.7270\u00b10.0000|0.0474\u00b10.0000|0.1897\u00b10.0000|0.5137\u00b10.0000|0.3690\u00b10.0000|0.0915\u00b10.0000|5.3|\n|D3VAE|0.7472\u00b10.0508|0.1640\u00b10.0928|0.4722\u00b10.1197|0.5628\u00b10.0419|0.7624\u00b10.5598|2.2094\u00b12.1646|8.3|\n|TimeDiff|1.5985\u00b10.0359|0.3714\u00b10.0073|0.5520\u00b10.0087|0.4955\u00b10.0147|0.5479\u00b10.0084|0.1412\u00b10.0099|8.3|\n|TimeGrad|0.6953\u00b10.0845|0.0348\u00b10.0057|0.0653\u00b10.0244|0.4092\u00b10.1332|0.2365\u00b10.0386|0.0870\u00b10.0106|2.3|\n|TACTiS|0.8532\u00b10.0851|0.0427\u00b10.0023|0.2270\u00b10.0159|0.6513\u00b10.1767|0.3387\u00b10.0097|-|6.8|\n|MG-TSD|**0.6178\u00b10.0418**|**0.0241\u00b10.0030**|**0.0563\u00b10.0230**|0.3001\u00b10.0997|**0.2334\u00b10.0313**|**0.0810\u00b10.0057**|**1.2**|\n\n**Table C: Comparison of $\\\\text{NMAE}\\_{\\\\text{sum}}$ (smaller is better) of models on six real-world datasets. The reported mean and standard error are obtained from 10 re-training and evaluation independent runs.**\n\n|Method|Solar|Electricity|Traffic|KDD-cup|Taxi|Wikipedia|Avg Rank|\n|-|-|-|-|-|-|-|-|\n|Vec-LSTM ind-scaling|0.5091\u00b10.0027|0.1261\u00b10.0211|0.1042\u00b10.0228|0.4193\u00b10.1902|0.4974\u00b10.0351|0.1416\u00b10.0180|7.3|\n|GP-Scaling|0.4945\u00b10.0065|0.0648\u00b10.0046|0.0975\u00b10.0163|0.2892\u00b10.0550|0.2867\u00b10.0264|0.1452\u00b10.1029|6.0|\n|GP-Copula|0.4302\u00b10.0046|0.0312\u00b10.0007|0.0769\u00b10.0022|**0.2140\u00b10.0124**|0.2390\u00b10.0098|0.0659\u00b10.0061|3.3|\n|Autoformer|0.6368\u00b10.0000|0.0388\u00b10.0000|0.0684\u00b10.0000|0.7658\u00b10.0000|0.2652\u00b10.0000|0.1239\u00b10.0000|6.5|\n|PatchTST|0.4351\u00b10.0000|0.0350\u00b10.0000|0.1219\u00b10.0000|0.4497\u00b10.0000|0.2887\u00b10.0000|0.0625\u00b10.0000|5.3|\n|D3VAE|0.4457\u00b10.0377|0.1434\u00b10.0892|0.3992\u00b10.1177|0.4874\u00b10.0520|0.6080\u00b10.5061|2.0151\u00b12.0005|8.5|\n|TimeDiff|1.3343\u00b10.0305|0.3519\u00b10.0075|0.4782\u00b10.0058|0.3630\u00b10.0127|0.4521\u00b10.0102|0.1146\u00b10.0106|8.0|\n|TimeGrad|0.3694\u00b10.0400|0.0266\u00b10.0049|0.0410\u00b10.0089|0.3614\u00b10.1334|0.1365\u00b10.0193|0.0631\u00b10.0080|2.5|\n|TACTiS|0.4448\u00b10.0313|0.0310\u00b10.0015|0.1352\u00b10.0159|0.6078\u00b10.1718|0.2244\u00b10.0036|-|6.3|\n|MG-TSD|**0.3347\u00b10.0220**|**0.0178\u00b10.0018**|**0.0370\u00b10.0140**|0.2463\u00b10.0865|**0.1300\u00b10.0150**|**0.0601\u00b10.0057**|**1.2**|\n\n**W3. Concerns about the significance of the model\u2019s improvements compared to baselines.**\n\nFor clarity, the original Table 1 results illustrate the MG-TSD model's performance with two granularities. Our results in Table 3 suggests employing more granularity levels could lead to further performance improvements. We have updated results in Tables 1, 4 and 5 to better reflect the MG-TSD model's optimal performance using multiple granularities."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329883864,
                "cdate": 1700329883864,
                "tmdate": 1700329883864,
                "mdate": 1700329883864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O5KJwH020k",
                "forum": "CZiY6OLktd",
                "replyto": "vLrkBmOWU5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3041/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zq3E (3/3)"
                    },
                    "comment": {
                        "value": "We provide more statistics summaries to evaluate the performance improvement.  We have recorded the rankings of different methods based on their performance on various datasets. The results are shown in table below.\n\n**Table D: Performance ranking of methods across datasets**\n|Method|$\\\\textbf{CRPS}\\_{\\\\textbf{sum}}$ Avg Rank|$\\\\textbf{NRMSE}\\_{\\\\textbf{sum}}$ Avg Rank|$\\\\textbf{NMAE}\\_{\\\\textbf{sum}}$ Avg Rank|\n|-|-|-|-|\n|Vec-LSTM ind-scaling|8.8|7.2|7.3|\n|GP-Scaling|7.3|6.3|6.0|\n|GP-Copula|5.7|4.0|3.3|\n|Autoformer|5.3|5.2|6.5|\n|PatchTST|4.0|5.3|5.3|\n|D3VAE|2.2|8.3|8.5|\n|TimeDiff|10.0|8.3|8.0|\n|TimeGrad|9.7|2.3|2.5|\n|TACTiS|8.2|6.8|6.3|\n|MG-TSD|1.0|1.2|1.2|\n\nFrom the results, our method achieved the best rank among all baselines on six datasets in terms of the $\\\\text{CRPS}\\_{\\\\text{sum}}$.  As for the $\\\\text{NRMSE}\\_{\\\\text{sum}}$ and $\\\\text{NMAE}\\_{\\\\text{sum}}$ metrics, we achieved the best performance on all datasets, except for KDD-cup dataset where the GP-Copular method was slightly better.\n\nAdditionally, we have recorded the percentage improvement of the MG-TSD method relative to the top-performing baseline in the table below for $\\\\text{CRPS}\\_{\\\\text{sum}}$.\n\n**Table E: Relative improvement of MG-TSD method over top-performing baseline**\n|Metric|Solar|Electricity|Traffic|KDD-cup|Taxi|Wikipedia|\n|-|-|-|-|-|-|-|\n|$\\\\text{CRPS}\\_{\\\\text{sum}}$|7.6%|35.8%|22.0%|36.7%|7.6%|4.7%|\n\nOn the six datasets, we achieved $\\\\text{CRPS}\\_{\\\\text{sum}}$ improvements, ranging from 4.7% to 35.8%. The modest improvements were achieved on the Wikipedia dataset. This might be due to its high-dimensional nature. The improvement of our model is not trivial, given the challenges posed by these datasets. \n\n**Action Taken**: In Tables 1, 4, and 5, we have updated the results to reflect the best performance of the MG-TSD model with multiple granularities, rather than just two. \n\n**W4. The use of the RNN architecture requires further explanation.**\n\nThanks for the question. To clarify, the Temporal Process Module serves as an encoder in the model, which captures and compresses the temporal dependencies in time series data up to a certain timestep. The Recurrent Neural Network (RNN) is adopted in our model as it presents a feasible and convenient method of implementation for this module. It is worth noting that our Temporal Process Module is adaptable and can accommodate other encoders capable of handling variable-length time steps. This includes, but is not limited to, autoregressive RNNs in an n-to-1 manner or transformers in an n-to-m manner.\n\nFurthermore, the choice of RNN as the backbone encoder in TPM aligns with various previous works, including TimeGrad[8] and GP-copula[9]. These studies utilize RNN for modeling temporal dependencies, which attests to the effectiveness of RNN. Notably, differing from these existing works, which typically use a single RNN for single-granularity data, we employ multiple RNNs in the model to leverage the pattern and temporal information in time series data at various granularity levels. These RNNs operate without parameter sharing and are trained simultaneously with the Guided Diffusion Process Module.\n\n\nReferences: \n\n[1]Bjerreg\u00e5rd M B, M\u00f8ller J K, Madsen H. An introduction to multivariate probabilistic forecast evaluation[J]. Energy and AI, 2021, 4: 100058.\n\n[2]Rasul K, Sheikh A S, Schuster I, et al. Multivariate probabilistic time series forecasting via conditioned normalizing flows[J]. arXiv preprint arXiv:2002.06103, 2020.\n\n[3]Lin L, Li Z, Li R, et al. Diffusion models for time series applications: A survey[J]. arXiv preprint arXiv:2305.00624, 2023.\n\n[4]Shen L, Kwok J. Non-autoregressive Conditional Diffusion Models for Time Series Prediction[J]. arXiv preprint arXiv:2306.05043, 2023.\n\n[5] Li Y, Lu X, Wang Y, et al. Generative time series forecasting with diffusion, denoise, and disentanglement[J]. Advances in Neural Information Processing Systems, 2022, 35: 23009-23022.\n\n[6] Nie Y, Nguyen N H, Sinthong P, et al. A Time Series is Worth 64 Words: Long-term Forecasting with Transformers[C]//The Eleventh International Conference on Learning Representations. 2022.\n\n[7] Wu H, Xu J, Wang J, et al. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting[J]. Advances in Neural Information Processing Systems, 2021, 34: 22419-22430.\n\n[8] Rasul K, Seward C, Schuster I, et al. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting[C]//International Conference on Machine Learning. PMLR, 2021: 8857-8868.\n\n[9] Salinas D, Bohlke-Schneider M, Callot L, et al. High-dimensional multivariate forecasting with low-rank gaussian copula processes[J]. Advances in neural information processing systems, 2019, 32."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3041/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329959442,
                "cdate": 1700329959442,
                "tmdate": 1700329959442,
                "mdate": 1700329959442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]