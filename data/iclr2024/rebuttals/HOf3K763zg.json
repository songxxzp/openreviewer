[
    {
        "title": "Beyond Differentiability: Neurosymbolic Learning with Black-Box Programs"
    },
    {
        "review": {
            "id": "6oYRLUAZsX",
            "forum": "HOf3K763zg",
            "replyto": "HOf3K763zg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7948/Reviewer_7YF3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7948/Reviewer_7YF3"
            ],
            "content": {
                "summary": {
                    "value": "The authors of this paper introduce a new approach to neurosymbolic learning called Infer-Sample-Estimate-Descend (ISED). Neurosymbolic learning aims to combine classical algorithms and deep learning. Unlike existing neurosymbolic frameworks, ISED allows for the use of black-box programs written in general-purpose languages, expanding its applicability. ISED is designed for algorithmic supervision, where a black-box program is applied to the output of a neural model, and the goal is to optimize the model parameters using end-to-end labels. ISED consists of four phases: Infer, Sample, Estimate, and Descend, where neural models predict distributions for inputs, samples are generated, the program is executed, probabilities are estimated, and the loss function is computed. ISED is evaluated on 30 benchmark tasks with black-box programs written in Python and achieves higher accuracy than end-to-end neural approaches, often outperforming a state-of-the-art neurosymbolic framework called Scallop."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Good performance and quality experiences with clear text ."
                },
                "weaknesses": {
                    "value": "Poor quantitative aspects of training, including memory requirements, training time for each model, and for each dataset."
                },
                "questions": {
                    "value": "What are the shortcomings related to the quantitative aspects of training, such as memory requirements, training time for each model, and for each dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698571103223,
            "cdate": 1698571103223,
            "tmdate": 1699636976690,
            "mdate": 1699636976690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8AC1O5txOW",
                "forum": "HOf3K763zg",
                "replyto": "6oYRLUAZsX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7948/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "There was a tradeoff between scalability and expressiveness in this work. ISED favors expressiveness, allowing the use of general-purpose languages such as Python, over scalability. Hence, we did not include memory requirements and training time for our benchmarks in this paper. For the sake of completeness, we will include these numbers in the final version, but we expect that the training time of ISED will generally be higher than the training time of Scallop."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578726543,
                "cdate": 1700578726543,
                "tmdate": 1700578726543,
                "mdate": 1700578726543,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SaW3wgC3js",
            "forum": "HOf3K763zg",
            "replyto": "HOf3K763zg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7948/Reviewer_9rTK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7948/Reviewer_9rTK"
            ],
            "content": {
                "summary": {
                    "value": "This article introduces a Neuro-Symbolic learning framework designed to utilize structured knowledge pertaining to the outputs of neural networks, expressed through black-box programs. The method proposed in this paper requires only that the reasoning part be capable of forward reasoning, without necessitating that the output of the reasoning part be differentiable with respect to the input. Authors validate the adaptability of the learning framework through extensive experimentation on a variety of synthetic and classic benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is easy to read, with Figures 1 and 2 providing a clear and intuitive illustration of the proposed method.\n2.\tThe paper conducts a comprehensive evaluation of the proposed method on synthetic and classic tasks such as MNIST Add, HWF and Sorting, demonstrating the method\u2019s adaptability."
                },
                "weaknesses": {
                    "value": "1.\tIn the abstract, the authors mention that \u2018existing general neuro-symbolic frameworks require that programs be written in differentiable logic programming languages\u2019. However, there already exist frameworks aiming at bridging machine learning and logic reasoning such as Semantic Loss [1], Abductive Learning [2] and NEUROLOG [3], which do not impose a requirement for differentiability and they use non-differentiable programs. The paper does not conduct comparisons with such methods.\n2.\tThe novelty of the proposed work needs to improve to meet the desired standards. The method proposed in this paper involves employing the REINFORCE algorithm to eliminate the requirement for differentiability in neural-symbolic system. However, this idea has already been introduced in the previous work [4]. Another critical component of the method, 'Estimate', fundamentally constitutes a sampling estimation of the well-known semantic loss [1], yet the paper does not provide reference to this work.\n\n[1] Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Van den Broeck. A Semantic Loss Function for Deep Learning with Symbolic Knowledge, ICML 2018.\n\n[2] Zhi-Hua Zhou. Abductive learning: Towards bridging machine learning and logical reasoning. Science China Information Sciences, 2019. \n\n[3] Tsamoura, Efthymia, Timothy Hospedales, and Loizos Michael. Neural-Symbolic Integration: A Compositional Perspective, AAAI 2021.\n\n[4] Cornelio Cristina, Jan Stuehmer, Shell Xu Hu, and Timothy Hospedales. Learning where and when to reason in neuro-symbolic inference, ICLR 2023.\n\n3.\tInconsistent styles: some NeurIPS references include page numbers, while others do not; some conference names have abbreviations, while others do not."
                },
                "questions": {
                    "value": "1.\tProlog is Turing-complete, possessing the same expressive capabilities as Python, and is also suitable for general-purpose use.\n2.\tThe neural network's initial performance is nearly equivalent to a random output, and methods based on sampling may encounter difficulties in capturing certain symbols. How you address this cold start issue?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7948/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7948/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7948/Reviewer_9rTK"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822235453,
            "cdate": 1698822235453,
            "tmdate": 1699636976574,
            "mdate": 1699636976574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A4L5S29PKM",
                "forum": "HOf3K763zg",
                "replyto": "SaW3wgC3js",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7948/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for pointing out these references. We will cite them in the final version of this paper. The ideas in [1] and [4] seem similar, but our main contribution is in integrating sampling-based estimation of loss in the neurosymbolic learning pipeline. We are trying to implement the exact algorithm in [4] as a baseline for experimental evaluation, and will include it in the final version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578711057,
                "cdate": 1700578711057,
                "tmdate": 1700578711057,
                "mdate": 1700578711057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t903covnto",
                "forum": "HOf3K763zg",
                "replyto": "A4L5S29PKM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7948/Reviewer_9rTK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7948/Reviewer_9rTK"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the author responses."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741906264,
                "cdate": 1700741906264,
                "tmdate": 1700741906264,
                "mdate": 1700741906264,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FRZXgT2cG1",
            "forum": "HOf3K763zg",
            "replyto": "HOf3K763zg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7948/Reviewer_Bw9k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7948/Reviewer_Bw9k"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a general Neuro-Symbolic solver approach using fixed black-box programs with the premise this allows anyone to write the program in any language as it removes the need for the program to be differentiable. This changes the neurosymbolic problem from program inference, to focus on parameter sampling and gradient propagation around the black box. The authors show that on three tasks  calculation, sorting and disease detection their method is able to outperform the baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- In principle, the approach is a general-purpose neuro-symbolic approach.\n- Despite the removal of gradients during the execution step, the performance is equivalent to the baseline\n- The idea of using user-defined programs in execution is interesting and novel.\n- The author's multi-dataset evaluation provides a broad context in different settings.  However, the leaf disease setting does not need to be a neuro-symbolic approach as shown by the simple program. A spatial reasoning test would probably have been a better choice."
                },
                "weaknesses": {
                    "value": "- The introduction of the black-box programs seems to constrain the problem largely. In the writing, it isn't clear if the programs are only used as supervision or explicitly used as the symbolic aspect. If it is the latter this greatly reduces the difficulty of the problem as the symbolic aspect is largely unneeded, as you could include an expert program to solve without needing symbolic, this is especially evident in the leaf disease test as there are a number of off the shelf expert models that could be applied without needing a threshold of % diseased. The approach would have been better argued by another approach, such as logical reasoning, ideally on Knowledge Graphs, which would be a compatible setting to prior methods, increasing the number of comparisons the authors could perform.\n- It isn't clear how this would scale to larger, more complex problems where the black-box program actually is complex. All examples are relatively trivial.\n- It isn't clear how they handle the gradients around the black box as they just state an optimizer solves this without any specificity."
                },
                "questions": {
                    "value": "- Greater explanation of whether the programs are learnt or if they only the inputs are being estimated.\n- Explanation of how gradients are propagated\n- Any prediction of how this would scale to complex problems"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833984385,
            "cdate": 1698833984385,
            "tmdate": 1699636976465,
            "mdate": 1699636976465,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i3USbqGG2r",
                "forum": "HOf3K763zg",
                "replyto": "FRZXgT2cG1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7948/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "ISED uses the programs for both supervision and the symbolic computation. In the leaf example, we calculate ground truth severity scores using the same program that we use as the symbolic aspect during training. To calculate the ground truth severity scores, we use human-labeled bounding boxes and corresponding areas calculated by SAM and pass them to the black-box program. Since all areas are labeled 1, the program simply returns the sum of the diseased areas in the bounding boxes. During training, we try to learn which of the segments returned by SAM contain diseased areas by using the same program as the symbolic component.\n\nWe agree that most examples are relatively simple; we do not have benchmarks as complex as say, Sudoku, a popular task in the neurosymbolic literature. We also acknowledge that similar to prior work in neurosymbolic learning, there is a tradeoff between scalability and expressiveness in our work. ISED favors expressiveness, allowing the use of general-purpose languages such as Python, over scalability. That being said, we did include some tasks in the Leetcode benchmark suite that have large input spaces, as shown in Table 3 of the paper. \n\nWe think it would be beneficial to add a benchmark where we can use the pure neural model to generate knowledge graphs, and the black box can just verify this reasoning. We will add a knowledge graph benchmark in the final version.\n\n\n**Q1:** The programs are not being learned, only the inputs are being estimated\n\n**Q2:** We do not compute gradients explicitly, but rather use a custom loss function that is made to reward certain inputs and penalize others.\n\n**Q3:** We still need to do more work to determine how ISED would scale to complex problems. We have found that simple tricks like increasing the sample count or moving from the add/mult to min/max computation in the loss can help solve problems that are more complex, such as HWF. However, we would need to find more complex benchmarks for testing ISED to answer this question."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578691483,
                "cdate": 1700578691483,
                "tmdate": 1700578691483,
                "mdate": 1700578691483,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xWtHZBDax3",
            "forum": "HOf3K763zg",
            "replyto": "HOf3K763zg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7948/Reviewer_4kqW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7948/Reviewer_4kqW"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a general framework for neurosymbolic learning with black-box programs (ISED). This framework does not need the differentiability of the program and uses a sampling-based method to approximate the gradient of the program execution. The evaluation results show that ISED is more accurate."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper uses lots of illustrations, which make the presentation clear."
                },
                "weaknesses": {
                    "value": "1. Limited Benchmark: The evaluation compares with Scallop and CNN. However, this work did not compare with another differentiating neurosymbolic program work: DeepProbLog, which limits the significance of the performance.\n2. Scalability. This work can only cover an input length of 7, concluding the multiplication and additional operation. However, a traditional neurosymbolic program\u2019s statement may not limit two these two operations."
                },
                "questions": {
                    "value": "1. How accurate the sampling-based method is? Is there any theoretical analysis about the gradient error from the estimation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7948/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698986256940,
            "cdate": 1698986256940,
            "tmdate": 1699636976363,
            "mdate": 1699636976363,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b3fZweCmTP",
                "forum": "HOf3K763zg",
                "replyto": "xWtHZBDax3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7948/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7948/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Prior work shows that Scallop achieves comparable or higher accuracy than DeepProbLog (DPL) on many simple arithmetic, comparison, and counting tasks involving MNSIT images ([1], Section 6.3, Figure 15 and Table 4). Because Scallop outperformed DPL on all these tasks, we did not include DPL as a baseline. We will make sure to add DPL experiments later for the sake of completeness.\n\nWe acknowledge that similar to prior work in neurosymbolic learning, there is a tradeoff between scalability and expressiveness in our work. ISED favors expressiveness, allowing the use of general-purpose languages such as Python, over scalability. To this end, we used a modified version of the HWF task in our benchmarks which included only multiplication and addition operations to show that hand-written formulas, with a smaller input space, can still be learned under ISED.\n\nWe don\u2019t have any theoretical analysis for the sampling-based method yet. \n\n[1] Scallop: A Language for Neurosymbolic Programming. Ziyang Li, Jiani Huang, Mayur Naik. PLDI 2023"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7948/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578565541,
                "cdate": 1700578565541,
                "tmdate": 1700578565541,
                "mdate": 1700578565541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]