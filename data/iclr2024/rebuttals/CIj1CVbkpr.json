[
    {
        "title": "Online Stabilization of Spiking Neural Networks"
    },
    {
        "review": {
            "id": "2X51jPHaiN",
            "forum": "CIj1CVbkpr",
            "replyto": "CIj1CVbkpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission159/Reviewer_W97A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission159/Reviewer_W97A"
            ],
            "content": {
                "summary": {
                    "value": "When batch normalization (BN) is implemented in spiking neural network (SNN) structures, it is a common practice to compute the statistics across all time steps for running BN. However, this commonly used BN is ill-suited for the online training of SNNs. To enable BN for online training, this paper introduces two strategies called Online Spiking Renormalization (OSR) and Online Threshold Stabilizer (OTS), which preserve the online training property and memory efficiency. Experimental results demonstrate the efficacy of the proposed methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed tricks are both simple to implement and logically sound, contributing to their practicality and ease of use.\n2. The performance is good when compared to online training methods and conventional methods."
                },
                "weaknesses": {
                    "value": "1. If the reviewer understands the OTS method correctly, the threshold $\\theta[t]$ is dynamically adjusted for each sample batch during both the training and inference phases. However, the reviewer thinks that $\\theta[t]$ should be precalculated and fixed during the inference stage. Otherwise, the batch size will highly influence the performance. Much worsely, the obtained network cannot be implemented on normal neuromorphic chips because the chips do not support calculating the mean and variance. the reviewer suggests the authors to maintain a running $\\theta[t]$ used for inference.\n\n\n2. This paper could be regarded as an engineering work. Then the reviewer thinks that the ablation experiments are not abundant:\n (i). When OSR is incorporated individually, there appears to be a significant performance degradation. It is important to clarify whether such a phenomenon is typical across various datasets.\n (ii). A simpler approach to integrating BN into the online training regime involves calculating the statistics solely based on data from each time step. In this approach, at every time step during training, the normalized I[t] is computed based on $\\mu[t]$ and $\\sigma[t]$, as illustrated in eq. 8. Additionally, the running mean and variance are also updated at each time step. This vanilla BN method should be considered as the baseline. It would be valuable to assess whether this method outperforms OSR and whether the combination of the baseline with OTS outperforms the combination of OSR with OTS.\n (iii). In the backward stage, the authors utilize a \"double transformation\" trick to facilitate more meaningful backpropagation. What if we directly conduct bachprop based on the ``linear transformation''? Is OSR better than that?\n\n\nMinor:\n1. eq.2: $s^{l-1}[t]W^l$ -> $W^ls^{l-1}[t]$, eq.4: $u^{l} [t] (1-s^{l} [t] )$  -> $u^{l}[t] \\odot (1-s^{l}[t])$\n2. The ``Online Calculation of All-time Mean and Variance'' part appears somewhat trivial. The reviewer thinks that it might be unnecessary to include this information in the list of contributions. The audiences may expect a more significant contribution when such a detail is highlighted.\n3. The statement preceding Section 5: ``our OTS mechanism helps our OTS mechanism''."
                },
                "questions": {
                    "value": "1. The line right before Section 4.3: is there typos in the formula? Should the formula be $...(\\theta[1]-\\mu[1])/\\sigma[1]$? Then does it mean that $\\theta[1]$ is fixed all the time?\n2. How to implement the proposed method on NF-Resnet-34 which is a normalizer-free architecture? The reviewer knows that the goal of this experiment is to compare the proposed method and OTTT. But still, the proposed method and normalizer-free nets are totally orthogonal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Reviewer_W97A"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission159/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698391575804,
            "cdate": 1698391575804,
            "tmdate": 1700729347186,
            "mdate": 1700729347186,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sa4uSUba5f",
                "forum": "CIj1CVbkpr",
                "replyto": "2X51jPHaiN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W97A"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your thoughtful and comprehensive review. Your insights highlight crucial aspects we overlooked or omitted in our paper. Each point you raised significantly contributes to enhancing the quality of our work. We are dedicated to addressing your concerns comprehensively and offering detailed responses in the upcoming sections.\n\n\n> If the reviewer understands the OTS method correctly, the threshold $\\theta[t]$ is dynamically adjusted for each sample batch during both the training and inference phases. However, the reviewer thinks that $\\theta[t]$ should be precalculated and fixed during the inference stage. Otherwise, the batch size will highly influence the performance. Much worsely, the obtained network cannot be implemented on normal neuromorphic chips because the chips do not support calculating the mean and variance. the reviewer suggests the authors to maintain a running $\\theta[t]$ used for inference.\n\nThank you for your valuable suggestion. Your understanding is right that the threshold $\\theta[t]$ is dynamically adjusted for each sample batch during both the training and inference phases.\nWe have conducted two extra experiments:\n- 1. We have tested the performance of using fixed running $\\theta[t]$ on Imagenet, its performance is 64.06\\% (original accuracy is 64.14\\%) (using the saved model of OSR+OTS). This result shows that your suggestion works well.\n- 2. We have tested the performance for $batch size=1$ on Imagenet (using our saved model), and the performance is 62.64\\%. Although there is a performance drop, it is still better than the baseline.\n\nIn addition, we think that $\\theta[t]$ could be dynamical during the inference stage if it is incorporated in the neuron model, since it can be viewed as a neural adaptation mechanism. This is a potential direction for future research.\n\n\n> This paper could be regarded as an engineering work. Then the reviewer thinks that the ablation experiments are not abundant: \n    > (i). When OSR is incorporated individually, there appears to be a significant performance degradation. It is important to clarify whether such a phenomenon is typical across various datasets. \n    > (ii). A simpler approach to integrating BN into the online training regime involves calculating the statistics solely based on data from each time step. In this approach, at every time step during training, the normalized $I[t]$ is computed based on $\\mu[t]$ and $\\sigma[t]$, as illustrated in eq. 8. Additionally, the running mean and variance are also updated at each time step. This vanilla BN method should be considered as the baseline. It would be valuable to assess whether this method outperforms OSR and whether the combination of the baseline with OTS outperforms the combination of OSR with OTS. \n    > (iii). In the backward stage, the authors utilize a \"double transformation\" trick to facilitate more meaningful backpropagation. What if we directly conduct backprop based on the ''linear transformation''? Is OSR better than that?\n\nThank you for mentioning this crucial point. Actually, the experimental baseline (BN(Vanilla) for the Imagenet dataset in Table 1) in the paper is almost the same (with slight difference) as you mentioned in (ii). The details are provided in **Experimental settings** in **Response to All Reviewers**.\n- For (i), We have conducted extra experiments on CIFAR10 and CIFAR100 dataset, and the results are shown in the following table:\n\n|                     | CIFAR10 | CIFAR100 |\n| ------------------- | ------- | -------- |\n| VGG+Vanilla BN      | 92.6    | 75.17    |\n| VGG+OSR             | 94.05   | 75.65    |\n| VGG+OSR+OTS         | 94.35   | 76.48    |\n| Resnet19+Vanilla BN | 92.96   | 73.68    |\n| Resnet19+OSR        | 95.14   | 74.03    |\n| Resnet19+OSR+OTS    | 95.20   | 77.86    |\n\nNote that Resnet19+OSR on CIFAR100 has a significant performance drop compared with Resnet19+OSR+OTS.\nIn addition, when conducting these experiments, we find that only adding OSR is sensitive to the $weight$ $decay$ parameter. It often requires lower $weight$ $decay$ to get a better result. The ablation experiments we reported in Table 1 uses the same set of parameters for vanilla BN, BN+OSR, BN+OTS, and BN+OSR+OTS. Maybe turning down the $weight$ $decay$ parameter for BN+OSR will promote its performance, but we do not have enough time to finish tuning this parameter during rebuttal. We are working on this and will provide a better result if we can.\n\n- For (iii), we have tested this ''linear transformation'' on the CIFAR100 dataset using VGGSNN and find it very hard to train. The final result we get is 53.25\\%, which is significantly worse than OSR."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558408442,
                "cdate": 1700558408442,
                "tmdate": 1700558408442,
                "mdate": 1700558408442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C32QGykpdo",
                "forum": "CIj1CVbkpr",
                "replyto": "2X51jPHaiN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer W97A (part 2)"
                    },
                    "comment": {
                        "value": "> eq.2: $s^{l-1}[t] W^l \\rightarrow W^l s^{l-1}[t]$, eq.4: $u^{l}[t] (1 - s^{l}[t]) \\rightarrow u^{l}[t] \\odot (1 - s^{l}[t])$\n\nThank you for such a careful review. We have revised these equations in the latest version.\n\n\n> The ``Online Calculation of All-time Mean and Variance'' part appears somewhat trivial. The reviewer thinks that it might be unnecessary to include this information in the list of contributions. The audiences may expect a more significant contribution when such a detail is highlighted.\n\nThank you for your suggestion. We have combined this part into the contribution of OSR.\n\n\n> The statement preceding Section 5: ''our OTS mechanism helps our OTS mechanism''.\n\nThank you for the careful review. We have changed it to ''our OTS mechanism helps our OSR mechanism'' in the latest version.\n\n\n> The line right before Section 4.3: is there typos in the formula? Should the formula be $\\theta[t] = \\mu_\\text{mem}[t] + \\sigma_\\text{mem}[t] \\cdot \\frac{\\theta[1] - \\mu_\\text{mem}[1]}{\\sigma_\\text{mem}[1]}$? Then does it mean that $\\theta[1]$ is fixed all the time?\n\nI appreciate your thorough review, and thank you for bringing up this specific point. You are right that the formula should be $\\theta[t] = \\mu_\\text{mem}[t] + \\sigma_\\text{mem}[t] \\cdot \\frac{\\theta[1] - \\mu_\\text{mem}[1]}{\\sigma_\\text{mem}[1]}$, and we have fixed it in the latest version. We set $\\theta[1] = 1$ for all training iterations.\n\n\n> How to implement the proposed method on NF-Resnet-34 which is a normalizer-free architecture? The reviewer knows that the goal of this experiment is to compare the proposed method and OTTT. But still, the proposed method and normalizer-free nets are totally orthogonal.\n\nThank you for bringing up this point. The NF-Resnet-34 in OTTT uses a normalizer-free network adding membrane potential in the shortcut connection.\nBriefly speaking, we keep adding membrane potential in the shortcut connection while replacing the normalizer-free part with our normalization modules.\nSpecifically, we do not use scaled weight standardization, and drop scaling factors $\\alpha, \\beta$ along with the corresponding operations to keep variance stable. Instead, we use our normalization modules here."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558433310,
                "cdate": 1700558433310,
                "tmdate": 1700558433310,
                "mdate": 1700558433310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M2DCwRrbd2",
                "forum": "CIj1CVbkpr",
                "replyto": "2X51jPHaiN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Reviewer_W97A"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Reviewer_W97A"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clear rebuttal. Now my concerns have been addressed and I am glad to increase my score from 5 to 6. I suggest the authors to revise the manuscript accordingly. \n\nBy the way, I am interested in how $\\theta[t]$ could be dynamical during the inference stage when implemented on neuromorphic hardware. How could it be incorporated in an adaptive LIF model since it does not follow some explicit dynamics?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729323364,
                "cdate": 1700729323364,
                "tmdate": 1700729323364,
                "mdate": 1700729323364,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GUOmPOW5EK",
            "forum": "CIj1CVbkpr",
            "replyto": "CIj1CVbkpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission159/Reviewer_pFYW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission159/Reviewer_pFYW"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the online training of SNNs. It adds the normalization mechanism into online training, which have not yet been fully explored by previous works. It proposes two modules to improve the standard Batch Normalization, named Online Spiking Renormalization and Online Threshold Stabilizer, which ensure consistent parameters and stable neuron firing rates across time steps. The paper demonstrates the effectiveness of the proposed methods on various datasets and shows that they outperform existing state-of-the-art online training methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper presents a novel approach to training spiking neural networks that integrates essential batch normalization into the online training process by introducing online spiking renormalization and online threshold stabilizers to enhance training stability.\n\n2. The paper is well-organized and well-written. Also, the figures and tables are well-designed and provide a clear representation of the results.\n\n3. The proposed method outperforms existing state-of-the-art online training methods."
                },
                "weaknesses": {
                    "value": "1. Although the online training approaches save the memory cost, the proposed method falls short of BPTT in performance."
                },
                "questions": {
                    "value": "1. What is the difference between OSR and batch renormalization [1] ?\n\n2. The online calculation of all-time mean and variance is interesting. However, where is it used? Is it a part of OSR?\n\n3. The authors have made an assumption that the membrane potential follows a normal distribution in OTS. Although experiments have shown the effectiveness of OTS+OSR, I am still curious about the real distribution of the membrane potential.\n\n[1]Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. Advances in neural information processing systems, 30, 2017\n\nI consider increasing my score if the authors can solve my concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Reviewer_pFYW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission159/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808328319,
            "cdate": 1698808328319,
            "tmdate": 1700665275971,
            "mdate": 1700665275971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BpKf4I4mha",
                "forum": "CIj1CVbkpr",
                "replyto": "GUOmPOW5EK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pFYW"
                    },
                    "comment": {
                        "value": "We deeply appreciate your valuable feedback and acknowledgment of our method's novelty, the paper's organization, and writing quality. Your insights are invaluable. We are committed to thoroughly addressing your concerns and providing detailed responses, as outlined in the following sections.\n\n> Although the online training approaches save the memory cost, the proposed method falls short of BPTT in performance.\n\nOnline training saves memory cost by backpropagating each time-step using information generated at the current time-step or at previous time-steps.\nThis advantage is the direct origin of its disadvantage that it cannot use the information from future time-steps (compared with BPTT-based training algorithms).\nAt the same time, it is hard to find a way to completely offset the information from future time-steps in online training, especially when the network has multiple layers (perfect solution such as eligibility traces [1][2] only works for single-layer networks without other mechanisms like BN).\nAs a result, the performance of online training approaches still fall short of BPTT.\n\n    [1] Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., & Maass, W. (2020). A solution to the learning dilemma for recurrent networks of spiking neurons. Nature communications, 11(1), 3625.\n    [2] Bohnstingl, T., Wo\u017aniak, S., Pantazi, A., & Eleftheriou, E. (2022). Online spatio-temporal learning in deep neural networks. IEEE Transactions on Neural Networks and Learning Systems.\n\n\n> What is the difference between OSR and batch renormalization [1] ?\n\nYou have raised a critical point. \nSince our OSR is applied in spiking neural networks, it involves a time dimension, which is not encountered by batch renormalization.\nActually, OSR exploits additional advantage by this additional time dimension:\nOne key aim of OSR is to alleviate the temporal covariate shift by applying the same forward transformation to input currents at all time-steps.\nAlthough OSR achieves this goal by the same form of double transformation as batch renormalization, this problem that OSR solves is not the aim of batch renormalization due to the extra time dimension.\nIn addition, to make the memory cost perfectly unrelated to the number of time-steps in simulation, OSR requires online calculation of all-time mean and variance.\n\n    [1]Sergey Ioffe. Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. Advances in neural information processing systems, 30, 2017\n\n\n> The online calculation of all-time mean and variance is interesting. However, where is it used? Is it a part of OSR?\n\nYes, it is a part of OSR.\nThe process of OSR is described as follows:\n\nIt is applied on $I[t]$ to get the normalized $\\tilde{I}[t]$:.\n  During training, first we normalize $I[t]$ to $\\hat{I}[t]$ (note $\\hat{I}[t]$ passes gradients to $\\mu[t]$ and $\\sigma^2[t]$):\n  $$\\hat{I}[t] = \\frac{I[t] - \\mu[t]}{\\sqrt{\\sigma^2[t] + \\epsilon}}$$\nThen we apply linear transformation from $\\hat{I}[t]$ to $\\tilde{I}[t]$ (note $\\tilde{I}[t]$ does not pass gradients to $\\mu[t]$ and $\\sigma^2[t]$ due to $no\\\\_grad()$, and $\\gamma, \\beta$ are learnable):\n  $$\\tilde{I}[t] = \\gamma \\cdot \\left( \\hat{I}[t] \\cdot no\\\\_grad\\left( \\frac{\\sqrt{\\sigma^2[t] + \\epsilon}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} \\right) + no\\\\_grad\\left(\\frac{\\mu[t] - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} \\right) \\right) + \\beta$$\n  As a result, $\\tilde{I}[t] = \\gamma \\cdot \\frac{ I[t] - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} + \\beta$ in the forward pass in training. Note that we apply conventional linear transformation $\\tilde{I}[t] = \\gamma \\cdot \\frac{ I[t] - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} + \\beta$ in the testing phase, so the input goes through the same linear transformations **among all time-steps in both training and inference**.\n\nThe calculation of $\\mu[t], \\sigma^2[t], \\hat{\\mu}, \\hat{\\sigma^2}$ during training can be found in **Response to All Reviewers**.\n\n\n> The authors have made an assumption that the membrane potential follows a normal distribution in OTS. Although experiments have shown the effectiveness of OTS+OSR, I am still curious about the real distribution of the membrane potential.\n\nThanks for your valuable advice. To verify the assumption that the membrane potential follows a normal distribution, we collect the membrane potential of a VGG network trained with OSR and OTS on the CIFAR-10 dataset and visualize the distribution of membrane potentials for each layer and each time step. We display the results in Fig. 3 in the appendix of the revised paper. These distributions displays the shape of bell curves, which similars to normal distributions. Thus, we think it is reasonable to use the assumption of normal distribution in OTS."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558319379,
                "cdate": 1700558319379,
                "tmdate": 1700560759004,
                "mdate": 1700560759004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TpPsWABFkd",
                "forum": "CIj1CVbkpr",
                "replyto": "BpKf4I4mha",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Reviewer_pFYW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Reviewer_pFYW"
                ],
                "content": {
                    "title": {
                        "value": "My concerns have been addressed"
                    },
                    "comment": {
                        "value": "I have increased my score as the authors have solved all my concerns. It would be better if the authors can add these clarifications in the final version."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665185675,
                "cdate": 1700665185675,
                "tmdate": 1700665185675,
                "mdate": 1700665185675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FPNWATNoip",
            "forum": "CIj1CVbkpr",
            "replyto": "CIj1CVbkpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission159/Reviewer_2ic1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission159/Reviewer_2ic1"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the online setting of SNN, and points out one important mismatch of BN happens in the training and testing stages. The authors proposed one nice solution to solve the issue, and the experimental results support the benefits with the new algorithms. Importantly, in addition to the experimental verification, the authors also provide necessary theoretical analysis to the new algorithm."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper have a very clear motivation: mismatch of BN happens in the training and testing stages. \n2. The authors proposed one nice solution to solve the issue, and the experimental results support the benefits with the new algorithms. \n3. The propblem of this paper is unsolved in the comminity before. The solution of this paper is interesting and novel. Importantly, it signicantly improve the performance. Considering the important role of BN in ML, I think the signicance and novelty of this work is good.\n4. The authors also provide the necessary theoretical analysis to the new algorithm.\n5. Presentation is good. Especially, I like figure 1 to intutiively explain the proposed algorithm."
                },
                "weaknesses": {
                    "value": "The paper is with a nice story, and I especially like the figure 1 to intutiively explain the proposed algorithm. However, I still have some questions or comments below.\n\n1. What is the intuitive reason to have double normalization in (12)\n2. I suggest to provide the experimental results to verify the Gaussion assumption.\n3. I did not find the calculation form of $\\hat{\\mu}$ and $\\\\hat{\\sigma}$. I suggest to provide a clear definition of them in the paper.\n\nMinor issue:\n1. I did not find  the meaning of m in (8)."
                },
                "questions": {
                    "value": "Please find the comments above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission159/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698992125991,
            "cdate": 1698992125991,
            "tmdate": 1699635941380,
            "mdate": 1699635941380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "etnlnd9DuN",
                "forum": "CIj1CVbkpr",
                "replyto": "FPNWATNoip",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2ic1"
                    },
                    "comment": {
                        "value": "Thank you for providing such an encouraging and positive response. It is truly gratifying to know that you value the whole story of our paper, including our motivation, our proposed method, our theoretical analysis and our presentation. In light of your input, we are fully dedicated to addressing your concerns and offering comprehensive responses to the questions you have raised, as outlined in the forthcoming sections.\n\n> What is the intuitive reason to have double normalization in (12)?\n\nThank you for your question.\nPrevious work [1] have summarized the Temporal Covariate Shift (TCS), which means the distribution of the input current in each time-step are different.\nThis difference will cause different mean and variance among time-steps.\n\nIf we just take batch normalization on each time-step separately (refer to BN(Vanilla) in **Experimental settings** in **Response to All Reviewers** for details), it will cause the linear transformations in BN forward pass differ among time-steps. This difference in BN transformation will degrade the overall performance intuitively.\n\nAs a result, we want a method that uses the same mean and variance to normalize layer input statistics each time-step. OSR accomplishes this task by applying the same BN forward transformation for each time-step.\nAdditionally, the BN forward transformation is the same among the training and testing phase under OSR, so it helps improve the test accuracy.\n\n    [1] Duan, C., Ding, J., Chen, S., Yu, Z., & Huang, T. (2022). Temporal effective batch normalization in spiking neural networks. Advances in Neural Information Processing Systems, 35, 34377-34390.\n\n\n> I suggest to provide the experimental results to verify the Gaussion assumption.\n\nThanks for your advice. In the main content, we make the assumption that the membrane potential have a Gaussian distribution. To verify the assumption, we collect the membrane potential of a VGG network trained with OSR and OTS on the CIFAR-10 dataset and visualize the distribution of membrane potentials for each layer and each time step. We display the results in Fig. 3 in the appendix of the revised paper. These distributions display the shape of bell curves, which similars to Gaussian distributions.\n\n\n> I did not find the calculation form of $\\hat{\\mu}$ and $\\hat{\\sigma}$. I suggest to provide a clear definition of them in the paper.\n\nThank you for mentioning this. We have summarized the calculation of $\\mu[t], \\sigma^2[t], \\hat{\\mu}, \\hat{\\sigma^2}$ during training in **Response to All Reviewers**.\n\n\n\n> I did not find the meaning of m in (8).\n\nIt means the number of elements in a group of normalization.\nFor an image feature map at time $t$ of size $B \\times C \\times H \\times W$ where $B$ is the batch size, $C$ is the number of channels, $H$ and $W$ are height and width, a group contains all elements in a channel, so $m = B \\times H \\times W$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558285346,
                "cdate": 1700558285346,
                "tmdate": 1700558285346,
                "mdate": 1700558285346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9HjjHT3szK",
                "forum": "CIj1CVbkpr",
                "replyto": "etnlnd9DuN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Reviewer_2ic1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Reviewer_2ic1"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for all clarifications"
                    },
                    "comment": {
                        "value": "I have read the author's comment to all reviewers. I am satisfied with your answers and have no further questions. I remain with my recommendation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732292854,
                "cdate": 1700732292854,
                "tmdate": 1700732292854,
                "mdate": 1700732292854,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7upFxRmgur",
            "forum": "CIj1CVbkpr",
            "replyto": "CIj1CVbkpr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission159/Reviewer_9e6u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission159/Reviewer_9e6u"
            ],
            "content": {
                "summary": {
                    "value": "The work focuses on online training techniques, especially addressing the limitation of not having access to future information in early time steps in online training. This paper tries to incorporate BN into online training, by two new modules, Online Spiking Renormalization (OSR) and Online Threshold Stabilizer (OTS).  \nThis online training setting will benefit memory consumption. However, there are several problems: \nthe presentation of the new idea is not clear at all, it depends on the running mean and running variance, but how to calculate them is not mentioned clearly. we still don't know how to save memory by using the proposed modules. \nThe other issue is that in the theoretical parts, authors tend to make unrealistic assumptions. How can the spike train be iid. and expect the weights to be iid. Another thing is that the conclusions of the theorems, why do we need expectation of the average of u[t], and expectation of the average of sigma[t], how can these results help with the main new modules?\nFor the experiments, there is only one method using the same structure as the new method, shouldn't compared with different network architectures? Shouldn't compare with different BN methods? That is absent in the main paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a novel approach to training spiking neural networks that is memory-efficient and biologically plausible. The proposed Online Spiking Renormalization and Online Threshold Stabilizer techniques ensure consistent parameters and stable neuron firing rates across time steps."
                },
                "weaknesses": {
                    "value": "The main idea should be better explained to make it easier to understand. \nHow to compute gradients in the backward stage if we use this forward transformation? The authors asked themselves the question, but did not give clear answers. Please make the core part of the paper clear."
                },
                "questions": {
                    "value": "This online training setting will benefit memory consumption. However, there are several problems: \nthe presentation of the new idea is not clear at all, it depends on the running mean and running variance, but how to calculate them is not mentioned clearly. we still don't know how to save memory by using the proposed modules. \nThe other issue is that in the theoretical parts, authors tend to make unrealistic assumptions. How can the spike train be iid. and expect the weights to be iid. Another thing is that the conclusions of the theorems, why do we need expectation of the average of $u[t]$, and expectation of the average of $\\sigma[t]$, how can these results help with the main proposed modules?\nFor the experiments, there is only one method using the same structure as the new method, shouldn't compared with different network architectures? Shouldn't compare with different BN methods? That is absent in the main paper. \nHow to calculate $\\hat{u}, \\hat{\\sigma}$? please explain. \nIs $\\gamma$, $\\beta$ learnable or fixed?\nNo point of Online Threshold Stabilizer (OTS) to stabilize the firing rate of each layer?\nNo experiment results on resnet on cifar datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission159/Reviewer_9e6u"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission159/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699017389715,
            "cdate": 1699017389715,
            "tmdate": 1700637075033,
            "mdate": 1700637075033,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K0n0bnyabP",
                "forum": "CIj1CVbkpr",
                "replyto": "7upFxRmgur",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9e6u (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback. We are committed to clarifying addressing your concerns and make the presentation of our paper clearer, as outlined in the following sections.\n\n> How to compute gradients in the backward stage if we use this forward transformation? Please make the core part of the paper clear.\n\nThank you for your question. \nIn the following we provide a comprehensive description of the process of our algorithm:\n\nFirst of all, our algorithm works under the online learning framework, which means the network goes through forward and backward propagations step by step from time-step $1$ to $T$ (instead of first forward from time step $1$ to $T$ and then backward from time step $T$ to $1$).\nSince the network is processed step by step, it does not require saving intermediate state from time-step $1$ to $T$ as in regular BPTT.\nIn each time-step, the information go from the input layer to the output layer of the network in forward pass, and then the gradients go from the output layer to the input layer in backward pass.\n\n**The process of the forward pass for each layer** are shown as follows (where the calculation of $\\mu[t], \\sigma^2[t], \\hat{\\mu}, \\hat{\\sigma^2}$ are shown separately in the following section):\n- Input: We need the output of last layer $s^{l-1}[t]$ (for input layer, this part is the input spike train / image at time $t$) and the weight between last layer and current layer $w^{l}$ ($s^{l-1}[t]$ and $w^{l}$ are both tensors instead of scalars).\n- Step 1: Calculate $I[t] = conv(s^{l-1}[t], w^{l})$ or $I[t] = linear(s^{l-1}[t], w^{l})$ or whatever else according to the layer type.\n- Step 2: Apply OSR on $I[t]$ to get the normalized $\\tilde{I}[t]$: \n  During training, first we normalize $I[t]$ to $\\hat{I}[t]$ (note $\\hat{I}[t]$ passes gradients to $\\mu[t]$ and $\\sigma^2[t]$):\n  $$\\hat{I}[t] = \\frac{I[t] - \\mu[t]}{\\sqrt{\\sigma^2[t] + \\epsilon}}$$\nThen we apply linear transformation from $\\hat{I}[t]$ to $\\tilde{I}[t]$ (note $\\tilde{I}[t]$ does not pass gradients to $\\mu[t]$ and $\\sigma^2[t]$ due to $no\\\\_grad()$, and $\\gamma, \\beta$ are learnable):\n  $$\\tilde{I}[t] = \\gamma \\cdot \\left( \\hat{I}[t] \\cdot no\\\\_grad\\left( \\frac{\\sqrt{\\sigma^2[t] + \\epsilon}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} \\right) + no\\\\_grad\\left(\\frac{\\mu[t] - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} \\right) \\right) + \\beta$$\n  As a result, $\\tilde{I}[t] = \\gamma \\cdot \\frac{ I[t] - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} + \\beta$ in the forward pass in training. Note that we apply conventional linear transformation $\\tilde{I}[t] = \\gamma \\cdot \\frac{ I[t] - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} + \\beta$ in the testing phase, so the input goes through the same linear transformations **among all time-steps in both training and inference**.\n- Step 3: Update membrane potential of neurons in layer $l$ according to the LIF neuron model and input $\\tilde{I}[t]$: $$u^{l}[t] = (1 - \\frac{1}{\\tau^{l}}) u^{l}[t-0.5] + \\tilde{I}[t]$$\n- Step 4: Apply OTS to update the threshold $\\theta[t]$: $\\theta[t] = \\mu_\\text{mem}[t] + \\sigma_\\text{mem}[t] \\cdot \\frac{\\theta[1] - \\mu_\\text{mem}[1]}{\\sigma_\\text{mem}[1]}$\n- Step 5: Fire spikes $s^{l}[t]$: $$s^{l}[t] = \\Theta(u^{l}[t] - \\theta[t])$$ and then reset membrane potential: $$u^{l}[t+0.5] = u^{l}[t] (1 - s^{l}[t])$$\n\n\nThe OSR forward transformation is shown in Step 2 in **The process of the forward pass for each layer**. \nThe gradient for layer input, $\\frac{\\partial \\mathcal{L}}{\\partial I[t]}$ is thus\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial I[t]} = \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{I}[t]} \\cdot \\frac{\\partial \\tilde{I}[t]}{\\partial \\hat{I}[t]} \\cdot \\frac{\\partial \\hat{I}[t]}{\\partial I[t]}.\n$$\nThe thing should be noticed is that $\\tilde{I}[t]$ does not pass gradients to $\\sqrt{\\sigma^2[t] + \\epsilon}$ while $\\hat{I}[t]$ do.\nThis is because $\\sqrt{\\sigma^2[t] + \\epsilon}$ is in the function $no\\\\_grad$ when calculating $\\tilde{I}[t]$, while it is not in $no\\\\_grad$ when calculating $\\hat{I}[t]$.\n\nThe remaining part is common: $\\frac{\\partial \\tilde{I}[t]}{\\partial \\hat{I}[t]}$ is just $\\gamma \\cdot \\frac{\\sqrt{\\sigma^2[t] + \\epsilon}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}}$ and $\\frac{\\partial \\hat{I}[t]}{\\partial I[t]}$ can be calculated in the same way as batch normalization.\nBesides, the gradients to scaling factors $\\beta, \\gamma$ can be calculated in the common way:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\gamma} = \\sum_x \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{I}_x[t]} \\left( \\hat{I}_x[t] \\cdot \\frac{\\sqrt{\\sigma^2[t] + \\epsilon}} {\\sqrt{ \\hat{\\sigma^2} + \\epsilon}} + \\frac{\\mu[t] - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma^2} + \\epsilon}} \\right),\n$$$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\beta} = \\sum_x \\frac{\\partial \\mathcal{L}}{\\partial \\tilde{I}_x[t]}.\n$$"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558154547,
                "cdate": 1700558154547,
                "tmdate": 1700560721060,
                "mdate": 1700560721060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vpQIC1dTmC",
                "forum": "CIj1CVbkpr",
                "replyto": "7upFxRmgur",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9e6u (part 2)"
                    },
                    "comment": {
                        "value": "> The presentation of the new idea is not clear at all, it depends on the running mean and running variance, but how to calculate them is not mentioned clearly.\n\nThank you for your constructive feedback. We have clarified the calculation of $\\mu[t], \\sigma^2[t], \\hat{\\mu}, \\hat{\\sigma^2}$ during training in **Response to All Reviewers**.\nIf you have any further questions, please feel free to ask.\n\n\n> We still don't know how to save memory by using the proposed modules.\n\nThank you for your question. Saving memory is a property of online training instead of our proposed modules.\nThe reason that online training approaches can save memory compared with BPTT approaches is as follows:\n- BPTT methods require information of all time-steps in backpropagation since they need to backpropagate gradients from the last time-step to the first time-step. As a result, they need to save gradient information of all time-steps during training, which is $O(T)$ where $T$ denotes the total number of time-steps.\n- Online training algorithms backpropagates at each time-step, and they only require information at (or before) the current time-step. If information before the current time-step is required, they can be aggregated to some variables (called eligibility traces in prior works) costing constant memory. Overall, it only require $O(1)$ memory cost with respect to $T$.\n\n\n> Authors tend to make unrealistic assumptions. How can the spike train be iid. and expect the weights to be iid.\n\nIt should be mentioned that BN itself relies on the fact that elements to be normalized are i.i.d samples from the training distribution [1][2].\nMoreover, some works analyzing BN-like mechanisms make assumptions of independence [3].\nAs a result, our i.i.d assumptions are not that unrealistic.\n\n    [1] Ioffe, S. (2017). Batch renormalization: Towards reducing minibatch dependence in batch-normalized models. Advances in neural information processing systems, 30.\n    [2] Summers, C., & Dinneen, M. J. (2019). Four things everyone should know to improve batch normalization. International Conference on Learning Representations.\n    [3] Yan, J., Wan, R., Zhang, X., Zhang, W., Wei, Y., & Sun, J. (2020). Towards stabilizing batch statistics in backward propagation of batch normalization. International Conference on Learning Representations.\n\n\n> Another thing is that the conclusions of the theorems, why do we need expectation of the average of $\\mu[t]$, and expectation of the average of $\\sigma[t]$, how can these results help with the main proposed modules?\n\nThank you for your question. The aim of our theoretical analysis is to show that our OTS helps stabilize our OSR mechanism. To achieve this goal, we show that OTS helps reduce the (expectation of) sample variance. \nThis variance reduction will lead to more accurate estimation of running mean and running variance (for all time-steps) since it reduces the variance in estimation.\nIn addition, this variance reduction come from the part of variance of mean among time-steps (lower 1st moment of distribution). This reduces the temporal covariate shift (which indicates the distribution difference) among time-steps.\n\n\n> For the experiments, there is only one method using the same structure as the new method, shouldn't compared with different network architectures? No experiment results on resnet on cifar datasets.\n\nWe have conducted experiments of Resnet-19 on CIFAR10 and CIFAR100.\nThe test accuracy for Resnet-19 is 95.20\\% on CIFAR10 and 77.86\\% on CIFAR100.\n\n\n> Shouldn't compare with different BN methods? \n\nWe have listed the result of a vanilla BN which normalizes batch data at each time-step (you can go to **Experimental settings** in **Response to All Reviewers** for more details) testing on the Imagenet dataset in Table 1 (the line BN(Vanilla)).\nIn addition, we have listed previous works of SNN BN methods (tdBN and TEBN) in Table 1. It should be noticed that these methods cannot be fitted into online training since they both require information of all time-steps.\n\n\n> How to calculate $\\hat{\\mu}, \\hat{\\sigma}$? please explain. Is $\\gamma$, $\\beta$ learnable or fixed?\n\nWe have summarized the calculation of $\\hat{\\mu}$ and $\\hat{\\sigma}$ in the **The calculation of $\\mu[t], \\sigma^2[t], \\hat{\\mu}, \\hat{\\sigma^2}$ during training** part in **Response to All Reviewers**.\nIn addition, $\\gamma$ and $\\beta$ are learnable.\n\n\n> No point of Online Threshold Stabilizer (OTS) to stabilize the firing rate of each layer?\n\nWe have plotted the firing rate statistics of different configurations in Figure.2 (b). Results have shown that the firing rate across time-steps is stabilized when OTS is added."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558214835,
                "cdate": 1700558214835,
                "tmdate": 1700558214835,
                "mdate": 1700558214835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bSDvACLfSm",
                "forum": "CIj1CVbkpr",
                "replyto": "vpQIC1dTmC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission159/Reviewer_9e6u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission159/Reviewer_9e6u"
                ],
                "content": {
                    "comment": {
                        "value": "After carefully reading the authors' responses, I have better understanding of the paper and my concerns have been addressed. I do not have new concerns, and I am more than willing to increase my score from 5 to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission159/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637057406,
                "cdate": 1700637057406,
                "tmdate": 1700637057406,
                "mdate": 1700637057406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]