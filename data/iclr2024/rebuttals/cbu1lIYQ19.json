[
    {
        "title": "Hybrid Kernel Stein Variational Gradient Descent"
    },
    {
        "review": {
            "id": "5Ob2ouY0Jf",
            "forum": "cbu1lIYQ19",
            "replyto": "cbu1lIYQ19",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2183/Reviewer_9Q7g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2183/Reviewer_9Q7g"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the theoretical properties of hybrid Stein variational gradient descent, which uses a different kernel for the attraction term and the repulsion term. It is shown that h-SVGD update direction is optimal in decreasing the KL divergence. An assortment of results for h-SVGD are proved, extending recent theoretical results regarding SVGD. Experiments are performed to verify that h-SVGD can mitigate the variance collapse problem in high dimensions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The writing is very clear and generally good, except it seems unmotivated at times.\n* The proofs are concise (I have only checked Thm 4.1 closely) and it is more careful (e.g. Remark 3) and has better generalization (e.g. Lemma B.2 and Proposition 4.7) compared to existing results.\n* While the idea of analyzing the update direction in the sum of two RKHSes is natural, it is nevertheless clean and well-explained.\n* The discussion in Appendix C on why h-KSD does not have an easily computable form (compared to KSD) is an important and interesting addition to the paper.\n* Experiments seem sufficient in illustrating the advantage of h-SVGD in mitigating variance collapse."
                },
                "weaknesses": {
                    "value": "* The analysis seems like straightforward extensions of existing results, e.g., most proofs in Section 4 are 1-liners.\n* While the result (Thm 4.1) on the optimality of the h-SVGD update direction and many theoretical results does not require $k_1$ to be related to $k_2$, in all experiments, $k_2$ is simply a scalar multiple of $k_1$, i.e., $k_2 = f(d)k_1$. This seems to suggest the more general theory does not give rise to diverse choices of $k_1$ and $k_2$ in applications.\n* Even if we only consider the case of $k_2 = f(d)k_1$, it remains a question of how to choose the scaling function $f(d)$. The paper suggests taking $f(d) = \\sqrt{d}$ or $\\ln(d)$, but further comparision (either empircal or theoretical) is lacking.\n* As discussed in Appendix C, h-KSD is not a valid discrepancy measure, which seems to suggest it is less useful as a metric than the vanilla KSD."
                },
                "questions": {
                    "value": "1. Could the authors explain why $\\phi_{\\mu, p}^{k_1,k_2} \\in \\mathcal{H}_1^d\\cap \\mathcal{H}_2^d$ in Thm 4.1?\n2. Are there any applications of choices of $k_1$ and $k_2$ such that $\\mathcal{H}_1$ and $\\mathcal{H}_2$ does not include one another?\n3. How is the bandwidth of the kernels affecting the variance collapse, compared to the choice of $f(d)$? Or to put the question in another way, how to simultaneously choose the bandwidth and $f(d)$ in applications?\n4. At the beginning of Section 5, the authors mention [Zhuo et al. 2018] that puts a \"conditional dependence structure\". What does this mean exactly?\n5. In Table 1, the test accuracy on Yacht for h-SVGD is poor compared to SVGD. Why is this the case?\n6. Where is the map $\\Phi_p^{k_1,k_2}$ defined in (9)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698367714978,
            "cdate": 1698367714978,
            "tmdate": 1699636151958,
            "mdate": 1699636151958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BgyxYXEVAQ",
                "forum": "cbu1lIYQ19",
                "replyto": "5Ob2ouY0Jf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time in providing their review and feedback.\n\n**Weaknesses:**\n\n1. We acknowledge that some proofs are presented as straightforward extensions and one-liners. However, we would like to mention some key technical details in the Appendix. Lemma B.1 is non-trivial and is required to establish the optimality of the h-SVGD update direction. Some oversights in proofs in the existing literature have been addressed. For example, our Assumption (B2) replaces a previous very restrictive assumption which, through Lemma B.2, makes Proposition 4.7 a stronger result than its counterpart in Korba et al. (2020). Also, Remark 3 addresses the interchangeability of expectation and inner products, which has not been addressed in the SVGD literature to our knowledge.\n\n2. Diversity of kernel choice: As noted, the developed theory does not require kernels $k_1$ and $k_2$ to be related, but the simulation results were based on kernels of the form $k_2=f(d)k_1$. We have now included additional BNN experiments with a range different kernels in Appendix D. Please see Main Response for further details.\n\n3. The current work serves to establish the theory of h-SVGD for the first time, and to demonstrate its ability to alleviate variance collapse. However, we agree that choosing the scaling function is a worthwhile direction for future research, and we now explicitly mention this in Section 5.2 and the conclusion.\n\n4. While, as correctly noted, h-KSD is not a valid discrepancy measure, the descent lemma (Theorem 4.2) shows that the KL divergence bounds can be written in terms of the vanilla KSD and guarantees a decrease in the KL divergence at every step. Some additional discussion has been added to Section 4.3. See the Main Response for further discussion.\n\n**Questions:**\n\n1. The set within which the optimisation takes place is chosen as a subset of this intersection. For standard kernel choices, Remark 2 ensures that this intersection will be either $\\mathcal H_1$ or $\\mathcal H_2$. This amounts to optimising within a unit ball on $\\mathcal H_1$ or $\\mathcal H_2$, although with a norm induced by the sum Hilbert space.\n \n2. To our knowledge, the SVGD literature does not contain kernel choices of this type.\n \n3. If $k_2$ is the RBF kernel, a larger bandwidth allows a steeper gradient $\\nabla k_2$ when evaluated on distant particles. For intuition, think of the plot of $\\exp(-x^2/h)$, so that a higher bandwidth increases the repulsive force, and thereby mitigates variance collapse. We now provide a comment on this in Appendix D.\n \n4. The conditional dependence structure refers to probabilistic graphical models that factorise into a set of lower dimensional inference problems. In this example, variance collapse is alleviated by working in lower dimensions. We have slightly modified the beginning of Section 5 to make this more clear.\n \n5. One possible reason is that the Yacht dataset has the smallest number of records (306) and the second smallest number of features (6) of the ten datasets considered. \n\n6. The paper has been updated with the definition of $\\Phi_p^{k_1,k_2}$ now just before equation (9)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476326684,
                "cdate": 1700476326684,
                "tmdate": 1700476326684,
                "mdate": 1700476326684,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VrM2jtueh3",
                "forum": "cbu1lIYQ19",
                "replyto": "BgyxYXEVAQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2183/Reviewer_9Q7g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2183/Reviewer_9Q7g"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thank you for your clarification. I appreciate the effort in the paper revision and the additional BNN experiments. I think the idea of using h-SVGD to prevent variance collapse, while not new, is thoroughly invested in this work. At the same time, I'm not fully convinced that using h-SVGD is always better than SVGD. I think one main disadvantage is that there are more hyperparameters (choices of two kernels, kernel parameters, and scaling parameter $f(d)$), and better heuristics on how to choose these parameters are missing. As such, I would like to keep my current score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605390201,
                "cdate": 1700605390201,
                "tmdate": 1700605390201,
                "mdate": 1700605390201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QamEia6EWB",
            "forum": "cbu1lIYQ19",
            "replyto": "cbu1lIYQ19",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2183/Reviewer_pUod"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2183/Reviewer_pUod"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a theoretical justification for using h-SVGD, a variant of the Stein variational gradient descent (SVGD) method in which different kernels are used for the gradient term and the repulsive terms. The authors show that this method can mitigate the variance collapse problem without extra computational cost while remaining competitive to standard SVGD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The background section surveys the relevant studies and concepts for this paper well.\n* The theoretical results in this paper seem to be novel and may be relevant for the community. \n* The authors indeed demonstrate that the variance collapse phenomena is reduced to some extent according to the proposed metric."
                },
                "weaknesses": {
                    "value": "* The paper focuses on h-SVGD, which is fine, but I am not convinced about the impact of this SVGD variant. The empirical results in this paper do not show a conclusive advantage for preferring this method over the standard SVGD, and the same applies to the original paper by D\u2019Angelo et al., (2021).  \n* Following the last point, although the scope of the paper is to provide a theoretical ground for h-SVGD, perhaps it will have a stronger contribution if the authors would clearly state (and evaluate) families of valid kernels for the repulsive term. \n* I find it odd that the test log-likelihood is not correlative with the dimension averaged marginal variance. If indeed the particles are more diverse with h-SVGD then I expected that it will be reflected in a better test log-likelihood.\n* The method section is not written clearly enough in my opinion. Specifically, the authors can provide better intuition for some of the results. Also, perhaps the authors should present only the main claims in the main text and provide a proof sketch for them."
                },
                "questions": {
                    "value": "* In D\u2019Angelo et al., (2021) the authors used a different kernel for the repulsive term from the ones used in this paper. Is there something in the theory that does not apply on their kernel? It may be interesting to evaluate the performance and variance shrinkage of that kernel as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2183/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2183/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2183/Reviewer_pUod"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680541162,
            "cdate": 1698680541162,
            "tmdate": 1700639250622,
            "mdate": 1700639250622,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dMUaXERX8p",
                "forum": "cbu1lIYQ19",
                "replyto": "QamEia6EWB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time in providing their review and feedback.\n\n**Weaknesses:**\n\n1. Significance of h-SVGD: Please see our Main Response for full details. We have now improved our presentation of our empirical results in Section 5 to more clearly demonstrate that h-SVGD has similar performance to SVGD in terms of test RMSE and test log-likelihood, but improved (or equivalent) variance estimation. This is now most clearly seen in Figure 1, revised Figure 2, and new Figure 5 in Appendix D, which summarises performance over 9 kernel variants. \n\n2. Evaluation of kernel families:\nThank you for this welcome suggestion. We have now added results using additional different kernel families (9 variants in total) for the repulsive kernel to Appendix D, including the kernel adopted in D'Angelo et al. (2021). The theory still applies to this latter kernel choice; in particular, the conditions for the descent lemma (Theorem 4.2) are satisfied, as detailed in Appendix D. \n\n3. Log likelihood not correlated with dimension averaged marginal variance: \nTo our understanding, log-likelihood is a measure of test accuracy. Better variance representation doesn't necessarily mean that log-likelihood is improved.\n\n4. Better intuition:\nWe have now included some additional commentary in Section 4 to provide more intuition on the results. In particular, we note Remark 1 and Remark 2 (which has now been moved slightly earlier in the paper) for some intuition on the main result (Theorem 4.1). Further discussion has also been added around Theorem 4.2 on how the KL divergence will always decrease under h-SVGD.\n\n**Questions:**\n\n1. D'Angelo et al. (2021)'s kernel: As requested we have now included simulations using this kernel for the repulsive term, as outlined above."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475960142,
                "cdate": 1700475960142,
                "tmdate": 1700475960142,
                "mdate": 1700475960142,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UKTSIgtcC6",
                "forum": "cbu1lIYQ19",
                "replyto": "dMUaXERX8p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2183/Reviewer_pUod"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2183/Reviewer_pUod"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their answers. I believe the authors addressed most of the concerns raised by me and other reviewers. I am still not convinced about the impact of this SVGD variant. Nevertheless, I acknowledge the value of this paper in terms of its theoretical contribution to the field and that it may help future studies, either theoretic or algorithmic ones. Therefore, I decided to raise my score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639220265,
                "cdate": 1700639220265,
                "tmdate": 1700639220265,
                "mdate": 1700639220265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "skeDroEXwb",
            "forum": "cbu1lIYQ19",
            "replyto": "cbu1lIYQ19",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2183/Reviewer_WGpG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2183/Reviewer_WGpG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a theoretical framework for Stein variational gradient descent with hybrid kernels in drift and repulsive terms. This paper mainly leverages the tools from the previous work to analyse the meaning of descent direction in SVGD, large time asymptotics, large particle limits and its gradient flow form. Empirically, the author conduct one synthetic and one Bayesian neural network."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a theoretical framework for hybrid kernel SVGD. By leveraging the tools from previous work, the analysis is extensive. If the reader is familiar with the Stein discrepancy, the presentation is clear. Originality is not the strongest selling point of this paper, since the theoretical analysis follows from the previous work and extend the previous analysis to the hybrid kernel space, but it is still good to see the hybrid kernel trick has a proper underlying theory associated with it."
                },
                "weaknesses": {
                    "value": "My primary concern pertains to the apparent significance of the hybrid kernel approach, as presented in the paper. The paper suggests that the hybrid kernel is proposed as a solution to circumvent the issue of variance collapsing. Nonetheless, it should be noted that there are numerous preceding studies such as S-SVGD, Grassman SVGD, among others, addressing similar challenges. Some of these methods have successfully established a proper goodness-of-fit test, ensuring that the resultant discrepancy is a valid one.\nDespite this, I observed a lack of empirical evidence showcasing the hybrid kernel approach\u2019s advantages over these established methods. In light of this, could you please elucidate on the specific benefits and improvements of the hybrid kernel approach, be it from a theoretical or empirical standpoint?\n\nMy second concern revolves around the convergence properties of the h-SVGD algorithm. The manuscript demonstrates that the descent magnitude is h-KSD, which, as acknowledged, is not a proper discrepancy. This raises questions regarding the algorithm\u2019s capability to minimize the KL divergence effectively, specifically, whether it can drive the KL divergence to zero. A descent magnitude (h-KSD) of zero does not implies that the distributions are equal or that the KL divergence has been minimized to zero.\nThis brings us back to the previous point on the need for the hybrid kernel approach\u2019s advantages. It is good to understand how h-SVGD, with its unique convergence characteristics, stands out amidst other existing methodologies addressing similar issues."
                },
                "questions": {
                    "value": "1. For theorem 4.1, how do you ensure the $H_1 \\cap H_2$ is not empty?\n2. From the experiment 5.1, it seems that the variance still collapses but at a slower speed. But from the plot in S-SVGD or GSVGD paper, the variance estimation does not drop at $d=100$. So what is the advantages of the hybrid approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773053420,
            "cdate": 1698773053420,
            "tmdate": 1699636151804,
            "mdate": 1699636151804,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RJnIUrsx9t",
                "forum": "cbu1lIYQ19",
                "replyto": "skeDroEXwb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time in providing their review and feedback.\n\n**Weaknesses:**\n\n1. Significance of h-SVGD: Please see our Main Response for full details. We have now improved our presentation of our empirical results in Section 5 to more clearly demonstrate that h-SVGD has similar performance to SVGD in terms of test RMSE and test log-likelihood, but improved (or equivalent) variance estimation. This is now most clearly seen in Figure 1, revised Figure 2, and new Figure 5 in Appendix D, which summarises performance over 9 kernel variants. In addition, at the start of Section 5 we now comment that other previous methods (such as GSVGD and S-SVGD) also tackle variance collapse, but they do so at a far greater computational cost. In particular, S-SVGD requires additional computation of the optimal test directions and GSVGD requires additional computation at each step to update the projectors. The advantage of h-SVGD is that there is no added computational cost over regular SVGD.\n\n2. Convergence properties of h-SVGD:\nThe question is about whether h-SVGD can minimise the KL divergence effectively: see our Main Response for full details. As noted, the descent magnitude is h-KSD (Theorem 4.1), which is not a proper discrepancy. However, the descent lemma (Theorem 4.2) bounds the decrease in KL divergence in terms of a proper discrepancy, namely the KSD of one of the kernels. We have now included further details in Section 4.3 that show for proper choice of step size, the KL divergence is strictly decreasing at all times, meaning that the h-SVGD algorithm avoids cases where $\\mathbb S_{k_1,k_2}(\\mu_\\ell^\\infty,\\nu_p)=0$ but $\\mu_\\ell^\\infty$ and $\\nu_p$ are not equal almost everywhere.\n\n**Questions:**\n\n1. Remark 2 mentions that either $\\mathcal H_1 \\subseteq \\mathcal H_2$ or $\\mathcal H_2 \\subseteq \\mathcal H_1$ for many common choices of kernel (e.g. RBF, IMQ, log-inverse, or Mat\u00e9rn). This includes when $k_1$ and $k_2$ are from different families mentioned above. So the intersection $\\mathcal H_1 \\cap \\mathcal H_2$ will be either $\\mathcal H_1$ or $\\mathcal H_2$. We are not aware of applications in the SVGD literature that use kernels other than those listed above. Remark 2 has been reworded slightly and moved so that it appears just after Theorem 4.1.\n\n2. As mentioned in response to the weaknesses above, the advantage of h-SVGD is that it alleviates the variance collapse at no additional computational cost, whereas S-SVGD and GSVGD require additional computations of the optimal test direction or the projectors at each step."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475909169,
                "cdate": 1700475909169,
                "tmdate": 1700476213938,
                "mdate": 1700476213938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gGwcZDhkZY",
                "forum": "cbu1lIYQ19",
                "replyto": "RJnIUrsx9t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2183/Reviewer_WGpG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2183/Reviewer_WGpG"
                ],
                "content": {
                    "comment": {
                        "value": "**Additional computation cost of GSVGD and S-SVGD**: Yes, those methods require additional costs. However, if those method can obtain better performance or variance estimation, sometimes it is affordable to have this additional cost. That is why I want to see the performance comparison. At for drawing samples from simple Gaussian distribution, it seems that GSVGD and S-SGVD have more stable variance estimation?\n\n**Convergence properties**: I am still a bit confused here. The descent magnitude is not a proper discrepancy, right? So if the magnitude is 0, that means the distribution is no longer moving, but it does not mean it is equal to the target distribution. But from the descent lemma, you claim to recover the target distribution. So where is the discrepancy, do I misunderstand something?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493402351,
                "cdate": 1700493402351,
                "tmdate": 1700493402351,
                "mdate": 1700493402351,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ph54mfgVy1",
            "forum": "cbu1lIYQ19",
            "replyto": "cbu1lIYQ19",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2183/Reviewer_5E3e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2183/Reviewer_5E3e"
            ],
            "content": {
                "summary": {
                    "value": "A hybrid kernel variant of SVGD is theoretically analysed in this paper. By defining a hybrid Stein operator and, subsequently, h-KSD, they prove that (1) the h-SVGD update direction is optimal within an appropriate RKHS, (2) h-SVGD guarantees a decrease in the KL divergence at each step and (3) other limit results. Experimentally, h-SVGD also mitigates the crucial variance collapse of SVGD algorithms at no additional cost and is shown to be competitive with other SVGD methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- h-SVGD has previously been proposed heuristically by D'Angelo et al. (2021). This paper provides a theoretical analysis of h-SVGD, which is a significant contribution to the literature: both the optimal update direction and the KL divergence decrease are important theoretical results for any new SVGD algorithm.\n- The large time asymptotics of h-SVGD are analysed, showing that h-SVGD always decreases the KL and converges to the true posterior in the limit. \n- Seemingly technical theoretical results are given adequate intuition and explanation, making the paper accessible to a wide audience, including applied users of SVGD algorithms.\n- Most SVGD algorithms suffer from variance collapse, which is a significant issue in practice. Some results show h-SVGD is shown to mitigate this issue, which would be a significant practical contribution."
                },
                "weaknesses": {
                    "value": "- Despite rigorous theoretical results, the experimental results are not sufficient to show that it mitigates the variance collapse issue better than previous methods (e.g. S-SVGD and G-SVGD). For (2), it would be useful to study the variance collapse issue with inference tasks in higher dimensions in comparison to previous approaches, such as the experiments in [1], as this is mainly an issue that arises in large dimensions."
                },
                "questions": {
                    "value": "- What is the computational cost of h-SVGD compared to SVGD? Is it the same or more expensive?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2183/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2183/Reviewer_5E3e",
                        "ICLR.cc/2024/Conference/Submission2183/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2183/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784999233,
            "cdate": 1698784999233,
            "tmdate": 1701044551848,
            "mdate": 1701044551848,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ap4DwP42fX",
                "forum": "cbu1lIYQ19",
                "replyto": "ph54mfgVy1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2183/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2183/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time in providing their review and feedback.\n\n**Weaknesses:**\n\n1. Strong experimental results for h-SVGD: Please see our Main Response for full details. However, note that the mitigation of variance collapse in high dimensions is demonstrated in Section 5.1 and Figure 1 (a-c), which shows a significant improvement in estimating the true variance in high dimensions over SVGD, even when the number of dimensions is greater than the number of particles ($\\gamma>1$). Also, revised Figure 2 now more clearly demonstrates an increase in DAMV for the BNN example, with consistent performance in test RMS and log-likelihood. Please also see new Figure 5 in Appendix D for additional qualitatively similar results with other kernel choices (9 kernel variations). Overall, the general performance of h-SVGD is comparable with SVGD but with the advantage that h-SVGD has superior performance in mitigating variance collapse.\n\n2. While the primary aim of this paper is to provide theoretical justification of h-SVGD, we recognise that it is useful to support this by also demonstrating the performance of h-SVGD in areas that are directly related to this theoretical development; namely performance with regard to different families of kernels. This has now been done, and has been implemented in Appendix D. We feel that comparison to other competitor methods (such as GSVGD and S-SVGD) is somewhat adjacent to the focus of this paper, and is more relevant for research that makes algorithmic contributions in this field.\n\n**Questions:**\n\n1. In terms of computational costs: the cost of updating the particles for both SVGD and h-SVGD is $O(N^2)$ at each step. We have added this statement at the beginning of Section 5. The SVGD update can be slightly faster when $k(x,y)$ can be reused to compute $\\nabla k(x,y)$ (this applies to the RBF kernel for example), but the order is still $O(N^2)$."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2183/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475863805,
                "cdate": 1700475863805,
                "tmdate": 1700475863805,
                "mdate": 1700475863805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]