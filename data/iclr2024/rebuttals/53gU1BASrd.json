[
    {
        "title": "Evaluating and Finetuning Models For Financial Time Series Forecasting"
    },
    {
        "review": {
            "id": "Rnc7GHzdGx",
            "forum": "53gU1BASrd",
            "replyto": "53gU1BASrd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6351/Reviewer_BmfC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6351/Reviewer_BmfC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed to evaluate the financial time series forecasting models by deciding which assets to buy and sell, i.e., form a portfolio, according to the prediction, rather than comparing the errors between predictions and ground-truths. The authors collected a dataset from S&P500 and CAC40, and evaluated several forecasting models (e.g., Scinet and DARNN) using the proposed evaluation pipeline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* In general, the idea of indirect comparison, through the proxy of portfolio optimisation and backtesting, is an interesting alternative to the direct comparison, i.e., on prediction errors.\n\n* The optimisation over Sharpe ratio is reasonable choice under the context.\n\n* The authors found that \"LAST\", the very simple baseline of always predicting the next value as its current one, is surprisingly hard to beat, which was often ignored in previous works."
                },
                "weaknesses": {
                    "value": "* It remains unclear how the datasets were collected. The authors claimed that \"we cleverly used web scrapping tools\" and the data sources are unknown thus the quality of data might be questionable.\n\n* The transaction cost is not discussed in this paper, so the metrics of portfolio may not be meaningful as the impact of transaction cost is significant, given the reasonably frequent rebalance."
                },
                "questions": {
                    "value": "The objective function in (1) does not look like a trivial problem, esp. given the non-negative and sum-to-one constraints, what was the optimiser used in this paper? did it converge?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685241770,
            "cdate": 1698685241770,
            "tmdate": 1699636700290,
            "mdate": 1699636700290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yGE8N9y2gI",
                "forum": "53gU1BASrd",
                "replyto": "Rnc7GHzdGx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "General Note:\n\nWe thank the reviewers for their valuable feedback. We hope to address their concerns in this rebuttal.\n\nBefore answering each concern, we want to recall our contribution and why it is essential.\n\nIt is well known that constructing good models is hard for finance professionals. It is tough to beat simple baselines. When we studied how deep learning forecasting methods can be applied to our problem, we were surprised to notice that many papers have been published on the topic, all claiming to beat previous state-of-the-art. These papers were published in top conferences and had a significant impact on the community (SciNet, NeurIPS 2022, 66 citations, DARNN, IJCAI 2017, 1200 citations, N-BEAT, ICLR 2020, 750 citations, N-HITS, AAAI 2023, 100 citations, ...). When we tried to reproduce the results, we could not beat trivial random baselines, which was very concerning. Therefore, we decided to compare financial forecasting models in a unified way (on the same dataset) and still observed worse than random performance. We tested two hypotheses that might break this barrier.\n\n1) We increased the training dataset size (usually, the more data, the better). We will provide a script to reconstruct our dataset and reproduce all our results.\n\n2) We increased the model size (which should be possible thanks to the first point). Because training is costly, we only tested the impact of size on the Transformers architecture.\n\nBoth these hypotheses were wrong, so the problem was more profound. Our investigations led us to a flaw in the evaluation process, and we proposed a new approach to evaluate financial forecasting models. With this new pipeline, we had encouraging results that will lead to thrilling future works.\n\nWe think our contribution is worthwhile. It seems obvious now that we exposed the problem and connected the dots. However, previous papers published at top conferences (including ICLR, NeurIPS, and AAAI), reviewed by the community and cited by many works, did not correctly evaluate their results. Nobody noticed before. Publishing complex models without a good evaluation seems useless to us.\n\nIf the reviewers still want to reject our paper after this rebuttal, we would like their input on the following question: Is it worth spending time to better understand already published models and why they perform worse than random in certain cases? If yes, what is missing in our paper to change how financial forecasting models are evaluated?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583036957,
                "cdate": 1700583036957,
                "tmdate": 1700583036957,
                "mdate": 1700583036957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1eJdyrkETt",
            "forum": "53gU1BASrd",
            "replyto": "53gU1BASrd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6351/Reviewer_Zb2D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6351/Reviewer_Zb2D"
            ],
            "content": {
                "summary": {
                    "value": "This article points out the existing imperfection of the evaluation pipeline used for financial time series forecasting. From this observation, the article explores the specific reason hidden in this phenomenon and put forward a brand-new evaluation pipeline based on portfolio construction, whose main idea lies at mapping the predictions of each model to an investment strategy. Based on this new evaluation pipeline, the authors' team test many baseline models' effect in financial time series forecasting task.\nThe contributions of this article contains:\n1. This article introduce a new evaluation pipeline better suited for financial time series.\n2. This article compare state-of-the-art deep learning methods for financial time series forecasting based on the brand-new evaluation pipeline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. originality: This article is innovative, figuring out the existing problem in evaluation pipeline of financial time series forecasting\uff0cand putting forward a brand-new angle to evaluate the effect.\n2. quality: The logical chain of this article is relatively complete, from definition of financial time series forecasting problem, the methodology of experimenting, to the final result of their experiments and their conclusions.\n3. clarity: The article is relatively clear\uff0cbut the meaning of many variables involved in formulas are not mentioned, which confused me a lot.\n4. significance: The contributions of this article are not significant. On the one hand, the evaluation pipeline put forward in this article only works for financial time series forecasting, which is not general. On the other hand, this article claim they are training a general model which could be used for many specific tasks, but they didn't list any experiment result about this \"general model\"."
                },
                "weaknesses": {
                    "value": "1. The meaning of many variables involved in formulas are not mentioned. For example, the $P_t$ put forward in section 3, which should be relevant with time step variant $t$, but the formula does not contain $t$.\n2. This passage is aimed to address the existing problem in the evaluation pipeline of financial time series forecasting, which means one of this passage's key point is putting forward the existing problem, but the problem is mentioned in a paragraph in section 6.1 \" FORECASTING EVALUATION RESULTS\". It's too \"convert\" to figure out, making it difficult for me to grasp the logical lines of the entire article. \n3. The author mentioned that one of the contribution of this article is \"We train large models on a large dataset\"\uff0cand \"We show that these models can be used to solve more specific tasks\". But he did not list the experiment result of these \"general models\" trained by his team\uff0cand there was no experiment result that could prove their general models can be used to solve more specific tasks."
                },
                "questions": {
                    "value": "1. About the definition of $R^i_j$ which is put forward in section 4.4\uff0cthe formula to calculate $R^i_j$ seems irrelevant with $j$\uff0cwhat's the meaning of $j$ \uff1f\n2. About the definition of $P_t$ (the value of a portfolio at time t) which is put forward in section 3\uff0cthe formula to calculate $P_t$ seems irrelevant with time step $t$\uff0cdoes the time step $t$ influence $x^i_k$\uff0cor $w_i$ \uff1f\n3. In the experiment, why not control the scale(the number of parameters) of baseline models\uff1fIf the  scales of baseline models are not consistent\uff0care the result convincing\uff1f"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823136922,
            "cdate": 1698823136922,
            "tmdate": 1699636700160,
            "mdate": 1699636700160,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DGMZ3KOSAB",
                "forum": "53gU1BASrd",
                "replyto": "1eJdyrkETt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "General Note:\n\nWe thank the reviewers for their valuable feedback. We hope to address their concerns in this rebuttal.\n\nBefore answering each concern, we want to recall our contribution and why it is essential.\n\nIt is well known that constructing good models is hard for finance professionals. It is tough to beat simple baselines. When we studied how deep learning forecasting methods can be applied to our problem, we were surprised to notice that many papers have been published on the topic, all claiming to beat previous state-of-the-art. These papers were published in top conferences and had a significant impact on the community (SciNet, NeurIPS 2022, 66 citations, DARNN, IJCAI 2017, 1200 citations, N-BEAT, ICLR 2020, 750 citations, N-HITS, AAAI 2023, 100 citations, ...). When we tried to reproduce the results, we could not beat trivial random baselines, which was very concerning. Therefore, we decided to compare financial forecasting models in a unified way (on the same dataset) and still observed worse than random performance. We tested two hypotheses that might break this barrier.\n\n1) We increased the training dataset size (usually, the more data, the better). We will provide a script to reconstruct our dataset and reproduce all our results.\n\n2) We increased the model size (which should be possible thanks to the first point). Because training is costly, we only tested the impact of size on the Transformers architecture.\n\nBoth these hypotheses were wrong, so the problem was more profound. Our investigations led us to a flaw in the evaluation process, and we proposed a new approach to evaluate financial forecasting models. With this new pipeline, we had encouraging results that will lead to thrilling future works.\n\nWe think our contribution is worthwhile. It seems obvious now that we exposed the problem and connected the dots. However, previous papers published at top conferences (including ICLR, NeurIPS, and AAAI), reviewed by the community and cited by many works, did not correctly evaluate their results. Nobody noticed before. Publishing complex models without a good evaluation seems useless to us.\n\nIf the reviewers still want to reject our paper after this rebuttal, we would like their input on the following question: Is it worth spending time to better understand already published models and why they perform worse than random in certain cases? If yes, what is missing in our paper to change how financial forecasting models are evaluated?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582934265,
                "cdate": 1700582934265,
                "tmdate": 1700582934265,
                "mdate": 1700582934265,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OMnNdWr6l4",
            "forum": "53gU1BASrd",
            "replyto": "53gU1BASrd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6351/Reviewer_9z9f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6351/Reviewer_9z9f"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors conduct a benchmarking study on deep learning applied to financial time series forecasting. \n\n\nThe authors construct a dataset of financial time series, including the price and volume histories of various stocks, options, etc.\nOn this dataset, the authors pre-train several relevant deep learning (DL) approaches. These models are then fine-tuned to forecast the price and volume of stocks in the S&P500 and CAC40 indices. The authors demonstrate that a na\u00efve baseline outperforms DL approaches on their selected tasks when evaluated using standard forecasting metrics. To address this, they propose an evaluating performance based on the returns and risks associated with using the DL approaches for portfolio management. They find that DL tends to perform better than the baseline under this evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The experimental evaluation is thorough, although the authors are encouraged to more completely describe their methodology.\n- The authors provide convincing empirical evidence that DL methods struggle to beat baseline methods."
                },
                "weaknesses": {
                    "value": "- Although the evaluation of Scinet and DA-RNN on finance data is novel as far as I am aware, the benchmarking of DL on financial data is not novel. For example, [1] evaluates Transformers on several indexes, and also considers the risk/return on trading strategies based on DL forecasts. As the authors point out, their benchmarking submission also does not include comparison to other families of time series forecasting methods.\n\n- While the authors claim to collect a comprehensive dataset, no description is given other than the number of samples. This makes evaluation of the significance and quality of their dataset difficult. Furthermore, while a complete collection of financial price histories could be beneficial to the community, it is ultimately not difficult to assemble publicly available historical price data, making this contribution limited.\n\n\n- I disagree with the author's characterization that the na\u00efve or LAST baseline (predicting the last known point) has been ignored in forecasting of financial time series. See [2], where a widely used textbook states that the na\u00efve approach works well in financial data. Furthermore, the authors should consider the existence of metrics such as the Mean Absolute Scaled Error (MASE), which compares forecast performance against the na\u00efve one-step forecast model. The M3/M4 papers as cited in this submission also perform evaluation against the na\u00efve baseline. While individual papers in the financial forecasting literature (such as [1]) do fail to make adequate comparison, I believe the authors claims that this is a systemic issue needs further support.\n\n\n- The authors are encouraged to add more clarity in the writing of this submission. While the flow and core narrative of this submission is clear, it often does not contain enough detail for critical evaluation. Also, table 3 appears to exceed the allowable margins.\n\n\n[1] Wang, Chaojie, et al. \"Stock market index prediction using deep Transformer model.\" Expert Systems with Applications 208 (2022): 118128.\n\n[2] https://otexts.com/fpp2/simple-methods.html#na%C3%AFve-method"
                },
                "questions": {
                    "value": "- The submission would benefit from a brief survey of any existing approaches which evaluate forecasting models based on portfolio performance. Given that the expected return, risk and Sharpe ratio are widely used metrics, it would be beneficial to understand the novelty of the portfolio evaluation proposed in this submission.\n\n- Any rebuttal to the above weaknesses would be appreciated."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6351/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6351/Reviewer_9z9f",
                        "ICLR.cc/2024/Conference/Submission6351/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698974271164,
            "cdate": 1698974271164,
            "tmdate": 1700603964078,
            "mdate": 1700603964078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FZ34YLzCDp",
                "forum": "53gU1BASrd",
                "replyto": "OMnNdWr6l4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "General Note:\n\nWe thank the reviewers for their valuable feedback. We hope to address their concerns in this rebuttal.\n\nBefore answering each concern, we want to recall our contribution and why it is essential.\n\nIt is well known that constructing good models is hard for finance professionals. It is tough to beat simple baselines. When we studied how deep learning forecasting methods can be applied to our problem, we were surprised to notice that many papers have been published on the topic, all claiming to beat previous state-of-the-art. These papers were published in top conferences and had a significant impact on the community (SciNet, NeurIPS 2022, 66 citations, DARNN, IJCAI 2017, 1200 citations, N-BEAT, ICLR 2020, 750 citations, N-HITS, AAAI 2023, 100 citations, ...). When we tried to reproduce the results, we could not beat trivial random baselines, which was very concerning. Therefore, we decided to compare financial forecasting models in a unified way (on the same dataset) and still observed worse than random performance. We tested two hypotheses that might break this barrier.\n\n1) We increased the training dataset size (usually, the more data, the better). We will provide a script to reconstruct our dataset and reproduce all our results.\n\n2) We increased the model size (which should be possible thanks to the first point). Because training is costly, we only tested the impact of size on the Transformers architecture.\n\nBoth these hypotheses were wrong, so the problem was more profound. Our investigations led us to a flaw in the evaluation process, and we proposed a new approach to evaluate financial forecasting models. With this new pipeline, we had encouraging results that will lead to thrilling future works.\n\nWe think our contribution is worthwhile. It seems obvious now that we exposed the problem and connected the dots. However, previous papers published at top conferences (including ICLR, NeurIPS, and AAAI), reviewed by the community and cited by many works, did not correctly evaluate their results. Nobody noticed before. Publishing complex models without a good evaluation seems useless to us.\n\nIf the reviewers still want to reject our paper after this rebuttal, we would like their input on the following question: Is it worth spending time to better understand already published models and why they perform worse than random in certain cases? If yes, what is missing in our paper to change how financial forecasting models are evaluated?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582795014,
                "cdate": 1700582795014,
                "tmdate": 1700582795014,
                "mdate": 1700582795014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iVQVLg4iUq",
                "forum": "53gU1BASrd",
                "replyto": "OMnNdWr6l4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6351/Reviewer_9z9f"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6351/Reviewer_9z9f"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to first thank the authors for their rebuttal.\n\nOverall, the author's point on the consistency of forecasting evaluation is valid. However, solely based on content in this submission, it is hard to understand whether this is a problem with individual papers, or a systemic issue in the community. Though individual papers fail to make comparisons against good baselines, this is not the case for all papers. For example, while papers such as N-BEATS or N-HITS don't make comparison against LAST, they do compare against other simple statistical baselines. As previously pointed out, the M3/M4 papers, while not specifically investigating finance, do make exact comparisons to the LAST baseline. I believe a more comprehensive related works section and expanding the experimental section to compare against N-BEATS / N-HITS and other papers could better support the position of this submission. \n\nUltimately, while I have increased my score due to the rebuttal, I believe this paper makes a large (though potentially true) claim that is not fully support in the manuscript due to a lack of in-depth textual and experimental comparison against existing works and their shortcomings.\n\nSpecific points:\n- While the technical details may not be \"interesting\" for the main text, a description of the process would have allowed the reviewers to independently evaluate one of the claimed key contributions of this submission. Thank you for offering to add this to the technical report or code in the supplementary, I believe this will strengthen the manuscript.\n\n- Thank you for outlining the issues with the example citation. As above, I believe the comparison of this submission against others papers that apply DL to finance should be expanded in the related works section. This would better convince readers of the shortcomings of existing literature."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603932781,
                "cdate": 1700603932781,
                "tmdate": 1700604019621,
                "mdate": 1700604019621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iTIWu3Mmpv",
            "forum": "53gU1BASrd",
            "replyto": "53gU1BASrd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6351/Reviewer_pP2m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6351/Reviewer_pP2m"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose model construction pipeline to be used to benchmark methods for returns forecasting and portfolio construction in financial applications. The author also proposes a procedure to transfer pre-trained models onto smaller datasets and fine tuned for trading applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Given the diversity of time series datasets, having access to a high quality specialist dataset for finance can be useful. Knowledge of standardised approaches to evaluate and benchmark both forecasting and portfolio construction methods can be useful for practitioners."
                },
                "weaknesses": {
                    "value": "While the paper does put together a sequence of standardised techniques (for evaluation and for transfer learning), it fails to 1) concretely demonstrate what novel methods have been proposed and 2) why existing methods are insufficient. \n\nOn the evaluation front, numerous papers have been proposed to evaluate machine learning-based trading strategies (see references for both forecasting and portfolio construction), including 1) which benchmarks a variety of techniques in a standardised fashion. All of which have not been referenced by authors.\n\nFurthermore, in contrast to claims that standardised datasets are lacking -- numerous open-source financial datasets can be found, and a list has been supplied below. The authors themselves do not open source their dataset (citing legal reasons that prevent publication), which run slightly contrary to the goal of developing a common framework for benchmarking.\n\nIn addition, transfer learning in the financial domain is also not a novel idea, and comparisons to previous works are absolutely required.\n\nReferences\n--\n1. Gu et al 2020. Empirical Asset Pricing via Machine Learning, The Review of Financial Studies, Volume 33, Issue 5\n2. Poh et al 2021. Building Cross-Sectional Systematic Strategies by Learning to Rank. The Journal of Financial Data Science Spring 2021.\n3. Marcos Lopez de Prado 2016. Building Diversified Portfolios that Outperform Out-of-sample. Journal of Portfolio Management.\n2. Koshiyama et al 2022. QuantNet: transferring learning across trading strategies, Quantitative Finance, 22:6, 1071-1090\n\n\nKaggle competitions/ datasets\n---\n1. M6 competition -- https://github.com/Mcompetitions/M6-methods\n2. G-Research Crypto Forecasting -- https://www.kaggle.com/competitions/g-research-crypto-forecasting\n3. JPX Tokyo Stock exchange Prediction -- https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction\n4. Kaggle sample NASDAQ dataset -- https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset"
                },
                "questions": {
                    "value": "1. Why is evaluation only performed on one day or one week of data? Most finanical papers test strategies over multiple years.\n2. Why is the proposed pipeline superior to existing methods for evaluating trading strategies?\n3. Is MSE benchmarked with regards to price forecasts (as seen from LAST)? Would returns or price change forecasts (with naive benchmark being returns=0) be more suitable approach given the non-stationarity of price data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699361648119,
            "cdate": 1699361648119,
            "tmdate": 1699636699946,
            "mdate": 1699636699946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K5MJ5CxMO9",
                "forum": "53gU1BASrd",
                "replyto": "iTIWu3Mmpv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "General Note:\n\nWe thank the reviewers for their valuable feedback. We hope to address their concerns in this rebuttal.\n\nBefore answering each concern, we want to recall our contribution and why it is essential.\n\nIt is well known that constructing good models is hard for finance professionals. It is tough to beat simple baselines. When we studied how deep learning forecasting methods can be applied to our problem, we were surprised to notice that many papers have been published on the topic, all claiming to beat previous state-of-the-art. These papers were published in top conferences and had a significant impact on the community (SciNet, NeurIPS 2022, 66 citations, DARNN, IJCAI 2017, 1200 citations, N-BEAT, ICLR 2020, 750 citations, N-HITS, AAAI 2023, 100 citations, ...). When we tried to reproduce the results, we could not beat trivial random baselines, which was very concerning. Therefore, we decided to compare financial forecasting models in a unified way (on the same dataset) and still observed worse than random performance. We tested two hypotheses that might break this barrier.\n1) We increased the training dataset size (usually, the more data, the better). We will provide a script to reconstruct our dataset and reproduce all our results.\n2) We increased the model size (which should be possible thanks to the first point). Because training is costly, we only tested the impact of size on the Transformers architecture.\nBoth these hypotheses were wrong, so the problem was more profound. Our investigations led us to a flaw in the evaluation process, and we proposed a new approach to evaluate financial forecasting models. With this new pipeline, we had encouraging results that will lead to thrilling future works.\n\nWe think our contribution is worthwhile. It seems obvious now that we exposed the problem and connected the dots. However, previous papers published at top conferences (including ICLR, NeurIPS, and AAAI), reviewed by the community and cited by many works, did not correctly evaluate their results. Nobody noticed before. Publishing complex models without a good evaluation seems useless to us.\n\nIf the reviewers still want to reject our paper after this rebuttal, we would like their input on the following question: Is it worth spending time to better understand already published models and why they perform worse than random in certain cases? If yes, what is missing in our paper to change how financial forecasting models are evaluated?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582627464,
                "cdate": 1700582627464,
                "tmdate": 1700582627464,
                "mdate": 1700582627464,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V4kQUtSVNX",
                "forum": "53gU1BASrd",
                "replyto": "iTIWu3Mmpv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6351/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Rebuttal:\n\n*Question 1*: The context size is one day or one week, but the training period spreads over several years. The models limit the context size. We can have such a context because we have access to data at a low granularity. Almost all previous works have a granularity of one day, which increases the time-span of data in the context. However, as noted in the introduction, training data can quickly become obsolete. Future work remains on integrating broader time periods into low-granularity models.\n\n*Question 2*: Our goal is not to evaluate trading strategies. This is an old problem, and we did not claim to have invented anything on this topic (return, volatility, and Sharpe Ratio are standrd metrics). What we proposed in our paper is to offer a methodology to evaluate financial time series forecasting models.\n\n*Question 3*: We do not predict a price but a return (see 5.2). The transition between the representations can quickly be done.\n\nAbout the datasets cited by the reviewer, they are samples of a specific market. We never claimed these samples did not exist, but there was a lack of an extensive dataset covering more instruments, assets, markets, and granularity :\n* M6: This repository does not contain data, but, like us, a script to gather the data. The data includes 100 time series at a daily granularity. Even if the data ranges over 20 years, we get 500k data points, which is 1000 times less than us. Besides, we have a wider variety of instruments and a finer granularity (minute instead of day).\n* G-Research Crypto Forecasting: This dataset only contains crypto (we have them too). It contains 13 time series and 24 million data points at the minute granularity. This is 42 times less than us. Besides, we have a wider variety of instruments, and data about cryptocurrencies are easy to gather as they are public. Cryptocurrencies also behave differently than standard assets and FOREX.\n* JPX Tokyo Stock Exchange Prediction: This dataset contains daily prices of 4.4k Japanese companies for a total number of data points of 2.3 million. In comparison, we have 500 times more data, a wider variety of instruments, and a finer granularity (minute instead of day).\n* NASDAQ 100: This dataset is already mentioned in the paper (see Table 1)\n\nThe references given by the authors are very interesting. We will include a paragraph on non-forecasting model evaluations. In particular, [1] suggests evaluating forecasting models by building a portfolio. However, we found key differences in our approach. First, the motivation is different: They found no problems with other metrics (because they use simple models only). Second, the evaluating setting is different. Their models do not take as input time series but a few (94) manually designed features that include exogenous information (not derived from the time series alone). Each month, for 30 years, they construct a portfolio. Therefore, the testing set contains only 360 data points. Besides, the training and testing spread over 60 years. It was shown (see reference in our paper) that models have trouble adapting over such long periods of time because of changes in legislation or trading techniques. Third, the forecasting output is only used to rank the assets into deciles. In the end, a maximum of 20% of the assets are kept (and therefore, the model is not really evaluated on 80% of the assets). An external source already gives the weights of the portfolio. The forecasting model is just here to filter the assets. In our case, we propose using all the predictions from the forecasting model (and nothing else) and constructing a portfolio to maximize the Sharpe Ratio (both keeping a high return and low volatility). Therefore, we better evaluate the performance of the forecast without any external help. This approach is novel. However, we will cite this suggested paper and discuss our article's differences.\n\nFor the other papers, [2] works like [1]. [3] applies machine learning techniques (not models) to a mathematical problem but is not related to our problem. [4] is not a forecasting model; it directly predicts the weights in the portfolio, knowing the entire market. This is not the case in our paper and the previous work papers we compared. The reported Sharpe ratio scores of the baselines are very low compared to the S&P500 at the time, which is strange."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582649068,
                "cdate": 1700582649068,
                "tmdate": 1700582747514,
                "mdate": 1700582747514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]