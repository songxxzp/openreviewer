[
    {
        "title": "Improving Code Style for Accurate Code Generation"
    },
    {
        "review": {
            "id": "qlSlHg0YZw",
            "forum": "maRYffiUpI",
            "replyto": "maRYffiUpI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1668/Reviewer_DhCg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1668/Reviewer_DhCg"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates data quality for code generation and finds that making the code more structured and readable leads to improved code generation performance. The authors build a data-cleaning pipeline to transform existing programs by 1.) renaming variables, 2.) modularizing and decomposing complex code into smaller helper sub-functions, and 3.) inserting natural-language based planning annotations. Experiments on two algorithmic code generation benchmarks indicate that fine-tuning on the transformed programs improves the code generation performance compared to fine-tuning on the original dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": ". A nice idea to enhance code quality for code generation.\n\n. A series of experiments were carried out to evaluate the effectiveness of the proposed method. \n\n. The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "1. The proposed code transformations (i.e., renaming, modularizaing, and annotations) are a bit simple, and rely heavily on the capability of ChatGPT (GPT-3.5-TURBO).  It is unknown whether the proposed method can be utilized for other, more powerful LLM-based code generation systems. For example, in Section 4.2.2, the authors acknowledge that the poor performance obtained on the planning dataset may stem from the model's inability to generate accurate annotations, indicating that the effectiveness of the proposed model depends on LLMs. \n2. The authors propose two steps of data cleaning (i.e., renaming, modularizaing) to the original source code. However, the authors did not validate the quality of the transformed code (the authors only tested whether the transformed code can be consistent to the original data). For example, whether the variable names actually became clearer and more readable after cleaning, and whether the model accurately segmented the code into modules. Hence, it is unknown whether such simple transformation process can enhance the quality of the training data.\n3. As observed from the experimental results, the improvement of the proposed method could be insignificant and inconsistent. Some negative instances can occasionally be observed such as the $CL-7B + D_{modular}$ in in-context learning and the $CL-7B + D_{rename}$ in fine-tuning on the CODE-CONTESTS dataset. Unfortunately, the authors did not provide an explanation for the decline in these results, which may undermine the method's validity.\n4. Code generation could be achieved by prompting a general LLM such as ChatGPT directly as well. Also, it has been found that by improving the prompts, the code generation performance can be improved. The authors may discuss this approach to accurate code generation: Liu et al., Improving ChatGPT Prompt for Code Generation, https://arxiv.org/abs/2305.08360"
                },
                "questions": {
                    "value": ". How is the quality of the transformed code? \n\n. Does the effectiveness of the proposed model depend on LLMs such as ChatGPT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698657574119,
            "cdate": 1698657574119,
            "tmdate": 1699636094914,
            "mdate": 1699636094914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WjQuS8bIkU",
                "forum": "maRYffiUpI",
                "replyto": "qlSlHg0YZw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer DhCg - Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and feedback.\n\n> The proposed code transformations \u2026 rely heavily on the capability of ChatGPT (GPT-3.5-TURBO). It is unknown whether the proposed method can be utilized for other, more powerful LLM-based code generation systems. \n\n\nWe acknowledge the reviewer's point that the efficacy of our code transformation approach is influenced by the underlying language model employed. We selected GPT-3.5-Turbo for its balance between transformation accuracy and operational cost. To substantiate the versatility of our method, we have conducted an additional ablation study by substituting GPT-3.5-Turbo with the more recent GPT-4-Turbo model. Owing to the significantly higher cost of GPT-4-Turbo (approximately 8-10 times higher), we use a subset of the original dataset and refer to the modularized version as D_Modularize_. To ensure comparability, we analyzed the same subset of original and transformed programs for both the base and modularized datasets. Further details on the experimental setup can be found in Appendix Section C.2.\n\n\nThe table below illustrates that the D_Modularize_4 dataset yields improved performance over the D_Modularize dataset presented in the main paper. This enhancement not only demonstrates the adaptability but also the potential improvements of our approach using stronger models.\n\n|                           | Pass@1   | Pass@10  | Pass@25 |\n|---------------------------|----------|----------|---------|\n| CL-7B+Base                | 16.3     | 31.6     | 37.6    |\n| CL-7B+D_Modularize        | 18.8     | 33.0     | 38.2    |\n| CL-7B+D_Modularize_4      | **19.4**   | **34.3**   | **40.0**  |\n\n\n\n> The authors propose two steps of data cleaning (i.e., renaming, and modularizing) to the original source code. However, the authors did not validate the quality of the transformed code (the authors only tested whether the transformed code can be consistent to the original data)....\n\nWe appreciate the reviewer's emphasis on the importance of quantifying code quality. We have conducted a thorough analysis of the transformed dataset, as detailed in Section 4.1, where we offer both qualitative and quantitative evidence of quality enhancement. Below, we summarize the insights from Section 4.1 that pertain to code *quality*:\n\n- Post-transformation, the median number of new functions introduced by the modularization process is three. This indicates that the modularization procedure actively decomposes existing programs to insert new, potentially more coherent functions.\n- The helper functions created during modularization typically embody standard algorithms frequently encountered in competitive programming, such as `dfs`, `build_graph`, `gcd`, `dp`, and `binary_search`. This suggests that the new helper functions are meaningful and positively contribute to the overall code.\n- A qualitative analysis of approximately 100 programs, including successful and unsuccessful transformations is presented with various success and failure modes detailed.\n\nMoreover, we posit that the quality of the dataset can be indirectly inferred from the improved performance of models trained on it. The performance gains achieved through our modularization approach satisfy this measure of quality.\n\nBased on reviewers\u2019 concerns, we have additionally performed an analysis of the generated dataset using GPT-4 as a judge, a method gaining traction for assessing outputs from free-form language models [1,2]. While this approach might have subtle limitations, it still offers valuable insights to address the concerns raised. We tasked GPT-4 to quantitatively assess the transformed programs in regards to the meaningfulness of the transformations and consistency of original and transformed programs (i.e. following the same program logic and structure). Specifically, we asked GPT-4 to rate the transformation on \n\n- variable names (rating between 1-3 corresponding to not helpful, somewhat helpful, very helpful)\n- function decomposition (rating between 1-3 corresponding to not helpful, somewhat helpful, very helpful)\n- program consistency (rating between 0-1 corresponding to inconsistent and consistent)\n\nAppendix C.1 provides prompts used for GPT-4 jude.  Analysis of about 1000 programs reveals that GPT-4 regards 99% of the transformations as helpful (of which only 3-5% of examples can judged as *can do better*). Similarly, over 99% of the transformed programs are judged as consistent with the original program. The following table provides a distribution of scores annotated\n\n| | Score distribution | Average |\n|---------------------------|----------|----------|\n|Variable names | {3 : 967, 2 : 28, 1 : 3} | 2.96 |\n| Function decomposition | {3 : 938, 2 : 59, 1 : 1} | 2.93 |\n| Consistency | {1 : 994, 0 : 4} | 0.994 |\n\nAppendix C.1 provides a qualitative analysis of GPT-4 scores and more details. \n\nFinally, we will release our generated dataset for further analysis for the community."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285221463,
                "cdate": 1700285221463,
                "tmdate": 1700593572046,
                "mdate": 1700593572046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4sp3lBrCm4",
                "forum": "maRYffiUpI",
                "replyto": "qlSlHg0YZw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer DhCg - Part II"
                    },
                    "comment": {
                        "value": "> As observed from the experimental results, the improvement of the proposed method could be insignificant and inconsistent. \n\nModest Improvements - While the improvements appear modest, this is in large part also an artifact of the algorithmic code generation task being a challenging domain. Following, we provide some context for the two benchmarks:\n\n- APPS. Prior works using the APPS dataset, CodeRL[1] and PPOCoder[2] have reported a similar range of performance improvements \u2013 pass@1 improvement from 5.6 to 7.1 by CodeRL and 3.6 to 5.2 by PPOCoder. \n\n- CodeContests.  CodeContests dataset is an even more challenging dataset in comparison to APPS and thus is also harder to improve upon. For context, within the AlphaCode models, the 41B model does not exhibit notable improvements over the 9B model. Similarly, even GPT-3.5-Turbo achieves a pass@100 of 18, compared to 12 with our best-performing CodeLLaMa7B model. To our knowledge, the only other study that has tackled this dataset without closed-source API-gated models is \u201coutline then generate\u201d [5]. They proposed a tokenization approach and showed similar improvements against the AlphaCode baseline\n\nInconsistent Improvements - We observe small performance drops on the CodeContests D_rename experiment. While we cannot ascertain the exact reason, we have the following hypotheses.\n\nHypothesis 1 - Training examples in the CodeContests datasets on average already consist of good or standard variable naming practices and further renaming does not help (as also evidenced to a lesser degree for the APPS dataset)\nHypothesis 2 - Variable names have a lesser impact on code generation capability and the primary impact comes from modularization, our key transformation.\n\n> Code generation could be achieved by prompting a general LLM such as ChatGPT directly as well. \n\nWe thank the reviewer for suggesting the related work. We have included more detailed related work in code generation with LLMs now. \n\nHowever, we would like to re-emphasize that in this work our goal is to study and propose methods for improving data quality for code generation. Data quality is an important aspect of training LLMs. We hope our proposed approach and findings will be useful for improving the training process for future (code) language models. \n\n[1] Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. Neurips datasets 2023\n\n[2] Large Language Models Are State-of-the-Art Evaluators of Code Generation. Arxiv 2023\n\n[3] Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Neuips 2022\n\n[4] Execution-based code generation using deep reinforcement learning. TMLR 2023\n\n[5] Outline, then details: Syntactically guided coarse-to-fine code generation. ICML 2023"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285236395,
                "cdate": 1700285236395,
                "tmdate": 1700426096784,
                "mdate": 1700426096784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XRVqC35NLR",
                "forum": "maRYffiUpI",
                "replyto": "qlSlHg0YZw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder for the rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer DhCg,\n\nThank you for your time in reviewing our paper and providing insightful and constructive comments. This is just a gentle reminder to revisit our responses and reply as to whether our response and clarifications have addressed the issues raised in the review. If you have any further questions, please do not hesitate to let us know."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550983346,
                "cdate": 1700550983346,
                "tmdate": 1700550983346,
                "mdate": 1700550983346,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "waxiGu5uTK",
            "forum": "maRYffiUpI",
            "replyto": "maRYffiUpI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1668/Reviewer_FtQG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1668/Reviewer_FtQG"
            ],
            "content": {
                "summary": {
                    "value": "The paper shows that improving the \u201cquality\u201d of a code dataset can improve the performance of a CodeLlama7B model fine-tuned on that dataset. Specifically, for every source file in a dataset, the authors propose to use a __second__ instruction-tuned language model (gpt-3.5-turbo) to perform three types of transformations in order:\n1) rename variables to have semantically meaningful names,\n2) \u201cmodularize\u201d the code by breaking up large chunks of code into smaller functions,\n3) prepend a \u201cplan\u201d before the code that summarizes the role of each individual function.\n\nThe authors use their synthetic dataset to a) provide few-shot examples to CodeLlama7B for in-context learning, b) fine-tune CodeLlama7B. Through this process, they show modest improvements in pass@k on the APPS and CodeContest data sets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed idea is simple and an interesting approach to re-format data using a neural approach. Especially given the fact that the domain is code, transformations can be verified using an oracle - which in this case is a set of test cases. \n\n- The paper validates that by fine-tuning an LLM on a smaller good quality dataset, it is possible to achieve better/equal results than fine-tuning on a larger lower quality dataset - something that has been pointed out by many other papers in different context. \n\n- The approach can be used as inspiration for other domains where the LLM is capable of editing an existing solution but incapable of generating an entirely new solution. However, the efficacy of this approach might be significantly impacted by the presence/absence of an oracle."
                },
                "weaknesses": {
                    "value": "- The experiment section of the paper reports numbers on the subsets of two datasets. It would be nice to clearly outline the filtering criteria that are used for each dataset. I have certain questions regarding this in the \u201cClarifications section\u201d.\u00a0\n\n- In many of the tables, there has been no reference or explanation to numbers that show a negative effect on results. For example, on the Code-Contest dataset, D_rename works worse than the baseline. It would be nice if the authors could be candid about this in their writing.\n\n- Additionally, I believe that the CL-7B + D_distill number is missing in table 4(a). Can you please include that number in the rebuttal ?\n\n- Overall, the effect of planning information is a mixed bag, and the conclusions are slightly confusing. For example, from the sentence, \u201cUpon inspection of the generated solutions, we find that often the generated plans are imprecise or incorrect, highlighting that planning still remains a bottleneck.\u201d \u2013 I am confused by this statement because I am unsure if this is because of the drawbacks of CodeLlama or a drawback of the paper\u2019s approach. Overall I am unsure if D_planning actually supports the paper\u2019s claim.\u00a0\u00a0\n\n- The improvements on Code-Contests seem to be not as effective as would be expected."
                },
                "questions": {
                    "value": "1. Comments on Fig 1:\n    - In the renaming step, the variable `n` is not renamed everywhere. I understand that the actual LLM output can have mistakes, but maybe for an explanatory diagram this could be avoided.\n    - Instead of \u201ca -> root_u,\u00a0 b -> root_v, \u2026\u201d as the text above the arrow, it would be clearer to show the natural language instruction that you provided (\u201cRename the variables in the program to be\u2026\u201d). As it stands currently, it looks like the renaming (\u201ca -> root_u,\u00a0 b -> root_v, \u2026\u201d) is the __input__ to the model. Same comment for the modularization and planning steps too.\n\n2. \u00a0It\u2019s not immediately obvious how the natural language \u201cplans\u201d are used (my initial understanding was that they are provided as a docstring for each function). Would be nice to clarify in Fig 1 that they are __prepended__ to the program as a comment.\n\n3. In the APPs benchmark, do you consider all problems from \u201ccodeforces\u201d, \u201ccodechef\u201d and \u201catcoder\u201d ? Or is there some further filtering done after that ? If further filtering has been done, can you please clarify what procedure has been followed?\n\n4. What does this line \u201cThese cases are sometimes due to incorrect programs but more \u2026. while only a single test solution is provided\u201d mean ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1668/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1668/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1668/Reviewer_FtQG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803469417,
            "cdate": 1698803469417,
            "tmdate": 1700693008376,
            "mdate": 1700693008376,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mZZAW4PBFu",
                "forum": "maRYffiUpI",
                "replyto": "waxiGu5uTK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer FtQG"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments and feedback. \n\n> However, the efficacy of this approach might be significantly impacted by the presence/absence of an oracle\n\nWe agree with the reviewer that the equivalence-checking oracle plays a crucial role in maintaining the *data quality*. However, our approach can also accommodate learning-based \"oracles\" to eliminate poor-quality data points. For instance, [1] developed a quality classifier (through the manual annotation of a small data subset) for filtering GPT-3 outputs, while [2] utilized an entailment model to estimate the quality of text summarization pairs.\n\n> \u2026 clearly outline the filtering criteria that are used for each dataset\u2026\n\nNo additional filtering was performed for APPS. Further details are added to the appendix\n\n> In many of the tables, there has been no reference or explanation to numbers that show a negative effect on results. \n\nWe thank the reviewer for the feedback and have updated the manuscript to reflect this. \n\n> Overall, the effect of planning information is a mixed bag, and the conclusions are slightly confusing .....\n\nWe agree with the reviewer that results on D_planning do not provide consistent improvements and acknowledge the confusion. We will further clarify the claims about how planning affects performance.\n\nTo present our findings and conclusion in perspective, we would like to highlight that improving the *reasoning* and *planning* capabilities is an active area of research [3]. Prior works have reported strong performance improvements by using a supervised learning paradigm by training on natural language plans or reasoning chains [4,5,6] albeit typically in less complex domains. In line with these approaches, we adopted the supervised learning paradigm, aiming to refine planning and reasoning for models by training on automatically curated natural language plans using our methodology. We do not assert that supervised learning is the definitive solution here, as discussed in Section 4.1 (final paragraph).\n\nWhile the results from D_planning are mixed, we believe they raise critical questions for future research. Two particularly promising avenues are:\n\n- Defining \"quality\" for natural language plans: The mixed success with planning might be attributed to inaccuracies in the automatically generated plans. Future data curation techniques that filter or augment this precision (akin to entailment models used in [2]) are promising.\n\n- Alternative learning paradigms for enhancing planning: The supervised learning paradigm followed in this work might be insufficient for models to generalize planning in complex domains. Future work can explore alternative learning algorithms, possibly over our modularization approach which naturally decomposes programs usually beneficial for solving complex problems.\n\nWe have revised the manuscript to reflect this message more clearly.\n\n> The improvements on Code-Contests seem to be not as effective as would be expected.\n\nWhile CodeContests improvements do not seem significant in magnitude, we want to emphasize that the CodeContests dataset is an even more challenging dataset in comparison to APPS and thus also harder to improve upon. For context, within the AlphaCode models, the 41B model does not improve notably over the 9B model, suggesting that even scaling has a limited effect. To our knowledge, the only other study that has tackled this dataset without closed-source API-gated models is \u201coutline-generate\u201d [7]. They proposed a tokenization approach and observed similar improvements against AlphaCode.\n\n> What does this line \u201cThese cases are sometimes due to incorrect programs but more \u2026. while only a single test solution is provided\u201d mean?\n\nThis refers to the observation that certain problems admit multiple correct solutions that are not equivalent to one another. For instance, a problem may allow the output of a list of numbers in any order. Our examination of the datasets revealed that both APPS and CodeContests typically include test cases that cover only one of the many correct solutions (e.g., only one specific ordering of the list). Example - https://codeforces.com/problemset/problem/1512/B.\n\n> Minor issues - Figure 1, missing distillation number, prepending plans\n\nFixed and updated in manuscript.\n\n[1] Symbolic knowledge distillation: from general language models to commonsense models. ACL 2021\n\n[2] Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation. 2022\n\n[3] Towards reasoning in large language models: A survey. 2023.\n\n[4] Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. 2022\n\n[5] Symbolic Chain-of-Thought Distillation: Small Models Can Also\" Think\" Step-by-Step. 2023\n\n[6] Orca: Progressive learning from complex explanation traces of gpt-4. 2023 \n\n[7] Outline, then details: Syntactically guided coarse-to-fine code generation. ICML 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285181486,
                "cdate": 1700285181486,
                "tmdate": 1700597699677,
                "mdate": 1700597699677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SARvFGjLIL",
                "forum": "maRYffiUpI",
                "replyto": "waxiGu5uTK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder for the rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer FtQG,\n\nThank you for your time in reviewing our paper and providing insightful and constructive comments. This is just a gentle reminder to revisit our responses and reply as to whether our response and clarifications have addressed the issues raised in the review. If you have any further questions, please do not hesitate to let us know."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550916740,
                "cdate": 1700550916740,
                "tmdate": 1700550930472,
                "mdate": 1700550930472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JiCQm6AvDt",
                "forum": "maRYffiUpI",
                "replyto": "mZZAW4PBFu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Reviewer_FtQG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Reviewer_FtQG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for answering all my questions. The planning section reads much better now. I will update my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692980647,
                "cdate": 1700692980647,
                "tmdate": 1700692980647,
                "mdate": 1700692980647,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uxoSd1oQlk",
            "forum": "maRYffiUpI",
            "replyto": "maRYffiUpI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1668/Reviewer_MNi4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1668/Reviewer_MNi4"
            ],
            "content": {
                "summary": {
                    "value": "This work explores applying a proposed data-cleaning pipeline (1: renaming variables, 2: refactoring into helper functions, and 3: inserting natural language comments to guide generation) to two major datasets (APPS and CodeContests). Fine-tuning a CodeLLAMA 7B model on these cleaned datasets dramatically improves fine tuning efficiency (requiring 8x less data to match the same performance) and improves accuracy a decent amount (often 1.2x-1.3x). They don't find the 3rd form of refactoring (introducing planning-based comments) to yield improvements, and through an additional experiment they narrow this down to be due to the inability of the model to generate good plans (as opposed to its ability to follow the plans)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This is a beautifully written paper, quite easy to follow and at just the right level of detail.\n - APPS and CodeContests are two very standard datasets so those were a great choice.\n - The experiment in Table 4b around ground truth plans is a very nice little experiment, I appreciate the inclusion of that\n- The data efficiency results in Figure 3 are quite good, showing 8x less finetuning data is needed to achieve the same Pass@1 when finetuning on the refactored programs.\n- I appreciate the smaller scale observations/insights on LLM prompting, which I think are generally nice thing to include in these sorts of conference papers for the community, e.g. \"Finally, in accordance with existing literature on prompting LLMS, we found that using simple and precise, low-level instructions improves the performance and accu- racy of the models in performing the operatons. Thus, for complex data cleaning operations, we find improvements by breaking it down and performing multiple operations iteratively.\"\n - The main results (Table 3 and 4a) are decent (not incredible, but reasonable in my opinion)."
                },
                "weaknesses": {
                    "value": "- The improvements in Table 3 are okay, not huge but still noticeable.\n- The planning results are also modest, but this is interesting in its own right, and the analysis of how ground truth plans would help considerably is a good way to isolate much of the problem to the plan creation rather than plan execution.\n- While most of the paper was easy to read, I was quite unclear on the distillation dataset baseline \u2013 see the Questions section for details\n- For more minor / easily fixed weaknesses see Questions section"
                },
                "questions": {
                    "value": "- In table 3 theres one missing entry \u2013 Pass@1 APPS Interview Distill. Where is it?\n\n- I don't understand the \"distill\" baseline dataset laid out at the end of 3.2, and referenced at various points\n    - My best guess is that you're doing synthetic data generation to generate a new dataset by prompting with few-shot examples from the modular dataset? Is it generating both the test cases *and* the solutions to them? Or are the test cases taken from somewhere and then its just generating solutions? \n\n- Two relevant pieces of work on synthetic data generation of code for LLMs are the Self Taught Reasoner (STaR) (Zelikman et al 2022) and Language Models Can Teach Themselves to Program Better (Haluptzok et al 2022). Those would be relevant to reference under the \"Synthetic data for LLMS\" section of Related Work, and could also relate to the distillation (though as mentioned before, I understand the distill baseline less).\n\nLow level confusing things:\n- Given that the 30% relative improvement is in Table 4, it's confusing that the caption of Table 3 brings up the 30% statistic (led to me spending a while trying to figure out which two numbers divide to get 30%, which is none in table 3)\n- Note: missing period in last paragraph of Section 1 right before \"Next\"\n- Typo at end of 2.1 with random sentence ending: \"steps quite effectively. effective in generating high-quality outputs.\"\n- The sentence \"We obtain three *parallel* datasets at the end of our cleaning process, one for each of renaming, modularization, and planning\" and in particular the world \"parallel\" is a bit misleading since at least to me it suggests that each dataset comes from applying a single transformation to the original dataset independent of the others, but actually the 3 transformations build on each other. This is clarified by Table 2 but would be helpful to have in the text as well.\n- Table 4 isn't actually labelled \"Table 4\" anywhere (since there's no shared caption)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1668/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1668/Reviewer_MNi4",
                        "ICLR.cc/2024/Conference/Submission1668/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1668/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698992135477,
            "cdate": 1698992135477,
            "tmdate": 1700610618358,
            "mdate": 1700610618358,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BjGsl6Zt3w",
                "forum": "maRYffiUpI",
                "replyto": "uxoSd1oQlk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer MNi4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive comments. \n\n> The improvements in Table 3 are okay, not huge but still noticeable.\n\nWhile the improvements in Table 3 do not appear to be huge, this is in large part also an artifact of the algorithmic code generation task being a challenging domain. For example, prior works using the APPS dataset, CodeRL[1] and PPOCoder[2] have reported a similar range of performance improvements \u2013 pass@1 improvement from 5.6 to 7.1 by CodeRL and 3.6 to 5.2 by PPOCoder. Finally, we study the data quality for improving code LLMs which is an important problem and complements existing works. Similarly, CodeContests is even more challenging and harder to improve upon.\n\n> The planning results are also modest, but this is interesting in its own right, and the analysis of how ground truth plans would help considerably is a good way to isolate much of the problem to the plan creation rather than plan execution.\n\nWe agree with the reviewer that planning results are modest/mixed bag. To add additional context and perspective \u2013 improving the reasoning and planning capabilities of language models is a challenging problem and a research direction actively explored by the community [10]. Our experiment of disentangling plan generation with code implementation shows that even after training on such plans, current models cannot generalize and construct plans for new challenging problems. To provide the insights more clearly, we have added some additional context in the paper summarized below: \n\nWe adopted the supervised learning paradigm, aiming to refine planning by training on natural language plans curated using our methodology akin to prior works [4,5,6]. While the results from D_planning are mixed, we believe they raise critical questions for future research. Two particularly promising avenues are:\n\n- Defining \"quality\" for natural language plans: The mixed success with planning might be attributed to inaccuracies in the automatically generated plans. Future data curation techniques that filter or augment this precision (akin to entailment models used in [2]) is promising.\n\n- Alternative learning paradigms for enhancing planning: The supervised learning paradigm followed in this work might be insufficient for models to generalize planning in complex domains. Future work can explore alternative learning algorithms, possibly over our modularization approach which naturally decomposes programs usually beneficial for solving complex problems.\n\n\n> Clarifications on the distillation experiment\n\nYes, in the distillation experiment, we take the problem statements from the training dataset and use the GPT-3.5-Turbo model to generate multiple, *diverse* solutions using a few-shot prompting approach. Next, we filter the generated solutions for correctness based on the ground truth test cases provided in the dataset to ensure we are not training on incorrect hallucinated solutions. This experiment is meant to provide a strong baseline that distills larger-model-generated outputs which has proven to be a very popular approach for both instruction tuning and task-specific finetuning [6,8,9]. \nIn contrast to distillation, our transformation approach enjoys the benefits of performing the simpler operation of transforming (editing) existing solutions instead of directly generating them. Thus it yields about 94\\% successful transformation rate whilst the distillation baseline can only generate solutions for about 50\\% of the problems. \n\n> Two relevant pieces of work on synthetic data generation \u2026\n\nYes, these works are relevant. STaR learns natural language reasoning (corresponding to our planning approach) in simpler domains. Haluptzok et al generates datasets using language models and further models on them. We have added them to the related works.\n\n> Smaller issues \u2013 missing distillation number, typos, usage of the word parallel, table 4 caption\n\nWe have fixed these issues and updated the paper.\n\n[1] Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Neuips 2022\n\n[2] Execution-based code generation using deep reinforcement learning. TMLR 2023\n\n[3] Outline, then details: Syntactically guided coarse-to-fine code generation. ICML 2023\n\n[4] Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. Arxiv 2022\n\n[5] Symbolic Chain-of-Thought Distillation: Small Models Can Also\" Think\" Step-by-Step. Arxiv 2023\n\n[6] Orca: Progressive learning from complex explanation traces of gpt-4. Arxiv 2023 \n\n[7] Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation. 2022\n\n[8] Wizardlm: Empowering large language models to follow complex instructions. Arxiv 2023\n\n[9] WizardCoder: Empowering Code Large Language Models with Evol-Instruct. Arxiv 2023\n\n[10] Towards reasoning in large language models: A survey. Arxiv 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285134805,
                "cdate": 1700285134805,
                "tmdate": 1700285134805,
                "mdate": 1700285134805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8nV6g9cL4O",
                "forum": "maRYffiUpI",
                "replyto": "uxoSd1oQlk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle reminder for the rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer MNi4,\n\nThank you for your time in reviewing our paper and providing insightful and constructive comments. This is just a gentle reminder to revisit our responses and reply as to whether our response and clarifications have addressed the issues raised in the review. If you have any further questions, please do not hesitate to let us know."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550895036,
                "cdate": 1700550895036,
                "tmdate": 1700550939792,
                "mdate": 1700550939792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uv8dwmg9iJ",
                "forum": "maRYffiUpI",
                "replyto": "8nV6g9cL4O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1668/Reviewer_MNi4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1668/Reviewer_MNi4"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the discussion in the rebuttal and for adding clarification to the text discussion around distillation \u2013 having gone through the paper again, I've adjusted my score to an 8."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1668/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611071175,
                "cdate": 1700611071175,
                "tmdate": 1700611071175,
                "mdate": 1700611071175,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]