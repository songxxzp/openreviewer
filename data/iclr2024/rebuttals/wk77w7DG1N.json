[
    {
        "title": "Evaluating and Improving Generation Consistency of Large Language Models via A Divide-Conquer-Reasoning Approach"
    },
    {
        "review": {
            "id": "8ITeKH7WCY",
            "forum": "wk77w7DG1N",
            "replyto": "wk77w7DG1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6094/Reviewer_Zcqt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6094/Reviewer_Zcqt"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a three-step algorithm for detecting and improving the consistency between a reference piece of text and a generated piece of text. The algorithm begins by segmenting the input texts, then using an LLM to generate a justification statement for each generated sentence. In the next step, the justifications are used as input to another LLM to produce numerical predictions, to be aggregated to output a single score. The final step is to use a third LLM to re-write the generated sentences if they are predicted to be inconsistent.\n\nThe method is evaluated in two paraphrase detection and two summarization datasets, and shows a lot of improvement over BERTScore, BARTScore, and a few other metrics."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Very well-written paper, both in terms of writing and also in terms of presentation.\\\nVery good results.\\\nThe research task is real, and has real world applications."
                },
                "weaknesses": {
                    "value": "My argument is very simple: What if all the improvement that the authors are getting is coming from using GPT3.5/4 ? I am saying, it is not fair to compare GPT3.5/4 to BERT, or even to GPT3.\\\nAuthors describe a long list of techniques and methods to justify their model. What if all of them are just distraction, and the main reason that their method works is because of good outputs generated by GPT3.5/4 ?\\\nIn fact the authors report an experiment that supports my argument. Tables 2-4 compare the results of their method when GPT3.5 is replaced with GPT4. The improvements in most cases are substantial. Which means a lot of work is carried by the underlying LLM.\\\nIn my opinion, all the experiments should be repeated with identical underlying LM or LLM to be able to claim that the method really works (please just do not say that because a method is called BERTScore, then we should only use BERT vectors in it, and for example BLOOM vectors cannot be used in it).\n\nMy second argument is to refute the authors argument about the robustness of their method against hallucination. On Page 2, the authors state that their model does not rely on LLMs, they also repeatedly claim that existing methods are prone to hallucination---multiple times in the intro section and in the other sections.\\\n But they are using LLMs through out their algorithm! I don\u2019t know how they cannot see that! Plus all the three steps of their algorithm can be easily subject to hallucination as well. For example in the first step the LLM can easily generate hallucinated justification. Why are the authors so certain that this cannot happen?! Not clear to me."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6094/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698445364770,
            "cdate": 1698445364770,
            "tmdate": 1699636657371,
            "mdate": 1699636657371,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uQKkwXEYzX",
                "forum": "wk77w7DG1N",
                "replyto": "8ITeKH7WCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zcqt - part 1/2"
                    },
                    "comment": {
                        "value": "Thank you very much for your detailed and thoughtful comments. We are glad that you found our paper well-written, our results strong, and the task practical. We address your questions and concerns below.\n\n>**W1: What if all the improvement that the authors are getting is coming from using GPT3.5/4 ? I am saying, it is not fair to compare GPT3.5/4 to BERT, or even to GPT3.**\n\nOur DCR method benefits from GPT3.5/4 but is not entirely dependent on GPT3.5/4 only. Our improvement is owing to our unique novel contributions of divide-conquer-reasoning strategies achieved by three LLM agents, DCE, AMC, and RAI. We leveraged the strength of GPT3.5/4 in our components but did not directly and completely rely on them for generating final outputs. Also, we highlight that unlike previous methods such as BertScore and G-Eval, DCR not only outputs a numeric metric but also clarifies the reasons behind the number. Such reasons enable humans to interpret and evaluate the DCR system and also empower the inconsistency-mitigation process, which is not available through BertScore or G-Eval.\n\n\nTo verify our effectiveness, we focus on the comparison with recent state-of-the-art studies, such as G-Eval with GPT-3.5 and GPT-4 on multiple benchmarks. **To ensure a fair comparison, we paid special attention in our experiments to use identical underlying LLM as G-Eval used, i.e., GPT-3.5 and GPT-4 to report empirical results**. As Table 3 shows, our DCE-AMC-3.5 (0.592 and 0.563) outperforms G-Eval-3.5 (0.386 and 0.318) by a large margin. Similarly, our DCE-AMC-4 (0.700 and 0.668) also presents superior performance compared to G-Eval-4 (0.507 and 0.425) on SummEval datasets. \n\n\nWe understand your concern about the effect of LLM on the final performance. Integrating multiple models, including BERT, BART, Llama, GPT-based, PaLM, Claude, etc with our DCR strategy is a promising and ambitious future research direction but is out of the scope of our current study. The current focus is GPT-3.5/4 and we have demonstrated strong advantages compared with G-Eval on multiple benchmarks. \n\n\nOn Page 8, we provide a discussion about the effect of LLM models. We noted that DCE-AMC-4 generally outperforms DCE-AMC-3.5 across most datasets, which suggests that GPT-4 can further enhance performance, especially for more complex evaluation tasks. Nonetheless, the benefits of GPT-3.5, such as higher computational efficiency and lower API costs, should not be overlooked."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327095398,
                "cdate": 1700327095398,
                "tmdate": 1700331570488,
                "mdate": 1700331570488,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Quw4RaadJG",
                "forum": "wk77w7DG1N",
                "replyto": "8ITeKH7WCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zcqt - part 2/2"
                    },
                    "comment": {
                        "value": "> **W2: But they are using LLMs through out their algorithm! I don\u2019t know how they cannot see that! Plus all the three steps of their algorithm can be easily subject to hallucination as well. Why are the authors so certain that this cannot happen?!**\n\n*First*, our complete statement on Page 2 is that \u201cOur approach does not rely on LLMs, which are prone to hallucination,  **to output numeric scores without justification**\u201d. Our intention was to emphasize that the DCR framework does not rely on LLMs to **directly** generate scores, which is in contrast to previous methods such as G-Eval, which may be possibly hallucinated.  As we discussed in Section 2, existing LLM-based evaluators have a major drawback as the generated verbal scores by LLMs are prone to hallucinations, resulting in abnormally higher ratings for LLM-generated content that diverge from human judgment. Such methods also generate no actionable insight to justify the score or mitigate inconsistencies after identifying them. \n\nTo overcome these issues, we intentionally avoid using LLM in the critical steps that involve generating numerical evaluation scores but instead use LLMs to perform reasoning which they have been proven to excel in. To operationalize this, we introduce an automatic metric converter (AMC) that aims to quantitatively measure the consistency evaluation by converting the reasons a numeric score system. AMC functions as a binary sentiment classifier that classifies the reasons to be either positive (marked by \u201c+1\" if the sentence is consistent), or negative (marked by \u201c-1\u201d otherwise) for each sentence, then utilizes this score array to calculate a comprehensive score to evaluate consistency, as explained in Section 3.2.  We believe such a numerical score calculated by AMC is more stable than the verbal score directly output by LLMs, as is verified by experiments. \n\nWe clarify this sentence in the revised version as \u201c**Our approach does not rely on LLMs to directly output numeric scores without justification, which are prone to hallucination**\u201d.\n\n--- \n\n*Second*, we clarify that we never state that LLM hallucinations cannot happen in our framework. On the contrary, we acknowledge hallucination as a key impediment to broader adoptions of LLMs in many critical domains which motivates our research that aims to design an automated framework for assessing inconsistency of LLM-generated contents to facilitate effective hallucination mitigation and reduction. This is through acknowledging that although currently there is no fundamental fix to LLM hallucinations, with the recent advances in generative AI there are certain tasks that LLM can achieve superhuman performance and are less prone to hallucination, such as summarization task [1].  Therefore, what we pursue is to leverage the strength of LLM in downstream consistency evaluation and improvement to minimize the risk and negative effect of LLM in open-domain generation tasks. Specifically, we take the following innovative approaches to mitigate LLM hallucinations in our framework, \n- Unlike direct paragraph check via LLM, our divide-conquer strategy (via DCE component) focuses on sentence level, which defines a more specific task for LLM to check consistency. \n- Rather than directly generating verbal scores by LLM, we use LLMs to generate reasons and translate these reasons into numeric scores (via ACE component) by calculating mathematically. \n- Instead of ending after consistency evaluation, we propose a multi-round improvement strategy (via RAI component), which eliminates the probability of LLM hallucinations occurring in a specific round, and also provides a more consistent response eventually. \n\nBeyond the above three strategies, we also carefully verify our approach and results via \n\n- Manual examination of the output response. There are some illustrative examples in Table 6, 13-17. \n- Experiments on evaluating the overall accuracy of the framework on three benchmarks, see Figure 3. Below is a table of the same results.\n\nWhile LLM hallucination possibly happened in each component, our DCR framework (DCE+AMC+RAI) demonstrates high accuracy and robust performance in consistency evaluation and improvement on all benchmarks.  \n\n| Metric  Name  | F1 | Precision  | Recall | \n| --------  | --------  |--------  |--------  |\n| SUMMEVAL | 0.933 | 0.929 | 0.936|\n| QAGS-XSUM | 0.811 | 0.718 | 0.930|\n| QAGS-CNN| 0.871 | 0.842 | 0.901|\n\n[1] Pu, Xiao, Mingqi Gao, and Xiaojun Wan. \"Summarization is (almost) dead.\" arXiv preprint arXiv:2309.09558 (2023)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327428390,
                "cdate": 1700327428390,
                "tmdate": 1700328228098,
                "mdate": 1700328228098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GF5p5ikOyU",
                "forum": "wk77w7DG1N",
                "replyto": "8ITeKH7WCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Reviewer_Zcqt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewer_Zcqt"
                ],
                "content": {
                    "comment": {
                        "value": "My apologies for the slow response---like yourselves I am a researcher too, and always struggling with my schedule.\n\nThank you for the clarifications.\n\nI am not convinced with your explanations about the lack of comparison with other methods. Yes, I agree with you, having a fair setup between all the metrics to achieve a realistic conclusion is ambitious. But so are your claims in the paper, in comparison to what other metrics do. In my opinion, the method is not a significant scientific step. In other words, I am not learning anything new from your method, which is breaking down paragraphs and individually processing them, and then asking another model to generate the scores for them. It would be significant, if you showed that out of all the metrics, if they use exactly the same tools, none of them achieve these results, which meant none of them perform these simple operations (breaking down and generating scores). I personally doubt this is the case, and you did not resolve my doubts. Unfortunately, you just reiterated your arguments in the paper.\n\nRegarding the second issue, which is your arguments about hallucination and the use of LLMs. I am not convinced that it was just one sentence, otherwise i would not written this in the review: \n> ... they also repeatedly claim that existing methods are prone to hallucination---multiple times in the intro section and in the other sections ....\n\nbut I am willing to revise my score. Because it is just a simple issue and is fixable, and now that you are aware of it, you can resolve it and improve your paper. The current review rating is \"Reject\", I wish I could increase the rating just by 0.5 or by 1, but it looks the only option is to increase if by 2! \n\n***\n\nps. please take this with a grain of salt, but when i was reading the experimental section of your paper, i was thinking that this is like comparing a lamborghini to a minivan. And arguing that the reason that the lamborghini can drive faster than the minivan is because of its aerodynamic design! but we both know that this is not true. the reason is the powerful engine inside the lamborghini.\\\nall the evidence that i am seeing supports the same conclusion about your method. it appears that the main performance improvement is coming from the engine (GPT3.5/4 models)."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670894577,
                "cdate": 1700670894577,
                "tmdate": 1700670894577,
                "mdate": 1700670894577,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lhTEGi81Q8",
                "forum": "wk77w7DG1N",
                "replyto": "8ITeKH7WCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zcqt"
                    },
                    "comment": {
                        "value": "We understand the tight schedule and appreciate your taking the time to respond before the due date. The following is our response to your comment *sentence-by-sentence*. \n\n\n>*\u201cYes, I agree with you, having a fair setup between all the metrics to achieve a realistic conclusion is ambitious. But so are your claims in the paper, in comparison to what other metrics do.\u201d*\n\nWe note that our contribution is not merely to design a metric for evaluation but also to develop a method for automatically improving the consistency of the generated text without requiring human supervision.\n\n>*\u201c\u2026 which is breaking down paragraphs and individually processing them, and then asking another model to generate the scores for them.\u201d*\n\nTo clarify, **we don't rely on generated scores from any model**. As previously highlighted, verbal scores generated by LLMs are prone to hallucinations, which is a well-documented issue that has been acknowledged in prior research and verified in our experiments.\n\n>*\u201cRegarding the second issue, which is your arguments about hallucination and the use of LLMs. I am not convinced that it was just one sentence, otherwise i would not written this in the review\u201d*\n\nWe have meticulously examined that the term \u201challucination\u201d appeared 21 times in our paper (14 times in the main paper and 7 times in the references), primarily used in context with \u201cmitigation\u201d and \u201cdetection\u201d. The sentence in the first paragraph of page two is the only single occurrence where we see that might have caused confusion. If the reviewer believes that there exist other cases where we might have misstated our contribution, we kindly ask you to point us to it such that we can revise it in our next version. \n\n>*\u201cbut I am willing to revise my score. Because it is just a simple issue and is fixable, and now that you are aware of it, you can resolve it and improve your paper. The current review rating is \"Reject\", I wish I could increase the rating just by 0.5 or by 1, but it looks the only option is to increase if by 2!\u201d*\n\nYour acknowledgment of this being an addressable issue and your willingness to reevaluate the score are greatly appreciated.\n\n>*\u201c\u2026 but when i was reading the experimental section of your paper, i was thinking that this is like comparing a lamborghini to a minivan. And arguing that the reason that the lamborghini can drive faster than the minivan is because of its aerodynamic design! but we both know that this is not true. the reason is the powerful engine inside the lamborghini \u2026 it appears that the main performance improvement is coming from the engine (GPT3.5/4 models).\u201d*\n\nWe respectfully disagree with this argument. We have indeed compared with other LLM-powered baselines such as G-eval and GPT-Score, which are also based on GPT series. **In particular, our experimental results summarized in Table 3 & Table 4 show that our method consistently outperforms G-eval across all considered metrics, and the comparison is done under the same GPT version, i.e., DCE-AMC-3.5 outperforms G-eval-3.5 and DCE-AMC-4 outperforms G-eval-4**. Note that G-eval and GPT-Score are both state-of-the-art works on NLG evaluation (published in 2023) and are well-cited (cited 141 and 105 times respectively according to Google Scholar).\n\nFinally, we would like to stress our contribution in leveraging the latest advancements in generative models to craft a framework that optimizes LLMs\u2019 capabilities while circumventing known pitfalls, which is a direction aligned with numerous recent studies [1-11]. We believe such research is a pivotal step towards the practical deployment of LLMs."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717931204,
                "cdate": 1700717931204,
                "tmdate": 1700742425134,
                "mdate": 1700742425134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pPSoGz4KFY",
                "forum": "wk77w7DG1N",
                "replyto": "8ITeKH7WCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "References"
                    },
                    "comment": {
                        "value": "**Recent studies**\n\n[1] Liu, Yang, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. \"Gpteval: Nlg evaluation using gpt-4 with better human alignment.\" arXiv preprint arXiv:2303.16634 (2023). EMNLP 2023. \n\n[2] Fu, Jinlan, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. \"Gptscore: Evaluate as you desire.\" arXiv preprint arXiv:2302.04166 (2023). \n\n[3] Manakul, Potsawee, Adian Liusie, and Mark JF Gales. \"Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.\" arXiv preprint arXiv:2303.08896 (2023). EMNLP 2023. \n\n[4] Wang, Jiaan, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. \"Is chatgpt a good nlg evaluator? a preliminary study.\" arXiv preprint arXiv:2303.04048 (2023).\n\n[5] Zeng, Zhiyuan, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. \"Evaluating large language models at evaluating instruction following.\" arXiv preprint arXiv:2310.07641 (2023).\n\n[6] Liu, Minqian, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, and Lifu Huang. \"X-Eval: Generalizable Multi-aspect Text Evaluation via Augmented Instruction Tuning with Auxiliary Evaluation Aspects.\" arXiv preprint arXiv:2311.08788 (2023).\n\n[7] Madaan, Aman, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon et al. \"Self-refine: Iterative refinement with self-feedback.\" arXiv preprint arXiv:2303.17651 (2023). NeurIPS 2023. \n\n[8] Liu, Yuxuan, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. \"Calibrating LLM-Based Evaluator.\" arXiv preprint arXiv:2309.13308 (2023).\n\n[9] Chan, Chi-Min, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. \"Chateval: Towards better llm-based evaluators through multi-agent debate.\" arXiv preprint arXiv:2308.07201 (2023).\n\n[10] Gao. Mingqi, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. \"Human-like summarization evaluation with chatgpt.\" arXiv preprint arXiv:2304.02554 (2023).\n\n[11] Jason, Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter Fei Xia, Ed H. Chi, Quoc V. Le, Denny Zhou \u201cChain-of-Thought Prompting Elicits Reasoning in Large Language Modelsz\u201d. arXiv:2201.11903 (2022) NeurIPS 2022."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718033004,
                "cdate": 1700718033004,
                "tmdate": 1700718064085,
                "mdate": 1700718064085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UYDjYm4VI3",
            "forum": "wk77w7DG1N",
            "replyto": "wk77w7DG1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6094/Reviewer_9wcP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6094/Reviewer_9wcP"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a sentence-granularity evaluation strategy based on LLM.\nThe evaluation is decomposed into pre-sentence assessments, which are then combined into a single score.\nThe experiments show that sentence-level evaluation is more consistent than token-level (e.g., BertScore) or passage-level assessment (e.g., GPT-Score) for paraphrasing and summarization tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1 Assessing the quality of candidate answers is best done using fine-grained signals such as sentence-level granularity.\n\n2 The proposed method achieves higher consistency with human annotations in tasks of paraphrasing identification and summarization, compared to a wide range of baseline methods."
                },
                "weaknesses": {
                    "value": "1 The proposed method has a limited scope and is not easily generalizable to broader instruction-following tasks. In the experiment, the method required hand-crafted prompts for each specific task, which can be overwhelming and even infeasible when evaluating a wide spectrum of tasks. Other LLM-based evaluators can handle this common scenario more effectively.\n\n2 The experiments lack human evaluations beyond the instance level, leaving unclear the overall accuracy of sentence-level judgments.\n\n3 The per-sentence assessment protocol is used to identify and fix inconsistencies, but it may be prone to LLM's overconfidence -- it prefers its predictions even if it is wrong.\n\n4 The evaluation focuses solely on consistency, but other criteria like factuality and coherence are also important for text generation. It's unclear why only consistency was considered."
                },
                "questions": {
                    "value": "1 Why it is stated in the Intro that DCE \u201cdoes not rely on LLM\u201c, considering Chatgpt/GPT4 is required for most components of the method?\n\n2 How is the reason-assisted improver (RAI) compared to self-refine?\n\n3 G-Eval does not use references during evaluation, so comparing the proposed method to G-Eval may not be fair since they use different evaluation approaches."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6094/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747032512,
            "cdate": 1698747032512,
            "tmdate": 1699636657180,
            "mdate": 1699636657180,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KjZre8FAht",
                "forum": "wk77w7DG1N",
                "replyto": "UYDjYm4VI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9wcP - part 1/3"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and suggestions to improve the paper. We are glad to hear that you found that our idea is good for assessing the quality of candidate answers, and our method achieves better performance than a wide range of baselines. We address your concerns and questions below.\n\n>**W1: The proposed method has a limited scope and is not easily generalizable to broader instruction-following tasks. In the experiment, the method required hand-crafted prompts for each specific task, which can be overwhelming and even infeasible when evaluating a wide spectrum of tasks. Other LLM-based evaluators can handle this common scenario more effectively.**\n\nOur work focuses on the evaluation and improvement of generated text consistencies. We do not claim DCR as a generic evaluation metric and our experiments demonstrated that DCR achieves the state-of-the-art performance on consistency evaluation compared with the existing baseline methods. Our DCR is easily generalizable to assess the consistency between any two text sequences without constraints on their format, that is, they can be either generated texts by LLM or written by human experts. \n\nWe agree that our DCR framework requires hand-craft prompts for specific tasks, and acknowledge that this is a general hurdle shared by all works relying on LLMs, which include G-Eval, GPTScore, and self-refine.  Specifically, in G-Eval, different prompts will need to be composed for different aspects: consistency, coherence, etc.  Self-refine defines multiple customized prompts to perform their INIT - FEEDBACK \u2013 REFINE components. Our current solution is to structure our prompts in a modularized manner so task-specific content can be updated easily. An automated prompt-tuning procedure is beyond the focus of our study. If you believe there exists coinciding work on an LLM-based evaluator that offers a more effective solution, we kindly ask you to point us to it.\n\n>**W2-3: The experiments lack human evaluations beyond the instance level, leaving unclear the overall accuracy of sentence-level judgments. The per-sentence assessment protocol may be prone to LLM's overconfidence -- it prefers its predictions even if it is wrong.**\n\nAlthough the experiments lack human evaluations on sentence-level judgments, our DCR method provides an overall numerical score by calculating and integrating sentence-level scores through an automatic metric converter (AMC) component. As shown in Fig.3 and the following table result, we verify the F1-score, precision, and recall performance of our method on sentence-level and paragraph-level evaluations compared with human evaluations. Such high accuracy provides solid support for our reason-assisted improver (RAI) which aims at reducing inconsistencies via multi-round iterations. We agree with the reviewer that LLMs have the potential tendency to be overconfidence. However, as demonstrated by our experiments, such overconfidence can be mitigated through the careful design of our framework, resulting in high accuracy that is well-aligned with human intuition.\n\n| Metric  Name  | F1 | Precision  | Recall | \n| --------  | --------  |--------  |--------  |\n| SUMMEVAL | 0.933 | 0.929 | 0.936|\n| QAGS-XSUM | 0.811 | 0.718 | 0.930|\n| QAGS-CNN| 0.871 | 0.842 | 0.901|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326613920,
                "cdate": 1700326613920,
                "tmdate": 1700328187415,
                "mdate": 1700328187415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8iNf8k3APZ",
                "forum": "wk77w7DG1N",
                "replyto": "UYDjYm4VI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9wcP - part 2/3"
                    },
                    "comment": {
                        "value": ">**W4: The evaluation focuses solely on consistency. It's unclear why only consistency was considered.**\n\nAssessing the consistency of LLM-generated content is crucial for ensuring AI safety and has become a critical step in improving the reliability of many real-world application domains that rely on LLM by preventing the generation of misinformation and harmful content. For example, consistency checking can significantly enhance the chain of thought reasoning in LLMs [1]. Recent studies employ consistency checking to detect hallucinations [2] based on pre-trained LLMs and instruction tuned LLMs [3]. Although these methods exhibit promising results on several specific tasks, including mathematical reasoning and factual assessment, the potential failures of self-consistency are often overlooked [4]. \n\nThis is essentially due to a lack of a generic, automatic, and reliable strategy that assesses the consistency of two responses, let alone remediating such inconsistency after identifying them. We highlight that our objective is not to propose a generic or comprehensive evaluation pipeline to handle broader metrics as G-Eval or GPTScore claimed. Instead, we mainly focus on the consistency scope and have achieved our state-of-the-art performance across multiple benchmarks in semantic, factual, and summarization consistency tasks, compared to existing baseline methods. Furthermore, our approach also substantially reduces nearly 90\\% output inconsistencies, showing promise for effective hallucination mitigation and reduction.\n\n[1] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n\n[2] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023.\n\n[3] Niels M\u00fcndler, Jingxuan He, Slobodan Jenko, and Martin Vechev. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. arXiv preprint arXiv:2305.15852, 2023.\n\n[4] Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R Bowman, and Kyunghyun Cho. Two failures of self-consistency in the multi-step reasoning of llms. arXiv preprint arXiv:2305.14279, 2023.\n\n>**Q1: Why it is stated in the Intro that DCE \u201cdoes not rely on LLM\u201c**\n\nOur complete statement on Page 2 is that \u201cOur approach does not rely on LLMs, which are prone to hallucination,  **to output numeric scores without justification**\u201d. Our intention was to emphasize that the DCR framework does not rely on LLMs to **directly** generate scores, which is in contrast to previous methods such as G-Eval, which may be possibly hallucinated.  As we discussed in Section 2, existing LLM-based evaluators have a major drawback as the generated verbal scores by LLMs are prone to hallucinations, resulting in abnormally higher ratings for LLM-generated content that diverge from human judgment. Such methods also generate no actionable insight to justify the score or mitigate inconsistencies after identifying them. \n\nTo overcome these issues, we intentionally avoid using LLM in the critical steps that involve generating numerical evaluation scores but instead use LLMs to perform reasoning which they have been proven to excel in. To operationalize this, we introduce an automatic metric converter (AMC) that aims to quantitatively measure the consistency evaluation by converting the reasons a numeric score system. AMC functions as a binary sentiment classifier that classifies the reasons to be either positive (marked by \u201c+1\" if the sentence is consistent), or negative (marked by \u201c-1\u201d otherwise) for each sentence, then utilizes this score array to calculate a comprehensive score to evaluate consistency, as explained in Section 3.2.  We believe such a numerical score calculated by AMC is more stable than the verbal score directly output by LLMs, as is verified by experiments. \n\nWe clarify this sentence in the revised version as \u201c**Our approach does not rely on LLMs to directly output numeric scores without justification, which are prone to hallucination**\u201d."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326745013,
                "cdate": 1700326745013,
                "tmdate": 1700328199114,
                "mdate": 1700328199114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HMFT8xYYki",
                "forum": "wk77w7DG1N",
                "replyto": "UYDjYm4VI3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9wcP - part 3/3"
                    },
                    "comment": {
                        "value": ">**Q2: How is the reason-assisted improver (RAI) compared to self-refine?**\n\nThanks for the suggestion.  As self-refine contains the process of INIT - FEEDBACK - REFINE, we summarize the difference between DCR with self-refine as follows.\n\n- DCR aims at achieving a similar goal as self-refine in providing supervision-free refinements to LLM-generated content. The focus of the two frameworks is different as self-refine aims at more general purposes with the ability to customize prompts for individual tasks whereas DCR aims at detecting and mitigating inconsistency.\n\n- There is no specific prompt dedicated to consistency checking within the context of self-refine, where it is simply defined as a criterion of \"Consistent\" in one specific task, i.e., dialogue-response. Also, self-refine uses 6 in-context examples for feedback generation, but the DCR is a zero-shot approach that does not require few-shot examples.\n\n- Comparing RAI to the REFINE step in self-refine, both approaches use LLM as the improver. Thus, it boils down to the feedback input and the prompt for performance differences. As the FEEDBACK prompt is more general regarding dialogue in self-refine, the feedback given from DCE is more specific.\n\n>**Q3: G-Eval does not use references during evaluation, so comparing the proposed method to G-Eval may not be fair since they use different evaluation approaches.**\n\nWe apologize for the confusion caused by the definition of \u201creference\u201d in our DCR framework. We would like to make the following clarifications: \n\n1. Our DCR method employs the same setting as G-Eval, which does not rely on a golden reference written by the human expert or ground truth labels.  \n\n2. Our DCR method aims to offer a generic way to assess the consistency between two text sequences. For instance, in the summarization task, one text sequence may be the original document, while the other might be the generated text summarization by LLMs. To distinguish between the two text sequences, in our paper we referred to one of the text sequences as \u201creference\u201d and the other as \u201ccandidate\u201d. Accordingly, we formulate the problem to check whether the candidate is consistent with the reference or not, as presented in Section 2.  We realize that such wording might have caused confusion and will change them to more neutral terms such as \u201ctext sequence 1\u201d and \u201ctext sequence 2\u201d.\n\nBelow we provide a detailed explanation of the \u201creference\u201d used in our experiments:\n- **Paraphrase detection tasks**. For example, in the Quora Question Pairs (QQP) dataset, each question pair is annotated with a binary value indicating whether the two questions are paraphrases of each other.   We consider \u201cquestion1\u201d as the \u201creference\u201d and \u201cquestion2\u201d as the \u201ccandidate\u201d, and our task is to evaluate if the candidate is consistent with the reference in semantic meaning.  \n- **Summarization tasks**. For example, SummEval datasets include original source articles, machine summaries, and human summaries. Our \u201creference\u201d in this task is the original source article, and our \u201ccandidate\u201d is the machine summaries. Our task is to check the factual consistency between them without relying on any additional golden reference or ground truth."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326937877,
                "cdate": 1700326937877,
                "tmdate": 1700328207595,
                "mdate": 1700328207595,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "12ln8caGjA",
            "forum": "wk77w7DG1N",
            "replyto": "wk77w7DG1N",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates consistency evaluation by employing a Large Language Model (LLM) as an evaluation agent. The authors introduce a \"divide-and-conquer\" prompting technique to break down both candidate and reference documents into individual sentences. This approach facilitates comparison at the sentence level initially, before combining these sentence-level results to produce a final evaluation. The proposed methodology is tested on SUMMeval consistency and QAGS benchmarks. Results indicate an improvement in correlation with human evaluators, specifically on the consistency criterion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper introduces an efficient \"divide-and-conquer\" evaluation prompt designed to enhance the assessment of the consistency criterion through an LLM agent. \n- The manuscript is both well-structured and easy to understand."
                },
                "weaknesses": {
                    "value": "- Novelty and Contribution:\n  - The approach of SMART [1], which suggests using sentences as the basic units for text evaluation, seems similar to the divide-and-conquer technique introduced in this paper. Specifically, methods like SMART-Bleurt and SMART-CHRF have shown strong agreement with human evaluations. Given that, the novelty of the divide-and-conquer strategy that focuses on sentence-level evaluation might not be very original when considering what SMART has already introduced. This makes me question the overall unique contribution of the present work.\n\n[1] SMART: Sentences as Basic Units for Text Evaluation\n\n- Evaluation Fairness:\n  - The authors use a \"reference\" document to help with consistency evaluation. However, from the prompts shown in Appendix A.2, it's unclear what \"reference\" means. In some cases, it seems to mean the \"true answer\" (like a gold standard in common situations), while in others, it refers to the \"article\" (or source document in common contexts). Could the authors clarify what they mean by \"reference\"?\n\n  - G-EVAL is described as a method that doesn't rely on a reference (reference-free method), where \"reference\" seems to mean a gold reference written by the human expert. If this study's prompts use the \"gold reference\" or the \"source\", it would be helpful if the authors could specify whether the proposed divide-and-conquer technique is based on a reference or not, and if it uses a source or not.\n\n  - Additionally, the lack of comparisons with other consistency methods like SMART, NLI, and Bleurt makes me question the thoroughness of the paper.\n\nUpdate after author feedback:\n\nThe authors have addressed my queries regarding evaluation fairness and have conducted additional experiments comparing with the \"consistency method.\" Given the constraints of the rebuttal period and the quality of open source code, they were able to partially reproduce the results from the SMART paper. This effort has largely resolved my concerns about evaluation fairness.\n\nConsequently, I am increasing my overall recommendation score to 6 (weak accept).\n\nHowever, I maintain that the novelty of the \"divide and conquer\" approach is somewhat limited, as the SMART paper has already proposed using \"sentence as evaluation unit.\" Additionally, the paper could benefit from revisions for methodological clarity."
                },
                "questions": {
                    "value": "Given that the reference and candidate documents might have varying numbers of sentences, how do you handle sentence-level comparisons? Specifically, do you employ any sentence matching techniques, and if so, how are they implemented?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6094/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783518325,
            "cdate": 1698783518325,
            "tmdate": 1700444441478,
            "mdate": 1700444441478,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fXkbG7hgRk",
                "forum": "wk77w7DG1N",
                "replyto": "12ln8caGjA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2NAw - part 1/2"
                    },
                    "comment": {
                        "value": "Dear reviewer 2NAw,\n\nThank you very much for the constructive comments and feedback. We are happy to hear that the reviewer found our method efficient, and our draft is both well-structured and easy to understand. Below, we have tried to address all of your feedback and questions. Please take a look and let us know in case you would like additional clarification on any of these points.\n\n> **W1: Could the authors clarify what they mean by \"reference\"? It would be helpful if the authors could specify whether the proposed divide-and-conquer technique is based on a reference or not, and if it uses a source or not.**\n\n1. Our DCR method employs the same setting as G-Eval, which does not rely on a golden reference written by the human expert or ground truth labels.  \n\n2. Our DCR method aims to offer a generic way to assess the consistency between two text sequences. For instance, in the summarization task, one text sequence may be the original document, while the other might be the generated text summarization by LLMs. To distinguish between the two text sequences, in our paper we referred to one of the text sequences as \u201creference\u201d and the other as \u201ccandidate\u201d. Accordingly, we formulate the problem to check whether the candidate is consistent with the reference or not, as presented in Section 2.  We realize that such wording might have caused confusion and will change them to more neutral terms such as \u201ctext sequence 1\u201d and \u201ctext sequence 2\u201d.\n\nBelow we provide a detailed explanation of the \u201creference\u201d used in our experiments:\n- **Paraphrase detection tasks**. For example, in the Quora Question Pairs (QQP) dataset, each question pair is annotated with a binary value indicating whether the two questions are paraphrases of each other.   We consider \u201cquestion1\u201d as the \u201creference\u201d and \u201cquestion2\u201d as the \u201ccandidate\u201d, and our task is to evaluate if the candidate is consistent with the reference in semantic meaning.  \n- **Summarization tasks**. For example, SummEval datasets include original source articles, machine summaries, and human summaries. Our \u201creference\u201d in this task is the original source article, and our \u201ccandidate\u201d is the machine summaries. Our task is to check the factual consistency between them without relying on any additional golden reference or ground truth. \n\n> **W2: The lack of comparisons with other consistency methods like SMART, NLI, and Bleurt makes me question the thoroughness of the paper.** \n\nThanks for your suggestion. We have added some additional experiments for comparison. Since SMART has not released open-source code yet, we cannot evaluate its performance in our experiment setting, i.e., the summary-level correlation on SummEval datasets. Instead, we implement the system-level correlation used in SMART, for a fair comparison of these baseline methods. As shown in the table below, our method outperforms SMART in terms of all metrics, including the SMART series provided in their paper. \n\n| Metric   | S1-CHRF| S2- CHRF | SL- CHRF | S1-BLEURT| S2-BLEURT | SL-BLEURT | DCE-AMC (ours) |\n| -----------------  | ------- |------- |------- |------- |------- |------- |------- |\n| Kendall tau | 0.733 | 0.700 | 0.733 | 0.667 | 0.750 | 0.567 | **0.799** |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326188018,
                "cdate": 1700326188018,
                "tmdate": 1700328156628,
                "mdate": 1700328156628,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qpfawhsr6Y",
                "forum": "wk77w7DG1N",
                "replyto": "12ln8caGjA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2NAw - part 2/2"
                    },
                    "comment": {
                        "value": "> **W3: Given that the reference and candidate documents might have varying numbers of sentences, how do you handle sentence-level comparisons? Specifically, do you employ any sentence matching techniques, and if so, how are they implemented?**\n\nWe would like to clarify that the sentence-level comparison strategy used in our DCR framework is not to compare each sentence in the candidate text sequence to each sentence from the reference text sequence (sentence-to-sentence), but to compare each sentence in the candidate text sequence to the entire reference text sequence (sentence-to-paragraph). This design reduces the number of comparison operations and ensures semantic consistency. For instance, in the SummEval task, our objective is to check if the candidate summaries are consistent with the original source article. This is achieved by checking if each sentence in the candidate summaries is consistent with the whole original articles, rather than a sentence-by-sentence matching. Thus, our method does not rely on any sentence-matching techniques, such that there is no issue with the varying number of sentences. \n\n> **W4: Compare with SMART paper and our unique contribution.**\n\nThere are several key differences we would like to highlight:\n\n- SMART leverages the reference summaries for the SUMMEVAL database, which relies on both \"reference summaries\" and \"original articles\" for checking. SMART is therefore a reference-based method. However, our DCR method is a reference-free method, as explained above, without requiring any additional references. \n\n- Our DCR framework does not rely on any sentence-matching techniques.  Instead, our task is to check if each sentence in the candidate is consistent with the original article, which is longer in size and more comprehensive, thus rendering the consistency checking more difficult in our opinion. We overcome this challenge by leveraging LLM agents and show outperformed results. \n\n- Our DCR framework is not only an evaluation metric but also provides an effective solution to consistency improvement. This is achieved by introducing the reasoning component. Our reason-assisted improver, implemented by an LLM agent, allow human to understand the logic and build the foundation for inconsistency mitigation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326302902,
                "cdate": 1700326302902,
                "tmdate": 1700328175677,
                "mdate": 1700328175677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SNuucN7LTS",
                "forum": "wk77w7DG1N",
                "replyto": "qpfawhsr6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response! Here is the code link to SMART https://github.com/google-research/google-research/tree/master/smart_eval"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327714648,
                "cdate": 1700327714648,
                "tmdate": 1700327714648,
                "mdate": 1700327714648,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3wE1KENKhq",
                "forum": "wk77w7DG1N",
                "replyto": "ajL14FNsGB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission6094/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6094/Reviewer_2NAw"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your efforts in expanding the experiment. Although the current results are limited by time constraints and open resource quality, I am confident these issues will be addressed in your next version. Regarding the challenges faced with SMART, I recommend reaching out directly to the first author of SMART for assistance.\n\nBased on these considerations, I am adjusting my score to 6 (weak accept), indicating my support for including this work in the conference."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6094/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443988500,
                "cdate": 1700443988500,
                "tmdate": 1700443988500,
                "mdate": 1700443988500,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]