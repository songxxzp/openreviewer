[
    {
        "title": "Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech"
    },
    {
        "review": {
            "id": "5WjUYJDxZ7",
            "forum": "ale56Ya59q",
            "replyto": "ale56Ya59q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1223/Reviewer_KXQH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1223/Reviewer_KXQH"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose to use VQ-VAE in self-supervised audio quality estimation and enhancement based on solely training with clean audio. Idea is to correlate quantization error in the latent space to quality metrics. Speech enhancement is then performed by the way of finetuning using the adversarial noise. So still no need to feed in noisy samples."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Key idea of using the quantization error in VQ-VAE as the qualityt metric is novel as far as I know and also the idea is quite neat. I like it a lot. Enhacement idea based on this innovation is also quite nice. Experimental results do support the hypotheses."
                },
                "weaknesses": {
                    "value": "- Very little theoretical analysis is found in the paper. When would the proposed method work and it would fail? Can anything be said about it?\n- Key parameters are not empirically, nor theorerically assessed. Especially codebook size appears to be extremely critical parameter. \n- Significance testing should be reported for each computed correlation."
                },
                "questions": {
                    "value": "- How was the commitment weight \\beta = 3 decided?\n- Basically quantization error is the measure that you are using and it for sure does make sense. It would be interesting to see whether some distrubutional arguments can be made about the quantization errors. Note that those errors are scalar quantities and thus could be easily plotted and visually inspected."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Reviewer_KXQH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698600896669,
            "cdate": 1698600896669,
            "tmdate": 1700661473513,
            "mdate": 1700661473513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7IdAcnl0jE",
                "forum": "ale56Ya59q",
                "replyto": "5WjUYJDxZ7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KXQH (Q1~Q3)"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer KXQH likes our idea of VQScore and considering our work is with novelty, and your comments are very helpful for us on improving presentation quality and clarifications with wider impacts. Please find the corresponding responses below:\n\n**Q1: Very little theoretical analysis is found in the paper. When would the proposed method work and it would fail? Can anything be said about it?**\n\n- Our preliminary experiment results show that the VQScore is only comparable to SpeechLMScore in the VoiceMOS 2022 challenge test set (a challenge to predict MOS for synthesized speech from TTS and voice conversion). One possible reason is that VQScore trained on LibriSpeech clean-460 hours only uses <10% training data of SpeechLMScore. Another possible reason is that if we look at each frame generated by a TTS system, it may look like a clean frame. However, in the TTS evaluation, the evaluation may focus more on global conditions such as **NATURALNESS**, etc. In other words, it cares more about the relation of each frame with the other. On the other hand, VQScore pays more attention to the degradation of each frame.\n\n**Q2: Key parameters are not empirically, nor theorerically assessed. Especially codebook size appears to be extremely critical parameter**\n\n- Thank you for pointing this out, we have added more description in the paper (section I in the Appendix) to describe how we decide the hyper-parameters. We decided the hyper-parameters (e.g., \\beta, and codebook size, etc.) based on the performance of DNSMOS (OVRL) on validation set. For quality estimation, it is the linear correlation coefficient (LCC) with VQScore. For the speech enhancement, it is the score itself.\n\n- LCC between DNSMOS (OVRL) and VQScores with different codebook sizes are shown in Table r1. From this table, we can observe that except for very small codebook dimensions (i.e., 16), the performance is quite robust to codebook number and dimension. The case for speech enhancement is similar, except for very small codebook dimensions and numbers, the performance is robust to the codebook setting. These results have been added to the Appendix of the paper.\n\n- Table r1: LCC between DNSMOS (OVRL) and VQScores with different codebook size.\n| Codebook size (number, dim)     | LCC | \n| :----:        |    :----:   |   \n| (1024, 32)| 0.8332| \n| (2048, 16)| 0.7668 | \n| **(2048, 32)**|\t**0.8386** | \n| (2048, 64)|\t0.8317 | \n| (4096, 32)|\t0.8297 | \n\n\n\n**Q3: Significance testing should be reported for each computed correlation.**\n\n- Thank you for the suggestion. In the following tables, we report the T-test between Proposed + AT and different baselines on the DNS1 test set to show the statistical significance. **!!Please check Section K in the Appendix for the colored version.!!** In the tables, results shown in red, and blue represent Proposed + AT is statistically significant (p-value<0.05) better and worse than the baseline, respectively (results with black color represent no statistically significant). From the tables, we can observe that Proposed + AT is usually statistically significant better on the DNSMOS (BAK) and DNSMOS (OVR) which implies better noise removal ability. This improvement mainly comes from VQ and AT. \n\n- Table r2: P-value of DNSMOS (SIG) between Proposed + AT and baselines \non the DNS1 test set.\n|| Noisy     | Wiener | Demucs| CNN-Transformer|\n| :----:  |    :----:   |   :---  |    :----:   |   :----:   | \n| Real  |   0.683   |   0.128 |   0.0191   |   0.0025  | \t\t\t\n| Noreverb\t|   0.743   |   2.29e-14 |   0.00126   |   0.265  |\t\t\t\t\n| Reverb\t|   4.35e-24   |   3.46e-09 |   1.55e-11   |   0.0019  |\t\t\t\t\n\t\t\t\n\n- Table r3: P-value of DNSMOS (BAK) between Proposed + AT and baselines \non the DNS1 test set.\n|| Noisy     | Wiener | Demucs| CNN-Transformer|\n| :----:  |    :----:   |   :---  |    :----:   |   :----:   | \n| Real  |  3.77e-104\t | 3.90e-83\t | 3.53e-14\t | 3.01e-10  | \t\t\t\n| Noreverb\t|   3.79e-61| 4.69e-49| 2.97e-08| 0.0018  |\t\t\t\t\n| Reverb\t|   4.74e-91| 1.92e-46| 0.114| 0.422 |\t\n\n\n- Table r4: P-value of DNSMOS (OVR) between Proposed + AT and baselines \non the DNS1 test set.\n|| Noisy     | Wiener | Demucs| CNN-Transformer|\n| :----:  |    :----:   |   :---  |    :----:   |   :----:   | \n| Real  |  1.35e-36  | \t4.84e-31 | 8.97e-07\t | 0.0001  | \t\t\t\n| Noreverb\t|   7.84e-43| 1.18e-47| 0.0042| 0.141  |\t\t\t\t\n| Reverb\t|   9.20e-54| 8.70e-34| 3.21e-08| 0.205 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410798517,
                "cdate": 1700410798517,
                "tmdate": 1700410798517,
                "mdate": 1700410798517,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5asjhks0E6",
                "forum": "ale56Ya59q",
                "replyto": "wZUIAGrhzb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1223/Reviewer_KXQH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1223/Reviewer_KXQH"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal looks good to me"
                    },
                    "comment": {
                        "value": "In terms of my concerns, I find that authors responded clearly and insightfully. The only concern left for me is Q1, I feel that authors  did not try to really answer my, admittedly, fairly vague comment. \n\nHowever, I am willing to raise my grade by one point."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661439882,
                "cdate": 1700661439882,
                "tmdate": 1700661439882,
                "mdate": 1700661439882,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CAhe1wQ22E",
            "forum": "ale56Ya59q",
            "replyto": "ale56Ya59q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1223/Reviewer_eoAZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1223/Reviewer_eoAZ"
            ],
            "content": {
                "summary": {
                    "value": "Contributions of the paper are two-fold; first, the authors propose a speech quality measure based on the comparison of speech embeddings before and after vector quantization using a VQ-VAE. Two metrics were used for comparison: a L_2 norm and a cosine similarity metric. During the experimental phase, the authors compare the proposed metric with some previously proposed objective speech quality metrics on four data sets that contain human perception metrics.  Among the metrics used for comparison, they included SNR, PESQ, SIG, BAK, and OVR.  Based on the SNR results, the authors also suggest that the proposed method can estimate SNR in a frame-based approach. Second,  the paper presents a model distillation approach using a two steps learning process where a noise component is learned such that it minimizes the performance of the quantization process, and a second step where the encoder of the student model is trained to revert that behavior, making it more robust to noisy samples. The decoder is trained to reduce the reconstruction error, as in any denoising approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses a problem of interest in the state of the art that does not have a clear solution. The proposed solution for speech quality assessment is simple, yet it could be effective. The proposed method for speech enhancement requires only clean data and the proposed adversarial training is an interesting alternative to"
                },
                "weaknesses": {
                    "value": "The novelty of the paper is limited. Quality metrics comparing embedding has already been proposed in multimodal or generative contexts including speech such as the Fr\u00e9chet Audio Distance. Moreover, the authors found a low correlation between the proposed score and the quality benchmarks when the speech quality is poor, limiting the proposed measure's reliability. The results of the proposed approach for speech enhancement are still behind those of supervised models."
                },
                "questions": {
                    "value": "- Notation of equations 5 and 6 is inconsistent. According to Eq. 5, Lce is a function of two arguments, but Eq. 6 does not develop it correctly. Notation in general, should be reviewed.\n- The authors should show evidence of the training stability during the proposed adversarial training. \u00bfIs there any risk that during training, the model collapses to select the same token?\n- Other adversarial training strategies suffer from the high cost of generating adversarial samples, and the proposed approach does not seem to do differently. The authors should include analyses regarding computational load and scalability.\n- The paper should also include experiments to support the claims that the proposed approach can exhibit better generalization capabilities to new domains than supervised models. Telephony speech or artificially generated speech should also be included."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Reviewer_eoAZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765351318,
            "cdate": 1698765351318,
            "tmdate": 1700661816691,
            "mdate": 1700661816691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nnvX5atSCd",
                "forum": "ale56Ya59q",
                "replyto": "CAhe1wQ22E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eoAZ (Q1~Q3)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer eoAZ for considering our work addresses a problem of interest that does not have a clear solution. Your suggestion and comments for further enhancing the submission quality are very helpful. Please find the responses below:\n\n**Q1: The novelty of the paper is limited. Quality metrics comparing embedding has already been proposed in multimodal or generative contexts including speech such as the Fr\u00e9chet Audio Distance.**\n\n- Thank you for the comment. One main difference between our VQscore and Fr\u00e9chet Audio Distance (FAD) is that FAD cannot evaluate the quality of each **INDIVIDUAL** audio clip as VQscore. As pointed out in the paper of FAD [r1] (section 3.1 Definition): \u201cUnlike existing audio evaluation metrics, FAD does not look at individual audio clips, but instead compares embedding statistics generated on the whole evaluation set with embedding statistics generated on a large set of clean music (e.g. the training set).\u201d In other words, FAD compares the distance between two sets (distributions), and the number of elements in the set has to be large enough to make FAD reliable. **In summary, FAD can only be used to evaluate the performance of a generative model, not a single audio clip.**\n\n- As far as we know, we are the **FIRST** one to propose using quantization error to measure speech quality without the need for any quality label during training. **Note that being able to calculate the quantization error is a unique property of the VQ-VAE that other autoencoders do not have.**\n\n[r1] Kilgour, K., Zuluaga, M., Roblek, D., & Sharifi, M. (2018). Fr\\'echet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms. arXiv preprint arXiv:1812.08466.\n\n**Q2: Moreover, the authors found a low correlation between the proposed score and the quality benchmarks when the speech quality is poor, limiting the proposed measure's reliability. The results of the proposed approach for speech enhancement are still behind those of supervised models.**\n\n- It is difficult to ask an objective quality metric to always align with subjective scores perfectly in **EVERY** condition. **For example, in Fig. 2 of [r2], the widely-used quality metrics, PESQ and POLQA also perform less reliably when the speech quality is poor.** From our experimental results shown in Table 2, our VQScore is comparable to supervised SOTA (DNSMOS), which is trained on large-scale paired data while our VQscore doesn\u2019t need any label for model training.\n\n- Our proposed SE model is only behind supervised models in the **matched** condition (Table 3), where the training and testing sets were from the **SAME** source. However, in practical applications, where the training and testing sets were from **DIFFERENT** sources (Tables 4 and 5), our self-supervised SE model shows better generalization capabilities than the supervised Demucs model (from Meta) and is comparable to CNN-Transformer, which has a similar model architecture as our self-supervised SE. \n\n- We have added an ASR experimental result in Table r1. We apply Whisper [r3] as the ASR model and compute the WER of speech generated by different SE models [r4] on the VoiceBank-DEMAND noisy test set. The results show that all the SE can improve the WER performance, and our proposed method can achieve the **lowest** WER.\n\n- Table r1: WER of speech generated by different SE models on the \nVoiceBank-DEMAND noisy test set.\n|| Noisy|\tWiener|\tDemcus|\tCNN-Transformer|\tProposed | \n| :----:        |    :----:   |    :----:   |  :----:        |    :----:   |    :----:   | \n|Whisper ASR|\t14.25|\t12.60|\t13.75|\t11.84|\t**11.65**|\n\n\n[r2] Reddy, C. K., Gopal, V., & Cutler, R. (2021, June). DNSMOS: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021.\n\n[r3] Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023, July). Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning (pp. 28492-28518). PMLR.\n\n[r4] Defossez, A., Synnaeve, G., & Adi, Y. (2020). Real time speech enhancement in the waveform domain. arXiv preprint arXiv:2006.12847.\n\n**Q3: Notation of equations 5 and 6 is inconsistent. According to Eq. 5, Lce is a function of two arguments, but Eq. 6 does not develop it correctly. Notation in general, should be reviewed.**\n\n- Sorry for the confusion. The additional argument Cv in Eq. 6 is the **FIXED** dictionary which can be treated as constant during AT. We have modified Eq.5 in the paper to include Cv as a function input to avoid confusion."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410127644,
                "cdate": 1700410127644,
                "tmdate": 1700410127644,
                "mdate": 1700410127644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MnaOvU77Ww",
                "forum": "ale56Ya59q",
                "replyto": "CAhe1wQ22E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1223/Reviewer_eoAZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1223/Reviewer_eoAZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answer. You clarified many of my previous questions. Regarding the FDA metric, It is clear that FDA compares the distributions of two sets of audio embeddings because it was proposed in the context of the audio generation task, where there are no target outputs to compare with. Notwithstanding the context, the comparison of embeddings as a quality metric was proposed in there. Anyway, the proposal of comparing embedding and quantized embeddings in VQ-VAE is an interesting alternative for individual evaluations. \n\nThe concerns about the model collapsing are not exclusive of GANs, as the authors claimed. The literature has reported problems of codebook collapse in VQ-VAE models, which is analogous to mode collapse in continuous generative models. So, concerns in this respect remain."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654245810,
                "cdate": 1700654245810,
                "tmdate": 1700654284819,
                "mdate": 1700654284819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OmFQhUWJbv",
                "forum": "ale56Ya59q",
                "replyto": "CAhe1wQ22E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eoAZ"
                    },
                    "comment": {
                        "value": "**We are happy to hear that some of your concerns are addressed.** Regarding codebook collapse in VQ-VAE, we have tried a technique proposed in SoundStream [r1] to increase codebook usage by replacing dead codes. However, using this technique only brings limited improvement. In fact, in our experience, **as long as the codebook size is large enough (see Table r1 in https://openreview.net/forum?id=ale56Ya59q&noteId=7IdAcnl0jE), codebook collapse is not a serious problem.**\n\nPlease note that the purpose of this paper is **NOT** to solve the codebook collapse in VQ-VAE. We try to apply VQ-VAE to solve some difficult challenges with **novel** methods.\n\n\n[r1] Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., & Tagliasacchi, M. (2021). Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30, 495-507."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656652064,
                "cdate": 1700656652064,
                "tmdate": 1700656836981,
                "mdate": 1700656836981,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wt5w8fVcWv",
            "forum": "ale56Ya59q",
            "replyto": "ale56Ya59q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1223/Reviewer_DUwZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1223/Reviewer_DUwZ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper authors propose VQScore to measure speech quality, which is based on the quantization error of VQ-VAE. It's a self-supervised metric without paired speech and noisy data in training. Based on it, the authors propose to improve speech enhancement with self-distillation with adversarial training. Experimental results show the effectiveness of the proposed methodology."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed methodology is technical sound. Its training uses clean speech data only, and this helps reduce dependencies on noisy/clean speech pairs to develop models for speech quality measure and speech enhancement. Overall, this paper clearly describes the proposed approach, with well designed experiments and analysis."
                },
                "weaknesses": {
                    "value": "I think the experimental section could be further strengthened with more details added. Please see the Questions section below."
                },
                "questions": {
                    "value": "1. How to determine the values for several hyper-parameters, e.g. \\beta in equation (3), codebook size etc.\n2. For the results tables, could authors include std to show if difference is statistically significant?\n3. For Table 3, could authors add a short summary about comparing model complexity for the proposed approach vs. baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887787753,
            "cdate": 1698887787753,
            "tmdate": 1699636048665,
            "mdate": 1699636048665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QPd27V3ums",
                "forum": "ale56Ya59q",
                "replyto": "Wt5w8fVcWv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DUwZ (Q1~Q2)"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer DUwZ for considering our work is clear with well-designed analysis and very thank your reviewing efforts and advice. Your comments for improvement are professional and constructive. Please find the responses below:\n\n**Q 1:\tHow to determine the values for several hyper-parameters, e.g. \\beta in equation (3), codebook size etc.**\n\n- Thank you for pointing this out, we have added more description in the paper (section I in the Appendix) to describe how we decide the hyper-parameters. We decided the hyper-parameters (e.g., \\beta, and codebook size, etc.) based on the performance of DNSMOS (OVRL) on validation set. For quality estimation, it is the linear correlation coefficient (LCC) with VQScore. For the speech enhancement, it is the score itself.\n\n- As pointed out in the original paper of VQ-VAE [r1]: \u201cWe found the resulting algorithm to be quite robust to \u03b2, as the results did not vary for values of \u03b2 ranging from\u2026\u201d. Our ablation study shown in the following table shows a similar trend. Although the performance is not sensitive to \u03b2, we still select \u03b2 based on the performance on the validation set. Specifically, the DNSMOS (OVRL) of the validation set.\n\n- Table r1: DNSMOS (OVRL) of enhanced speech from models trained with different \u03b2.\n| \u03b2      | DNSMOS (OVRL) | \n| :----:        |    :----:   |   \n| 1      | 2.865       | \n| 2\t  | 2.872 | \n| **3**\t  | **2.876**      | \n\t\t\n\n- LCC between DNSMOS (OVRL) and VQScores with different codebook sizes are shown in Table r2. From this table, we can observe that except for very small codebook dimensions (i.e., 16), the performance is quite robust to codebook number and dimension. The case for speech enhancement is similar, except for very small codebook dimensions and numbers, the performance is robust to codebook setting. These results have been added to the Appendix of the paper.\n\n- Table r2: LCC between DNSMOS (OVRL) and VQScores under different codebook size.\n| Codebook size (number, dim)     | LCC | \n| :----:        |    :----:   |   \n| (1024, 32)| 0.8332| \n| (2048, 16)| 0.7668 | \n| **(2048, 32)**|\t**0.8386** | \n| (2048, 64)|\t0.8317 | \n| (4096, 32)|\t0.8297 | \n\n**Q2:\tFor the results tables, could authors include std to show if difference is statistically significant?**\n\n- Thank you for the suggestion. In the following tables, we report the T-test between Proposed + AT and different baselines on the DNS1 test set to show the statistical significance. **!!Please check Section K in the Appendix for the colored version!!** In the tables, results shown in red, and blue represent Proposed + AT is statistically significant (p-value<0.05) better and worse than the baseline, respectively (results with black color represent no statistically significant). From the tables, we can observe that Proposed + AT is usually statistically significant better on the DNSMOS (BAK) and DNSMOS (OVR) which implies better noise removal ability. This improvement mainly comes from VQ and AT. \n\n- Table r3: P-value of DNSMOS (SIG) between Proposed + AT and baselines \non the DNS1 test set.\n|| Noisy     | Wiener | Demucs| CNN-Transformer|\n| :----:  |    :----:   |   :---  |    :----:   |   :----:   | \n| Real  |   0.683   |   0.128 |   0.0191   |   0.0025  | \t\t\t\n| Noreverb\t|   0.743   |   2.29e-14 |   0.00126   |   0.265  |\t\t\t\t\n| Reverb\t|   4.35e-24   |   3.46e-09 |   1.55e-11   |   0.0019  |\t\t\t\t\n\t\t\t\n\n- Table r4: P-value of DNSMOS (BAK) between Proposed + AT and baselines \non the DNS1 test set.\n|| Noisy     | Wiener | Demucs| CNN-Transformer|\n| :----:  |    :----:   |   :---  |    :----:   |   :----:   | \n| Real  |  3.77e-104\t | 3.90e-83\t | 3.53e-14\t | 3.01e-10  | \t\t\t\n| Noreverb\t|   3.79e-61| 4.69e-49| 2.97e-08| 0.0018  |\t\t\t\t\n| Reverb\t|   4.74e-91| 1.92e-46| 0.114| 0.422 |\t\n\n\n- Table r5: P-value of DNSMOS (OVR) between Proposed + AT and baselines \non the DNS1 test set.\n|| Noisy     | Wiener | Demucs| CNN-Transformer|\n| :----:  |    :----:   |   :---  |    :----:   |   :----:   | \n| Real  |  1.35e-36  | \t4.84e-31 | 8.97e-07\t | 0.0001  | \t\t\t\n| Noreverb\t|   7.84e-43| 1.18e-47| 0.0042| 0.141  |\t\t\t\t\n| Reverb\t|   9.20e-54| 8.70e-34| 3.21e-08| 0.205 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409161793,
                "cdate": 1700409161793,
                "tmdate": 1700409161793,
                "mdate": 1700409161793,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zphhgQSiS4",
            "forum": "ale56Ya59q",
            "replyto": "ale56Ya59q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1223/Reviewer_CJHD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1223/Reviewer_CJHD"
            ],
            "content": {
                "summary": {
                    "value": "This is an interesting paper about developing a self-supervised speech enhancement solution. It does not use external noise corpus and uses a variation of VQ-VAE. It focuses on developing robust encoder and decoder using adversarial training (AT). They first train a regular VQ-VAE. Authors aptly describe the main idea as, \"Once the encoder can map the noisy speech to the corresponding tokens of clean speech, or the decoder has the error correction ability, speech enhancement can be achieved.\" AT is then used to fine-tune encoder and decoder. Authors show high correlation of their proposed metric with other quality metrics (real+hand engineered)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Novelty: Authors have attempted to combine VQ-VAE and AT to create an enhancer. This is novel per my knowledge.\n2. Choice of models to compare with is good."
                },
                "weaknesses": {
                    "value": "1. No downstream evaluation (diarization, speaker recognition, ASR, etc.) provided which I would expect for ICLR.\n2. I am not able to determine if high linear correlation of proposed metric is enough to say this enhancement will work on real noisy datasets. Remember the goal of enhancement is to remove noise (and other unwanted information) such that it can used on a plethora end applications. It is not just about perceptually making it better. STOI-like metrics are ignored in this work which quantifies intelligibility. Note that it is also possible to produce good sounding audio which is not very intelligible.\n3. Lack of ablation or other analysis on proposed method. Since the proposed method is the main technical contribution, I would expect it to be evaluated more robustly.\n4. Noise corpora is not used which is readily available. It would be interesting to see how using external noises can improve model performance. AT noise is not the only noise that is readily available. In fact AT is slow.\n5. If TorchaudioSquim has mismatch issues (as authors point out), it can be retrained to make it more appropriate for comparison with proposed method.\n6. Table 4,5 is missing PESQ, STOI numbers. (CHECK: https://paperswithcode.com/sota/speech-enhancement-on-demand). I dont understand how Weiner is best in SIG (real subset, Table 4). I am not sure dereverberation should be investigated in this paper. DNS1 details are also not mentioned."
                },
                "questions": {
                    "value": "1. Why role of PESQ is downplayed? Authors say it is something to do with generative models but they did not expand or give citations to support this idea.\n2. Why downstream evaluation is not done? To publish a new enhancement solution in ICLR, in my personal opinion, it becomes critical."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1223/Reviewer_CJHD"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1223/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699587558937,
            "cdate": 1699587558937,
            "tmdate": 1700738881579,
            "mdate": 1700738881579,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WyPTkltNqM",
                "forum": "ale56Ya59q",
                "replyto": "zphhgQSiS4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1223/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CJHD (Q1~Q2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer CJHD for considering our work as an interesting paper with novelty. Your suggestions are with decent insights for us to revise parts of the submission draft. Please find the responses below:       \n\n**Q1: No downstream evaluation (diarization, speaker recognition, ASR, etc.) provided which I would expect for ICLR.**\n\n**Q8: Why downstream evaluation is not done? To publish a new enhancement solution in ICLR, in my personal opinion, it becomes critical.**\n\n- Thank you for the suggestion, we have added an ASR experimental result in Table r1 and the Appendix of the paper. We apply Whisper [r1] as the ASR model and compute the WER of speech generated by different SE models [r2] on the VoiceBank-DEMAND noisy test set. The results show that all the SE can improve the WER performance, and our proposed method can achieve the lowest WER.\n\n- Table r1: WER of speech generated by different SE models on the \nVoiceBank-DEMAND noisy test set.\n||Noisy|Wiener|Demcus|CNN-Transformer|Proposed|\n|  :----: |  :----: |  :----: |  :----: |  :----: |  :----: |\n|Whisper ASR |14.25 |12.60 |13.75 |11.84 |**11.65**| \n\n [r1] Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023, July). Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning (pp. 28492-28518). PMLR.\n\n[r2] Defossez, A., Synnaeve, G., & Adi, Y. (2020). Real time speech enhancement in the waveform domain. arXiv preprint arXiv:2006.12847.\n\n\n**Q 2: I am not able to determine if high linear correlation of proposed metric is enough to say this enhancement will work on real noisy datasets. Remember the goal of enhancement is to remove noise (and other unwanted information) such that it can used on a plethora end applications. It is not just about perceptually making it better. STOI-like metrics are ignored in this work which quantifies intelligibility. Note that it is also possible to produce good sounding audio which is not very intelligible.**\n\n- **For speech quality estimation:**\nWe have added new results of linear correlation between STOI and our VQScore on the VoiceBank-DEMAND noisy test set in Table r2 (also added the results in Table 1 in the paper). We can observe that, for STOI, the VQScore calculated in the code space (z) still has a much larger correlation than that calculated in the signal space (x) and the self-supervised baseline, SpeechLMScore.\n\n- For real noisy datasets, our test sets, \u2018Tencent_wR\u2019 contains speech recorded in a realistic reverberant room, and \u2018IUB_cosine\u2019 dataset includes a close-talking mic (reference) and chest/shoulder mic (noisy). Therefore, real noisy datasets have already been considered in our test sets.\n\n- Table r2: LCC between STOI and our VQScore on the VoiceBank-DEMAND noisy test set.\n||SpeechLMScore|VQScore(cos,x)|VQScore(cos,z)|\n|  :----: |  :----: |  :----: |  :----: | \n|STOI| 0.6023|0.5012|**0.7490**|\n\n\n- **For speech enhancement:**\nThe reason we downplayed the role of intrusive metrics (clean reference is needed) such as PESQ, and STOI for SE evaluation is detailly explained in the response of Q7. In summary, the goal of generative models is to model the **distribution** of clean speech **rather than minimize the difference with its clean counterpart in the signal space**. Therefore, intrusive metrics may not be able to appropriately evaluate its performance.\n\n- Please refer to the response of Q7 for the discussion and related citations, etc. On the other hand, the results shown in Table r1 show that the proposed method can effectively decrease WER, which quantifies the intelligibility of an ASR.\n\n- For real noisy datasets, both DNS 1 (Table 4) and DNS3 (Table 8) test sets contain REAL noisy subsets, where our self-supervised SE model shows better performance than the supervised model, Demucs."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407293129,
                "cdate": 1700407293129,
                "tmdate": 1700407293129,
                "mdate": 1700407293129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lNaQDjZRE9",
                "forum": "ale56Ya59q",
                "replyto": "BAlLXUrFvw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1223/Reviewer_CJHD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1223/Reviewer_CJHD"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for addressing my concerns. It has been delightful to learn more about this topic and see the detailed results. I have therefore substantially increased my score from 3 to 6."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1223/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739172373,
                "cdate": 1700739172373,
                "tmdate": 1700739172373,
                "mdate": 1700739172373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]