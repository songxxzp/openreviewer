[
    {
        "title": "Understanding and addressing spurious correlation via Neural Tangent Kernels: A spectral bias perspective"
    },
    {
        "review": {
            "id": "PBZuM8UOwZ",
            "forum": "89AOrk05uy",
            "replyto": "89AOrk05uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3451/Reviewer_XMhp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3451/Reviewer_XMhp"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies how spurious correlations are exploited and encoded in neural networks through the lens of the NTK in the infinite width limit, where the kernel governing the dynamics is fixed at initialization. This allows the training dynamics to be entirely predictable (in closed form), and allows for an eigendecomposition that allows for the extraction of eigenfunctions that are learned at different speeds based on the magnitude of the corresponding eigenvalue. In particular, through \"saliency maps\", the authors exploit this to identify that the leading eigenfunctions have high activations in the regions where the spurious features are present. This indicates that the network does make use of the spurious features to make predictions. The authors explain this in terms of the \"simplicity bias\" of the NTK: when the spurious feature can be explained with low-frequency functions, the signal will be picked up by the low-frequency eigenfunctions of the NTK, thus resulting in poor generalization. On the other hand, when the spurious feature is relatively complex compared to the generalizing signal, the model does generalize."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Presentation**: The paper is very well-written, and the results are extremely well-presented. The logical argument of how spurious correlations affect generalization in the NTK regime flows very nicely from the author's considerations on previously observed phenomena such as simplicity bias, and adversarial robustness of the NTK. \n2. **Quality**: the experiments are directly targeting the research question addressed by the authors. The visualization of the saliency map in Figure 2 explains well how the eigenfunctions encode the information of the spurious features. I also appreciated the in-depth discussion on the role of feature complexity, and how the spurious feature needs to be \"simple\" in order to cause problems in generalization for the NTK.\n3. **Originality**. The problem of explaining spurious correlation under the NTK framework is novel, to my knowledge."
                },
                "weaknesses": {
                    "value": "Main concern:\n1. **Role of depth**. The authors state that deeper networks learn more complex features, and suggest that this can help to improve performances under the presence of spurious correlation as it is shown in Figure 5. However, it is unclear whether the performance improvement is due to the \"feature complexity\" argument put forth by the authors. There is no evidence provided that a similar improvement would happen even in the absence of the spurious feature. Also, there is no saliency map visualization confirming that the spurious features are not encoded in the leading eigenfunctions at larger depths. In Figure C.1, it is hard to confirm the authors' claim, as the gradient correlation seems to increase uniformly across all the groups. \nMinor:\n2. **Scope of the paper**. The paper adopts all existing techniques. For instance, saliency maps have been applied to NTK to study adversarial robustness (Tsilivis and Kempe, 2022), correctly cited by the paper. Also, the scope of the paper could have been more broadly enlarged to include the role of feature learning and changes in the NTK through training (some experiments are run in the Appendix, but the role of feature learning is not discussed). However, I consider these minor issues. I think that even without these minor improvements, the paper still deserves publication."
                },
                "questions": {
                    "value": "1. Role of regularization. How does weight decay or other type of regularization affect the encoding of spurious features in the NTK?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698417119052,
            "cdate": 1698417119052,
            "tmdate": 1699636297631,
            "mdate": 1699636297631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nx1xrybFy4",
                "forum": "89AOrk05uy",
                "replyto": "PBZuM8UOwZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XMhp"
                    },
                    "comment": {
                        "value": "Thank you for your review. Your concerns will be addressed as follows:\n\n> There is no evidence provided that a similar improvement would happen even in the absence of the spurious feature\n\nAs demonstrated by [1] in both theoretical and empirical contexts, elevating the depth (below the optimal depth) increases the capacity to learn complex features. This is attributed to the condition number approaching one with increasing depth, signifying a faster convergence rate for high-frequency components. Importantly, this argument remains valid irrespective of the existence of spurious correlations.\n\n> In Figure C.1, it is hard to confirm the authors\u2019 claim, as the gradient correlation seems to increase uniformly across all the groups\n\nAs depicted in Figure 6, once surpassing the optimal depth, the robustness diminishes. Figure C.1 explains this phenomenon, revealing that deeper models result in a broader spectrum that inevitably captures noise. Consequently, gradient correlations become similar across all groups.\n\n> Role of regularization. How does weight decay or other type of regularization affect the encoding of spurious features in the NTK?\n\nThank you for the suggestion. In this work, we have not considered the effect of regularization. However, it has been thoroughly studied by [2], and their work will serve as a valuable reference for our future work.\n\n--- \n[1] G. Yang et al. A Fine-Grained Spectral Perspective on Neural Networks https://arxiv.org/abs/1907.10599\n\n[2] Jaehoon Lee et al. Finite Versus Infinite Neural Networks: An Empirical Study http://arxiv.org/abs/2007.15801"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236186351,
                "cdate": 1700236186351,
                "tmdate": 1700236186351,
                "mdate": 1700236186351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ObOccRd61U",
                "forum": "89AOrk05uy",
                "replyto": "Nx1xrybFy4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3451/Reviewer_XMhp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3451/Reviewer_XMhp"
                ],
                "content": {
                    "title": {
                        "value": "Answer to rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their explanation regarding the role of depth. I would still suggest performing a preliminary experiment where the saliency map is plotted at multiple points during training to see the effect of feature learning. This would give more food for thought and motivate future work. I am keeping my score for now."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567343232,
                "cdate": 1700567343232,
                "tmdate": 1700567343232,
                "mdate": 1700567343232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x0K2AEeDo0",
            "forum": "89AOrk05uy",
            "replyto": "89AOrk05uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3451/Reviewer_hgH8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3451/Reviewer_hgH8"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the problems posed by spurious correlations when learning with the neural tangent kernel and proposes a technique for improving NTK robustness when such correlations exist. The paper introduces a model of spurious correlation, where true labels $Y$ are independent of spurious features $S$ at test time, but have a planted dependence at training time modulated by parameter $\\alpha$. They consider several tasks that combine spurious features from a simpler learning task with real features from a more difficult task.\n\nThey demonstrate empirically that the spurious features are most strongly associated with low-frequency eigenvectors of the NTK matrix $H$ by giving a visualization of salience maps for each feature and computing eigenvector alignment scores. Given the association between spurious features and low-frequency features, they propose a modification of the kernel that scales eigenvalues to reduce the impact of low-frequency features. They demonstrate empirically that this approach improves generalization performance on the tasks and erases the gap between performance on different subgroups (characterized by the label $Y$ and the spurious features $S$). The appendix includes a hyper-parameter sweep over the kernel modification parameters $\\gamma$ and $\\beta$."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is overall well-written and its survey of the robustness literature appears to be strong. The NTK background is presented in an intuitive and understandable way. The tasks are cleanly presented and dataset visualizations in Figures 1 and 3 are useful. The thoroughness demonstrated by the hyper-parameter sweeps in Figures C.6 and C.7 is appreciated. The problem is certainly compelling, and the idea of tuning the spectrum of the kernel matrix is interesting. While I find the connections to deep neural networks limited, I think this work is a good foundation for a rigorous experimental study of spurious bias reduction for kernel methods."
                },
                "weaknesses": {
                    "value": "I have two high-level critiques of the work, which I would be interested to hear the authors respond to. The first pertains to the relevance of the NTK and the methods of this work to neural networks, and the second to the generality of the paper's claims about the spectrum of spurious features.\n\n## Relevance of NTK and algorithm to DNNs\n\nThe authors motivate their work by introducing the important issue of spurious correlations in deep neural networks, but they restrict their focus to the NTK approximation of a neural network. While the neural tangent kernel is an appealing model that allows one to apply convex analysis tools to deep learning, both empirical and theoretical work shows that the NTK fails to match neural network performance and cannot perform key deep learning functions like feature learning; consider reading and citing [works](https://papers.nips.cc/paper_files/paper/2019/hash/c133fb1bb634af68c5088f3438848bfd-Abstract.html) [like](https://arxiv.org/abs/2011.14522) [these](https://arxiv.org/abs/2206.10012). The NTK and kernel methods broadly are certainly worthy of study, but this paper does not address the critiques of these models in their introduction or NTK discussion.\n\nThe feature learning critique of the NTK is a particular issue for this paper because the methods use the spectrum of the kernel matrix at initialization, without accounting for the fact that gradient descent tends to encode task-dependent features in the bottom layers of neural networks. Thus, the features analyzed in this work are unlikely to correspond to the actual features encoded in a trained neural network. In addition, the mitigation algorithm presented in the paper only applies when the optimization algorithm has direct access to the kernel, and it doesn't appear to work for neural networks trained with gradient descent. \n\nPerhaps the paper could be more strongly motivated if it were grounded in studying spurious correlations in kernel methods without needing the analogy to DNNs to hold? If the paper is to focus on DNNs, it needs to include a stronger accounting of the limitations of the NTK and a proposal for how it can be applied the feature learning that occurs in neural networks trained with gradient descent.\n\n## Generality of low-frequency spurious features\n\nI am concerned that the principal claim of the paper---that spurious correlations are aligned with low-frequency eigenvectors of the kernel matrix and that they can be mitigated by down-weighting the respective eigenvalues---is an artifact of the datasets considered in Sections 4 and 5. Specifically, the five datasets considered have artificially planted spurious features (e.g. digit color) that are far easier to learn than the true labels (e.g. 0-4 vs 5-9). The dominance of the spurious labels in the low-frequency features of the biased data is made evident for CMNIST and Biased-MNIST in Figure 4, but it's less strong for other tasks, and it's unclear whether this holds for \"real world\" datasets with spurious features. \n\nAs a result, it's unclear whether the algorithmic approach in Section 5 would perform well outside the collection of tasks considered. The paper does little to rule out the possibility that flattening the spectrum of $\\Lambda$ with $\\nu$ will cause problems on other tasks, since it's conceivable that salient features could be lower frequency elsewhere. For this paper to be more compelling, I would like to see whether these results still hold on less artificial datasets, such as Waterbirds and CelebA, which were used in addition to the MNIST-type tasks to evaluate the methods of [the](https://arxiv.org/abs/2210.00055) [papers](https://openreview.net/pdf?id=Zb6c8A-Fghk) they take dataset inspiration from.  \n\n## Other significant concerns\n\nSeveral figures in the paper have significant presentation issues. \n- Figures 4 and 5 are missing numerical axes. \n- Using the normalized index $k$ as the x-axis in Figures 3 and 5 adds some confusion, since it's unclear what number of features is represented by $k$, as it's given as a fraction. \n- There's no discussion in Appendix B about how the choices of planted bias $\\alpha$ were chosen and why choices differ for each dataset. \n- The CIFAR-MNIST task is introduced in Section 4.1 without being referenced elsewhere in the paper body.\n\n## Minor comments\n- In Section 3.1, the actual datasets do not strictly belong to the regime where $\\mathcal{X} = \\mathcal{X}_y \\times \\mathcal{X}_s$, since $\\mathcal{X}$ defines the input space, and several tasks (e.g. CMNIST) have both relevant and spurious features operating on the same pixels.\n- There is some confusing mathematical notation in Section 3.2. $\\dot{\\theta}_t$ is somewhat atypical notation, since $\\theta_t$ is the typical presentation of a step of a discrete dynamical system, rather than $\\theta(t)$ for continuous systems; perhaps $\\dot{\\theta}(t)$ is more appropriate? Equation (3) defines the time derivative as a discrete-time gradient step, which is slightly confusing without limit notation.\n- In Section 4.1, the terms \"bias-aligned\" and \"bias-conflicting\" are introduced, but are only used in the appendix.\n- In Section 4.2 in the sentence starting with \"Based on the decomposition in Eq (10), training a DNN with gradient descent...,\" I would recommend adding the qualification \"in the lazy or NTK regime.\" \n- The caption of Table C.3 ends with an incomplete sentence."
                },
                "questions": {
                    "value": "Can the approaches presented in the paper by applied to all kernel methods, rather than just the NTK? Given the aforementioned issues of the analogy between the NTK and neural networks, perhaps the paper could be motivated more strongly as a method for kernel methods, rather than neural networks.\n\nIs the model of spurious correlation introduced in Section 3.1 novel? If not, I would recommend adding citations to where it's defined in the literature, and if so, I would recommend explaining how it differs from other formulations.\n\nDid you consider including theoretical results about the effectiveness of the algorithm? I suspect that there's a natural theorem to prove about when the approach will provably work for some reasonable data distribution assumptions on the spurious features aligning with the low-frequency eigenvectors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3451/Reviewer_hgH8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772005625,
            "cdate": 1698772005625,
            "tmdate": 1699636297540,
            "mdate": 1699636297540,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zsobQyalzy",
                "forum": "89AOrk05uy",
                "replyto": "x0K2AEeDo0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hgH8"
                    },
                    "comment": {
                        "value": "Thank you for your review. Concerns related to presentation and minor comments will be resolved in the updated version. \n\n> Relevance of NTK and algorithm to DNNs\n\nWe agree that the NTK has its limitations and may not fully match the performance of neural networks in certain aspects. We will revise our introduction and NTK discussion to better acknowledge and address these limitations. Despite the absence of feature learning, NTKs have been extensively utilized [1-5] for understanding and improving model performance. Importantly, the additional results (refer to our response to Reviewer cfRQ) highlight the effectiveness of our method for the learned features, analogous to fine-tuning [1].\n\n> Generality of low-frequency spurious features\n\nPlease refer to our response to Reviewer cfRQ for additional results on Waterbirds and CelebA datasets.\n\n> Is the model of spurious correlation introduced in Section 3.1 novel? If not, I would recommend adding citations to where it's defined in the literature, and if so, I would recommend explaining how it differs from other formulations.\n\nThe model mentioned in Section 3.1 is a well-established notion of spurious correlation [6, 7]. We appreciate the reviewer for bringing this to our attention, and we will include some relevant references in the revised version.\n\n---\n\n[1] Sadhika Malladi et al. A Kernel-Based View of Language Model Fine-Tuning https://arxiv.org/abs/2210.05643\n\n[2] Chia-Hung Yuan and Shan-Hung Wu. Neural Tangent Generalization Attacks https://proceedings.mlr.press/v139/yuan21b.html\n\n[3] Nikolaos Tsilivis and Julia Kempe. What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness? http://arxiv.org/abs/2210.05577\n\n[4] Haonan Wang et al. Deep Active Learning by Leveraging Training Dynamics https://arxiv.org/abs/2110.08611\n\n[5] Mohamad Amin Mohamadi, Wonho Bae, and Danica J. Sutherland. Making LookAhead Active Learning Strategies Feasible with Neural Tangent Kernels https://arxiv.org/abs/2206.12569\n\n[6] Robert Geirhos et al. Shortcut Learning in Deep Neural Networks https://arxiv.org/abs/2004.07780\n\n[7] Yuzhe Yang et al. Change Is Hard: A Closer Look at Subpopulation Shift https://arxiv.org/abs/2302.12254"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236116045,
                "cdate": 1700236116045,
                "tmdate": 1700236116045,
                "mdate": 1700236116045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZZ7AQ8kxEF",
                "forum": "89AOrk05uy",
                "replyto": "zsobQyalzy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3451/Reviewer_hgH8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3451/Reviewer_hgH8"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response and for their willingness to run additional experiments. \n\nWhile I agree with the authors that the NTK remains generally relevant to some extent and that there are some circumstances where training dynamics without feature learning are worth studying, I maintain my belief that feature learning is a particularly central dynamic to consider when evaluating spurious features in neural nets.\n\nI appreciate the work done to implement the additional experiments, but the sharp reduction in average accuracy for Waterbirds and the almost negligible impact on CelebA do not change my current views about the generality of low-frequency features.\n\nAs of now, I maintain my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700884400,
                "cdate": 1700700884400,
                "tmdate": 1700700884400,
                "mdate": 1700700884400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lUC5MmEf5q",
            "forum": "89AOrk05uy",
            "replyto": "89AOrk05uy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3451/Reviewer_cfRQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3451/Reviewer_cfRQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper establishes a connection between spectral bias of neural networks and how that effects their subgroup robustness. They first analyze the eigenfunctions of the NTK Gram matrix and show that features that are spurious correspond to lower-order eigenfunctions, which correspond to larger eigenvalues. When such features are entangled with the label, it can cause the model to rely on these more strongly because eigenfunctions with larger eigenvalues are learned faster. In order to mitigate this effect, the authors propose a modification to the Gram matrix, to make the eigenvalues more similar to each other and encourage learning diverse features. They use some simple variants of MNIST and CIFAR datasets to demonstrate their observations and evaluate their approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper presents a connection between spectral bias of NNs and their tendency to learn spurious features which leads to low subgroup robustness, which is interesting. \n\n2. They propose an approach to improve subgroup robustness that seems effective on some variants of MNIST and CIFAR.\n\n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. ****Although the connection between spectral bias and subgroup robustness is nice, it doesn't really offer much new insights.****\n\nFrom the spectral bias perspective, there are several works [2,3], which have theoretically shown that eigenvectors of the Gram matrix with larger eigenvalues correspond to 'simpler' features, using some notions of simplicity. From the subgroup robustness perspective, there are works [4,5] that show that spurious features are simpler and simplicity bias of NNs causes them to rely more on such features. Based on these works, one can expect that spurious features would correspond to lower order eigenfunctions which will have higher eigenvalues.  \n\n2. ****The idea to make the eigenvalues of the NTK Gram matrix more balanced to improve subgroup robustness is not new (see [1]). The proposed approach is not scalable.****\n\nThe paper is missing discussion about a very closely related work [1]. [1] mainly talks about how spectral bias can cause NNs to rely strongly on a subset of features, and also proposes an approach for spectral decoupling (making the eigenvalues of the NTK Gram matrix more balanced). They show that regularizing the logits of the model can help do that (in some simple settings) and also empirically validate their approach on a range of datasets (including subgroup robustness datasets). Their approach is also more efficient and scalable than the approach presented in the current work, which does not scale to large datasets. Based on this, the proposed method doesn't seem very valuable.\n\n3. ****Limited evaluation.****\n\nThe paper considers MNIST and CIFAR datasets, but evaluation on the usual subrgoup robustness benchmarks (CelebA and Waterbirds) seems missing. The paper also does not compare their approach with any other method to improve subgroup robustness.\n\n\n****References:****\n\n[1] M. Pezeshki et al. ****Gradient Starvation: A Learning Proclivity in Neural Networks**** https://arxiv.org/abs/2011.09468\n\n[2] Y. Cao et al. ****Towards Understanding the Spectral Bias of Deep Learning**** https://arxiv.org/abs/1912.01198\n\n[3] G. Yang et al. ****A Fine-Grained Spectral Perspective on Neural Networks**** https://arxiv.org/abs/1907.10599\n\n[4] Y. Yang et al. ****Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias**** https://arxiv.org/abs/2305.18761\n\n[5] H. Shah et al. ****The Pitfalls of Simplicity Bias in Neural Networks**** https://arxiv.org/abs/2006.07710"
                },
                "questions": {
                    "value": "(See weaknesses above)\n\nCan the authors discuss the contributions of their work in light of this related work [1-5]? I suggest doing a more thorough review of related work and including a detailed discussion on the contributions and what are the insights that are new compared to these other works. And particularly, what is the contribution of this work compared to [1]?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817918081,
            "cdate": 1698817918081,
            "tmdate": 1699636297449,
            "mdate": 1699636297449,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gNNhmBrKPL",
                "forum": "89AOrk05uy",
                "replyto": "lUC5MmEf5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cfRQ"
                    },
                    "comment": {
                        "value": "Thank you for your review. Your concerns will be addressed as follows:\n\n> Although the connection between spectral bias and subgroup robustness is nice, it doesn\u2019t really offer much new insights.\n\nBuilding upon [2,3], our work aims to tackle the issue of spurious correlation. Particularly in our context, the class labels are entangled with simpler features, leading to challenges in the model's ability to learn semantic representations. We agree that [4], [5] and our work are based on the similar concept, i.e., spurious features induce a simplistic bias that degrades the performance of the model. However, none of these studies formulate their investigation within the NTK regime, nor do they present explicit evidence supporting Reviewer\u2019s statement \u201done can expect that spurious features would correspond to lower-order eigenfunctions which will have higher eigenvalues.\u201d In addition to that, [4] proposed the reweighing training set approach, a common technique to address spurious correlation problems, as discussed in the related works section. Our work offers a novel perspective, suggesting that robustness can be achieved without altering the training distribution or the training objective. Instead, it emphasizes that modifying the model\u2019s properties can achieve this objective\n\n> The idea to make the eigenvalues of the NTK Gram matrix more balanced to improve subgroup robustness is not new (see [1]). The proposed approach is not scalable.\n\nOur work attributes the impact of spurious correlation to spectral bias. While spectral decoupling [1] is theoretically scalable, we observed that practical implementation requires distinct treatments for the logits of each class, (requiring hyperparameters per class) and thus elevating the implementation complexity.\n\n\n> Limited evaluation.\n\nThe following are the results for the Waterbirds and CelebA datasets, and our approach achieves the state-of-the-art performance (cf. Appendix E.3 in [7]). The experiment follows a linear probing approach (similar to [6]), using feature embeddings extracted from a trained model as inputs to the NTK. It's worth noting that the consideration of linear probing as a solution has also been discussed in recent NTK literature [8].\n\n**Waterbids**\n|                   |   Avg. Acc   |  Worst Acc.  |   $\\Delta$   |\n|:-----------------:|:------------:|:------------:|:------------:|\n|     original      | $92.4\\pm0.3$ | $72.9\\pm0.8$ | $19.5\\pm0.8$ |\n| modified spectrum | $86.1\\pm1.2$ | $80.0\\pm3.0$ | $6.1\\pm2.3$  |\n\n**CelebA**\n|                   |   Avg. Acc   |  Worst Acc.  |   $\\Delta$   |\n|:-----------------:|:------------:|:------------:|:------------:|\n|     original      | $99.0\\pm0.1$ | $88.9\\pm3.0$ | $10.1\\pm3.0$ |\n| modified spectrum | $99.0\\pm0.1$ | $89.0\\pm3.2$ | $9.9\\pm3.1$  |\n\nNote that, for the CelebA dataset, we used a subset of the training split and the entire evaluation split, following the same procedure as outlined in [1] (refer to Appendix B.5.1 in [1]). \nWe observed superior performance with the orignal NTK. Importantly, modifying the spectrum does not lead to a deterioration in performance.\n\n\n> Can the authors discuss the contributions of their work in light of this related work [1-5]?\n\nWe appreciate Reviewer for highlighting the work of [1], and we are well aware of this study. We will discuss it in our related work section. References [2-5] are cited in the related work section, and we will provide more detailed elaboration on the connection to these prior works.\n\n---\n\n[1] M. Pezeshki et al. Gradient Starvation: A Learning Proclivity in Neural Networks https://arxiv.org/abs/2011.09468\n\n[2] Y. Cao et al. Towards Understanding the Spectral Bias of Deep Learning https://arxiv.org/abs/1912.01198\n\n[3] G. Yang et al. A Fine-Grained Spectral Perspective on Neural Networks https://arxiv.org/abs/1907.10599\n\n[4] Y. Yang et al. Identifying Spurious Biases Early in Training through the Lens of Simplicity Bias https://arxiv.org/abs/2305.18761\n\n[5] H. Shah et al. The Pitfalls of Simplicity Bias in Neural Networks https://arxiv.org/abs/2006.07710\n\n[6] Pavel Izmailov et al. On Feature Learning in the Presence of Spurious Correlations https://arxiv.org/abs/2210.11369\n\n[7] Yuzhe Yang et al. Change Is Hard: A Closer Look at Subpopulation Shift https://arxiv.org/abs/2302.12254\n\n[8] Sadhika Malladi et al. A Kernel-Based View of Language Model Fine-Tuning https://arxiv.org/abs/2210.05643"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700235893389,
                "cdate": 1700235893389,
                "tmdate": 1700236566347,
                "mdate": 1700236566347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yVcLLfXq46",
                "forum": "89AOrk05uy",
                "replyto": "gNNhmBrKPL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3451/Reviewer_cfRQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3451/Reviewer_cfRQ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for providing a detailed response and for sharing additional experimental results. I still have the following concerns and as of now, I will maintain my score.\n\n1. While prior work does not present explicit evidence that spurious features correspond to lower order eigenvectors with larger eigenvalues, this work does not really offer new insights. Based on the findings in prior work [1-5], one can expect such a behaviour. It is important to discuss the insights and contribution of this work compared to [1-5].\n\n2. The authors of [1] use a single hyperparameter for the logit regularization and not specific values for each class. The statement about the increase in implementation complexity due to additional hyperparameters does not seem correct.\n\n3. Thank you for sharing additional results on Waterbirds and CelebA. The statement that these results are state of the art is not correct, there are methods in App. E.3 in [7] (cited by the authors) that are better in terms of worst-group accuracy and the gap between average and worst-group accuracy. Also, the improvement in CelebA is negligible. This does not address the concern about the usefulness of the approach (given that the approach is also not scalable)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723731068,
                "cdate": 1700723731068,
                "tmdate": 1700723731068,
                "mdate": 1700723731068,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]