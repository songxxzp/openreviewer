[
    {
        "title": "Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape"
    },
    {
        "review": {
            "id": "1NmORreb5c",
            "forum": "zNzVhX00h4",
            "replyto": "zNzVhX00h4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7424/Reviewer_n6qD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7424/Reviewer_n6qD"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a landscape analysis for the differentiable regions of the two-layer ReLU network and proves the absence of bad local minima under mild overparameterization level."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper provides a novel perspective for the landscape analysis of 2-layer NN, from a geometric and combinatoric point of view. The theoretical analysis looks solid.\n\n2. Figure 2 and 3 clearly show that the theorems are validated by numerical simulations."
                },
                "weaknesses": {
                    "value": "1. The theoretical contribution can be better contextualized by readers if authors can provide some clarification on their intuitions. See bullet points 1 and 3 in Questions. \n\n2. The key contribution, mild overparameterization level is only achieved for data of dimension 1. For the data with general dimension $d$ the counting method seems intractable. The authors are welcome to elaborate more on how to generate their analysis to high dimensional data or even deeper networks."
                },
                "questions": {
                    "value": "1. I find it very difficult to follow the logic around Corollary 8: before Corollary 8, the authors claim that under general position assumption of the dataset and d larger than n, one can show that most activation regions are non-empty. However, Corollary 8 still focuses on the \"non-empty activation regions\". If most activation regions are indeed non-empty, why not drop the term \"non-empty\"? \n\n2. Among the related works being provided in the first section, can you compare your work with Liu (2021)? They seem to provide a stronger result.\n\n3. When $d_0=1$, Theorem 5 already provides some strong results, why do we bother deriving Theorem 10?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Reviewer_n6qD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698214881337,
            "cdate": 1698214881337,
            "tmdate": 1699636890972,
            "mdate": 1699636890972,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1fMDLeKOEk",
                "forum": "zNzVhX00h4",
                "replyto": "1NmORreb5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7424/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your helpful comments. We address all of your concerns and hope that our response clarifies your questions on our work.\n\n> The key contribution, mild overparameterization level is only achieved for data of dimension 1. For the data with general dimension the counting method seems intractable.\n\nWe address the high-dimensional case in Theorem 5. One consequence of this result is that for high-dimensional data $(d_0 \\geq n)$, if $d_1 \\geq \\Omega(1)$, then most activation regions have no bad critical points. This is a mild level of overparameterization since $d_0d_1 \\sim n$. \n\n> The authors are welcome to elaborate more on how to generate their analysis to high dimensional data or even deeper networks. \n\nWe have added a new Appendix H which generalizes our analysis to deep networks. Our methods for the shallow case can be applied to deeper networks by considering gradients with respect to individual layers. We find conditions on the activation patterns of each layer which ensure that the gradients will be full rank. \n\n> However, Corollary 8 still focuses on the \u201dnon-empty activation regions\u201d. If most activation regions are indeed non-empty, why not drop the term \u201dnon-empty\u201d?\n\nIndeed, if $d$ is large relative to $n$, then most activation regions will be non-empty. We include the term non-empty here to emphasize that our result is not just being applied to empty activation regions which trivially do not contain any critical points. \n\n> Among the related works being provided in the first section, can you compare your work with Liu (2021)? They seem to provide a stronger result.\n\nLiu (2021) shows that in a given activation region, all local minima are global *within the given region*. This means that for a critical point $\\theta$ within an activation region $\\Omega$, $\\theta$ is a global minimizer of the loss restricted to $\\Omega$, but there could still exist $\\theta' \\notin \\Omega$ which attains a smaller loss than $\\theta$. In contrast, we show that in most activation regions, all critical points are global minima *over the entire loss landscape*. We are proving a proving stronger statements about the critical points themselves by incorporating information about the activation patterns. \n\n> When $d_0 = 1$, Theorem 5 already provides some strong results, why do we bother deriving Theorem 10?\n\nIn the one-dimensional case, most activation regions are empty (in contrast to the high-dimensional case where every activation pattern is realized). We provide a more refined analysis by focusing specifically on the non-empty activation regions, characterizing them, and showing that most of them have a full rank Jacobian."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717356810,
                "cdate": 1700717356810,
                "tmdate": 1700717356810,
                "mdate": 1700717356810,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L8Dj74qsaH",
            "forum": "zNzVhX00h4",
            "replyto": "zNzVhX00h4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7424/Reviewer_fi53"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7424/Reviewer_fi53"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the loss landscape of the 1-hidden-layer ReLU network. The authors consider the partition of parameter space into different *activation regions*, defined by the activation patterns of ReLU neurons for different data points in the training dataset. \n\n1. Under mild overparameterization, Theorem 5 in Section 3 shows that almost all activation regions have the property that every differentiable critical point of the training loss (with nonzero 2nd layer weights) is a global minimum; i.e., \"no bad local minima\" holds for most regions.\n\n2. However, not all activation regions are non-empty (i.e., the inequalities defining the region are feasible). Section 4 studies the number of non-empty activation regions. For the high-dim case of $d \\geq n$, for data points in general position, Theorem 5 can be extended to \"almost all *non-empty* activation regions\" (Corollary 8)\n\n3. The paper then discusses the case of one-dimensional input + bias, in which stronger statements can be shown. The authors show in Theorem 10 that almost all non-empty activation regions satisfy the \"no bad local minima\" property. Under a stronger assumption on the 2nd layer weights, Theorem 12 shows existence of an affine set of global minima in almost all non-empty activation regions.\n\n4. Some discussions on function space and experiments are provided in Sections 6 and 7, respectively."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The paper shows that most linear regions in the parameter space satisfy the desirable \"no bad local minima\" property, which agrees with practical observations with overparameterized networks. The proof techniques (especially the one employing random sign matrices) seem to be new in the literature.\n\nS2. The paper is relatively well-written and easy to digest."
                },
                "weaknesses": {
                    "value": "W1. Unfortunately, the scope of this paper looks rather limited. The paper studies a single-hidden-layer model and only the first layer $W$ is trained. Hence, given an activation region, the prediction $F$ is linear in $W$, which makes the Jacobian matrix constant in the region and it makes proofs much easier. For Sections 5-6, the authors show results specialized to one-dimensional input space. I question if these proof techniques could be extended to deeper networks and higher-dimensional input space.\n\nW2. The paper only considers differentiable local minima, which misses the possible existence of non-differentiable local minima. In fact, Laurent & von Brecht (2018) show that in the hinge loss case, local minima can only occur at the non-differentiable boundaries except for flat regions. Is there any hope of including non-differentiable points into the analysis?\n\nW3. Most results in the paper only prove that for most activation regions, \"all critical points are global minima\". If we take a closer look at it, the theorems do not talk about the existence of critical points in these regions; they may or may not exist. Therefore, the statements cannot rule out the following pathological scenario: critical points do not exist at all in the $1-\\epsilon$ fraction of the linear regions and the $\\epsilon$ fraction of the regions contain bad local minima. In order to make the results stronger, a more complete characterization of the existence of critical points should be helpful.\n\nW4. Some directly relevant citations are missing. [A, B, C] show the existence of bad local minima in ReLU networks and thus are directly relevant. [D] and some other papers by the same authors also consider the partition of parameter space with respect to the sign of pre-activations. I feel there should be more missing relevant papers, so please consider reviewing the literature again and updating the paper accordingly. Lastly, the citation to Safran & Shamir (2016) in page 1 should be corrected to Safran & Shamir (2018).\n\n[A] Small nonlinearities in activation functions create bad local minima in neural networks\n\n[B] Piecewise linear activations substantially shape the loss surfaces of neural networks\n\n[C] Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory\n\n[D] THE HIDDEN CONVEX OPTIMIZATION LANDSCAPE OF REGULARIZED TWO-LAYER RELU NETWORKS: AN EXACT CHARACTERIZATION OF OPTIMAL SOLUTIONS"
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681748356,
            "cdate": 1698681748356,
            "tmdate": 1699636890847,
            "mdate": 1699636890847,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LB1Ez9w2MA",
                "forum": "zNzVhX00h4",
                "replyto": "L8Dj74qsaH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7424/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and suggestions, which we address below. We hope that our response and updated manuscript address your concerns and you consider raising your score.\n\n> Unfortunately, the scope of this paper looks rather limited. The paper studies a single-hidden-layer model and only the first layer is trained. Hence, given an activation region, the prediction is linear in W, which makes the Jacobian matrix constant in the region and it makes proofs much easier. For Sections 5-6, the authors show results specialized to one-dimensional input space. I question if these proof techniques could be extended to deeper networks and higher-dimensional input space.\n\n> The paper only considers differentiable local minima, which misses the possible existence of non-differentiable local minima. In fact, Laurent \\& von Brecht (2018) show that in the hinge loss case, local minima can only occur at the non-differentiable boundaries except for flat regions. Is there any hope of including non-differentiable points into the analysis?\n\nWe have added appendices to address non-differentiable critical points and trainable output weights (Appendix G) as well as the deep case (Appendix H). Although our focus has been on the two-layer case, our techniques can be used to understand more general settings. Even when looking at non-differentiable points of activation regions, we can show in the case of one-dimensional input data that most activation regions contain no spurious minima. \n\nTo generalize to the deep case, one can compute gradients with respect to individual layers of the network and find conditions on the activation patterns for which the gradients are full rank. We demonstrate this technique in Appendix H. \n\nWe would also like to note that we consider a different setting from Laurent and von Brecht by considering the square loss rather than the hinge loss. When we restrict the hinge loss to a given activation region, the loss is locally given by an affine function, so there are no critical points on the interior of the region. In contrast, when using the square loss, the loss restricted to an activation region is given by a quadratic function, so a more careful analysis is needed. \n\n> Most results in the paper only prove that for most activation regions, \"all critical points are global minima\u201d. If we take a closer look at it, the theorems do not talk about the existence of critical points in these regions; they may or may not exist. Therefore, the statements cannot rule out the following pathological scenario: critical points do not exist at all in the fraction of the linear regions and the fraction of the regions contain bad local minima. In order to make the results stronger, a more complete characterization of the existence of critical points should be helpful.\n\nWe address the issue of the existence of global minimizers in Theorem 12, where we show that with one-dimensional inputs, most activation regions contain global minimizers. In the one-dimensional case, we are able to show that $d_1 \\sim n \\log n$ is sufficient parameterization for most regions to have a full rank Jacobian and for the existence of global minima, with the existence of global minima requiring a constant factor of additional parameterization. For the higher-dimensional case, we provide experiments (in Appendix E.2) which confirm that approximately linear scaling of $d_1$ with $n$ is sufficient for most regions to contain global minima. \n\n> Some directly relevant citations are missing\n\nThanks for highlighting these references. We have added pointers to these works in the introduction."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717203728,
                "cdate": 1700717203728,
                "tmdate": 1700717203728,
                "mdate": 1700717203728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1nETF6dGpO",
            "forum": "zNzVhX00h4",
            "replyto": "zNzVhX00h4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7424/Reviewer_T8Cv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7424/Reviewer_T8Cv"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the loss landscape of two-layer ReLU networks. Following the observation that critical points where Jacobian is full rank are global minimizers, the authors show that most activation regions have no bad differentiable local minima. When the input is one-dimensional, they further proved that most regions contain a high-dimensional set of global minimizers. Experiments support that most regions of the loss landscape have full rank Jacobian in overparametrized networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tUnderstanding the loss landscape of neural networks is a fundamental and important problem. This paper contributes a new angle by bounding the number of activation regions with no bad local minima.\n-\tThe results are general as they are independent of specific choice of initialization of parameters or distribution of dataset.\n-\tThe paper provides a rigorous definition for generic points using the fact that algebraic subset of $\\mathbb{R}^n$ has measure 0. This method could be useful in various areas beyond studying landscape properties."
                },
                "weaknesses": {
                    "value": "-\tThe setting described at the beginning of section 3 is different from a typical 2-layer ReLU network in that only one weight matrix is considered as parameters. It would be helpful to clarify whether including $v$ in the parameter space changes the conclusions in Theorem 5.\n-\tWhile smooth critical points are global minima in most of the activation regions, it is not clear what the volume of these regions are. In particular, results in this paper do not rule out the possibility that the small number of regions with bad local minima make up most of the parameter space. Quantifying the size of the activation regions appears difficult since the setting considered is independent of the distribution of dataset.\n-\tDespite interesting theoretical results, there is not much discussion on the implication on neural network training. As the authors also mention, this paper has not formalized how most regions having not bad local minima in their interior leads to gradient descent\u2019s success in finding the global minima."
                },
                "questions": {
                    "value": "-\tThe beginning of the second paragraph of section 6 states that a ReLU with one input with bias is equivalent to a ReLU with two input dimensions and no bias. Could the authors add a bit more explanation, as this equivalence does not seem straightforward?\n-\tAre the constants in the bound known? Can they be quantified in experiments?\n-\tThis paper shows that most activation regions do not have bad local minima in their interior. Do there exist non-differentiable points that are local minimizers? If so, will these points cause problems for gradient descent?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Reviewer_T8Cv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804602787,
            "cdate": 1698804602787,
            "tmdate": 1699636890734,
            "mdate": 1699636890734,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MV2OOgbUDb",
                "forum": "zNzVhX00h4",
                "replyto": "1nETF6dGpO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7424/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments which we address in our comments below and in the additions to our paper. In light of these improvements, we hope that you might consider increasing your score. \n\n> The setting described at the beginning of section 3 is different from a typical 2-layer ReLU network in that only one weight matrix is considered as parameters. It would be helpful to clarify whether including $v$\n in the parameter space changes the conclusions in Theorem 5.\n\nWe focused on computing the gradients with respect to the input weight matrix rather than the output head to facilitate our analysis. However, we would like to emphasize that our results showing that $\\nabla_W F$ is full rank also imply that $\\nabla_{(W, v)}F$ is full rank, since the former is a submatrix of the latter. In particular, our conclusions still hold when we include $v$. We have added a new Appendix G in which we analyze the effect of parameterizing $v$ as well as an extension to non-differentiable critical points. \n\n> While smooth critical points are global minima in most of the activation regions, it is not clear what the volume of these regions are. In particular, results in this paper do not rule out the possibility that the small number of regions with bad local minima make up most of the parameter space.\n\nSome of the methods we used to enumerate the activation regions with full rank Jacobian can be adapted to compute the volumes of such regions. We have added a new Appendix F illustrating this. For the one-dimensional case, we compute the volume of each activation region as a function of the dataset. We also bound the amount of parameterization needed for most of the parameter space (by volume) to have full rank Jacobian in terms of the separation between the data points. For the higher-dimensional case, we introduce an anticoncentration property of the data. Under this property, we prove that the portion of the loss landscape with full rank Jacobian is large by volume. \n\n> Despite interesting theoretical results, there is not much discussion on the implication on neural network training. As the authors also mention, this paper has not formalized how most regions having not bad local minima in their interior leads to gradient descent\u2019s success in finding the global minima.\n\nFrom an optimization perspective, one significant benefit of most regions having full rank is that the neural tangent kernel in those regions is strictly positive definite. This means that while the parameters of the network remain in those regions, gradient flow will decay the square loss at an exponential rate.\n\n> The beginning of the second paragraph of section 6 states that a ReLU with one input with bias is equivalent to a ReLU with two input dimensions and no bias. Could the authors add a bit more explanation, as this equivalence does not seem straightforward?\n\nFor a neural network on one-dimensional data with bias, each neuron is a function of the form $x \\mapsto b + wx$. Instead, we could think of the data point $x$ as an element of 2-dimensional space, writing it as $(1, x)$. Then the mapping is of the form $(x_1, x_2) \\mapsto bx_1 + wx_2 = (b, w)^T(x_1, x_2)$. The same trick applies in higher dimensions as well. \n\n``Are the constants in the bound known? Can they be quantified in experiments?\" \n\nFor the one-dimensional case, we include the constants in our bounds for Theorems 10 and 12. For the high-dimensional case, we rely on a bound from Bourgain et al. [1] which states that the probability of a random $d \\times d$ matrix being singular is at most $(1 / \\sqrt{2} + o(1))^d$. Recovering the exact constant from this bound is challenging. However, the exact scaling can be checked experimentally, as we demonstrate in our experiments in Section 7. For example, Figure 3b shows that if we use the scaling $\\frac{n}{d_0} = 2$, then $d_1 \\approx 7$ is sufficient for at least 80 percent of activation regions to have no bad critical points. Note also that for fixed $\\epsilon$, if $\\frac{n}{d_0} \\gg 1$, then Theorem 5 implies that $d_1 = \\frac{n}{d_0}$ is sufficient for most regions to have a full rank Jacobian. \n\n> This paper shows that most activation regions do not have bad local minima in their interior. Do there exist non-differentiable points that are local minimizers? If so, will these points cause problems for gradient descent? \n\nIndeed, there can exist non-differentiable critical points which are local minimizers of the loss. We have added Appendix G to address this question, where we establish that in most activation regions, the set of spurious non-differentiable critical points forms a low-dimensional subset of the region."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717032408,
                "cdate": 1700717032408,
                "tmdate": 1700717032408,
                "mdate": 1700717032408,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dfs24cJWmq",
            "forum": "zNzVhX00h4",
            "replyto": "zNzVhX00h4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7424/Reviewer_Vqi8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7424/Reviewer_Vqi8"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the optimization landscape of mildly over-parameterized 2-layer ReLU networks. By looking at the rank of Jacobian in different activation regions, the authors claim that for most activation regions there are no bad differentiable local minima."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The theoretical finding of this paper is rigorous and interesting. It provides good insight on the optimization landscape of mildly overparameterized NNs. \n- The writing of this paper is easy to follow and the presentation is clear."
                },
                "weaknesses": {
                    "value": "- The assumption of 2-layer network with fixed last layer weights $v$ is restrictive and impractical. \n- The paper only discusses differentiable critical points, but there can be a lot of indifferentiable critical points depending on network settings. In fact generic deep ReLU networks with cross-entropy loss will have non-differentiable sub-optimal local minima [1]. This means that GD can still fall into bad non-differentiable local minima, even if the differentiable local minima are good. So it's still not clear to me that 'most of the landscape is favorable to optimization'. \n\n[1] Bo Liu, Zhaoying Liu, Ting Zhang, Tongtong Yuan, Non-differentiable saddle points and sub-optimal local minima exist for deep ReLU networks, Neural Networks, Volume 144, 2021."
                },
                "questions": {
                    "value": "- Continued on bad local minima. It seems to me that in figure 9(b) GD does not converge to good local minima when network is only mildly overparameterized, despite at initialization the Jacobian has full rank. Can you explain this? Does this contradict your theoretical findings? \n- Numerical experiment. \n-- How do you determine whether a Jacobian matrix is full rank? \n-- For figure 9(a) is the network overparameterized at all? The experiment setting for figure 9 is not clear to me. Can you elaborate on this, just like what you did for other figures?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7424/Reviewer_Vqi8"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7424/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699418704086,
            "cdate": 1699418704086,
            "tmdate": 1699636890628,
            "mdate": 1699636890628,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3G6aNmh3OQ",
                "forum": "zNzVhX00h4",
                "replyto": "Dfs24cJWmq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7424/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7424/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your helpful comments and suggestions. We address each of your points below.\n\n> The assumption of 2-layer network with fixed last layer weights is restrictive and impractical. \nThe paper only discusses differentiable critical points, but there can be a lot of indifferentiable critical points depending on network settings.\n\nWe have added appendices to address non-differentiable critical points and trainable output weights (Appendix G) as well as the deep case (Appendix H). Although our focus has been on the two-layer case, our techniques can be used to understand more general settings. Even when looking at non-differentiable points of activation regions, we can show in the one-dimensional case that most regions have no spurious local minima. To generalize to the deep case, one can compute gradients with respect to individual layers of the network and find conditions on the activation patterns for which the gradients are full rank. We demonstrate this technique in Appendix H.  In Appendix F we now also discuss the volume of activation regions, providing further insight in regard to the comment \"most of the landscape\".\n\n> It seems to me that in figure 9(b) GD does not converge to good local minima when network is only mildly overparameterized, despite at initialization the Jacobian has full rank. Can you explain this? Does this contradict your theoretical findings?\n\nThis experiment agrees with our theoretical results in the sense that when the network width $d_1$ scales linearly in the sample size $n$, gradient descent converges to a global minimizer with high probability. The amount of parameterization necessary for convergence to a global minimizer appears to be a constant multiple of the amount necessary for a full-rank Jacobian at initialization. \n\n> How do you determine whether a Jacobian matrix is full rank?\n\nWe compute the Jacobian and its rank with PyTorch. The method ```torch.linalg.matrix_rank``` computes the SVD of the Jacobian and counts the number of singular values which are larger than a fixed machine tolerance.\n\n> For figure 9(a) is the network overparameterized at all? The experiment setting for figure 9 is not clear to me. Can you elaborate on this, just like what you did for other figures?\n\nYes, the network is overparameterized in this experiment because the number of parameters is given by $m = d_0d_1$, where $d_0$ is the input dimension and $d_1$ is the hidden dimension. Here the dataset, MNIST, consists of images represented by 784-dimensional vectors. Hence the hidden dimension (network width) does not need to be large for the network to be overparameterized. We have updated the paper to clarify this by adding the comment \"The number of network parameters matches the training set size $n$ when the width satisfies $d_1=n/d_0$, where for MNIST the input dimension is $d_0=784$.\""
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7424/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716871413,
                "cdate": 1700716871413,
                "tmdate": 1700716871413,
                "mdate": 1700716871413,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]