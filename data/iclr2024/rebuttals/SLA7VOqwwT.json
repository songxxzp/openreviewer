[
    {
        "title": "Split-Ensemble: Efficient OOD-aware Ensemble via Task and Model Splitting"
    },
    {
        "review": {
            "id": "1WOsIvoVYl",
            "forum": "SLA7VOqwwT",
            "replyto": "SLA7VOqwwT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4085/Reviewer_Fg3Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4085/Reviewer_Fg3Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a subtask-splitting ensemble training objective to enhance the  out of distribution(ood) detection as well as estimate the uncertainty. In detail, the authors split the original classification task into several complementary subtasks. When we focuses on one subtask, data from the other subtasks can be considered as ood data. Then the training scheme can take both id and ood task into consideration. In addition, the authors propose a tree-like Split-Ensemble architecture that splits and prunes the networks  based on one shared backbone to extract low level features. To verify the proposed method, the authors conduct experiments on several image classification datasets such as CIFAR10 CIFAR100 and Tiny-ImageNet. The classification results on id data has an enhancement in terms of classification accuracy. According to the ood detection criterion, the ood detection ability seems to improve significantly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors offer us a clear presentation for the proposed method. And the idea is quite interesting, it can be considered as use multi task and domain classifier to enhance the performance. This paper presents the whole detail of the training scheme clearly including dealing with the class imbalance and splitting the subtasks. For the splitting and pruning process, the authors propose a novel splitting criterion and utilize global pruning to reduce the model size. To verify the proposed method, extensive experiments are conducted. For the proposed ood setting, the enhancement of the proposed method is very significant. Further analysis of the task splitting is also present."
                },
                "weaknesses": {
                    "value": "1 For table 1, the authors present us the classification results on several datasets including CIFAR10, CIFAR100 and Tiny ImageNet. For CIFAR10, the proposed method is slightly better than single models. But the deep ensemble has a significant drop. However on CIFAR100, deep ensemble enhance the performance significantly. It is weird. In addition, the proposed method can optimize the network structure, to give a more complete comparison,  other methods focusing on search structures could be considered for comparison. \n\n2 The performance on Tiny-ImageNet is very significant, could the authors show us the performance on ImageNet. If the proposed method can have significant improvement on ImageNet, it can be exciting.\n\n3 For ood detection, could the authors use commonly used dataset for ood detection or report the performance of other ood detection methods on your setting?\n\n4 For related works, it would be better for the authors to add some works about split-based structure search such as [1]-[3] \n\n[1] Wang D, Li M, Wu L, et al. Energy-aware neural architecture optimization with fast splitting steepest descent[J]. arXiv preprint arXiv:1910.03103, 2019.\n\n[2] Wu L, Wang D, Liu Q. Splitting steepest descent for growing neural architectures[J]. Advances in neural information processing systems, 2019, 32.\n\n[3] Wu L, Ye M, Lei Q, et al. Steepest descent neural architecture optimization: Escaping local optimum with signed neural splitting[J]. arXiv preprint arXiv:2003.10392, 2020."
                },
                "questions": {
                    "value": "Please refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4085/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698598678013,
            "cdate": 1698598678013,
            "tmdate": 1699636373239,
            "mdate": 1699636373239,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9jEwMHJfJj",
                "forum": "SLA7VOqwwT",
                "replyto": "1WOsIvoVYl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fg3Y"
                    },
                    "comment": {
                        "value": "We thank reviewer Fg3Y for your thorough reviews and valuable comments. Hopefully the following responses can address your concerns.\n\n&nbsp;\n\n**W1: Classification Results Comparison in Tab. 1**\n\nA1: In response to your concern on Table 1, we have re-implemented the benchmarking results on CIFAR-10, and the Deep Ensemble's performance on CIFAR-10 has also been re-evaluated. These results are updated to the revised paper. \n\nThe following table shows the updated image classification results on CIFAR-10. All accuracies are given in percentage with ResNet-18/ResNet-34 as backbone. Best score for each metric in **bold**, second-best *italicized*. We re-implement all baseline methods using default hyperparameter.\n\n| Method                | FLOPs | CIFAR-10*         | \n|-----------------------|-------|-------------------|\n| Single                | 1x    | 94.7 / 95.2       | \n| Deep-ensemble         | 4x    | **95.7** / 95.5   | \n| MC-Dropout            | 4x    | 93.3 / 90.1       | \n| MIMO                  | 4x    | 86.8 / 87.5       | \n| MaskEnsemble          | 4x    | 94.3 / 90.8       | \n| BatchEnsemble         | 4x    | 94.0 / 91.0       | \n| Film-Ensemble          | 4x    | 87.8 / 94.3    |  \n| Split-Ensemble (ours) | 1x    | *95.5* / **95.6** | \n\n - The observation is similar to the original Tab. 1 in the paper, where Split-Ensemble beats single model significantly without additional computation cost. Compared to previous ensemble methods, Split-Ensemble also achieves similar or even better performance with significantly less cost.\n\n\nTo understand the impact of structure searching methods, we report the results of applying the same sensitivity-based pruning strategy to prune a naive ensemble model towards single model computation cost. We use the SC-OOD benchmark as suggested by Reviewer 37MR\n\n&nbsp;\n\nImage classification and OOD detection results on the SC-OOD benchmarks. Models with ResNet18 as the backbone are trained and tested on the CIFAR-10 dataset. Best score for each metric in **bold**, second-best *italicized*.\n\n| Method                | FLOPs | Accuracy $\\uparrow$ | FPR95 $\\downarrow$ | AUROC $\\uparrow$ | AUPR $\\uparrow$ |\n|-----------------------|-------|----------|-------|-------|------|\n| Naive Ensemble        | 4x    | **95.7**     | **42.34** | 90.4  | **90.6** |\n| Naive Ensemble pruned | 1x    | 94.0     | 48.19 | 90.3  | 88.7 |\n| Split-Ensemble (ours) | 1x    | *95.5*     | *45.5*  | *91.1*  | *89.9* |\n\n - Note that without the subtask-splitting training objective and parameter sharing enabled by splitting, pruning the naive ensemble to a single model cost significantly impacts its performance. Whereas Split-Ensemble achieves a similar performance to the naive ensemble with significantly less cost.\n\n&nbsp;\n\n**W2: Performance on ImageNet**\n\nA2: Ensemble-based methods are not commonly tested on the ImageNet dataset due to their significant computation cost. As Split-Ensemble achieves single-model computation cost, here we perform an evaluation on ImageNet as suggested. The following table compares image classification results on the large-scale ImageNet1K dataset with ResNet18 as backbone. Best score in **bold**.\n\n| Method         | Acc $\\uparrow$|\n|----------------|---------------|\n| Single         | 69.0          |\n| Naive Ensemble | 69.4          |\n| Split-Ensemble (ours) | **70.9**          |\n\n - Our Split-Ensemble model outperforms the single model and the ensemble model by 1.9\\% and 1.5\\% respectively, demonstrating the effectiveness of our method on the large-scale dataset. Details on ImageNet experiment settings are available in the appendix of the revised paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508714406,
                "cdate": 1700508714406,
                "tmdate": 1700508714406,
                "mdate": 1700508714406,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FOIk69eVtT",
                "forum": "SLA7VOqwwT",
                "replyto": "bqF2zFNHFe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4085/Reviewer_Fg3Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4085/Reviewer_Fg3Y"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the response, but I will keep my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726134116,
                "cdate": 1700726134116,
                "tmdate": 1700726134116,
                "mdate": 1700726134116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aLqT0ZaLSg",
            "forum": "SLA7VOqwwT",
            "replyto": "SLA7VOqwwT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4085/Reviewer_FimD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4085/Reviewer_FimD"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a new method, Split-Ensemble, is proposed to improve the accuracy and OOD detection of a single model by splitting a multi-classification task into multiple complementary subtasks. And a dynamic segmentation and pruning algorithm based on relevance and sensitivity is proposed to construct a more efficient tree-like Split-Ensemble model, which performs well on several experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tAn innovative approach to task segmentation and model partitioning is proposed, which can improve the performance and reliability of a single model without increasing the computational overhead.\n2.\tThe data distribution information in the original task is effectively utilized to achieve the goal of OOD-aware training without external data.\n3.\tAn automated segmentation and pruning algorithm is designed that dynamically adjusts the model structure according to the correlation and sensitivity between subtasks.\n4.\tFull experiments on multiple publicly available datasets demonstrate that the Split-Ensemble approach outperforms baseline."
                },
                "weaknesses": {
                    "value": "1.\tThere is no adequate theoretical analysis and discussion of the principles of subtask segmentation, and there is no explanation of how to choose the optimal number of subtasks and the way to divide the categories.\n2.\tLack of detailed explanation of the definition and importance of OOD-awareness in some sections\n3.\tNo experiments are conducted on more complex or larger datasets, and there is relatively little in the way of discussion of the limitations of its approach and potential directions for improvement."
                },
                "questions": {
                    "value": "1.\tIn the introductory section on page 1, please enhance the background on uncertainty estimation\n2.\tDoes the subtask splitting mentioned in the text take into account the category imbalance? Please give a clarification.\n3.\tThe visualization in the experimental section is low, it is suggested to add\n4.\tPlease describe in one paragraph the structure of your Split-Ensemble model in detail, including the detailed construction of each submodel\n5.\tFor the evaluation of the model, could you provide more description of the evaluation metrics, such as the definition and calculation of AUROC?\n6.\tPlease derive equations (1) and (2) in detail to help the reader better understand your thinking\n7.\tIn the concluding section, could there be a more detailed discussion of future directions of work or potential applications of this methodology?\n8.\tThroughout the paper, could an additional time complexity analysis of the method be considered?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4085/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638834290,
            "cdate": 1698638834290,
            "tmdate": 1699636373165,
            "mdate": 1699636373165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YbIfPKRGXU",
                "forum": "SLA7VOqwwT",
                "replyto": "aLqT0ZaLSg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FimD"
                    },
                    "comment": {
                        "value": "We thank reviewer FimD for your thorough reviews and valuable comments. Hopefully the following responses can address your concerns.\n\n&nbsp;\n\n**W1: Optimal subtask number and category grouping**\n\nA1: This work focuses on the training objective of performing the subtask-splitting training for OOD awareness and achieving efficient architecture with iterative splitting and pruning. While for subtask number and category grouping we use a heuristic design in the paper. We design the subtask splitting based on the intuition that semantically close classes shall be grouped together (Sec. 3.1) so as to make the ID and OOD data in each subtask more distinguishable, and the results in Table 5 show this choice truly makes a difference. \nFor selecting an optimal number of task splits, the tradeoff is that a larger number of splits enables each submodel to learn its OOD-aware objective more easily with fewer ID classes, therefore leading to better AUROC;. Yet the performance may suffer from aggressive pruning to fit the additional branches within the budget. This is observed in Tab. 6 in Appendix C. We choose the optimal subtask number for experiments through this ablation trial.  We leave a more rigorous analysis of the optimal subtask grouping strategy as future work. \n\n&nbsp;\n\n**W2: OOD-awareness**\n\nA2: We take the intuition of OOD-awareness from previous work Outlier-Exposure (OE), where the model will better detect the OOD data if it is exposed to OOD samples during training, thus becoming OOD-aware. As OE and its variants achieve the goal of OOD-awareness with external OOD dataset, our Split-Ensemble utilizes a novel subtask-splitting training objective to make each submodel OOD-aware with only ID data.\n\n&nbsp;\n\n**W3: Experiment on complex dataset**\n\nA3: Following your suggestion, we further compare our approach with baselines on ImageNet1k.\nEnsemble-based methods are not commonly tested on the ImageNet dataset due to their significant computation cost. As Split-Ensemble achieves single-model computation cost, here we perform an evaluation on ImageNet as suggested. The following table compares image classification results on ImageNet1k with ResNet18 as backbone.\n\n| Method         | Acc $\\uparrow$|\n|----------------|---------------|\n| Single         | 69.0          |\n| Naive Ensemble | 69.4          |\n| Split-Ensemble | **70.9**          |\n\n - Split-Ensemble outperforms single model and the $4\\times$ larger ensemble model by 1.9\\% and 1.5\\% respectively. Details on ImageNet experiment settings are available in the appendix of the revised paper.\n\n&nbsp;\n\nBesides ImageNet, we follow the suggestion of Reviewer 37MR to include the new SC-OOD [C1] benchmark and CIFAR-10LT dataset [C2] in our experiments. \nalong with some additional baselines like ODIN [C3], EBO [C4], OE [C5], MCD [C6], UDG [C1] are listed below and have been included in the Appendix of the revised manuscript.\n\nThe following two tables show the comparison between previous state-of-the-art methods and ours on the SC-OOD CIFAR10 benchmarks. The results are reported for models with ResNet-18 backbone. Some baseline methods use the Tiny-Imagenet dataset as additional OOD training data. Best score for each metric in **bold**, second-best *italicized*.\n\n| Method                | Additional Data | FPR95 $\\downarrow$ | AUROC $\\uparrow$ | AUPR $\\uparrow$ |\n|-----------------------|-----------------|-------|-------|------|\n| ODIN                  |  &cross;        | 52.0  | 82.0  | 85.1 |\n| EBO                   |  &cross;        | 50.0  | 83.8  | 85.1 |\n| OE                    | &check;         | 50.5  | 88.9  | 87.8 |\n| MCD                   |  &check;        | 73.0  | 83.9  | 80.5 |\n| UDG                   |  &cross;        | 55.6  | 90.7  | 88.3 |\n| UDG                   |  &check;        | **36.2**  | **93.8**  | **92.6** |\n| Split-Ensemble (ours) | &cross;    | *45.5*      | *91.1* | *89.9*      |\n\n - The Split-Ensemble model outperforms single-model approaches in OOD detection without incurring additional computational costs or requiring extra training data. Its consistent high performance across key metrics highlights its robustness and efficiency, underscoring its practical utility in OOD tasks.\n\n&nbsp; \n\n| Method    | FLOPs | FPR95 $\\downarrow$ | AUROC $\\uparrow$ | AUPR $\\uparrow$ |\n|-----|--|---|--|--|\n| Naive Ensemble | 4x    | **42.3**   | 90.4   | *90.6*       |\n| MC-Dropout  | 4x    | 54.9   | 88.7   | 88.0       |\n| MIMO   | 4x    | 73.7   | 83.5   | 80.9       |\n| MaskEnsemble    | 4x    | 53.2    | 87.7    | 87.9        |\n| BatchEnsemble         | 4x    | 50.4   | 89.2   | 88.6       |\n| FilmEnsemble          | 4x    | *42.6*   | **91.5**    |  **91.3**           |\n| Split-Ensemble (ours) | 1x    | 45.5    | *91.1*    | 89.9        |\n\n- The Split-Ensemble model consistently outshines other ensemble-based methods in both image classification and OOD detection, achieving similar or even better performance with $4\\times$ less computation cost."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508359626,
                "cdate": 1700508359626,
                "tmdate": 1700508359626,
                "mdate": 1700508359626,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "96eunrfrYR",
            "forum": "SLA7VOqwwT",
            "replyto": "SLA7VOqwwT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4085/Reviewer_ZwnE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4085/Reviewer_ZwnE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an ensemble based method for out-of-distribution detection (OOD). Specifically, the original classification task is split into several sub-tasks trained on ID data but with OOD aware class targets. One model is trained for each sub-task. A weight split and pruning strategy is proposed to reduce the computational cost. In the inference stage, probabilities produced by each model is concatenated and a sample is considered OOD if all the probabilities are below some threshold."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using task-splitting on ID data to train an ensemble for OOD is interesting."
                },
                "weaknesses": {
                    "value": "1. The effectiveness of the proposed method is not convincingly evaluated as benchmarking experiments are not enough. Table 1: benchmarking results on CIFAR-10 and TinyIMNET are missing; numbers reported for Deep Ensemble ON CIFAR-10 are problematic as it should not underperform single network; Table 2: lacking benchmarking with SOTA methods."
                },
                "questions": {
                    "value": "1. How to determine the optimal number of task splits? It seems that using a larger number of sub-tasks increase AUROC, but the computational cost is also increased.\n2. How can the method be applied to OOD detection in object detection and semantic segmentation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4085/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4085/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4085/Reviewer_ZwnE"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4085/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659352413,
            "cdate": 1698659352413,
            "tmdate": 1699636373087,
            "mdate": 1699636373087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V0LbgjuhvQ",
                "forum": "SLA7VOqwwT",
                "replyto": "96eunrfrYR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZwnE"
                    },
                    "comment": {
                        "value": "We thank reviewer ZwnE for your thorough reviews and valuable comments. Hopefully the following responses can address your concerns.\n\n&nbsp;\n\n**W1: Benchmarking Results and Comparison**\n\nA1: In response to your concern on Table 1, we have re-implemented the benchmarking results on CIFAR-10 and TinyIMNET, and the Deep Ensemble's performance on CIFAR-10 has also been re-evaluated. These results are updated in the revised paper. \n\nThe following table shows the updated image classification results on CIFAR-10/100 and Tiny-ImageNet. All accuracies are given in percentage with ResNet-18/ResNet-34 as backbone. Best score for each metric in **bold**, second-best *italicized*. \u2217 refers to our own implementation using default hyperparameter.\n\n| Method | FLOPs | CIFAR-10* | CIFAR-100* | Tiny-ImageNet* |\n|--|--|--|--|--|\n| Single | 1x | 94.7 / 95.2 | 75.9 / 77.3 | 36.4 / 40.2 |\n| Deep-ensemble | 4x | **95.7** / 95.5 | **80.1** / **80.4** | 46.8 / 47.8 |\n| MC-Dropout | 4x | 93.3 / 90.1 | 73.3 / 66.3 | 58.13 / 60.3 |\n| MIMO | 4x | 86.8 / 87.5 | 54.9 / 54.6  | 46.39 / 47.76 |\n| MaskEnsemble | 4x | 94.3 / 90.8 | 76.0 / 64.8 | *61.2* / **62.6** |\n| BatchEnsemble | 4x | 94.0 / 91.0 | 75.5 / 66.1 | **61.7** / *62.3* |\n| FilmEnsemble | 4x | 87.78 / 94.25 |  77.4 / 77.2 | 51.5 / 53.2 |\n| Split-Ensemble (ours) | 1x | *95.5* / **95.6** | *77.7* / *77.35* | 51.6 / 47.6 |\n\n- The observation is similar to the original Tab. 1 in the paper, where Split-Ensemble beats single model significantly without additional computation cost. Compared to previous ensemble methods, Split-Ensemble also achieves similar or even better performance with significantly less cost.\n\nFor Table 2, we follow the suggestion of Reviewer 37MR to include results with recent benchmarks and baselines like Yang et al. [C1] and Cao et al. [C2] as well as some additional baselines like ODIN [C3], EBO [C4], OE [C5], MCD [C6], UDG [C1]. Results have been included in the Appendix of the revised manuscript.\n\nThe following two tables show the comparison between previous state-of-the-art methods and ours on the SC-OOD CIFAR10 benchmarks. The results are reported for models with ResNet-18 backbone. Some baseline methods use the Tiny-Imagenet dataset as additional OOD training data. Best score for each metric in **bold**, second-best *italicized*.\n\n| Method | Additional Data | FPR95 $\\downarrow$ | AUROC $\\uparrow$ | AUPR $\\uparrow$ |\n|--|--|--|--|--|\n| ODIN |  &cross;| 52.0  | 82.0  | 85.1 |\n| EBO  |  &cross;| 50.0  | 83.8  | 85.1 |\n| OE  | &check; | 50.5  | 88.9  | 87.8 |\n| MCD  |  &check; | 73.0  | 83.9  | 80.5 |\n| UDG |  &cross; | 55.6  | 90.7  | 88.3 |\n| UDG |  &check; | **36.2**  | **93.8**  | **92.6** |\n| Split-Ensemble (ours) | &cross; | *45.5* | *91.1* | *89.9* |\n\n - The Split-Ensemble model outperforms single-model approaches in OOD detection without incurring additional computational costs or requiring extra training data. Its consistent high performance across key metrics highlights its robustness and efficiency, underscoring its practical utility in OOD tasks.\n\n&nbsp; \n\n| Method | FLOPs | FPR95 $\\downarrow$ | AUROC $\\uparrow$ | AUPR $\\uparrow$ |\n|--|--|--|--|--|\n| Naive Ensemble | 4x | **42.3**   | 90.4   | *90.6*       |\n| MC-Dropout  | 4x    | 54.9   | 88.7   | 88.0       |\n| MIMO   | 4x    | 73.7   | 83.5   | 80.9       |\n| MaskEnsemble  | 4x    | 53.2    | 87.7    | 87.9        |\n| BatchEnsemble    | 4x    | 50.4   | 89.2   | 88.6       |\n| FilmEnsemble   | 4x    | *42.6*   | **91.5**    |  **91.3**           |\n| Split-Ensemble (ours) | 1x    | 45.5    | *91.1*    | 89.9        |\n\n- The Split-Ensemble model consistently outshines other ensemble-based methods in both image classification and OOD detection, achieving similar or even better performance with $4\\times$ less computation cost.\n\n&nbsp; \n\nThe following table shows the comparison between previous state-of-the-art ensemble-based methods and ours on the SC-OOD CIFAR10-LT benchmarks. The results are reported for models with ResNet-18 backbone. Best score for each metric in **bold**, second-best *italicized*.\n\n| Method           | Accuracy $\\uparrow$ | FPR95 $\\downarrow$ | AUROC $\\uparrow$ | AUPR $\\uparrow$  |\n|------------------|----------|-------|-------|-------|\n| Naive Ensemble   | 12.7     | 98.4 | 45.3 | 50.9 |\n| MC-Dropout       | 63.4     | 90.6 | 66.6 | 66.1 |\n| MIMO             | 35.7     | 96.3 | 55.1 | 56.9 |\n| MaskEnsemble     | 67.7    | 89.0 | 66.82 | 67.4 |\n| BatchEnsemble    | 70.1     | 87.45 | 68.0 | 68.7 |\n| FilmEnsemble     |  *72.5*        |   *84.32*    |  *75.5*     |    *76.0*   |\n| Split-Ensemble (ours) | **73.7**     | **80.5** | **81.7** | **77.6** |\n\n- The Split-Ensemble model excels at handling challenging long-tailed dataset, as evidenced by its top-tier performance across major metrics in the SC-OOD benchmarks. This achievement is particularly notable given its efficiency, as it attains these results without incurring additional computational costs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508096375,
                "cdate": 1700508096375,
                "tmdate": 1700508096375,
                "mdate": 1700508096375,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KNeOgC6yB6",
            "forum": "SLA7VOqwwT",
            "replyto": "SLA7VOqwwT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4085/Reviewer_37MR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4085/Reviewer_37MR"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to train a \u201cSplit-Ensemble\u201d model for detection of OOD inputs. The main idea is to split classes into (semantically related) groups and train a submodel on each group. Further,\n\n- Submodels are trained to correctly classify a (disjoint) subset of classes plus an additional OOD class that refers to the rest of the classes (i.e., those in the subsets of other submodels).\n\n- Submodels share a backbone and a method is proposed to branch out from the backbone using sensitivity criteria until each submodel has an individual branch.\n\n- Submodels are \u201ccalibrated\u201d so that classification may be performed as argmax of concatenated logits. \n\nExperimental results on CIFAR-10/100, Tiny-ImageNet and other datasets (used as OOD data) show that:\n- The proposed model has better accuracy than a single model and some ensemble models with 4 members.\n\n- The proposed model has better OOD detection (e.g., in terms of AUROC) than a sigle model and a 4-member ensemble."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**S1.** The method is well motivated and the presentation is easy to follow.\n\n**S2.** The method shows a level of measurable success."
                },
                "weaknesses": {
                    "value": "**W1.** Some key aspects of the method are not discussed properly nor validated theoretically or experimentally. For example:\n- How is the OOD detection criteria probabilistically sound?\n\n- When a split is decided it is not stated what architecture and parameters are used for the new branches.\n\n- The experiments on subtask grouping are in the appendix and are not specified in detail.\n\n- There is a predefined computation budget that is also not specified.\n\n**W2.** Important recent baselines and benchmarks were not discussed or incorporated. For example, (Yang et al. ICCV21) and (Wang et al. ICML22). The current set of benchmarks and baselines do not represent the more performant or challenging cases.\n\n**W3.** For the OOD detection experiments it is not specified how the OOD detection threshold was determined for each model.\n\n**References:**\n\nYang et al. \u201cSemantically Coherent Out-of-Distribution Detection.\u201d ICCV 2021.\n\nWang et al. \u201cPartial and Asymmetric Contrastive Learning for Out-of-Distribution Detection in Long-Tailed Recognition.\u201d ICML 2022."
                },
                "questions": {
                    "value": "Besides looking for some reply to the issues noted above,\n\n**Q1.** Like other OOD detection methods, this method does not seem to address the issue of the distribution of OOD data being unknown. What would the authors say with regards to this in relation to the method and the reported results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4085/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698714526163,
            "cdate": 1698714526163,
            "tmdate": 1699636372961,
            "mdate": 1699636372961,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qDxRnPMozH",
                "forum": "SLA7VOqwwT",
                "replyto": "KNeOgC6yB6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 37MR"
                    },
                    "comment": {
                        "value": "We thank reviewer 37MR for your thorough reviews and valuable comments. Hopefully the following responses can address your concerns.\n\n\n&nbsp;\n\n**W1.1: OOD Detection Criteria**\n\nA1.1: We follow the practice of ODIN [C1] and outlier exposure [C2] to consider the softmax probability of the model output as an estimation of model confidence, which is used as the OOD detection criteria. Since we train each submodel with a separated OOD-aware objective, as in Eq. (4), we perform the uncertainty estimation with the submodel $f_i$ that contributes to the ensemble output label, as discussed in Sec. 3.3.\n\n&nbsp;\n\n**W1.2: Post-Split architecture and parameter**\n\nA1.2: For the new branch after the splitting, the architecture and parameters are initialized as the exact copy of the original layers it is splitting from. This guarantees the same model functionality before and after the split. The branches will then be updated, pruned, and further split independently after the splitting is performed. We have updated this clarification in Sec. 4.1.\n\n&nbsp;\n\n**W1.3: Subtask Grouping Experiments**\n\nA1.3: This work focuses on the training objective of performing the subtask-splitting training for OOD awareness and achieving efficient architecture with iterative splitting and pruning. We heuristically design the subtask splitting based on the intuition that semantically close classes shall be grouped together (Sec. 3.1) so as to make the ID and OOD data in each subtask more distinguishable, and the results in Table 5 show this choice truly make a difference. We will explain the intuition behind semantically close grouping more clearly in Sec. 3.1 and point to this result. Meanwhile, we leave a more rigorous analysis of the optimal subtask grouping strategy as future work.\n\n&nbsp;\n\n**W1.4: Computation Budget**\n\nA1.4:  As stated at the end of Sec. 4.2, the Floating Point Operations (FLOPs) of the original backbone model is used as the computation budget to guide the splitting and pruning of the Split-Ensemble. This is also reflected in Tab. 1 as the FLOPs of Split-Ensemble is reported as 1x, the same as the single backbone model.\n\n&nbsp;\n\n**W2: Recent Baseline and Benchmark**\n\nA2: Please see the next post due to space limitation.\n\n&nbsp;\n\n\n**W3: OOD Detection Threshold Determination**\n\nA3: Note that with the OOD detection metrics reported in the paper, we do not manually determine the OOD detection threshold for each model. For AUROC and AURP computation, we scan through all the possible thresholds to get the corresponding tradeoff curves. For FPR and detection error evaluation, we report the number with the threshold achieving 95\\% TPR. We have clarified this in the description of Tab. 2.\n\n&nbsp;\n\n**W4: Addressing Unknown OOD Data Distribution**\n\nA4: One unique feature of the proposed Split-Ensemble method is that we perform OOD-aware training without the need for external OOD data, so no knowledge of the OOD to be faced is required. Specifically, we propose subtask splitting training in Sec. 3, where each submodel is trained with part of the dataset as ID while the other as OOD. With the OOD objective in Eq. (3), each submodel becomes OOD aware, which is generalizable to unknown OOD distributions. This generalizability is verified by our results in Tab. 2, and on the new SC-OOD benchmarks you suggested. Our model, trained with only the ID data, can achieve better performance on different OOD distributions compared to baseline methods, even those trained with additional OOD data."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507577869,
                "cdate": 1700507577869,
                "tmdate": 1700507577869,
                "mdate": 1700507577869,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7ckiFxuFk9",
                "forum": "SLA7VOqwwT",
                "replyto": "vVG5OJjCtb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4085/Reviewer_37MR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4085/Reviewer_37MR"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for the clarifications and additional experiments. The latter should strengthen the submission.\n\nI still wonder what the authors can add regarding sections 3.2 and 3.3. As of now, the ensemble members are trained with different labels (e.g., eq. 3) but ultimately used as an ensemble -- for which at least some sort of calibration is required (e.g., eq 4). \n\nCan more be said to justify these choices theoretically? For instance, is the resulting model a mixture model?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595054806,
                "cdate": 1700595054806,
                "tmdate": 1700595054806,
                "mdate": 1700595054806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1R5v02FTFI",
                "forum": "SLA7VOqwwT",
                "replyto": "KNeOgC6yB6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4085/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Training objective of Split-Ensemble"
                    },
                    "comment": {
                        "value": "Thank you for your timely reply!\n\nThe motivation of subtask-splitting training is to reduce the redundancy of having multiple ensemble submodels learn the same task, and allow each submodel to have better uncertainty calibration with the OOD-aware training objective. To this end, the OOD-aware training objective in Equ. (2) and (3) allows each submodel to well classifiy the $K$ ID classes assigned to itself, and output low confidence for OOD samples outside of these $K$ classes, including the samples from the other $N-K$ classes of the overall training task and generalizable to external unseen OOD data. Since this OOD detection ability is learnt by each submodel, we use submodel output to perform uncertainty estimation as in Sec. 3.3.\n\nFor ensemble output, as introduced in Sec 3.3, we concatenate all the ID logits from submodels to form the ensemble logit for classification. In an ideal case, only the submodel which the correct class falls into will output a high confidence in the ID logits, while all the other submodels output low confidence. So the correct classification can be made with the concatenated ensemble logits. In practice, there may be a mismatch between the range of logits output from different submodels, where a low-confidence logit from one submodel may get a larger value than a high confidence logit of another independently-trained submodel. We therefore introduce Equ. (4) to calibrate logits ranges across submodel with the overall $\\mathcal{L}_{CE}$. Note that the CE loss weight $\\lambda$ can be as small as 1e-4 to achieve this calibration, so the calibration does not impact the individual performance of each submodel on its subtask, including its ability to detect OOD samples."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598398711,
                "cdate": 1700598398711,
                "tmdate": 1700598465365,
                "mdate": 1700598465365,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]