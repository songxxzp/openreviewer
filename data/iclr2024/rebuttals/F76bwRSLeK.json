[
    {
        "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models"
    },
    {
        "review": {
            "id": "1k43kwisPy",
            "forum": "F76bwRSLeK",
            "replyto": "F76bwRSLeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_apJf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_apJf"
            ],
            "content": {
                "summary": {
                    "value": "The authors train a sparse autoencoder to find interpretable hidden units in an LLM. The sparse autoencoder linearly transforms the learned representation in a layer of the LLM and then applies a rectifying nonlinearity. The resulting code is regularized by L1 penalty so as to prefer sparse representations and it is trained to do well at reconstructing the LLM representation through a linear map. Using previously published methods to quantify the level of interpretability of a feature with a score,  the authors show that some of the resulting sparse features (those with the highest interpretability score) are more interpretable than those obtained with ICA, PCA and random projections. Other experiments study the effect of perturbing one of these sparse features on the output as well as whether some features are associated with a single word (monosemantic features)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Interpretability is an important subject for improving the safety of frontier AI systems, so this work belongs in a socially important research niche. The main contribution of showing greater interpretability with sparse features derived from the learned LLM layers is useful, although similar sparse representations have already been shown to be useful from an interpretability point of view for neural networks."
                },
                "weaknesses": {
                    "value": "The paper claims in the abstract and conclusion that the results show *greater* monosemanticity than other methods but I do not see such comparisons in the paper (it should be in section 5.1). Either I missed something or one of the main claims is not supported.\n\nThe authors used linear encoders (followed by a ReLU) while it is well known that this is suboptimal (the optimal encoder needs to be much more nonlinear). A useful contribution would have been to evaluate and compare different types of encoding mechanisms for obtain a sparse representation. A simple variant is to introduce hidden layers (and depth) in the encoder. There are also optimization-based encoders that work very well and are well-known for sparse coding.\n\nMost of the pieces in this paper (the sparse autoencoder and the interpretability score) are pre-existing, and so is the idea and demonstration that sparse features yield greater interpretability. This makes the overall level of originality and significance of the paper pretty low.\n\nIn the first paragraph of sec. 5.1, the authors talk about 'real-word features'. Please clarify what that means. If it is a \"human-understable concept\", one crucial problem is that such a concept (except for single-token cases studied in the paper) can be represented in many ways as a sequence of words, making any practical extension of the proposed idea (in sec 5.1) not obvious at all (or maybe not even feasible).\n\nIn section 5.3, it would be good to add evaluations using not just the top hidden layer (which has a very special semantics because it lives in the space of token embeddings) but also lower hidden layers\n\nIn section 6 about limitations, I would venture that two reasons for the poor reconstruction could be that (a) the proposed linear encoder does not have enough expressive power or (b) there exists no sparse linear basis to explain the LLM layers or (c) there is no compact natural language description for the sparse features. \n\nFinally, the whole study is only looking at the *most interpretable* features in the lot, but in order to obtain any kind of safety guarantee, one will need to make sense of *all* the information in the LLM layers. We seem very far from that, which makes me doubt that the whole approach will ever be providing sufficient safety reassurances one day."
                },
                "questions": {
                    "value": "Please try to address the questions I raised in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698592760909,
            "cdate": 1698592760909,
            "tmdate": 1699636651366,
            "mdate": 1699636651366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "34593QQ2It",
                "forum": "F76bwRSLeK",
                "replyto": "1k43kwisPy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer apJf"
                    },
                    "comment": {
                        "value": "Our claim that we have greater monosemanticity rests on our comparisons in section 3 to the neuron basis, random directions, and directions found by PCA and ICA.\n\nOn the originality of the approach, while we agree that none of the individual elements is novel on its own, the pipeline of using a sparse autoencoder to decompose activations in a large model (section 2), which are then passed to an automatic interpretation protocol (section 3), and then analysed in terms of the circuits that build up later features (section 5) represents a meaningful step in our ability to peer into the inner workings of language models.\n\nWhile we didn\u2019t show these results, preliminary experiments using multilayer encoders did not show improvements in the reconstruction/sparsity curves and manual examination of the features found showed that they tended to be less interpretable, so we abandoned this line of work at an early stage.\n\nWhile there are optimization-based encoders, the iterative way that the coefficients are calculated means that they take significantly longer to train, we wanted an approach that is scalable to lots of data (which we found to be key to training these models) and larger models.\n\nWe appreciate the point about \u2018real-world features\u2019 lacking clarity, and we\u2019ve edited the text to amend this.\n\nWe think your characterization that we\u2019re \u201conly looking at the most interpretable features\u201d applies only to Section 5 of the paper, and not to \u201cthe whole study\u201d. In particular, in Section 3 we compute the interpretability scores on randomly chosen features, and find our average score is much higher than that of our comparison methods. Additionally, in Section 4 our selection method for features is their impact on the IOI task, not any a-priori interpretability. You\u2019re correct to say that a true safety guarantee would only be obtained by a complete enumeration of features, though we do reconstruct the vast majority of the variance. The most fundamental limitation appears to us to be the in-built assumption that features can be represented by directions and thus are linear. We agree that how much this assumption can be relied upon is uncertain and a very important direction for future work.\n\n> In section 5.3, it would be good to add evaluations using not just the top hidden layer (which has a very special semantics because it lives in the space of token embeddings) but also lower hidden layers\n\nThe section 5.3 figure (previously Figure 5, now numbered Figure 6) does show results for the lower hidden layers. Namely, the diagram is generated recursively, where we start by with a layer 5 feature and identify its most causally-relevant layer 4 features, and then for each of those layer 4 features independently find the previous causally-relevant layer 3 features, etc.\n\nWe thank you for your constructive feedback, and hope that our answers and updates have addressed your concerns. If you have other feedback or concerns, please let us know. If you think these changes have improved the paper, we hope you will update your scores to reflect that."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093742123,
                "cdate": 1700093742123,
                "tmdate": 1700093742123,
                "mdate": 1700093742123,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WL0mHnp2xT",
                "forum": "F76bwRSLeK",
                "replyto": "34593QQ2It",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_apJf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_apJf"
                ],
                "content": {
                    "title": {
                        "value": "partial interpretation problem"
                    },
                    "comment": {
                        "value": "I have the impression that my comment on \"only looking at the most interpretable features\u201d was misunderstood. Let me try to clarify. Even if one looks at a random *subset* of features, most features are not interpreted. And doing better than the competition in terms of interpretability is great but still does not address the problem I had in mind, i.e., that a dangerous capability could still easily go unnoticed if we are not lucky in capturing it with one of our sparse features, either because it does not correspond to any of the sufficiently interpretable features, or because it would not be humanly feasible to \"look\" at a number of features comparable (or greater because of the sparsity) to the number of dimensions of the representation. \n\nTo put things in perspective, a regulator is likely to have to take a decision about accepting the deployment of a future LLM which could greatly facilitate misuse in the form of national security threats. Can the regulator really rely on this kind of approach, is there a roadmap where this sort of approach would be sufficiently strong to reassure the regulator that catastrophic risks will be avoided?\n\nOf course, I did not expect this paper to solve the interpretability paper, which I understand is hard.\nOn the other hand, reflecting on the limitations and potential future workarounds or completely different approaches (if this approach may end up being a dead end) would greatly enhance the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174603656,
                "cdate": 1700174603656,
                "tmdate": 1700174603656,
                "mdate": 1700174603656,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lC2LAtV2HG",
            "forum": "F76bwRSLeK",
            "replyto": "F76bwRSLeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_k1em"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_k1em"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a way to make the individual features of Large Language Models more interpretable by learning simple autoencoders with activation sparsity. They demonstrate that the learned features are more interpretable than original or simple methods like PCA via several ways, such as automated interpretability, example neurons and helpfulness in detecting circuit components."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Tackles a very important issues with mostly sound methodology and original results. The improvement in interpretability is quite noticeable, and evaluation is diverse.\nFigure 4 is a good clear explanation of 1 feature, and the comparison to the relevant feature in standard residual stream is very useful. \nFigure 5 is also interesting and a good example of the potential usecases of this method."
                },
                "weaknesses": {
                    "value": "Missing a lot of important basic information.\n - What is R? It is an important part of many text captions such as Figure 2 and Figure 3 but never explained in main text. Only explained in Appendix C.\n- Almost all the results do not mention the details of how they were achieved. In particular, table 1, and figures 2, 4, 5 don't mention the model used at all, and most figures do not mention R and alpha used to train the in question.\n\nIn general the main text needs more discussion of the basic methods and results like reconstructions loss, some of the content from Appendix B and C should be moved to main text.\n\nThe high reconstruction loss seems like a problem for the usefulness/faithfulness of this method, and I would like more discussion on it, such as the i.e. large increase in perplexity discussed in section 6.2. What R and Alpha were used for this result? How does this change as a function of different parameters like R and alpha or different layers?"
                },
                "questions": {
                    "value": "- Figure 2: Is identity ReLU the default basis?\n- Section 3.2: Why is ReLU applied to the default and random basis? It is said its applied to make activations nonnegative, but why is this necessary? The actual model also uses negative activations right?\n- What is the intended way to use these learned features? Adding the autoencoder changes the network behavior, and increases computational cost. Are you supposed to use the expanded network in practice to make explainable decisions, or is this better thought as an approximation to explain decisions made by the unmodified network."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698624335055,
            "cdate": 1698624335055,
            "tmdate": 1699636651253,
            "mdate": 1699636651253,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2J1fumgjDS",
                "forum": "F76bwRSLeK",
                "replyto": "lC2LAtV2HG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer k1em"
                    },
                    "comment": {
                        "value": "We appreciate your call for more clarity on how results have been achieved! All tables and figures now indicate which model and hyperparameter values were used.\n\nWe think the point that some of the material in Appendices B and C should be in the main text is an interesting one. We\u2019ve brought some of Appendix B forward into a new Figure 2 which hopefully gives the reader better insight into the nature of the models that we trained. While the material in Appendix C is interesting, we feel that the fact that the learned bases are under-complete in many cases represents a significant drawback to them that wouldn\u2019t be appropriate to show in the main text, and which is best shown in the appendix as a very enticing but ultimately incomplete direction for future work to build on. (We also value the range of experiments and don\u2019t have pages to spare!)\n\nAnswers to questions:\n\n> What is R?\n\nR is defined in the second paragraph of section 2: \u201cThe autoencoder is a neural network with a single hidden layer of size d_hid = R d_in, where d_in is the dimension of the language model internal activation vectors1, and R is a hyperparameter that controls the ratio of the feature dictionary size to the model dimension.\u201d\n\n> Figure 2: Is identity ReLU the default basis? \n\nYes\n\n> Section 3.2: Why is ReLU applied to the default and random basis? It is said its applied to make activations nonnegative, but why is this necessary? The actual model also uses negative activations right? \n\nWe restricted all activations to be positive here because we were concerned that negative activations were a source of additional variance that would bias the results in favour of our method. The interpretation score is based on a correlation metric and thus has variance in the denominator. If an explanation could explain the same amount of variance in the positive domain for one of our features and one default basis features, the resulting correlation score would be higher for our feature, thus biasing our results. In our early experiments we found that autointerpretability scores were similar with or without ReLU. It is likely that some features are bidirectional and so would be best explained by allowing a feature to have both positive and negative activations, but this would be penalized equally for our features and default/random/PCA features.\n\n> What is the intended way to use these learned features? Adding the autoencoder changes the network behavior, and increases computational cost. Are you supposed to use the expanded network in practice to make explainable decisions, or is this better thought as an approximation to explain decisions made by the unmodified network. \n\nThe latter is definitely more what we have in mind. A core principle of this research is to find interpretability techniques that don\u2019t come at a performance cost - we want to be able to run the model as normal and use this \u2018featurized\u2019 version of a layer to understand what the model is doing as best we can, hopefully eventually being able to detect behaviour that\u2019s deceptive or otherwise not in line with the developer\u2019s intentions. We also think that the utility of the current technique is as a platform for further work in understanding how models compute their outputs - we think we should eventually be able to learn better features, and also to build them up into more interesting units like circuits for particular behaviours.\n\nAnother hypothesis, though as yet unproven, is that they may be useful for model editing. For example, model editing often involves learning a vector to add to a residual stream, but finding the right direction across dozens of 10K+ dimensional spaces requires quite a bit of data and these features could provide a useful prior for productive directions to edit.\n\nWe appreciate the helpful comments and questions and we hope this makes the presentation better. If it seems markedly improved we\u2019d really appreciate it if the score were updated to reflect that, else if there are further changes you\u2019d recommend please let us know."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093625022,
                "cdate": 1700093625022,
                "tmdate": 1700093625022,
                "mdate": 1700093625022,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h0DReZ9wpZ",
                "forum": "F76bwRSLeK",
                "replyto": "2J1fumgjDS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_k1em"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_k1em"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "This has addressed several of my concerns regarding clarity, and I believe the paper is stronger than before. However, it would be useful to have more transparency around the changes to the manuscript during discussion period such as using a different color for changes in the manuscript.\n\nIn addition, some of my concerns were still unaddressed such as expanded discussion on perplexity in section 6.2. In fact, it seems you have done the opposite and removed it from the paper all together, why is this the case?\n\nAs a result I will retain my original rating."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734879946,
                "cdate": 1700734879946,
                "tmdate": 1700734879946,
                "mdate": 1700734879946,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XDBQaVVF3J",
            "forum": "F76bwRSLeK",
            "replyto": "F76bwRSLeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_oRDV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_oRDV"
            ],
            "content": {
                "summary": {
                    "value": "This paper is addressing the challenge of polysemanticity in DNNs. In DNNs context neurons appear to activate in multiple, semantically distinct contexts. This paper is trying to address the challenge of disambiguating neurons. They suggest sparse auto-encoders for this challenge."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ Very interesting research question"
                },
                "weaknesses": {
                    "value": "- Overly complex approach, where the auto-encoder will likely add noise to the results\n- No real baseline comparison \n- Experimentation is weak \n- Results are difficult to interpreter and would need to be better presented or colored in the context"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786127906,
            "cdate": 1698786127906,
            "tmdate": 1699636651148,
            "mdate": 1699636651148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SMulVOh7SF",
                "forum": "F76bwRSLeK",
                "replyto": "XDBQaVVF3J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer oRDV"
                    },
                    "comment": {
                        "value": "We\u2019re sorry that you didn\u2019t find the paper of value though we hope that the other reviews which recommend acceptance suggest that there\u2019s content which is informative for others.\n\nWe agree that interpretability is a difficult area to find real benchmarks in, and the benchmarks are not as robust as would be expected in most areas of ML, though through the use of automatic interpretability we have tried hard to give performance measurements which have clear definitions and can be replicated. We note that Bills et al represents a core push by OpenAI to understand their models and so being able to find primitives which are advances according to their own protocols represents a significant step among those working in this area.\n\nWe disagree that the approach is overly complex - to us it is both simple and directly motivated by the theoretical considerations in Toy Modes of Superposition. The computational simplicity of the linear autoencoder over iterative methods for sparse coding (which you may be implicitly referring to as the less complex approach?) is likely to be very important in scaling this technique up to cutting edge models."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093476377,
                "cdate": 1700093476377,
                "tmdate": 1700093476377,
                "mdate": 1700093476377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CkHl5QlST9",
                "forum": "F76bwRSLeK",
                "replyto": "SMulVOh7SF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_oRDV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_oRDV"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal. It would be nice to have some factual figures to back up: *The computational simplicity of the linear autoencoder over iterative methods for sparse coding (which you may be implicitly referring to as the less complex approach?) is likely to be very important in scaling this technique up to cutting edge models.* Would you have such evidence?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491967139,
                "cdate": 1700491967139,
                "tmdate": 1700491967139,
                "mdate": 1700491967139,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pUhTTQRaPx",
            "forum": "F76bwRSLeK",
            "replyto": "F76bwRSLeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_FVSC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_FVSC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes using sparse autoencoders to learn interpretable and monosemantic features from the internal activations of language models. The key idea is to reconstruct model activations as sparse linear combinations of latent directions, which aims to disentangle the model's features from superposition. The authors train autoencoders with an L1 sparsity penalty on various model layers and analyze the resulting feature dictionaries.\n* The learned dictionary features are shown to be more interpretable than baseline methods like PCA/ICA according to automated scores.\n* The features localize target model behaviors more precisely for a counterfactual evaluation task.\n* Case studies demonstrate some highly monosemantic features that influence model outputs in predictable ways.\n* The approach is scalable and unsupervised, requiring only unlabeled activations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Applies sparse coding in a novel way for model interpretability. The approach is intuitive and theoretically motivated.\nProvides solid evidence that the learned features are more interpretable and monosemantic.\nDemonstrates the technique can pinpoint features for analyzing model behaviors.\nCase studies show highly intuitive individual features.\nThe method is scalable and unsupervised."
                },
                "weaknesses": {
                    "value": "The reconstruction loss is not fully minimized, indicating the dictionaries do not capture all information.\nThe method works less well for later model layers and MLPs compared to early residual layers.\nMore analysis needed on:\n1) generalizability of behaviors across tasks,\n2) sparsity of dependencies between features.\n\nLimited comparison to other interpretability and disentanglement techniques.  For example, beta-VAE, infoGAN, FactorVAE had similar disenanglement goals.  And Michaud 2023 \"The Quantization Model of Neural Scaling\" suggested a spectral clustering approach for identifying \"monogenic\" signals in large models. The paper would be strengthened if it provided a more complete comparison to previous disentanglement approaches.\n\nThe authors could mention the previous works in the field of sparse autoencoding and dictionary learning for word embeddings. Subramanian et al. [1] similarly found linear factors for word embeddings, in this case using a sparse autoencoder. Zhang et al. [2] solved a similar problem using methods from dictionary learning. Their method discovers elementary structures beyond existing word vectors.\n\n[1] Spine: Sparse interpretable neural embeddings Subramanian, A., Pruthi, D., Jhamtani, H., Berg-Kirkpatrick, T. and Hovy, E., 2018. Proceedings of the AAAI Conference on Artificial Intelligence, Vol 32(1).\n\n[2] Word embedding visualization via dictionary learning Zhang, J., Chen, Y., Cheung, B. and Olshausen, B.A., 2019. arXiv preprint arXiv:1910.03833."
                },
                "questions": {
                    "value": "How well do the features transfer across different models and architectures?\nCould you incorporate model weights or adjacent layer features to improve dictionary learning?\nWhat changes allow learning overcomplete MLP bases?\nHow well do the features generalize to unseen tasks? More thorough evaluation would be useful.\nThe authors claim the method is scalable, but have not demonstrated it on very large models. Experiments on models with billions of parameters could better support the scalability claims."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837722381,
            "cdate": 1698837722381,
            "tmdate": 1699636651040,
            "mdate": 1699636651040,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ALvnWD9Apz",
                "forum": "F76bwRSLeK",
                "replyto": "pUhTTQRaPx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer FVSC"
                    },
                    "comment": {
                        "value": "We agree that the autoencoder\u2019s imperfect reconstruction is a drawback, but getting much higher auto-interpretation scores still marks this out as a significant step towards being able to understand a layer in a large language model. \n\nWe think that betaVAEs are interesting points of comparison though their motivation and structure are sufficiently different that they can\u2019t be directly compared to our approach.\n\nThe work from Subramanian et al and others working on word embeddings is indeed an important precursor to our work that we weren\u2019t aware of, and we really appreciate these comments and have updated the related works section accordingly.\n\nWe think all of your questions are interesting points and look a lot like our plans for future work, especially incorporating model weights and trying harder to learn overcomplete bases in the residual stream (we suspect much longer training times and reinitialization are likely both to be key). As for scalability, we note that our subject models, Pythia70M and less often Pythia410M are larger than other examples, with the largest being Yun et al using a BERT model with ~100M parameters. The fact that we use autoencoders rather than iterative methods such as FISTA makes scaling significantly easier as only a single pass is needed to compute the coefficients. Nonetheless, the compute required is quadratic in the width of the layer (and possibly even larger if larger ratios and longer training runs are required), so scaling to multi-billion parameters while maintaining quality models requires engineering beyond the scope of this paper.\n\nReplies to questions:\n\n> How well do the features transfer across different models and architectures? \n\nWe did not test this, but there is no a-priori reason to expect these features to transfer, since they are just linear directions and a model\u2019s residual stream is (in theory) linear, so therefore two models will have \u201carbitrarily transformed\u201d residual streams relative to each other.\n\n> Could you incorporate model weights or adjacent layer features to improve dictionary learning? \n\nWhile we have not attempted this, a recent work (https://www.lesswrong.com/posts/YJpMgi7HJuHwXTkjk/taking-features-out-of-superposition-with-sparse) has explored using model information to initialize the dictionaries to improve training speeds.\n \n> What changes allow learning overcomplete MLP bases? \n\nCould you clarify what you\u2019re asking here? \n\n> How well do the features generalize to unseen tasks? \n\nBoth the Pythia models and our autoencoders are trained on The Pile, a large text corpus, so all are equally seen (or unseen) up to their relatively frequencies in the corpus. \n\nWe thank you for your constructive feedback, and hope that our answers and updates have addressed your concerns. If you have other feedback or concerns, please let us know. If you think these changes have improved the paper, we hope you will update your scores to reflect that."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093442146,
                "cdate": 1700093442146,
                "tmdate": 1700093442146,
                "mdate": 1700093442146,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UGbSjkB4Lu",
                "forum": "F76bwRSLeK",
                "replyto": "ALvnWD9Apz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_FVSC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_FVSC"
                ],
                "content": {
                    "comment": {
                        "value": "> > What changes allow learning overcomplete MLP bases?\n\n> Could you clarify what you\u2019re asking here?\n\nI'm asking about your findings when applying your methods to MLPs, e.g., Appendix C.3.  You have mentioned that your method learns a lot of dead features and never is able to learn an overcomplete basis of actual live features.\n\nParticularly:\n* Why does this happen - i.e., why doesn't your method not work well from MLP fan-out layers?  You've noted the problems and we're asking \"why.\"  These seem like the most promising layers to understand.\n* Do you have any insights on why Eq5 and 6 worked on earlier layers, and what is your hunch on how to resolve the lower performance on later layers?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497740566,
                "cdate": 1700497740566,
                "tmdate": 1700497740566,
                "mdate": 1700497740566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ma4mN4eoO0",
                "forum": "F76bwRSLeK",
                "replyto": "TouePT0IEs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_FVSC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Reviewer_FVSC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed discussion of the issues and possibilities for future work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736030583,
                "cdate": 1700736030583,
                "tmdate": 1700736030583,
                "mdate": 1700736030583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CLgs4HlIkg",
            "forum": "F76bwRSLeK",
            "replyto": "F76bwRSLeK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_V2wZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6054/Reviewer_V2wZ"
            ],
            "content": {
                "summary": {
                    "value": "Superposition refers to the hypothesis that neural model representations are in fact linear compositions of sparse features. This paper attempts to identify these sparse features using dictionary learning. More specifically, the authors train an autoencoder with sparsity penalties on language model representations of interest (as both input and output), and use the learned sparse encoding as a proxy to understand the original language model. The resulting sparse encodings are passed through an auto-interpretation model to identify meaningful features. Under the auto-interpretation evaluation, the learned sparse encoding by the proposed method beat several baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and easy to follow. The idea of using sparse autoencoder to tackle superposition makes intuitive sense, and it turns out that it works surprisingly well. The activation patching experiments validates the effectiveness of the proposed method beyond the auto-interpretation metrics, which emphasizes the method's practical value."
                },
                "weaknesses": {
                    "value": "Using dictionary learning for superposition isn't particularly novel.\nAuto-interpretation score hasn't been shown to correlate with actual interpretability use cases yet.\nActivation patching experiment was only conducted on one relatively synthetic dataset."
                },
                "questions": {
                    "value": "Any comments on how the proposed method scales with complexity of the underlying task / number of possible features? Is the approach bottlenecked by auto-interpretation method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6054/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699509766672,
            "cdate": 1699509766672,
            "tmdate": 1699636650941,
            "mdate": 1699636650941,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QknXH2jVtl",
                "forum": "F76bwRSLeK",
                "replyto": "CLgs4HlIkg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6054/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer V2wZ"
                    },
                    "comment": {
                        "value": "We\u2019re glad you found it well written and the results surprisingly positive!\n\nWe agree that it would be very valuable to see more work tying automatic interpretation to more concrete results and we acknowledge some of the limitations in the auto-interpretation section. We still think that its underlying method (measuring interpretability by the correlation between the actual activation of a neuron or feature and the activation as estimated from a potential description of the meaning of that feature) is well-founded, and we hope that by giving people better primitives than neurons to work with, we\u2019ll be able to see more foundational work in this area soon.\n\nWhile the activation patching experiment was only conducted on a single, synthetic dataset, the IOI task that we chose is the most studied example of an LLM circuit that we\u2019re aware of. It was therefore the first thing that we tried, so we think this demonstrates the practicality of the method, as you mention.\n\n> Any comments on how the proposed method scales with complexity of the underlying task / number of possible features?\n\nOne of the big benefits of the approach is that it\u2019s task agnostic - ideally a dictionary should contain all the relevant features - we certainly can\u2019t claim that we reached this point we found all of them, if that\u2019s even a well defined notion, but we don\u2019t see any fundamental blocker to capturing features relevant to any task the model is capable of in a single training run.\nThe method scales somewhere above linearly with the number of features - if you double the width of the autoencoder, you\u2019ve twice as many parameters and there\u2019s some additional slowness to converge though we didn\u2019t try to quantify exactly how much extra time is needed.\n\n> Is the approach bottlenecked by auto-interpretation method?\n\nAt the scale of this paper (Pythia70M, ~100M datapoints of training) it\u2019s easier to train lots of autoencoders than to evaluate lots of them via automatic interpretation - a single feature evaluated using the procedure from Bills et al cost around \u00a30.20 so getting a good sample size of around 200 features interpreted cost about \u00a340 in August 2023 OpenAI credits, significantly more than the training cost per encoder (and meant that we couldn\u2019t run automatic interpretation on most of the dictionaries we trained). Training the autoencoder is more than quadratic in the width of the layer being examined though, but interpretation is unaffected by layer width, so at larger scales the cost of the autoencoder will dominate.\n\nWe thank you for your constructive feedback, and hope that our answers and updates have addressed your concerns. If you have other feedback or concerns, please let us know. If you think these changes have improved the paper, we hope you will update your scores to reflect that."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6054/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093087561,
                "cdate": 1700093087561,
                "tmdate": 1700093087561,
                "mdate": 1700093087561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]