[
    {
        "title": "Bit Cipher \u2014 A Simple yet Powerful Word Representation System that Integrates Efficiently with Language-Models"
    },
    {
        "review": {
            "id": "KmdtALoezG",
            "forum": "BqJamqGwp1",
            "replyto": "BqJamqGwp1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7569/Reviewer_eyQ8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7569/Reviewer_eyQ8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method Bit-cipher to learn word representations efficiently while leveraging contextual information. It evaluation the efficacy of Bit-cipher on POS and NER tasks. The results of Bit-cipher is competitive to the two classic word embedding methods: word2vec and Glove. The paper also demonstrates the efficiency of Bit-cipher for LM training and fine-tuning with several   experiments intergrating LMs and Bit-cipher."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using bit-cipher to represent words is interesting.\n2. Integration of word embeddings and LM is an important perspective to evaluate the proposed method."
                },
                "weaknesses": {
                    "value": "1. The motivation of this paper is not strong enough. It lacks an explanation about the reason why bit-cipher achieves such performance. The advantanges of bit-cipher compared to the classic word embeddings are not clear. Only experimental results are provided. More theoretical proofs and straightfoward intuition should be provided.\n2. The definition of bit-cipher is not clear. Section 3.1 is hard to follow. $\\mathcal V_1^b$ is not introduced before using.\n3. Probing experiments are conducted on only two downstream tasks (i.e., NER and POS). The results are only from one dataset for each task. The experiments are not convincing.\n4. Again, to demonstrate the effeciency of Bit-cipher for LM integration, the experiments are not enough. For instance, it reach the claim \"This approach enables models to converge more rapidly compared to traditional methods\" without any comparison to classic word embeddings."
                },
                "questions": {
                    "value": "1. What is the basic assumption of bit-cipher for learning word representations? What are the advantages of bit-cipher over classic methods?\n2. Are there any theoretical proofs to support the advantages of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671541843,
            "cdate": 1698671541843,
            "tmdate": 1699636916515,
            "mdate": 1699636916515,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "5XAVU4WLrt",
            "forum": "BqJamqGwp1",
            "replyto": "BqJamqGwp1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7569/Reviewer_WMqp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7569/Reviewer_WMqp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Bit-cipher, a word-embedding technique that eliminates the need of backpropagation while leveraging\ncontextual information and hyper-efficient dimensionality reduction techniques\nbased on unigram frequency, providing strong interpretability, alongside efficiency.\nExperiments illustrate the notable efficiency of cipher in accelerating\nthe training process and attaining better optima compared to conventional training\nparadigms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed method is novel.\n* This paper provides counter-intuitive results that a simple embedding algorithm could yield competitive performance."
                },
                "weaknesses": {
                    "value": "* This paper is quite hard to follow, especially section 3.1 and 3.2. Many notations in those chapters are used without any clarification. I cannot figure out the method until I read section 3.4, which provides a concrete example of building cipher embeddings. Unless authors make great improvements on presentation in the next version, I lean to reject this work.\n* In LLM finetuning experiments, I did not see comparation between cipher embeddings and Word2Vec or Glove."
                },
                "questions": {
                    "value": "See Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675790319,
            "cdate": 1698675790319,
            "tmdate": 1699636916374,
            "mdate": 1699636916374,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mRg9SWVKw5",
            "forum": "BqJamqGwp1",
            "replyto": "BqJamqGwp1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7569/Reviewer_Py7Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7569/Reviewer_Py7Q"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new word embedding approach called \"bit-cipher\". The authors test the approach in NER and POS, and use it for language models. The topic seems old in the large language model era, but this is fine. My main concern is that how the approach contributes to the community given that we already have Word2vec, Glove and many other variants; as a reviewer, I did not see a clear advantage of \"bit-cipher\" over existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The topic is fundamental in NLP\n-  The authors tried to use the proposed method in some modern language models like OPT and T5"
                },
                "weaknesses": {
                    "value": "- **sum** and **cat** seem vert common for NLP.\n- The evaluated tasks like POS and NER, it may not aligned with the performance of downstream tasks.\n-  The performance comparison should be compared with more careful way, such as aligning it with pre-trained corpora and parameter scales to have a apple-to-apple comparison."
                },
                "questions": {
                    "value": "- What is the advantage of \"bit-cipher\" over Word2vec, Glove and many other variants?  what is the siginificance of \"bit-cipher\"? If it is insignificant,  people in the LLM era might not learn anything from it.  Please clarify the siginificance.\n- For benchmarking, is it possible to consider some downstream tasks which are sensitive to word embeddings? The paper https://arxiv.org/pdf/1507.05523.pdf might help."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7569/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7569/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7569/Reviewer_Py7Q"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7569/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718518372,
            "cdate": 1698718518372,
            "tmdate": 1699636916199,
            "mdate": 1699636916199,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]