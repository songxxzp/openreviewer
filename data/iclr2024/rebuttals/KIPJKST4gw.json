[
    {
        "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?"
    },
    {
        "review": {
            "id": "uvtMuxvWvx",
            "forum": "KIPJKST4gw",
            "replyto": "KIPJKST4gw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3194/Reviewer_L6FP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3194/Reviewer_L6FP"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors aim to explore the impact of introducing code at different training stages of large language models (LLMs) and how it affects LLMs\u2019 reasoning capability. The experiments are conducted by introducing code data in the pre-training stage, the instruction tuning stage, and both stages to evaluate LLM through six inference tasks in five domains. They provide deep-in insights via comprehensive experiments and critical analyses. The resources of this paper are released."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper is well-motivated. The impact of code data in LLMs is a hot research question. This paper answers this issue from the reasoning capability aspect.  \n\n- The experiments are comprehensive, and the insights are remarkable. The reasoning capability of LLMs is evaluated via six tasks in five domains. The authors provide critical analyses and significant insights on training LLMs and the reasoning capability of LLMs. \n\n- The idea of dynamic mixed strategy is easy to follow yet effective. It helps LLMs learn reasoning skills progressively during training. \n\n- The authors provide comprehensive open-source resources, demonstrating the reproducibility of the models. These resources are valuable for the LLM community."
                },
                "weaknesses": {
                    "value": "- Missing discussion on the applications. Although the authors conduct experiments and provide insights on training LLMs and improving their reasoning capability, this paper does not discuss how to apply the insights to enhance the LLM products in different domains. \n\n- Unclear construction of training corpus. The author should provide more details about data collection, data cleaning, and training data construction. The authors use fuzzy data deduplication, but they have not explained the tools of fuzzy. They should open-source the data for reproducibility. Besides, the detailed model architecture is missing. \n\n- Table 7 is confusing. The results in Table 7 show the code data will lead to a performance drop on four out of five datasets. It indicates that the code data may not help to improve the reasoning capability of LLMs. The authors should provide valid reasons.\n\n- The related work is limited. Recently, there have been various papers discussing the reasoning capability of LLMs. Therefore, the authors should survey more related papers and compare with them. \n\n- Fix the grammar errors and improve the presentation. On page 8, \u201cThe experiment found that\u2026\u2019\u2019 -> \u201cThe experiment showed that\u2019\u2019. \n\n- Missing future work. The authors should provide the potential future work on LLMs based on the experimental results and insights provided in this paper. \n\n[1] Roziere B, Gehring J, Gloeckle F, et al. Code llama: Open foundation models for code[J]. arXiv preprint arXiv:2308.12950, 2023."
                },
                "questions": {
                    "value": "Please check in Strengths and Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Reviewer_L6FP",
                        "ICLR.cc/2024/Conference/Submission3194/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508074181,
            "cdate": 1698508074181,
            "tmdate": 1700548327428,
            "mdate": 1700548327428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HXeWOVKlqE",
                "forum": "KIPJKST4gw",
                "replyto": "uvtMuxvWvx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L6FP [1/2]"
                    },
                    "comment": {
                        "value": "Thanks for your valuable and constructive comments. We carefully address your concerns as follows. \n\n## **Applications**\nThe main contribution of this paper is to explore the impact of code data on LLM reasoning capabilities at different training stages and draw the following conclusions. Firstly, adding code data in the pre-training stage can effectively improve the model's general reasoning capabilities. Secondly, adding code instructions in the instruction fine-tuning stage can improve specific code reasoning capabilities. Moreover, the dynamic mixing strategy of code and text data assists LLMs in learning reasoning capability step-by-step during training. Below, we provide some ideas for further applications of these conclusions. We have added these discussions to the revised paper and highlighted them. \n\n\n+ **Improve General Reasoning Skills.** The conclusions obtained in this article have direct implications for the application of general LLMs in multiple reasoning-intensive fields (such as legal support, scientific question answering, etc.). Secondly, for the construction of large models in vertical fields (such as large legal models, etc.), it is often necessary to use vertical field data for continued pre-training. High-quality data is the key to model training. If there is a lack of data in vertical fields, you can try to mix certain high-quality code data to improve the model's reasoning capabilities in vertical fields.\n\n\n+ **Improve Domain-specific Reasoning Skills.** This paper finds that mixing in code data during the fine-tuning phase can improve capabilities in specific areas of the code. This may be because instructions for related tasks may activate reasoning and response abilities in the corresponding tasks. Therefore, for specific fields, you can also consider mixing in instructions from specific fields to further improve reasoning capabilities.\n\n\n+ **Importance of Training Data.** This paper verifies the impact of code data through ablation experiments and proves the importance of data. However, there are still data that may be similar to code data, such as scientific paper data, mathematical data, etc. This may also be the key to improving the reasoning ability of language models. The experimental exploration in this article provides a direction for further research on how multiple types of data affect model training.\n\n## **Training Corpus**\n+ **Data Collection**. The data for pre-training is collected from public datasets, including BaiDuQA, CAIL2018, SogouCA, and CodeParrot, and crawled data, including Common Crawl, Encyclopedias, News, and e-books. The constructed corpus contains about 42B tokes.\n\n\n+ **Data Cleaning**. We use the rule-based data cleaning and model-based data filtering methods to ensure the quality of the data. More details can be found in Section F in the Appendix of the original paper. Regarding the cleaning of code data, you can refer to the **Quality of Code Data** section in the reply to the Reviewer PRkj.\n\n\n+ **Tool of Fuzzy**. For fuzzy data deduplication, we employed Spark's MinHashLSH algorithm [1], which is a widely adopted technique in advanced LLMs such as GPT-3. We have revised our paper to expound on these specifics, and the revised parts have been highlighted. \n\n      [1] https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.MinHashLSH.html\n\n\n+ **Data Mixing Strategy**. The mixing ratios of different data resources are demonstrated in Figure 2 of our original paper. Concretely, the ratios of CodeParrot, Common Crawl, e-Books, News, Encyclopedias, and Public Datasets are 36.1\\%, 6.00\\%, 13.50\\%, 13.50\\%, 14.00\\%, and 16.90\\%, respectively. Besides, to ensure that the model is not biased towards any specific data type during the pre-training process, we randomly shuffle the samples in the training corpus. In addition, during the instruction-tuning phase, we discuss the impact of different data-mixing strategies on model performance. For details, please refer to Section 3.3.4 of the original article.\n\n+ **Open-souce Data**. Thanks for your constructive suggestion. Most of the components of our training corpus are open-source datasets. We will consider releasing our training corpus to guarantee reproducibility and make a data contribution if the paper is accepted."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700069924065,
                "cdate": 1700069924065,
                "tmdate": 1700069982822,
                "mdate": 1700069982822,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7FmBVO7b5n",
                "forum": "KIPJKST4gw",
                "replyto": "uvtMuxvWvx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L6FP [2/2]"
                    },
                    "comment": {
                        "value": "## **Detailed Model Architecture**\nOur model, along with other LLMs like Llama and Google PaLM, shares a common architecture based on the GPT-3 causal decoder architecture. Besides, the pre-training task is the next token prediction task. Furthermore, we refer to the structural design of open-source models such as PanGu-alpha, CPM2.6B, and EVA2.6B to retain the setting of the 32-layer transformer decoder. The detailed model architecture diagram is shown in Appendix B.\n\n\n\n## **Table 7**\nOur paper extensively evaluates various reasoning tasks, including logical and code reasoning, highlighting the positive impact of code-related data. Additionally, we sought to ascertain whether code data would affect common-sense tasks (which do not require high reasoning abilities). To address this, we conducted experiments outlined in Table 7, revealing that code data had minimal influence on performance across other tasks.\n\n\n\n## **Related Work**\nThanks for your comment. We have added the following discussion to related work in our revised paper. This part has been highlighted in the paper. HELM test [2] provides a large-scale evaluation of existing large language models. They found that the code-cushman-001model trained on code data had more reasoning power than other natural language models. The AI2 [3] work also shows that, when equipped with complex thought chains, the code-davinci-002 model is currently the best-performing model on important mathematical benchmarks such as GSM8K. In addition, some work analyzed the reasoning ability of large language models through the way of thinking chain and found that because the original GPT-3 had not been subjected the code training, it could not do CoT reasoning [4]. The PaLM training data contains 5% of the code training data, which shows that PaLM can effectively perform CoT reasoning. However, in the above work, the evaluated models have different parameters and data sizes, and there are problems, such as unknown training details. Therefore, it is not rigorous to speculate the exact impact of code data on reasoning ability only by comparing existing models.\n\n    [2] Liang P, Bommasani R, Lee T, et al. Holistic evaluation of language models[J]. arXiv preprint arXiv:2211.09110, 2022.\n    \n    [3] Fu Y, Peng H, Sabharwal A, et al. Complexity-based prompting for multi-step reasoning[J]. arXiv preprint arXiv:2210.00720, 2022.\n    \n    [4] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[J]. Advances in Neural Information Processing Systems, 2022, 35: 24824-24837.\n\n\n\n## **Grammar Errors**\nThank you for your comments on the expression of the paper. We have corrected this grammar error in the revised paper. We will carefully correct the typos and further improve the presentation in the future. \n\n\n\n## **Future Work**\nModels such as ChatGPT illustrate that larger-scale models will produce emergent capabilities. Therefore, the follow-up of this paper will study how to code the impact of data on models of different sizes and explore the relationship between code data and emergent capabilities. We have added these discussions to the revised paper and highlighted them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700070043135,
                "cdate": 1700070043135,
                "tmdate": 1700070063505,
                "mdate": 1700070063505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V1jZM9kl6P",
                "forum": "KIPJKST4gw",
                "replyto": "uvtMuxvWvx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Reviewer_L6FP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Reviewer_L6FP"
                ],
                "content": {
                    "title": {
                        "value": "Respond to the Authors"
                    },
                    "comment": {
                        "value": "Thanks for the clarification, my concerns have been addressed. After reading the responses of the authors and other reviews, I decided to raise my score. The reasons are as follows. The authors add a discussion of applications, details of dataset construction, and related work. Other reviewers also considered the paper to be a valuable research work."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548304332,
                "cdate": 1700548304332,
                "tmdate": 1700548304332,
                "mdate": 1700548304332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dXTv0waMHt",
            "forum": "KIPJKST4gw",
            "replyto": "KIPJKST4gw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3194/Reviewer_PRkj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3194/Reviewer_PRkj"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to answer an important research question: at which training stage does code data help LLMs reasoning? The authors introduce code at the pre-training stage, instruction-tuning stage, and both. The reasoning capability of LLMs is evaluated by six reasoning tasks. Through comprehensive experiments and careful analyses, they provide inspiring conclusions and insights. The authors open-source the code and model parameters."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Valuable research question. The paper raises a meaningful research question: at which training stage introducing code data can really help the reasoning capabilities of LLM? This question is of critical significance for understanding the training and application of LLM. \n \n2. Comprehensive experimental design. This paper provides a comprehensive and fair evaluation of the reasoning capabilities of LLMs on six reasoning tasks covering five domains. This broad experimental scope ensures the generalizability and reliability of the conclusions. Additionally, the authors compare models with different sizes to verify the generalization of the conclusion. \n \n3. In-depth analyses and insights. The paper not only provides experimental results but also performs in-depth analysis, providing insights into mixing code and text data to enhance the general reasoning capabilities and code reasoning capabilities of LLM. Specifically, in the pre-training stage, mixed code data helps LLM improve general reasoning capabilities, and in the SFT stage, mixed code data helps LLM improve specific code reasoning capabilities."
                },
                "weaknesses": {
                    "value": "1. Experimental details are insufficient. The paper may not provide enough details on experimental settings and parameter selection in some parts (such as data mixing strategies, decoding strategies, etc.). This might challenge researchers attempting to replicate or extend this work.\n\n2. Recently, various code foundation models, such as CodeLlama [1], have been opened. However, the authors do not conduct any discussions or experiments on them. In my opinion, the reasoning capability of code foundation models is also an essential part of the interest scope of this work. \n\n3. Quality of code data. The quality of code data can have a significant impact on the reasoning capabilities of LLM. How the author ensures the high quality of code data is not discussed in depth in the article.\n\n4. The related work part is weak. Missing important papers, such as [1,2,3].\n[1] Roziere B, Gehring J, Gloeckle F, et al. Code llama: Open foundation models for code[J]. arXiv preprint arXiv:2308.12950, 2023.\n[2] Yang A, Xiao B, Wang B, et al. Baichuan 2: Open large-scale language models[J]. arXiv preprint arXiv:2309.10305, 2023.\n[3] Li P, Sun T, Tang Q, et al. CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors[J]. arXiv preprint arXiv:2305.05711, 2023."
                },
                "questions": {
                    "value": "1. Experimental details. In the paper, certain key sections, such as data mixing strategy and decoding strategy, do not seem to give sufficient details of experimental settings and parameter selection. This lack of information can lead to difficulties in reproducing and scaling. How was it set up by the author in the actual experiment?\n\n2. Quality of code data. How were the coding data used by the authors in their experiments collected and screened? The quality of code data can have a significant impact on the reasoning capabilities of LLM. For example, high-quality code data may provide more reasoning impact, while low-quality data may cause the model to learn wrong patterns. How did the authors ensure that the code data used were representative and of high quality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Reviewer_PRkj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549288469,
            "cdate": 1698549288469,
            "tmdate": 1699636267360,
            "mdate": 1699636267360,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "87M2Eq4k3D",
            "forum": "KIPJKST4gw",
            "replyto": "KIPJKST4gw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3194/Reviewer_BTot"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3194/Reviewer_BTot"
            ],
            "content": {
                "summary": {
                    "value": "Previous work has shown that models trained on code perform better on reasoning tasks. The research question in this paper is at which training stage code data helps reasoning. Specifically, this work looks into two training stages: pre-training and instruction-tuning. The results show that for performance on reasoning tasks, adding code data at the pre-training stage is effective whereas at the instruction-tuning the effect is far less (sometimes leading to lower performance); however, instruction tuning on code can improve model performance on code-related problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Clear Hypothesis:** The paper asks a clear question and provides a clear setup for testing various hypothesis about that question.\n- **Clear message:** The results show a clear message about the research question, albiet only on smaller-sized models.\n- **Clarity of the writing:** The writing was mostly clear and easy to follow"
                },
                "weaknesses": {
                    "value": "- **Datasets:** I was not familiar with some of the datasets used in this work and after looking into some of them, I could not get a sense of how general and challenging they are. The majority of the computation cost for this project seems to be on the training stage, so I believe reporting results on a few more datasets (maybe only for Tables 2 and 3) can strengthen the main arguments of the paper. That could include datasets from other reasoning domains (e.g., math might be an important one that does not appear in the results) or from the same domains but on datasets that are more established.\n- **Mixture experiment:** The experiment on exploring ways to mix code and text data is interesting, but given that adding code at the instruction-tuning stage was already shown to be not that effective, I wonder why it was tested for the instruction-tuning stage. I understand the high computation cost of pre-training, but it seems to me that given the previous set of results, this experiment makes sense mostly at the pre-training stage where we have seen that code data can be effective.\n- **Language Inconsistency:** - For the results in Section 3.3.5, the authors conclude that training with code data *has little negative impact* on the performance of other tasks. But the numbers in Table 7 don't seem to show little negative impact. The large impact on the DuReader dataset has been already pointed out by the authors. Moreover, on CMNLI the performance decreases from 45.07 to 43.49 which is almost equal in magnitude to some of the gains reported in Table 2. I believe the language should be more consistent on the amount of improvement/decrement that can be considered a significant amount for the datasets and the experimental setup of the paper.\n- **Minor suggestions:** 1- In Table 3, there are multiple equal numbers but only one of them is in bold face. 2- In Table 4, I suggest reversing the rows and columns to make it consistent with the other tables."
                },
                "questions": {
                    "value": "- For the Logic dataset, I see that the accuracy for many different models is 40.9. This value appears in Table 2, Table 3 and Table 6. Does this hint at a potential problem in this dataset? \n- Some of the datasets don't seem to be available in English (or at least I could not find an English version of them). Could you include some examples from these datasets translated to English, so the reader can get a sense of the nature and the difficulty level?\n- It has been observed that larger LLMs often behave differently than smaller LLMs. I'd be curious to hear the authors' thoughts on how they think their results might transfer to larger models? (to be clear, I understand the computation cost and I'm not suggesting that you run those experiments)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Reviewer_BTot"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779161278,
            "cdate": 1698779161278,
            "tmdate": 1700514615129,
            "mdate": 1700514615129,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "46bvFXkHUS",
                "forum": "KIPJKST4gw",
                "replyto": "87M2Eq4k3D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BTot [1/3]"
                    },
                    "comment": {
                        "value": "## **Datasets**\nIn terms of performance evaluation, the datasets used in this paper are all public datasets, which are widely adopted by many methods for testing model performance. For example, the Logic data comes from C_Eval [1], and models such as ChatGLM [2] and Qwen [3] have been tested on this dataset. Besides, MBPP [4] is a widely used code testing benchmark and is widely used by models such as GPT-4 [5], Llama 2 [6], etc.\n\n    [1] Huang Y, Bai Y, Zhu Z, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models[J]. arXiv preprint arXiv:2305.08322, 2023.\n\n    [2] Zeng A, Liu X, Du Z, et al. Glm-130b: An open bilingual pre-trained model[J]. arXiv preprint arXiv:2210.02414, 2022.\n\n    [3] Bai J, Bai S, Chu Y, et al. Qwen technical report[J]. arXiv preprint arXiv:2309.16609, 2023.\n\n    [4] Austin J, Odena A, Nye M, et al. Program synthesis with large language models[J]. arXiv preprint arXiv:2108.07732, 2021.\n\n    [5] https://openai.com/research/gpt-4\n\n    [6] Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023.\n\n\n+ **General and Challenging Datasets**. \n\n  + **For the general test**, to comprehensively test the reasoning abilities of the models, we used different types of reasoning tasks and datasets for performance testing, including logical reasoning, legal reasoning, scientific question and answer, and code generation. These tasks comprehensively reflect the model's performance on different types of reasoning capabilities.\n\n  + **For the challenge**, the evaluated models need to achieve promising reasoning results on all types of reasoning tasks, which is very challenging. Besides, the datasets used in this article usually require multi-step reasoning to obtain correct results, further improving the difficulty of reasoning.\n\n\n+ **Examples of Datasets**.\nSorry for causing trouble for your understanding. For the non-English data sets used in this article, following your suggestion, we have translated them into English for display. Below are examples from the Logic, JEC-QA, and E-KAR datasets. We have added these examples to the revised paper and highlighted them. \n\n  + **One Example of Logic (Logical Reasoning)**\n    \n        Problem: \n\n        Regarding the physical education standard test for Class A, three teachers made the following predictions: Teacher Zhang said, \"Not everyone will fail.\" Teacher Li said, \"Someone will fail.\" Teacher Wang said, \"Both the class president and the study committee member will pass.\" If only one of these teachers' predictions is correct, which of the following must be true?\n\n        Answer List:\n\n        A: \"Both the class president and the study committee member failed.\"\n        B: \"Both the class president and the study committee member passed.\"\n        C: \"The class president passed, but the study committee member failed.\"\n        D: \"The class president failed, but the study committee member passed.\"\n\n        Answer: A\n\n  + **One Example of JEC-QA (Legal QA)**\n\n\n        Problem: \n\n        A miscellaneous article written by person A caused a significant stir after its publication. The article was reprinted by several newspapers and websites without compensation. Person B translated the article into French and person C translated it into Uighur. Both translations were published domestically without A's consent and without any remuneration. Which of the following viewpoints is correct?\n\n        Answer List:\n\n        A: \"The act of newspapers and websites reprinting the article does not constitute infringement.\"\n        B: \"The actions of both B and C do not constitute infringement.\"\n        C: \"B's action does not constitute infringement, but C's action does.\"\n        D: \"B's action constitutes infringement, but C's action does not.\"\n        \n        Answer: C\n\n\n\n  + **One Example of E-KAR (Analogical Reasoning)**\n\n        Problem: \n\n        Based on the given relationship [Speed:Time:Distance], choose the option that fits this relationship.\n        \n        Answer List:\n\n        A: \"Interest Rate:Principal:Interest\"\n        B: \"Quality:Variety:Quantity\"\n        C: \"Profit:Cost:Value\"\n        D: \"Income:Expenditure:Surplus\"\n\n        Answer: A \n\n        Explanation: From the given relationship, we infer the following: \"Speed\" multiplied by \"Time\" equals \"Distance\". In option A, \"Interest Rate\" multiplied by \"Principal\" equals \"Interest\". In option B, there is no clear logic connecting \"Quality\", \"Variety\", and \"Quantity\". In option C, the product of \"Profit\" and \"Cost\" is not \"Value\". In option D, \"Surplus\" is the difference between \"Income\" and \"Expenditure\". Therefore, the correct choice is A."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201531064,
                "cdate": 1700201531064,
                "tmdate": 1700208293970,
                "mdate": 1700208293970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uoTGFhqlWr",
                "forum": "KIPJKST4gw",
                "replyto": "87M2Eq4k3D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BTot [2/3]"
                    },
                    "comment": {
                        "value": "## **Datasets**\n+ **Other Datasets**.\n\n  In order to more comprehensively verify the observations of this article, we selected the high school mathematics and high school physics problem parts of the MMLU[1] test set to evaluate the model in the pre-training stage. MMLU is a currently widely used dataset to evaluate the comprehensive ability of LLMs [2, 3], among which mathematics and physics can better reflect the reasoning ability of the model. The results obtained and the conclusions are shown in the table below. We have added these results to the revised paper and highlighted them. Thanks for your suggestions again.\n\n      [1] Hendrycks D, Burns C, Basart S, et al. Measuring massive multitask language understanding[J]. arXiv preprint arXiv:2009.03300, 2020.\n\n      [2] https://openai.com/research/gpt-4\n\n      [3] Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023.\n\n\n\n  | Task       | NL (2.6B) | NL (13B) | **CODE (2.6B)** | p value| *LLaMA (7B)*  |\n  |------------|--------|-------|--------------|------------------------------------------|---------|\n  | MMLU_Math  | 24.16*  | 22.30 | **24.91**   | <0.05    |  *24.97*  |\n  | MMLU_Physics| 20.00 | 22.67* | **26.67**    | <0.05    |  *27.97*  |\n\n  *Note: Source of LLaMA results: https://github.com/baichuan-inc/Baichuan-7B*\n\n  In this table, **bold** values denote the best result, and the values* denote the runner-ups in the scope of this paper. The t-test is conducted to verify the significant improvement of the best values compared with the runner-up values. Besides, the italics denotes the models out of the scope of this paper. From these experimental results, we have three conclusions as follows. \n\n  + On mathematical and physical reasoning tasks, the CODE (2.6B) model shows advantages over NL (2.6B) and NL (13B), which strengthens the effectiveness of introducing code data in the pre-training stage. We performed a t-test for statistical significance, and the results showed that the p-value was less than 0.05, which indicated that the results were statistically significant.\n  + We admit that models, including LLaMA (7B) and CODE (2.6B), perform relatively poorly on mathematical and physical tasks, but we believe that the relative improvement brought by code data is trustworthy. This also shows how it is necessary to further improve mathematics-related tasks.\n\n\n+ **Logic Dateset**.\nThanks for your reminder. We carefully inspected the Logic dataset and the output of models. We found that the distribution of the predicted scores is different between different models. Besides, some of the answers to the questions overlapped while others were different. The similar low accuracy might be due to the difficulty of the Logic dataset.\n\n\n## **Mixture Experiment**\nAs you said, due to resource constraints, we only conducted data mixing experiments during the instruction-tuning phase. In this experiment, we found that using a higher proportion of code data in the early stage can improve the specific code reasoning ability of the language model. We think this may have implications for some vertical scenarios that require fine-tuning to implement. At the same time, in view of the findings in the fine-tuning phase, we are also planning to explore data mixing experiments in the pre-training phase. We believe this is a promising direction. Thanks for your suggestion. \n\n## **Language Inconsistency**\nThanks for your suggestion. After carefully verifying the results of our article, particularly in section 3.3.5, we found that the code data had an impact on the DuReader task. We believe this may be due to the model not fully learning from the code and text data, leading to confusion when generating answers to reading comprehension questions. To avoid misleading the readers, we have improved our conclusion in the revised paper to state: \"Using code data for training may negatively impact the performance of other tasks.\"\n\n## **Minor Suggestions**\nThanks for your suggestions.\n- Regarding the bolding of identical performances, we have made the corrections in the revised paper. \n- For Table 4, following your suggestion, we have reversed the rows and columns to make it consistent with other tables."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700208284872,
                "cdate": 1700208284872,
                "tmdate": 1700330508782,
                "mdate": 1700330508782,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cuVybwP3jH",
                "forum": "KIPJKST4gw",
                "replyto": "FFJFG8Fplb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Reviewer_BTot"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Reviewer_BTot"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the detailed responses and for the new experiments and clarifications. I have increased my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700514645394,
                "cdate": 1700514645394,
                "tmdate": 1700514645394,
                "mdate": 1700514645394,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ndP5xZp2gJ",
            "forum": "KIPJKST4gw",
            "replyto": "KIPJKST4gw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3194/Reviewer_viag"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3194/Reviewer_viag"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the effect of including code data during pre-training and instruction tuning over the reasoning capabilities of LLMs. The authors conduct a set of ablation experiments where code is added/removed from both pre-training and fine-tuning and the model performance is measured over a set of reasoning tasks ranging from logical to legal reasoning. The results show that including code data during pre-training is more effective than during instruction tuning. Also, the results show that instruction tuning over the code can end up hurting performance on some tasks. The authors also experimented with a dynamic text-code mixing strategy during instruction tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The problem studied is interesting: the relationship between code data and reasoning and the paper aims to somehow tackle this issue.\n* The paper is well-written and the results are well-presented."
                },
                "weaknesses": {
                    "value": "* From my understanding (and correct me If I'm wrong), the code model is trained on more overall tokens than the NL model. I would expect a study like that to control for the number of pre-training tokens while changing their nature i.e., text vs. code. If the code model is trained on as many natural text tokens as the baseline model in addition to having code in the pre-training data, then the code model should be expected to perform better because it was trained on more data. No surprise there. \n* It's hard to say whether the results reported have statistical significance. For example, in Table 2, the code model is only 0.13 points better than the NL model on ScienceQA. Are these results significant? And are these enough to conclude that code reasoning? I would expect the authors to run a statistical significance test to support their results. \n* The proposed dynamic mixing strategy produces very marginal improvements (except over logical reasoning) and has a negative effect on the performance over three tasks. The paper does not thoroughly investigate why this is the case. Also, the design of the mixing strategy seems rather arbitrary. \n* The evaluation does not cover mathematical reasoning, although it's one type of reasoning where we should expect great improvements since code data is roughly similar to math data."
                },
                "questions": {
                    "value": "* The choice of the baseline model is unclear. Why use PanGu2.6B as your baseline model? \n* Why does your mixing strategy follow the 7:3, then 6:4, then 5:5? Why not, for example, 9:1, 7:3, 5:5, 3:7, 1:9? which would be both increasing and decreasing. And why only 4 phases? What's the intuition behind that design?\n\n\n\n===== POST REBUTTAL =====\n\nI thank the authors for their response. Based on the response and the expectation that the authors will add results from training a model on 150G of natural data, I have decided to increase my score to 5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3194/Reviewer_viag",
                        "ICLR.cc/2024/Conference/Submission3194/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3194/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807872340,
            "cdate": 1698807872340,
            "tmdate": 1700854780532,
            "mdate": 1700854780532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2SigTHvC6i",
                "forum": "KIPJKST4gw",
                "replyto": "ndP5xZp2gJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer viag [1/3]"
                    },
                    "comment": {
                        "value": "## **Training Data**\nThanks for your insightful and constructive comments.\n\n+ **Training Corpus**. Your understanding is partly correct. Actually, NL (2.6B) is trained on about 100G natural language data. Besides, CODE (2.6B) is trained on about 100G natural language data plus 50G code data. In addition, NL (13B) is trained on more than 150G natural language data. \n\n+ **Effectiveness**. In the pre-training and instruction tuning stages, we compare the above three models, including NL(2.6B), NL(13B), and CODE(2.6B). In the pre-training stage, Table 2 shows that, on the reasoning task, CODE (2.6B) with mixed code data and natural language not only outperforms NL (2.6B) with natural language but also outperforms the larger pure text model NL (13B) trained on more text data. These experimental results highlight the importance of code data in the pre-training stage. Besides, at the instruction tuning stage, Table 3 demonstrates that CC (2.6B), which is fine-tuned on code instructions, performs better on specific code tasks, highlighting the importance of code data in the instruction tuning stage.\n\n+ **Additional Study**. Thanks for your insightful suggestion! We have already begun our pre-training of NL(2.6) on 150G natural language data. Due to the limitation of computational resources, it will take about 15 days. We will add the results and conclusions in the revised paper to further improve the comprehensiveness and quality of this paper. \n\n+ **More Data**. Actually, as claimed in **Effectiveness**, the fact that CODE (2.6B) beats NL (13B) on reasoning can show the importance of code data in improving the reasoning capacity of models. Besides, we do not agree that simply adding more data can definitely result in better performance. The reasons are as follows. 1) Large amounts of data may easily lead to underfitting problems of the models. 2) The low-quality data may lead to the hallucination problem in LLMs. 3) According to many scaling laws [1, 2, 3], the model size, dataset size, and the amount of training computation should be amplified simultaneously to achieve better performance. Therefore, according to the extensive experiments and analyses as claimed in **Effectiveness**, we demonstrate the importance of the code data in both pre-training and fine-tuning of LLMs. \n\n      [1] Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language models[J]. arXiv preprint arXiv:2001.08361, 2020.\n\n      [2] Hoffmann J, Borgeaud S, Mensch A, et al. Training compute-optimal large language models[J]. arXiv preprint arXiv:2203.15556, 2022.\n\n      [3] OpenAI R. Gpt-4 technical report. arxiv 2303.08774[J]. View in Article, 2023, 2.\n\n## **t-Test**\nThanks. \n+ In order to comprehensively evaluate the model's reasoning ability, we used different types of reasoning tasks and datasets for testing, including logical reasoning, legal reasoning, scientific question and answer, and code generation. These tasks comprehensively reflect the model's performance on different types of reasoning capabilities. Due to the diversity and complexity of these reasoning tasks, achieving promising performance on all tasks is a challenge. In Table 2, compared with NL (2.6B), we find that CODE (2.6B) can bring 4.54% and 3.88% performance improvement on MBPP and E-KAR datasets. \n\n+ Besides, following your suggestion, in order to illustrate the significance of these results, we conducted a t-test on the predicted scores in the following table. It demonstrates that all p-values are less than 0.05, indicating that the results are statistically significant. We have already add these results to the revised paper and highlighted them.\n\n  | Dataset | p-value (<0.05) |\n  |-----------|-----------------|\n  | Logic     | 4.197e-06       |\n  | JEC-QA    | 1.956e-25       |\n  | ScienceQA | 0.014          |\n  | E-KAR     | 7.013e-07       |\n  | CosQA     | 1.066e-40       |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328307819,
                "cdate": 1700328307819,
                "tmdate": 1700329678908,
                "mdate": 1700329678908,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3qIqCPnL4w",
                "forum": "KIPJKST4gw",
                "replyto": "ndP5xZp2gJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer viag [2/3]"
                    },
                    "comment": {
                        "value": "## **Model Choice**\n- PanGu, along with other LLMs like Llama and Google PaLM, shares a common architecture based on the GPT-2 decoder-only architecture and next token prediction task. In addition, due to resource limitations, we selected the 2.6B model for experiments and compared it with the 13B model. Because open-source models such as CPM2.6B [3], EVA2.6B [4], and ChatGLM2.6B [5] all use the 2.6B scale and show that they already have some general capabilities.\n\n- At the same time, we look forward to the emergence of LLMs with different architectures than GPT, and we also look forward to following up and verifying more LLMs with different architectures.\n\n- In this research, the GPU and funding supporters adopt the PanGu model. Similar facts are that Google prefers PaLM, OpenAI prefers GPT, and Meta prefers LLaMA. We focus more on exploring the research question itself, i.e., at which stage can code data help LLMs reasoning. The model choice should not be an advantage or a disadvantage. \n\n      [3] Zhang Z, Han X, Zhou H, et al. CPM: A large-scale generative Chinese pre-trained language model[J]. AI Open, 2021, 2: 93-99.\n\n      [4] Zhou H, Ke P, Zhang Z, et al. Eva: An open-domain chinese dialogue system with large-scale generative pre-training[J]. arXiv preprint arXiv:2108.01547, 2021.\n\n      [5] https://github.com/THUDM/ChatGLM2-6B\n\n## **Mathematical Reasoning**\n\nFollowing your suggestion, we selected the high school mathematics and high school physics problem parts of the MMLU [6] test set to evaluate the model in the pre-training stage. MMLU is a currently widely used dataset to evaluate the comprehensive ability of LLMs [7, 8], among which mathematics and physics can better reflect the reasoning ability of the model. The results obtained and the conclusions are shown in the table below. We have added these results to the revised paper and highlighted them. Thanks for your suggestions again.\n\n    [6] Hendrycks D, Burns C, Basart S, et al. Measuring massive multitask language understanding[J]. arXiv preprint arXiv:2009.03300, 2020.\n\n    [7] https://openai.com/research/gpt-4\n\n    [8] Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023.\n\n\n\n  | Task       | NL (2.6B) | NL (13B) | **CODE (2.6B)** | p value| *LLaMA (7B)*  |\n  |------------|--------|-------|--------------|------------------------------------------|---------|\n  | MMLU_Math  | 24.16*  | 22.30 | **24.91**   | <0.05    |  *24.97*  |\n  | MMLU_Physics| 20.00 | 22.67* | **26.67**    | <0.05    |  *27.97*  |\n\n  *Note: Source of LLaMA results: https://github.com/baichuan-inc/Baichuan-7B*\n\n  In this table, **bold** values denote the best result, and the values* denote the runner-ups in the scope of this paper. The t-test is conducted to verify the significant improvement of the best values compared with the runner-up values. Besides, the italics denotes the models out of the scope of this paper. From these experimental results, we have three conclusions as follows. \n\n  + On mathematical and physical reasoning tasks, the CODE (2.6B) model shows advantages over NL (2.6B) and NL (13B), which strengthens the effectiveness of introducing code data in the pre-training stage. We performed a t-test for statistical significance, and the results showed that the p-value was less than 0.05, which indicated that the results were statistically significant.\n  + We admit that models, including LLaMA (7B) and CODE (2.6B), perform relatively poorly on mathematical and physical tasks, but we believe that the relative improvement brought by code data is trustworthy. This also shows how it is necessary to further improve mathematics-related tasks."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329610110,
                "cdate": 1700329610110,
                "tmdate": 1700330566249,
                "mdate": 1700330566249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BtlWVP0XrY",
                "forum": "KIPJKST4gw",
                "replyto": "ndP5xZp2gJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3194/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer viag [3/3]"
                    },
                    "comment": {
                        "value": "## **Dynamic Mixing Strategy**\nThanks for your concern, which can help us to improve the paper. \n\n- In order to comprehensively evaluate the model's reasoning ability, we used different types of reasoning tasks and datasets for testing, including logical reasoning, legal reasoning, scientific question and answer, and code generation. These tasks comprehensively reflect the model's performance on different types of reasoning capabilities. Due to the diversity and complexity of these reasoning tasks, achieving promising performance on all tasks is a challenge. We admit that the dynamic mixing strategy can not achieve the best results on all reasoning tasks. But it is really an interesting idea that can improve the reasoning capacity of LLMs during training. We have not carefully studied the segment setting and mixing ratios. The experimental setting is a preliminary attempt. Therefore, further improvements can be made in the future, such as setting up more segments and analyzing different ratios. We believe this research question will be a promising direction. \n\n- Still, under these settings, we obtained some interesting and insightful experimental conclusions in the original paper, especially on the coding task. We have added these details in the revised paper and highlighted them.\n  - Using a larger code proportion in the early stage can improve the performance of LLM in coding tasks (CosQA and MBPP). The reason may be that a higher code proportion in the early stage can better activate code-related reasoning capabilities under a higher learning rate.\n  - The descending strategy can improve the performance of logic. Since the code data is more logical, giving more codes in the initial stage may improve the performance of logic reasoning.\n  - In the other three datasets, uniform is better, probably because these tasks require both logical reasoning ability and common sense as well as natural language understanding ability.\n Therefore, we recommend choosing different hybrid strategies based on the characteristics of different downstream tasks.\n\n- Designs\n  - The experimental settings of the proposed dynamic mixing strategy are a preliminary exploration. We divided the experiment into four stages based on experience and the proportion of the dataset. Based on your suggestions, we will carefully explore more training stages in the future. We believe it will be a promising and useful training strategy. \n  - In order to maintain the ability of the general model, we do not set widely different stage proportions. Because a ratio that is too high or too low may cause the model to turn into a proprietary code model or a text model. Among them, the 5:3 ratio in the uniform sampling stage is the original text and code instruction data ratio."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3194/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331044959,
                "cdate": 1700331044959,
                "tmdate": 1700376747252,
                "mdate": 1700376747252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]