[
    {
        "title": "Hypergraph Dynamic System"
    },
    {
        "review": {
            "id": "Xq8wUrrJtV",
            "forum": "NLbRvr840Q",
            "replyto": "NLbRvr840Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3379/Reviewer_HBiR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3379/Reviewer_HBiR"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends hypergraph neural networks (HGNNs) to model hypergraph dynamic systems. Specifically, the authors integrate the graph propagation scheme of HGNN into a control-diffusion ODE form to capture dynamics. Theoretical analysis highlights the controllability and stabilization properties of the proposed HDS$^{ode}$, which allows it to capture long-range correlations among vertices. Experimental results demonstrate the effectiveness of HDS$^{ode}$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The combination of Neural ODEs with hypergraphs is an interesting idea, bringing together two distinct approaches to modeling dynamic systems.\n+ The authors introduce a Lie-Trotter splitting method as the ODE solver, which is a notable contribution.\n+ The theoretical analysis on stability and eigenvalue properties of hypergraphs is solid and persuasive."
                },
                "weaknesses": {
                    "value": "- The proposed model appears to be a straightforward combination of existing efforts on HGNN and neural graph ODEs, essentially replacing the message-passing scheme of GNNs with a diffusive hypergraph adjacency. It would be beneficial to clarify how this combination advances the field beyond existing approaches.\n- The application of hypergraph ODEs is not sufficiently motivated. Although the proposed model claims to capture system dynamics, it is evaluated on node classification tasks using static graphs, leaving the potential benefits on dynamical systems unclear.\n- The experimental results show limited improvement over baseline methods, and from the comprehensive comparison it seems that variations in model structures have little influence on performance."
                },
                "questions": {
                    "value": "1. What is the rationale for applying ODEs and continuous methods to static graphs? How does the model benefit from the system dynamics brought by its structure?\n2. Could you provide insights into the model's performance when the hypergraph convolution is replaced with simpler graph operators like GCN, as part of ablation studies? The functionality of hypergraph convolution in the model is unclear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Reviewer_HBiR"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697701677872,
            "cdate": 1697701677872,
            "tmdate": 1700728209560,
            "mdate": 1700728209560,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eg1DC1AJdO",
                "forum": "NLbRvr840Q",
                "replyto": "Xq8wUrrJtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBiR (Part 1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for providing valuable comments and feedback. We hope that our response can address your concerns.\n\n$\\newline$\n\n**Response to Weakness 1**\n\nThank you for your valuable feedback. I appreciate the opportunity to clarify how our proposed model represents a significant benefit beyond the existing HGNN and graph ODEs.\n\n\n\n**Leveraging Strengths of Hypergraph Neural Networks and Graph ODEs**. Our model leverages the strengths of both hypergraph neural networks and graph ODE. Our model captures high-order correlation beyond pairwise interactions that are often overlooked in graph models. Unlike discrete models, such as HGNN, which can only capture representation at a few hidden layers, our ODE-based hypergraph method continuously tracks the evolution of these representations until finally stable. \n\n\n\n**Effective Operation at Higher Layer Depths**. Another significant advantage of our model lies in its capacity to operate effectively at higher layer depths. This capability contrasts with most traditional (hyper)graph neural networks, which are typically constrained to lower layer depths due to issues like over-smoothing. In (hyper)graph neural networks, the restriction to fewer layers means that each vertex primarily interacts with its immediate, local neighbors. This localized view often neglects the potential influence of more distant vertices, leading to a limited understanding of the overall (hyper)graph. Our model overcomes this limitation with its capacity for higher layer depths enabled by the diffusive hypergraph.\n\n\n\n**Stability with Increased Layers**. The key advantage of our model is its stability at increased layer depths. While additional layers in most existing (hyper)graph approaches can lead to instability, our model maintains stable performance as the number of layers grows, which means that our model is more robust. This stability is crucial for delivering consistent and reliable results, marking a notable improvement over current methods and a meaningful contribution to the hypergraph field.\n\n$\\newline$\n\n**Response to Weakness 2**\n\nThank you for your insightful feedback regarding the motivation and application of hypergraph ODEs in our work. We are confident that our hypergraph ODE has the potential to advance continuous industrial scenarios involving hypergraphs. While our current evaluation focuses on static graph node classification tasks, we believe this serves as a foundational step in showcasing the stability and accuracy of our model.\n\n\n\nIn industrial applications, particularly those involving hypergraphs, stability in model predictions is as crucial as accuracy. Our model's ability to provide consistent and reliable predictions across diverse data types and conditions is a key advantage. Our model is available for various tasks. The choice of vertex classification tasks for our experiment is driven by their effectiveness in evaluating the stability and performance of hypergraph neural networks across common benchmarks. \n\n\n\nIn our article, we take a step towards achieving dynamic continuity in previous hypergraph neural networks by supporting the dynamic continuous representation of vertex and hyperedge representations through the ODE. We thank the reviewer for your suggestion that both hypergraph structure and representation be dynamic, which will serve as a future direction for our research.\n\n$\\newline$\n\n**Response to Weakness 3**\n\nThank you for your observations regarding the experimental results. We understand your concerns about the performance improvement over baseline methods. However, we wish to emphasize that the **primary contribution** of our work lies in achieving stable representation learning in (hyper)graphs, a general challenge in (hyper)graph neural networks [1,2]. \n\n\n\n**We emphasize the dual aspects of performance** in our work: not just the direct performance metrics but also, and importantly, the stability of performance. In practical applications of hypergraph neural networks, achieving optimal performance often depends on specific conditions, a requirement that many existing methods struggle to meet consistently. This is why the robustness becomes valuable. Our focus on **robustness** and reduced dependence on such conditions ensures reliable and consistent performance across a variety of scenarios and layer depths. Further, our model not only solves the stability problem of increasing the number of layers, but also provides performance enhancement.\n\n\n\nOur model exhibits consistently high performance across an increasing number of layers, a notable advancement compared to many existing hypergraph neural networks where performance typically degrades with layer depth. This focus on maintaining stability at greater layer depths is a key advancement compared to models that only emphasize peak performance under specific conditions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214278346,
                "cdate": 1700214278346,
                "tmdate": 1700214278346,
                "mdate": 1700214278346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J1vtSn8wRQ",
                "forum": "NLbRvr840Q",
                "replyto": "Xq8wUrrJtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBiR (Part 2/3)"
                    },
                    "comment": {
                        "value": "**Response to Question 1** \n\nThank you for your insightful query. There are several key rationales for applying ODEs and continuous methods to static hypergraphs. \n\n\n\n**Continuous Representations**. By modeling the continuous changes in vertex and hyperedge representations over time, we enable the neural network's hidden layer representation to transform continuously [3]. This approach facilitates a more effective capture of the evolving nature of the representation, ensuring stable outcomes as the number of layers increases.\n\n\n\n**Extended Interaction Range**. Furthermore, integrating ODEs and continuous methods addresses a critical limitation of general (hyper)graph neural networks, which often focus only on neighbors with low distances due to layer depth.  In general neural differential equations, the discretization method corresponds to a multi-step model [4]. Therefore, the model we construct based on ODEs allows for interactions across extended distances. This capability enables vertices to interact with long-distance neighbors, thereby broadening the interaction scope from merely local to encompassing global distances.\n\n\n\n**Fine-Tuning Through Control Term**. A distinctive distinction of our model lies in the inclusion of a control term, designed for fine-tuning representations during the diffusion process. The control term is an auxiliary function, allowing for adjustments to the diffusion process, to suit the specific dataset. This added layer of adaptability not only enhances the model's performance but also provides flexibility and precision in hypergraph representation learning.\n\n\n\nOur model integrates ODEs within the hypergraph framework. This integration results in a more nuanced representation of hidden layers, providing a smoother transition between them. This contrasts with general (hyper)graph neural networks, where features can exhibit more substantial variations between layers due to fewer intermediary layers. Second, the continuous diffusion results become stable as the number of layers in the model increases. It shows that, given sufficient depth, the model can reach an equilibrium state in system dynamics, providing consistent and reliable output."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214322844,
                "cdate": 1700214322844,
                "tmdate": 1700214322844,
                "mdate": 1700214322844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BGHDn9wayA",
                "forum": "NLbRvr840Q",
                "replyto": "Xq8wUrrJtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBiR (Part 3/3)"
                    },
                    "comment": {
                        "value": "**Response to Question 2**\n\nWe add an ablation study on whether the model uses hypergraph operators or graph operators in different hypergraph datasets. The ablation study include four methods, namely Graph Convolutional Network (GCN), Hypergraph Neural Networks (HGNN), HDS$^{ode}$ using graph operators, and HDS$^{ode}$. In the following table, \"OOM\" and \"HDS$^{ode}$(graph op.)\" represent \"out of memory\" and \"HDS$^{ode}$ using graph operators\", respectively.\n\n$\\newline$\n\n\n| Model                  | Cora-CA             | DBLP-CA             | News20              | IMDB4k-CA           | IMDB4k-CD           | DBLP4k-CC           | DBLP4k-CP           |\n| ---------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- | ------------------- |\n| GCN                    | $65.99_{\\pm3.69}$   | $82.22_{\\pm1.05}$   | $67.57_{\\pm0.70}$   | $43.47_{\\pm2.39}$   | $41.02_{\\pm2.22}$   | $90.18_{\\pm1.22}$   | $64.47_{\\pm0.90}$   |\n| HGNN                   | $67.58_{\\pm1.83}$   | $82.83_{\\pm1.09}$   | $76.58_{\\pm0.94}$   | $43.21_{\\pm2.39}$   | $41.08_{\\pm2.43}$   | $93.46_{\\pm0.77}$   | $67.99_{\\pm2.12}$   |\n| HDS$^{ode}$(graph op.) | $67.48_{\\pm3.04}$   | $82.78_{\\pm1.93}$   | OOM                 | $43.81_{\\pm3.16}$   | $41.53_{\\pm2.18}$   | $90.67_{\\pm 0.83}$  | $66.75_{\\pm0.79}$   |\n| HDS$^{ode}$            | ${68.92_{\\pm1.28}}$ | ${83.05_{\\pm0.53}}$ | ${76.75_{\\pm1.07}}$ | ${44.26_{\\pm2.11}}$ | ${42.30_{\\pm2.92}}$ | ${93.85_{\\pm0.50}}$ | ${69.52_{\\pm1.19}}$ |\n\n$\\newline$\n\nAccording to the results in the above table, we found that in the hypergraph dataset, the hypergraph method achieves better results than the corresponding graph method (i.e., HGNN beats GCN, and HDS$^{ode}$ beats HDS$^{ode}$(graph op.), respectively), which indicates that it is necessary to use hypergraph operators in hypergraph correlation structures. \n\n\n\nAs a generalization of graphs, hypergraphs have richer correlation expression capabilities than graphs. Such as in film background, hypergraphs adeptly capture the multifaceted connections among actors. Each actor can be represented as a vertex, and a hyperedge encompasses various correlations, such as actors co-starring in the same movie or originating from the same country. This hypergraph approach allows for a comprehensive representation of connections beyond pairwise interactions. Traditional graph models fall short since edges link only two nodes. They struggle to directly represent these group-based relationships without becoming complex or losing clarity.\n\n\n\nIn the graph operator, each vertex interacts with its adjacent vertices based on the edge connecting two vertices, while the hypergraph operator generates pair-wise interaction or group interaction based on the hyperedge that can connect two or more vertices. This is a key advantage of hypergraph operators. \n\n$\\newline$\n\nThank you once again for your valuable time and consideration. We hope our response addresses your concerns. We also welcome any new questions you may have.\n\n$\\newline$\n\n[1] Qimai Li, Zhichao Han, Xiao-Ming Wu. \"Deeper insights into graph convolutional networks for semi-supervised learning.\" *Proceedings of the AAAI conference on artificial intelligence*, 2018.\n\n[2] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, Xu Sun. \"Measuring and relieving the over-smoothing problem for graph neural networks from the topological view.\" *Proceedings of the AAAI conference on artificial intelligence*, 2020.\n\n[3] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David K. Duvenaud. \"Neural ordinary differential equations.\" *Advances in neural information processing systems*, 2018.\n\n[4] Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong. \"Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations.\" *International Conference on Machine Learning*, 2018."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214397735,
                "cdate": 1700214397735,
                "tmdate": 1700214397735,
                "mdate": 1700214397735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m1V3qhhZjw",
                "forum": "NLbRvr840Q",
                "replyto": "Xq8wUrrJtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBiR"
                    },
                    "comment": {
                        "value": "We understand your concerns regarding the performance improvement in our experiments, and we add an additional ablation study that we have conducted to further validate our model's performance. In difficult scenarios, the hypergraph structure can better mine the complex relationships behind the data and better solve problems.\n\n\n\n**Enhanced Performance in Few-Shot Scenarios**. Under conditions with limited effective supervisory information, such information cannot be sufficiently propagated through the network, resulting in limitations of general hypergraph neural networks. However, our ODE-based model, through the use of more layers, allows for the more extensive propagation of this limited supervisory information. This more comprehensive message passing enables our model to overcome the performance bottlenecks with the propagation capabilities of general hypergraph neural networks (with average 0.95 and 1.90 accuracy enhancement in 5 and 1 training vertex each class, respectively). In our experiments, we have explored scenarios with significantly fewer training samples each class, specifically, datasets with only 5 and even 1 training vertex each class. These few-shot conditions present a more challenging environment for learning accurate vertex representations, as the available information is considerably limited. The results of these experiments are presented in the following table.\n\n\n\n**Advantages of Hypergraph Methods over Graph Methods**. In our analysis, we also observed that hypergraph methods exhibit a more significant improvement over traditional graph methods under conditions of limited effective supervisory information. This advantage stems from the inherent strengths of hypergraph modeling compared to graph modeling. Hypergraphs **transcend the pairwise modeling** capabilities of graphs, enabling a more nuanced and comprehensive representation of complex relationships within the data. The **multi-modal property** of hypergraphs allows for the integration of diverse types of interactions and dependencies, which is particularly beneficial in scenarios where conventional graph models might struggle due to their limited expressive power. These strengths of hypergraph modeling become even more pronounced in scenarios where supervisory information is sparse.\n\n\n\n**Utilizing Broader Interaction in Hypergraphs**. Our model shows a more obvious advantage over general hypergraph neural network methods. This improvement becomes clearer in these few-shot scenarios. The reason for this can be attributed to the fact that with fewer training vertices, the need to capture information beyond 1-2 hops becomes more critical since fewer labels within the neighbor range with low distance. Our model's ability to leverage a broader scope of interaction allows it to access and utilize more comprehensive information, which is especially beneficial in situations where known vertex information is scarce.\n\n\n\nOur model shows greater potential for hypergraph inference tasks with limited known vertex information, indicating its capacity to achieve superior performance in more challenging and realistic scenarios. Our model's broader interaction scope is instrumental in capturing more complex and global relationships within the data, thereby enhancing performance.\n\n\n\nIn addition to the previously mentioned experiments, we show the performance of our model in diverse scenarios. Specifically, we have included tests on datasets with unique characteristics, without initial vertex features (Cooking Dataset), and without an initial hypergraph structure (NTU Dataset [1]).  Our experiments on these datasets demonstrate that our model outperforms standard hypergraph neural network approaches even in scenarios lacking initial vertex features or hypergraph structures. These results further validate the capability of our model to adapt in various complex scenarios."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678000561,
                "cdate": 1700678000561,
                "tmdate": 1700678000561,
                "mdate": 1700678000561,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "55z4SmFvNK",
                "forum": "NLbRvr840Q",
                "replyto": "Xq8wUrrJtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBiR"
                    },
                    "comment": {
                        "value": "Results on 10 training vertex each class (reported in our paper).\n\n|    Model    |       Cora-CA       |       DBLP-CA       |       News20        |      IMDB4k-CA      |      IMDB4k-CD      |      DBLP4k-CC      |      DBLP4k-CP      |\n| :---------: | :-----------------: | :-----------------: | :-----------------: | :-----------------: | :-----------------: | :-----------------: | :-----------------: |\n|     GCN     |  $65.99_{\\pm3.69}$  |  $82.22_{\\pm1.05}$  |  $67.57_{\\pm0.70}$  |  $43.47_{\\pm2.39}$  |  $41.02_{\\pm2.22}$  |  $90.18_{\\pm1.22}$  |  $64.47_{\\pm0.90}$  |\n|  GraphSAGE  |  $66.44_{\\pm2.82}$  |  $81.07_{\\pm1.50}$  |  $69.59_{\\pm0.89}$  |  $42.05_{\\pm1.95}$  |  $41.07_{\\pm2.11}$  |  $92.18_{\\pm0.38}$  |  $64.34_{\\pm1.58}$  |\n|    HGNN     |  $67.58_{\\pm1.83}$  |  $82.83_{\\pm1.09}$  |  $76.58_{\\pm0.94}$  |  $43.21_{\\pm2.39}$  |  $41.08_{\\pm2.43}$  |  $93.46_{\\pm0.77}$  |  $67.99_{\\pm2.12}$  |\n|  HGNN$^+$   |  $66.85_{\\pm2.24}$  |  $82.40_{\\pm1.27}$  |  $76.49_{\\pm1.30}$  |  $43.74_{\\pm1.42}$  |  $41.49_{\\pm2.54}$  |  $93.46_{\\pm1.09}$  |  $68.76_{\\pm2.73}$  |\n|   UniGCN    |  $66.47_{\\pm2.04}$  |  $82.36_{\\pm1.09}$  |  $76.56_{\\pm1.21}$  |  $43.34_{\\pm3.26}$  |  $41.33_{\\pm2.50}$  |  $93.28_{\\pm0.87}$  |  $67.68_{\\pm1.90}$  |\n|   UniSAGE   |  $68.59_{\\pm1.61}$  |  $82.16_{\\pm1.25}$  |  $75.52_{\\pm1.22}$  |  $42.82_{\\pm2.66}$  |  $41.62_{\\pm3.05}$  |  $93.64_{\\pm0.58}$  |  $67.81_{\\pm2.12}$  |\n| HDS$^{ode}$ | ${68.92_{\\pm1.28}}$ | ${83.05_{\\pm0.53}}$ | ${76.75_{\\pm1.07}}$ | ${44.26_{\\pm2.11}}$ | ${42.30_{\\pm2.92}}$ | ${93.85_{\\pm0.50}}$ | ${69.52_{\\pm1.19}}$ |\n\n\n\nResults on 5 training vertex each class.\n\n|    Model    |           Cora-CA |           DBLP-CA |            News20 | IMDB4k-CA         | IMDB4k-CD         | DBLP4k-CC         | DBLP4k-CP         | Cooking           | NTU               |\n| :---------: | ----------------: | ----------------: | ----------------: | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- | ----------------- |\n|     GCN     | $54.61_{\\pm4.64}$ | $73.65_{\\pm6.84}$ | $62.81_{\\pm2.65}$ | $39.14_{\\pm2.97}$ | $38.54_{\\pm2.44}$ | $89.46_{\\pm1.17}$ | $56.96_{\\pm5.85}$ | $32.87_{\\pm3.99}$ | $79.64_{\\pm1.85}$ |\n|  GraphSAGE  | $53.11_{\\pm3.74}$ | $73.58_{\\pm5.40}$ | $62.66_{\\pm3.73}$ | $39.72_{\\pm1.16}$ | $38.51_{\\pm1.34}$ | $91.08_{\\pm1.03}$ | $55.95_{\\pm5.33}$ | $32.73_{\\pm5.11}$ | $76.78_{\\pm1.13}$ |\n|    HGNN     | $60.52_{\\pm4.12}$ | $75.55_{\\pm4.27}$ | $75.05_{\\pm1.57}$ | $40.63_{\\pm1.76}$ | $39.42_{\\pm2.58}$ | $93.35_{\\pm0.77}$ | $58.07_{\\pm6.02}$ | $45.35_{\\pm6.71}$ | $85.35_{\\pm1.62}$ |\n|  HGNN$^+$   | $58.17_{\\pm5.10}$ | $77.34_{\\pm2.49}$ | $75.43_{\\pm1.81}$ | $40.92_{\\pm2.92}$ | $39.41_{\\pm2.58}$ | $93.03_{\\pm1.46}$ | $60.83_{\\pm4.70}$ | $47.87_{\\pm4.21}$ | $85.27_{\\pm0.92}$ |\n|   UniGCN    | $58.36_{\\pm5.11}$ | $74.54_{\\pm4.88}$ | $74.98_{\\pm1.70}$ | $41.51_{\\pm2.56}$ | $39.94_{\\pm2.31}$ | $93.04_{\\pm1.28}$ | $59.91_{\\pm4.92}$ | $45.98_{\\pm7.12}$ | $84.10_{\\pm0.86}$ |\n|   UniSAGE   | $62.56_{\\pm3.27}$ | $77.94_{\\pm2.39}$ | $73.90_{\\pm2.85}$ | $41.67_{\\pm2.50}$ | $39.56_{\\pm2.85}$ | $93.28_{\\pm0.42}$ | $59.74_{\\pm3.22}$ | $47.26_{\\pm5.19}$ | $84.26_{\\pm1.80}$ |\n| HDS$^{ode}$ | $64.73_{\\pm2.27}$ | $78.89_{\\pm2.95}$ | $75.42_{\\pm2.05}$ | $42.19_{\\pm1.90}$ | $41.10_{\\pm2.88}$ | $93.41_{\\pm0.85}$ | $61.62_{\\pm1.83}$ | $49.37_{\\pm3.38}$ | $86.64_{\\pm1.28}$ |"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678069815,
                "cdate": 1700678069815,
                "tmdate": 1700678069815,
                "mdate": 1700678069815,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cEuKfQH6N1",
                "forum": "NLbRvr840Q",
                "replyto": "Xq8wUrrJtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBiR"
                    },
                    "comment": {
                        "value": "Results on 1 training vertex each class.\n\n|    Model    |           Cora-CA | DBLP-CA            | News20             | IMDB4k-CA         | IMDB4k-CD         | DBLP4k-CC         | DBLP4k-CP         | Cooking            | NTU               |\n| :---------: | ----------------: | ------------------ | ------------------ | ----------------- | ----------------- | ----------------- | ----------------- | ------------------ | ----------------- |\n|     GCN     | $25.14_{\\pm7.72}$ | $46.24_{\\pm6.78}$  | $40.85_{\\pm11.39}$ | $36.65_{\\pm2.11}$ | $36.36_{\\pm1.73}$ | $83.46_{\\pm7.62}$ | $36.65_{\\pm9.91}$ | $25.90_{\\pm6.46}$  | $61.48_{\\pm4.07}$ |\n|  GraphSAGE  | $25.21_{\\pm3.91}$ | $46.92_{\\pm10.31}$ | $42.44_{\\pm12.20}$ | $36.23_{\\pm0.83}$ | $36.10_{\\pm1.17}$ | $77.55_{\\pm8.27}$ | $37.13_{\\pm8.01}$ | $26.68_{\\pm5.91}$  | $60.83_{\\pm6.53}$ |\n|    HGNN     | $27.35_{\\pm2.53}$ | $47.88_{\\pm6.49}$  | $46.63_{\\pm10.01}$ | $37.28_{\\pm1.97}$ | $37.12_{\\pm1.73}$ | $93.32_{\\pm0.85}$ | $42.15_{\\pm9.86}$ | $28.83_{\\pm7.05 }$ | $67.38_{\\pm2.07}$ |\n|  HGNN$^+$   | $34.98_{\\pm3.93}$ | $52.73_{\\pm3.96}$  | $49.50_{\\pm11.17}$ | $34.70_{\\pm1.84}$ | $37.31_{\\pm2.33}$ | $93.37_{\\pm0.35}$ | $41.92_{\\pm8.02}$ | $32.94_{\\pm5.37}$  | $67.34_{\\pm2.52}$ |\n|   UniGCN    | $27.94_{\\pm1.31}$ | $50.91_{\\pm6.86}$  | $47.08_{\\pm9.65}$  | $37.02_{\\pm1.70}$ | $36.83_{\\pm1.47}$ | $92.73_{\\pm1.15}$ | $39.83_{\\pm8.70}$ | $28.25_{\\pm7.16}$  | $66.60_{\\pm3.37}$ |\n|   UniSAGE   | $38.80_{\\pm2.04}$ | $53.33_{\\pm3.14}$  | $49.23_{\\pm9.24}$  | $37.05_{\\pm1.68}$ | $36.93_{\\pm1.53}$ | $92.64_{\\pm2.54}$ | $42.39_{\\pm7.13}$ | $35.26_{\\pm5.00}$  | $67.96_{\\pm2.96}$ |\n| HDS$^{ode}$ | $39.58_{\\pm3.59}$ | $60.22_{\\pm2.17}$  | $52.70_{\\pm9.58}$  | $37.12_{\\pm2.20}$ | $37.38_{\\pm1.29}$ | $93.43_{\\pm1.56}$ | $43.46_{\\pm7.51}$ | $36.15_{\\pm4.18}$  | $71.36_{\\pm1.36}$ |\n\n\n\n**Cooking and NTU Dataset Introduction**. We briefly introduce the two datasets. The Cooking dataset consists of vertices representing dishes, with hyperedges indicating dishes that use the same ingredients. Each dish is also associated with categorical information indicating its cuisine type\uff0c such as French, Japanese. This dataset poses a unique challenge as it lacks initial vertex features, testing our model's ability to infer relationships solely based on hypergraph structure. The NTU dataset includes 3D shapes categorized into various classes like chairs, doors, etc. The vertex features are extracted using Multi-View Convolutional Neural Networks (MVCNN) [2] and Group-View Convolutional Neural Networks (GVCNN) [3] for 3D shapes. Since the NTU dataset does not come with an initial hypergraph structure, we constructed it by treating each 3D shape as a vertex and using a k-nearest neighbors method to build hyperedges based on MVCNN features and GVCNN features, respectively, thereby establishing the hypergraph structure. The following table shows the statistics of the Cooking dataset and NTU dataset. Since the Cooking dataset has no initial features, we initialize a unique feature for each vertex so that its degree is exactly the number of vertices.\n\n\n\n| Dataset | #Vertices | #Hyperedges | #Feature |\n| ------- | --------- | ----------- | -------- |\n| Cooking | 7,403     | 2,755       | 7,403    |\n| NTU     | 2,012     | 3,365       | 6,144    |"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678129969,
                "cdate": 1700678129969,
                "tmdate": 1700678129969,
                "mdate": 1700678129969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YxaR8UcPjt",
                "forum": "NLbRvr840Q",
                "replyto": "Xq8wUrrJtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HBiR"
                    },
                    "comment": {
                        "value": "**Improvements in Cooking and NTU**. In our experimental evaluation, we observe significant performance improvements in both the Cooking and NTU datasets when using our model. Specifically, when training with only 5 samples per class, our model achieves an enhancement of 3.13% on the Cooking dataset and 1.51% on the NTU dataset. With only 1 sample per class, the performance gains are even more pronounced, 2.52% on the Cooking dataset and a 5.00% on the NTU dataset. This indicates a notable enhancement in our model\u2019s ability to capture and propagate limited supervisory information effectively across the hypergraph structure, even in scenarios with minimal training data. Moreover, in a more challenging few-shot scenario, these results highlight our model's proficiency in leveraging the overall hypergraph structure to extract and utilize latent relational information, especially when dealing with limited known vertex labels. \n\n\n\nIn the case of the NTU dataset, where we construct the hypergraph structure from 3D shape features using MVCNN and GVCNN, the performance improvement emphasizes our model's capacity to exploit complex relational patterns from visually extracted features. Overall, these results not only validate the effectiveness of our dynamical system-based hypergraph neural network in diverse settings but also illustrate its potential to address challenges in hypergraph learning tasks where general hypergraph models may be limited.\n\n\n\nWe hope this additional empirical evidence addresses your concerns and demonstrates the robustness and performance capabilities of our model in a variety of conditions.\n\n\n\n[1] Chen, Ding\u2010Yun, et al. \"On visual similarity based 3D model retrieval.\" *Computer graphics forum*, 2003.\n\n[2] Su, Hang, et al. \"Multi-view convolutional neural networks for 3d shape recognition.\" *Proceedings of the IEEE international conference on computer vision*, 2015.\n\n[3] Feng, Yifan, et al. \"Gvcnn: Group-view convolutional neural networks for 3d shape recognition.\" *Proceedings of the IEEE conference on computer vision and pattern recognition*, 2018."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678193239,
                "cdate": 1700678193239,
                "tmdate": 1700678193239,
                "mdate": 1700678193239,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TjcXEabSmc",
                "forum": "NLbRvr840Q",
                "replyto": "Xq8wUrrJtV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Inquiry and Gratitude Before Discussion Phase Conclusion"
                    },
                    "comment": {
                        "value": "Dear Reviewer HBiR,\n\n\nWe would like to express our gratitude for the time and effort you have dedicated to reviewing our paper. As the discussion phase is nearing its conclusion, we are eager to know if our responses have adequately addressed your concerns. If so, whether you could consider changing your rating.\n\n\nPlease rest assured that we are completely dedicated to addressing any further questions or clarifications you may have before the end of the discussion. Your insights are invaluable to us, and we aim to incorporate them to enhance the quality of our work.\n\n\nThank you again for your valuable contribution to the review process. We are looking forward to hearing from you soon.\n\n\nBest,\n\n\nThe Authors"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724706656,
                "cdate": 1700724706656,
                "tmdate": 1700724706656,
                "mdate": 1700724706656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dgryVXsAkv",
                "forum": "NLbRvr840Q",
                "replyto": "TjcXEabSmc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_HBiR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_HBiR"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for addressing my concerns and conducting extra experiments. Nonetheless, the concept transforming graph/hypergraph's propagation process into continuous form is not a novel idea in this field. And I maintain my initial concern that the dynamical characteristics of the model could be validated only with dynamic, rather than statistic datasets. \nDespite this concern, I appreciate the authors' efforts in providing a more comprehensive stability analysis, as well as the experimental results. Therefore I will update my score to 5."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728182116,
                "cdate": 1700728182116,
                "tmdate": 1700728182116,
                "mdate": 1700728182116,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Val6hT4y6H",
            "forum": "NLbRvr840Q",
            "replyto": "NLbRvr840Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3379/Reviewer_KkAS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3379/Reviewer_KkAS"
            ],
            "content": {
                "summary": {
                    "value": "This paper theoretically introduces hypergraph dynamical systems based on a control-diffusion\nODE, which bridge hypergraphs and dynamical systems. It then proposes a neural implementation $HDS^{ode}$ and presents stability analysis of it and the connection to hypergraph neural networks. Finally, the paper empirically evaluates $HDS^{ode}$ using benchmark datasets and show its effectiveness to some extent. Some ablation studies are also included for more thorough investigation of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tGiven the graph counterpart, it is a natural and interesting idea to develop a hypergraph neural ODE to improve the controllability and stabilization of information diffusion on hypergraphs. On the other hand, given that graphs and hypergraphs are different in nature, it is also a challenging problem how the system should be designed. \n2.\tThe paper is well-written and easy to follow.\n3.\tThe paper is very complete, with clear presentation of the method, some theoretical analysis and thorough empirical evaluation."
                },
                "weaknesses": {
                    "value": "The main weakness of the paper is the weak empirical results supporting the effectiveness of the proposed method. It is unconvincing why small diffusion steps themselves constitute a real problem for hyper graph neural networks. Although $HDS^{ode}$\u2019s performance does not suffer from more layers, $HDS^{ode}$ has very marginal improvement in terms of optimal performance. This is evident from all the experimental results (Figure 1, Table 1 and Table 2), where the improvement over baseline methods is barely noticeable and almost never statistically significant."
                },
                "questions": {
                    "value": "\"Dynamic systems\" reads weird, and it is used kind of interchangeably with \"dynamical systems\" in the paper. Is there any specific reason to sometimes use \"dynamic systems\" instead of \"dynamical systems\" in the paper? If not, sticking with \"dynamical systems\" and \"hypergraph dynamical systems\" might be more appropriate. See discussion on Mathoverflow https://mathoverflow.net/questions/366856/why-is-a-dynamical-system-not-a-dynamic-system\" about why the right mathematical jargon is \"dynamical systems\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Reviewer_KkAS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698458231343,
            "cdate": 1698458231343,
            "tmdate": 1699636288728,
            "mdate": 1699636288728,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SzdItz4INt",
                "forum": "NLbRvr840Q",
                "replyto": "Val6hT4y6H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KkAS"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and feedback. We hope that our response can address your concerns.\n\n$\\newline$\n\n**Response to Weakness**\n\nThank you for your critical assessment of our empirical results. We acknowledge your concerns regarding the performance improvement demonstrated in our results. However, it is important to emphasize that the **primary focus** of our work is addressing a common and significant challenge in (hyper)graph neural networks (that is, the notable decrease in performance with an increasing number of layers) [1,2]. \n\n\n\n**We wish to highlight the dual aspects of performance** in our work: not only the performance metrics but also the stability of performance. In practical applications of hypergraph neural networks, the specific dependencies required for optimal performance of many existing methods may be challenging to meet, since they depend on specific conditions. Therefore, our emphasis on **robustness** and reduced dependence ensures reliable and consistent performance across various scenarios and layer depths. This is why the robustness becomes particularly valuable. Our model's ability to maintain stability and consistency at increased layer depths addresses a fundamental limitation. Our model not only tackles the stability issues with increased layers but also provides performance enhancements.\n\n\n\nThe performance improvement stems from three main factors. \n\n- **Continuous Representations**. The adoption of an ODE-based model in HDS$^{ode}$ allows for smoother dynamics in the hidden layers [3], promoting more effective learning and representation.\n- **Extended Interaction Range**. General hypergraph neural networks are often limited to 2 layers, restricting vertices to interact only with 1-hop and 2-hop neighbors. HDS$^{ode}$'s diffusion mechanism effectively extends this range, enabling interactions across the entire hypergraph [4]. This broader scope of interaction is instrumental in capturing more complex and global relationships within the data, thereby improving performance.\n- **Fine-Tuning Through Control Term**. A distinctive part of our model is the inclusion of a control term, which allows for the fine-tuning of representations during diffusion. This aspect is not commonly found in other hypergraph neural network models. The control term acts as an auxiliary function, fine-tuning the diffusion process to better align with the specific dataset. This capability adds an additional layer of adaptability to our model, further enhancing its performance.\n\n$\\newline$\n\n**Response to Question**\n\nWe appreciate your insightful comment regarding the terminology of \"dynamic system\" versus \"dynamical system\". After reflecting on your feedback, we recognize the importance of consistent and precise terminology in our field. Given that our model represents a new form of hypergraph neural network with a focus on the study of system dynamics, we agree that \"dynamical system\" is the more appropriate and established term in mathematical terminology. We are in the process of revising our manuscript to consistently use \"dynamical system\" throughout, ensuring alignment with standard terminology and enhancing the clarity of our work.\n\n$\\newline$\n\nThank you once again for your valuable time and consideration. We hope our response addresses your concerns.\n\n$\\newline$\n\n[1] Qimai Li, Zhichao Han, Xiao-Ming Wu. \"Deeper insights into graph convolutional networks for semi-supervised learning.\" *Proceedings of the AAAI conference on artificial intelligence*, 2018.\n\n[2] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, Xu Sun. \"Measuring and relieving the over-smoothing problem for graph neural networks from the topological view.\" *Proceedings of the AAAI conference on artificial intelligence*, 2020.\n\n[3] Xhonneux, Louis-Pascal, Meng Qu, and Jian Tang. \"Continuous graph neural networks.\" *International Conference on Machine Learning*, 2020.\n\n[4] Johannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. \"Diffusion improves graph learning.\" *Advances in neural information processing systems*, 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213666512,
                "cdate": 1700213666512,
                "tmdate": 1700213666512,
                "mdate": 1700213666512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fHnrToYzuQ",
                "forum": "NLbRvr840Q",
                "replyto": "Val6hT4y6H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_KkAS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_KkAS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the response. However, I am not convinced by the argument that since robustness is the main focus, the weak improvement can be well justified. In particular, to justify the claim such as \"this broader scope of interaction is instrumental in capturing more complex and global relationships within the data, thereby improving performance\", one would expect that the proposed method is not only robust, but also improves the performance significantly, because of the capability to capture additional information.\n\nThe current empirical results just imply that more information beyond 1-2 hops has very limited value, at least in the datasets and tasks demonstrated in this paper. If that is the case, previous methods, despite being shallow, suffice."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603773216,
                "cdate": 1700603773216,
                "tmdate": 1700603791989,
                "mdate": 1700603791989,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f3EEnfIYfR",
                "forum": "NLbRvr840Q",
                "replyto": "Val6hT4y6H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Inquiry and Gratitude Before Discussion Phase Conclusion"
                    },
                    "comment": {
                        "value": "Dear Reviewer KkAS,\n\n\nWe would like to express our gratitude for the time and effort you have dedicated to reviewing our paper. As the discussion phase is nearing its conclusion, we are eager to know if our responses have adequately addressed your concerns. If so, whether you could consider changing your rating.\n\n\nPlease rest assured that we are completely dedicated to addressing any further questions or clarifications you may have before the end of the discussion. Your insights are invaluable to us, and we aim to incorporate them to enhance the quality of our work.\n\n\nThank you again for your valuable contribution to the review process. We are looking forward to hearing from you soon.\n\n\nBest,\n\n\nThe Authors"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724421757,
                "cdate": 1700724421757,
                "tmdate": 1700724421757,
                "mdate": 1700724421757,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8DtlAEWlsL",
            "forum": "NLbRvr840Q",
            "replyto": "NLbRvr840Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3379/Reviewer_4j9N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3379/Reviewer_4j9N"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors target on the task of representation learning with high-order correlations on hypergraph, in which the challenge is the sub-optimal problem during the process the neural network. Existing hypergraph neural networks cannot be deeper than 2 layers and is unstable. This problem is a common but challenged issue in this field. The authors introduce the framework of hypergraph dynamic systems, which connects hypergraph learning and dynamic systems to achieve continuous dynamics of representation using high-order correlations. The authors further propose an implementation of hypergraph dynamic systems based on ordinary differential equation and experiments have shown stable and satisfied performance. This control-diffusion process introduced in this paper have been demonstrated effectiveness through the results and theoretical discussions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper targets on an important but challenged task in representation learning, i.e., how to achieve stable representation learning in hypergraph neural network, which is also a common issue in the general graph neural networks. Usually, HGNNs cannot be more than 2 layers, which leads to performance degradation significantly. The introduced hypergraph dynamic systems framework in this paper bridges hypergraph learning and dynamic systems, which can take the advantages of hypergraph on high-order correlation modeling and dynamic systems on controllable diffusion process. The idea of hypergraph dynamic systems is novel. It is a good attempt towards better representation learning and could be helpful to a broad field.\n\nThe authors also propose an implementation of HDS using ODE, and a multi-layer HDS-ode is given. The stability analysis has also been detailed analyzed. The difference between HDS-ode and traditional HGNNs has discussed.\n\nExperiments are sufficient. Experiments on semi-supervised vertex classification with two different settings have been conducted on 7 datasets. Experimental results have clearly shown the superior performance of HDS-ode compared with recent state-of-the-art GNN/HGNN methods. From the results, we can observe the control-diffusion process of HDS-ode is stable, which solves the limitations of existing HGNNs, i.e. only 1 or 2 layers can be used.\n\nIn general, this paper is well organized and writing. The related works are sufficient and the motivation is clear. The method has been detailed introduced. This paper brings in a new aspect of representation learning of taking both high-order correlation modeling and dynamic systems into consideration simultaneously, which has the potential to have broad impact."
                },
                "weaknesses": {
                    "value": "For the framework figure (Fig. 2), a more detailed and clearer introduction should be helpful.\n\nThere are a few typos. Please find and correct them."
                },
                "questions": {
                    "value": "1. For the framework figure (Fig. 2), a more detailed and clearer introduction should be helpful.\n2. As shown in Fig. 1, the performance of HDS-ode increases during the first 8 layers and becomes stable then. Can the authors further explain how to control the diffusion speed during this procedure?\n3. There are a few typos. Please find and correct them."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Reviewer_4j9N"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630313128,
            "cdate": 1698630313128,
            "tmdate": 1699636288650,
            "mdate": 1699636288650,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d5A4poq8BZ",
                "forum": "NLbRvr840Q",
                "replyto": "8DtlAEWlsL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4j9N"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your valuable comments and suggestions. We are currently revising the manuscript to provide a more detailed and comprehensible description of the framework. Additionally, we are reviewing the paper to identify and correct all typos to enhance the overall clarity and readability of our work. \n\n$\\newline$\n\n**Response to Question 2**\n\nThank you for your question regarding the control of diffusion speed. Each layer in HDS$^{ode}$ includes a diffusion term and a control term. The hyperparameters $\\alpha_v,\\alpha_e$ in the diffusion matrix $\\begin{bmatrix}\n            -\\alpha_v I& \\alpha_v D_v^{-1}H \\newline\n            \\alpha_e D_e^{-1}H^\\top &-\\alpha_e I\n        \\end{bmatrix}$ can be considered as the speed in the diffusion process. Taking vertex representations for explanation, $\\alpha_v$ represents the proportion of vertex representations in each layer obtained from hyperedges in the diffusion process. \n\n\n\nExpanding the vertex representation of Eq. (7), we get $X_v(t+1)=(1-\\alpha_v)X_v(t+\\frac{1}{2})+\\alpha_v D_v^{-1}HX_e$. Specifically,  $\\alpha_v$ dictates the proportion of vertex representations derived from hyperedges during diffusion. For instance, with a lower $\\alpha_v$ value, close to 0, the diffusion speed is minimized, and the vertex representation $X_v(t+1)$ remains closer to its previous state $X_v(t+\\frac{1}{2})$. Conversely, as $\\alpha_v$ approaches 1, $X_v(t+1)$ primarily consists of hyperedge representations $D_v^{-1}HX_e$.\n\n\n\nIn our experimental setup, $\\alpha_v,\\alpha_e$  are kept constant, implying a consistent diffusion speed across layers. When the number of layers increases (for example, reaching 8 layers), the representation of vertices and hyperedges is stable after diffusion, and the performance no longer increases significantly. \n\n$\\newline$\n\nThanks again for your time and consideration. We hope our response addresses your concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213451169,
                "cdate": 1700213451169,
                "tmdate": 1700213451169,
                "mdate": 1700213451169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sx7LOBJ0SW",
            "forum": "NLbRvr840Q",
            "replyto": "NLbRvr840Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces hypergraph dynamic systems (HDS) to characerize the continuous dynamics of representations, and then proposes a control-diffusion HDS by an ODE. A multi-layer HDS-ODE is designed as a neural implementation, having the properties of controllability and stabilization, and   can capture long-range correlations among vertices. The paper performs evaluation experiments on 7 datsets to show its dominant performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The pape present the implementation of HDS-ODE framework by posing the neural implementation of the control step and diffusion step in sequence  via Lie-Trotter splitting method.\n2. The time complexity and relation to HGNN+ are analyzed. And the experiments show its very good performance.\n3. The properties of HDS-ODE are discussed and we appreciate such an effort on analysis in theory, although we do have concerns on these contents (please see comments below)."
                },
                "weaknesses": {
                    "value": "1. To the terminology (and the preliminary math tools) of dynamical systems used in this paper, it seems that the authors does not soundly cook its article based on the strict math that has been widely accepted in math and engineering. See Queation 1, Weakness point 2 and 3.\n\n2. The discussion of statibility seems wrong. That's why we say the authors may not well pick up knowledge of (linear/nonlinear) dynamical systems. Sec. 5.1 discussed the stability of HDS-ODE, where the statement of the first sentence in this section is basically wrong. The so-called \"control\" term also iterate over time. It cannot be simplified the stability analysis of HDS-ODE as simply a dissusion of linear system X_dot = A X. Let us use the eq.(3) to clarify the point simply. Supposing we accept the split of the general state-space equation X_dot = f(X, t) as eq.(3), it is obviously that the stability analysis is a general stability analysis of nonlinear system, besides AX(t) there also exists g(X(t))! If you said your discussion of HDS-ODE refer to the neural implementation, we can see that in eq. (6) the nonlinear term is still there, except it is in NN form, and then embeds into eq.(7) to complete the iteration t+1. Furthermore, these propositions on stability for your \"simplified\" linear systems are well-known in the field of electrical engineering.\n\n3. Your abstract and contribution summary tell that yours studies the \"controllability\", which we could find anything related to it. And regarding the starting point of the most general eq.(2), there cannot be any controllablity-related problem can be formulated. Maybe you refer to something different? We strongly recommend the authors to learn essentials of dynamical systems, it helps to avoid conceptual misunderstandings and misuse for better communications.\n\n   To help you with essential knowledge on dynamical systems, you may refer to the following classic textbooks (basics on linear, nonlinear systems):\n\n   - Zhou, K., Doyle, J. C., & Glover, K. (1995). Robust and Optimal Control. Pearson.\n   - H. K. Khalil, \u201cNonlinear Systems,\u201d 3rd Edition, Prentice Hall, Upper Saddle River, 2002.\n\nYour idea may be valuable and appreciated, considering your sound performance in experiments. However, you really have to first carefully deal with theory and fix any possible mistakes."
                },
                "questions": {
                    "value": "1. Why do you call these two terms in eq.(3) as the \"control\" term and the \"diffusion\" term. We are not familiar with the terminology in the \"small\" field (that is consisted of these ~5 papers in introduction). However, in the mature fields of \"dynamical systems\" in math, \"stochastic analysis\" in probability or mathematical finance  and \"control theory/engineering\" or \"cybernetics\" in engineering, neither the name of \"control\" nor \"diffusion\" may be  properly defined. The control term usually refers to external signals or extrogeneous variables (in econometrics) that can be designed or modified. Indeed, if you assume, eq.(3) is not an autonomous system in nature (if purely looking at (3) itself, it is), but the feedback system by using the nonlinear state feedback law g(X) as the control \"u\" for the linear dynamical system X_dot = A X + u. It somehow legitimate the name \"control\". However, it is still not recommended in this way since it is confusing. Referring to the diffusion term, we cannot see why it can be call as this name, since the diffusion term is h(X)dW in the SDE dX(t) = f(X, t) dt + h(X,t) dW, where W is the Brownian motion. The AX(t) term in eq.(3) is actually the term of f(X,t), so the drift term. \n2. See Weakness point 2 on Sec.5.1. Please explain, in particular, the first sentence of Sec.5.1, which seems not correct."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS",
                        "ICLR.cc/2024/Conference/Submission3379/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699156009488,
            "cdate": 1699156009488,
            "tmdate": 1700725774179,
            "mdate": 1700725774179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v4Bx0UaBkH",
                "forum": "NLbRvr840Q",
                "replyto": "sx7LOBJ0SW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3YqS (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and feedback. \n\nWe apologize for the lack of clarity in our manuscript and appreciate the opportunity to elucidate our approach. Our method references some methods from dynamical systems to address a common and significant challenge of a decrease in performance with an increasing number of layers in hypergraph neural networks. **It's important to clarify that our focus is on enhancing hypergraph neural networks by dynamical systems concepts, rather than developing a dynamical system.** Considering your valuable feedback, we believe that renaming our model to \"Hypergraph Dynamic Neural Network\" would more accurately represent the essence of our work. Our proposed model is an enhancement of general hypergraph neural networks, designed to be a next-generation framework within this domain. Our model significantly enhances the stability of existing hypergraph neural networks. This advanced framework effectively combines the correlation hypergraph structure properties with the dynamic aspects of neural networks, offering a more comprehensive model than general hypergraph neural networks.\n\n\n\n**Response to Weakness 2, Question 2**\n\nThank you for your feedback on the stability discussion in our paper. \n\n\n\nIn our analysis, we primarily focus on the stability of the diffusion component, which is the main element of our model and a critical aspect of hypergraph neural networks in general. This is due to the diffusion component's significant role in determining the overall effectiveness of the hypergraph model. To clarify, the first sentence in Section 5.1 is intended to convey that diffusion in our model represents the teleportation of vertex and hyperedge representations over time. As this diffusion process is a main component of our ODE-based approach, its stability analysis is crucial. \n\n\n\nThe control term in our model is designed to act as a secondary, fine-tuning term. Its primary function is to add minor perturbations to the main diffusion process, allowing the model to adapt more effectively to different data. The diffusion component dictates the primary dynamics of the network, the control term provides the minor necessary adjustments to enhance performance and adaptability without fundamentally altering the core stability characteristics of the model. That's the reason why we mainly analyze the stability of diffusion.\n\n\n\nIf the parameters in the function $g$ are too large, they amplify $X$, thereby affecting the main diffusion component. By controlling the parameter size of $W$ and $b$, we ensure that the transformations they induce in the representation space are bounded. In our neural network implementation, we use L2 regularization to keep the values of $W$ and $b$ within a lower range. This helps control the size of these parameters, thereby preventing potential instability that large weights may cause. This approach is a standard practice in neural network training, suitable for our system architecture, and helps ensure the boundedness of the model's output, maintaining stability.\n\n\n\n**Response to Weakness 3**\n\nWe apologize for the confusion caused by our use of the term \"control\" in our paper. We realize that this term may carry different connotations in various fields.  We are in the process of revising our manuscript to enhance the clarity of our work. In our research, \"control\" refers to the process of fine-tuning or adjusting the diffusion in our hypergraph neural network model. This usage of the word is distinct from the concept of \"controllability\" in control theory.  \n\n\n\nIn our model, \"controllability\" refers to the capability to fine-tune and adjust the diffusion process. Diffusion or vertex-(hyper)edge-vertex representation teleportation process is the central aspect of most graph and hypergraph neural networks. Traditionally, these networks primarily focus on representing diffusion by vertex-(hyper)edge-vertex teleportation process without additional fine-tuning. Our model includes a learnable function $g$, which acts as an auxiliary element. The function is specifically designed to fine-tune and control the primary diffusion component. By incorporating this learnable control function, we enhance the model's ability to adapt more precisely to the unique features of the hypergraph data. The \"controllability\" refers to our model's enhanced capability to refine the diffusion process through function $g$, which marks a significant enhancement over general (hyper)graph neural networks."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212914328,
                "cdate": 1700212914328,
                "tmdate": 1700212914328,
                "mdate": 1700212914328,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jcrWPFRsI0",
                "forum": "NLbRvr840Q",
                "replyto": "sx7LOBJ0SW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3YqS (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Response to Question 1**\n\nThank you for your insightful comments regarding the terminology used in our paper. In the field of graph and hypergraph, particularly when discussing (hyper)graph neural networks, \"diffusion\" is commonly used to describe the process by which features or representations teleport across the vertices of the graph or across the vertices and hyperedges of the hypergraph. This usage aligns with the intuitive notion of diffusion as a teleport process. In our model, the term \"diffusion\" refers to the $AX$ term in Eq. 3, representing the spread of representations across the hypergraph's vertices and hyperedges with a similar definition as existing methods [1-5]. \n\n\n\nIn our model, the term \"control\" specifically refers to a fine-tuning step that complements the primary diffusion process. It acts as an auxiliary function, adjusting and controlling the diffusion $AX$ to align more precisely with the downstream goals. This control step is internal, and designed to refine the diffusion of representations across the hypergraph's vertices and hyperedges, enhancing the performance and accuracy of the diffusion. By integrating this fine-tuning control function, our model achieves a balance between diffusion of vertex and hyperedge representations and task-aware adjustments, leading to more reliable and accurate performance. \n\n\n\nWe hope our response addresses your concerns. We also welcome any new questions you may have.\n\n\n\n[1] Johannes Gasteiger, Stefan Wei\u00dfenberger, and Stephan G\u00fcnnemann. \"Diffusion improves graph learning.\" *Advances in neural information processing systems*, 2019.\n\n[2] Xhonneux, Louis-Pascal, Meng Qu, and Jian Tang. \"Continuous graph neural networks.\" *International Conference on Machine Learning*, 2020.\n\n[3] Yifei Wang, Yisen Wang, Jiansheng Yang, Zhouchen Lin. \"Dissecting the diffusion process in linear graph convolutional networks.\" *Advances in Neural Information Processing Systems*, 2021.\n\n[4] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, Emanuele Rossi. \"Grand: Graph neural diffusion.\" *International Conference on Machine Learning*, 2021.\n\n[5] Peihao Wang, Shenghao Yang, Yunyu Liu, Zhangyang Wang, Pan Li. \"Equivariant Hypergraph Diffusion Neural Operators.\" *International Conference on Learning Representations*, 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212984537,
                "cdate": 1700212984537,
                "tmdate": 1700212984537,
                "mdate": 1700212984537,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aNItQkFrFq",
                "forum": "NLbRvr840Q",
                "replyto": "sx7LOBJ0SW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the authors for their detailed explanations and responses. \n\n**What we have been convinced or we agree with**:\n\nWe got your whole story of your topic and what you really mean by terms like \"control term\", \"controllability\", etc. And indeed your work is not developing a dynamical model on hypergraphs, but enhancing a better hypergraph neural networks. Actually, what you have responded, as quoted below, soundly explained why we call it \"control\" term.  \n> The control term in our model is designed to act as a secondary, fine-tuning term. \n\nThis follows the naming convention by considering the control term as a feedback control law on state variable $x$. However, you are suggested to provide more description on the \"control\" term (e.g., that can be fine-tuned, or explicitly, as $u(t)$ first then $u(t) = g(X(t))$, etc.) to clarify confusions. For the \"controllability\", actually what you described (as quote below) is exactly covered/described by this term. \n> \"controllability\" refers to the capability to fine-tune and adjust the diffusion process. \n\nHowever, it has a strict definition, which is somehow simple in LTI systems while fairly complicated in nonlinear systems. If you like to use this term, you have to follow its definition and reason/analyze it strictly in math, which may not be an easy task. Otherwise, you may consider using an alternative word.\nAlthough people in CS on diffusion NN may not be quite strict with terminology from math (dynamical systems) or control theory, the early papers on diffusion model (NN) comply with naming convention and can be strictly explained in math. We strongly suggest the authors to be carefully deal with them to avoid any possible confusions.\n\n**The main issue that concerns the reviewer**:\n\nFor the stability, sorry, you still need to pick up a bit on stability on feedback systems (for nonlinear dynamical systems). This is a serious topic/question you have to address carefully. You cannot careless say the secondary control term is minor in value (which seems not according to your description) or fine-tuned later, then ignore this term and oversimplify the analysis of the stability of the whole nonlinear system as an LTI system.  For ODEs / dynamical systems, you cannot simply say that the term is or is kept small in fine-tuning and thus we don't need to consider it in stability analysis and the stability result is just the classic one for LTI systems. This key lesson we have learn for dynamical systems, in a comparison with regular algebraic-equation models. Well, if this control term is small in value, FYI referring to some results from optimal/robust control (the reviewer cannot name specific results, but we believe there exist several useful theorems/propositions), we are able to gain the stability of the whole system given certain conditions on the (uniform) bounds of this terms.  Well, your case seems a bit more general. No matter which case you are dealing with, you have to take serious the \"stability\" analysis for ODE  and address it rigorously. You may provide far stronger conditions that guarantee the stability than what you need in general in applying the model, which is common since theoretical analysis is hard and usually cannot be done at one-shot. \n\nOverall, we like to upgrade our score to 3 or 5 up to what you have explained. Again, if you like to improve your paper further to an acceptable level, in the reviewer's perspective, you need to **improve your stability analysis and address it rigorously in math**, as you also have said *its stability analysis is crucial*. What you have presented, **to our view, it is not a niche but an error** (which may not matter for some CSers, while leaving such an obvious misunderstanding in math may not be acceptable for ICLR). This is an interesting work, and thus please pick up stability analysis for ODEs and fix it."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576862050,
                "cdate": 1700576862050,
                "tmdate": 1700577245355,
                "mdate": 1700577245355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dxhgICQZ9M",
                "forum": "NLbRvr840Q",
                "replyto": "sx7LOBJ0SW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks! Your current description on stability analysis sounds a reasonable path. Please ensure the stability analysis is rigorously addressed in your later version; even if it leads to sufficient conditions that are much stronger than the practical or what you expected (it is not surprising in theoretical analysis; the theory is usually much behind the practice), it is still better than leaving the current LTI theorem in the paper and it will certainly help readers to understand the stability issue.  \n\nWe have upgraded the score to 5. (If you had presented your update theorem here and it is correct, we would be very happy to upgrade to a higher score. Well we understand the difficulties behind and time is demanded, and we are sorry for feedback late and leaving you less time to make a fix during rebuttal.)"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637094917,
                "cdate": 1700637094917,
                "tmdate": 1700639834882,
                "mdate": 1700639834882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5ixI1T8xBD",
                "forum": "NLbRvr840Q",
                "replyto": "g6d2rhZTgD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
                ],
                "content": {
                    "comment": {
                        "value": "FYI, the description of the \"control\" term, you don't have to follow the path we gave (i.e. explicitly introducing u(t), which may not look as well as the math style you prefer). That is just an example (to describe what we think it about), which may deserve more consideration. You may find any better way to clarify this term in texts or math. Our comments just stress that more descriptions on this term in the paper will help a lot to avoid confusions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637640635,
                "cdate": 1700637640635,
                "tmdate": 1700637640635,
                "mdate": 1700637640635,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mRUWS7WRP6",
                "forum": "NLbRvr840Q",
                "replyto": "sx7LOBJ0SW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3379/Reviewer_3YqS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks! Regarding your early-access updates on stability, please check the dimension-matching of matrix and vectors in $\\dot{X} = AX + \\sigma(XW + b)$. It is very good to take into account the specific tunable form of the control term; due to the specific form in NN, we don't have deal with complicated nonlinearity.  However, if we did not miss something, $X$ is the state variable right? How could these terms match their matrix-vector dimensions (for adding/multiplication), in $\\dot{X} = AX + XW + b$ (I know your XW+b comes from NN, which seems not directly match the dimensions). It is not a big issue and can be easily fixed. And note that, if you have multiple-layer NN to implement the control term, the you cannot just analyze one layer. \n\nOverall we think you have been on the right way to discuss the stability. We would like to upgrade to 6, according to the preliminary results. Please make sure that the stability result will be properly updated in your final version."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725744537,
                "cdate": 1700725744537,
                "tmdate": 1700725836020,
                "mdate": 1700725836020,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]