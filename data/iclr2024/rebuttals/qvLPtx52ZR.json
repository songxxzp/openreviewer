[
    {
        "title": "Classifier Guidance Enhances Diffusion-based Adversarial Purification by Preserving Predictive Information"
    },
    {
        "review": {
            "id": "l1JM8jwKZU",
            "forum": "qvLPtx52ZR",
            "replyto": "qvLPtx52ZR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3342/Reviewer_mPZU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3342/Reviewer_mPZU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an adversarial purification algorithm called COUP which incorporates the information from the classifier. Intuitively, it can increase the density and at the same time increase the confidence of classifiers. By doing this, the method can achieve better results than previous methods such as DiffPure. Also, the authors provide a theoretical analysis and toy examples to show that the proposed method is well-founded."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method is simple but effective, classifier guidance is overlooked in DM-based purification\n- The paper is well-written and easy to read\n- The performance is competitive, better than previous baseline methods"
                },
                "weaknesses": {
                    "value": "- Assume we have a stronger diffusion model, which can model the real distribution better, will the classifier-guided purification have smaller privilege, a good choice is the current SOTA EDM [https://github.com/NVlabs/edm]\n- Only present results on CIFAR-10, which may not be sufficient."
                },
                "questions": {
                    "value": "- For Section 4.2 line:4 , the input into the reverse SDE should be noised x_adv (pass through forward), or x_adv?\n- Can the proposed COUP attack against anti-purification attacks, such as [1, 2] ?\n-  What is the bound without classifier guidance, it is better than the boun in proposition 4.2 ?\n-  What is the difference between COUP w/o guidance and DiffPure?\n\n[1] Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability\n\n[2] Diffusion Models for Imperceptible and Transferable Adversarial Attack"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3342/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3342/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3342/Reviewer_mPZU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735874112,
            "cdate": 1698735874112,
            "tmdate": 1699636283790,
            "mdate": 1699636283790,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "5beej7Dnbt",
            "forum": "qvLPtx52ZR",
            "replyto": "qvLPtx52ZR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3342/Reviewer_7jaB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3342/Reviewer_7jaB"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of adversarial purification, finds a drawback of it and then proposes a novel strategy to alleviate the issue. \nAdversarial purification methods (Nie et al. 2022) consider a pretrained diffusion model to purify any given adversarial example. Such a mechanism works well if the data distribution is well separated because then the marginal data distribution learned by the diffusion model is equivalent to the conditional distribution of the classifier. However, in many cases, this assumption does not hold and consequently lead to mixing of the samples between neighboring distributions of different classes. This paper proposes a simple mechanism to address this issue\u2014to consider guidance from the classifier during the sampling process. \n\nThe authors also utilize a toy example with 1-d Gaussian distribution to present their use-case. Additionally, they perform experiments on widely used classifiers against robust benchmarks under l-2 and l-inf settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors present their ideas in a simple, yet understandable manner. They motivate their use-case using intuitive visualizations and make their case with toy data. They also show how label flip probability is aligned with their method providing a strong motivation for their methodology."
                },
                "weaknesses": {
                    "value": "Although this paper is well written, there are few problems I have here:\n\nMethodology: \n1. The method utilizes the \u2207x log max y p\u02c6(y|x) to consider during the purification process. Since x is unknown, r(x_adv) is used to compute this gradient. This tells me that the first projection by the diffusion model is critical for this method to work. The conditional gradient then prevents mixing between different class labels. \n2. While this would work well for low adversarial noise, I think this method would struggle under high adversarial noise. With slightly larger noise (l-inf \u03f5 > 8), the first step of the diffusion model may not project the adversarial example to the right cluster. Consequently, this may lead to mixing between different class labels.  \n3. Another point is also the amount of overlap between the clusters of different class labels. With a marginal overlap, \u2207x log p(x) could be equivalent to \u2207x log p(y|x). Only under a strong overlap do they diverge. In ideal settings, not many clusters may have such a strong overlap. \n\nExperiments: \n1. If my understanding about the methodology is correct (regarding first projection by diffusion model), then the BPDA attack should be exposed to this hypothesis, instead of the standard setting used. The attacker needs to make sure that the first step is attacked with a higher weight. \n2. Table 1 does not show the results for WRN-70-16."
                },
                "questions": {
                    "value": "1. Under the 1-d Gaussian setting, at what point of separation one could see that the gradient of the marginal distribution is equivalent to the gradient of the conditional distribution. This also relates to Figure 1a (visualization), where I believe the overlap is slightly lesser than the 1-d case and it could be that the gradient direction is incorrect for p(x).  \n2. Another important setting I see missing from the paper is that a simple cooling of the marginal distribution as mentioned in the `Guided Diffusion` paper [1] Appendix G may reduce the overlap of the clusters and prevent the mixing of the samples between different class labels. Why should classifier guidance be better than simple cooling? \n3. Table 1 does not show the results for WRN-70-16. It would be intuitive to see the results on more than one classifier.\n4. It is not clear to me what are the settings used in Figure 2. \n\n[1] Dhariwal, Prafulla, and Alexander Nichol. \"Diffusion models beat gans on image synthesis.\" Advances in neural information processing systems 34 (2021): 8780-8794."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748820252,
            "cdate": 1698748820252,
            "tmdate": 1699636283698,
            "mdate": 1699636283698,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "3b6skvvXlG",
            "forum": "qvLPtx52ZR",
            "replyto": "qvLPtx52ZR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3342/Reviewer_Jfjx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3342/Reviewer_Jfjx"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the issue of adversarial defense, a famous problem that how to improve the robustness of a given model against adversarial attacks. They proposes one kind of adversarial purification method, called COUP algorithm, to approach the goal.\n\nThe key ideas and the features in this paper includes:\n\n- COUP uses classifier confidence to guide the adversarial purification process with diffusion models. This helps preserve predictive information while removing adversarial noise.\n- It provides theoretical analysis showing classifier guidance can mitigate label shift and avoid misclassification.\n- Experiments on CIFAR-10 dataset demonstrate COUP achieves higher adversarial robustness compared to prior purification methods like DiffPure.\n- Ablation studies validate the benefits of classifier guidance and removing the forward diffusion process.\nCase studies and example purified images provide insights into how COUP works."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Present two theoretical guarantees. Though the first one (Proposition 4.1) works under the case of 1-dim SDE; and the second one (Proposition 4.2) require three bounds $C_s,\\,C_p,$ and $C_x$ as conditions.\n- Design Case Study and toy experiment (2-Gaussian distribution) to demonstrate that classifier guidance alleviates the information loss, as well as improves adversarial robustness."
                },
                "weaknesses": {
                    "value": "- Lack of experimental results. Maybe consider CIFAR-100 (larger number of classes) as dataset is necessary. Since \u201cclassifier guidance\u201d is important in your main subject, the number of classes should be an important variable to be considered.\u0000\n- The theoretical results and the presented toy experiment may not fully support the subject, there is a large gap from the practical situation."
                },
                "questions": {
                    "value": "1. In p. 5, the $t$ variable is missing in the RHS of the score function $s_\\theta(x,t)=\\nabla_x\\log\\hat{p}(x)$. Does it need to be corrected?\n2. In Fig. 2(a), what does the word \u201cdisminative\u201d mean? I don\u2019t really understand.\n3. How do you evaluate the diffusion-based models like COUP and GDMP? Did you evaluate them under DiffPure\u2019s environment setting?\n4. What does the quoted sentence mean: \u201cTo focus on the purification process, we do not\nconsider Brownian motion at inference time.\u201d? Does this mean that you fix a random seed in Case Study?\n5. It seems that your proposed Proposition 4.2 looks similar to Theorem 3.2 in DiffPure. Is there any relevance? Also, how to control $C_s,\\,C_p,$ and $C_x$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3342/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3342/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3342/Reviewer_Jfjx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751670344,
            "cdate": 1698751670344,
            "tmdate": 1699636283615,
            "mdate": 1699636283615,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "npU3AIxgDn",
            "forum": "qvLPtx52ZR",
            "replyto": "qvLPtx52ZR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3342/Reviewer_Ac9j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3342/Reviewer_Ac9j"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to improve the existing solution for adversarial purification. The main idea is to add one more term about the classifier scores to the optimization objective of the diffusion-based model. The goal is to strike a good trade-off between adversarial noise removal and classification information preservation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The idea is straightforward and technically sound. It is rationale to add the classifier posterior for better classification accuracy.\n\n+ The paper is clear and easy to follow. The authors clearly presented the motivation for their approach.\n\n+ Experimental results indicate improvements in terms of purification metrics."
                },
                "weaknesses": {
                    "value": "- The proposed method is technically sound but incremental. It is basically doing multi-task learning by simultaneously purifying images and predicting the class labels. Getting improved performance is not surprising at all.\n\n- The motivation is to address the balance between noise removal and information preservation. However, I didn't see too much discussion or design to address this balance. By adding the classification loss, the information preservation can be addressed. But how about the nose removal? Is there any tradeoff between noise removal and information preservation? Any theoretical analysis? Any empirical validation?\n\n- I find the experiments are weak. Very limited baseline methods are compared. More state-of-the-art methods with strong performance should be compared. Only CIFAR-10 is used for experiments. How about other real-world datasets? How about other types of images or data?\n\n- The theory analysis is hard to follow. I tried to read the supplementary but found it needs better organization to make the proof clearer and more concise."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3342/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699037317857,
            "cdate": 1699037317857,
            "tmdate": 1699636283522,
            "mdate": 1699636283522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]