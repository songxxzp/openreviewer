[
    {
        "title": "Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization"
    },
    {
        "review": {
            "id": "X3hSi21xUw",
            "forum": "yfdtkYQesu",
            "replyto": "yfdtkYQesu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3568/Reviewer_u1Lz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3568/Reviewer_u1Lz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to apply adaptive gradient methods, such as Adam, to learning-rate-free methods for deep learning.\nThe experiments demonstrate the proposed method works on various scenarios, including image classification to reinforcement learning and semantic segmentation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experiments are conducted on cases where learning rate configuration is crucial, such as reinforcement learning and training of ViT from scratch and demonstrate that the approach is comparable or even better to the baselines.\nI hope this approach relieve us from learning rate tuning."
                },
                "weaknesses": {
                    "value": "* To my understand $c$ in Algorithm 1 is a hyperparameter. If so, does it mean that this method introduces a parameter to eliminate learning rate? How sensitive the proposed method to this parameter?\n* Algorithm 1 also requires $f^*$, which I think is the loss value at the optimum. For deep models, obtaining such a value sounds quite challenging."
                },
                "questions": {
                    "value": "* How to tune $\\gamma_k$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726490201,
            "cdate": 1698726490201,
            "tmdate": 1699636311588,
            "mdate": 1699636311588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CPXiyEFo54",
                "forum": "yfdtkYQesu",
                "replyto": "X3hSi21xUw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1) We apologize for the confusion. Below is our response to weakness 1:\n\nAlthough Alg. 1 introduces a hyperparameter, $c$, it demonstrates robustness to this hyperparameter. Specifically, we consistently employed $c=0.5$ across all experiments, including supervised classification, self-supervised learning, fine-tuning, and reinforcement learning.\n\n&nbsp;\n\nW2) Yes. $f^*$ represents the loss value at the optimum, and achieving this value can be challenging in deep learning applications. However, we can simply use the theoretically minimum value of the loss function as $f^*$. For instance, we set $f^*=0$ for cross-entropy losses and the SimCLR loss. On the other hand, since we cannot determine the minimum loss values for reinforcement learning models, we have omitted the experiments in Table 3. (SPS and PS-SPS in reinforcement learning tasks)\n\n&nbsp;\n\nQ1) $\\gamma_k$ is an optional hyperparameter for learning rate annealing. We can simply employ the same learning rate annealing schedule used in previous methods, such as a step decay or a cosine annealing. It is worth noting that the annealing should have a base learning rate of $1$. For example, a step decay should start from $1$ and decay to $0.1$ and $0.01$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699707988395,
                "cdate": 1699707988395,
                "tmdate": 1699707988395,
                "mdate": 1699707988395,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K7k7dyU5X6",
                "forum": "yfdtkYQesu",
                "replyto": "CPXiyEFo54",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3568/Reviewer_u1Lz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3568/Reviewer_u1Lz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comments.\n\n* W1\n> we consistently employed  $c=0.5$\n\nAlong with the reply to Q1 that you use annealing, I got uncertain how to see your empirical results.\nI think one can train neural networks very well by using Adam, for example, with a default learning rate with a (careful) annealing strategy.\nThe difference between using learning-rate-free methods and Adam, or things like this, is unclear to me.\n\n* W2\nThank you for the clarification. It would be appreciated if you state it clearly in the manuscript."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644863396,
                "cdate": 1700644863396,
                "tmdate": 1700644863396,
                "mdate": 1700644863396,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r0w7v41ZQf",
            "forum": "yfdtkYQesu",
            "replyto": "yfdtkYQesu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3568/Reviewer_jkmf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3568/Reviewer_jkmf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an efficient learning-rate free gradient-descent type optimization technique. The approach reconciles learning-rate-free approaches with parameter-wise adaptive gradient scaling methods. This result is achieved intuitively by reinterpreting gradient scaling as parameter rescaling. The approach builds on recently introduced methods for learning-rate-free optimization techniques and extends those to a parameter-wise step-size adaptation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The presentation of the paper is clear and the approach is simple yet original and efficient and has potentially a promising impact."
                },
                "weaknesses": {
                    "value": "While the approach is intuitive and a convergence proof is given, the approach exhibits heuristic qualities and doesn't discuss the resulting dynamic of the adaptation. Especially, in consideration of the potential complex resulting dynamics by applying parameter wise step-size adaptation."
                },
                "questions": {
                    "value": "Is there something that can be said about label noise sensitivity/robustness of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778397590,
            "cdate": 1698778397590,
            "tmdate": 1699636311509,
            "mdate": 1699636311509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gAxa8N49k4",
                "forum": "yfdtkYQesu",
                "replyto": "r0w7v41ZQf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1) We apologize for the confusion. Our proposed method estimates one step-size for one network. \n\nFor example, when employing Adam to train a network, there exists one step-size hyperparameter, and our method is designed to estimate this value.\n\nThe parameter-wise scaling of the proposed method corresponds to the gradient scaling of Adam, where the gradient of each parameter is divided by the square root of the moving average of the second moment of the gradient. Similarly, the proposed method scales each parameter by the fourth root of the moving average of the second moment of the gradient. The effects of these scaling are equivalent as shown in Eqs. 1-4.\n\n&nbsp;\n\nQ1) Thank you for the insightful suggestion. To measure the label noise sensitivity of the proposed method, we conducted an additional experiment using the ResNet-32 network on the CIFAR-100 dataset. To introduce label noise, we randomly shuffled some labels in the training dataset. The Adam optimizer with a learning rate of $0.003$ was chosen as the baseline, which is the optimal learning rate value found by grid-search on the dataset without label noise.\n\n| Label noise | 0.0   | 0.2   | 0.4   | 0.6   | 0.8   |\n|-------------|-------|-------|-------|-------|-------|\n| Adam        | 66.31 | 50.95 | 39.75 | 25.10 | 9.65  |\n| PS-DA-SGD   | 64.49 | 52.57 | 41.34 | 25.26 | 10.41 |\n\nThe proposed method demonstrates robustness to the label noise by surpassing the baseline when the label noise is injected. It is due to the optimal learning rate depends on the level of label noise. We used a learning rate of $0.003$ for Adam across all label noise levels. While this value was the optimal value in the absence of label noise, it was suboptimal when the level of label noise changed. Contrarily, the proposed method found the optimal learning rate regardless of the label noise level."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699804729385,
                "cdate": 1699804729385,
                "tmdate": 1699804729385,
                "mdate": 1699804729385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3Mzo55emlc",
            "forum": "yfdtkYQesu",
            "replyto": "yfdtkYQesu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3568/Reviewer_AxR5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3568/Reviewer_AxR5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes two new algorithms Parameter-scaled stochastic Polyak step-size and Parameter-scaled D-Adapt from the intuition of parameter scaling, and compares their performance to other algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The problem studied is interesting. It is important to learn whether we can make adaptive gradient methods learning rate-free."
                },
                "weaknesses": {
                    "value": "The contribution of the paper is unclear. In the first sentence in the abstract, the authors claim that they \"address the challenge of estimating the learning rate for adaptive gradient methods.\" The issue is important, but after reading the paper, I do not follow how they addressed the issue.\n\nThen, the authors claim they \"interpret adaptive gradient methods as steepest descent applied on parameter-scaled networks .\" Authors need to explain why their new interpretation is important to the ICLR community.\n\nAlso, the authors claim they \"propose learning rate-free adaptive gradient methods\". It appears that algorithm 2 is the method they propose. However, in algorithm 2, there are a lot of hyper-parameters, including $\\eta_k$, $\\gamma_k$, and even $\\alpha_k$. It is not clear to me why Algorithm 2 is \"learning rate-free\". The explanation about the notations in Algorithm 2 should be clearer.\n\nIn section 5.2, authors should report the metrics for their reinforcement learning experiment. It is hard to understand the value of the proposed PS-SPS and PS-DA-SGD from Table 3. Also, the authors mention that they removed all the batch normalization layers in the CIFAR-100 experiment. What are the benefits of such removal?\n\nAuthors should also clearly write their assumptions and conclusions into a formal theorem in Section 4.2."
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781873797,
            "cdate": 1698781873797,
            "tmdate": 1699636311442,
            "mdate": 1699636311442,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cXEUrjP7Pw",
                "forum": "yfdtkYQesu",
                "replyto": "3Mzo55emlc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We apologize for any confusion. We have clarified the statements in our paper as follows:\n\nW1) In our paper, we present two algorithms to estimate the learning rate of adaptive gradient methods. In Alg. 1, line 6 represents the update rule of an optimizer, with the learning rate $\\eta_k$ estimated in line 5. Similarly, in Alg. 2, line 11 represents the update rule of an optimizer, with the learning rate $\\eta_k$ estimated in line 10.\n\n&nbsp;\n\nW2) Our proposed interpretation of adaptive gradient methods enables the transformation of learning-rate-free optimizers developed for steepest descent [1], [2] into adaptive gradient learning-rate-free optimizers.\n\n&nbsp;\n\nW3) Training a deep neural network with an adaptive gradient optimizer involves various design choices:\n1. Selecting a gradient rescaling strategy, such as AdaGrad, Adam, or AMSGrad.\n2. Choosing a base learning rate within the range of $0.0003$ to $0.01$ or broader.\n3. Selecting a learning rate annealing schedule, such as constant, step decay, or cosine annealing.\n\nIn this context, Alg. 1 relieves the second and third design choices, while Alg. 2 addresses the second design choice. These algorithms are termed learning-rate-free methods as they alleviate the manual tuning of the base learning rate.\n\nThe $\\alpha_k$ term in Alg. 1 and Alg. 2 corresponds to the first design choice. For example, if we choose Adam as the optimizer, then $\\alpha_k$ will be $v_k^{1/4}$ (see Table 1 and Eq. 4). The $\\gamma_k$ of Alg. 2 corresponds to the third design choice, representing the learning rate annealing schedule. For example, if we choose a step decay schedule, then $\\gamma_k$ will start from $1$ and decay by a factor of $10$.\n\nThe remaining parameters, $c$ of Alg. 1 and $d_0$ of Alg. 2, are hyperparameters. However, the algorithms are robust to their values. We used $c=0.5$ and $d_0=10^{-6}$ across all experiments, including supervised classification, self-supervised learning, fine-tuning, and reinforcement learning.\n\n&nbsp;\n\nW4) In our experiments evaluating reinforcement learning, we utilized the average of the reward output of the OpenAI Gym environment as the performance metric. The values in parentheses represent the training success rate. It is important to note that SPS and PS-SPS were omitted from these experiments due to their reliance on the loss value at optimum, denoted as $f^*$, which is challenging to estimate before training within the context of reinforcement learning models.\n\nWe have removed all batch normalization layers from VGG networks, while ResNet networks remain unchanged. This modification aims to assess the impact of batch normalization on learning-rate-free algorithms. Our rationale for investigating this impact of batch normalization stems from the observation that many learning-rate-free methods exhibit subpar performance in reinforcement learning experiments, where batch normalization is not employed.\n\n&nbsp;\n\nW5) A formal theorem of Sec. 4.2 is as follows.\n\nAMGRrad scaling rule:\n\n$\\alpha_k=(\\max_{i\\le k}v_i)^{1/4}$, where $v_{i+1}=\\beta v_i + (1 - \\beta) g^2_i$, $0<\\beta<1$.\n\nFor a convex G-Lipschitz function $f$ and AMSGrad scaling rule, Alg. 1 converges to the minimum of $f$.\n\nFor a convex G-Lipschitz function $f$, AMSGrad scaling rule, and a decreasing sequence $\\gamma_k$ satisfying $\\sum^\\infty_{k=1}\\gamma_k=\\infty$ and $\\sum^\\infty_{k=1}\\gamma_k^2<\\infty$, Alg. 2 converges to the minimum of $f$.\n\n&nbsp;\n\n[1] Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence, Loizou, Nicolas, et al., AISTATS (2021)\n\n[2] Learning-Rate-Free Learning by D-Adaptation, Aaron Defazio, Konstantin Mishchenko, ICML (2023)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699955237227,
                "cdate": 1699955237227,
                "tmdate": 1699955237227,
                "mdate": 1699955237227,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UhpduiMX1X",
            "forum": "yfdtkYQesu",
            "replyto": "yfdtkYQesu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3568/Reviewer_jSjZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3568/Reviewer_jSjZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a learning rate tuning method for adaptive optimization algorithms. Besides, this paper also proposes a method to interpret adaptive gradient methods as parameter-scaled SGD. Experimental results show that the proposed method can be comparable with adaptive gradient methods with hand-tunned learning rates."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper uses parameter rescaling to interpret adaptive gradient methods, which could be helpful for further investigating the behavior of adaptive gradient methods.\n2. The proposed learning-rate-free methods can be useful to avoid the hyperparameter tunning in adaptive gradient methods while still achieving fast convergence."
                },
                "weaknesses": {
                    "value": "1. The paper organization is not clear to me. In particular, Algorithm 2 looks pretty complicated to me. The authors just explain each steps after the algorithm, while I am still not very clear about the motivation and why such a method can be developed.\n\n2. Second, the equations are also not clear. The authors claim that the adaptive methods can be viewed as applying the steepest descent to parameter-scaled networks based on Eqs 1-4. However, the notations are not clear, what's the formal definition of $f'$, why $f'$ needs to be introduced, and how to leverage it?\n\n3. In Section 3, equation (7) is also confusing, in Adam, $\\alpha$ is also depending on the randomness of the stochastic gradients, when why $E u = \\nabla f(w)/\\alpha^2$ can hold?\n\n4. The reasoning from (9)-(11) is also not clear to me, if you only want to mention that the learning rate should not depend on $\\alpha$, why do you still need equations (10) and (11)?\n\n5. The convergence analysis is also not clear, the authors just provide a very simple proof in the appendix, in the main part, I actually do not see anything that is related to the convergence. Additionally, the proof is also not clear, many notations such as D,G are not clearly presented; the assumption that $\\alpha_k$ coverges to $\\alpha$ is also not presented; Eq. (15) is also not well justified."
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699112219760,
            "cdate": 1699112219760,
            "tmdate": 1699636311364,
            "mdate": 1699636311364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xtoitQ6UBP",
                "forum": "yfdtkYQesu",
                "replyto": "UhpduiMX1X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W2) Sorry for the ambiguous explanation. Below is our response to weakness 2:\n\nGiven a locally convex function $y=f(w)$, an optimizer tries to find its local minimum $w_*$ with an initial guess of $w_0$. Here, we stretch $f$ in the $w$-axis by a factor of $\\alpha$. Therefore, the formal definition of $f'$ is given as $f'(w)=f(w/\\alpha)$. [Eq. (1)]\n\nThe reason we introduce $w'$ is to emphasize the relationship between the local minimum and the initial guess of $f'$ and $f$, where the local minimum of $f'$ becomes $\\alpha w_* = w'_*$ and our initial guess becomes $\\alpha w_0 = w'_0$.\n\nThe above observation leads to our main idea: stretching $f$ by $\\alpha$ **increases** the distance to the solution by the factor of $\\alpha$, but the gradient **decreases** by the factor of $\\alpha$. Therefore, if we use a learning rate of $\\eta$ to find the local minimum on $f'$, its behavior will be equivalent to using a learning rate of $\\eta/\\alpha^2$ to find the local minimum on $f$. [Eqs. (2-4)]\n\nIn the above discussion, we focused on the case of a scalar-valued $\\alpha$. However, the same principles can be applied to transform adaptive gradient methods into steepest descent by stretching $f$ with different values for each axis.\n\n&nbsp;\n\nW4) Sorry for the confusion. Below is our response to weakness 4:\n\nWhile the learning rate may depend on $\\alpha$, we would like to emphasize that the factor that should not depend on $\\alpha$ is the trajectory of parameter. Assume an optimizer updates the parameter of $f$ with $w_0$, $w_1$, $w_2$, ..., $w_t$ and updates $f'$ with $w'_0$, $w'_1$, $w'_2$, ..., $w'_t$. Here, we used the stretched function $f'(w)=f(w/\\alpha)$. If a learning-rate-free optimizer actually estimates the optimal learning rate, then the trajectory should be identical regardless of $\\alpha$, i.e., $w'_i=w_i/\\alpha$ for all $i$.\n\nHowever, without employing our parameter scaling approach, simply scaling gradients while leaving parameters intact (which is the case of previous adaptive gradient methods) results in trajectories that depends on $\\alpha$. [Eqs. (9-11)]\n\nIn contrast, it can be easily shown that our parameter scaling approach ensures identical trajectories as following. In this case, we estimate the learning rate on $\\alpha$-stretched $f\u2019(w\u2019)$, which means $f'(w')=f(w)$, $\\nabla f'(w')=\\nabla f(w)/\\alpha$, and $w\u2019=\\alpha w$. Therefore, $E[\\eta u]$, which means the estimated step, becomes proportional to $\\alpha$. Since $\\alpha$-stretched function leads to an estimated step that is proportional to $\\alpha$, out parameter scaling approach ensures identical parameter trajectories across different scalings.\n\n&nbsp;\n\nW3) Thank you for pointing out the ambiguity. Below is our response to weakness 3:\n\nEq. 7 holds true only when the coefficient for computing the moving average of second moment (denoted as $\\beta_2$ in the case of Adam) is sufficiently large so that the stochasticity of gradient can be ignored."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699689302692,
                "cdate": 1699689302692,
                "tmdate": 1699689302692,
                "mdate": 1699689302692,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R6E3yCUIG2",
                "forum": "yfdtkYQesu",
                "replyto": "UhpduiMX1X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "W1) Sorry for the insufficient explanation. We appreciate the opportunity to further provide details about Alg 2.\n\nAlg. 2 describes the procedures for transforming D-Adapt SGD [1] into a form suitable for adaptive gradient methods. The main challenge we encountered during this transformation arose from its method of estimating learning rate, denoted as $\\eta=\\frac{d \\gamma}{\\lVert g_0 \\rVert}$ [Alg. 4 of [1]].\n\nIn adaptive gradient methods, $\\alpha$ is decreased to ensure the consistent gradient scaling throughout training. However, when a parameter converges earlier than others, the corresponding element of $\\alpha$ approaches zero, which hinders the straightforward application of our parameter scaling approach. If we scale parameters using $\\alpha$, one of whose elements is near zero, $\\lVert g_0' \\rVert=\\lVert g_0/\\alpha\\rVert$ diverges. Consequently, $\\eta$ becomes zero, leading to early termination of the training process if we directly apply the parameter scaling approach to D-Adapt SGD.\n\nTo address the risk of early termination, we opted to scale gradients using $\\alpha_M$ instead of $\\alpha_k$. [line 6, 9]\n\nHowever, this modification introduces an issue -- specifically, it no longer ensures the identical trajectories of parameter across different scalings, which is described in our response to weakness 4. To mitigate this concern, we introduced line 7. In the D-Adaptation algorithm, $d$ represents the estimated lower bound of the distance to solution from the initial guess. Given that the distance to solution is also influenced by the change in parameter scaling, the adjustment in line 7 is justifiable.\n\nAdditionally, line 8 provides a minor improvement. In the D-Adaptation algorithm, the Lipshitz constant of $f$, denoted as G, is used to estimate the learning rate. While [1] uses the gradient norm at initial guess as a proxy of G, we utilize the norm of the maximum gradient of each element.\n\nThe other lines are either identical to or straightforwardly derived from Alg. 4 of [1].\n\nIn contrast, SPS [2] does not suffer from the early termination issue, therefore, does not require extra modifications as outlined in Alg. 1.\n\n&nbsp;\n\nW5) Sorry for the insufficient explanation. Below is our response to weakness 5:\n\nWe omit the details about the proof in the main text, because they are simple and constitute only a minor part of our contribution. $G$ denotes the Lipschitz constant of function $f$ and $D$ denotes the distance to optimum from the initial guess. While Alg. 1 and Alg. 2 accept arbitrary scaling factors $\\alpha_k$, we employed the AMSGrad [3] scaling rule for the convergence proof, denoted as $\\alpha^2_k=\\sqrt{\\max_{i\\le k} v_i}$. As $\\alpha_k$ is a non-decreasing sequence with an upper bound of $\\sqrt{G}$, it converges to a constant and we denote the constant as $\\alpha$.\n\nFurther details regarding Eq. 15 are provided below.\n\nIf we consider two vectors $x=[x_0, x_1, ..., x_n]$ and  $y=[y_0, y_1, ..., y_n]$ with $x_i \\ge y_i$ for all $i$, we can define the element-wise division $x \\circ y^-1=[x_0/y_0,...,x_n/y_n]$. Since $x_i/y_i \\ge 1$, it follows that $\\max (x \\circ y^-1) \\ge \\prod_i x_i / \\prod_i y_i$. We define $\\beta_k$ as the product of all elements of $\\alpha_k$. Since $\\alpha_k$ is a bounded vector with a finite number of elements, $\\beta_k$ is also bounded. Finally, we define $\\beta_n \\cdot \\beta_m ^{-1}=A^2$.\n\nAdditionally, as $\\alpha_k$ is a non-decreasing sequence, it follows $\\max (\\alpha_k \\circ \\alpha^{-1}_{k-1}) \\ge 1$.\n\n&nbsp;\n\n[1] Learning-Rate-Free Learning by D-Adaptation, Aaron Defazio, Konstantin Mishchenko, ICML (2023)\n\n[2] Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence, Loizou, Nicolas, et al., AISTATS (2021)\n\n[3] On the Convergence of Adam and Beyond, Reddi, Sashank J., Satyen Kale, and Sanjiv Kumar., ICLR (2018)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699689913300,
                "cdate": 1699689913300,
                "tmdate": 1699689913300,
                "mdate": 1699689913300,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]