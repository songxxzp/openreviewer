[
    {
        "title": "Closed-Form Diffusion Models"
    },
    {
        "review": {
            "id": "VwPET9995o",
            "forum": "f5juXkyorf",
            "replyto": "f5juXkyorf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5670/Reviewer_pdyz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5670/Reviewer_pdyz"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces closed-form diffusion models by smoothing the closed-form score function with finite training data. By explicitly introducing error through this smoothing process, the resulting diffusion models, referred to as $\\\\sigma$-CFDM, exhibit generalization capabilities. The paper also provides proof that $\\\\sigma$-CFDM's support contains the exact barycenters of $M$-tuples from the training points. To expedite the sampling process, the authors propose techniques such as initializing with unsmoothed CFDM samples at $T>0$ and utilizing an approximate nearest neighbor search method. Notably, experiments illustrate that $\\\\sigma$-CFDM can generate novel samples without the need for a training stage."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: Although Equation (1,2) has been referenced in several existing works, the extension of this concept to closed-form diffusion models with generalization capabilities is an intriguing innovation. The deliberate introduction of error through a smoothing process not only distinguishes this work but also explicitly defines the inductive bias, a valuable contribution in the current deep learning-dominated era.\n\nQuality: The work is well-motivated and logically compact. The presented propositions and the theorem are solid.\n\nClarity: The paper is generally well-written and easy to follow.\n\nSignificance: The paper offers a fresh perspective on the study of diffusion models. However, some concerns about its overall significance are detailed in the following section."
                },
                "weaknesses": {
                    "value": "My primary concern about this paper revolves around the apparent simplicity of the generalization achieved by $\\sigma$-CFDM. Indeed, the generalization abilities of generative models trained on finite datasets inherently arise from their deviation from a precise fit to the empirical distribution. Introducing explicit error to the empirical distribution, as presented in this paper, seem more elegant and interpretable compared to conventional deep learning methods. However, it's important to note that the proposed smoothing method essentially makes an assumption on the underlying data distribution (or the inductive biases). It posits that the barycenters of empirical data points also reside within the supports of the true data distribution. This assumption shares similarities with the mixing of training data points. An alternative approach would be to directly sample the mixup, rendering the diffusion process unnecessary. For instance, a generative model could be defined by initially sampling $M$ training data points and then returning their barycenter (with probabilities related to the variance) as the generated sample. Alternatively, one could employ a weighted average by further sampling weighting parameters from the $(M-1)$-simplex. It would be valuable to provide a comparative analysis with these alternative methods to offer a more comprehensive evaluation (e.g. FID score and visualization). As the authors mention in Section 6.4, for image data, the barycenters may not be well-registered, and an auto-encoder is adopted, such a comparison should also be conducted in the latent space for real image generation."
                },
                "questions": {
                    "value": "- How does $M$ affect the behavior of $\\\\sigma$-CFDM?\n- Can this method be extended to conditional generation?\n\nPlease also see my questions in the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5670/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651316327,
            "cdate": 1698651316327,
            "tmdate": 1699636591428,
            "mdate": 1699636591428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lKZdeMS8FK",
                "forum": "f5juXkyorf",
                "replyto": "VwPET9995o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission5670 by Reviewer pdyz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments, which will improve the quality of our manuscript, and are glad they share our interest in closed-form diffusion models that generalize. We have uploaded a revision to OpenReview, where the reviewer will find that we have incorporated many of their suggestions. In this revision, we have typeset our relevant changes in teal. We respond to the reviewer's comments below.\n\n**Inductive bias of our method.**\n\nAs the reviewer points out, our method assumes that the barycenters of nearby training points lie within the support of the data distribution. This is a reasonable prior if the data distribution is supported on a manifold whose curvature isn\u2019t too large relative to the number of training samples. As a classical example, Roweis and Saul\u2019s \u201clocally linear embedding\u201d imposes a very similar prior on its inputs.\n\nWhile this assumption often isn\u2019t true of the image manifold in pixel space (barycenters of images typically don\u2019t look like natural images), by mapping our training data to a latent space with an appropriately-regularized autoencoder, we obtain a latent data manifold whose structure is more favorable to our method. This explains why our method is able to generate plausible CIFAR samples when operating in the latent space of an autoencoder with a tight bottleneck. We have updated our discussion in Section 6.4 to clarify this point.\n\nFor future work, we will explore training an autoencoder that enforces local linearity in the latent space; see e.g. Section 4.2 in \u201cLearning Signal-Agnostic Manifolds of Neural Fields\u201d by Du et al. (2021). We hope this strategy will yield a latent data manifold with a locally-linear structure that is highly favorable to our method and produce natural-looking decoded samples. We have updated our conclusion to include this future direction.\n\n**Alternative sampling processes.**\n\nNaively-implemented \u201cgenerative mixup\u201d would draw pairs of training points uniformly from the training set and then output some data-independent convex combination of these points. This amounts to sampling from the convex hull of the training set, and is not a suitable inductive bias for most target distributions. One exception is when the target distribution is log-concave; in this case, convex combinations of pairs of target samples have log-likelihood at least as large as one of the endpoint samples. For example, all Gaussian distributions are log-concave. Our fitted Gaussian baseline in Section 6.4 (which performs very poorly) demonstrates that this log-concavity prior is unsuitable for image generation.\n\nOn the other hand, directly sampling tuples of $M$ training points with probability depending on their variance and returning their barycenters is challenging, because there are $N^M$ such tuples for training sets of size $N$.\n\nInstead, we have implemented and experimented with a new sampler in Appendix D.4 of the revision, which implements the one-step sampling procedure described in Proposition B.1. This is an extreme case of our existing strategy for reducing the number of sampling steps using a warm start from an unsmoothed CFDM, and uses Gumbel perturbations $u_{i,m}$ to the distance weights in Equation (4) for analytical convenience. Proposition B.1 shows that this method is equivalent to drawing a batch of query points $z_{S-1}$ lying near the support of the data distribution $\\rho_1$, drawing $M$ training points $x_i$ with probabilities  $\\pi^i_\\sigma = \\textrm{softmax}\\left(-\\frac{1}{\\sigma} \\|z_{S-1} - x_i \\|^2 \\right)$, and then returning their barycenters.\n\nAs expected, this method achieves significantly higher sample throughput than our existing approach ($\\sim 4$ times faster for CIFAR generation in latent space). While the decoded samples are reasonable, the speedup comes at a cost to sample quality: Many of our decoded samples retain the appearance of superpositions of training samples.\n\n**Effect of $M$ on CFDM samples.**\n\nWe have included an experiment in Appendix E of the revised manuscript demonstrating the impact of $M$ on our method\u2019s samples.\n\n**Extension to conditional generation.**\n\nIn future work, we intend to explore using our method for conditional generation by using Dhariwal and Nichol (2021)'s classifier guidance, which would amount to augmenting our velocity field (7) with the gradient of a pretrained classifier. We have updated our conclusion to include this future direction.\n\n**Conclusion.**\n\nWe thank the reviewer for their comments and hope that we have adequately addressed their concerns. If they are satisfied with our answers and our revision, we respectfully request that they raise their score for this paper. Otherwise, we would be pleased to continue this discussion during the reviewer-author discussion period."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074855421,
                "cdate": 1700074855421,
                "tmdate": 1700159011681,
                "mdate": 1700159011681,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kkZuW7cYT7",
                "forum": "f5juXkyorf",
                "replyto": "VwPET9995o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for Reviewer pdyz"
                    },
                    "comment": {
                        "value": "Dear Reviewer pdyz -- as the discussion period will close in a few days, we would greatly appreciate if you would take a look at our response to your review soon and let us know if you would like to see further changes to our manuscript. We look forward to addressing your remaining concerns before the end of discussion period. If our response was satisfactory, we ask that you consider raising your score for our submission. Thank you for your time."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489239880,
                "cdate": 1700489239880,
                "tmdate": 1700489239880,
                "mdate": 1700489239880,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C327lBXBOq",
                "forum": "f5juXkyorf",
                "replyto": "lKZdeMS8FK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Reviewer_pdyz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Reviewer_pdyz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I appreciate the additional insights into the inductive bias, although I still hold the view that the introduced inductive bias might be somewhat trivial. Regarding the convex hull, I believe $\\sigma$-CFDM also samples from the convex hull, so I don't see a clear drawback of \"naive mixup generation\" compared to $\\sigma$-CFDM. However, I appreciate the idea of building generative models in a nonparametric sense, where introducing an effective and nontrivial inductive bias is both challenging and intriguing. Therefore I would like to maintain my original rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559643690,
                "cdate": 1700559643690,
                "tmdate": 1700559643690,
                "mdate": 1700559643690,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HkqXFzuGlm",
            "forum": "f5juXkyorf",
            "replyto": "f5juXkyorf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5670/Reviewer_P7kJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5670/Reviewer_P7kJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers smoothed closed-form diffusion models. In particular, the authors study the properties of smoothed score function and propose a new sampling algorithm. Numerical experiments are organized to evaluate the performance of this new model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors propose a new diffusion model with closed-form score function.\n2. The paper studies analytic properties of smoothed diffusion models, including support of output distribution and the approximation error.\n3. Comprehensive numerical experiments are organized. The smoothed diffusion model is compatible with neural network-based diffusion models such as DDPM.\n4. The model is training-free. Sampling can be implemented even in CPUs."
                },
                "weaknesses": {
                    "value": "1. If my understanding is correct, the motivation is to have a diffusion model with good generalization capacity. Although the smooth diffusion model performs as well as DDPM, it is still unclear how it is connected to the generalization of diffusion models. \n2. In terms of modeling, the only novelty seems to be an additional smooth term added to $k$. Could you point out your contribution more clearly?\n3. The writing looks good but can still be improved."
                },
                "questions": {
                    "value": "I have some minor questions:\n\n1. Why do we have a closed-form score function as in Section 3? Could you give some references or provide derivations in the appendix?\n2. Seemingly, you only provide a comparison to DDPM in terms of training time and LPIPS. Could you provide more comprehensive experiments compared to other benchmarks and additional metrics?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5670/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5670/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5670/Reviewer_P7kJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5670/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777977840,
            "cdate": 1698777977840,
            "tmdate": 1699636591328,
            "mdate": 1699636591328,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0j0Xc0l9sf",
                "forum": "f5juXkyorf",
                "replyto": "HkqXFzuGlm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission5670 by Reviewer P7kJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments, which we believe will improve the quality of our manuscript. We have uploaded a revision to OpenReview, where the reviewer will find that we have incorporated many of their suggestions. In this revision, we have typeset our relevant changes in teal. We respond to the reviewer's comments below.\n\n**Relationship to the generalization of diffusion models.**\n\nOur goal is to design a diffusion model that generalizes controllably while requiring no training and being efficient to sample. While our method is indeed inspired by recent work studying the generalization of diffusion models, our contribution is not a rigorous study of the generalization of existing (neural) diffusion models, but rather introducing a new class of diffusion models that do not require trained approximations to the score function.\n\nIn particular: Pidstrigach (2022) shows that a score-based generative model (SGM) can generalize only if the model score incurs unbounded approximation error relative to the true score; otherwise, the model memorizes its training data. We combine this strong result with the observation that neural networks tend to learn overly smooth approximations to their target function and propose a simple, training-free method to induce the error that Pidstrigach (2022) shows to be necessary, in a form similar to that of neural SGMs.\n\nWe have updated our introduction and our lead-in to Section 4 to clarify these points.\n\n**Clarifying our contribution.**\n\nIn addition to smoothing the closed-form score and showing that this promotes generalization, we introduce two approximation techniques to render this method tractable: Initializing our sampler with unsmoothed CFDM samples at $T>0$ and a nearest-neighbor-based score approximation. These approximations enable our method to achieve very fast inference speeds (for example, 138 latents/sec on our CIFAR experiments) while running on a consumer-grade laptop without a dedicated GPU.\n\nWe have updated our introduction to clarify this point.\n\n**References for closed-form score formula in Section 3.**\n\nWe have updated our discussion in Section 3 to include several references for this formula. In particular, Appendix B.3 of \u201cElucidating the Design Space of Diffusion-Based Generative Models\u201d by Karras et al. (2022) includes a full derivation of the closed-form score formula; their \u201cideal denoiser\u201d corresponds to our function $k_t$.\n\n**Additional benchmarks and metrics.**\n\nIn our revised manuscript (Table 1), we have also reported our image generation results in terms of the kernel inception distance (KID). Furthermore, in Appendix D.4 of the revision, we implement and experiment with an alternative sampler. This sampler implements the one-step procedure described in Proposition B.1 and is an extreme case of our existing strategy for reducing the number of sampling steps, using a warm start from an unsmoothed CFDM and Gumbel perturbations to the distance weights in Equation (4) for analytical convenience. Proposition B.1 shows that this equivalent to drawing a batch of query points $z_{S-1}$ lying near the support of the data distribution $\\rho_1$, drawing $M$ training points $x_i$ with probabilities  $\\pi^i_\\sigma = \\textrm{softmax}\\left(-\\frac{1}{\\sigma} \\|z_{S-1} - x_i \\|^2 \\right)$, and then returning their barycenters. \n\nAs expected, this method achieves significantly higher sample throughput than our existing approach ($\\sim 4$ times faster for CIFAR generation in latent space). While the resulting outputs are reasonable, the speedup comes at a cost to sample quality: Many of our decoded samples retain the appearance of superpositions of training samples.\n\n**Conclusion.**\n\nWe thank the reviewer for their comments and hope that we have adequately addressed their concerns. If they are satisfied with our answers and our revision, we respectfully request that they raise their score for this paper. Otherwise, we would be pleased to continue this discussion during the reviewer-author discussion period."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074517292,
                "cdate": 1700074517292,
                "tmdate": 1700158982385,
                "mdate": 1700158982385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TArjlxNqot",
                "forum": "f5juXkyorf",
                "replyto": "HkqXFzuGlm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for Reviewer P7kJ"
                    },
                    "comment": {
                        "value": "Dear Reviewer P7kJ -- as the discussion period will close in a few days, we would greatly appreciate if you would take a look at our response to your review soon and let us know if you would like to see further changes to our manuscript. We look forward to addressing your remaining concerns before the end of discussion period. If our response was satisfactory, we ask that you consider raising your score for our submission. Thank you for your time."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489191854,
                "cdate": 1700489191854,
                "tmdate": 1700489191854,
                "mdate": 1700489191854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y0yN0AqrNl",
                "forum": "f5juXkyorf",
                "replyto": "0j0Xc0l9sf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Reviewer_P7kJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Reviewer_P7kJ"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the revision of this paper. Most of my concerns have been well-addressed. However, I also agree with Reviewer sZVo that additional baselines and metrics should be included. The experiments in the revision are much better compared to the original version but still not enough for acceptance. Also, the proposed model seems to be a little improvement to the existing work. Unfortunately, in this situation, I believe my evaluation is fair."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552544448,
                "cdate": 1700552544448,
                "tmdate": 1700552544448,
                "mdate": 1700552544448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fQGkDaDQDR",
            "forum": "f5juXkyorf",
            "replyto": "f5juXkyorf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5670/Reviewer_sZVo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5670/Reviewer_sZVo"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the score-based generative models (SGMs) - precisely, their ability to generating novel data and prevent from the memorization the training data. The usual solution is approximating the score training a neural network via score-matching. Despite promoting the generalization, the neural SGMs are costly to train and sample. Instead, the authors propose to explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. In this work, they also formulate an efficient k-NN based estimator of their score function."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has a few significant strengths overall, which I will outline below:\n1. The proposed method is easy, but elegant. \n2. The significant advantage of the proposed method is the training and sampling times and the possibility to use the standard CPU.\n3. The authors tested the method in different settings.\n4. Overall, the flow of the manuscript is well-organized."
                },
                "weaknesses": {
                    "value": "However, despite the strengths, the paper has a few major and minor weaknesses:\n1. The method is compared only on a small resolution datasets - the largest resolution have the Butterflies dataset, which still is only 128x128 (and has small number of examples). I\u2019m not sure if this model will be working well on a larger resolutions, e.g. at least 256x256 ImageNet. Maybe the comparable results to the DDPM is only a matter of not so large resolutions?\n2. The authors compared their method only against the DDPM. Could you compare with different diffusion models, which might generalize better?\n3. The presented results are not sufficient and unclear. They are unclear, because even in the Table 1, the proposed method is better on one dataset, whereas being worse on another. It would be helpful to include also other metrics than LPIPS (e.g., FID, SSIM).\n4.  In the whole paper, the Figures and Tables are too small. It is very hard to see what is in the Table and if the proposed method is better than DDPM. The presented samples are also way too small - based on this presentation I just cannot compare the proposed model against DDPM."
                },
                "questions": {
                    "value": "I would like to see especially the following experiments and improvements regarding specifically to the Weaknesses section:\n1. Please if you could include comparison on datasets having higher resolutions, like ImageNet.\n2. The comparison against others diffusion models (e.g., DDIM) is needed.\n3. I would like to see results in other metrics also (like FID or SSIM).\n4. Please if you be able to enlarge all the Figures and all the Tables."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5670/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5670/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5670/Reviewer_sZVo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5670/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789850345,
            "cdate": 1698789850345,
            "tmdate": 1699636591235,
            "mdate": 1699636591235,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pYwc8UV3uY",
                "forum": "f5juXkyorf",
                "replyto": "fQGkDaDQDR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission5670 by Reviewer sZVo"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments, which we believe will improve the quality of our manuscript. We have uploaded a revision to OpenReview, where the reviewer will find that we have incorporated many of their suggestions. In this revision, we have typeset our relevant changes in teal. We respond to their comments below.\n\n**Additional baselines.**\n\nIn Appendix D.4 of the revision, we implement and experiment with an alternative sampler which serves as an additional baseline for our model. This sampler implements the one-step procedure described in Proposition B.1 and is an extreme case of our existing strategy for reducing the number of sampling steps using a warm start from an unsmoothed CFDM and Gumbel perturbations to the distance weights in Equation (4) for analytical convenience. Proposition B.1 shows that this equivalent to drawing a batch of query points $z_{S-1}$ lying near the support of the data distribution $\\rho_1$, drawing $M$ training points $x_i$ with probabilities  $\\pi^i_\\sigma = \\textrm{softmax}\\left(-\\frac{1}{\\sigma} \\|z_{S-1} - x_i \\|^2 \\right)$, and then returning their barycenters. \n\nAs expected, this method achieves significantly higher sample throughput than our existing approach ($\\sim 4$ times faster for CIFAR generation in latent space). While the resulting outputs are reasonable, the speedup comes at a cost to sample quality: Many of our decoded samples retain the appearance of superpositions of training samples.\n\n**Additional metrics.**\n\nIn our revised manuscript, we have also reported our image generation results in terms of the kernel inception distance (KID).\n\n**Conclusion.**\n\nWe thank the reviewer for their comments and hope that we have adequately addressed their concerns. If they are satisfied with our answers and our revision, we respectfully request that they raise their score for this paper. Otherwise, we would be pleased to continue this discussion during the reviewer-author discussion period."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700082567077,
                "cdate": 1700082567077,
                "tmdate": 1700158934236,
                "mdate": 1700158934236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "czMPyIQu4N",
                "forum": "f5juXkyorf",
                "replyto": "pYwc8UV3uY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Reviewer_sZVo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Reviewer_sZVo"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the Authors for their response and revision of the paper. I went through all the added changes in the manuscript, the response and additional experiments. Unfortunately, my concerns and indicated weaknesses weren\u2019t properly addressed in the revised version of the paper.\nIn the following paragraphs, I will go through the specific points bolded in the authors response to my review.\n\n\n**1. Additional baselines.** \n\n\nThe Authors propose to consider the additional sampler (being just an extreme case of their method) as another baseline. I agree that it is a good idea to include such a scenario in the paper - to see the limits of the proposed method. However, I asked exactly for comparison with another diffusion models frameworks (like DDIM) and to compare on the higher-dimensional data (e.g., ImageNet) being much closer to the current real-world problems in diffusion models. Unfortunately, the presented experiments and comparison with such baselines don\u2019t answer the most important question to me, i.e., if the proposed method are able to scale to the higher-dimensional (more real) problems.\n\n\n**2. Additional metrics.**\n\n \nI appreciate and thank for adding the KID metric. However, it will be good to see also other metrics (like proposed FID and SSIM) to better comparison with the baselines.\n\n\n\nOverall, I thank the Authors for the response and including the revised version of the manuscript. I agree that the proposed method is interesting. However, I think that presented experiments do not show that the method is able to scale to higher-dimensional problems, at least 256x256 images (I asked for). The last I recognize as crucial for this paper. Otherwise, we don\u2019t know the practical and theoretical limits of the proposed method. Unfortunately, in this situation, I cannot raise my score."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443258431,
                "cdate": 1700443258431,
                "tmdate": 1700443258431,
                "mdate": 1700443258431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eKNZOEWlS0",
            "forum": "f5juXkyorf",
            "replyto": "f5juXkyorf",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5670/Reviewer_t4Fr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5670/Reviewer_t4Fr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new variant of diffusion models based on the fact that the score function can be equivalently written as the expectation over score functions of conditional distributions. This expectation can be written in closed-form as a weighted sum over all training points, thus the \"closed-form\" diffusion models. The paper then proposes to smooth the closed-form scores by integrating it over small noise perturbation of the inputs, akin to the denoising score matching approach. Due to the closed-form expression, sampling can be implemented without a parametric approximation to the score function. Because each evaluation of the score requires going through all training examples, a nearest-neighbor estimator is used to reduce the computation cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper is clear and pleasant to read. \n* The experimental study is carefully designed and investigated most questions that I could think of about the closed-form model.\n* It's interesting that the proposed method can fill in the gaps between the sparse training samples in the 3D point cloud experiment (although it's unclear to me why the method was able to do so--see below)."
                },
                "weaknesses": {
                    "value": "* The proposed method is not well-positioned in literature. It's worth pointing out that the key idea of representing the marginal score as the expectation of scores of distributions conditioned on inputs is actually quite well-known. It has been used, for example, to develop the original denoising score matching objective [1]. It is also used in the literature as \"score-interpolation\" [2]. I just named a few but I would recommend the authors to do a thorough literature review as I believe this property is used in many more works. \n\n* The definition of the notation \\hat{c}_k (baycenters) is missing. \n\n* The exponential dependence of the sampling error on T is concerning. Although empirical evidence is provided to justify that this error bound is pessimistic, it also renders the bound unnecessary. Meanwhile, it's unclear if the conclusion that under sigma < 0.4, a large starting T is harmless will generalize to other datasets. \n\n* The 3D point cloud experiment is interesting but I don't understand why the proposed method fills the gap there. Could the authors elaborate on this?\n\n* The practical utility of the proposed closed-form models is also unclear. Given that the model can only sample from baycenters of data point tuples, is there a clear case where we would prefer such a model over a trained score model?\n\n* This is minor, but the readability of section 3 can be greatly improved if not going to the notation convention used by rectified flow, as the proposed method can be described using the standard diffusion model formulation (where the time is reversed and stochastic transition is used). \n\n[1] Vincent, Pascal. \"A connection between score matching and denoising autoencoders.\" Neural computation 23.7 (2011): 1661-1674.\n\n[2] Dieleman, Sander, et al. \"Continuous diffusion for categorical data.\" arXiv preprint arXiv:2211.15089 (2022)."
                },
                "questions": {
                    "value": "Please see questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5670/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841823603,
            "cdate": 1698841823603,
            "tmdate": 1699636591148,
            "mdate": 1699636591148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jWeNdrxuQL",
                "forum": "f5juXkyorf",
                "replyto": "eKNZOEWlS0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Official Review of Submission5670 by Reviewer t4Fr"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments, which will improve the quality of our manuscript. We have uploaded a revision to OpenReview, where the reviewer will find that we have incorporated many of their suggestions. In this revision, we have typeset our relevant changes in teal. We respond to the reviewer's comments below.\n\n**Positioning in the literature.**\n\nThe closed-form expression for the score of $p_t$ (Equations (1) and (2) in our manuscript) is indeed well-known, with similar formulas having appeared in the empirical Bayes literature as early as 1961 in Miyasawa\u2019s \u201cAn empirical Bayes estimator of the mean of a normal population.\u201d \n\nWe have updated Sections 1 and 3 to highlight this point and include several references to appearances of this formula in the statistics and ML literature.\n\nHowever, we emphasize that this formula is not one of our contributions. Rather, we use this formula as a starting point to develop a score-based generative model (SGM) that generalizes without requiring trainable neural approximations, and we introduce two strategies in Sections 5.2 and 5.3 (starting sampling at $T>0$ and the NN-based score estimator) that greatly accelerate our sampler. \n\nWe have updated our statement of contributions in Section 1 to further emphasize this point.\n\n**Definition of notation $\\bar{c}_k$ for barycenters.**\n\nWe thank the reviewer for pointing out this oversight and have defined this notation in Section 4.2 of the revised manuscript.\n\n**Exponential dependence of the sampling error on $T$.**\n\nAs the reviewer notes, the error bound that in Theorem 5.2 is pessimistic in its dependence on $T$. Given that our experiments in Section 6.2 show little accuracy loss for practical values of $\\sigma$, we believe this bound to be loose as a consequence of the elementary techniques used to prove it. We would be interested in alternative methods that lead to an error bound which reflects the small error that we observe in practice.\n\nHowever, the bound in Theorem 5.2 also indicates that the error incurred by starting at $T>0$ scales linearly with $\\sigma$. When $\\sigma=0$, the smoothed and unsmoothed CFDMs are identical, and one can sample the unsmoothed CFDM (i.e. mixture of Gaussians) at any time without incurring error. For $\\sigma>0$, this is no longer true \u2013 but it is not obvious how the error incurred by sampling the unsmoothed CFDM at $T>0$ depends on $\\sigma$. Our result confirms that this dependence is linear, so small values of $\\sigma$ lead to small errors. \n\nWe have updated our discussion in Section 5.2 to clarify this point, namely that the interest of the theorem is the $\\sigma$ dependence rather than the $T$ dependence.\n\n**Generality of conclusion that under $\\sigma<0.4$, a large starting $T$ is harmless.**\n\nThis was true across all datasets considered in this paper\u2014not just the checkerboard dataset used in Section 6.2. \n\nFor example, in Sections 6.3 and 6.4, we used a start time of $T=0.98$ for all image generation experiments with little degradation in sample quality. The fact that $[0,0.4]$ is the appropriate range for $\\sigma$ is partially a consequence of having rescaled the training images/latents to lie within the unit ball before sampling and then reversed this rescaling on the generated samples. We have updated our discussion in Section 6.2 to clarify this point. \n\nSubject to time constraints within the discussion period, we would be happy to run any further experiments the reviewer has in mind to further confirm this observation.\n\n**Filling in gaps between sparse manifold samples in the 3D point cloud experiment.**\n\nWe have updated our discussion of this experiment in Section 6.1 to clarify why our method fills in gaps between sparse samples from the surface.\n\n**Practical utility of CFDMs.**\n\nThe benefits of our method over a neural SGM are twofold:\n\n1. Computationally: Our method requires no training and can be sampled very quickly while running on consumer-grade CPUs.\n2. Theoretically: Unlike with neural SGMs, we are able to characterize the support of our model\u2019s samples (Theorem 5.1) and can read off the weights of the training points that contributed to each generated sample.\n\nAs the reviewer highlights, our model samples barycenters of training points. In light of this fact, sampling in an appropriately-structured latent space (one where the data manifold is \u201clocally linear\u201d) is likely the most realistic path towards obtaining sample quality comparable to neural SGMs on high-resolution image generation tasks. Our CIFAR results in Section 6.4 indicate that this is a promising direction, and we continue to work toward scaling our method up to larger and higher-resolution datasets.\n\n**Conclusion.**\n\nWe thank the reviewer for their comments and hope that we have addressed their concerns. If they are satisfied with our answers and our revision, we respectfully request that they raise their score. Otherwise, we would be pleased to continue this discussion."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074027099,
                "cdate": 1700074027099,
                "tmdate": 1700158907434,
                "mdate": 1700158907434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SThCIEiDzl",
                "forum": "f5juXkyorf",
                "replyto": "eKNZOEWlS0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5670/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder for Reviewer t4Fr"
                    },
                    "comment": {
                        "value": "Dear Reviewer t4Fr -- as the discussion period will close in a few days, we would greatly appreciate if you would take a look at our response to your review soon and let us know if you would like to see further changes to our manuscript. We look forward to addressing your remaining concerns before the end of discussion period. If our response was satisfactory, we ask that you consider raising your score for our submission. Thank you for your time."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5670/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489129147,
                "cdate": 1700489129147,
                "tmdate": 1700489129147,
                "mdate": 1700489129147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]