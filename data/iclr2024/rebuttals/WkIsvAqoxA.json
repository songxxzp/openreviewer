[
    {
        "title": "Dolfin: Diffusion Layout Transformers without Autoencoder"
    },
    {
        "review": {
            "id": "YUQ4gdEVHu",
            "forum": "WkIsvAqoxA",
            "replyto": "WkIsvAqoxA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission505/Reviewer_qsqM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission505/Reviewer_qsqM"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a diffusion-based layout generation model utilizing transformers. It removes the autoencoder layer typically used in a diffusion model for layout/image generation and directly operates on the layout input space. The proposed two model variants (Dolfin and Dolfin-AR) empirically improves performance across various metrics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is clearly written and easy to follow. \n2. The proposed models notably improve quantitative results against generative layout benchmarks."
                },
                "weaknesses": {
                    "value": "1. The main difference with previous models is by operating directly on the input space of layouts (the coordinates and corresponding class labels) instead of processing the layouts with VAE/dedicated modules. However the reasons for the brought-in performance gains are not sufficiently justified.  \n2. \"enhancing transparency and interoperability\" is overclaimed since it is a property of the standard diffusion process itself. \n3. From the paper presentation it is not clear what are the modifications to the original DiT transformer other than omitting a category input."
                },
                "questions": {
                    "value": "1. Why is operating on the original layout space better, especially when processing such data with dedicated neural modules is quite standard ? e.g. other than mentioned related works also standard in other generative models such as [1]. Could it be that the training data is insufficient? \n2. Please check the metric arrow directions in Table 3, 4, 5.\n3. In Fig.6, the generated samples exhibit some obvious unnaturalness (e.g. blue frames, bottom left, the window lines). Similar patterns exist in Fig.13. Is it because of insufficient training ? Could you compare it with PLay? \n\n[1] GLIGEN: Open-Set Grounded Text-to-Image Generation"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698477193014,
            "cdate": 1698477193014,
            "tmdate": 1699635977050,
            "mdate": 1699635977050,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6WOGrq1HJt",
                "forum": "WkIsvAqoxA",
                "replyto": "YUQ4gdEVHu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission505/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you.\n\nBy directly operating on the input space, the model can enhance transparency, which is beneficial for performance. Additionally, failing to operate directly on the original space leads to unsatisfactory results in line segment generation. In diffusion-based layout generation, previous works have focused on latent space rather than the original space.\n\n**From the paper presentation it is not clear what are the modifications to the original DiT transformer other than omitting a category input.**\n\nWe mainly do the following two modifications:\n\n(1) we omit the category input\n\n(2) we remove the VAE encoder and decoder that match the data to the original space.\n\n**Why is operating on the original layout space better, especially when processing such data with dedicated neural modules is quite standard ? e.g. other than mentioned related works also standard in other generative models such as [1]. Could it be that the training data is insufficient?**\n\nPlease refer to the first paragraph that explains why operating on original space is better.In our paper, we focus on vectorized layout generation from scratch, using the same dataset as other works dealing with similar tasks. However, works like GLIGEN focus on generating images with text and additional conditions, a completely different task.\n\n**In Fig.6, the generated samples exhibit some obvious unnaturalness (e.g. blue frames, bottom left, the window lines). Similar patterns exist in Fig.13. Is it because of insufficient training ? Could you compare it with PLay?**\n\nThe blue and red frames are added to make the white-background images clearer. The generated line segments generally follow the distribution of our training data (ShanghaiTech Wireframe dataset). In other works, such as PLay, they do not support direct operation on the input space, making it difficult to handle line segment tasks. What's more, they do not provide open-source code, which makes it difficult to reimplement their algorithm on new tasks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161775813,
                "cdate": 1700161775813,
                "tmdate": 1700161775813,
                "mdate": 1700161775813,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iZ6r6yCMlB",
            "forum": "WkIsvAqoxA",
            "replyto": "WkIsvAqoxA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission505/Reviewer_SyW2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission505/Reviewer_SyW2"
            ],
            "content": {
                "summary": {
                    "value": "Diffusion Layout Transformers without Autoencoder (Dolfin) is proposed, with an efficient bi-directional (non-causal joint) sequence representation. An autoregressive diffusion model (Dolfin-AR) is also proposed to capture rich semantic correlations for the neighboring objects. The method is validated on 2D layout generation and line segment generation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- not requiring the autoencoder layer in the diffusion model\n- autoregressive diffusion model to capture the rich semantic correlation between objects/items\n- experiment on generating geometric structures beyond layout, such as line segments"
                },
                "weaknesses": {
                    "value": "- not using auto encoder is not a new idea, Imagen model is processing directly on pixels\n- there is no intuition on why auot-regressive design leads to better semantic correlation, although this is observed from experiments\n- not many baselines comparison for the line segment generation"
                },
                "questions": {
                    "value": "- explain the intuition of the advantage of auot-regressive design\n- compare with image diffusion results for line generation, line representation can be obtained followed by a line detector\n- each object in a layout is represented by a 4 \u00d7 4 tensor, why we need 4 entires for the entire layout width/height? Once it's normalized, is that always -1/1?\n- in Algorithm 1 and 2, is it better to use a different index than \"t\" in the for loop? The for loop index has different meaning than the diffusion step t."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698520008886,
            "cdate": 1698520008886,
            "tmdate": 1699635976981,
            "mdate": 1699635976981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "14UFk6T9yc",
                "forum": "WkIsvAqoxA",
                "replyto": "iZ6r6yCMlB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission505/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you.\n\n**Q1: not using auto encoder is not a new idea, Imagen model is processing directly on pixels**\n\nThe original resolution in Imagen is the 1024x1024 images, yet they employ a super-resolution model to upscale from 64x64 to the original 1024x1024 resolution. Therefore, claiming they operate on the original resolution is inaccurate. Furthermore, our work focuses mainly on vectorized structured data, which differs from the pixel data in Imagen.\n\n**Q2: explain the intuition of the advantage of auot-regressive design**\n\nBy implementing the autoregressive model, each token, corresponding to a bounding box, has a tighter association with previous tokens, enhancing the model's alignment capabilities.\n\n**Q3: compare with image diffusion results for line generation, line representation can be obtained followed by a line detector**\n\nWe generate the line segments from scratch, whereas line detectors require an original image to generate them. To our knowledge, there is no existing work focused on this generation task.\n\n**Q4: each object in a layout is represented by a 4 \u00d7 4 tensor, why we need 4 entires for the entire layout width/height? Once it's normalized, is that always -1/1?**\n\nThe layouts in the datasets have different heights and widths. There is no parameter set to normalize them. Instead, the actual heights and widths are divided by 1000."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700161203416,
                "cdate": 1700161203416,
                "tmdate": 1700161203416,
                "mdate": 1700161203416,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AEInTzfRbi",
            "forum": "WkIsvAqoxA",
            "replyto": "WkIsvAqoxA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission505/Reviewer_eh4o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission505/Reviewer_eh4o"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"Dolfin,\" a generative model that uses a transformer-based diffusion process for layout generation. The proposed method directly applies on the input space of the geometric objects. The method benefits from bi-directional representation and consists of two versions, the non-auto regressive version that process all tokens simultaneously and the auto-regressive version that predicts each token sequentially. The authors provided experiments on RICO and PublayNet datasets for layout generation task as well as additional experiments on Line Segments Generation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is detailed and easy to follow.\n\nAdditional experiments on line segment generation can be useful to consider along with the other tasks."
                },
                "weaknesses": {
                    "value": "The paper offers potential value to the community. However, concerns regarding its novelty and the robustness of its experimental evaluations need to be addressed for it to be ready for publication.\n\nNovelty: The core proposition of the paper, which involves the utilization of the input coordinate space for layout design generation through continuous diffusion models, is not entirely novel. Similar approaches have been discussed in prior works such as [1, 2].\n\nExperiments and Comparison: The experiments presented currently lack comprehensiveness. The results in Table 1 do not facilitate a fair comparison between the proposed method and existing methods. Although Tables 2 and 3 provide more data points, they restrict their focus to MaxIoU and Alignment scores. Furthermore, the results suggest that the proposed method underperforms compared to the baselines. This underscores the need for a more in-depth analysis and comparison.\n\n\n[1] LEGO-Net: Learning Regular Rearrangements of Objects in Rooms, CVPR 2023\n[2] HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising, CVPR 2023"
                },
                "questions": {
                    "value": "Considering the large batch sizes which are used in the experiments (10k and 6k) compared to the conventional batch sizes up to 2048, adding a table on the effect of the batch size on the final result can be very insightful.\n\nI couldn't find any direct comparison on pros and cons of the Dolfin and Dolfin-AR. It is better if you also add both versions to other tables as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801433397,
            "cdate": 1698801433397,
            "tmdate": 1699635976898,
            "mdate": 1699635976898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fR6bPw9CL5",
                "forum": "WkIsvAqoxA",
                "replyto": "AEInTzfRbi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission505/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you. We hope our responses adequately address the concerns regarding the novelty of our work. \n\nBefore the advent of diffusion models and the widespread use of transformers, structural data generation mainly relied on CNN-based models such as VAE and GAN. However, these methods often operated on latent spaces, potentially leading to the loss of the data's structural features. In early diffusion image model development, U-Net was commonly used as the denoising backbone instead of Transformers, requiring the mapping of data to a continuous latent space or the application of techniques to manage aspects of discrete data. Following the introduction of Transformer-based diffusion (DiT), a series of studies have been inspired by it. However, they still adhere to DiT's original design, mapping inputs into a continuous latent space. We propose a transformer-based diffusion model dedicated to general layout generation. This model operates directly on the space of geometric items, improving explainability and facilitating easier adaptation by other models.\n\n**Q1: The core proposition of the paper, which involves the utilization of the input coordinate space for layout design generation through continuous diffusion models, is not entirely novel. Similar approaches have been discussed in prior works such as [1, 2].**\n\nLEGO [1] mainly focuses on using additional constraints to rearrange the vectorized input data instead of generating vectorized data from scratch. HouseDiffusion [2] uses graph-based layouts instead of bounding boxes, diverging from our method. Besides, it deals with layouts in separate (continuous and discrete) sections, potentially impacting overall structural coherence.\n\n**Q2: Experiments and Comparison: The experiments presented currently lack comprehensiveness. The results in Table 1 do not facilitate a fair comparison between the proposed method and existing methods. Although Tables 2 and 3 provide more data points, they restrict their focus to MaxIoU and Alignment scores. Furthermore, the results suggest that the proposed method underperforms compared to the baselines. This underscores the need for a more in-depth analysis and comparison.**\n\nFor a fair comparison, we use the data from papers that provide a clear methodology for computing metrics. Play and UniLayout are not open-sourced. The authors of Play responded to us with details. Because implementation details lead to significantly different results, for example, adopting Play's approach using 1024 samples to compute scores results in an FID score of 1.73 for conditional generation (conditioned on category), which is better than the number presented in the LayoutDM paper. However, using only 512 samples, the score becomes 4.21, which is worse than LayoutDM. While our model underperforms the baselines in some circumstances, our method generally performs better overall.\n\n**Q3: Considering the large batch sizes which are used in the experiments (10k and 6k) compared to the conventional batch sizes up to 2048, adding a table on the effect of the batch size on the final result can be very insightful.**\n\nWe have tried to train the model using batch sizes of 10k, 5k, 3k, 1k. The outcomes indicate that all configurations yield similar FID scores within the range of \u00b1 0.2, which can be attributed to the randomness of the diffusion sampling process.\n\n**Q4: I couldn't find any direct comparison on pros and cons of the Dolfin and Dolfin-AR. It is better if you also add both versions to other tables as well.**\n\nFor unconditional generation, we provide results for Doldin and Dolfin-AR. Dolfin outperforms in metrics like the FID score, while Dolfin-AR excels in the alignment score. Regarding conditional generation, since Dolfin-AR does not support it, we only present results for Dolfin."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700160639456,
                "cdate": 1700160639456,
                "tmdate": 1700160639456,
                "mdate": 1700160639456,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]