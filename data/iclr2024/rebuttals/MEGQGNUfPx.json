[
    {
        "title": "The Effectiveness of Random Forgetting for Robust Generalization"
    },
    {
        "review": {
            "id": "zD6gDu31la",
            "forum": "MEGQGNUfPx",
            "replyto": "MEGQGNUfPx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8178/Reviewer_FBLx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8178/Reviewer_FBLx"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to address the generalization gap in adversarial training. The authors exploit the random forgetting to adjust the  weights of models. Three datasets and two adversarial attacks are used to evaluate the proposed method. The experimental results show that the method can improve the robust accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work proposed a new method to solve the generalization gap in adversarial training.  The perspective of random forgetting is interesting.\n\n2. The introduction to the forgetting mechanism in Methodology is clear."
                },
                "weaknesses": {
                    "value": "1. The description in the caption of Figure 2 is inconsistent with the content of the image. The former states that the consolidation phase is behind the forgetting phase, while the latter expresses that theconsolidation phase is before the forgetting phase. In addition, please present more clearly in the figure what the generalized information is.\n\n2. In Figure 1, FOMO is only compared with standard adversarial training, but not with methods that aim to reduce the generalization gap (such as AWP). This result may not appreciably represent the effectiveness of the proposed method.\n\n3. The authors use two adversaial attacks to evaluate the proposed method, they can consider more adversarial attacks (such as L2-norm CW, DDN) to conduct a more comprehensive evaluation.\n\n4. Figures can be clearer and more aesthetically pleasing."
                },
                "questions": {
                    "value": "Please see weaknesses.\n\n============After rebuttal============\nThe authors provide adequate explanations for most of my questions, so I am willing to raise the rating score to 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8178/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8178/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8178/Reviewer_FBLx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698559158209,
            "cdate": 1698559158209,
            "tmdate": 1700641818773,
            "mdate": 1700641818773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y9CIaCARDl",
                "forum": "MEGQGNUfPx",
                "replyto": "zD6gDu31la",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to  Reviewer FBLx"
                    },
                    "comment": {
                        "value": "> The description in the caption of Figure 2 is inconsistent with the content of the image. The former states that the consolidation phase is behind the forgetting phase, while the latter expresses that the consolidation phase is before the forgetting phase. \n\nWe appreciate your valuable feedback on our manuscript. We acknowledge the inconsistency in the description of Figure 2, specifically regarding the positioning of the consolidation and forgetting phases. In the revised manuscript, we have ensured that the caption accurately reflects the chronological order depicted in Figure 2.\n\n> In Figure 1, FOMO is only compared with standard adversarial training, but not with methods that aim to reduce the generalization gap (such as AWP). This result may not appreciably represent the effectiveness of the proposed method.\n\nFigure 1 primarily serves as an illustrative aid to help readers grasp the concept of robust overfitting in adversarial training more effectively. To ensure a more comprehensive comparison, we have meticulously presented relevant details in Table 1. This table includes a dedicated section for comparing FOMO with methods like AWP and KD+SWA, which specifically aim to reduce the generalization gap.  In light of this, we have refrained from displaying the direct comparison with AWP in Figure 1 to avoid visual clutter. We value the reviewer's perspective, and in response, we will incorporate these changes in the revised version.\n\n> The authors use two adversaial attacks to evaluate the proposed method, they can consider more adversarial attacks (such as L2-norm CW, DDN) to conduct a more comprehensive evaluation.\n\nBased on reviewer's feedback, we have evaluated our proposed method under CW attack for comprehensive evaluation. Table 10  (Appendix section A8) presents the evaluation results under CW attack on CIFAR-10/100 using the PreActResNet-18 architecture. The robust accuracy is assessed under CW attacks, and checkpoints with the best robust accuracy under PGD-20 attacks on the validation set are selected for comparison. FOMO consistently outperforms both baselines across different datasets and attack scenarios, demonstrating its effectiveness in enhancing robust accuracy under CW attacks. The best and final robust accuracies for FOMO are generally close, indicating that FOMO maintains its performance during training and does not suffer from significant overfitting under these attacks. These results emphasize the promising performance of FOMO in mitigating adversarial attacks, particularly under CW attack.\n\n| Dataset   | Norm   | Radius          | Methods | CW Attack  |                 |\n|-----------|--------|-----------------|---------|------------|-----------------|\n|           |        |                 |         | **Best**   | **Final**       |\n|-----------|--------|-----------------|---------|------------|-----------------|\n| CIFAR-10  | $\\ell_2$ | $\\frac{128}{255}$ | PGD-AT  | 67.18      | 64.29           |\n|           |        |                 | KD+SWA  | 68.87      | 68.90           |\n|           |        |                 | FOMO    | **70.52**  | **70.35**       |\n| CIFAR-10  | $\\ell_\\infty$ | $\\frac{8}{255}$ | PGD-AT  | 47.00      | 39.96           |\n|           |        |                 | KD+SWA  | 49.35      | 49.44           |\n|           |        |                 | FOMO    | **52.14**  | **51.95**       |\n| CIFAR-100 | $\\ell_2$ | $\\frac{128}{255}$ | PGD-AT  | 37.16      | 33.43           |\n|           |        |                 | KD+SWA  | 40.56      | 40.61           |\n|           |        |                 | FOMO    | **42.73**  | **42.35**       |\n| CIFAR-100 | $\\ell_\\infty$ | $\\frac{8}{255}$ | PGD-AT  | 22.73      | 18.11           |\n|           |        |                 | KD+SWA  | 25.42      | 25.35           |\n|           |        |                 | FOMO    | **26.98**  | **26.61**       |\n\n> Figures can be clearer and more aesthetically pleasing.\n\nThank you for your feedback on the figures. We'll work on improving their clarity and visual appeal in the final version of the paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607780692,
                "cdate": 1700607780692,
                "tmdate": 1700607780692,
                "mdate": 1700607780692,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aykEC1SMmR",
                "forum": "MEGQGNUfPx",
                "replyto": "y9CIaCARDl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8178/Reviewer_FBLx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8178/Reviewer_FBLx"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Many thanks to authors for their careful responses. The authors has revised and improved the description of the figures and they add additional experimental results. I think the authors provide adequate explanations for most of my questions, so I will raise the rating score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641744910,
                "cdate": 1700641744910,
                "tmdate": 1700641744910,
                "mdate": 1700641744910,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fvOpNBFB8X",
            "forum": "MEGQGNUfPx",
            "replyto": "MEGQGNUfPx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8178/Reviewer_WNii"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8178/Reviewer_WNii"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method, FOMO, to deal with the adversarial overfiting issue. The proposed method alternates between the forgetting phase and the relearning phase."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is in good structure and easy to follow. \n\nThe topic, which is to deal with adversarial overfitting, is interesting.\n\nThe method is simple yet effective. \n\nAn ablation study is provided."
                },
                "weaknesses": {
                    "value": "The description of the method is too intuitive. \n\nIn Table 1, the delta, which measures the adversarial overfitting, never favors the proposed method. This cannot show that the proposed method is good at dealing with adversarial overfitting."
                },
                "questions": {
                    "value": "In this paper, the author only shows the result under a combination of white box and black box attacks, i.e., Autoattack. However, this cannot show \"the efficacy of FOMO against the black box and white box attacks\". Standard Autoattack has 4 adversarial attacks: three white-box attacks and one black-box attack. It is possible that FOMO has a strong resistance against white-box attacks while being vulnerable to the black-box attack."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8178/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8178/Reviewer_WNii",
                        "ICLR.cc/2024/Conference/Submission8178/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807201882,
            "cdate": 1698807201882,
            "tmdate": 1700624428087,
            "mdate": 1700624428087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7e1VGaRpGh",
                "forum": "MEGQGNUfPx",
                "replyto": "fvOpNBFB8X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer WNii"
                    },
                    "comment": {
                        "value": ">  The description of the method is too intuitive.\n\nWe appreciate the reviewer's feedback and the opportunity to clarify the description of our method. Our FOMO approach is founded on the premise that emulating active forgetting, a phenomenon observed in the human brain, can effectively mitigate robust overfitting in Adversarial Training (AT). Our method synthesizes selective forgetting in later layers, consolidation for long-term memory, and interleaved training as mechanisms aimed at enhancing generalization.\n\nWe want to emphasize that our method's development wasn't solely intuitive; it is substantiated by evidence from active forgetting studies in neuroscience, which we extensively discuss in Section 2.3 and the methodology section. Additionally, we conducted rigorous empirical evaluations across multiple datasets to validate our hypothesis. The discernible enhancements in robustness and generalization provide empirical evidence reinforcing the effectiveness of our method.\n\nWe believe that while our approach may seem intuitive in its concept, its implementation and validation have been rooted in both neuroscience understanding and empirical substantiation. We remain open to further discussions or specific areas where additional clarification might be beneficial.\n\n> In Table 1, the delta, which measures the adversarial overfitting, never favors the proposed method. This cannot show that the proposed method is good at dealing with adversarial overfitting.\n\nWe would like to clarify that delta alone may not be a comprehensive metric for quantifying robust overfitting, as some methods with strict regularization (such as KD+SWA) during training might exhibit lower delta values at the expense of achieving less-than-optimal the best robust accuracy. FOMO is designed to strike a balanced tradeoff between delta and best robust accuracy.\n\nCompared to KD+SWA, FOMO exhibits a notable percentage increase in the best robust accuracy: 2.08% on CIFAR-10, 2.01% on CIFAR-100, and 5.98% on SVHN, all while maintaining a minimal difference between best and final robust accuracy. These percentages highlight FOMO's effectiveness in not only mitigating adversarial overfitting but also in significantly enhancing the overall robustness of the model compared to the baseline.\n\n> In this paper, the author only shows the result under a combination of white box and black box attacks, i.e., Autoattack. However, this cannot show \"the efficacy of FOMO against the black box and white box attacks\". Standard Autoattack has 4 adversarial attacks: three white-box attacks and one black-box attack. It is possible that FOMO has a strong resistance against white-box attacks while being vulnerable to the black-box attack.\n\nThank you for your question. In response to your observation, we have made revisions to the paper to explicitly address \"the efficacy of FOMO against AutoAttacks.\" We present results on AutoAttack, a combination of white-box and black-box attacks, mirroring our considered baselines. It's crucial to note that baseline methods, such as KD+SWA and AWP, similarly lack separate results for white-box and black-box attacks. To facilitate a fair comparison, we also present the AutoAttack results. We acknowledge this and will update the paper to reflect this clarity."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608077637,
                "cdate": 1700608077637,
                "tmdate": 1700608077637,
                "mdate": 1700608077637,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UO6vOHeGec",
            "forum": "MEGQGNUfPx",
            "replyto": "MEGQGNUfPx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8178/Reviewer_Jeha"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8178/Reviewer_Jeha"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of robust overfitting in adversarial training of deep neural networks, which affects their generalization performance. The authors propose a new method called \"Forget to Mitigate Overfitting (FOMO),\" drawing inspiration from the brain's mechanism of active forgetting. FOMO operates by periodically resetting a subset of the network's weights to promote the learning of more generalizable features. The approach suggests a promising direction for enhancing neural network robustness against adversarial attacks by mitigating overfitting through controlled forgetting and relearning. Experimental results show that FOMO is a promising method to improve model robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The authors conducted comprehensive experiments to demonstrate the effectiveness of FOMO. The proposed method is effective and outperforms the existing method, according to Table 3 and other experimental results.  \n* The proposed method is intuitive and easy to implement."
                },
                "weaknesses": {
                    "value": "* My main concern is that the proposed method seems to be heuristic and empirical. there is not enough discussion on its intuition or theoretical foundation.\n* I don't think the running time and convergence analysis are well-studied in this paper, the authors may need to provide a table showing how many epochs are needed to converge and compare the running time with the existing methods. \n* Minor: Please refrain from only using color to distinguish curves and bars as in Figures 3, 4, 5, and 6, as it is not friendly to readers with color blindness.\n* Minor: Missing reference on robust generalization: Zhang, et al. \"The limitations of adversarial training and the blind-spot attack.\" ICLR 2019."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8178/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699421953368,
            "cdate": 1699421953368,
            "tmdate": 1699637013905,
            "mdate": 1699637013905,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UaQMTGty0c",
                "forum": "MEGQGNUfPx",
                "replyto": "UO6vOHeGec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8178/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8178/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Jeha"
                    },
                    "comment": {
                        "value": "> My main concern is that the proposed method seems to be heuristic and empirical. there is not enough discussion on its intuition or theoretical foundation.\n\nWe genuinely value the reviewer's insights. Our FOMO approach stems from the hypothesis that simulating active forgetting, as observed in the human brain, can effectively address robust overfitting in Adversarial Training (AT). Our method integrates selective forgetting in later layers, consolidation for long-term memory, and interleaved training as mechanisms to improve generalization. Our rationale is supported by evidence from active forgetting studies in neuroscience, which is extensively elaborated in Section 2.3 and the methodology section. Furthermore, we have empirically validated our hypothesis by conducting comprehensive evaluations across multiple datasets. The noticeable improvements in robustness and generalization serve as empirical evidence reinforcing the effectiveness of our method.\nWhile we acknowledge the importance of theoretical analysis, our primary focus was to showcase the practical efficacy of our proposed method. However, subsequent to our empirical findings, a theoretical exploration can further deepen our understandings of the learning mechanisms employed by FOMO. This aspect is explicitly discussed in our future work section, emphasizing the necessity of further theoretical analysis to complement our empirical findings.\n\n> I don't think the running time and convergence analysis are well-studied in this paper, the authors may need to provide a table showing how many epochs are needed to converge and compare the running time with the existing methods.\n\nWe appreciate your suggestion and thus, we have conducted additional experiments and present the training time (per epoch) for several methods in our revised manuscript. To ensure a fair comparison, all methods were integrated into a universal training framework, and each test was performed on a single NVIDIA GeForce 3090 GPU. Table 9 (Appendix section A7) in the revised manuscript now includes the required information.\n\nNotably, FOMO and our baselines were trained for the same number of epochs (i.e., 200 epochs for CIFAR-10/100). From the table, it is evident that FOMO imposes nearly no extra computational cost compared to vanilla PGD-AT, with specific values being 137.1s for FOMO and 132.6s for vanilla PGD-AT per epoch. This implies that FOMO is an efficient training method in practical terms. It is important to highlight that KD+SWA, a formidable method designed to counter robust overfitting, comes with an increased computational cost. This arises from its approach, which entails the pretraining of both a robust and a non-robust classifier, serving as the adversarial and standard teacher, respectively. Additionally, the method incorporates the process of distilling knowledge from these teachers. Moreover, KD+SWA employs stochastic weight averaging to smoothen the weights of the model, further contributing to its computational demands.We believe that this addition enhances the practical insights into the efficiency of FOMO and its comparison with existing methods. We would be happy to address any remaining concern or suggestion.\n\n### Table: Training Time per Epoch on CIFAR-10 under \u03b5\u221e = 8/255 Perturbation (PreActResNet-18)\n\n| Methods  | Training Time per Epoch (s) |\n| ------------- | ------------------------|\n| PGD-AT   | 132.6                    |\n| WA           | 133.1                    |\n| KD+SWA | 132.6 (pretraining of AT) +16.5 +141.7 |\n| AWP         | 143.8                   |\n| FOMO      | 137.1                   |\n\n> Minor: Please refrain from only using color to distinguish curves and bars as in Figures 3, 4, 5, and 6, as it is not friendly to readers with color blindness.\n\nWe value your feedback. In the final version, we'll modify these figures to ensure they're accessible to readers with color vision deficiencies. Thank you for highlighting this important consideration.\n\n> Minor: Missing reference on robust generalization: Zhang, et al. \"The limitations of adversarial training and the blind-spot attack.\" ICLR 2019.\n\nThank you for your suggestions. We have added this in the related work under robust generalization."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8178/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607253290,
                "cdate": 1700607253290,
                "tmdate": 1700607253290,
                "mdate": 1700607253290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]