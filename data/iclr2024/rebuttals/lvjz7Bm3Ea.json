[
    {
        "title": "ChronoGAM: An End-to-End One-Class Time Series Gaussian Mixture Model"
    },
    {
        "review": {
            "id": "Sooinjh3Y2",
            "forum": "lvjz7Bm3Ea",
            "replyto": "lvjz7Bm3Ea",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7095/Reviewer_ahgt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7095/Reviewer_ahgt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method specifically targeted at time series data in the one-class learning problem. The proposed method consists of an autoencoder type with a convolutional layer that can capture the basic features of time series and a Gaussian mixture model that can capture multiple cluster structures in the latent space.\nThe authors have embedded some innovations in the proposed method to solve some problems that related methods have had in practical use. Specifically,\n- The proposed method is a data-driven method using data energy instead of manual thresholding, which is often required in one-class learning.\n- Instead of the ${\\rm exp}$ function, which causes instability in the calculation of data energy, the proposed method improves the stability of the calculation by proposing an ${\\rm explu}$ function.\n\nFurthermore, the paper demonstrates the practical usefulness of the proposed method with objective measures for a remarkably large number of experimental data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes a very solid way to solve in a reasonable way several problems that have been practically and empirically intractable in the latest developments in one-class learning research. In particular, the following points can be listed as notable strengths: \n- The paper is very well written, the arguments are coherent, and the organization is well thought out for a diverse audience. The authors' perspectives on the latest developments and current issues are well organized, especially in the context of one-class learning.\n- The objective evaluation experiments that demonstrate the usefulness and effectiveness of the proposed method are notably large, comprehensive, and fair. It is suggested that the models assumed by the proposed method (time series representation by convolutional auto encoder and cluster structure in latent space by Gaussian mixture model) fit well on many real data."
                },
                "weaknesses": {
                    "value": "My concern is that the discussion of the validity of the improvements from the Deep Autoencoding Gaussian Mixture Model (DAGMM) [Zong+2018], which is the inspiration for this study, may be somewhat lacking. The model used in this paper can be viewed as a legitimate successor to DAGMM from an overarching and conceptual perspective. Therefore, I was reading the manuscript with the expectation that the manuscript would carefully explain the validity of each improvement from it. Certainly, I agree that empirically and experimentally those improvements work well, as the authors have shown us in their evaluation experiments, but I am left with some points that are not clear to me why they work well from the perspective of scientific and technological development. My concerns can be summarized as follows.\n- This paper introduces a convolutional approach to time series modeling with reference (maybe, as a standard method described in [Wang+2017]) to existing research on time series classification problems. It seems to me that the strengths of this approach (i.e., for what time series can well-fitted features be captured) and the weaknesses (what time series are difficult to represent) have not been adequately discussed.\n- I am having difficulty understanding the validity of the explu function, one of the devices of the proposed method. explu function can be seen as an approximation/substitution for computational stability of what is originally an exp function, but I have not been able to find an explanation of what the sacrifice is.\n\nThese two concerns could be misunderstandings due to my lack of understanding. If there is any misunderstanding on my part, I would be very grateful if the authors could clear it up in their response."
                },
                "questions": {
                    "value": "I would like to ask the following questions to see if my understanding of the two concerns I raised in the weaknesses section above is incorrect.\n\n(1) Regarding the convolution approach that this paper employs to represent time series.\n- Is the adoption of the convolution approach the recommended setting by the authors? Or is it adopted because it is known as the most basic and standard in the field concerned? For example, if the user has prior knowledge of the time series of interest, is it easy to change the proposed method to a time series model specific to that interest (in this context, the design of the AE layer)?\n- Is this a robust setting for local (near temporal) features of the time series of interest, as the standard convolution assumes? Or is it also possible to capture very distant temporal dependencies (in the extreme case, where the first frame determines the last frame) or periodic structures?\n\n(2) Regarding ${\\rm explu}$ function.\nI believe that instability in the computation of covariance (the inverse of covariance) is a challenge often faced in statistical machine learning. For example, this problem often arises in Gaussian process regression (GPR) when the observed data are of high dimension. In GPR, this problem is usually addressed indirectly (the direct motivation is to reduce the computational complexity of the inverse of the covariance matrix) using, for example, variational methods or induced points [Titsias+2009]. While these methods require rather complicated handling, I feel that the explu function in this paper is a very simple and impressive potential new way to deal with this problem. So let me ask a question.\n- What are the disadvantages that arise when replacing the exp function with the explu function?\n- Can that disadvantage be ignored in practical application situations?\n\nM. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics, pages 567\u2013574, 2009."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7095/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7095/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7095/Reviewer_ahgt"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7095/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765845231,
            "cdate": 1698765845231,
            "tmdate": 1700713500491,
            "mdate": 1700713500491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nKkWiJiDQf",
                "forum": "lvjz7Bm3Ea",
                "replyto": "Sooinjh3Y2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7095/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7095/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review. There are very interesting points to explore in your criticisms, which are, in a way, encouraging to answer. Starting with questions about the representation of time series:\n\n**\u201cIs the adoption of the convolution approach the recommended setting by the authors? Or is it adopted because it is known as the most basic and standard in the field concerned? For example, if the user has prior knowledge of the time series of interest, is it easy to change the proposed method to a time series model specific to that interest (in this context, the design of the AE layer)?\u201d**\n\nAs we carried out extensive testing on different data sets generated and transformed to adapt to a One-Class Learning problem, we adopted an autoencoder structure capable of generalizing some information from different sets without major problems. Therefore, the proposed architecture, its layers, and parameters are a general recommendation that can be changed in different sets, changing the structure of the autoencoder and parameters such as the number of layers, filters, and size of the applied kernels. Activation is a recommendation given in the article [1], which presents a theorem about neural networks and the ability to reconstruct periodic patterns, so we adopted it to preserve the periodicity of the series without giving up other advantages.\n\n[1] Ziyin, Liu, Tilman Hartwig, and Masahito Ueda. \"Neural networks fail to learn periodic functions and how to fix it.\" Advances in Neural Information Processing Systems 33 (2020): 1583-1594.\n\n**\u201cIs this a robust setting for local (near temporal) features of the time series of interest, as the standard convolution assumes? Or is it also possible to capture very distant temporal dependencies (in the extreme case, where the first frame determines the last frame) or periodic structures?\u201d**\n\nWe evaluated the capacity of the model through the different sizes of the series available in the data sets, see Table 3. It is possible to observe that the performance is maintained for time series of up to 1000 observations, being impaired in series with a size greater than that or in series very small. Once this is observed, one of its causes is the size of the kernels applied during the convolutions. Note that optimizing different kernel sizes to find the best solution for each of the 652 generated datasets would result in an unfeasible experiment load. Therefore, we opted for a model with a generalist kernel size that obtains good performance for most solutions. However, this does not prevent other works from exploring these parameters to increasingly improve results in very large time series or treat this type of situation with other structures capable of capturing these long-distance dependencies.\n\n\nRegarding the Explu function:\n\n**\u201cWhat are the disadvantages that arise when replacing the exp function with the explu function?\u201d**\n\nBy removing the positive exponential side of the function, we have a loss in the size of the penalty for positive values since the positive side of the function's domain becomes linear. Another point is that we have a constant derivative equal to 1 on this side of the function, which can bring some disadvantages in specific scenarios.\n\n**\u201cCan that disadvantage be ignored in practical application situations?\u201d**\n\nWe adopt the \u201cthere is no free lunch\u201d theorem in this case. It is almost impossible to try to specify the disadvantages in all domains of all problems. However, a brief investigation into the nature of the problem can give clues about the application of the explu.\n\nWe can exemplify this with the case we used. In our scenario, the existence of instances with very high energy values could be maintained as follows:\n\nReplacing exp with explu we would have a constant derivative that would avoid exploding gradients due to high values.\nThe model compensates in the layer weights for the need for high values for certain instances in the one-class scenario.\n\nTherefore, the disadvantage of explu could be kept under control in the domain, and by keeping small values close to 0 in the negative domain, we would still have a penalty applied to these cases.\n\nAssuming a scenario with an exp applied from a threshold and 0 for values below the threshold. We could somehow penalize values below the threshold and avoid the explosion of the gradient by replacing it with an explu, which would make learning slower in some cases due to the derivative being equal to 1. However, it would avoid instabilities caused by extremely large values when we apply the derivative of an exponential."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7095/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540379335,
                "cdate": 1700540379335,
                "tmdate": 1700540379335,
                "mdate": 1700540379335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SVoSgWdYQb",
                "forum": "lvjz7Bm3Ea",
                "replyto": "nKkWiJiDQf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7095/Reviewer_ahgt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7095/Reviewer_ahgt"
                ],
                "content": {
                    "title": {
                        "value": "My two concerns have been resolved."
                    },
                    "comment": {
                        "value": "I greatly appreciate the author's thoughtful response. The content of the answer is very valid for me. My two concerns have been addressed. Therefore, I would like to raise my grade."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7095/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713481572,
                "cdate": 1700713481572,
                "tmdate": 1700713481572,
                "mdate": 1700713481572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d5iIilyaXt",
            "forum": "lvjz7Bm3Ea",
            "replyto": "lvjz7Bm3Ea",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7095/Reviewer_5RCZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7095/Reviewer_5RCZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses one-class detection for time series. To this end, the authors use an auto-encoder similar to the one introduced by Zong et al. (2018), while including some variations to tackle the temporal features as proposed by Fawaz et al. (2019)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper tackles an interesting problem, which is one-class detection on time series with deep learning."
                },
                "weaknesses": {
                    "value": "A major issue of this paper is the incremental contribution. The authors use roughly the same the auto-encoder proposed by Zong et al. (2018), which was introduced and investigated to address unsupervised anomaly detection using a Gaussian mixture model. The only difference seems to be the convolution layers as proposed by Fawaz et al. (2019), and the snake activation of Ziyin et al. (2020).\n \nAnother major issue is that the paper does not demonstrate clearly the relevance of these modifications. The authors should conduct an ablation study in order to show how the implemented modifications impact the obtained results, including convolution layers of Fawaz et al. (2019) and the snake activation of Ziyin et al. (2020).\n\nAnother major issue is experiments and comparative analysis. The authors compare the proposed method to 4 other detection methods: OCSVM, IsolationForest, DeepSVDD and DAGMM. All these methods are not relevant to address time series. Therefore, this is not enough as the authors did not provide any comparative analysis with related methods, namely methods from the deep leaning literature that address anomaly detection in time series. For a review, see\n* Choi, Kukjin, Jihun Yi, Changhwa Park, and Sungroh Yoon. \"Deep learning for anomaly detection in time-series data: review, analysis, and guidelines.\" IEEE Access 9 (2021): 120043-120065.\nSee also related methods\n* Kim, Siwon, Kukjin Choi, Hyun-Soo Choi, Byunghan Lee, and Sungroh Yoon. \"Towards a rigorous evaluation of time-series anomaly detection.\" In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 7, pp. 7194-7201. 2022.\n* Xu, Jiehui, Haixu Wu, Jianmin Wang, and Mingsheng Long. \"Anomaly transformer: Time series anomaly detection with association discrepancy.\" arXiv preprint arXiv:2110.02642 (2021).\n* Zhang, Chuxu, Dongjin Song, Yuncong Chen, Xinyang Feng, Cristian Lumezanu, Wei Cheng, Jingchao Ni, Bo Zong, Haifeng Chen, and Nitesh V. Chawla. \"A deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, pp. 1409-1416. 2019.\n* Garg, Astha, Wenyu Zhang, Jules Samaran, Ramasamy Savitha, and Chuan-Sheng Foo. \"An evaluation of anomaly detection and diagnosis in multivariate time series.\" IEEE Transactions on Neural Networks and Learning Systems 33, no. 6 (2021): 2508-2517.\n* Li, Dan, Dacheng Chen, Baihong Jin, Lei Shi, Jonathan Goh, and See-Kiong Ng. \"MAD-GAN: Multivariate anomaly detection for time series data with generative adversarial networks.\" In International conference on artificial neural networks, pp. 703-716. Cham: Springer International Publishing, 2019.\n* Zhang, Yuxin, Yiqiang Chen, Jindong Wang, and Zhiwen Pan. \"Unsupervised deep anomaly detection for multi-sensor time-series signals.\" IEEE Transactions on Knowledge and Data Engineering (2021).\n* Tuli, Shreshth, Giuliano Casale, and Nicholas R. Jennings. \"Tranad: Deep transformer networks for anomaly detection in multivariate time series data.\" arXiv preprint arXiv:2201.07284 (2022).\n* Carmona, Chris U., Fran\u00e7ois-Xavier Aubet, Valentin Flunkert, and Jan Gasthaus. \"Neural contextual anomaly detection for time series.\" arXiv preprint arXiv:2107.07702 (2021).\n\nFinally, there are some spelling and grammatical errors, such as \u201cautoncoder\u201d, \u201ccossine similarity function\u201d, \u201crecomendation\u201d."
                },
                "questions": {
                    "value": "Why didn't you compare to other deep anomaly methods for time series ?\n\nWhy there is no ablation study that allows to demonstrate the relevance of the proposed modifications ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7095/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769719530,
            "cdate": 1698769719530,
            "tmdate": 1699636837155,
            "mdate": 1699636837155,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uWHB9WNoqh",
                "forum": "lvjz7Bm3Ea",
                "replyto": "d5iIilyaXt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7095/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7095/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Firstly, we would like to thank you for the review and we will try to resolve the points that remained as doubts. To do this, let's contextualize some points:\n\nOne-class learning is a type of supervised learning that involves modeling examples belonging to a single class. According to the definition, only instances of the class of interest are available for training. On the other hand, Anomaly Detection is a type of learning that encompasses a set of techniques that can be supervised, semi-supervised, or unsupervised learning (training with normal and anomaly classes, i.e., using binary learning that is different from one-class learning). Anomaly Detection aims to identify abnormal instances or points in a data set and can also be referred to as Outlier Detection. [1]\n\nWith these definitions, we can move on to the characteristics that differentiate both:\n\n- Nature of the datasets: in some cases of outlier detection, there are anomalies present during the training set. In time series, differences like the data sets become more noticeable, while anomaly detection or outlier detection can focus on capturing anomalous observations that may exist in the same class, one-class is a classification problem, where an instance (entire series) will or will not belong to a class of interest. This way, Anomaly Detection can handle point-wise detection.\n- Characteristics of the problems: One of the main differences lies in the characteristics of the problem definition. In One-Class Learning, we only have instances of the class of interest in the data set, and this statement makes it a supervised learning problem, since the presence of unlabeled instances or instances outside this class directly impacts the learning and performance of the model, changing the paradigm of the task performed. In Anomaly Detection, some works focus on different statements, such as the mandatory presence of anomalous instances in the training set, the mandatory non-existence of these instances, or even semi-supervised learning more similar to Positive and Unlabeled Learning (PUL), where normal instances are present next to unlabeled ones.\n- Structure of the models: Due to the characteristics and statements, the models have different learning strategies, using binary objective functions or those that collapse when brought to one-class scenarios. That said, checking these models and studying and adapting them to work in one-class learning scenarios requires a greater effort, which would involve different work.\n\nTherefore, one-class learning methods cannot be directly applied to anomaly detection or vice versa. [1, 2]\n\nAnother important point highlighted by researchers such as Eamonn Keogh is that papers that work with anomaly detection suffer from several problems, from the nature of the datasets that suffer from several flaws to complex methods solving simple problems. [3]\n\n[1] Perera, Pramuditha, Poojan Oza, and Vishal M. Patel. \"One-class classification: A survey.\" arXiv preprint arXiv:2101.03064 (2021).\n\n[2] Seliya, Naeem, Azadeh Abdollah Zadeh, and Taghi M. Khoshgoftaar. \"A literature review on one-class classification and its potential applications in big data.\" Journal of Big Data 8.1 (2021): 1-31.\n\n[3] Eamonn Keogh. Irrational Exuberance Why we should not believe 95% of papers on Time Series Anomaly Detection. Available at: https://youtu.be/Vg1p3DouX8w?si=_NvqMdF9Ir-DTMnl (2021).\n\n**\u201cWhy there is no ablation study that allows to demonstrate the relevance of the proposed modifications ?\u201d**\n\nWe included a more in-depth study of the results in the appendices. However, due to limited space, we focused on the main points during the analysis of the results in the paper. We show and compare the approach using the new proposed loss function and without using it (adapting only the autoencoder) in Figure 2 (CDD).\n\nWe also raised some analyses about the sets' characteristics and the autoencoder's performance in capturing temporal dependencies. To see the relevance of these results, simply compare ChronoGAM (wo regularizer) in Figure 2 with DAGMM, which implements the same loss function and differs in the autoencoder. The gain shows that adapting the model structures to work with temporal dependencies is enough to surpass classic Machine Learning models. However, it is still unable to perform without a significant statistical difference when we analyze it together with the complete ChornoGAM, which implements the new loss function."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7095/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540265026,
                "cdate": 1700540265026,
                "tmdate": 1700540265026,
                "mdate": 1700540265026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "blUPHeRGpx",
                "forum": "lvjz7Bm3Ea",
                "replyto": "uWHB9WNoqh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7095/Reviewer_5RCZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7095/Reviewer_5RCZ"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgments"
                    },
                    "comment": {
                        "value": "We thank the authors for their reply. However, the issues that I have raised are not addressed. \n\nOf particular interest is why the authors did not compare the proposed method to other deep anomaly methods that have demonstrated their relevance on time series. \n\nThe reply of the authors in this rebuttal is that papers that work with anomaly detection suffer from several problems, citing Eamonn Keogh presentation \"Irrational Exuberance Why we should not believe 95% of papers on Time Series Anomaly Detection\". This reply raises other questions, if the authors are following thus presentation:\n- Please consider that this is only a presentation, and not a rigorous scientific analysis: 95% of papers is just an illustrative figure, and the presentation is only about a couple of papers.\n- That presentation provides a benchmark that aims to overcome the raised issues. The authors of the submitted paper did not investigate this benchmark.\n- The presentation has a positive takehome message, including several constructive points following \"We should see these facts as a wonderful opportunity...\"\nAt the end, the authors of the submitted paper consider that such a presentation invites the researchers not to compare to other methods from the literature. It is a pity for advancing research.\nPlease check my review for related literature on anomaly detection on time series.\n\nAnother point in the rebuttal is the sentence \"one-class learning methods cannot be directly applied to anomaly detection or vice versa. [1, 2]\".\nI did not understand what the authors want to say, since, in the contrary, there are too many one-class learning methods applied to anomaly detection and vice versa, as provided by the surveys [1, 2].\nI hope that the authors can acknowledge that their submitted paper is not the first in the literature to apply one-class learning methods applied to anomaly detection.\n\nThe issue of the ablation was raised by several reviewers. The reply of the authors is not correct because the Appendix does not contain an ablation study and no interesting information or any deep analysis.\n\nMoreover, I did not understand why 2/3 of the rebuttal is a general presentation of anomaly detection. To the best of my knowledge, and considering my review, I think that I know what anomaly detection is."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7095/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681901584,
                "cdate": 1700681901584,
                "tmdate": 1700681901584,
                "mdate": 1700681901584,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3IGdDncT8k",
            "forum": "lvjz7Bm3Ea",
            "replyto": "lvjz7Bm3Ea",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7095/Reviewer_JBii"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7095/Reviewer_JBii"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an end-to-end one-class time series Gaussian mixture model to resolve the collapse of hyperspheres, manual thresholds, numerical instabilities, and even the use of unlabeled instances during training. \n\nThe proposed ChronoGAM aims at improving the temporal importance of the representations learned by the autoencoding system. \n\nThe techniques are to penalize the small values on the covariance matrix without resulting in exploding gradient propagation, causing numerical instabilities, and adapting the energy calculus to avoid the use of exponential functions. \n\nExperimental results show the effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Simple and efficient reminiscences of numerical challenges in One-Class-Learning.\n- Extensive experimental results show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "- Limited concrete evidence for how the numerical challenges in One-Class-Learning are released\n- By using an autoencoder, the numerical problems are resolved but with limited theoretical justification - and no ablation study is presented."
                },
                "questions": {
                    "value": "it is overall interesting to see that with an end-to-end design, the performance is enhanced. However, it can be improved via the ablation perspective of the experiments in such an unsupervised setting. \n\nThe impact of adopting an autoencoder is welcomed but it requires concrete examples and evidence regarding: \n- what temporal and structural features are well captured.\n- how it helped in resolving the numerical challenges and why an autoencoder is the solution in that situation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7095/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7095/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7095/Reviewer_JBii"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7095/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699245101205,
            "cdate": 1699245101205,
            "tmdate": 1699636837055,
            "mdate": 1699636837055,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LQIIFkjANk",
                "forum": "lvjz7Bm3Ea",
                "replyto": "3IGdDncT8k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7095/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7095/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the reviews and pointed questions. Based on the demonstration of the problems found in (Ruff et al., 2018), we inserted an appendix to highlight the numerical challenges that involve the use of hyper-sphere loss, including the n-ball problem [2], as well as the experimental results that highlight the numerical instability of the DAGMM in Figure 3.\n\nThe problems of numerical instability were solved by inserting a regularizing term of the covariance matrix different from that proposed in DAGMM, see the final part of Equation 4, as well as equations 2 and 3 that insert the use of the proposed Explu function.\n\n\u201cwhat temporal and structural features are well captured.\u201d\n\nUnlike a simple MLP network, a convolutional filter can be used to obtain results for all timestamps of a time series, allowing the learning of invariant filters along the temporal dimension. These CNN networks allow the learning of these filters dynamically, optimizing the parameters for each data set used and learning filters that make it easy to discriminate between classes [1].\n\nSee in Figures 4 and 5 that this type of network optimizes the generation of more discernible embeddings between time series instances, making the representations more robust for the method.\n\n[1] Ismail Fawaz, Hassan, et al. \"Deep learning for time series classification: a review.\" Data mining and knowledge discovery 33.4 (2019): 917-963.\n\n[2] Weisstein, Eric W. \"Hypersphere.\" From MathWorld--A Wolfram Web Resource. https://mathworld.wolfram.com/Hypersphere.html\n\n\u201chow it helped in resolving the numerical challenges and why an autoencoder is the solution in that situation\u201d\n\nIn this case, the method can work more robustly with better embeddings. However, in the results, we show that just adapting the autoencoder will not solve the problem of numerical instability. The method started from this hypothesis to modify the loss function and add the term:\n\n$\\text{min} \\left ( \\sum^K_{k=1}\\sum^d_{j=1}-\\log(\\mathbf{\\hat{\\Sigma}}_{kjj}), 100 \\right )$\n\nIn Figure 2, it is possible to observe the CDD constructed from the results obtained, so we have an implementation that only adapts the autoencoder (ChronoGAM wo Regularizer) and the complete implementation of the method (ChronoGAM) that uses the new proposed loss function. In this Figure, it is possible to observe that we can obtain better results by adapting the method with the autoencoder and using the new loss function."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7095/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540117398,
                "cdate": 1700540117398,
                "tmdate": 1700540117398,
                "mdate": 1700540117398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LPPLZnnudc",
            "forum": "lvjz7Bm3Ea",
            "replyto": "lvjz7Bm3Ea",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7095/Reviewer_yKkP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7095/Reviewer_yKkP"
            ],
            "content": {
                "summary": {
                    "value": "The paper is focused on the problem of one-class learning with time series data. Existing methods face problems like hypersphere collapse and numerical instability, etc. The proposed method uses a Gaussian Mixture Model, combines an autoencoder for feature extraction, and improves temporal importance. It's tested on many datasets and shows outperformance in 369 of them."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is an applied paper and has little technical contribution. \n\nThe experiment is relatively comprehensive on 85 benchmark datasets and derived 652 datasets."
                },
                "weaknesses": {
                    "value": "1. The author claims the gain of the proposed method in 369 datasets out of a total of 652. The issue with this massive test is the lack of insights into the reasons for the proposed method's effectiveness and ineffectiveness on these many datasets. \n\n2. The proposed method appears to have no significant technical novelty; thus, it is a question of where the outperformance stems from. The authors are expected to provide a conceptual or theoretical explanation of why the proposed method can outperform in addition to experimental results."
                },
                "questions": {
                    "value": "See the weakness section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7095/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699488752716,
            "cdate": 1699488752716,
            "tmdate": 1699636836937,
            "mdate": 1699636836937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rqmegfqFnp",
                "forum": "lvjz7Bm3Ea",
                "replyto": "LPPLZnnudc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7095/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7095/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Firstly, we would like to thank you for the review and the suggestions. We will try to resolve as many of them as possible below to improve the quality and contributions of the paper.\n\nWe modified the discussion of the results to clarify discussions of why ChronoGAM (the proposed method) outperforms other methods.\n\n\u201c(...) Unlike traditional machine learning methods, such as IsolationForest and OCSVM, which interpret observations as attributes without temporal dependence, ChronoGAM uses convolutions to capture this dependence and thus obtains better results. Compared to its competitors that use deep learning strategies, DeepSVDD suffers from the problem of hypersphere collapse, leading to poor results which ChronoGAM easily overcomes due to not using a hypersphere loss.\n\nChronoGAM has been more successful than its main competitor (DAGMM) for two primary reasons. Firstly, its autoencoder architecture is more flexible, using convolutional structures that can detect temporal correlations in the data, as demonstrated in various studies on time series classification (Fawaz et al, 2019, Wang et al, 2017, Zong et al, 2018). In contrast, DAGMM only uses an MLP-based autoencoder, which cannot capture these dependencies. The second reason is the covariance matrix regularizer, while the DAGMM uses a \n\n$p=\\sum^K_{k=1}\\sum^d_{j=1}\\frac{1}{\\mathbf{\\hat {\\Sigma}}_{kjj}}$\n\nwhich can easily cause very large values, making it difficult to correctly propagate the error to the task, leading to numerical instabilities and exploding gradients, ChronoGAM uses the penalty $p=\\text{min} (\\sum^K_{k=1}\\sum^d_{j=1}-\\log(\\mathbf{\\hat{\\Sigma}}_{kjj}), \\alpha)$ where $\\alpha = 100$ , so values that would cause the gradient to explode and cause numerical instabilities as well as disregarding the other tasks that make up the objective function to simply minimize the covariance matrix are more easily avoided. (...)\u201d\n\nWe emphasize that the innovation of the proposed method lies in the new loss function used, proposing the explu to avoid very high values as occur in the exp, in addition to the motivation of the term that will regularize the covariance matrix that is used by the method (avoiding division to extremely low values that causes the exploding gradients problem). As for the results, the detailed results of each dataset, label, and metric will be made available in the project repository. To clarify this issue, we modify the following sentence:\n\n\u201c(...) The experimental evaluation codes and results are publicly available1. (...)\u201d\n\t\nAnd later the link will be updated to the project's public repository (link omitted for double-blind purposes)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7095/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540026719,
                "cdate": 1700540026719,
                "tmdate": 1700540026719,
                "mdate": 1700540026719,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]