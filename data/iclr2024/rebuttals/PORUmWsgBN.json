[
    {
        "title": "DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models"
    },
    {
        "review": {
            "id": "ayYKTfFWmK",
            "forum": "PORUmWsgBN",
            "replyto": "PORUmWsgBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5218/Reviewer_rH7c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5218/Reviewer_rH7c"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a diffusion-based method to generate co-speech 3D facial motions, particularly motions conditioned on various speaking styles. To this end, the proposed learning model consists of multiple components: the Wav2Vec2 Encoder to encode the speech, a transformer-based denoising decoder to generate current face motion parameters from the speech embeddings and noisy, past face motion parameters, and a transformer-based style encoder trained on a set of reference videos with a contrastive loss to incorporate the desired speaking styles into the face motions. The authors show the benefits of their proposed method through quantitative evaluations, ablation studies, and user studies."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method of generating facial motion parameters from speech through a denoising diffusion process is technically sound.\n\n2. The style encoder offers customizability to the generated face motions and improves the utility of the proposed method."
                },
                "weaknesses": {
                    "value": "1. In Sec. 1, para 2, the authors note that facial motions require more \"precise\" alignment with speech than other human body motions, such as gestures and dance. However, it is not clear what this \"precision\" entails and how, or if, it can be measured. What are the specific design choices in the denoising diffusion architecture, or the training loss functions and hyperparameters, or some other aspects, that are necessary for the successful learning of facial motions from speech? In other words, why would an existing diffusion architecture for body motion generation (such as [A] or [B], albeit with different training features) not work for this problem?\n\n[A] Ao, Tenglong, Zeyi Zhang, and Libin Liu. \"GestureDiffuCLIP: Gesture diffusion model with CLIP latents.\" ACM Transactions on Graphics, August 2023.\n[B] Dabral, Rishabh, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. \"Mofusion: A framework for denoising-diffusion-based motion synthesis.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9760-9770. 2023.\n\n2. While the style encoder offers more customizability to the generated face motions, it seems to be limited to the reference videos received during training. Is this understanding correct, or can the style encoder generalize to novel styles during inference?"
                },
                "questions": {
                    "value": "1. What is the latency of the end-to-end generation pipeline during inference? What is the latency of the stylization component?\n\n2. For Eqn. 2, is there any specific reason to operate on the vertex space and on the parameter space? The vertex space is much higher dimensional and less constrained, which could make the training more unstable."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723082285,
            "cdate": 1698723082285,
            "tmdate": 1699636519843,
            "mdate": 1699636519843,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h4WpOacsGD",
                "forum": "PORUmWsgBN",
                "replyto": "ayYKTfFWmK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer rH7c"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments. \n\nQ1: In Sec. 1, para 2, the authors note that facial motions require more \"precise\" alignment with speech than other human body motions, such as gestures and dance. However, it is not clear what this \"precision\" entails and how, or if, it can be measured. What are the specific design choices in the denoising diffusion architecture, or the training loss functions and hyperparameters, or some other aspects, that are necessary for the successful learning of facial motions from speech? In other words, why would an existing diffusion architecture for body motion generation (such as [A] or [B], albeit with different training features) not work for this problem?\n\nA1: This precision refers to the need for accurate synchronization between the articulation of sounds and the corresponding lip movements. In contrast, gestures can have more flexibility in their timing, allowing them to occur slightly before or after the associated speech. Also, gestures can be less specific in their correlation with speech. As shown in [1], the correlation coefficient between voice pitch and jaw is 0.69, whereas with other body motions it is below 0.42.\n\nAs stated in Sec. 1, Para. 2, lip motions are semantically rich (can indicate the speech articulation compared to gestures and dancing). Thus, speech-driven facial animations require stronger speech encoder to extract phoneme-level features. Furthermore, it requires specific designs to align generated motions precisely to speech articulations. However, existing diffusion architectures for body motion generation typically lack these designs. For example, neither [A] nor [B] employ encoders for audio processing (they use traditional acoustic features), and neither have a design that aligns local motion features with local audio feature. To this end, we adapted the well-established transformer encoder-decoder structure, which was widely used in previous non-diffusion-based speech-driven facial animation works, to our diffusion model. The encoder, which was a pretrained Wav2Vec2 encoder, provided robust and contextualized speech features. The alignment mask serves as a crucial component to align the local speech features with the local motion features from the decoder. As the ablation study indicated, removing the alignment mask significantly degraded lip synchronization (from 9.44 to 16.06).\n\n- [1] M. Berry, S. Lewin, and S. Brown, \u201cCorrelated expression of the body, face, and voice during character portrayal in actors\u201d, Scientific Reports, vol. 12, no. 1, p. 8253, 2022.\n\nQ2: While the style encoder offers more customizability to the generated face motions, it seems to be limited to the reference videos received during training. Is this understanding correct, or can the style encoder generalize to novel styles during inference?\n\nA2: The style encoder can generalize to novel styles during inference. In fact, all the experiments and examples in this paper used novel videos as the reference videos.\n\nQ3: What is the latency of the end-to-end generation pipeline during inference? What is the latency of the stylization component?\n\nA3: Although our method can inference at 30 fps, for live streaming scenarios, it introduces a 7.33-second delay. This delay is primarily due to our windowing strategy, which involves an initial 4 seconds wait to fill the first window and an additional 3.33 seconds for processing. However, once this initial processing is complete, our inference does not introduce any further latency. Regarding the style encoder, inference on a reference segment takes just 2ms, and the style feature can actually be extracted beforehand. We have added these discussions to Sec. A.3 of the revised appendix.\n\nQ4: For Eqn. 2, is there any specific reason to operate on the vertex space and on the parameter space? The vertex space is much higher dimensional and less constrained, which could make the training more unstable.\n\nA4: Actually, Eqn. 2 (the simple loss), calculates the MSE of the predicted results and the ground truth in the parameter space instead of in the vertex space. We only calculate the geometric losses in the vertex space because the vertex space can yield better loss guidance for the position, velocity, and acceleration of face vertices than the parameter space."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493756752,
                "cdate": 1700493756752,
                "tmdate": 1700493756752,
                "mdate": 1700493756752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IDdfj0ZtVy",
                "forum": "PORUmWsgBN",
                "replyto": "h4WpOacsGD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5218/Reviewer_rH7c"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5218/Reviewer_rH7c"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their responses, which address my concerns and questions. I maintain my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682915968,
                "cdate": 1700682915968,
                "tmdate": 1700682915968,
                "mdate": 1700682915968,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zx9Sl3uRgW",
            "forum": "PORUmWsgBN",
            "replyto": "PORUmWsgBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5218/Reviewer_3BSS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5218/Reviewer_3BSS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework DiffPoseTalk that utilizes a diffusion model with a style encoder. DiffPoseTalk generates 3D speech-driven talking face videos and it is capable of changing the style of the generated videos and can generate video depicting the style of the given short reference video. The model uses a pre-trained Wav2Vec2 encoder as an audio feature extractor and 3DMM as face representation. Also, it uses a transformer-based denoising network for 3D talking face generation. Moreover, they built a talking face dataset TFHP with 704 videos of 302 subjects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "A talking face dataset is proposed.\n\nThe manuscript is clear and easy to follow.\n\nThe manuscript has equations and figures that support the writing.\n\nThe model is evaluated well and it is compared to state-of-the-art (see question 1)"
                },
                "weaknesses": {
                    "value": "There is no discussion of ethical considerations.\n\nLimitations are not elaborated."
                },
                "questions": {
                    "value": "The ablation study model without CFG shows that it does not have much effect on the results (especially LVE and FDD). Can you elaborate this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5218/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5218/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5218/Reviewer_3BSS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757580963,
            "cdate": 1698757580963,
            "tmdate": 1699636519747,
            "mdate": 1699636519747,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7xs4jGNP9d",
                "forum": "PORUmWsgBN",
                "replyto": "zx9Sl3uRgW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 3BSS"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments. \n\nQ1: Discussion of ethical considerations.\n\nA1: We have added the following discussion to Sec. A7 of the revised appendix. \n\u201cSince our approach is able to generate realistic talking head sequences, there are risks of misusing, such as deepfake generation and deliberate manipulation. Therefore, we firmly require that all talking face sequences generated by our method be marked or noted as synthetic data. Moreover, we will make our code publicly available to the deepfake detection community to further ensure that our proposed method can be applied positively. We also hope to raise awareness of the risks and support regulations preventing technology abuse involving synthetic videos.\u201d\n\nQ2: Description of limitations.\n\nA2: The following discussions on limitations have been added to the revised appendix.\n\u201cAlthough our method is able to generate high-quality stylistic 3D talking face animation with vivid head poses, there are still some limitations within our framework that could be addressed in follow-up works. Firstly, the computational cost of inference is relatively high due to the sequential nature of the denoising process. To mitigate this, future research can explore more advanced denoisers such as DPM-solver++. Secondly, like existing SOTAs, our approach focuses on animating the face shape while ignoring the inner mouth (including teeth and tongue). Exploring representation and animation of the inner mouth can lead to more realistic results.  Lastly, a promising direction for future research would be to collect real-world 3D talking data that encompasses a broader range of identities and styles, which would further enhance the effectiveness of our approach and contributes to the research community.\u201d \n\nQ3: Discussion on the ablation study model without CFG.\n\nA3: We hypothesize that the relatively small improvement may arise from two reasons: (1) Classifier-free guidance is a technique in diffusion models that aims to reduce the diversity of generated samples while enhancing the quality of each individual sample. Unlike text-to-image problem, our task exhibits relatively less variation with respect to speech, style, and motion. Consequently, the trade-off between diversity and the quality of each individual sample becomes less apparent. Therefore, the effectiveness of classifier-free guidance in this context might be somewhat limited. (2) There might be a mild train-test mismatch issue during the CFG process, where the generated values fall outside the range observed during training [1]. In future work, we will explore advanced techniques from the diffusion literature (e.g., [2]) to mitigate this mismatch. Nonetheless, we want to emphasize that our method outperforms the baselines by a wide margin.\n\n- [1] C. Saharia et al., \u201cPhotorealistic text-to-image diffusion models with deep language understanding\u201d, NeurIPS, 2022.\n- [2] S. Lin, B. Liu, J. Li, and X. Yang, \u201cCommon Diffusion Noise Schedules and Sample Steps are Flawed\u201d, arXiv, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493676756,
                "cdate": 1700493676756,
                "tmdate": 1700668753152,
                "mdate": 1700668753152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "66Jw0GXekj",
            "forum": "PORUmWsgBN",
            "replyto": "PORUmWsgBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5218/Reviewer_WSFi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5218/Reviewer_WSFi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a generative diffusion model (DiffPoseTalk) combined with a style encoder that extracts style embeddings from short reference videos for 3D facial animations driven by speech. The generation process is extended to include the generation of head poses. The model is trained on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main contributions are:\n- A diffusion-based approach is proposed to jointly generate diverse and stylistic 3D facial motions with head poses from speech.\n- A style encoder is developed to extract personalized speaking styles from reference videos, which can be used to guide the motion generation in a classifier-free manner.\n- An audio-visual dataset is constructed that includes a range of diverse identities and head poses."
                },
                "weaknesses": {
                    "value": "- In Section 4.1 it is motivated the use of synthesis data as a source for training the proposed architecture. It is also reported the 3D face reconstruction method in Filntisis et al., 2022) based on a 3DMM is used for accurate reconstruction of lip movements. This reconstruction than learning of a set of 3DMM parameters with related lips movements based on these 2D datasets. However, this is not much convincing to me. Lips movements associated to speech are quite subtle and with specificity from each individual. Reconstructing such subtle movements with a 3DMM from a 2D video sequence seems not capable of capturing the reality of movement. In my opinion, authors should convince the readers more about this point which is also fundamental for the proposed approach. \n- Table 1 is not fully convincing because the other compared methods were developed for working with different data and so the proposed evaluation could be somewhat biased. \n- The supplemental video material is not fully convincing. For some speech the lips movement and the audio track are not well synchronized. The lips movement is also quite synthetic with the lips that are not able to close. The movement is also on the lips only without facial expression shown during face lips animation.\n- There is not much discussion on the proposed method. In particular, limitations of the proposed approach are not evidenced."
                },
                "questions": {
                    "value": "Q1: authors should make it evident the methods for lips reconstruction can provide results similar to the real one. I understand this is not the focus of the paper but the proposed approach relies on this assumption which is not enough supported and makes evidence in my opinion. \nFor example authors could compare results obtained with synthetic data and real one using their solution\nQ2: an experiments using real data could have been reported (e.g., VOCASET) for a better comparison with other methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761961173,
            "cdate": 1698761961173,
            "tmdate": 1699636519658,
            "mdate": 1699636519658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KtkyxZb4Dw",
                "forum": "PORUmWsgBN",
                "replyto": "66Jw0GXekj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer WSFi (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments. \n\nWe understand that your main concern is regarding our use of \"3D data reconstructed from 2D datasets\" rather than \"scanned real 3D data\" in our work. We would like to respond to this concern first, and respond to other detailed questions in the next comment.\n\nFirstly, we emphasize that our contribution is to propose a novel and **general** diffusion-based method for generating speech-to-3D animation. Specifically, our method is able to control the speaker's style using any reference style and support the prediction of head movements. All reviewers agree that this application scenario is valuable. We also note that **existing scanned real 3D datasets cannot be used to build algorithms for this new scenario** due to their limited scale and coverage of identities, styles, and head movements. Therefore, there are two possible strategies: one is to collect new scanned 3D real data, and the other is to reconstruct from existing 2D data. The method proposed in this paper belongs to the latter. We are also very much looking forward to someone coming up with methods of the former in the future, and comparing them with ours. \n\nSecondly, **the strategy of using synthetic data from 3D reconstruction is well-established in various SOTA methods**. Due to the limited scale and coverage in existing 3D real datasets, three recent SOTAs [1-3], including two on emotional 3D facial animations generation [2, 3], used reconstructed data from 2D. In addition, [3] also adopted the reconstruction method we used. Additionally, many successful 2D talking head videos methods that use 3DMM as an intermediate face representation, such as [4-7], also utilize reconstructed data to train their speech-to-3D animation generation networks. Thus, **the practice of using reconstructed data is reasonable and well-established in this field**.\n\nWe understand your concern about the quality of reconstructed lips. However, this is not a key issue in our method because existing SOTA reconstruction methods can reconstruct reasonable lip shapes most of the time, especially since **we used a method optimized for lip shape reconstruction (Filntisis et al., 2022)**. Quantitative results, qualitative experiments, and user studies in this paper demonstrated that our method achieved satisfactory generation and outperformed baseline methods. Moreover, **our method does not rely on how 3DMM coefficients are obtained**. In fact, the regression-based reconstruction method we currently use achieves satisfactory results. Of course, if an advanced optimization-based reconstruction algorithm is used or an even stronger regression-based reconstruction algorithm that can provide accurate 3DMM coefficients becomes available in the future, our method can readily use it. \n\nLastly, we would like to point out that our method\u2019s usage of 3DMM coefficients presents several advantages over the methods utilizing scanned 3D meshes.  First, 2D video data is much easier to collect and reconstructed 3DMM data can **cover a broader range**, leading to models with **stronger generalization capabilities**. Second, different from 3D meshes, the lower dimensionality of 3DMM coefficients **reduces computational load and enhances inference speed**, which is crucial for diffusion models. This reduced dimensionality, combined with blendshapes as a prior, **makes the** **learning process easier and improves generalization** [2]. Third, using 3DMM coefficients **facilitates integration with downstream applications** such as driving rig-based gaming avatars or 3DMM-based expression editing [8, 9].\n\n- [1] C. Zhang et al., \u201c3D Talking Face With Personalized Pose Dynamics\u201d , TVCG, 2023.\n- [2] Z. Peng et al., \u201cEmoTalk: Speech-driven emotional disentanglement for 3D face animation\u201d, ICCV, 2023.\n- [3] R. Dan\u011b\u010dek, K. Chhatre, S. Tripathi, Y. Wen, M. J. Black, and T. Bolkart, \u201cEmotional Speech-Driven Animation with Content-Emotion Disentanglement\u201d, SIGGRAPH Asia Conference, 2023.\n- [4] J. Thies, M. Elgharib, A. Tewari, C. Theobalt, and M. Nie\u00dfner, \u201cNeural Voice Puppetry: Audio-driven Facial Reenactment\u201d, ECCV, 2020.\n- [5] Z. Zhang, L. Li, Y. Ding, and C. Fan, \u201cFlow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset\u201d, CVPR, 2021.\n- [6] W. Zhang et al., \u201cSadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation\u201d, CVPR, 2023.\n- [7] C. Xu et al., \u201cHigh-Fidelity Generalized Emotional Talking Face Generation With Multi-Modal Emotion Space Learning\u201d, CVPR, 2023.\n- [8] Z. Geng, C. Cao, and S. Tulyakov, \u201c3D Guided Fine-Grained Face Manipulation\u201d, CVPR, 2019.\n- [9] Z. Sun et al., \u201cContinuously Controllable Facial Expression Editing in Talking Face Videos,\u201d arXiv, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493331608,
                "cdate": 1700493331608,
                "tmdate": 1700624854005,
                "mdate": 1700624854005,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1tGjCOJYXy",
                "forum": "PORUmWsgBN",
                "replyto": "66Jw0GXekj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer WSFi (2/2)"
                    },
                    "comment": {
                        "value": "Q1: Fairness in Table 1 for the methods that were designed for working with different data.\n\nA1: We understand your concern: FaceFormer/CodeTalker were designed for animation with fixed styles and no head movements, while our method is designed for 3D face animation that **generalizes to unseen styles along with head movements**, thus the comparison may appear unfair. **Our comparison follows previous work** [2, 3]. In particular, we train these methods with zero-posed mesh data converted from 3DMM coefficients (which matches the original data's topology) and provide one-hot speaker labels **for fair comparison**. Additionally, the integration of generalizable style and head movements are essential components towards more natural and realistic 3D face animation.\n\nQ2: Supplemental video: for some parts, the lip movements and the audio track are not well synchronized, and movement is on the lips only without facial expression shown during face lips animation.\n\nA2: First, we clarify that **our method can automatically generate upper face motions**, such as eye blinks (e.g., in demo video 2:07-2:20), that are weakly-related or unrelated to speech, as stated in \u201cQualitative Evaluation\u201d in Sec. 4.3. In contrast, the other methods show less upper face motions. Our FDD metrics also surpass other methods by a wide margin, **which indicates that our upper face motions are closer to the ground truth**. This improvement is largely because we adapted the diffusion model for facial animation generation. The probabilistic approach can capture the data distribution of speech-unrelated movements more effectively than regression-based methods. Second, our method **shows significant improvements in lip synchronization, style control, and naturalness over previous SOTA methods**, as demonstrated by our quantitative results, qualitative experiments, and user studies.\n\nQ3: More limitation discussions on the proposed method.\n\nA3: The following discussions on limitations have been added to the revised appendix.\n\u201cAlthough our method is able to generate high-quality stylistic 3D talking face animation with vivid head poses, there are still some limitations within our framework that could be addressed in follow-up works. Firstly, the computational cost of inference is relatively high due to the sequential nature of the denoising process. To mitigate this, future research can explore more advanced denoisers such as DPM-solver++. Secondly, like existing SOTAs, our approach focuses on animating the face shape while ignoring the inner mouth (including teeth and tongue). Exploring representation and animation of the inner mouth can lead to more realistic results.  Lastly, a promising direction for future research would be to collect real-world 3D talking data that encompasses a broader range of identities and styles, which would further enhance the effectiveness of our approach and contributes to the research community.\u201d\n\nQ4: Evidence that lips reconstruction can provide results similar to the real one.\n\nA4: We agree that existing SOTA reconstruction methods may not provide 3D data as accurately as scanned real data. However, we note that the reconstructed data is already feasible for capturing the dynamics of the lips. As mentioned in the previous comment, the use of reconstructed 3D data from 2D datasets is a **reasonable and well-established practice** [1-7]. Furthermore, the reconstruction method (Filntisis et al., 2022) we adopted was **designed and optimized for accurate lip shape reconstruction**. As demonstrated in their paper and demo video, this method can yield plausible lip reconstructions from 2D data.\n\nQ5: Report an experiment using real data (e.g., VOCASET) for a better comparison.\n\nA5: We have compared our method with FaceFormer and CodeTalker on the VOCASET. We list the quantitative results here. More details about the experiment can be found in Sec. A.6 of the revised appendix, and qualitative results can be found in the updated demo video (5:12-5:31).\n\n| Method     | LVE (mm) $\\downarrow$ | FDD ($\\times 10^{-5}$m $\\downarrow$) |\n| ---------- | ----------- | ----------- |\n| FaceFormer | 4.43    | 13.61    |\n| CodeTalker | 4.55    | 10.34    |\n| Ours       | **4.35** | **8.85** |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493546547,
                "cdate": 1700493546547,
                "tmdate": 1700493546547,
                "mdate": 1700493546547,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ClKKL6zw13",
            "forum": "PORUmWsgBN",
            "replyto": "PORUmWsgBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5218/Reviewer_xqkb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5218/Reviewer_xqkb"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for audio-driven facial animation, where reference style is modeled using a reference video (3D tracking of the video) encoder (and not generic one-hot encoding) and diffusion-based denoising network for final vertex animation. The main contribution of the paper is to introduce the use of diffusion (similar to Tevet '23 et al) for audio-driven talking head generation. While overall no new loss functions (except a simple smoothness loss) are introduced, however, the end-to-end formulation is novel, along with reference style encoding using transformers and contrastive learning and head pose movement. Finally, like previous approaches W2V2 is used for audio feature extraction. The paper also introduces a small dataset of tracked videos with richer set of emotion and style variations. Overall the method outperforms several SOTA methods wrt to quality of articulation and style transfers and provides ablations for their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A novel formulation for target style-encoding that's able to extract salient features from target video via tracking and use these for \"style-transfer\".\n- Lack of high-quality data in this space is a big problem, and hence the small dataset release for audio-driven facial animation is a welcome contribution.\n- Diffusion-based approach for final audio-driven facial animation. Given that for this part the work build up on Tevet '23 et al, it's a less novel contribution\n- The SOTA comparisons and user-studies suggest the proposed solution provides improvement over previous approaches."
                },
                "weaknesses": {
                    "value": "- While the paper claims that they can meaningfully extract \"style\" from the reference videos, in the examples shown in the video (2:42), they don't show the reference styles so it's hard to understand how faithfully the reference style was matched, further only a a single example for the same is shown.\n- In the user-study the authors claim that they have superior scores, but none of the user study samples are shared, so it's hard to understand what the users were really scoring. \n- The paper does not discuss or define, what they define as style, and how they measure \"style\" disentanglement from \"content\"."
                },
                "questions": {
                    "value": "- What metric will you use to measure the disentanglement between style and content? Can you show a couple of example of style or content muted performances (where the original video is assumed to have both)?\n- Can you explain the motivation behind the contrastive loss for your problem? It's unclear why contrastive training makes sense intuitively."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5218/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5218/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5218/Reviewer_xqkb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802329050,
            "cdate": 1698802329050,
            "tmdate": 1699636519565,
            "mdate": 1699636519565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KPPH8SFS4b",
                "forum": "PORUmWsgBN",
                "replyto": "ClKKL6zw13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer xqkb"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments. \n\nQ1: Contribution novelty compared to Tevet '23 et al.\n\nA1: Although our usage of diffusion is similar to Tevet \u201923 et al., we introduced several special designs to make the diffusion model work for speech-driven facial animation. First, different from the transformer encoder structure of the denoising network in Tevet \u201923 et al., we followed previous successful non-diffusion-based speech-driven facial animation methods to adopt a transformer encoder-decoder structure, where the Wav2Vec2 encoder provided robust speech features. Second, we introduced the alignment mask from FaceFormer to align speech features and motion features, without which the model was unable to generate synchronized facial motions. Third, while Tevet \u201923 et al. has a maximum limit on the generated sequence, we introduced a windowing design and a random truncation strategy to allow arbitrary length generation. Finally, we added style condition and achieved style control with incremental classifier-free guidance.\n\nQ2: Show more examples and the reference styles in video for better understanding.\n\nA2: We have added more examples with both the style reference videos and the generated videos in the updated demo video (2:34-3:53).\n\nQ3: Share user study samples for better understanding. \n\nA3: We have added screenshots and more detailed description of our user study in Sec. A.5 of the revised appendix.\n\nQ4: Discuss or define what is style. \n\nA4: As stated in the first paragraph of Sec. 3.3.1, \u201cSpeaking style is a multifaceted attribute that manifests in various aspects such as the size of the mouth opening (Cudeiro et al., 2019), facial expression dynamics \u2014 especially in the upper face (Xing et al., 2023) \u2014 and head movement patterns (Yi et al., 2022; Zhang et al., 2023a)\u201d. Given the complexity and difficulty in quantitatively describing a speaking style with an exact metric, we adopted several metrics to evaluate different speaking behavior characteristics. As stated in \u201cQuantitative Evaluation\u201d of Sec. 4.3, \u201cFDD evaluates the upper face motions, which are closely related to speaking styles, by comparing the standard deviation of each upper face vertex\u2019s motion over time between the prediction and ground truth.\u201d The beat alignment score (BA) was used to \u201cassess head motion\u201d, and the \u201cmouth opening difference (MOD), which measures the average difference in the size of the mouth opening between the prediction and ground truth\u201d, also reflects the speaking style difference in mouth opening. \n\nQ5: Metric to measure the disentanglement between style and content, and show more examples of style or content muted performances\n\nA5: Similar to the challenges in quantitatively evaluating the disentanglement of emotion and content in EmoTalk (Peng et al., 2023), quantitatively measuring the disentanglement of style and content presents similar difficulties. However, it is possible to evaluate this qualitatively. To illustrate, we did a qualitative experiment: we randomly selected 10 speaking styles and 20 audio clips to generate 10*20 animations. The speaking styles from these animations were then extracted using our style encoder. We employed t-SNE for visualizing these extracted speaking styles. The results demonstrated that animations with identical reference styles cluster together, regardless of content differences. For more details, please refer to Sec. A.4 in the revised appendix . In addition, we also show examples of style or content muted performances in the demo video (4:28-4:46).\n\nQ6: Motivation behind the contrastive loss.\n\nA6: As stated in the first paragraph of Sec. 3.3.1, \u201cGiven the complexity and difficulty in quantitatively describing speaking styles, we opt for an implicit learning approach through contrastive learning. We operate under the assumption that the short-term speaking styles of the same person at two proximate times should be similar.\u201d Note that many works were also built on similar assumptions: Yi et al. (2022) assumes the speaking style within the same video is similar. FaceFormer and CodeTalker assume the speaking style from the same person is similar."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493214219,
                "cdate": 1700493214219,
                "tmdate": 1700493214219,
                "mdate": 1700493214219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]