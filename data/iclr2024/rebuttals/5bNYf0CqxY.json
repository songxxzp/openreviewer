[
    {
        "title": "Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks"
    },
    {
        "review": {
            "id": "KU8wKsLy30",
            "forum": "5bNYf0CqxY",
            "replyto": "5bNYf0CqxY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6976/Reviewer_FaRA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6976/Reviewer_FaRA"
            ],
            "content": {
                "summary": {
                    "value": "This paper pointed out that rate-encoded SNNs have superior robustness compared to constant-encoded SNNs, and then proposed an adversarial training method based on rate encoding."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The author's relevant mathematical derivation about the perturbation degree $\\boldsymbol{\\delta}$ is quite convincing, especially the connection between $\\boldsymbol{\\delta}$ and $T$ established in Theorem 5."
                },
                "weaknesses": {
                    "value": "1. As shown in Tab.1, compared to constant-encoded SNN, rate-encoded SNN seems to have a significant loss in its clean accuracy (CIFAR10: 91.29% v.s. 83.22%, CIFAR-100: 70.85% v.s. 55.27%). I think this will significantly hinder the practical application of rate-encoded SNN.\n\n2. In Tab.1, the author's comparative perspective is that the robustness of constant-encoded SNN under PGD (C) is weaker than that of rate-encoded SNN under PGD (R), so rate-encoded SNN is better. However, I noticed that the robustness of constant-encoded SNN is better than that of rate-encoded SNN under GN attacks (CIFAR10: 90.62% v.s. 78.62%, CIFAR-100: 66.46% v.s. 50.43%), and at the same time, constant-encoded SNN maintains good defense against FGSM (R) and PGD (R). **Therefore, it seems that the so-called superior robustness of rate-encoded SNN does not have a good generalization effect, and it cannot maintain good defense ability under different attacks, which is an obvious limitations.**"
                },
                "questions": {
                    "value": "See Weakness Section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Reviewer_FaRA",
                        "ICLR.cc/2024/Conference/Submission6976/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6976/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698567514100,
            "cdate": 1698567514100,
            "tmdate": 1700658863830,
            "mdate": 1700658863830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A2ZhLncj5M",
                "forum": "5bNYf0CqxY",
                "replyto": "KU8wKsLy30",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6976/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6976/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**As shown in Tab.1, compared to constant-encoded SNN, rate-encoded SNN seems to have a significant loss in its clean accuracy. I think this will significantly hinder the practical application of rate-encoded SNN.**\n \nAns: Indeed, it is well known that rate-encoding has the limitation of lower clean accuracy, especially for low latency inference. However, we would like to emphasize that the proposed connection to randomized smoothing reveals that by better approximating the smooth classifier (i.e., using larger m), we can improve the accuracy of the rate-encoded classifier across different training strategies.\n Thus, our results improve the evaluation of rate-encoded classifier ( CIFAR-10 CLEAN: 79.55 (m=1) to 83.22 (m=10), CIFAR-100 CLEAN: 50.9 (m=1) to 55.27 (m=10)), which was not observed earlier. Fig 1(a) reports the improvement in accuracy of rate-encoded models with m.\n\nFurther, as we can see in Fig 1(d), the accuracy of the constant encoded models quickly breaks down under small adversarial perturbations, while rate-encoded models continue to offer better robust accuracy. Thus, under the threat of adversarial attacks, one can find the rate-encoded models more suitable compared to their constant encoded counterpart.\n\n**In Tab.1, the author's comparative perspective is that the robustness of constant-encoded SNN under PGD (C) is weaker than that of rate-encoded SNN under PGD (R), so rate-encoded SNN is better. However, I noticed that the robustness of constant-encoded SNN is better than that of rate-encoded SNN under GN attacks, and at the same time, constant-encoded SNN maintains good defense against FGSM (R) and PGD (R). Therefore, it seems that the so-called superior robustness of rate-encoded SNN does not have a good generalization effect, and it cannot maintain good defense ability under different attacks, which is an obvious limitations.**\n    \n Ans: First, we would like to highlight that fgsm(R)/pgd(R) attacks are ineffective against constant encoded models, similar to the ineffectiveness of the fgsm(C)/pgd(C) attacks against the rate-encoded models. The reason behind this is discussed in detail to answer the questions from reviewer 77u8.\n    \nThe reported results in Table 1 with GN attack uses Gaussian perturbation: $x+\\delta$, where $\\delta \\sim N(0,\\sigma^2)$, with $\\sigma= 8/255 \\approx 0.031$. The better results of the constant encoded models against Gaussian noise attacks quickly break down as we increase the strength of the attacks, as computed in the table below. For example, at $\\sigma=0.1$, CLEAN(R) with m=10, provides 21.77\\% higher accuracy than CLEAN(C)+RAT.\n    \n\n |$\\sigma$  |  0.05 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 |\n | ----- | ------- | ------ | ------ | ------ | ------ | ------ |\n | CLEAN(C) | 86.53 | 59.58 | 22.8 | 14.63 | 13.14 | 11.73 |\n | CLEAN(C)+RAT | 84.3 | 60.3 | 26.01 | 16.01 | 12.66 | 11.26 |\n | CLEAN (R), m=1 | 78.38 | 77.74 | 72.58 | 61.77 | 48.38 |\t35.13 |\n | CLEAN (R), m=10 | 83.13 | 82.07 | 77.65 | 66.26 | 51.14 | 36.66|\n\n\nWe have now included the above table in the appendix of the paper. We would also like to emphasize that while comparing the empirical robust accuracy of two models, we compare the minimum accuracy obtained by a model across any attack, as some attacks can be weak for one model but strong for another. This can be further seen from Table 5 (recently included in the appendix following suggestions from NxbL), where we implement adversarial attacks using different back-propagation algorithms."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520423312,
                "cdate": 1700520423312,
                "tmdate": 1700520423312,
                "mdate": 1700520423312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZsCdGW9ksb",
                "forum": "5bNYf0CqxY",
                "replyto": "A2ZhLncj5M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6976/Reviewer_FaRA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6976/Reviewer_FaRA"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the relevant responses from the authors and I think they have effectively addressed my conserns. Therefore, I choose to increase my rating to 8."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658823221,
                "cdate": 1700658823221,
                "tmdate": 1700658823221,
                "mdate": 1700658823221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "duT6zeCPc3",
            "forum": "5bNYf0CqxY",
            "replyto": "5bNYf0CqxY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6976/Reviewer_wpSa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6976/Reviewer_wpSa"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a certified robust training framework for SNN. They mainly consider the Bernoulli noise in input and bounded by l1-norm. During robust training, they adopt STE for BP."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Apply certification based robust training on SNN is an interesting topic."
                },
                "weaknesses": {
                    "value": "1.\tBased on my understanding, the bound is conducted through repeated sampling (m times), which is a statistic boundary instead of a rigorous boundary. Please make it clearly. \n2.\tThe robust training framework is not clear. Based on my understanding, the robust training is more like adding noise to inputs repletely to conduct the boundary, which seems trivial.\n3.\tThe labels in experiments are misleading, i.e. I did not find RAT in tables. \n4.\tPlease discuss [1]\n\nLiang, Ling, et al. \"Toward robust spiking neural network against adversarial perturbation.\" Advances in Neural Information Processing Systems 35 (2022)."
                },
                "questions": {
                    "value": "See weaknesses for details"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Reviewer_wpSa",
                        "ICLR.cc/2024/Conference/Submission6976/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6976/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654576497,
            "cdate": 1698654576497,
            "tmdate": 1700660417820,
            "mdate": 1700660417820,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cry4evuAdb",
                "forum": "5bNYf0CqxY",
                "replyto": "duT6zeCPc3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6976/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6976/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The bound is conducted through repeated sampling (m times), which is a statistic boundary instead of a rigorous boundary. Please make it clearly.**\n    \nAns: Following the randomized smoothing framework, we estimate the output of the smooth classifier g, and probabilities $\\underline{p_a}, \\overline{p_b}$, through Monte Carlo estimation as given in eqn. (9). As stated after theorem 1, these bounds are obtained with high probability. This was further discussed in section 3.3, where we point to the proofs for the confidence of the estimates as obtained by Algorithms 1 and 2.\n\n**The robust training framework is not clear. Based on my understanding, the robust training is more like adding noise to inputs repeatedly to conduct the boundary, which seems trivial.**\n\n Ans: In simple words, the adversarial training framework trains the models on adversarial perturbed images $x+\\delta$, along with the original label y. Here, the adversarial perturbation $\\delta$ for each image $x$ is calculated separately for each image, with respect to the present model weight obtained after each epoch. In theoretical formulation, the adversarial training is given as a min-max problem, where the inner maximization is performed over image perturbation and the outer minimization is done over the model weights, as described in eqn. (16). \n    \n Algorithmically, adversarial training differs from vanilla robust training in the sense that the perturbed image $x+\\delta$ is found through some adversarial attack (e.g., FGSM/PGD) computed on the present model predictions, approximating the inner maximization. In contrast, in the robust training framework, the perturbation $\\delta$ is a noise, possibly independent of the image $x$. \n\nIn Table 1, column GN implements robust training with Gaussian noise, while columns FGSM and PGD implement adversarial training with respective attacks. To our knowledge, we are the first to report adversarial training results with rate-encoded SNNs, which beats the prior adversarial training results with constant encoded SNNs, as reproduced in Table 1. Further, we show that the results of the rate-encoded classifiers can be improved with the better approximation of the smooth classifier (m=1 vs. m=10) \n\n      \n**The labels in experiments are misleading, i.e. I did not find RAT in tables.**\n\n Ans: For each dataset CIFAR-10 and CIFAR-100, we report 4 sets of results in Table 1. Namely, constant encoding, constant encoding with RAT, rate-encoding with m=1, and m=10. Under each set there are four training methods, namely CLEAN (training with clean data), GN (training with Gaussian noise), FGSM (adv. training with FGSM attack), PGD (adv. training with PGD attack)\n\n**Please discuss [1]**\n\nAns: The work adopts the existing certified training framework of IBP-CROWN based on bound-propagation, to the Heaviside function used in the SNN network, dubbed as S-IBP and S-CROWN. It is an important line of work, which we have now included in the related work section of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700520855567,
                "cdate": 1700520855567,
                "tmdate": 1700520855567,
                "mdate": 1700520855567,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VaRlMTNuyM",
                "forum": "5bNYf0CqxY",
                "replyto": "cry4evuAdb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6976/Reviewer_wpSa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6976/Reviewer_wpSa"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response to answer my concerns. Considering the authors' responses and the robust training framework, I acknowledge the valuable contribution of this work. The formulation of noise in Theorem 5 is particularly commendable, and I am inclined to raise my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660395681,
                "cdate": 1700660395681,
                "tmdate": 1700660395681,
                "mdate": 1700660395681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4gjahLvoil",
            "forum": "5bNYf0CqxY",
            "replyto": "5bNYf0CqxY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6976/Reviewer_NxbL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6976/Reviewer_NxbL"
            ],
            "content": {
                "summary": {
                    "value": "Firstly, the author established a connection between rate coding and randomized smoothing, theoretically providing robustness guarantees against adversarial perturbations under the l1 norm. Additionally, a novel adversarial training method was introduced for rate coding, significantly enhancing state-of-the-art empirical robust accuracy results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The author establishes a connection between rate coding and randomized smoothing, providing a robustness proof for adversarial perturbations under the l1 norm.\n2. The introduction of a new adversarial training strategy for rate coding notably enhances the state-of-the-art empirical robust accuracy."
                },
                "weaknesses": {
                    "value": "The connection between encoding and robustness has indeed been explored in many previous studies, as evidenced by the references provided:\n1.\t\"Rate Gradient Approximation Attack Threats Deep Spiking Neural Networks\" from CVPR 2023\n2.\t\"HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training with Crafted Input Noise\" from ICCV 2021\n3.\t\"Rate Coding Or Direct Coding: Which One Is Better For Accurate, Robust, And Energy-Efficient Spiking Neural Networks?\" from ICASSP 2022\n4.\t\"Spike timing reshapes robustness against attacks in spiking neural networks\" from Neural Networks\nGiven the substantial amount of existing work on this topic, it does seem that the author's contributions might not be as groundbreaking. Additionally, some studies, like [1], have indicated that SNNs encoded with Rate Coding using LIF neurons can be vulnerable. This further suggests a need for more innovative approaches or a unique perspective to truly stand out in this field."
                },
                "questions": {
                    "value": "1.\tIf the author believes that rate coding is crucial for the robustness of SNNs, then it is essential to conduct experiments on converted SNNs and provide a comprehensive theoretical analysis. This will not only strengthen the validity of the claim but also offer insights into how rate coding impacts the robustness across different SNN architectures.\n2.\tThe author should indeed expand the literature review to include more recent works related to SNN's attack and defense mechanisms. Relying solely on PGD and FGSM limits the scope and depth of the study. By incorporating a broader range of attack methodologies, the author can present a more holistic view of SNN's vulnerabilities and strengths in the face of adversarial attacks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Reviewer_NxbL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6976/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720721270,
            "cdate": 1698720721270,
            "tmdate": 1699636815557,
            "mdate": 1699636815557,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UXbVJEmovh",
                "forum": "5bNYf0CqxY",
                "replyto": "4gjahLvoil",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6976/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6976/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the detailed comments on the paper. We have carefully considered the suggestions and tried to incorporate them. We acknowledge the reference to the existing literature and promise to add them to the future version of the paper. We agree that the connection between encoding and robustness has been explored in various studies. However, we would like to emphasize that our work goes beyond the empirical observations of robustness and provides a provable robustness guarantee for rate-encoded SNNs against adversarial perturbations. We would like to briefly discuss the referred works:\n\n\n \n\n**RGA[1]** explores the rate-encoded nature of spikes that govern information propagation within SNN layers and argues that the rate of spikes is sufficient to encode the information. Following your recommendation, we have incorporated the RGA back-propagation technique, along with its close cousin called Backward Pass Through Rate (BPTR) as given in SNN-RAT(Ding et al., 2022). Thus, for each adversarial attack, we now have three implementations, namely, BPTT, BPTR, and RGA, reported in Table 5, which is now updated in the appendix of the paper.\n\n**HIRE-SNN[2]** explore specially crafted input noise to the constant coded inputs, where the noise is updated within the time-steps. We did not compare with this work directly, for (i). the robust training results were demonstrated on constant-encoding\n(ii). Under constant encoding, SNN-RAT (Table 4 of their paper) shows superior performance against adversarial attacks. However, we refer to their work in our related work section as they compare the spiking activity of constant and rate-encoded SNNs.\n\nThe work **Rate Coding Or Direct Coding**[3] compares the accuracy of rate-encoded and constant encoded SNN trained with clean images under FGSM and PGD attacks and show superior accuracy of rate-encoded SNNs under different attack radius (Fig. 3 of their paper). This is consistent with our claims of superior robustness of rate-encoded SNNs. Our experimental contribution adds to their work (i). by including adversarial training of (smooth) rate-encoded classifier, (ii). by showing a smooth approximation of rate-encoded SNNs can significantly improve the performance of rate-encoded SNNs.  \n\n\n\n**Analysis of converted SNNs** The theory of randomized smoothing holds irrespective of the base classifier that we choose. That is, whether we use the base classifier $f$ (in eqn. 6) from an adversarially trained model or a converted model, the corresponding smooth classifier g, will be robust against adversarial perturbation, i.e., $g(x+\\delta) = g(x)$. However, if the base classifier is unfit, the prediction $g(x)$ can be incorrect, leading to poor clean and robust accuracy. \n\nWith your suggestion, we conducted experiments with ANN-to-SNN converted VGG-16 models on the CIFAR-10 dataset using a recent conversion technique [Bu et. al. 2021]. We find that under rate-encoding, converted SNNs do not have good clean accuracy at small latency T, which is rather a limitation of the base classifier trained to work with direct inputs, without Bernoulli noise. This can even happen to constant encoded SNN trained on clean images if we add large Gaussian noise to the input (please see the table provided in reply to reviewer  FaRA )\n\n**Constant Encoding**\n |Attack|  T=4|  32|  64| 128 |  \n| ----- | ----- | ----- | ----- | ----- | \n | clean|  92.58|  95.47|  95.47| 95.51 |\n | gn |  85.84|  89.1|  88.98| 88.97 |\n | fgsm(C)|  21.02|  14.25|  15.85| 17.56 |\n | pgd(C)|  **0.2**|  **0.03**|  **0.02**| **0.03**| \n | fgsm(R)|  73.59|  64.54|  37.34| 14.7| \n | pgd(R)| 87.46| 82.53| 58.36|11.9 |\n\n\n**Rate Encoding**\n |Attack|  T=4|  32|  64| 128 |  \n | ----- | ----- | ----- | ----- | ----- |\n | clean| 11.83| 24.12| 41.5|63.11|\n| gn | 11.54| 22.8| 38.35|57.4|\n| fgsm(C) | 11.14| 17.43| 23.64|28.8|\n | pgd(C) | 11.24| 17.56| 23.54|26.2|\n | fgsm(R) | 11.49| **16.36**| **18**|**15.76**|\n | pgd(R) | 11.46| 17.54| 18.94|13.92|\n\n It is interesting to note that, while the clean accuracy of rate encoding is lower than that of directly trained SNNs, the robust accuracy (minimum accuracy across among the attacks) of rate encoding surpasses that of SNNs with constant encoding.\n (ii). the clean accuracy of rate encoding keeps improving larger T, but the robust accuracy initially increases (due to better prediction) but eventually drops, possibly due to the effect of T as found in our theory. (T=4: 11.49, T=32:16.36, T=64: 18, T=128: 15.76)\n A similar observation was made in directly trained SNNs, as given in Table 4 in the appendix, where we evaluated the models at T=4 vs T=8.\n (iii) A similar drop in robust accuracy for constant encoded models may hint at the rate-encoded nature of spikes in general SNN layers, as reported by [1].\n\nWe have now included these results in Table 7 in the appendix of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522608679,
                "cdate": 1700522608679,
                "tmdate": 1700523597172,
                "mdate": 1700523597172,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DcAaaxhKRM",
            "forum": "5bNYf0CqxY",
            "replyto": "5bNYf0CqxY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6976/Reviewer_77u8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6976/Reviewer_77u8"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript investigates the adversarial robustness of spiking neural networks (SNN). It establishes a connection between rate-encoding used for SNNs and the randomized smoothing framework from adversarial robustness. It clarifies why rate-encodings are more adversarial robust than constant-encodings, as observed previously in empirical studies. The established theory confirms the empirical observation that this benefit diminishes with increased SNN inference latency. In addition, empirical robustness is investigated using a variety of adversarial training techniques optionally combined with randomized smoothing for both constant and rate encodings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* **Original Contribution:** The paper offers a unique perspective on adversarial robustness for SNNs using rate encoding. The connection between rate encoding and randomized smoothing is a novel and interesting insight contributing to the field.\n* **Theoretical work**: The authors provide a theoretical foundation for the robustness of rate-encoded SNNs. While the results rely on previous work, the extension to SNNs is novel and non-trivial (lemma 3 and 4). \n* **Comprehensive empirical results**: Additionally to the theoretical contributions, the authors combine their results with various adversarial training settings to improve empirical robustness. \n * **Clarity of introduction and background:** The paper is well-structured and provides clear explanations of the relevant concepts, making it accessible to a wide audience, including those not specialized in spiking neural networks or adversarial robustness."
                },
                "weaknesses": {
                    "value": "## Major issues\n* **Claims and limitations**: The manuscript claims to \"introduce a novel adversarial training algorithm ... improves state-of-the-art ...\". Yet it is not very clear what the novelty exactly is (e.g. as also stated by the authors Stalman et al. 2009 did combine adversarial training and randomized smoothing. Is it the adaption to SNNs? The modifications required for the attack?). This should be more clear. Further, the manuscript claims new state-of-the-art performance in the abstract. But only \"compare the performance of the proposed methods with state-of-the-art adversarial training algorithms\" in the Experiments. These are two slightly different statements, and they should be clearly formulated (i.e. what is state-of-the-art (is there a well-established state-of-the-art in the field?), and exactly which proposed method beats it?). While the authors do discuss the obtained certified radius in section 5.3, these certifications seem way smaller than for the ANN case [2]. While a direct comparison is not necessary, this should be discussed more explicitly. \n\n* **Empirical details and clarity of experimental results**: The manuscript lacks a more comprehensive description of experimental details, which should be added to the appendix i.e., how many PGD iterations, all the training details for all adversarial training methods, the cost of training with the different methods (GPUs, time), \u2026 . There also is only a brief discussion of the empirical results, and some of the results are unclear/surprising to me (see questions). This should be discussed in more detail.\n\n* **Comparison between certified and empirical robustness**: The manuscript does analyze both the empirical and certified robustness. Yet, does treat them mostly independently. It would be helpful to e.g. take Fig 1 (d) (i.e. the empirical robustness for various epsilon) and also plot the certified robustness (with radius r=epsilon). This information is already displayed in Table 3 (although for a much smaller radius). This would nicely showcase the gap between empirical and certified robustness. While the authors do not claim tightness of the certifications, this at least should be discussed/visible.\n\n\n\n## Minor issues and recommendations:\n\n* The authors state that certified defense methods based on convex relaxations and branch and bound are restricted to ReLU activations. This is incorrect as [1], does generalize to general activation functions. The claim that these methods do not apply to SNNs may still hold and should be checked by the authors.\n* Some of the proofs rely on the differentiability of g with respect to x. It might be good to point out that g is differentiable with respect to x, even if f is not differentiable with respect to z (as is the case for SNNs, just for clarity).\n* There are some language errors (e.g. page 2 row 25) and formatting errors (whitespace in front of citations e.g. page1 first paragraph last row, page 2 row 11, ...).\n\n\n\n\n[1] Formal Verification for Neural Networks with General Nonlinearities via Branch-and-Bound  (Zhouxing Shi et. al.)\n\n[2] https://sokcertifiedrobustness.github.io/leaderboard/"
                },
                "questions": {
                    "value": "Multiple open questions about results in Table 1, for example:\n* Why do fgsm(R) and pgd(R) attacks not work on models trained with constant encoding? As fgsm(C) and pgd(C) do work well as expected, shouldn't this indicate that rate-encoding messes up the attack algorithms (i.e., because the gradients become noisy)?\n* As one tests attacks on both encodings (C/R), it might be good also to have clean (C/R), no? (it is rather interesting to me that models trained on C, do work that well on fgsm(R) and pgd(R) and visa versa)\n* Adversarial training is performed both for FGSM and PGD. As PGD is the \u201cstronger\u201d adversary, I would expect it to be more robust, yet it is not. Especially, the PGD-adversarially trained network performs worse than an FGSM-adversarially trained network if tested against a PGD adversary. This result seems counterintuitive and is consistent across different datasets. Why? What PGD adversary was employed during training (as the evaluation PGD is as expected better than FGSM) ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6976/Reviewer_77u8",
                        "ICLR.cc/2024/Conference/Submission6976/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6976/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793153520,
            "cdate": 1698793153520,
            "tmdate": 1700668154602,
            "mdate": 1700668154602,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D5Hy464Gjb",
                "forum": "5bNYf0CqxY",
                "replyto": "DcAaaxhKRM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6976/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6976/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are grateful to the reviewer for detailed review and insightful comments. We list our answers below:\n\n\n**Specify algorithmic novelty:** As suggested by the reviewer, we now mention in the paper, that the algorithm novelty lies in adapting the randomized smoothing technique for rate-encoded SNN. More concretely, \n(i) Table 1, improves the accuracy of rate-encoded classifiers through better approximation of the smooth classifier, i.e., we could observe results with m=10, are better than m=1.\n(ii) Adversarial training of rate-encoded classifier, by employing rate-encoded adversarial attacks are also reported first time to our knowledge. \n\n**State-of-the-art:** We intend to say the state-of-art in adversarial training, we compare our results with another existing adversarial training algorithm, SNN-RAT, and show improvement in robust accuracy (i.e., minimum empirical accuracy found under any attack). (FGSM(C):39.75, FGSM(C)+RAT: 42.76 vs. FGSM(R): 51.63)\n\n**Comparison of certified and empirical robustness:** We agree with the reviewer to discuss this gap in detail in the paper. However, we would like highlight, that the certified radius for robustness in Table 3 is obtained under $l_1$-norm, while figure 1(d) reports attack radius under $l_\\infty$-norm, as customary for the FGSM/PGD attacks. Thus, these results can not be plotted together unless we perform some bound transfer for the certified radius.\n\n**Training Details:** As suggested by the reviewer, we have added all the training hyper-parameters/details in the appendix. We answer more specific questions next.\n\n**Why do fgsm(R) and pgd(R) attacks not work on models trained with constant encoding?**\n\nAns: We would like to clarify how fgsm(R) works on a constant encoded model. To find the perturbation $\\delta$, we use rate-encoding of $x$, compute the loss, perform the gradient ascent step, and find $\\delta$. Now the perturbed image stands to be $x+\\delta$, which we evaluate with constant encoding. Thus, $C/R$ here stands for the encoding used to compute the attack,\nhowever, once the attack is found, the perturbed input is evaluated with default model encoding.\n\nWe observe, when a model is trained on a particular encoding and the attack also uses the same encoding, the attacks are more effective in fooling the model parameters. As $\\delta$ found in this process is not informative enough, fgsm(R)/pgd(R) attacks are not able to find effective perturbation on a constant encoded model. And similarly, fgsm(C)/pgd(C) do not work well on the rate-encoded models. Please see this answer in conjunction with the next answer.\n   \n\n\n\n**As one tests attacks on both encodings (C/R), it might be good also to have clean (C/R).**\nAns:  As clarified above, although the attack fgsm(R) uses rate encoding to find $\\delta$, the perturbed image $x+\\delta$ is evaluated using the model encoding. Thus, while evaluating a clean image, since there is no attack involved, we chose the same encoding as the model encoding.\n\nHowever, as the reviewer suggests, we can always evaluate a model trained with constant encoding with rate encoding, and vice versa. But as the true model weights are not trained on these inputs, the clean accuracies are not good enough. We report the accuracies below:\n\n\n\n**CIFAR-10, Train: Constant Encoding**\n|               | CLEAN  | GN | FGSM(C)| PGD(C) |\n| ----------- | ----------- |----------- |----------- |----------- |\n| clean(C)  | 92.15 | 91.7 | 79.4 | 79.15 |\n| clean(R)   | 13.93 | 20.01 | 11.55 | 19.88 |\n\n**CIFAR-10, Train: Rate Encoding**\n|               | CLEAN  | GN | FGSM(R)| PGD(R) |\n| ----------- | ----------- |----------- |----------- |----------- |\n| clean(R)  | 79.55 | 79.36 | 76.89 | 76.36 |\n| clean(C)  | 12.01 | 15.66 | 39.11 | 13.63 |\n\nThe results further explain the reason fgsm(R) attack does not work well with constant encoded models, as to compute the perturbation, we should start with output which is correct. As the clean accuracy with the opposite encoding is itself not very good, the perturbations found by the attack are not effective.\n\n However, the result that rate-encoding does not work on a model trained with constant encoding should not be seen as a limitation of the rate-encoding technique. The randomized smoothing technique only guarantees that the prediction of the smooth classifier will not change upon perturbation of the input, which does not require that the prediction of the smooth classifier in the first place is correct. To obtain robustness along with correct prediction, we need to supply a good base classifier, such as the different classifiers (adversarially) trained with rate encoding."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518384838,
                "cdate": 1700518384838,
                "tmdate": 1700518384838,
                "mdate": 1700518384838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rwZIxbebEE",
                "forum": "5bNYf0CqxY",
                "replyto": "DQfr1qYd9u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6976/Reviewer_77u8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6976/Reviewer_77u8"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the efforts you put into addressing the concerns raised in my review. I appreciate the clarifications and additional details provided in your response.\n\nWith the provided clarifications and updated results, the empirical results are now in agreement with my expectations. The authors addressed my concerns, and hence, I tend to raise my rating.\n\nI think one could still include a better comparison between certified and empirical robustness. While I acknowledge that the current results are based on different norms, hence hard to compare. It is rather easy to modify both FGSM and PGD to work with L1-norms (but would require additional work). The revised manuscript did introduce some formatting issues i.e., no white space between text and citation or no brackets at all (e.g., at the end of the introduction, \u2026). This should be fixed for the final version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6976/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668116318,
                "cdate": 1700668116318,
                "tmdate": 1700668116318,
                "mdate": 1700668116318,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]