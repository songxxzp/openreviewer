[
    {
        "title": "Graph Decoding via Generalized Random Dot Product Graph"
    },
    {
        "review": {
            "id": "YcCnDrseV5",
            "forum": "AxYTFpdlvj",
            "replyto": "AxYTFpdlvj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3375/Reviewer_hJLN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3375/Reviewer_hJLN"
            ],
            "content": {
                "summary": {
                    "value": "This paper identifies the problem of the current GNN's inability to reconstruct a graph whose adjacency matrix has negative eigenvalues. It then proposes a method to manually negate several entries of the embedding vectors of nodes to allow negative eigenvalue. Experimental results show consistent improvement across datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The identified problem is interesting and allowing models to generate an adjacency matrix with negative eigenvalues is a critical task. The authors explain the intuition behind the negative eigenvalue adjacency matrix well."
                },
                "weaknesses": {
                    "value": "The presentation needs significant improvement. The use of bold/unbold letters to represent vectors is inconsistent, and some subscripts are missing.  While the core idea is interesting, it is poorly presented. As the authors emphasize the importance of the negative eigenvalues \nand mention irregularity in graphs multiple times, there is no support for how often and when negative eigenvalues appear. (Though toy examples are provided in the appendix.) Some justification should be presented, either empirically obtained from data or theoretically quantified.\n\nThe proposed method is also without in-depth analysis. While the proposed matrix multiplication does allow negative eigenvalues, the effectiveness is unclear and never justified in the paper. Also, the negate matrix is manually picked, which is not flexible for more graphs (as the authors acknowledged in the conclusion)."
                },
                "questions": {
                    "value": "- The experiments only compared the proposed method with the backbone GNN. However, the advances recent GNN significantly outperforms GAE on various tasks. How does the proposed method compare to those methods.\n\n- Does the generated matrix indeed have negative eigenvalues? If so, does the original matrix that the GNN is trained on also contain negative eigenvalues?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3375/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3375/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3375/Reviewer_hJLN"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698181733721,
            "cdate": 1698181733721,
            "tmdate": 1699636288095,
            "mdate": 1699636288095,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gh5pyuZPPI",
                "forum": "AxYTFpdlvj",
                "replyto": "YcCnDrseV5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3375/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, we thank you for your contribution to our work. We wanted to address some of the comments you left in your review. \n\nWe purposely choose the vanilla implementations of such architectures since we believe that the proposed methodology only makes sense when compared to the traditional inner product decoder. \n\nYes, both of them have negative eigenvalues. The adjacency matrix normalisation operations in GCNs do not alter the spectra of negative eigenvalues beyond their scale, and the generative model indeed generates negative eigenvalues, as stated in the text."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738484024,
                "cdate": 1700738484024,
                "tmdate": 1700738484024,
                "mdate": 1700738484024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oji0ykRqdc",
            "forum": "AxYTFpdlvj",
            "replyto": "AxYTFpdlvj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3375/Reviewer_io5N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3375/Reviewer_io5N"
            ],
            "content": {
                "summary": {
                    "value": "This work introduce a graph decoder by employing the Generalized Random Dot Product Graph (GRDPG) as a generative model for graph decoding. This methodology significantly enhances the performance of encoder-decoder architectures across a range of tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Clear and straightforward illustraction of method."
                },
                "weaknesses": {
                    "value": "1. Novelty is limited.  Equation (3), the center of the method, has been proposed in [1].\n\n2. The meaning of negative eigenvalue in adjacency matrix remains unclear. Does it corresponds to some specific substructure or some graph statistics?\n\n3. Uncertainties are not provided in experiment results.\n\n4. Though this work proposes a decoder, generation tasks are not included in experiments.\n\n[1] Patrick Rubin-Delanchy, Joshua Cape, Minh Tang, and Carey E. Priebe. A statistical interpretation of spectral embedding: the generalised random dot product graph. arXiv:1709.05506, 2021."
                },
                "questions": {
                    "value": "1. Lorentz model in hyperbolic GNN [1] also uses a diagonal matrix with $\\pm 1$. Could you please compare your Equation 3 with hyperbolic GNNs?\n\n2. What is the detailed meaning of negative eigenvalues in graph? Some specific graph structures?\n\n3. Can use approximate graph laplacian matrix ($L=D-A$) instead of adjacency matrix to avoid the positive-semi definite problem?\n\n[1] Menglin Yang, Min Zhou, Zhihao Li, Jiahong Liu, Lujia Pan, Hui Xiong, Irwin King. Hyperbolic Graph Neural Networks: A Review of Methods and Applications. CoRR abs/2202.13852."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698405995455,
            "cdate": 1698405995455,
            "tmdate": 1699636288006,
            "mdate": 1699636288006,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sCkWNnYIQP",
                "forum": "AxYTFpdlvj",
                "replyto": "oji0ykRqdc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3375/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, we thank you for your contribution to our work. We wanted to address some of the comments you left in your review. \n\nWe were well aware of the contribution made by [1], we believe that even though our work is not catered around the development of the kernel, its application to a graph decoding process in the context of GAEs provides novelty to the field.\n\nWe did not delve into the topological effects of the presence of negative eigenvalues, but it is clearly something we want to pursue shortly. Nevertheless we could informally link it with the presence of cycles and triangles.\n\nWe believe that uncertainties are provided in the results tables in the form of measured deviations from the mean in the Supplementary Material. We will include them in the main text in future versions of the work.\n\nThank you for your question regarding the comparison between our Equation 3 and the Lorentz model in hyperbolic GNNs. Our Equation 3, given by, provides a framework for computing the probability of an adjacency matrix\nUnlike the Lorentz model, which is specifically designed for hyperbolic space with its constant negative curvature, our model is more general. The diagonal matrix in our equation does not inherently assume a hyperbolic structure and could potentially be adapted to represent other types of geometric metrics, including Euclidean space.\nWhile the Lorentz model is adept at handling data with hierarchical structures characteristic of hyperbolic space, our model aims to be more versatile, allowing for adjustment of the diagonal matrix to cater to a wider range of graph topologies and data geometries. This adaptability makes our Equation 3 potentially applicable to a broader spectrum of graph-based learning tasks. It is important to note, however, that if it is specified to reflect hyperbolic geometry, our model could operate in a manner similar to the Lorentz model, thus serving as a specific case within our more general framework.\n\nAbout using a Laplacian. Indeed this is a valid approach, however, adopting this paradigm would heavily complicate the generative paradigm of the model. Implementing this model, would duplicate the target matrices needed, as the degree matrix D would also need to be generated for the adjacency matrix reconstruction. We consider this to be out of the scope of our proposed article, however, we will consider this further for future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738602712,
                "cdate": 1700738602712,
                "tmdate": 1700738602712,
                "mdate": 1700738602712,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y1BL7XaPjY",
            "forum": "AxYTFpdlvj",
            "replyto": "AxYTFpdlvj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3375/Reviewer_ggnG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3375/Reviewer_ggnG"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a modification to the decoder, i.e. the dot product of the existing graph encoder-decoder framework called GRDPG. It intends to empower the decoder to represent the adjacency with negative eigenvalues. Experiments show some improvement."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The proposed method can be flexibly applied on any existing dot-product-based reconstruction functions.\n- Experiments show some improvement somehow."
                },
                "weaknesses": {
                    "value": "- The paper is skeptically to be considered as lacking good structure, especially the connection between the dot product and its deficiency of presenting negative eigenvalues.\n- Very often the sentences are repeated, presenting the same meaning, e.g. the first two paragraphs on page 2, the fourth paragraph on page 3, and so on.\n- Typos are quite frequent, e.g. the third equation $p(Z|X,A \\$ (also there is no index for all equations or formulas except those in the method), $Z=z_i^Tz_j$, and so on.\n- The experimental results are not convincing, especially on the structure perception link prediction task, and also the random seeds and variances are missing to report.\n- Overall, the quality of the paper is far below the requirements of ICLR."
                },
                "questions": {
                    "value": "- My first question is that it would be nice to make sure that the existing papers that hold the semi-positive defined assumption of the probability of reconstruction is entailed by its assumption of undirected graphs.\n- It is highly recommended to further investigate which type of $I_{p,q}$ works best for different graph structures.\n- Why is node clustering an appropriate experiment to verify the model? Should it be more related to the task of graph structure perception?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3375/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3375/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3375/Reviewer_ggnG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792141423,
            "cdate": 1698792141423,
            "tmdate": 1699636287909,
            "mdate": 1699636287909,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pwwVb1lc4E",
                "forum": "AxYTFpdlvj",
                "replyto": "y1BL7XaPjY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3375/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, we thank you for your contribution to our work. We wanted to address some of the comments you left in your review. \n\nIn future submissions, we will address the relationship between graph structure and the type of I. We see that elucidation of this point can bring value to the presented work.\n\nNode clustering is a standard task for unsupervised graph representation learning. The objective of this task is to showcase the increased capability of algorithms in capturing the topology of the graph, trying to reproduce node labels without explicitly optimising for them. For further information the reviewer can check the following paper: Pan, S., Hu, R., Long, G., Jiang, J., Yao, L., & Zhang, C. (2018). Adversarially Regularized Graph Autoencoder for Graph Embedding. arXiv:1802.04407"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738443855,
                "cdate": 1700738443855,
                "tmdate": 1700738443855,
                "mdate": 1700738443855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ruMyeeKrke",
            "forum": "AxYTFpdlvj",
            "replyto": "AxYTFpdlvj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3375/Reviewer_AHQZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3375/Reviewer_AHQZ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose to use a GRDPG-based decoder, instead of the classical RDPG or inner-product decoder, in the framework of GAEs and GVAEs.\n\nThe introduction of the paper is very good. However, the rest of the paper is very repetitive, and the important details are missing.\n\nI think that four pages to describe the impact of negative eigenvalues is too much. And the paper jumps from the description of the GRDPG (Section 3) to the experiments (Section 4), without explaining the proposed method, architecture, or something."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The introduction is very good, as said before. And the idea of using GRDPGs as decoder architecture is interesting, but I think it needs more work."
                },
                "weaknesses": {
                    "value": "Throughout the paper, the authors insist on non-bipartite graphs, as if it was a requirement or something. Even for showing the existence of graphs with negative eigenvalues, while it would have made much more sense to say, \"bipartite graphs have spectra of $A$ symmetric around 0\".\n\nIn this sense, Section 2.2 (and Appendix A) are not needed at all.\n\nThe GRPDG method requires knowing in advance the number of positive/negative eigenvalues, so it's very important to know how the authors addressed this. Is it always the same? Is it chosen for each dataset? How? \nIf I guessed correctly, the authors try several different values of $q$ (for each dataset maybe?) and keep the result with the best performance.\nThe lack of information on this crucial part is not acceptable."
                },
                "questions": {
                    "value": "In the preliminary section, the description for the GAE loss functions says the expectation with respect to some q, which is not defined before.\n\nMinor comments:\n - the equations are part of the text. The punctuation after the equation is missing in general."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3375/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890863343,
            "cdate": 1698890863343,
            "tmdate": 1699636287837,
            "mdate": 1699636287837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P4FXH78IJl",
                "forum": "AxYTFpdlvj",
                "replyto": "ruMyeeKrke",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3375/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3375/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer, we thank you for your contribution to our work. We wanted to address some of the comments you left in your review. \n\nWe stress the fact that bipartite graphs, by definition, are a case that fulfils the proposed requirements for negative eigenvalues even though they are not subject of study in this work purposely. \nThe value of q, the proportion of negative eigenvalues, is searched by greedy search of the parameter per residue. This work does not establish any relationship between q and the data.\n\nThe equation defining the GAE is extracted from Kipf, T. N., & Welling, M. (2016). Variational Graph Auto-Encoders. Advances in Neural Information Processing Systems. arXiv:1611.07308 (2016). There, in eq. (1) is stated q as q(Z | X, A) = QN i=1 q(zi | X, A), with q(zi | X, A) = N (zi | \u00b5i , diag(\u03c3 2 i ))."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3375/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738407164,
                "cdate": 1700738407164,
                "tmdate": 1700738407164,
                "mdate": 1700738407164,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]