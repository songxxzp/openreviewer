[
    {
        "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?"
    },
    {
        "review": {
            "id": "icdODKUceu",
            "forum": "vSh5ePa0ph",
            "replyto": "vSh5ePa0ph",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7726/Reviewer_dEpy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7726/Reviewer_dEpy"
            ],
            "content": {
                "summary": {
                    "value": "Using a modified version of a single-layer linear attention model, the authors derive a dimension-independent complexity bound which suggests that efficient pretraining is possible even with a large number of model parameters for effective in-context learning. In the process, the authors demonstrate novel techniques for analyzing higher-order tensors that may be independently applicable."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors provide a timely contribution to an important phenomenon of in-content learning.\n- I am positive that this work provides a good theoretical checkpoint for future work to build on top of the provided analysis. \n- The text is fairly well-presented and the research community will appreciate it as such.\n- I also specially highlight that the authors have made proper effort to delineate the assumptions behind the theoretical results."
                },
                "weaknesses": {
                    "value": "- The obvious shortcomings of the theoretical results come from the nature of assumptions that deviate from standard practice - the choice of linear attention and a restricting the structure of Q/K/V matrices. Also see Question 1.\n- While the community is focused on Transformers, I am wondering if the analysis also holds for a different linear parametrization of the function $f$. In the broader context, if similar results hold for another parametrization, then attention would not appear that unique of a function. As an example, imagine a different parametrization that also leads to a dimension-free bound. In the broader context, such a result would then not distinguish what attention brings to the table, when in practice we have ample evidence that other parametrizations don't carry as flexible and generalizable inductive biases.\n- Regarding the distributional assumptions of the fixed sized dataset in Assumption 1, it would be great to have experiments where the model is misspecified. Please correct me if I missed, but I don't think I see such an experiment in Appendix A. Misspecification is really important, since for all practical purposes, our models are misspecified, i.e. the data does not really come from the distribution we assume to be. If Transformers can still achieved a good decay of the empirical risk as the number of pretraining tasks increase, it would certainly be a unique characteristic. Also see Question 2.\n- I would strongly recommend highlighting the experiments and moving them up to the main text. Perhaps Section 6 could be compressed a little to accommodate."
                },
                "questions": {
                    "value": "1. Could the authors confirm if the restrictions are significantly restrictive? It would appear significant at face value. I certainly don't discount the fact that the work aims to provides theoretical grounding for similar observations in literature.\n2. In Theorem 4.1, how is the inner product between matrices defined?\n3. It would appear that the theorems do not necessarily say anything about misspecification, except that the excess risk is controlled via terms in Equation 8. In a sense, the Gaussian assumptions are used for derivations so anything in the Gaussian family would qualify for the bound. Is that the correct assessment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698095027864,
            "cdate": 1698095027864,
            "tmdate": 1699636942514,
            "mdate": 1699636942514,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1G6B00C8Ns",
                "forum": "vSh5ePa0ph",
                "replyto": "icdODKUceu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dEpy"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback!\n\n---\n\n**Q1**. The obvious shortcomings of the theoretical results come from the nature of assumptions that deviate from standard practice - the choice of linear attention and a restricting the structure of Q/K/V matrices\u2026 Could the authors confirm if the restrictions are significantly restrictive?... I certainly don't discount the fact that the work aims to provide theoretical grounding for similar observations in literature.\n\n**A1**. We acknowledge that our setup deviates from practice by considering linear attention and linear reparameterization. We think the statistical bounds of SGD-trained $\\\\Gamma$ and SGD-trained standard softmax attention model might not be identical. One challenge is that our loss function is convex, but the loss for standard softmax attention is non-convex. So there will be an extra layer of complexity from non-convex optimization in the latter setup. With that being said, our results still provide meaningful insights into the statistical task complexity of ICL. Without a full understanding of the statistical behavior of SGD in the simplified setup, it seems difficult that one can achieve this in more complicated cases. \n\n---\n\n**Q2**. \u201cWhile the community is focused on Transformers, I am wondering if the analysis also holds for a different linear parametrization of the function $f$. In the broader context, if similar results hold for another parametrization, then attention would not appear that unique of a function\u2026 In the broader context, such a result would then not distinguish what attention brings to the table, when in practice we have ample evidence that other parametrizations don't carry as flexible and generalizable inductive biases.\u201d\n\n**A2**. We would like to highlight that attention provides a natural way to parameterize gradient descent (GD), which is a key inductive bias for the model to achieve Bayes-optimal ICL. In equation (1), we perform a linear reparameterization to simplify our analysis, but our simplification still allows equation (1) to realize a GD procedure (in terms of the mean square loss). Other linear parameterization of $f$, while might be easy to pretrain, may not correspond to a meaningful GD procedure, so it might not be able to achieve Bayes-optimal ICL. Therefore, the inductive bias carried by the attention mechanism is not easily replaceable according to our theory in Section 5.\n\n---\n\n**Q3**. ... it would be great to have experiments where the model is misspecified.\u2026 It would appear that the theorems do not necessarily say anything about misspecification\u2026. the Gaussian assumptions are used for derivations so anything in the Gaussian family would qualify for the bound. Is that the correct assessment?\n\n**A3**. This is a great point. You are correct that neither our current theory nor our current experiments cover misspecified linear regression. Our theory can be extended to deal with non-Gaussian distributions with proper (up to 8-th) moment conditions. However, extra efforts would be required to extend our theory to deal with the correlations between noise and covariates. \n\nAccording to your suggestion, we conduct experiments for our attention model in misspecified settings. Please refer to Figure 4 in Appendix A in the revised paper. When we replace the Gaussian noise with an independent and uniformly distributed noise, the attention model can still perform good ICL. However, when the expected response is generated from a nonlinear function (e.g., square and sigmoid functions), the performance of ICL will decrease. \n\n---\n\n**Q4**. I would strongly recommend highlighting the experiments and moving them up to the main text. Perhaps Section 6 could be compressed a little to accommodate.\n\n**A4**. We will move our numerical results to the main body provided we are granted additional pages in the final version.  \n\n\n---\n\n**Q5**. In Theorem 4.1, how is the inner product between matrices defined?\n\n**A5**. For two matrices $A$ and $B$, we define $\\\\langle A, B \\\\rangle = \\\\text{tr}(A\\^\\\\top B)$. We have clarified this (in blue text) before Theorem 3.1 in the revised paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519031009,
                "cdate": 1700519031009,
                "tmdate": 1700519031009,
                "mdate": 1700519031009,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gLoZFvTvKc",
                "forum": "vSh5ePa0ph",
                "replyto": "1G6B00C8Ns",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7726/Reviewer_dEpy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7726/Reviewer_dEpy"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for all your comments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522215838,
                "cdate": 1700522215838,
                "tmdate": 1700522215838,
                "mdate": 1700522215838,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "18Hs2MtTK7",
            "forum": "vSh5ePa0ph",
            "replyto": "vSh5ePa0ph",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7726/Reviewer_m56U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7726/Reviewer_m56U"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a statistical task complexity bound for a one-layer linear attention model in solving fixed-length linear regression problems. The authors investigate the required number of independent sequences or tasks necessary for pretraining the model. They conclude that only a small number of sequences are needed compared to the model parameters, and this is independent of the input dimension. Additionally, the paper theoretically demonstrates a performance drop when the number of in-context examples during evaluation differs from the training phase."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-organized, with clear contributions and promising results in the statistical analysis of in-context learning for linear regression problems using a one-layer linear attention model.\n2. The theoretical results provided in the paper address the sample complexity of learning linear regression problems, highlighting that it is approximately $O(1/T)$. This indicates that the number of tasks required for learning the model is independent of both the model size and the input dimension ($d$).\n3. The paper underscores the significance of context length during testing, proving that optimal prediction is attainable only when the number of context examples matches that of the pre-training phase."
                },
                "weaknesses": {
                    "value": "1. While the theoretical analysis presented is commendable and contributes to our understanding, the paper could be greatly enhanced by including more empirical results. Given that implementing linear regression over a transformer model is feasible and has been achieved in prior work, specific implementations that could improve this paper include:\n\n    - Demonstrating through examples that training models on linear tasks of varying dimensions yields similar performance, supporting the paper's claim that task complexity is independent of the task dimension $d$.\n    - Conducting experiments to show that a pre-trained model performs poorly when there is a significant discrepancy between the in-context length during testing and training.\n2. The paper\u2019s assertion that task complexity is independent of $d$ is counterintuitive. It is typically expected that the number of tasks required would be on the order of $O(d^2/T)$ since the task distribution or covariance, encompassing at least $d^2$ parameters, needs to be retrieved. The authors are encouraged to provide additional clarification on this aspect of independence. Furthermore, the experiments presented seem to contradict the paper\u2019s statement, as $T$ appears to be exponentially larger than $d^2$, which doesn\u2019t align with the claim that \"$T$ could be much smaller than $d^2$\".\n\n3. Despite Equation (9) not explicitly showing dependence on $d$, it seems that $d$ is implicitly involved in the matrix operations, which should be addressed for clarity."
                },
                "questions": {
                    "value": "1. Is it possible to ensure that the first term of Equation (9) will consistently reach zero? Given the exponentially decreasing learning rate $\\gamma_t$, this relationship does not seem immediately apparent.\n2. In Theorem 4.1, the learning rate $\\gamma_0$ is upper bounded. Could the authors elaborate on the reasoning behind this? Is it connected to the implicit convergence rate of gradient descent in linear problems? Additionally, what are the considerations and trade-offs in selecting an appropriate initial learning rate $\\gamma_0$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7726/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7726/Reviewer_m56U",
                        "ICLR.cc/2024/Conference/Submission7726/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784072637,
            "cdate": 1698784072637,
            "tmdate": 1700688645600,
            "mdate": 1700688645600,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "25WkXTZtRa",
                "forum": "vSh5ePa0ph",
                "replyto": "18Hs2MtTK7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer m56U"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback!\n\n---\n\n**Q1-1**. Demonstrating through examples that training models on linear tasks of varying dimensions yields similar performance, supporting the paper's claim that task complexity is independent of the task dimension $d$....\n\n**A1-1**. We have provided Figure 2 in Appendix A in the revised paper, which plots curves for risk vs. dimension. The results are consistent with our theory and suggest that the risk does not explicitly depend on the ambient dimension. \n\n---\n\n**Q1-2**. Conducting experiments to show that a pre-trained model performs poorly when there is a significant discrepancy between the in-context length during testing and training.\n\n**A1-2**. We have provided Figure 3 in Appendix A in the revised paper, plotting curves for risk vs. number of context examples. We observe that when $M$ and $N$ are different (up to $4$ times), the pretrained model exhibits relatively stable performance compared to optimal ridge regression. We suspect that only when $M$ and $N$ are drastically different, the pre-trained model will deviate from optimal ridge regression. We will run additional experiments to verify this.\n\n---\n\n\n**Q2**. The paper\u2019s assertion that task complexity is independent of $d$ is counterintuitive. It is typically expected that the number of tasks required would be on the order of $O(d\\^2 / T)$ since the task distribution or covariance, encompassing at least $d\\^2$ parameters, needs to be retrieved. \u2026Furthermore, the experiments presented seem to contradict the paper\u2019s statement, as $T$ appears to be exponentially larger than $d\\^2$, which doesn\u2019t align with the claim that \"$T$ could be much smaller than $d\\^2$\n\n**A2**. We would like to clarify that our effective dimension $D\\_{\\\\text{eff}}$ does not explicitly depend on $d$ but it still depends on the eigenvalues of $H$. Though we always have $D\\_{\\\\text{eff}} \\\\le d\\^2$, the equality can be attained in the worst case (e.g., $H=I$ and $T$ is large). Therefore, our bound in the worst case would be on the order of $\\\\tilde{O}(d\\^2 / T)$, which is consistent with your intuition. \n\nFor Figure 1(a) in the initial version, we would like to point out that $d\\^2 = 100\\^2 \\approx 2\\^{13.28}$, so the sample complexity is not exponential in the number of parameters and does not contradict our theory. \n\n---\n\n**Q3**. Despite Equation (9) not explicitly showing dependence on $d$, it seems that $d$ is implicitly involved in the matrix operations, which should be addressed for clarity.\n\n**A3**. Please refer to our **A2** in the above.\n\n---\n\n**Q4**. Is it possible to ensure that the first term of Equation (9) will consistently reach zero? Given the exponentially decreasing learning rate $\\\\gamma\\_t$, this relationship does not seem immediately apparent.\n\n**A4**. Yes. Note that the learning rate is piecewise constant and is only decreased by a constant factor after every $\\\\log(T)$ steps. So the constant initial learning rate $\\gamma_0$ is used in the first $T/\\\\log(T)$ steps. As a consequence, the first term in equation (9) tends to zero consistently as $T$ tends to infinity. \n\n---\n\n**Q5**. In Theorem 4.1, the learning rate $\\\\gamma\\_0$ is upper bounded. Could the authors elaborate on the reasoning behind this? Is it connected to the implicit convergence rate of gradient descent in linear problems? Additionally, what are the considerations and trade-offs in selecting an appropriate initial learning rate $\\\\gamma\\_0$?\n\n**A5**. If the initial learning rate $\\\\gamma\\_0$ is too large, the SGD could diverge. The upper bound on $\\\\gamma\\_0$ is a technical condition to ensure the convergence of SGD. \n\nFor a larger initial learning rate, the first term (i.e., bias error) in equation (9) will be smaller but the second term (i.e., variance error) in equation (9) will be larger. So an optimal initial learning rate requires a balance between the bias and variance errors. Prior knowledge such as the norm of $\\\\Gamma\\^*\\_N$ and the scale of $\\\\psi\\^2 tr(H) + \\\\sigma\\^2$ could be useful in choosing the initial learning rate."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518874839,
                "cdate": 1700518874839,
                "tmdate": 1700518874839,
                "mdate": 1700518874839,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yKQeF4OX3V",
                "forum": "vSh5ePa0ph",
                "replyto": "25WkXTZtRa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7726/Reviewer_m56U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7726/Reviewer_m56U"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their response. After reviewing the revised paper and the additional experiments provided, I have decided to increase my evaluation score. However, the paper would benefit from a more thorough discussion on the implicit relationship between data dimension and sample complexity, as well as the choice of learning rate."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688628923,
                "cdate": 1700688628923,
                "tmdate": 1700688628923,
                "mdate": 1700688628923,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qVGZfHJ7BU",
            "forum": "vSh5ePa0ph",
            "replyto": "vSh5ePa0ph",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7726/Reviewer_HbBb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7726/Reviewer_HbBb"
            ],
            "content": {
                "summary": {
                    "value": "The paper dives into the statistical understanding of pre-training of a single linear layer attention on in-context examples generated with Gaussian data and linear regression with a Gaussian prior.  The important theoretical contributions can be summarized as follows.\n\n(a) The authors first show the optimal solution and its excess risk in terms of the pre-training sequence lengths and the covariance matrix underlying the Gaussian data.\n\n(b) The authors then show the behavior of the excess risk with training steps via SGD, where each step uses a randomly sampled sequence from the underlying distribution.\n\n(c) The authors further show that when evaluation sequence lengths match to the training sequence lengths, the model matches the Bayes optimal solution. However, discrepancies between the two can lead to sub-optimalty. \n\nOverall, the paper takes an important step toward statistical understanding of the dependence of pre-training data and in-context abilities of attention models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main strength of the paper lies in its clinical approach to relate the existing literature on ridge regression with Gaussian prior to the statistical understanding of linear attention pre-training. Furthermore, the authors introduce novel techniques like operator polynomials to solve order-8 tensors that show up in the risk analysis of SGD training, which might be of independent interest to the community. \n\nThe advantages of the theoretical framework can be summarized as follows. First, a single linear attention model reaches the optimal linear regression solution with SGD. The framework gives statistical convergence bounds with dependence on training sequence lengths and the data covariance matrix. Secondly, one can pinpoint the gaps between evaluation and training based on the discrepancies in data properties e.g. sequence length. Overall, the paper will be an important addition to the theory community."
                },
                "weaknesses": {
                    "value": "Overall, the paper doesn't have many pitfalls and issues as is.\n\nI have a question about the theoretical setup. Linear attentions used in practice have a query $Q$, key $K$, and value $V$ matrix. However, the authors use structural modifications in $Q$, $K$, and $V$ to represent the formulation with a single matrix $\\Gamma$ and compute SGD convergence of $\\Gamma$. Without the modification, I believe the optimal solution can be shown to be a $3$-matrix factorization of the optimal solution $\\Gamma^*$ given in theorem 1. But what will be the statistical bounds of training these $3$ matrices (or any pair among the $3$ matrices)? Can the authors discuss whether it is answerable from their theoretical framework and if not, the difficulties one might face to solve?\n\nFurthermore, how will the theory change when instead of using training sequences of a single length, we use randomly sampled training sequences with varying lengths? How will the excess risk in theorem 5.3 change then? \n\nThe authors also conduct a few experiments in the appendix on a real-world transformer to verify their theoretical claims. It would be interesting to empirically check the training time convergence with different singular value behaviors (as they pick in corollary 4.2) and observe differences in convergence and ICL performance throughout training.\n\nFinally, I haven't looked deeply into the proof, since it is extremely long to read through. But at a glance, the paper seems to be utilizing similar proof techniques that prior works have used for ridge regression with Gaussian prior. Hence, I still recommend strong acceptance."
                },
                "questions": {
                    "value": "Please see my questions in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848134248,
            "cdate": 1698848134248,
            "tmdate": 1699636942253,
            "mdate": 1699636942253,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "baRvNRtzaO",
                "forum": "vSh5ePa0ph",
                "replyto": "qVGZfHJ7BU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HbBb"
                    },
                    "comment": {
                        "value": "Thank you for your strong support!\n\n---\n\n**Q1**. ... Without the modification, I believe the optimal solution can be shown to be a 3-matrix factorization of the optimal solution $\\\\Gamma\\^*$ given in theorem 1. But what will be the statistical bounds of training these 3 matrices (or any pair among the 3 matrices)? Can the authors discuss whether it is answerable from their theoretical framework and if not, the difficulties one might face to solve?\n\n**A1**. You are correct that the optimal solution can be shown to be a 3-matrix factorization, which can be seen from Appendix B. We think the statistical bounds of SGD-trained $\\\\Gamma$ and SGD-trained 3 matrices might not be identical. One challenge is that the loss is convex for $\\Gamma$ but is non-convex for $Q, K, V$. So there will be an extra layer of complexity from non-convex optimization when SGD is run for the 3 matrices. Our techniques for analyzing SGD in $\\Gamma$ might not be enough to deal with this non-convex optimization issue. With that being said, we believe our results still provide meaningful insights into the statistical task complexity of ICL. \n\n---\n\n**Q2**. \u2026 how will the theory change when instead of using training sequences of a single length, we use randomly sampled training sequences with varying lengths? How will the excess risk in theorem 5.3 change then?\n\nA2. This is a good question. A model pretrained with a varying context length cannot match the Bayes optimal method for every $M \\le N$ (in the setup of Theorem 5.3). This is because to achieve Bayes optimality with $M$ context examples, the model in equation (1) needs to have a parameter $\\Gamma$ as a function of $M$. However, the pretrained model in equation (1) has a fixed model parameter (even when the context length is varying during pretraining) that cannot adapt to a varying context length $M$, therefore such a model cannot be Bayes optimal uniformly for $M$.\n\n---\n\n**Q3**. \u2026 It would be interesting to empirically check the training time convergence with different singular value behaviors (as they pick in corollary 4.2) and observe differences in convergence and ICL performance throughout training.\n\nA3. Thank you for your suggestion. We have empirically checked the pretraining convergence under two $H$\u2019s of different spectrums. Please refer to Figure 1 in Appendix A in the revised paper. We can see that under polynomial eigen-decay ($i\\^{-4}$) and exponential eigen-decay ($2\\^{-i}$), ICL exhibits slightly different convergence behaviors though the trend is the same, which are aligned with our theory."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518566135,
                "cdate": 1700518566135,
                "tmdate": 1700518566135,
                "mdate": 1700518566135,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "t8hIow4NyO",
            "forum": "vSh5ePa0ph",
            "replyto": "vSh5ePa0ph",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7726/Reviewer_ozh2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7726/Reviewer_ozh2"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates a single-layer linear attention model, which is equivalent to a linear model with parameters derived from a one-matrix-step gradient descent from the origin. The authors make Gaussian data generating assumptions and consider the population ICL risk, identifying the optimal step size. They demonstrate that applying gradient descent to the step size parameterization can lead to an excess risk characterized by an exponential decay plus a 1/T-like decay. The authors also compare this with the Bayes optimal estimator, analyzing their respective risks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper's originality lies in analyzing the single-layer linear attention model with its one-step gradient descent parameterization. The authors' identification of the optimal step size under Gaussian data generating assumptions is a valuable contribution. The quality of the paper is evident in the rigorous mathematical analysis."
                },
                "weaknesses": {
                    "value": "The paper's clarity could be improved. \n\n- The assumption of the pretraining algorithm in equation (6) seems arguable, as the equivalence of the function classes does not necessarily imply identical behavior under different parameterizations in gradient descent. \n- The paper is heavily reliant on mathematical notation and could benefit from more intuitive explanations to aid understanding. \n- The choice of step size in Theorem 4.1 is not adequately justified, and the assumption that the initialization \u03930 commutes with H is not clearly motivated. \n- The assumption that H can be diagonalized without loss of generality (WLOG) is also not sufficiently explained. \n- The paper could be enhanced by including some numerical results in the main body. \n- The terminology used is occasionally confusing, such as the use of \"number of contexts\"."
                },
                "questions": {
                    "value": "- Could you provide more justification for the pretraining algorithm assumed in equation (6)?\n- Could you explain the choice of step size in Theorem 4.1?\n- Why is it reasonable to assume that the initialization \u03930 commutes with H in Theorem 4.1?\n- Could you provide more insight into why H can be assumed to be diagonal WLOG?\n- Would it be possible to include some numerical results in the main body to support the theoretical findings?\n- Could you clarify the term \"number of contexts\"? It reads like the number of in-context samples, but I guess you mean the number of sequences/datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7726/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699169495476,
            "cdate": 1699169495476,
            "tmdate": 1699636942101,
            "mdate": 1699636942101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s7ponEQBFv",
                "forum": "vSh5ePa0ph",
                "replyto": "t8hIow4NyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ozh2"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback.\n\n---\n\n**Q1**. The assumption of the pretraining algorithm in equation (6) seems arguable, as the equivalence of the function classes does not necessarily imply identical behavior under different parameterizations in gradient descent\u2026 Could you provide more justification for the pretraining algorithm assumed in equation (6)?\n\n**A1**. Equation (6) corresponds to the one-pass SGD update for parameter $\\\\Gamma$, which is a natural algorithm. Considering SGD in $\\\\Gamma$ space is meaningful as it provides insights into the statistical hardness of learning the ICL model. We agree with you that the behavior of SGD in $\\\\Gamma$ parameter space is not identical to that of SGD in $Q, K, V$ parameter space. For example, the loss is convex for $\\\\Gamma$ but is non-convex for $Q, K, V$. Nevertheless, considering SGD in $\\\\Gamma$ space simplifies the challenge from non-convex optimization. \n\nExtending our results to more complicated $Q, K, V$ parameterization is an important direction. However, even for the simplified $\\\\Gamma$ parameterization, there is no such kind of result before our work. Without a comprehensive understanding of the statistical behavior of SGD in the simpler $\\\\Gamma$ space, achieving this in the more complex $Q, K, V$ space appears challenging. As our work is the first one on statistical pretraining task complexity for ICL, we believe this is an important step in this direction and the contribution of our work is significant.\n\n---\n\n**Q2**. Could you explain the choice of step size in Theorem 4.1?\n\n**A2**. Equation (7) characterizes a decreasing, piecewise-constant stepsize scheduler. Specifically, the stepsize is initialized from $\\\\gamma\\_0$ and is decreased by a constant factor (we choose 2 in the paper) for every $T/\\\\log(T)$ steps, where $T$ is the total number of iterates. This is a commonly used stepsize scheduler for SGD in deep learning. \n\n---\n\n**Q3**. Why is it reasonable to assume that the initialization $\\\\Gamma\\_0$ commutes with H in Theorem 4.1?\n\n**A3**. Our assumption is reasonable as it holds when setting $\\\\Gamma\\_0$ to zero or a scalar matrix, that is, $\\\\Gamma\\_0 = c I$ where $c$ is a constant, without prior information. Additionally, it allows setting $\\\\Gamma\\_0$ to any matrix when $H$ is a scalar matrix. \n\n---\n\n**Q4**. Could you provide more insight into why $H$ can be assumed to be diagonal WLOG?\n\n**A4**. We assume that $H$ is diagonal to simplify our discussion in \u201cKey idea 1: diagonalization\u201d (and related places in the appendix). This is made without loss of generality. If $H$ is not diagonal, let $H = V \\\\Lambda V\\^\\\\top$ be the eigen-decomposition of $H$, then we can replace \u201cdiagonal matrices\u201d with \u201cmatrices of form $V D V\\^\\\\top$ where $D$ is a diagonal matrix\u201d, and our arguments all go through. Taking equation (30) in Appendix D.4 as an example, we re-define a \u201cdiagonalization of an operator\u201d by \n$$\n\\\\mathring {\\\\mathcal{O}} (A) = V \\\\text{diag} ( {\\\\mathcal{O}} ( \\\\text{diag}(V\\^\\\\top A V)  ) ) V\\^\\\\top.\n$$\nIn this way, the current proof adapts to the eigenspace of $H$. \n\n---\n\n**Q5**. Would it be possible to include some numerical results in the main body to support the theoretical findings?\n\n**A5**. We will move our numerical results (see Appendix A) to the main body provided we are granted additional pages in the final version. \n\n---\n\n**Q6**. Could you clarify the term \"number of contexts\"? It reads like the number of in-context samples, but I guess you mean the number of sequences/datasets.\n\n**A6**. The term \u201cnumber of contexts\u201d indeed refers to the \u201cnumber of in-context samples\u201d."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518459350,
                "cdate": 1700518459350,
                "tmdate": 1700518459350,
                "mdate": 1700518459350,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PKCnhnDyXE",
                "forum": "vSh5ePa0ph",
                "replyto": "t8hIow4NyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7726/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Dear Reviewer ozh2, we hope all of your concerns have been addressed. Please let us know if there is anything else that requires our clarification!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7726/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685219442,
                "cdate": 1700685219442,
                "tmdate": 1700685219442,
                "mdate": 1700685219442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]