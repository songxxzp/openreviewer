[
    {
        "title": "Exploring the Promise and Limits of Real-Time Recurrent Learning"
    },
    {
        "review": {
            "id": "hhTMFxgKnJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8675/Reviewer_cmdn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8675/Reviewer_cmdn"
            ],
            "forum": "V2cBKtdC3a",
            "replyto": "V2cBKtdC3a",
            "content": {
                "summary": {
                    "value": "The authors explore the practicality of RTRL as a method for learning in RNNs. In particular, the authors consider an RNN architecture whereby RTRL is relatively cheap to employ, and demonstrate its advantages over standard truncated BPTT on a range of reinforcement learning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- paper is generally well written\n- though RTRL has now been around for some decades, its practical applications remain unclear. Towards this end the authors do offer interesting results which should be interesting to the broader ML community\n- the RNN architecture based on element-wise application the authors use, though not novel itself, offers an interesting avenue for making RTRL more practical"
                },
                "weaknesses": {
                    "value": "- One concern I have is that the authors do not convincingly demonstrate the benefit of their model compared to the status quo, which is a fully connected RNN + truncated BPTT. In terms of complexity, the authors do reduce the memory/computational cost on pointwise RNNs to order N^2 which is certainly better than RTRL on standard RNNs, but this is still as expensive as BPTT on a fully connected RNN. In fact, for a pointwise RNN I wonder if BPTT is in fact of order N (as the forward pass?) - so still cheaper than RTRL - but I may be incorrect there. In terms of performance, the authors show that eLSTM + RTRL outperforms truncated BPTT on all RNN arhitectures for one task (watermaze) which is impressive, but I would have liked to seen this for more tasks; e.g. can RTRL outperform fully connected RNN + BPTT on any of the 5 Atari tasks in Fig 2?\n- Related to the above, it is not clear whether online learning has any intrinsic benefits for performance. The authors note that online learning \"allows for updating weights immediately after consuming very new input\", but in practice the weights are only updated at the same rate as truncated BPTT (at rate M) to avoid 'sensitivity matrix staling'. I appreciate this is a difficult question to answer, and I believe this shortcoming is mentioned as the last presented limitation, but I think this deserves more direct clarification/exploration given the subject of this work.  \n- All main results presented are with respect to performance in reinforcement learning (RL) tasks, but what about supervised learning tasks? e.g. sequence modeling in language. The authors do use the 'copy task', which is a supervised learning task, as a diagnostic task, but I would have liked to see comparisons with BPTT for hard supervised learning tasks. Or at least a rational for only considering RL tasks\n- The connection to biology and the 'memory' trace (e.g. Eqs 12-14) is interesting but not presently quite vague/weak. Some references to biological evidence for such traces (e.g. eligibility traces) would be advised.\n- Given this is a paper which explores the practical/emperical outcomes of RTRL, I would advise a more thorough analysis of the ideal conditions for RTRL. For example, why do the authors believe RTRL is only valuable in some of the 5 Atari environments in Fig 2? For what types of task would RTRL be ill-advised? The space for technical description and equations in the main text is significant (e.g. equations 8-14) and in general I believe this should be in part sacrificed (to the Appendix) for more intepretability of the results."
                },
                "questions": {
                    "value": "- The authors suggest that their work shows tractable RTRL in Quasi-RNN (Bradbury et al. 2017) and Simple Recurrent Units (Lei et al. 2018). I understand this for the latter, since as far as I understand the eLSTM is just a one-layer instance, but I don't understand this for Quasi-RNN. Doesn't this architecture use convolutions? Does that also mean that RTRL has a space/time complexity of N^2?\n- The authors state that, \"in practice, it is known that M > 1 is crucial (for TD learning of the critic) for optimal performance\". Is this true? My belief was that one-step TD learning was frequently employed. Is there a relevant citation?\n- How were the hyperparameters (Table 5) selected?\nTypos: \n- Mozer (1989; 1991) already explore an RNN -> Mozer (1989; 1991) already explored/s an RNN\n- several modern RNN architectures such Quasi-RNN -> several modern RNN architectures such as Quasi-RNN"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8675/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8675/Reviewer_cmdn",
                        "ICLR.cc/2024/Conference/Submission8675/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697466386398,
            "cdate": 1697466386398,
            "tmdate": 1700310743138,
            "mdate": 1700310743138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "91JnAFbxTQ",
                "forum": "V2cBKtdC3a",
                "replyto": "hhTMFxgKnJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cmdn, part 1/2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing this work.\nWe noted that several points listed under \u201cweakness\u201d are actually clarification questions.\nAnswers to many of these questions can be actually found in the paper, either in the main text or in the appendix. We are happy to help the reviewer find the answers as follows. We also appreciated that the reviewer reflected her/his doubts through a confidence score of 3.\n\n> *One concern I have is that the authors do not convincingly demonstrate the benefit of their model compared to the status quo, which is a fully connected RNN + truncated BPTT. In terms of complexity, the authors do reduce the memory/computational cost on pointwise RNNs to order N^2 which is certainly better than RTRL on standard RNNs, but this is still as expensive as BPTT on a fully connected RNN. In fact, for a pointwise RNN I wonder if BPTT is in fact of order N (as the forward pass?) - so still cheaper than RTRL - but I may be incorrect there.*\n\nNo, that is not correct: BPTT for eLSTM remains the same as the standard RNN. We had also provided a review/background material on BPTT in Appendix A.3 in case this might be helpful.\n\n> *In terms of performance, the authors show that eLSTM + RTRL outperforms truncated BPTT on all RNN arhitectures for one task (watermaze) which is impressive, but I would have liked to seen this for more tasks; e.g. can RTRL outperform fully connected RNN + BPTT on any of the 5 Atari tasks in Fig 2?*\n\nYes, eLSTM + RTRL outperforms the LSTM + TBPTT baseline on 3 / 5 tasks (Gravitar, Q*Bert, Seaquest) and perform similarly on the two others. The scores (corresponding to Fig 2) were provided in Table 5 in the appendix. That said, the most important result is that of Watermaze which is the hardest task we have in terms of memory.\n\n> *Related to the above, it is not clear whether online learning has any intrinsic benefits for performance. The authors note that online learning \"allows for updating weights immediately after consuming very new input\", but in practice the weights are only updated at the same rate as truncated BPTT (at rate M) to avoid 'sensitivity matrix staling'.*\n\nFirst of all, we\u2019d like to clarify that as we wrote in Sec 3.2 (last sentence): *\u201cThe main focus of this work is to evaluate learning with untruncated gradients, rather than the potential for online learning.\u201c* That said, a potential benefit of *more online* learning is to be more sample efficient. At least on DMLab Select-Non-Matching-Object (Figure 1 (a) vs (d)), we do observe some sample efficiency benefits of RTRL with a small M: at x = 2.5M steps, RTRL with M=10 achieves a score over 50, while the one with M=100 is below 40 at the same number of steps.\nHowever, if the goal is to obtain the best performing model, our conclusion is that a too small M is indeed not a good idea (more online learning deteriorates the performance, likely due to sensitivity staling).\n\n\n> *All main results presented are with respect to performance in reinforcement learning (RL) tasks, but what about supervised learning tasks? e.g. sequence modeling in language. The authors do use the 'copy task', which is a supervised learning task, as a diagnostic task, but I would have liked to see comparisons with BPTT for hard supervised learning tasks. Or at least a rational for only considering RL tasks*\n\nWe agree we all want to see RTRL applied to hard supervised learning tasks or any tasks in general. But that is the very challenge of the RTRL research. This is exactly what we discuss in the 3rd and 4th paragraph of Appendix B1 starting from *\u201cRemarks on language modelling and supervised learning\u201d*. There, we essentially explain that (1) with the one-layer constraint, it is hard to obtain competitive models on meaningful supervised learning tasks (and alike), e.g., language modelling, and (2) TBPTT implementations used for supervised sequence learning tasks are highly optimised; we argue that, to be competitive with them, it requires both engineering (custom CUDA implementation) and algorithmic (referring to hybrid RTRL-BPTT algorithms we discuss in Appendix A.4) efforts.\nWith RL tasks, (1) is not really an issue as we can still obtain good/competitive systems even with one-layer models (compared to the well-known IMPALA and R2D2 baselines).\nThe second point (2) is not just a limitation of our work but an open challenge for RTRL itself, which we honestly discussed in the paper (reflecting the title of our paper). This is also our unique contribution, which can not be found in any prior work on RTRL. We really hope this perspective improves the reviewer\u2019s view on the contribution of our work, currently rated as low as 2.\n\n> *The connection to biology and the 'memory' trace (e.g. Eqs 12-14) is interesting but not presently quite vague/weak.*\n\nYes, we\u2019ll add some references there for the final version. If the reviewer has any specific suggestions, that\u2019ll be also welcome."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029892692,
                "cdate": 1700029892692,
                "tmdate": 1700029892692,
                "mdate": 1700029892692,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OEPsBWr4zw",
                "forum": "V2cBKtdC3a",
                "replyto": "WL1fvnMUii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_cmdn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_cmdn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their detailed response and clarifications. It is clear to me the authors have worked hard in this area and demonstrate good expertise. \n\nReanalysing Table 5, my one concern remains that there does not seem to be significant difference (looking at error bars) between RTRL with the proposed eLSTM architecture and standard LSTM + BPTT. I think the authors certainly make a convincing case that RTRL can be useful/competitive for long temporal credit assignment, but the argument that, as the authors respond, \"in any tasks that benefit from a recurrent policy, there is no reason not to use RTRL\" certainly seems an overly strong and premature statement to me. \n\nThat said, I think it's a solid paper and I would be happy for it to be accepted. I will increase both my contribution score and overall score. May the area chair also note that my confidence score remains modest."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700310663512,
                "cdate": 1700310663512,
                "tmdate": 1700310663512,
                "mdate": 1700310663512,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zUZt03bZcu",
                "forum": "V2cBKtdC3a",
                "replyto": "hhTMFxgKnJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the score update"
                    },
                    "comment": {
                        "value": "Thank you very much for your response and the score update!\n\nJust a clarification regarding: \n\n> \"in any tasks that benefit from a recurrent policy, there is no reason not to use RTRL\" certainly seems an overly strong and premature statement to me.\n\nWe wrote this because we literally cannot find any reason not to use RTRL *when* RTRL is tractable (which is the core problem in the general case; but not in our restricted 1-layer element-wise recurrence case). We do not have any reason not to use true/untruncated gradients when we can compute them. They either provide some large performance improvements (e.g., as in Watermaze) when they are useful, or close to no degradation (e.g., as in some of the Atari games) when they are irrelevant. We can not see any downside (apart from engineering efforts to efficiently implement a non-standard learning algorithm)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326137190,
                "cdate": 1700326137190,
                "tmdate": 1700326186308,
                "mdate": 1700326186308,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J23ngJaa4t",
            "forum": "V2cBKtdC3a",
            "replyto": "V2cBKtdC3a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8675/Reviewer_nFUx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8675/Reviewer_nFUx"
            ],
            "content": {
                "summary": {
                    "value": "Real-time recurrent learning enables computing gradients in RNNs as the input sequence is processed. However, its complexity makes it impractical in realistic scenarios. In this work, the authors introduce a new version of the LSTM architecture for which RTRL is exact and cheap. They test the resulting algorithm in realistic RL tasks and show that it performs competitively to alternatives (TBPTT as well as approximations of RTRL on more standard LSTM architectures).\n\n**After rebuttal**: After clarifications from the author, I would like to increase my score from 5 to 7. Given that 7 does not exist, I updated it to 6."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies in depth the known, yet underappreciated, fact that RTRL becomes much cheaper when recurrence is element-wise. The paper nicely cites early references containing similar insights. The empirical results show that it is better to change the architecture so that RTRL becomes exact, rather than approximating it to make it tractable."
                },
                "weaknesses": {
                    "value": "I see three main weaknesses:\n\n- Most of the experimental results (all except baseline comparison) seem to some extent obvious to me: the only difference between TBPTT to RTRL is an extended context length. It is thus not surprising that RTRL > TBPTT in all experiments. The context length for TBPTT is relatively short (it seems far smaller than the one modern GPUs would offer) so those experiments do not seem to answer the question:\n    > In what scenarios would RTRL be able to replace BPTT in today\u2019s deep learning?\n    \n    asked by the authors. Adding a baseline that uses as much context length as possible would help better contextualize the findings of the paper. To me, this is the main weakness of the paper, and fixing it may require substantial changes.\n    \n- The paper is missing a few relevant references:\n    - Bellec et al. 2017: RTRL from a computational neuroscience perspective. One of the main contributions of the paper is to discuss how RTRL can be implemented biologically through eligibility traces.\n    - Gupta et al. 2022: show that it is possible to parametrize some classes of linear RNNs (more precisely linear SSMs) diagonally, without losing too much performance. Orvieto et al. 2023, which is cited in this paper, build on this result.\n    - Zucchet et al. 2023: show that spatial backpropagation combined with element-wise complex-valued recurrence enables learning multilayer networks with a performance close to the one of BPTT.\n- The paper mentions the multilayer case as an important limitation but does not discuss any possible or existing solutions (layer-wise training of Javed et al. 2023 / spatial backpropagation of errors of Zucchet et al. 2023)"
                },
                "questions": {
                    "value": "The paper focuses on one layer, but mentions that multiple layers are key in deep learning. Would the experimental results presented here would benefit from RNNs with multiple layers? From the vision module being fine-tuned?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8675/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8675/Reviewer_nFUx",
                        "ICLR.cc/2024/Conference/Submission8675/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697620334240,
            "cdate": 1697620334240,
            "tmdate": 1700213737478,
            "mdate": 1700213737478,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pFG8iInNtl",
                "forum": "V2cBKtdC3a",
                "replyto": "J23ngJaa4t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nFUx"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing our work.\nWhile we also thank the reviewer for acknowledging certain strengths of our work, we also would like to draw the reviewer\u2019s attention to other important aspects of our work which seem to be currently overlooked.\n\n> *Most of the experimental results (all except baseline comparison) seem to some extent obvious to me: the only difference between TBPTT to RTRL is an extended context length. It is thus not surprising that RTRL > TBPTT in all experiments.*\n\nWe\u2019d like to clarify that the goal of research on RTRL is not to *\u201csurprise\u201d* the readers; we all know that RTRL should be \u201cbetter\u201d but the whole challenge of any practical RTRL research is to build an actual system trained by RTRL that exhibits practical improvements. \n\n>  *The context length for TBPTT is relatively short (it seems far smaller than the one modern GPUs would offer) so those experiments do not seem to answer the question: In what scenarios would RTRL be able to replace BPTT in today\u2019s deep learning? asked by the authors. Adding a baseline that uses as much context length as possible would help better contextualize the findings of the paper. To me, this is the main weakness of the paper, and fixing it may require substantial changes.*\n\nWe\u2019d like to clarify two important points here.\n\nFirst of all, our experiments on DMLab memory tasks (Table 1) already show two contrastive cases. On Select-Non-Matching-Object (where the mean episode length is short; about 100 steps) TBPTT with a large M is nearly equal to full BPTT: consequently we only observe very limited improvements with RTRL. On the other hand on Watermaze (where the mean episode length is about 1000 steps; in Appendix B.2 DMLAB, paragraph *\u201cPractical Computational Costs\u201d*, we discuss the corresponding memory requirements), clear benefits of RTRL are observed (Table 1). Overall, RTRL is effectively promising for tasks with long-span dependencies, and rather irrelevant otherwise.\n\nNow regarding the question \u201cIn what scenarios would RTRL be able to replace BPTT in today\u2019s deep learning?\u201d, the reviewer is missing one very important perspective of our work.\nWe formulated this as a fundamental question that anybody interested in RTRL should ask.\nWe do not intend to answer this question only through these experiments (not because something is missing in our experiments; but because the scope of this question is much broader). Rather, our goal is to encourage more broad thinking about RTRL in the scope of today\u2019s (and maybe future) machine learning, going beyond the current focus on approximation methods. \nIt is absolutely true that for many tasks (like Select-Non-Matching-Object here) full backpropagation spans are short enough to be fitted to current GPUs. This will be the case for even more tasks in the future with advances in the hardware. This is exactly what we discuss in Sec 5 paragraph *\u201cPrincipled vs. practical solution\u201d*. In a similar spirit, we also discuss the limitations and challenges of RTRL in supervised learning (3rd and 4th paragraphs in Appendix B1 starting from *\u201cRemarks on language modelling and supervised learning\u201d*).\n\nOverall, all these honest questions and discussions bring new perspectives on RTRL research which are completely ignored in prior work on RTRL. \u201cPromise\u201d and \u201climits\u201d in our title reflect this.\nWe argue that this is an important and novel contribution, which, we believe, merits more than a contribution score of 2.\n\n> *The paper is missing a few relevant references ...*\n\nThank you very much for pointing out these papers. We'll cite them in our revision (except Javed et al. 2023 which is already cited), and add comments on existing efforts to deal with the multi-layer case. \n\n> *The paper focuses on one layer, but mentions that multiple layers are key in deep learning. Would the experimental results presented here would benefit from RNNs with multiple layers?*\n\nYes, we observed slight improvements with deeper models (see the next paragraph) but here, we intentionally focused on RL tasks where we can still achieve good performance (compared to well-known IMPALA and R2D2 baselines) even with one layer. This does not contradict our statement on the general importance of deep models though. Also generally, a learning algorithm that is only exact and efficient for one-layer models is too limited.\n\nIn our preliminary studies, we evaluated a baseline LSTM (trained with TBPTT) on DMLab-30 Watermaze with 1 or 2 layers: we found the 2-layer model (score 39.7 +- 4.6) to slightly outperform the 1-layer model (37.6 +- 5.3) while both of them underperform our 1-layer eLSTM (45.6 +- 4.7).\n\nWe hope our response brings clarification on all the concerns, in particular the first one marked as *\u201cthe main weakness\u201d*. The scope of our paper is different from previous RTRL papers, which is our unique contribution.\nIf the reviewer finds our response convincing, we\u2019d greatly appreciate it if it is reflected as a score change."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029772685,
                "cdate": 1700029772685,
                "tmdate": 1700029772685,
                "mdate": 1700029772685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9RCrPi7u0w",
                "forum": "V2cBKtdC3a",
                "replyto": "J23ngJaa4t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_nFUx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_nFUx"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the clarifications.\n\nThere was an important point that I didn't fully appreciate: when one wants to update parameters every $M$-steps one can only use context lengths of size at most $M$ with approaches rooted in BPTT when RTRL approaches can use, in principle, infinite context length. This is particularly important when sample efficiency is critical, as it is nicely demonstrated in the RL experiments. The authors may want to emphasize more that this will hold whatever the hardware memory size is. A posteriori, I think the discussion on the memory side distracted me from appreciating this argument that the authors make. Maybe quickly recalling the discussion in Section 3.2 in the \"Principled vs. practical solution\" would help other readers (the discussion in appendix B.1 on \"Remarks on language modeling and supervised learning\" would also help). \n\nReviewer Pxbt asked about the expressivity of element-wise recurrence, here are a few references on the expressivity of linear element-wise recurrence that might interest the authors:\n- \"Fading memory and the problem of approximating nonlinear operators with Volterra series\", Boyd & Chua 1985\n- \"Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems\", Grigoryeva & Ortega 2018\n- \"On the Universality of Linear Recurrences Followed by Nonlinear Projections\", Orvieto et al 2023\n\nOverall, I think the paper should be accepted and want to increase my score to 7. (EDIT: ICLR does not allow giving 7, so the updated score does not fully reflect my position)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213536839,
                "cdate": 1700213536839,
                "tmdate": 1700213664772,
                "mdate": 1700213664772,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bUooBS1mZb",
            "forum": "V2cBKtdC3a",
            "replyto": "V2cBKtdC3a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8675/Reviewer_8WmR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8675/Reviewer_8WmR"
            ],
            "content": {
                "summary": {
                    "value": "This work studies RTRL method on element-wise recurrent neural networks, examined with the actor-critic policy gradient algorithm (R2AC) on reinforcement learning tasks. Although the eLSTM architecture was not first proposed by authors, the connection between element-wise recurrence and RTRL training was first pointed out. The eLSTM, RTRL and R2AC are examined together on several subsets of DMLab-30, ProcGen and Atari-2600 envrionments and outperforms the IMPALA and R2D2 baselines trained on 10B frames."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The study of RTRL on element-wise recurrence provides a clean examination condition to study the potential of RTRL learning algorithm. From the provided experimental results the RTRL outperforms TBPTT on eLSTM and other baselines on most of the tasks. \n2. Very clear writing, easy to follow. It is a big strength that the authors provided a thorough analysis of the limitations including multi-layer RTRL, the practicality of RTRL etc."
                },
                "weaknesses": {
                    "value": "1. Only one element-wise RNN instantiation, namely the eLSTM was provided for comparison. It would be better to show the generality of the method on some other element-wise RNN instantiations.\n2. The experiments are conducted on reinforcement learning tasks, which is fine as it is stated clearly as the study goal. However, it would be much more convincing if it is shown with certain supervised learning tasks."
                },
                "questions": {
                    "value": "1. The variances shown in Figure 2 across different Atari environments are not consistent. For example, in Gravitar the variance for RTRL is larger than the other two and it\u2019s the opposite in Seaquest. But overall it seems RTRL has a larger variance. Could the authors provide some explanation or intuition for that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8675/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8675/Reviewer_8WmR",
                        "ICLR.cc/2024/Conference/Submission8675/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782993898,
            "cdate": 1698782993898,
            "tmdate": 1700409954286,
            "mdate": 1700409954286,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "66ogNZ3D6R",
                "forum": "V2cBKtdC3a",
                "replyto": "bUooBS1mZb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8WmR"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing our work, and for many positive comments.\n\n> *Although the eLSTM architecture was not first proposed by authors, the connection between element-wise recurrence and RTRL training was first pointed out.*\n\nWe\u2019d like to clarify that we are not the first to point out that the complexity of RTRL reduces for certain element-wise recurrent NNs (we clarify this upfront in the introduction/3rd paragraph by citing Mozer\u2019s work from the late 1980s). This is a \u201ctechnically not new\u201d but \u201clargely ignored/unexplored\u201d fact which we revisit in the modern context.\n\n> *Only one element-wise RNN instantiation, namely the eLSTM was provided for comparison. It would be better to show the generality of the method on some other element-wise RNN instantiations.*\n\nWe had to make a decision given our limited compute resource. As we expressed in the main text Sec 3.1 (*\u201cWhile one could further discuss myriads of architectural details (Greff et al., 2016), most of them are irrelevant to our discussion on the complexity reduction in RTRL; the only essential property here is that \u201crecurrence\u201d is element-wise.\u201d*), all these existing element-wise RNN architectures are similar up to certain details. Therefore, instead of evaluating various architectures with small differences, we opted for conducting experiments across many tasks. We hope this clarifies the logic of our decision.\n\n> *The experiments are conducted on reinforcement learning tasks, which is fine as it is stated clearly as the study goal. However, it would be much more convincing if it is shown with certain supervised learning tasks.*\n\nThis is a natural remark which we actually discuss in the paper (3rd and 4th paragraphs in Appendix B1 starting from *\u201cRemarks on language modelling and supervised learning\u201d*). There, we essentially comment that (1) with the one-layer constraint, it is hard to obtain competitive models on meaningful supervised learning tasks (and alike), e.g. language modelling, and (2) TBPTT implementations typically used for supervised sequence learning tasks are highly optimised; we argue that, to be competitive with them, it requires both engineering (custom CUDA implementation) and algorithmic (referring to hybrid RTRL-BPTT algorithms we discuss in Appendix A.4) efforts. This is not just a limitation of our work but an open challenge for RTRL itself, which we honestly discuss in the paper, reflecting the title of our paper. Such discussions are also our unique contributions, which can not be found in any prior work on RTRL. We really hope this perspective improves the reviewer\u2019s view on the contribution of our work, currently rated as low as 2.\n\n> *The variances shown in Figure 2 across different Atari environments are not consistent. For example, in Gravitar the variance for RTRL is larger than the other two and it\u2019s the opposite in Seaquest. But overall it seems RTRL has a larger variance. Could the authors provide some explanation or intuition for that?*\n\nRight, these variances do not have to be consistent. Our intuition is as follows. There are two key phenomena: discovering a key credit assignment within a trajectory, and encountering \u201cgood\u201d trajectories containing such credit assignment.  RTRL should facilitate the former (thanks to untruncated gradients) as long as the latter is successful. The latter, experiencing good \u201ctrajectories\u201d containing the relevant credit assignment paths, depends on the nature of the environment in particular. We speculate that it happens to be easy to encounter good trajectories in Seaquest, allowing RTRL to consistently find good strategies (low variance), while this does not seem to be the case for Gravitar where RTRL can yield good performance only when \u201cgood\u201d trajectories are found (high variance). This may be a possible explanation of what we observe there.\n\nWe hope our response improves the reviewer\u2019s overall view of this work and its contributions in particular.\nIf you find our response convincing, please consider increasing the score. Thank you very much."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029559335,
                "cdate": 1700029559335,
                "tmdate": 1700029559335,
                "mdate": 1700029559335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mv93Yim3XX",
                "forum": "V2cBKtdC3a",
                "replyto": "66ogNZ3D6R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_8WmR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_8WmR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors. I have a better understanding of the unique contributions of this work now. I am increasing the contribution score and the confidence score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700409929689,
                "cdate": 1700409929689,
                "tmdate": 1700409929689,
                "mdate": 1700409929689,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vkk3pR9Hr7",
            "forum": "V2cBKtdC3a",
            "replyto": "V2cBKtdC3a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8675/Reviewer_PxbF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8675/Reviewer_PxbF"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors explore the advantages of real-time recurrent learning (RTRL) compared to truncated BPTT in many reinforcement learning settings scoped to memory tasks. They use a variant of LSTM with element-wise recurrence in a singe layer to make RTRL tractable, and show that RTRL does have an advantage over truncated BPTT in these scenarios. They also provide a discussion of the difficulty of using RTRL in multi-layer recurrent networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "While RTRL is known to be computationally intractable in the general case, there has not been much prior work in exploring if specific architectural choices can help make exact RTRL more feasible and useful. Especially given the potential advantages RTRL could have due to its computational requirements being independent of sequence length unlike many other sequence models. So this work is very timely and relevant to the community, and the use of element-wise recurrence to make exact RTRL feasible is novel to my knowledge. \n\nThe authors demonstrate many tasks where RTRL with this architecture does better than truncated BPTT, which is very interesting given the very limited recurrence used. The discussion of multi-layer RTRL is also significant and useful to the community. Overall, the quality and clarity of the paper are high."
                },
                "weaknesses": {
                    "value": "- The elementwise recurrence that the authors propose for eLSTM is very similar to that used in IndyLSTM [1], but a comparative discussion and citation is missing.\n- Since exact RTRL computes exactly the same gradient as full BPTT, a specific analysis of why TBPTT rather than BPTT is used in each of the demonstrated tasks would be important to provide context (for e.g. in regards to memory requirements).\n- A discussion/analysis of the computational expressivity of having restricted recurrent and its implications is missing.\n- On a related note, a discussion (or even speculation) of why eLSTM works as well as it does given its limited recurrence would be useful.\n- Certain combinations of baselines are missing: eLSTM with SnAP-1, full BPTT.\n- The clarity of the experiments section could be improved. See below in \"Questions\".\n\n[1] (Gonnet & Deselaers 2019) https://arxiv.org/abs/1903.08023"
                },
                "questions": {
                    "value": "- Why does IMPALA with TBPTT+eLSTM do better than standard IMPALA (with LSTM+TBPTT presumably?)\n- Is having a small value of M beneficial for RTRL in any other way? Not clear what the motivation for studying the effect of M is.\n\n## Clarity:\n- The specific tasks the authors test on is not described anywhere in the paper (the Appendix points to a webpage). Having this would help a lot with understanding the tasks used in the paper.\n- The differences between the baseline methods of various variants of IMPALA and R2D2 are not sufficiently described."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8675/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698863201806,
            "cdate": 1698863201806,
            "tmdate": 1699637087010,
            "mdate": 1699637087010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fASxBa7cMh",
                "forum": "V2cBKtdC3a",
                "replyto": "Vkk3pR9Hr7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PxbF"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing our work and for many very positive comments on the significance of this work. We respond to the remaining questions and concerns as follows:\n\n> *The elementwise recurrence that the authors propose for eLSTM is very similar to that used in IndyLSTM [1], but a comparative discussion and citation is missing.*\n\nYes, the reviewer is right. We listed/cited many of them in Sec 3.1., including \u201cIndLSTM\u201d which even has almost the same name, but this one was indeed missing. Thank you for pointing this out; we\u2019ll cite it.\nWe emphasise that they are all \u201csimilar\u201d but not identical, and eLSTM has a minimalistic design.\n\n> *Since exact RTRL computes exactly the same gradient as full BPTT, a specific analysis of why TBPTT rather than BPTT is used in each of the demonstrated tasks would be important to provide context (for e.g. in regards to memory requirements).*\n\nWe primarily use TBPTT as it is the standard algorithm used to train recurrent policies in general. As we also wrote in the main text (paragraph \u201cPrincipled vs. practical solution\u201d in Sec. 5), in certain tasks such as DMLab Select-Non-Matching-Object, TBPTT with a large M is nearly full BPTT. Regarding the memory benefits of RTRL, we discuss its details in Appendix \u201cB.2 DMLAB\u201d paragraph \u201cPractical Computational Costs.\u201c\n\n> *A discussion/analysis of the computational expressivity of having restricted recurrent and its implications is missing.*\n\nWe actually discuss this in Appendix \u201cB.1 DIAGNOSTIC TASKS\u201d (Page 22), paragraph \u201cRemarks on computational capabilities of eLSTM.\u201d In particular, we refer to Merrill et al. ACL 2020\u2019s theoretical work on quasi-RNNs\u2019 expressivity (which is a representative element-wise RNN). That said, we admit that this paragraph should be hard to find currently. We will add a sentence in the main text pointing to this appendix section so that people can find this (and we likely should make this an independent subsection instead of a paragraph).\n\n> *On a related note, a discussion (or even speculation) of why eLSTM works as well as it does given its limited recurrence would be useful.*\n> *Why does IMPALA with TBPTT+eLSTM do better than standard IMPALA (with LSTM+TBPTT presumably?)*\n\nOne speculation is that element-wise recurrence is easier to learn as it does not involve any full matrix-vector multiplication for temporal/recurrent dependencies, making it more sample efficient. But this remains a tricky question.\n\n> *Certain combinations of baselines are missing: eLSTM with SnAP-1, full BPTT.*\n\nThere should be some confusion here: \u201cSnAP-1\u201d and \u201cfull BPTT\u201d are both learning algorithms which can only be used exclusively. Also, the combination \u201ceLSTM with SnAP-1\u201d does not exist: SnAP-1 is an approximation method for the fully recurrent models, which does not apply to eLSTM whose recurrence is element-wise. As far as we can tell, our current Table 2 is complete: it already covers all interesting/relevant cases.\n\n> *The clarity of the experiments section could be improved. See below in \"Questions\".*\n> *The specific tasks the authors test on is not described anywhere in the paper (the Appendix points to a webpage).*\n\nWe originally skipped these descriptions as they will be very long given the number of tasks we cover. But if the reviewer thinks they are really necessary, we\u2019ll be happy to add the corresponding task descriptions to the appendix. \nRegarding the full details about the external baseline models, we refer to the corresponding papers.\n\n> *Is having a small value of M beneficial for RTRL in any other way? Not clear what the motivation for studying the effect of M is.*\n\nA potential benefit of small M is *more online* learning which could be more sample efficient. At least on DMLab Select-Non-Matching-Object (Figure 1 (a) vs (d)), we do observe some sample efficiency benefits of RTRL with a small M: at x = 2.5M steps, RTRL with M=10 achieves a score over 50, while the one with M=100 is below 40 at the same number of steps.\nHowever, if the goal is to obtain the best performing model, our conclusion is that a too small M is indeed not a good idea (more online learning deteriorates the performance, likely due to sensitivity staling).\nOverall, studying/evaluating the effect of M here is natural. We also note that it is rare that we can conduct such experiments using the *exact* RTRL without worrying about the approximation quality, which is a clear benefit of our framework.\n\nWe hope our response above brings clarifications to all the remaining questions. Otherwise, we\u2019ll be happy to clarify more."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700029388870,
                "cdate": 1700029388870,
                "tmdate": 1700029388870,
                "mdate": 1700029388870,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FQC4wTX99p",
                "forum": "V2cBKtdC3a",
                "replyto": "Vkk3pR9Hr7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_PxbF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_PxbF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses and explanations -- they helped me understand your points better, and may be helpful for readers of the papers as well. Pointers in the main text to interesting analyses in the supplement would definitely be very helpful.\n\n\n>> Certain combinations of baselines are missing: eLSTM with SnAP-1, full BPTT.\n\n> There should be some confusion here: \u201cSnAP-1\u201d and \u201cfull BPTT\u201d are both learning algorithms which can only be used exclusively. Also, the combination \u201ceLSTM with SnAP-1\u201d does not exist: SnAP-1 is an approximation method for the fully recurrent models, which does not apply to eLSTM whose recurrence is element-wise. As far as we can tell, our current Table 2 is complete: it already covers all interesting/relevant cases.\n\nMy understanding of SnAP-1 is that it discards non-diagonal terms in the influence matrix update. So I'm not sure why it would not be possible to use this to train eLSTM.\n\nAs for full BPTT, is the argument that since it will calculate exactly the same gradients as RTRL with M set to sequence length, it doesn't make sense to list it explicitly? What is the sequence length for the experiments in Table 1?\n\n>> The clarity of the experiments section could be improved. See below in \"Questions\". The specific tasks the authors test on is not described anywhere in the paper (the Appendix points to a webpage).\n\n> We originally skipped these descriptions as they will be very long given the number of tasks we cover. But if the reviewer thinks they are really necessary, we\u2019ll be happy to add the corresponding task descriptions to the appendix. Regarding the full details about the external baseline models, we refer to the corresponding papers.\n\nWhat I had in mind is just single sentence conceptual descriptions of what the tasks are, but I'll defer to the authors on whether to put in the main text."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700476204809,
                "cdate": 1700476204809,
                "tmdate": 1700476277026,
                "mdate": 1700476277026,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V4IKYt8uce",
                "forum": "V2cBKtdC3a",
                "replyto": "lTHauXwvwy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_PxbF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8675/Reviewer_PxbF"
                ],
                "content": {
                    "comment": {
                        "value": ">> My understanding of SnAP-1 is that it discards non-diagonal terms in the influence matrix update. So I'm not sure why it would not be possible to use this to train eLSTM.\n\n> Yes, the reviewer's understanding is correct, but the non-diagonal terms in that \"influence\" matrix are already zero in element-wise recurrent models. This is why we introduced an additional architecture 'feLSTM' (Table 2, Appendix A.5) which is a minimum modification to eLSTM with fully recurrent components that still benefit from the SnAP-1 approximation.\n\n>> As for full BPTT, is the argument that since it will calculate exactly the same gradients as RTRL with M set to sequence length, it doesn't make sense to list it explicitly?\n\n> Exactly.\n\nMakes sense. Thanks for the clarification."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8675/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700507186267,
                "cdate": 1700507186267,
                "tmdate": 1700507186267,
                "mdate": 1700507186267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]