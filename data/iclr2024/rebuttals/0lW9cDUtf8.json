[
    {
        "title": "FairReweighing: density estimation-based reweighing framework for improving separation in fair regression"
    },
    {
        "review": {
            "id": "kQRLXVGFhg",
            "forum": "0lW9cDUtf8",
            "replyto": "0lW9cDUtf8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2987/Reviewer_gSA5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2987/Reviewer_gSA5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a bias mitigation method called FairReweighing for regression tasks. FairReweighing extends the preprocessing approach by Kamiran & Calders for classification to also work for regression by using k-nearest neigbors or kernel density estimation instead of frequenty counting to estimate density. Experiments show that model trained after using FairReweighing are significantly less biased."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* Unfairness mitigation for regression is indeed relatively understudied.\n* The fair reweighing extension seems simple and practical."
                },
                "weaknesses": {
                    "value": "* The technical contribution seems marginal. The core issue seems to be how to estimate density. Compared to a classification setup where we can use frequency counts, the idea is to use k-nearest neighbors or kernel density estimation instead for regression. This looks like a straightforward generalization of Kamiran & Calders, but not substantial enough for a full paper. \n\n* It is not clear why RQ2 is important. FairReweighing's goal is to improve fairness for regression, so why is it critical to validate its performance on classification problems? Instead, there should be other important research questions regarding regression. For example, how does pre-processing approaches compare to or complement in-processing approaches? How efficient is the unfairness mitigation? And so on. \n\n* It is not clear why the inconsistency in (8) leads to unfairness as described in the paper. The authors say that the model would \"prioritize these data points and produce more accurate predictions while disregarding\" the other examples, but there does not seem to be any theoretical or empirical evidence. Therefore, it is difficult to understand why the weighting in (9) solves the problem.\n\n* Satisfying a single fairness measure seems a bit limited, and the contributions would be stronger if other fairness measures in the literature proposed by Jiang et al. (2022) and Narasimhan et al. (2020) are also supported.\n\n* It is not clear how accurate the approximation in (7) is. \n\n* In Section 4.1.1, concluding that the density estimation approximation is accurate only based on synthetic data is not convincing.\n\n* In Sections 4.1.2 and 4.2, only using two real datasets for the performance comparisons does not seem extensive enough."
                },
                "questions": {
                    "value": "Same as the weak points above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697523745051,
            "cdate": 1697523745051,
            "tmdate": 1699636243473,
            "mdate": 1699636243473,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C2PgTFrigD",
                "forum": "0lW9cDUtf8",
                "replyto": "kQRLXVGFhg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2987/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First, we would like to thank the reviewer for the detailed comments. We have carefully considered all the comments and we believe that some of the comments have driven us to improve the paper greatly.\n\nWeakness 1. We have updated the contributions at the end of Introduction to highlight that, one major contribution is that, we now show in Section 3.1, FairReweighing can ensure separation in regression problems under the condition that $X\\perp A | Y$. Before this paper, there is no treatment specifically designed for separation in regression problems.\n\nWeakness 2. RQ2 is explored to validate \n1) FairReweighing is a generalized version of Reweighing; \n2) Applying FairReweighing on classification is the same as applying Reweighing. RQ2 is indeed less important than RQ1 for this paper\u2019s topic. But it serves as a validation of the generalizability of FairReweighing.\n\nWeakness 3. We would like to thank the reviewer for pointing out that there were not enough details on how FairReweighing is related to separation. This is why we have added a new Section 3.1 to show how FairReweighing ensures separation of the learned model under a conditional independence assumption of $X\\perp A | Y$. We also demonstrate the effectiveness of our approach through empirical evidence on real world datasets.\n\nWeakness 4. As we have shown in Section 3.1, FairReweighing is specifically designed to satisfy separation in regression problems. As a result, there is no reason to apply it for other fairness notions. We also kindly point out that fairness notions are often contradictory to each other and are impossible to be satisfied at the same time. (*Friedler, Sorelle A., Carlos Scheidegger, and Suresh Venkatasubramanian. \"The (im) possibility of fairness: Different value systems require different mechanisms for fair decision making.\" Communications of the ACM 64, no. 4 (2021): 136-143.*) Yes, there are in-processing algorithms which can be used to optimize for different fairness notions, however as shown in Table 2, none of those can satisfy separation as good as FairReweighing.\n\nWeakness 5 & 6. Since both KDE and radius neighbors are well-established density estimation algorithms, we did not provide the theoretical analyses of the accuracy of these algorithms. We would kindly ask the readers to refer to their original literature for the theoretical analyses. On the other hand, we provide some empirical results on a synthetic dataset in Figure 1 to show how accurate the estimations are when compared to the known ground truth distributions. Such comparison is impossible to make on real world datasets since we do not know the actual distributions of the variables.\n \nWeakness 7. We agree that experimenting on two real world datasets to provide validation for practical application scenarios does not seem extensive enough. However, we believe that both datasets we selected are sufficiently complex and diverse in nature and the method we proposed in the paper is indeed demonstrated to be effective in improving fairness. Meanwhile, we also kindly point out that, unlike the abundant number of classification fairness datasets, there are not many regression fairness datasets available to experiment on. For future work, we\u2019ll continue our evaluation on datasets of all variety and complexity to provide more context on practical application scenarios. \n\nAgain, we want to thank the reviewer for the detailed comments and suggestions. Please let us know if there are further concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688202966,
                "cdate": 1700688202966,
                "tmdate": 1700688224320,
                "mdate": 1700688224320,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3ePmQxXfIi",
            "forum": "0lW9cDUtf8",
            "replyto": "0lW9cDUtf8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2987/Reviewer_xkbJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2987/Reviewer_xkbJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the fairness criterion known as separation in the regression setting.  It proposes using KDE to audit/measure the violation of separation from finite samples (recall that separation involves conditioning on $Y\\in\\mathbb R$, whose values may be unique in a given samples).  Then, it extends the instance reweighting technique previously proposed for achieving fairness in the classification setting to regression, also via KDE (the author also consider binning and radius neighbors)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The motivation is clear, and the criterion of separation in regression problem receives relatively less attention in the fairness literature.\n- The presented material is clear and easy-to-follow."
                },
                "weaknesses": {
                    "value": "1. This paper uses KDE to deal with difficulties in using finite samples to achieve fairness, but does not provide any analysis for the proposed procedures for auditing and bias mitigation.\n\n\t- What is the convergence rate of $\\hat r_\\mathrm{sep}$ to $r_\\mathrm{sep}$ (eqs. 4 and 7)?\n\t- Similarly, what is the convergence rate of $\\widehat W(A,Y)$ to $W(A,Y)$ (eq. 9), when estimated and inferred from finite samples using KDE/radius neighbors/binning?\n\n2. Although an intuition is provided for the proposed FairReweighting procedure (in paragraphs surrounding eq. 8), critically, no proof or guarantees is provided for it.  Why should I trust that a regressor trained using FairReweighting would satisfy separation?\n\n\t- Under what assumptions would a regressor trained using FairReweighting satisfy separation?  Beyond toy examples?  Would it depend on the capacity/expressiveness of the regressor, or the optimization algorithm?\n\t- It is known that overparameterized neural networks can fit arbitrary training examples, and the effects of instance reweighting is empirically observed to be weakened [1].  Would the proposed procedure still work?\n\t- Is there a tradeoff between performance and fairness?  Such tradeoffs have been observed in fair classification via reweighting [2].\n\t- I expect some theorem upper bounding $r_\\mathrm{sep}$ of regressors trained using the FairReweighting procedure, likely involving the complexity of the regressor and the number of samples.\n\n3. Claims that the proposed FairReweighting is \"free of parameters and tuned automatically\" are wrong.  KDE involves specifying the variance of the Gaussian bumps, and radius neighbors needs to provide the radius and metric.\n\n[1] Bryd and Lipton.  What is the Effect of Importance Weighting in Deep Learning?  ICML 2019.  \n[2] Han et al.  Balancing out Bias: Achieving Fairness Through Training Reweighting.  EMNLP 2022."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698089511701,
            "cdate": 1698089511701,
            "tmdate": 1699636243396,
            "mdate": 1699636243396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dZNNJ1b7kC",
                "forum": "0lW9cDUtf8",
                "replyto": "3ePmQxXfIi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2987/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First, we would like to thank the reviewer for the detailed comments. We have carefully considered all the comments and we believe that some of the comments have driven us to improve the paper greatly.\n\nWeakness 1. \n1) The metric $\\hat{r}_{sep}$ is adopted from Steinberg et al. (2020), due to the space limitation, we will kindly refer the readers to the original paper for more details regarding this metric.\n\n2) KDE and radius neighbors are both commonly used density estimation methods. This is why we did not analyze their convergence in this paper. Instead, we provided an empirical result in Figure 1 showing how close the density estimations are compared to the actual distributions on a synthetic dataset where the actual distributions are known.\n\nWeakness 2a. We would like to thank the reviewer for pointing out that there were not enough details on how the scheme that we introduced is related to separation. This is why we have added a new Section 3.1 to show how FairReweighing ensures separation of the learned model under a conditional independence assumption of $X\\perp A | Y$. We also demonstrate the effectiveness of our approach through empirical evidence on real world datasets.\n\nWeakness 2b. In this paper, we will not be able to add results with deep neural networks due to space and time limitation. However, we do have some preliminary results for applying FairReweighing with an VGG-16 model on an image regression problem. \n\n| Pre-processing    | MAE |  R-Squared | $\\hat{r}_{sep}$ |\n| -------- | ------- |  -------- | ------- |\n| None  | 0.585    | 0.622 | 1.05 |\n| FairReweighing |   0.598  | 0.603 | 1.00 |\n\n\nThe above preliminary result suggests that\n 1) the effects of instance reweighting are in fact weakened by deep neural networks\u2014 without FairReweighing,  $\\hat{r}_{sep}$ is already close to 1.0;\n2) applying FairReweighing still improves separation\u2014 $\\hat{r}_{sep}$ is even better after applying FairReweighing.\n\nWeakness 2c. There\u2019s indeed a trade-off between model performance and fairness. However, as a pre-processing mechanism that is free of trade-off or \u201cPrice of Fairness\u201d parameters (weights)--- there is no threshold or parameter to tune for the trade-off. As shown in Section 3.1, FairReweighing could achieve perfect separation under the assumption $X\\perp A | Y$. Note that Reweighing is the same in terms of the trade-off.\n\nWeakness 2d. We shown Section 3.1 that FairReweighing could achieve perfect separation on the training data under the assumption $X\\perp A | Y$. So, when the assumption is valid, $\\hat{r}_{sep}=1.0$ on the training data.\n\nWeakness 3. The statement is indeed incorrect and we rephrased it into \u201cfree of trade-off or Price of Fairness parameters (weights)\u201d\n\nAgain, we want to thank the reviewer for the detailed comments and suggestions. Please let us know if there are further concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687933564,
                "cdate": 1700687933564,
                "tmdate": 1700691287542,
                "mdate": 1700691287542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M1kpghCMes",
            "forum": "0lW9cDUtf8",
            "replyto": "0lW9cDUtf8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2987/Reviewer_7rTx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2987/Reviewer_7rTx"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe a technique for achieving group fairness in regression tasks. Their approach is a simple preprocessing technique that weights samples based on the observed joint distribution of protected attributes and the target variable, in order to nudge the model trained on the weighted dataset towards satisfying the standard separation fairness constraint $\\hat{Y} \\perp A \\mid Y$, where $A$ is the protected attribute, $Y$ the target variable, and $\\hat{Y}$ the model's prediction. In several simple tabular test cases, the proposed method compares favorably with other fair regression techniques."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Regression fairness has received only negligible attention compared to classification fairness, and any progress on this important topic is highly welcome. The presented approach is very straightforward - that being a good thing -, easy to implement, and very general and widely applicable to all kinds of models and training schemes. The paper is well-written and easy to read."
                },
                "weaknesses": {
                    "value": "As outlined above, I consider the topic important and the solution proposed by the authors straightforward and generally promising. I do, however, sadly see quite a few significant weaknesses in the present manuscript.\n\nFirstly, I am honestly quite confused about what it is that the authors actually implemented, and how that matches the textual description. Sections 1 and 2 suggest strongly that the authors implement a scheme to achieve separation, i.e., $\\hat{Y} \\perp A \\mid Y$. However, the weighting scheme (section 3) seems to me to be constructed such that $p(y \\mid a_1) = p(y \\mid a_2)  \\forall y$ - which has nothing to do with separation (note that the algorithm's predictions do not even occur) and, instead, creates a synthetic (reweighted) dataset that fulfills *demographic (or statistical) parity*, i.e., $Y \\perp A$? It is completely unclear to me how this would help achieve separation for the classifier trained on such a reweighted dataset. (Note that, in any case, reweighting a training set, while possibly empirically useful, can never provide any *guarantees* about the resulting classifier having a certain property.)\n\nSecondly, it might have been easier for me to infer what the method is actually doing if the experimental results were analyzed more comprehensively and the experiments and metrics better described. How do the trained models perform on the two protected groups separately in all of the experiments? What are the \"Convex\", \"AOD\" and \"DP\" metrics? (I am sure the latter is a demographic parity-related metric, but how exactly is this computed?) Why is R-Squared so abysmally low in the Law School dataset; are these all typos? What are the base rates $p(y \\mid a)$ and TPR/FPR in the classification test cases? What are the actual regression models being fit; is it linear regression? Also, for the classification cases, standard equalized odds / equal opportunity classification fairness techniques as per Hardt. et al. should be included as baselines.\n\nThirdly, there are various statements throughout the manuscript that convey a shallow conception of \"fairness\" and \"bias\". To provide a few specific examples:\n- \"Separation [...] promises that a perfect predictor will always be considered the most unbiased.\" Separation doesn't \"promise\" anything. Also, \"perfect\" predictors can be completely biased if the target variable is noisy or biased, the dataset suffers from sampling biases, etc. Cf. e.g. Petersen et al. for a recent discussion of these issues.\n- \"The underlying assumption is that the unfairness we observed in machine learning models already includes discrimination rooted in training data.\" While data is an important source of bias, it is not the only one. Cf. e.g. Hooker and Hall et al.\n- \"To ensure a dataset is unbiased, the sensitive characteristics A have to be statistically independent of the ground-truth label Y. [...] Such discrepancies would lead to a biased regressor or classifier.\" Again, this describes a notion of statistical / demographic parity on the dataset level, not separation. Is a breast cancer dataset in which women have breast cancer more often than men \"biased\"? And, equivalently, is a classifier which predicts breast cancer more often for women than for men \"biased\"?\n\nFourthly, given that this is an immediate application of widely used standard techniques, the discussion of and references to prior relevant work are quite sparse.\n- Weighting and over/undersampling techniques are widely used throughout the algorithmic fairness literature, see e.g. Caton and Haas.\n- This is essentially an application of covariate shift adaptation techniques as per Sugiyama et al., with the \"target population\" being the one where a desired notion of group fairness is satisfied.\n- While prior work on fair regression is discussed quite well, there are a few further methods that might be relevant to mention, such as Komiyama et al. and Calders et al. In particular Calders et al. use a propensity score-based approach, which is very closely related to what the authors are describing here; the differences should be spelled out clearly.\n\n**References**\n- Calders et al. (2013), Controlling Attribute Effect in Linear Regression, https://ieeexplore.ieee.org/document/6729491\n- Caton and Haas (2023), Fairness in Machine Learning: A Survey, https://doi.org/10.1145/3616865\n- Hall et al. (2022), A Systematic Study of Bias Amplification, https://arxiv.org/abs/2201.11706\n- Hooker (2021), Moving beyond \u201calgorithmic bias is a data problem\u201d, https://doi.org/10.1016/j.patter.2021.100241\n- Komiyama et al. (2018), Nonconvex Optimization for Regression with Fairness Constraints, http://proceedings.mlr.press/v80/komiyama18a.html\n- Petersen et al. (2023), The path toward equal performance in medical machine learning, https://www.cell.com/patterns/fulltext/S2666-3899(23)00145-9"
                },
                "questions": {
                    "value": "- The authors correctly point out that direct density estimation is known to be more efficient than estimating the two likelihoods separately (a reference on this statement would also be appropriate) - and then they proceed to estimate the two likelihoods separately and do *not* do direct density estimation?\n- I believe Eq. (4) is missing an expectation? (Currently, it does not evaluate to a number.)\n- I believe in Eq. (7), the ratio is inverted compared to Eq. (4)?\n- Which of the two discussed density approximation methods is used for the results in Fig. 1, and for the further results? Also, \"this suggests that our methods can correctly estimate the density of any given variable\" is a very strong claim given that the method was evaluated on exactly one very simple test case.\n- What is \"Models = Ground Truth\" in Table 1?\n- I assume the tau in all tables should be a tauhat?\n- Where can I find the results for the described experiments with continuous protected attributes?\n- The authors write that \"FairReweighting\" is \"essentially equivalent\" to the previously proposed Reweighting method - isn't it precisely, mathematically, identical?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2987/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2987/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2987/Reviewer_7rTx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698424235475,
            "cdate": 1698424235475,
            "tmdate": 1699636243326,
            "mdate": 1699636243326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zyhvk1Xnc7",
                "forum": "0lW9cDUtf8",
                "replyto": "M1kpghCMes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2987/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First, we would like to thank the reviewer for the detailed comments. We have carefully considered all the comments and we believe that some of the comments have driven us to improve the paper greatly.\n\nWeakness 1. We would like to thank the reviewer for pointing out that there were not enough details on how the scheme that we introduced is related to separation. This is why we have added a new Section 3.1 to show how FairReweighing ensures separation of the learned model under the condition that $X\\perp A | Y$. We also demonstrate the effectiveness of our approach through empirical evidence on real world datasets.\n\nWeakness 2. We apologize for the confusions raised in the previous experiment descriptions. We have substantially revised the experiment section:\n\n1) We have removed fairness metrics other than $\\hat{r}_{sep}$ from Table 1 and 2 to avoid confusions. \n2) We explained that the base models for FairReweighing are linear regression for regression and logistic regression for classification in the first paragraph of Section 4.\n3) We also explained the baselines compared in Table 2 in the first paragraph of Section 4 and in Section 4.1.3.\n4) We clarified in Section 4.2.2 that the experiment on the classification data is just to show that FairReweighing is the same as Reweighing in classification. And that $\\hat{r}_{sep}$ can evaluate separation as good as AOD\u2014 average odds difference (a metric used to evaluate equalized odds) in classification. This is why we did not analyze too much for the classification results\u2014 the Reweighing paper already did that. The conclusion we draw from this experiment is that FairReweighing can be applied to classification and when it is applied to classification, it is the same as Reweighing. This validates our claim that FairReweighing is a generalized form of Reweighing on regression problems.\n\n\nWeakness 3. Thanks for pointing these out! We have rephrased multiple statements throughout the manuscript so it would be more accurate and correctly reflect the conception of \"fairness\" and \"bias\".\n\nWeakness 4. We greatly appreciate the literature that the reviewer recommended. After careful selection, we include those that we believe could complement our discussion and comparison with prior relevant works.\n\nQuestion 1. We have clarified in Section 3.2 that the \u201cdirect estimation\u201d only applies to classification problems. In regression problems, we have to use the nonparametric probability density estimation techniques for regression problems.\n\nQuestion 2, 3 & 6. Fixed\n\nQuestion 4. We rephrased the claim into an observation of the results.\n\nQuestion 5. \u201cGround Truth\u201d refers to implementing FairReweighing with the actual ground truth distribution (we know this because it is a synthetic dataset) instead of density estimation approximation. It was confusing and we added an explanation in the result analysis in Section 4.1.1.\n\nQuestion 7. Currently we are only evaluating our approach\u2019s effectiveness on binary protected attributes upon which separation is defined. We have specified this in the problem statement of Section 3. It\u2019s definitely one of the major directions of our future work to generalize it to continuous sensitive attributes.\n\nQuestion 8. The major contribution of this work is a generalization of Reweighing (Kamiran & Calders) onto regression problems with k-nearest neighbors or kernel density estimation. The math is indeed identical with the previously proposed Reweighting but not how we calculate the weight. We also demonstrated in RQ2 that when applied to classification problems, FairReweighing is identical to Reweighing.\n\nAgain, we want to thank the reviewer for the detailed comments and suggestions. Please let us know if there are further concerns."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687362519,
                "cdate": 1700687362519,
                "tmdate": 1700687362519,
                "mdate": 1700687362519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7qyVJpeXyh",
            "forum": "0lW9cDUtf8",
            "replyto": "0lW9cDUtf8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2987/Reviewer_NWYi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2987/Reviewer_NWYi"
            ],
            "content": {
                "summary": {
                    "value": "The author proposes a density estimation-based preprocessing algorithm to train regression models, with the goal of reducing the bias of the original data before the entire training process. This method is more efficient and has lower overhead. This method has no parameters and can be automatically tuned to achieve a balance between all specified protected attributes. The experimental results also indicate that the algorithm proposed by the author improves separation in fair regression while maintaining high prediction accuracy, and its performance is superior to the most advanced existing schemes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper includes a comprehensive summary of related work. The ideas of the paper are clearly presented. The author proposes a universal preprocessing framework, which is a training framework based on density estimation. By adjusting the impact of each data item through weight allocation to achieve fairness, classification and regression models can be effectively trained. This article extends the fairness issue in classification problems to regression problems based on previous studies."
                },
                "weaknesses": {
                    "value": "Lack of analysis on the robustness and stability of the algorithm: The paper did not analyze the robustness and stability of the algorithm. In practical applications, algorithms need to be able to handle various uncertainties and noise while maintaining stable performance. The analysis of the robustness and stability of algorithms can provide a more comprehensive evaluation."
                },
                "questions": {
                    "value": "1. The paper did not provide specific details and implementation methods for the kernel density estimation algorithm and lacked transparency in the algorithm to verify its effectiveness.\n2. Lack of comparison with other methods: There are relatively few existing classification fairness methods in the paper, making it difficult to determine the advantages and disadvantages of this method in classification tasks.\n3. The paper mentioned some fairness measures but did not provide a detailed discussion on the selection and applicability of these measures. The selection of fairness measures is crucial for evaluating the fairness of algorithms and requires more in-depth discussion and explanation.\n4. The paper provides evaluation results for both synthetic and real-world data but does not provide validation for practical application scenarios. The data in practical application scenarios may be more complex and diverse and is the weight allocation method proposed in the paper still effective in improving fairness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698667729206,
            "cdate": 1698667729206,
            "tmdate": 1699636243251,
            "mdate": 1699636243251,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mJbrR2hPo1",
                "forum": "0lW9cDUtf8",
                "replyto": "7qyVJpeXyh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2987/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First, we would like to thank the reviewer for the detailed comments. We have carefully considered all the comments and we believe that some of the comments have driven us to improve the paper greatly.\n\nQuestion 1. We acknowledge the lack of details on how kernel density estimation is implemented in our experiment and hence added more information at the start of Section 4. Figure 1 also shows the estimation accuracy of kernel density estimation on synthetic data where we know the ground truth probabilities.\n\nQuestion 2. The contribution of this paper focuses on the extension of both fairness metrics and bias mitigation techniques from binary classification tasks to regression problems. Therefore we have focused on the experiments on regression tasks and the comparisons against other regression-friendly algorithms. The experiments and comparison on classification is merely a demonstration of the generalizability of our method in both situations. Also note that on the classification data, FairReweighing is the same as Reweighing and Separation can be evaluated by equalized odds.\n\nQuestion 3. We have provided a more thorough discussion on the selection and applicability of all the fairness measures we are comparing at the end of Section 2.2.1. In short, we focus on the Separation criterion only in this paper since it always allows the perfect predictor to be evaluated as fair while individual fairness requires similar individuals (from different sensitive groups) to be treated similarly, demographic parity requires the acceptance rates to be the same across different sensitive groups.\n\nQuestion 4. Following the suggestion from the reviewer, we have added descriptions (highlighted in Section 4.1.2) for the potential practical application scenarios for our two real world data. \n\nAgain, we want to thank the reviewer for the detailed comments and suggestions. Please let us know if there are further concerns."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686779620,
                "cdate": 1700686779620,
                "tmdate": 1700686779620,
                "mdate": 1700686779620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]