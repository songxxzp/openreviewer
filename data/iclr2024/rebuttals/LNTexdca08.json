[
    {
        "title": "P2P: Transforming from Point Supervision to Explicit Visual Prompt for Object Detection and Segmentation"
    },
    {
        "review": {
            "id": "k17FGt1H6U",
            "forum": "LNTexdca08",
            "replyto": "LNTexdca08",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1568/Reviewer_6Xmh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1568/Reviewer_6Xmh"
            ],
            "content": {
                "summary": {
                    "value": "This paper concentrates on the task of point supervision and introduces a novel approach named  P2P that transforms point supervision into accurate pseudo-labels. The method harnesses the potential of the visual foundation model, SAM, and introduces an iterative framework for the generation of these pseudo-labels. This framework comprises two stages: an SEPG stage, which translates the point annotations into visual prompts, and a PGSR stage for converting these visual prompts into pseudo masks and bounding boxes. Experimental results show that the proposed approach achieves SOTA performance on COCO and PaSCAL datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper introduces a novel method to generate pseudo-labels from point supervision that leverages the potential with a visual foundation model.\n- The paper is well-written and experiments demonstrate superior qualitative results  compared to baselines."
                },
                "weaknesses": {
                    "value": "- The performance of the model largely depends on the capability of SAM itself. From the quantitative results, the proposed approach does not have a significant improvement compared with directly using SAM.\n- The method seems to be computationally demanding since the training process is conducted with 4*RTX 4090 GPUs."
                },
                "questions": {
                    "value": "Could the author provide a comparison of training time and memory cost compared with other methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1568/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1568/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1568/Reviewer_6Xmh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698131526236,
            "cdate": 1698131526236,
            "tmdate": 1699636085316,
            "mdate": 1699636085316,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KZEq27FcOd",
                "forum": "LNTexdca08",
                "replyto": "k17FGt1H6U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 6Xmh"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments. We hope our responses can address your concerns. Further comments are welcomed.\n\n------\n\nQ1: *The performance of the model largely depends on the capability of SAM itself. From the quantitative results, the proposed approach does not have a significant improvement compared with directly using SAM.*\n\nA1: Thanks for your comment. The emergence of large models has brought new insights and performance improvements to many downstream tasks. We think that the trend of applying foundation models to solve problems is irreversible. \n\nFor point-supervised detection and segmentation, the absence of size information makes it challenging. SAM, due to its lack of semantic understanding, also struggles to fully address this issue. In our method, P2P is the first attempt to iteratively enhance the quality of prompts, guiding SAM to generate accurate masks and achieving further improvement upon SAM.\n\nwe conducted a more detailed comparison with SAM. We found that, for mIoU and CorLoc, our approach improves mIoU and CL@0.5 by 11.4 and 18.37, respectively. Additionally, our approach also demonstrates a notable improvement in $AP_{50}$, with a performance increase of 8.5 on COCO dataset. \n\n\n\n| Method | mIoU  | CL@0.5 | CL@0.7 | CL@0.9 |  AP  | AP50 | AP75 |\n| :----: | :---: | :----: | :----: | :----: | :--: | :--: | :--: |\n| P2BNet | 57.5  |   -    |   -    |   -    | 22.1 | 47.3 |  -   |\n|  SAM   | 58.25 | 60.32  | 47.32  | 21.27  | 27.3 | 45.3 | 28.5 |\n|  Ours  | 69.70 | 79.05  | 62.40  | 24.07  | 31.6 | 53.8 | 32.7 |\n\n \n\nQ2: *The method seems to be computationally demanding since the training process is conducted with 4\\*RTX 4090 GPUs. Could the author provide a comparison of training time and memory cost compared with other methods?*\n\nA2: As suggested, we have supplemented detailed comparisons of memory, training time, and inference time in the appendix of revised version. This is also explained in the *unified response* part. \n\nP2P consumes a bit higher time and memory during the training phase than P2BNet, but it greatly enhances the quality of pseudo-labels, which is crucial for subsequent detection and segmentation tasks. With high-quality pseudo-labels, one can freely choose any task-specific subnetwork for optimization, achieving commendable performance. Therefore, compared to the substantial improvement in performance, the time consumption during training is not an issue."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490295441,
                "cdate": 1700490295441,
                "tmdate": 1700537811589,
                "mdate": 1700537811589,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gfYVmtw3uY",
                "forum": "LNTexdca08",
                "replyto": "k17FGt1H6U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 6Xmh,\n\nThank you very much for reviewing our work. Your comments and feedback will be invaluable in helping us improve the final version of our paper. \n\nWe think we have responded to the reviewer's questions and concerns, but if there is anything that is missing, please let us know and we will be happy to add it.\n\nLooking forward to your feedback and discussion. Thanks again!\n\nBest,\n\nPaper 1568 Authors."
                    },
                    "title": {
                        "value": "Response to reviewer 6Xmh (2st)"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636600609,
                "cdate": 1700636600609,
                "tmdate": 1700636640093,
                "mdate": 1700636640093,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W8vYHTybyp",
            "forum": "LNTexdca08",
            "replyto": "LNTexdca08",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1568/Reviewer_j4vZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1568/Reviewer_j4vZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel iterative learning framework, Point to Prompt (P2P), for point-supervised object detection and segmentation. The P2P is formulated as an iterative refinement process of two stages: Semantic Explicit Prompt Generation (SEPG) and Prompt Guided Spatial Refinement (PGSR). Experiments on multiple datasets are performed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method has a superior performance on multiple datasets.\n- The motivation is resonable and easy to understand."
                },
                "weaknesses": {
                    "value": "- The authors claim that the existing methods aim to release the annotation burden while still achieving decent performance. However, the proposed method seems still far behind fully-supervised method.\n- It is better to give an inference time comparison, which can help readers better understand the propose method.\n- SAM has a good ability to generate the mask of objects based on point. Thus, it may give a good performance using SAM and Cascade structure, which is similar to PGSR."
                },
                "questions": {
                    "value": "please see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698495876518,
            "cdate": 1698495876518,
            "tmdate": 1699636085236,
            "mdate": 1699636085236,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FBQQ9H7rqE",
                "forum": "LNTexdca08",
                "replyto": "W8vYHTybyp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer j4vZ"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments. We hope our responses can address your concerns. Further comments are welcomed.\n\n-----\n\nQ1: *The authors claim that the existing methods aim to release the annotation burden while still achieving decent performance. However, the proposed method seems still far behind fully-supervised method.* \n\nA1: \n\n1. **point-supervised tasks is challenging:** Due to the absence of target size information, accurate detection and segmentation are challenging under point supervision alone. In theory, the performance of fully supervised methods serves as an upper bound for the performance of all weakly supervised methods. As shown in the following tables, under equivalent conditions (i.e., point supervision using the same fully supervised method fine-tuned), previous state-of-the-art point-supervised methods achieved only around **60% (for detection) or 50% (for segmentation)** of the fully supervised performance. \n2. **We further narrows the performance gap:** With only point annotations, our method can achieve performance levels of around **80%** of a fully supervised detector, significantly reducing the required annotation effort.\n\n\n\n|    Method    | Sup. |  AP  | AP50 | AP75 |\n| :----------: | :--: | :--: | :--: | :--: |\n| Faster  RCNN |  F   | 37.4 | 58.1 | 40.4 |\n|     DINO [1]     |  F   | 57.2 | 75.7 | 62.7 |\n|  P2BNet-FR   |  P   | 22.1 | 47.3 |  -   |\n|   Ours-FR    |  P   | 31.6 | 53.8 | 32.7 |\n|  Ours-DINO   |  P   | 45.1 | 66.1 | 48.9 |\n\n|    Method     | Sup. |  AP  | AP50 | AP75 |\n| :-----------: | :--: | :--: | :--: | :--: |\n|   Mask RCNN   |  F   | 35.4 | 56.4 | 37.9 |\n|  Mask2Former [2]  |  F   | 46.1 | 69.4 | 49.8 |\n| AttenShift-MR |  P   | 21.2 | 42.0 | 19.4 |\n|    Ours-MR    |  P   | 26.4 | 48.6 | 26.2 |\n|    Ours-MF    |  P   | 34.9 | 58.9 | 36.1 |\n\n*Note: FR indicates Faster RCNN, MR indicates Mask RCNN, and MF indicates Mask2Former*\n\n------\n\n*[1] Zhang, Hao, et al. \"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.\" The Eleventh International Conference on Learning Representations. 2022.*\n\n*[2] Cheng, Bowen, et al. \"Masked-attention mask transformer for universal image segmentation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.*\n\n------\nQ2: *It is better to give an inference time comparison, which can help readers better understand the propose method.*\n\nA2: As suggested, we have supplemented detailed comparisons of memory, training time, and inference time in the appendix of revised version. This is also explained in the *unified response* part. \n\nFor P2P, the inference speed is a bit lower than P2BNet. However, we think that the time consumption of point-supervised methods itself may not be a primary limitation in practical applications. As illustrated  in the section 3.1, the point supervision task involves training a regressor to predict pseudo-labels. One can choose an appropriate subnetwork (*e.g.*, a detector ) for specific tasks or applications and opimized it in a fully supervised manner with the predicted pseudo labels, (*e.g.*, we can use more powerful detectors, such as DINO, to ensure performance, or choose faster detectors like YOLO to guarantee real-time capabilities.).\n\n\n\nQ3: *SAM has a good ability to generate the mask of objects based on point. Thus, it may give a good performance using SAM and Cascade structure, which is similar to PGSR.*\n\nA3: Thank you for your comment.  \n\n**Results**: Following your advice, we devised the cascaded-SAM. However, experimental results (in the following table) indicate that it **did not yield satisfactory performance**. \n\n**Analysis**: We attribute SAM's performance to the quality of prompts. A critical issue with point prompts is semantic ambiguity. SAM cannot ascertain whether the semantic of a point is local or global (e.g., a car or a car window). Under point prompts, numerous local masks are generated (e.g., clothing on a person, the glass of a car), which becomes more pronounced in complex scenes, such as the COCO dataset. Even when using the bounding box of local masks as prompts and reapplying SAM, the resulting masks remain local and fail to enhance performance effectively. In contrast, **PGSR progressively regresses proposal seeds to the correct semantic regions based on given semantic labels**, (e.g., transitioning from clothing to the entire person), which used as a prompt can better guide SAM in generating accurate masks.\n\n|   Method    | mIoU  | CL@0.5 | CL@0.7 | CL@0.9 |  AP  | AP50 | AP75 |\n| :---------: | :---: | :----: | :----: | :----: | :--: | :--: | :--: |\n|     SAM     | 58.25 | 60.32  | 47.32  | 21.27  | 24.3 | 43.8 | 24.3 |\n| Cascade SAM | 58.56 | 60.64  | 47.76  | 22.05  | 24.7 | 44.3 | 24.8 |\n|    Ours     | 69.70 | 79.05  | 62.40  | 24.07  | 26.4 | 48.6 | 26.2 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490246159,
                "cdate": 1700490246159,
                "tmdate": 1700537742536,
                "mdate": 1700537742536,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PJeqoIo8S2",
                "forum": "LNTexdca08",
                "replyto": "W8vYHTybyp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer j4vZ (2st)"
                    },
                    "comment": {
                        "value": "Dear Reviewer j4vZ,\n\nThank you very much for reviewing our work. Your comments and feedback will be invaluable in helping us improve the final version of our paper. \n\nWe think we have responded to the reviewer's questions and concerns, but if there is anything that is missing, please let us know and we will be happy to add it.\n\nLooking forward to your feedback and discussion. Thanks again!\n\nBest,\n\nPaper 1568 Authors."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636561325,
                "cdate": 1700636561325,
                "tmdate": 1700636561325,
                "mdate": 1700636561325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4h1ppLJ38Z",
            "forum": "LNTexdca08",
            "replyto": "LNTexdca08",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1568/Reviewer_iqSb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1568/Reviewer_iqSb"
            ],
            "content": {
                "summary": {
                    "value": "This paper innovatively introduces the Point to Prompt task, transforming point-label inputs into visual prompt learning, and leveraging a foundation model (SAM). It utilizes an iterative refinement process to obtain high-quality prompt to complete object detection and semantic segmentation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This method simultaneously accomplishes segmentation and horizontal object detection tasks through point labels, demonstrating a certain level of versatility.\n- This paper employs Exponential Moving Average (EMA) to update the prototypes of the corresponding categories. This approach is dynamic and remains unaffected by the inherent instability of Multi Instance Learning.\n- Novelty: this paper introduces a novel approach by incorporating visual foundation model into a point-supervised task while serving two downstream tasks simultaneously. Furthermore, the proposed prototype representation updated by Exponential Moving Average (EMA) within the Multiple Instance Learning (MIL) framework can enhance the quality of the selected proposals. \n- Writing: The paper exhibits a generally smooth logical flow and suitable symbol usage."
                },
                "weaknesses": {
                    "value": "- Novelty: Some parts of the proposed method lack novelty, particularly in the proposed Prompt Refiner II, phrases like \"similar to previous works\" and \"follow the common practice\" resemble the refinement stage of P2BNet [1]. \n- Writing: Certain parts of the language are not as concise and straightforward as desired. Additionally, there is a slight delay in providing specific explanations for some concepts, such as \"proposal seeds\" and \"selected high-quality embedding features,\". Furthermore, Section 3.4 should be repositioned closer to the beginning of the paper to provide readers with a clearer understanding of the overall method's workflow.\n\n[1] Chen, Pengfei, et al. \"Point-to-box network for accurate object detection via single point supervision.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
                },
                "questions": {
                    "value": "- In Section 3.1, the paper mentions that \u201cthe core of this task lies in designing an accurate point-to-box regressor\u201d, and introduce the P2P framework (point-to-visual prompt), but it is inconsistent between \u201cpoint-to-box\u201d and \u201cpoint-to-visual prompt\u201d, it is suggested to add the description of process of visual prompt-to-box/mask after.\n- What\u2019s the meaning of the orange arrow in the lower part of the Figure 2? (i.e., the arrow on the right of \u2018Refine box b*\u2019).\n- The concept \u2018proposal seeds\u2019 is proposed in Section 3.1, but its specific generation way (i.e., outer rectangle of the mask generated by SAM) is given until Section 3.4, it is suggested to explain its specific concept early on to avoid confusion.\n- What\u2019s the concrete content of \u2018sharing some of the weights\u2019 about the two refiners mentioned in Section 3.2? \n- What are the key differences in the workflow of the proposed Refiner2 compared to the refining stage in previous works [1]?\n- How about the computational complexity of the proposed method compared to other methods?\n- In Section 3.4, the actual input of the subsequent P2P refiner is the initial proposal seed box (i.e., outer rectangle of the mask generated by SAM). If possible, it is recommended to use the box (e.g., the fully supervised bounding box label) as the prompt of SAM to conduct a more comprehensive experimental comparison in segmentation, rather than just using the point as the prompt of SAM for comparison.\n\n[1] Chen, Pengfei, et al. \"Point-to-box network for accurate object detection via single point supervision.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1568/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1568/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1568/Reviewer_iqSb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698591783724,
            "cdate": 1698591783724,
            "tmdate": 1699636085153,
            "mdate": 1699636085153,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RASjgCgYlG",
                "forum": "LNTexdca08",
                "replyto": "4h1ppLJ38Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer iqSb"
                    },
                    "comment": {
                        "value": "# Response to reviewer iqSb\n\nThanks for your constructive comments. We hope our responses can address your concerns. Further comments are welcomed.\n\n-----\n\nQ1: *In Section 3.1, the paper mentions that \u201cthe core of this task lies in designing an accurate point-to-box regressor\u201d, and introduce the P2P framework (point-to-visual prompt), but it is inconsistent between \u201cpoint-to-box\u201d and \u201cpoint-to-visual prompt\u201d, it is suggested to add the description of process of visual prompt-to-box/mask after*.\n\nA1: Thanks for your detailed comment. In P2P, the two stages, SEPG and PGSR, respectively serve the roles of \u2018point to prompt\u2019 and \u2018prompt to pseudo-mask\u2019. We have added this description in Section 3.1 to make the logic more complete.\n\n\n\nQ2: *What\u2019s the meaning of the orange arrow in the lower part of the Figure 2? (i.e., the arrow on the right of \u2018Refine box b\\*\u2019)*.\n\nA2: Thanks for your detailed comment. This refined box b* is generated by SEPG stage and serves as one part of the prompt, subsequently inputted into PGSR.\n\n \n\nQ3: *The concept \u2018proposal seeds\u2019 is proposed in Section 3.1, but its specific generation way (i.e., outer rectangle of the mask generated by SAM) is given until Section 3.4, it is suggested to explain its specific concept early on to avoid confusion. Furthermore, Section 3.4 should be repositioned closer to the beginning of the paper to provide readers with a clearer understanding of the overall method's workflow.*\n\nA3: Thanks for your suggestion. We have reorganized Sections 3.1 and 3.4 in the revised version.\n\n\n\nQ4: *What\u2019s the concrete content of \u2018sharing some of the weights\u2019 about the two refiners mentioned in Section 3.2?*\n\nA4: The two refiners involve a shared backbone and, additionally, in the head, there are two shared fully connected (FC) layers. In the revised version, we have added additional details in the Appendix.\n\n \n\nQ5: *What are the key differences in the workflow of the proposed Refiner2 compared to the refining stage in previous works [1]?*\n\nA5: The main differences lies in:\n\n1) **Structural differences**: The refiner in P2P comprises a classification head and an embedding head, which calculate classification scores and feature embeddings, respectively. The classification scores are utilized for optimizing proposals, while the feature embeddings are employed for updating semantic prototypes. On the other hand, P2BNet utilizes a multi-instance learning structure with two classification heads for computing classification scores and instance scores. \n2) **Optimization differences**: We use the proposal scores from *Refiner II* as indicators to update the semantic prototypes with high-quality embeddings. The high-quality prototypes are then used in *Refiner I* to compute instance probabilities, obtaining high-quality proposals. When these proposals are input into Refiner 2, it further enhances the quality of semantic prototypes. We believe there is a mutually reinforcing relationship between *Refiner I* and *Refiner II*, which is absent in [1].\n\n\n------\n*[1]* *Chen, Pengfei, et al. \"Point-to-box network for accurate object detection via single point supervision.\" European Conference on Computer Vision, 2022.*\n\n------\n\n\nQ6: *How about the computational complexity of the proposed method compared to other methods?*\n\nA6: As suggested, we added a comparison of memory and time consumption in the appendix, and the explanation for this is provided in the *unified response* part. \n\n \n\nQ7: *In Section 3.4, the actual input of the subsequent P2P refiner is the initial proposal seed box (i.e., outer rectangle of the mask generated by SAM). If possible, it is recommended to use the box (e.g., the fully supervised bounding box label) as the prompt of SAM to conduct a more comprehensive experimental comparison in segmentation, rather than just using the point as the prompt of SAM for comparison.*\n\nA7: In the revised version, we supplemented the results using GT bounding box as a prompt in Tab. 3, which can be considered an upper bound for point supervision methods. Using only point as supervision, we reached performance levels of **85% (26.4 vs 31.1)**, **91% (48.6 vs 53.1)**, and **80% (26.2 vs 32.6)** on three respective metrics when using bounding box prompts. This indicates that P2P approaches the performance of bounding box supervision, reaffirming the effectiveness of our method.  \n\n| Method | Sup. |AP | AP50 | AP75 |\n| :-------------: | :-----------: | :---------: | :--------------: | :--------------: |\n|     SAM-MR      |     Point     |    24.3     |       43.8       |       24.3       |\n|     SAM-MR      |      box      |    31.1     |       53.1       |       32.6       |\n|     Ours-MR     |     Point     |    26.4     |       48.6       |       26.2       |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490148978,
                "cdate": 1700490148978,
                "tmdate": 1700537606505,
                "mdate": 1700537606505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e4PJPiErri",
                "forum": "LNTexdca08",
                "replyto": "4h1ppLJ38Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer iqSb (2st)"
                    },
                    "comment": {
                        "value": "Dear Reviewer iqSb,\n\nWe would like to express our sincere gratitude for your time and efforts in improving our paper.\n\nConsidering your concerns about our paper, we have revised some of the content again and summarized it below:\n\n- We've reorganized the methods section, merging 3.4 into 3.1 to give the reader a better understanding of our framework.\n\n- Regarding *Prompt refiner II*, we placed the homogenization with the previous method in the Appendix to highlight the innovations. \n\nWe hope the above improvement could clarify your concerns. Looking forward to your feedback and discussion. Thanks again!\n\nBest,\n\nPaper 1568 Authors."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632982274,
                "cdate": 1700632982274,
                "tmdate": 1700632982274,
                "mdate": 1700632982274,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6lIrer1Mog",
            "forum": "LNTexdca08",
            "replyto": "LNTexdca08",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduce foundation model SAM into point supervised object detection and segmentation task. Benefit from the high generalized ability of SAM, the object segmentation map can be easier obtained with the point prompt. However, the mask provided by SAM are not what we need sometimes because SAM is semantic-free and brings ambiguity. The author proposed a method to generate better-quality segmentation map as the pseudo label for training of detection segmentation task. I support for what the author said: \u2018rather than directly designing large foundation models, it is more meaningful to leverage them for specific tasks in resource-constrained situations.\u2019"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I support for the combination of foundation model and specific tasks and interested in the point-supervised tasks.\nThe proposed method bridges the gap between PSOD (or PSIS) and fully-supervised method."
                },
                "weaknesses": {
                    "value": "There are some questions here:\n1\u3001The author said \u2018we observe that only 40% of the masks covered more than 70% foreground pixels\u2019. However, as I know, because the semantic-free of SAM, the highest score mask generated by SAM may not what we what, but the top-3 masks may contains what we want in most situation. What we should do is to select the best one (or with extra refinement) from the top-3 masks. But the paper only choose the highest score one and claimed \u2018only 40% of the masks covered more than 70% foreground pixels\u2019, I think this is a handmade problem. In other words, the author do not make full use of foundation model and make the problem more difficult.\n2\u3001I think the framework is lack of novelty and a little engineering. If I understand you correctly\uff0c the structure is (SAM+P2BNet(CBP stage + PBR stage))+(SAM+ P2BNet)+ (SAM+ P2BNet) ....... The whole paragraph of \u2018SEMANTIC-EXPLICIT PROMPT GENERATION\u2019 is P2BNet (the network, the loss, the sampling is similar) and the \u2018PROMPT GUIDED SPATIAL REFINEMENT\u2019 is the application of SAM. I think the combination of P2BNet and SAM is OK, but the author did not propose some insights or other challenges in combination. And the ITERATIVE LEARNING is a little engineering, I am interested in the time cost in practical application.\n3\u3001Some other problems: (1) The visualization is unclear if I do not enlarge the image or I print it. The line is too thin. (2) And I think the title is not suitable because another paper is named as P2P: Rethinking Counting and Localization in Crowds:A Purely Point-Based Framework, ICCV2021. The tasks are different but relevant, and are all point-based. (3) I think the experiments on better detectors or segmentation network (or better backbone?) are needed. The faster RCNN is classic but too old. The fully-supervised method is far beyond 30+ or 40+ on COCO."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1568/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG",
                        "ICLR.cc/2024/Conference/Submission1568/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1568/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769183561,
            "cdate": 1698769183561,
            "tmdate": 1700580257585,
            "mdate": 1700580257585,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GxVuLuX65n",
                "forum": "LNTexdca08",
                "replyto": "6lIrer1Mog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 7jkG (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive comments. We hope our responses can address your concerns. Further comments are welcomed.\n\n------\n\nQ1: *\u201cThe author said \u2018we observe that only 40% of the masks covered more than 70% foreground pixels\u2019. However, as I know, because the semantic-free of SAM, the highest score mask generated by SAM may not what we what, but the top-3 masks may contain what we want in most situation. What we should do is to select the best one (or with extra refinement) from the top-3 masks. But the paper only chooses the highest score one and claimed \u2018only 40% of the masks covered more than 70% foreground pixels\u2019, I think this is a handmade problem. In other words, the author does not make full use of foundation model and make the problem more difficult.\u201d*\n\n \n\nA1: Thank you for your insightful comment. We perceive this as two distinct approaches. To obtain semantically accurate masks, as you mentioned, one could design a selector to choose the most accurate mask from the three. This might yield better results, but it does not guarantee success in complex scenarios.\n\nWe analysis that the significant semantic differences in masks generated by SAM can be attributed fundamentally to the semantic uncertainty in the provided prompts. Thus, we can address this by improving the quality of the prompts themselves. By progressively enhancing the semantic confidence of the prompts, we aim to ensure the foundation model outputs accurate masks.\n\n \n\nQ2: *\u201cI think the framework is lack of novelty and a little engineering. If I understand you correctly\uff0c the structure is (SAM+P2BNet(CBP stage + PBR stage))+(SAM+ P2BNet)+ (SAM+ P2BNet) ....... The whole paragraph of \u2018SEMANTIC-EXPLICIT PROMPT GENERATION\u2019 is P2BNet (the network, the loss, the sampling is similar) and the \u2018PROMPT GUIDED SPATIAL REFINEMENT\u2019 is the application of SAM. I think the combination of P2BNet and SAM is OK, but the author did not propose some insights or other challenges in combination. And the ITERATIVE LEARNING is a little engineering, I am interested in the time cost in practical application.\u201d*\n\nA2:\n\n1. Firstly, the *main distinctions between SEPG and P2BNet* are as follows: \n\n   - **differences in Structure**: Diverging from classical MIL frameworks, we employ a more stable feature prototype for computing instance-level probabilities. The refiner in P2P comprises a classification head and an embedding head, which calculate classification scores and feature embeddings, respectively. P2BNet utilizes the MIL structure with two classification heads for computing classification scores and instance scores. \n\n   - **Differences in Sampling**: In order to mitigate the randomness associated with sampling and optimization, we employ a **seed-based group sampling** strategy, leveraging prior knowledge provided by the foundation model. In contrast, P2BNet employs point-centered neighbor sampling. \n\n   - **Differences in optimization**: P2P follows a **group-then-individual** fashion to reduce the solution space. In Prompt refiner I, semantic and instance probabilities for each group are computed, and through optimizing the joint probability, the group with the highest score is selected. And then, the proposals in the selected group are augmented as the input of the Prompt refiner II. P2BNet employs a multi-level individual optimization approach.\n2. Furthermore, we would like to emphasize that our novelty extends beyond individual components. Our work contributes novel insights to the community in addressing weakly supervised or point-supervised tasks, specifically focusing on how to better leverage foundation models in the era of the foundation model to improve solutions for weakly supervised problems.\n3. Regarding time consumption, while P2P consumes a bit higher time and memory during the training phase, it yields pseudo-labels with higher quality. We think that the time consumption of point-supervised methods itself may not be a primary limitation in practical applications. Leveraging these high-quality pseudo-labels, it is free for users to employ any detectors to conduct specific tasks or applications. Therefore, compared to the substantial improvement in performance, the time consumption during training is not an issue."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489730100,
                "cdate": 1700489730100,
                "tmdate": 1700537427515,
                "mdate": 1700537427515,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V3mwlGLpS1",
                "forum": "LNTexdca08",
                "replyto": "6lIrer1Mog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 7jkG (2/2)"
                    },
                    "comment": {
                        "value": "Q3.1: *\u201cThe visualization is unclear if I do not enlarge the image or I print it. The line is too thin.\u201d*\n\nA3.1: Thank you for your reminder. In the revised version, we have redrawn the visualization to address the concerns.\n\n\n\nQ3.2: *\u201cAnd I think the title is not suitable because another paper is named as P2P: Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework, ICCV2021. The tasks are different but relevant, and are all point-based.\u201d*\n\nA3.2: Thanks for the comment. The paper you referenced presents a point-to-point network named **P2PNet** designed for crowd counting tasks. While both employ the term *P2P*, the tasks and expressions hold distinct meanings. Our reference to \"point to prompt\" reflects a different context. Hence, we consider this to be coincidental. We appreciate your feedback and are open to making adjustments if necessary.\n\n\n------\n[1] *Song, Qingyu, et al. \"Rethinking counting and localization in crowds: A purely point-based framework.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.*\n\n------\n\nQ3.3: *I think the experiments on better detectors or segmentation network (or better backbone?) are needed. The faster RCNN is classic but too old. The fully-supervised method is far beyond 30+ or 40+ on COCO*.\n\nA3.3: Thank you for your insightful advice. To ensure a fair comparison with previous methods, we refrained from employing more advanced detectors or segmentation networks. In the revised version, as suggested, we have supplemented the experiments in the appendix on advanced detectors and segmentation networks.\n\n1) When **DINO (both R-50 and Swin-L are used as backbone)** [2] is adopted as the detector, our method achieves a 38.2 AP (R-50) and 45.1 AP (Swin-l) on COCO dataset, approximately 80% of the fully supervised performance, and a 57.2 AP50 (R-50) and 66.1 AP50 (Swin-l), about 87% of the fully supervised performance. \n\n2) When the **Mask2Former** [3] is employed as the segmentation network, achieving a 34.9 AP and 58.9 AP50. \n\nDue to the limited time, we do not adjust the parameters carefully, but it is sufficient to verify the effectiveness of the proposed method. We believe that the performance will be further improved. In summary, when utilizing more advanced detector or segmentation network, our method still closely approaches the performance of fully supervised methods.\n\n|   Method    | backbone |  AP  | AP50 | AP75 |\n| :---------: | :------: | :--: | :--: | :--: |\n|    DINO     |   R-50   | 49.0 | 66.4 | 55.3 |\n|    DINO     |  Swin-l  | 57.2 | 75.7 | 62.7 |\n|  Ours-DINO  |   R-50   | 38.2 | 57.2 | 40.9 |\n|  Ours-DINO  |  Swin-l  | 45.1 | 66.1 | 48.9 |\n| Mask2former |  Swin-s  | 46.1 | 69.4 | 49.8 |\n|   Ours-MF   |  Swin-s  | 34.9 | 58.9 | 37.6 |\n\n\n------\n*[2] Zhang, Hao, et al. \"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.\" The Eleventh International Conference on Learning Representations. 2022.*\n\n*[3] Cheng, Bowen, et al. \"Masked-attention mask transformer for universal image segmentation.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489850324,
                "cdate": 1700489850324,
                "tmdate": 1700537484055,
                "mdate": 1700537484055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E3T1e0Dngi",
                "forum": "LNTexdca08",
                "replyto": "GxVuLuX65n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG"
                ],
                "content": {
                    "comment": {
                        "value": "R to A1: Yep, I agreed with the motivation to progressively enhance the semantic confidence of the prompts for better SAM result. I thought about it\uff0cand I agreed that think they are two approaches that do not conflict. So, I want to let them as the advice that select the best proposal from the three will give better initialisation for your approach. We hope this will bring you some insight.\n\nR to A2: First, I agree with that the novelty is how to make full use of the ability of foundation model in weakly-supervised task, which extends beyond individual components. In original review, the homogeneity with P2BNet results in low score. In rebuttal, I think the author make it clear in A2-1 and address my concern.  I hope the author will reorganise the methods section, simplify the same and highlight the differences. Due to this will not affect the main contribution,  I will raise my score.\n\nR to A3\uff1aBecause in inference stage, only the retrained detector or segmentation model will be used, so the inference time depends on the retrained network. I am still interested in the implement details in  ITERATIVE LEARNING\uff0cwhich will affect the training time I think. Do you extract the image feature wil SAM and save it in memory first, then you conduct your method and generate better prompt for SAM and use the prompt to produce better mask in mask decoder of SAM, finally do the interative learning? So you can make it clear that which component will be repeatly used in interative refinement and which will be conduct only one time."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579387666,
                "cdate": 1700579387666,
                "tmdate": 1700579387666,
                "mdate": 1700579387666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lafmkt4toW",
                "forum": "LNTexdca08",
                "replyto": "V3mwlGLpS1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Reviewer_7jkG"
                ],
                "content": {
                    "comment": {
                        "value": "R to A3.2. Because they are exactly the same, so the title depends on you. I just tell you that I have read a paper with a similar abbreviation of your method (P2PNet vs. P2P)\n\nR to A3.3, I am satisfied with the experiments on sota detector and segmentation method. Add it to the table in the paper. By the way, please mark the training details (e.g. the training epoch (2P and retained, respectively), the backbone (P2P and retained, respectively) and others you think important). And, please add a swin-l mask2former when you have time afterwards."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580168529,
                "cdate": 1700580168529,
                "tmdate": 1700580168529,
                "mdate": 1700580168529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KwWEO2EkVX",
                "forum": "LNTexdca08",
                "replyto": "6lIrer1Mog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 7jkG (2st)"
                    },
                    "comment": {
                        "value": "1. Thank you very much for your suggestion, we strongly agree that this is a good idea and inspired us a lot. We will try it if time permits.\n\n2. We are encouraged that your concerns about the novelty of our approach have been addressed and thank you for your positive comments.  We have reorganized the methods part following your advice in the latest revised version. \n\n3. In the iterative learning part, each iteration consists of (SEPG+PGSR), we first train SEPG for 12 (when T=1) or 6 (when T > 1) epochs to get a semantically clear prompt, and then input it into SAM as PGSR stage and output a better mask. if T=1, the iteration is terminated, if T>1, we continue to transform the obtained mask into a proposal seed for the next round of iteration. \nInstead of pre-saving the features, we re-extract the features in every iteration. \n\nWe strongly agree that it is a good method to reduce the time cost  by pre-saving the features into memory. This won't make a difference to the results, but it will save time for SAM to extract the features. We will try to apply this idea. \n\nThank you again for your time and effort in improving our work. Further comments are welcomed."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631462922,
                "cdate": 1700631462922,
                "tmdate": 1700633403059,
                "mdate": 1700633403059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3wThP7vZZd",
                "forum": "LNTexdca08",
                "replyto": "6lIrer1Mog",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1568/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 7jkG (2st)"
                    },
                    "comment": {
                        "value": "We added the number of retrained epochs in the table, following the default settings. And we have been added detailed explanations to the revised version. Thanks for your advice.\n\n\n|   Method    | Backbone | Retrained Epoch |  AP  | AP50 | AP75 |\n| :---------: | :------: | :-------------: | :--: | :--: | :--: |\n|    DINO     |   R-50   |       12e       | 49.0 | 66.4 | 55.3 |\n|    DINO     |  Swin-l  |       12e       | 57.2 | 75.7 | 62.7 |\n|  Ours-DINO  |   R-50   |       12e       | 38.2 | 57.2 | 40.9 |\n|  Ours-DINO  |  Swin-l  |       12e       | 45.1 | 66.1 | 48.9 |\n| Mask2former |  Swin-s  |       50e       | 46.1 | 69.4 | 49.8 |\n|   Ours-MF   |  Swin-s  |       50e       | 34.9 | 58.9 | 37.6 |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1568/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631512054,
                "cdate": 1700631512054,
                "tmdate": 1700633425831,
                "mdate": 1700633425831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]