[
    {
        "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction"
    },
    {
        "review": {
            "id": "VyEobgXm2s",
            "forum": "DmD1wboID9",
            "replyto": "DmD1wboID9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4365/Reviewer_wpkj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4365/Reviewer_wpkj"
            ],
            "content": {
                "summary": {
                    "value": "Prompt-tuning is a fine-tuning paradigm based on large-scale pre-trained language models (PLMs), which can reduce the gap between downstream tasks and pre-training objectives. This paper focus on the challenge of poor generalization to specific few-shot patterns of the Prompt-tuning. Through distribution analysis, they reveal that the root cause of this issue is the overabundance of conceptual knowledge in PLMs and the truncated knowledge for target downstream domains. This collective effect misaligns the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To address this issue, they propose BayesPrompt, an approach that intuitively explores debiased approximation of unabridged target domains of downstream tasks. BayesPrompt generates domain-discriminative prompts to provide unambiguous guidance for PLMs. Further, they theoretically show that BayesPrompt tightens the upper bound of the classification error on PLMs' downstream inference on classification error bounds. The experimental results show that the proposed method achieves SOTA performance on benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe paper reveals the principles of the challenge of prompt-tuning on pre-trained large models for few-shot tasks.\n2.\tThe methodology of using the Bayesian prompt is novel and effective.\n3.\tThe theoretical guarantees the performance of the proposed method.\n4.\tThe evaluation presents the benefits of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tThis paper utilizes the GMM to approximate the distribution of the target domain which may not be unabridged. The real distribution of the target domain is complex and unknown.\n2.\tThe PLMs utilized in the evaluation are not clear. Using various PLMs may be better to show the generality of the proposed method."
                },
                "questions": {
                    "value": "1.\tWhy can GMM approximate the target domain? What are its benefits than a learnable generator (VAE or GAN)?\n2.\tIs it required to train a specific GMM for each input sentence (X, Y)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Reviewer_wpkj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698497181215,
            "cdate": 1698497181215,
            "tmdate": 1699636408849,
            "mdate": 1699636408849,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tDfRnahCG0",
                "forum": "DmD1wboID9",
                "replyto": "VyEobgXm2s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reasons for Choosing GMM and the Usage on Input Sequences, Descriptions of the PLM Used in the Evaluation, Added Experiments and Analysis of the Generality (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer wpkj for the valuable feedback and constructive suggestions. We are encouraged that the reviewer found that this work is novel and effective, the theoretical proof is integrated, and the presentation is good. The mentioned issues are addressed as follows:\n\n**W1: This paper utilizes the GMM to approximate the distribution of the target domain which may not be unabridged. The real distribution of the target domain is complex and unknown.**\n\n**A:** Thanks for the review. Since the factual distribution of the target domain is complex and unknown, the distribution approximation possesses the potential for further optimization. Due to the insufficiency of samples of the target domain, the corresponding distribution of the target domain may not adhere to the typical Gaussian distribution, i.e., the central limit theorem may not well fit the distribution of the target downstream domain.\n\nTheoretically, GMM can fit any probability density distribution, so we use Gaussian mixture distribution to approximate the factual distribution of the target domain. We further provide empirical proofs to demonstrate the effectiveness and validation of our motivation and the behavior of introducing GMM to approximate the distribution of the target domain, which is shown in **Figure 4(a)**, Section 6, Page 9, of the submitted manuscript. We observe from the empirical results that as the sample size increases, the performance gap between our method using GMM and our method using the typical Gaussian distribution decreases, which further proves our analysis that when the samples are limited, the distribution of the target domain does not fit the typical Gaussian distribution, while according to the central limit theorem, as the increasing of the sample size, the distribution of the target domain gradually fits the typical Gaussian distribution. Concretely, our proposed approach, using GMM instead of the typical Gaussian distribution, is theoretical and technically solid. We will add the above analysis in the discussion of Figure 4(a) of the final version of our manuscript to better explain our contributions.\n\n**W2: The PLMs utilized in the evaluation are not clear. Using various PLMs may be better to show the generality of the proposed method.**\n\n**A:** Thanks for the review. The PLM used in the evaluation is **RoBERTa-large**, which is mentioned in Appendix C.2, Page 19, of the revised manuscript. Theoretically, the proposed method can generally apply to all problems that involve \u201c**sub-knowledge domains**\u201d, such as text classification, entity recognition, entity disambiguation, etc.\n\nTo further demonstrate the generality of the proposed BayesPrompt, we conducted text classification tests on a specific automotive industry dataset using another pre-trained language model, i.e., **BERT**, with a parameter count of 110 million. The obtained classification accuracy was 74.0%. We also employed TextRCNN to train the classification model with a parameter count of 15 million. The resulting classification accuracy reached 95.3%, significantly outperforming the pre-trained language model BERT, which is due to the over-multitudinous conceptual knowledge learned by BERT. However, when using BayesPrompt, it achieves the best classification accuracy of 97.4%. Note that the method does not require training (fine-tuning) the network of BERT. Therefore, the proposed method can be generalized into various tasks with various PLMs and obtain performance improvements to baselines."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226065559,
                "cdate": 1700226065559,
                "tmdate": 1700226065559,
                "mdate": 1700226065559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "95gh912POl",
                "forum": "DmD1wboID9",
                "replyto": "VyEobgXm2s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reasons for Choosing GMM and the Usage on Input Sequences, Descriptions of the PLM Used in the Evaluation, Added Experiments and Analysis of the Generality (2/2)"
                    },
                    "comment": {
                        "value": "**Q1: Why can GMM approximate the target domain? What are its benefits than a learnable generator (VAE or GAN)?**\n\n**A:** Thanks for the review. GMM is a probabilistic model, which assumes that all data points are generated from a mixture of Gaussian distributions with unknown parameters. Based on the fact that the Gaussian mixture model can theoretically fit any probability density distribution and the fact that due to the limited sample size in the target downstream domain, the factual distribution of the target downstream domain **does NOT** fit the conventional Gaussian distribution well, directly adopting the Gaussian distribution-based approach, e.g., conventional VAE, may learn biased distribution of the target downstream domain, such that we use the Gaussian mixture distribution to approximate the debiased factual distribution of the downstream domain within the knowledge space of PLMs.. In contrast, VAE and GAN, as learnable generators, can also approximate the target domain, but the inherent defects of VAE and GAN degenerate their performance of approximating the distribution of the target domain with insufficient samples, and we elaborate on the reasons as follows.\n\nVAE conventionally holds the prior distribution as the **typical Gaussian distribution** and then learns the latent representation of data by maximizing the KL divergence of latent variables of the posterior distribution and the prior distribution. The inherent mechanism presents that conventional VAE can only perform limited distortion on the prior distribution, i.e., the typical Gaussian distribution, and cannot well fit the complex distribution, e.g., the distribution of the target domain with **limited sample size**.\n\nGAN learns to generate data by training two neural networks in an adversarial manner **without any priori**. One network is the generator, responsible for generating new data, while the other is the discriminator, tasked with distinguishing between real and generated data. Due to the lack of priori, GAN requires **sufficient data** to approximate the distribution of the target domain. However, in few-shot scenarios, the limited samples are far from completely supporting the sufficient training of GAN.\n\nCompared to learnable generators, using GMM to approximate the target domain provides the following advantages:\n\n1.**Strong interpretability**. GMM is a probabilistic model, and its parameters directly correspond to the mean, covariance, and mixture coefficients of the data. This makes the results of GMM more easily interpretable and understandable.\n\n2.**High computational and sample efficiency**. Compared to VAE and GAN, the training and inference of GMM are generally more efficient, and GMM requires relatively fewer samples for optimization. GMM is a classical statistical model based on maximum likelihood estimation, while GAN and VAE typically involve more complex optimization processes.\n\n3.**Stable training**. Compared to GAN, GMM tends to be more stable during the training process. GAN training may be affected by issues such as mode collapse, while GMM is less prone to these problems.\n\n**Q2: Is it required to train a specific GMM for each input sentence (X, Y)?**\n\n**A:** Thanks for the review. In our approach, we do not train a specific GMM for each input sentence (X, Y). Instead, we train a GMM on the downstream task dataset and use the obtained Gaussian mixture distribution to approximate the debiased factual distribution of the target domain. Subsequently, prompts with domain discriminative information are generated by sampling from the approximated distribution. During each prediction, the prompt containing domain discriminative information needs to be added to the input of the downstream task to provide de-ambiguous guidance for PLMs. As a result, it increases the time complexity. In experiments, several optimization strategies can be considered to reduce the time complexity, such as sampling or subsampling datasets, employing feature selection or dimensionality reduction techniques to decrease the input feature dimensions, or implementing a training strategy with a warm-up. \n\nAdditionally, the time complexity of BayesPrompt is only slightly higher than the baseline. For instance, in the 1-shot setting of the SemEval dataset, the training time cost of each epoch at the main baseline is 16 seconds, while the training time cost of each epoch at BayesPrompt is 27 seconds. Nevertheless, the main baseline achieves an F1 score of 28.6%, whereas BayesPrompt achieves an F1 score of 35.1%. Compared to the improvement of BayesPrompt on the F1 score, the time complexity brought by Bayesprompt is acceptable."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700226132901,
                "cdate": 1700226132901,
                "tmdate": 1700226132901,
                "mdate": 1700226132901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9XrEIBBJcj",
                "forum": "DmD1wboID9",
                "replyto": "95gh912POl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_wpkj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_wpkj"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thanks for your response."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585866461,
                "cdate": 1700585866461,
                "tmdate": 1700585866461,
                "mdate": 1700585866461,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JXKPMbV4ft",
            "forum": "DmD1wboID9",
            "replyto": "DmD1wboID9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes BayesPrompt, a Bayesian approach to approximate the factual distributions of downstream domains \nand thereby generating discriminative prompts for PLMs. The authors articulate that the intrinsic issues behind the poor performance of finetuned PLMs on few-shot downstream tasks roots from two main shortcomings: (i) over-multitudinous of conceptual knowledge contained in PLMs, (ii) an abridged knowledge for target downstream domains. The paper takes a stride in addressing this challenge with both theoretical (tailored towards a classification problem) as well as experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written, well structured and has a clear narrative. \n- The authors pay utmost attention to details, from notations and math to presentation of the results, making the paper easy to follow.\n- The paper has a healthy mix of a (simplified) theoretical and qualitative arguments, based on which the approach is devised. \n- The results seem to be promising, comparing against some recent baselines. \nOverall, it seems like a solid contribution."
                },
                "weaknesses": {
                    "value": "- The paper is essentially a shortened version of a much longer manuscript, where the authors are constantly cutting the content short and referring the reader to different sections of the appendix (appendix is referred to 11 times throughout the paper!). So, the main body of the paper is not really self-contained and heavily relies on the appendix. By the same token, the main algorithm of the paper had to be pushed to the Appendix, which could be a natural choice in the main text to clarify the end-to-end procedure. \n- The impact of the proposed approach is rather marginal when compared to the closest competitors (say RetrievalRE), especially on standard RE performance in Table 3, while at the same it comes at the cost of extra training complexity. Any reason behind this?  \n- No Ablation studies. There are design choices that could potentially establish the basis for Ablation studies (such as Kernel size and so)."
                },
                "questions": {
                    "value": "No further questions (beyond what's already raised in weaknesses), and after reading through the Appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777904048,
            "cdate": 1698777904048,
            "tmdate": 1699636408761,
            "mdate": 1699636408761,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wcxmXUvAaK",
                "forum": "DmD1wboID9",
                "replyto": "JXKPMbV4ft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revised Manuscript of Making the Main Body More Self-Contained, Reasons Behind the Performance Improvement, Added Experiments and Analysis of the Ablation Study (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate the thoughtful feedback of Reviewer ox7r. We are glad the reviewer found that this work is promising, the contribution is solid, the theoretical validation is sufficient, and the writing is excellent. The mentioned issues are addressed as follows:\n\n**W1: The paper is essentially a shortened version of a much longer manuscript, where the authors are constantly cutting the content short and referring the reader to different sections of the appendix (appendix is referred to 11 times throughout the paper!). So, the main body of the paper is not really self-contained and heavily relies on the appendix. By the same token, the main algorithm of the paper had to be pushed to the Appendix, which could be a natural choice in the main text to clarify the end-to-end procedure.**\n\n**A:** Thank you for your review and suggestions. Due to the page limitation, we have tried to organize our original manuscript, but there are still some critical parts of our paper that need to be improved, such that we have revised the paper to make the main body more self-contained and then submitted the revised manuscript for now.\n\nSpecifically, in the revised version, we have relocated the main algorithm from the Appendix to the main body of the paper to enhance overall clarity, which is demonstrated in Section 4, Page 6, of the revised manuscript. Additionally, a wealth of the referred contents in Appendix are the extended descriptions and extended experiments of the main paper, which are unnecessary for understanding our works, such that we have removed certain references of the Appendix in the main content of our paper to make the manuscript more understandable, e.g., \u201c**Appendix C.3** provides further empirical evidence.\u201d in Introduction, Page 1, of the original manuscript, \u201cRefer to **Appendix B** for the procedure of BayesPrompt\u201d in Section 4, Page 6, of the original manuscript, \u201cThe statistical details are provided in **Appendix D.1**.\u201d and \u201cRefer to **Appendix D.2** for more implementation details.\u201d and \u201cand please refer to **Appendix C.2** for details\u201d in Section 6, Page 8, of the original manuscript, and \u201cRefer to **Appendix C.5** for further analysis of the empirical results.\u201d in Section 6, Page 9, of the original manuscript. Concretely, there are only a few necessary references of the Appendix that are reserved in the main paper, and the main paper of the revised manuscript is well self-contained for now."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225861279,
                "cdate": 1700225861279,
                "tmdate": 1700225861279,
                "mdate": 1700225861279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QJcGBjHQcg",
                "forum": "DmD1wboID9",
                "replyto": "JXKPMbV4ft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revised Manuscript of Making the Main Body More Self-Contained, Reasons Behind the Performance Improvement, Added Experiments and Analysis of the Ablation Study (2/3)"
                    },
                    "comment": {
                        "value": "**W2: The impact of the proposed approach is rather marginal when compared to the closest competitors (say RetrievalRE), especially on standard RE performance in Table 3, while at the same it comes at the cost of extra training complexity. Any reason behind this?**\n\n**A:** Thanks for the review. In this paper, we choose KnowPrompt as our primary baseline. RetrievalRE is the follow-up work of KnowPrompt, such that RetrievalRE achieves superior performance. However, whether in the few-shot or standard setting, on average, BayesPrompt consistently outperforms both KnowPrompt and RetrievalRE, confirming the effectiveness of this approach. Specifically, in few-shot scenarios, the proposed BayesPrompt widely improves the benchmark methods, including the primary baseline (KnowPrompt) and RetrievalRE, e.g., BayesPrompt beats KnowPrompt by 3.24% and beats RetrievalRE by 1.29% on average. In standard scenarios, the improvements of benchmark methods to their baseline are consistently limited, e.g., RetrievalRE beats KnowPrompt by 0.2%, such that the improvement of BayesPrompt to RetrievalRE is relatively acceptable. Note that, the primary baseline of BayesPrompt is KnowPrompt, and BayesPrompt beats KnowPrompt by 0.4%, which is relatively significant. We also perform the significance test, i.e., t-test, with the primary baseline, and observe that the P values are consistently lower than 0.05, e.g., 0.045 on the SemEval dataset, indicating that the improvement of BayesPrompt is significant. Concretely, for the reason behind such a deviation existing in the few-shot scenarios and standard scenarios, we further derive a conclusive analysis as follows:\n\nAs we discussed in the Abstract and Introduction sections on Page 1 and Page 2, of the submitted manuscript, the limited and discrete semantic information contained in the training samples from downstream domains can barely support the conventional trainable prompts to acquire sufficient supervision, such that the guidance of the generated prompts is trivial to PLMs. Especially, such a challenge further exacerbates the performance of PLMs in few-shot scenarios. With this motivation, the proposed BayesPrompt aims to learn a prompt that contains relatively sufficient discriminative knowledge for the target downstream domain, which is achieved by proposing the debiased domain abstraction and then generating the prompt. Thus, the empirical observation, i.e., the performance improvement derived by introducing BayesPrompt on few-shot scenarios is more significant than that on standard scenarios, jointly proves the motivation and the effectiveness of the proposed BayesPrompt.\n\nFrom the perspective of time complexity, the time complexity introduced by BayesPrompt primarily stems from the necessity of adding prompts containing domain discriminative information for downstream tasks during each prediction, providing de-ambiguous guidance for PLMs. In experiments, several optimization strategies can be considered to reduce the time complexity, such as sampling or subsampling datasets, employing feature selection or dimensionality reduction techniques to decrease the input feature dimensions, or implementing a training strategy with a warm-up. Additionally, the time complexity of BayesPrompt is only slightly higher than the baseline. For instance, in the 1-shot setting of the SemEval dataset, the training time cost of each epoch for the main baseline is 16 seconds, while the training time cost of each epoch for BayesPrompt is 27 seconds. Nevertheless, the main baseline achieves an F1 score of 28.6%, whereas BayesPrompt achieves an F1 score of 35.1%. Compared to the improvement of BayesPrompt on the F1 score, the time complexity brought by Bayesprompt is acceptable."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225915272,
                "cdate": 1700225915272,
                "tmdate": 1700225915272,
                "mdate": 1700225915272,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sVxlR4q8Z2",
                "forum": "DmD1wboID9",
                "replyto": "JXKPMbV4ft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Revised Manuscript of Making the Main Body More Self-Contained, Reasons Behind the Performance Improvement, Added Experiments and Analysis of the Ablation Study (3/3)"
                    },
                    "comment": {
                        "value": "**Table 1:** F1 scores (%) of BayesPrompt with different numbers of GMM components.\n\n| Dataset | Split | Number of Components | BayesPrompt    |\n| :--------: | :-----: | :----------: | :--------------: |\n|          |    | 9 |  33.7(\u00b13.6)    |\n|          | K=1 | **18** |  **35.1(\u00b12.9)**    |\n|          |    | 9 | 70.3(\u00b12.8)     |\n| SemEval | K=5   | **18** |  **71.6(\u00b13.3)**    |\n|          |    | 27 |  70.8(\u00b12.7)   |\n|          |    | 9 | 80.8(\u00b11.1) |\n|          | K=16   | **18** |**81.8(\u00b11.2)**|\n|          |    | 27 |80.8(\u00b11.5)|\n\n**W3: No Ablation studies. There are design choices that could potentially establish the basis for Ablation studies (such as Kernel size and so).**\n\n**A:** Thanks for the review. In this paper, we conduct the ablation study to prove the effectiveness of the type prompt words, i.e., the discriminative prompts proposed by BayesPrompt, and the results are shown in **Figure 4(b) and (c)**. The intuition behind the discriminative prompts is that the domain discriminative information is injected into the prompt in BayesPrompt. It can be seen that BayesPrompt consistently outperforms the ablation model on all datasets within various experimental settings. Additionally, the performance improvement brought by the discriminative prompts in the few-shot scenario (Figure 4(b)) is significantly stronger than its performance enhancement in the full dataset (Figure 4(c)). We attribute this difference to the fact that, in the few-shot scenario, the limited amount of data may hinder PLMs from fully learning the distribution and features of the downstream task data, leading to relatively poorer performance. So, when utilizing the discriminative prompts to guide PLMs in locating knowledge domains relevant to the downstream domain, the deficiencies caused by the limited data are noticeably mitigated. However, in the full dataset, the relatively abundant downstream task data provides a rich knowledge background, resulting in a substantial reduction in the deviation between the knowledge domain located by PLMs and the real downstream knowledge domain. This, in turn, weakens the de-biasing effect of the discriminative prompts on the few-shot dataset. Therefore, the performance improvement brought by the discriminative prompts is more pronounced in the few-shot scenario than in the full dataset, which can effectively prove the validation of our exploration and motivation in the Introduction.\n\nTo better perform a sufficient ablation study, we also ablate the GMM in BayesPrompt and impose the conventional Gaussian distribution as the prior distribution in BayesPrompt, which is demonstrated in Figure 4(a), Page 9, of the original manuscript. We further perform the hyper-parameter sensitivity experiments, and the results are shown in Figure 5, Page 19, Appendix C.2, of the submitted manuscript.\n\nAdditionally, thanks for your suggestion, and we further impose the ablation study on the component number of GMM introduced in BayesPrompt as Table 1 above. The bolded entries in Table 1 correspond to the number of GMM components we selected in the experiment, along with the results obtained. It can be observed that, compared to other settings, our choice achieved the best performance. This indicates that our selection of the number of components is appropriate. The corresponding extended ablation study and the analysis are added in Appendix B.7, Page 18, of the revised manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225987921,
                "cdate": 1700225987921,
                "tmdate": 1700225987921,
                "mdate": 1700225987921,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uj3JKATei3",
                "forum": "DmD1wboID9",
                "replyto": "wcxmXUvAaK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
                ],
                "content": {
                    "title": {
                        "value": "Good adjustment."
                    },
                    "comment": {
                        "value": "Good to see the main algorithm is pulled back in the main text. I still think a wealth of info is push into appendices, but I understand the page limitations."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700228798198,
                "cdate": 1700228798198,
                "tmdate": 1700228798198,
                "mdate": 1700228798198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "StrS4zDRPg",
                "forum": "DmD1wboID9",
                "replyto": "sVxlR4q8Z2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
                ],
                "content": {
                    "title": {
                        "value": "Extended ablations"
                    },
                    "comment": {
                        "value": "Good to see the extended ablations, even though little focus on ablation studies remains to be the weakest point of the paper in my eyes. Thanks for the effort."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700229125357,
                "cdate": 1700229125357,
                "tmdate": 1700229125357,
                "mdate": 1700229125357,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6FjNfi2fY2",
                "forum": "DmD1wboID9",
                "replyto": "JXKPMbV4ft",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up to Responses"
                    },
                    "comment": {
                        "value": "Thanks again for your understanding and your positive evaluation, and we cherish your constructive suggestions for helping us to improve our manuscript. We will continue to explore ways to improve the organization of our paper.\n\nFor the ablation study, we will still explore to perform further ablation study and the corresponding analyses. Due to the page limitation, the most important ablation studies can be contained in the main paper, and the extended analyses will appear in the Appendix. We agree with the reviewer that the comprehensive ablation study can helps the readers to understand our works and inspire our readers."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700291770917,
                "cdate": 1700291770917,
                "tmdate": 1700292111609,
                "mdate": 1700292111609,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5xstKuZkfy",
                "forum": "DmD1wboID9",
                "replyto": "qqwOe0pAdd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_ox7r"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thanks for the updated draft."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565781877,
                "cdate": 1700565781877,
                "tmdate": 1700565781877,
                "mdate": 1700565781877,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KjsRhanczu",
            "forum": "DmD1wboID9",
            "replyto": "DmD1wboID9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4365/Reviewer_JjWU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4365/Reviewer_JjWU"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a prompting method named BayesPrompt to generate prompts for PLMs. The authors argue that the over-multitudinous knowledge implicit in PLMs can hinder the performance of prompt-tuning methods in few-shot settings. Thus, BayesPrompt aims to approximate the unbiased target distribution to generate discriminative prompt for specific domains.\nExperimental results show the effectiveness of BayesPrompt on relation extraction (RE) tasks. Also, the authors provide theoretical analysis over BayesPrompt on lowering the classification error upper bound."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The task that improves the generalization capabilities of PLMs is challenge in the prompt tuning community. The authors provide a new view from the \"mislocated knowledge distributions\" between PLMs and target domain, which is interesting. \n\n2) The motivation that adopts the Bayesian approaches to model dataset-specific information and performing prompting on the latent space is novel.\n\n3) The provided theoretical analyses and extensive experiments help readers to understand the method."
                },
                "weaknesses": {
                    "value": "1) As can be seen from Tables 1 and 3, the proposed BayesPrompt presents a completely different improvement. Can the authors provide a detailed explanation?\n\n2) Please provide more discussion about the ablation results at Figure 4(c)."
                },
                "questions": {
                    "value": "BayesPrompt's training complexity is higher than its baseline, is there any potential for optimization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4365/Reviewer_JjWU"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826624941,
            "cdate": 1698826624941,
            "tmdate": 1699636408665,
            "mdate": 1699636408665,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6rwDjkkQWv",
                "forum": "DmD1wboID9",
                "replyto": "KjsRhanczu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparative Analysis Between Table 2 and Table 3, Discussion About the Ablation Results in Figure 4(c), Optimization Potential for Training Complexity (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer JjWU for the valuable comments and constructive suggestions. We are encouraged that the reviewer found that this work is novel and technically sound, the theoretical analysis is sufficient, and the presentation is good. The mentioned issues are addressed as follows:\n\n**W1: As can be seen from Tables 2 and 3, the proposed BayesPrompt presents a completely different improvement. Can the authors provide a detailed explanation?**\n\n**A:** Thanks for the review. By comparing Table 2 and Table 3, it can be observed that the performance improvement of BayesPrompt in few-shot scenarios widely surpasses its performance improvement in the standard scenario, i.e., with the full dataset. This observation precisely validates our motivation and the effectiveness of the method. \n\nAs we discussed in the Abstract and Introduction sections on Page 1 and Page 2, of the submitted manuscript, the limited and discrete semantic information contained in the training samples from downstream domains can barely support the conventional trainable prompts to acquire sufficient supervision, such that the guidance of the generated prompts is trivial to PLMs. Especially, such a challenge further exacerbates the performance of PLMs in **few-shot** scenarios. With this motivation, the proposed BayesPrompt aims to learn a prompt that contains relatively sufficient discriminative knowledge for the target downstream domain, which is achieved by proposing the debiased domain abstraction and then generating the prompt. Thus, the empirical observation, i.e., the performance improvement derived by introducing BayesPrompt on few-shot scenarios is more significant than that on standard scenarios, jointly proves the motivation and the effectiveness of the proposed BayesPrompt.\n\nSpecifically, in few-shot scenarios, the limited amount of data may hinder PLMs from fully learning the distribution and features of the downstream task data, leading to relatively poorer performance. However, when utilizing prompts obtained from BayesPrompt, which contain domain discriminative information, to guide PLMs in locating knowledge domains relevant to the downstream domain, the deficiencies caused by the limited data are noticeably mitigated.\n\nIn the standard scenario, where the amount of downstream task data is relatively sufficient, PLMs can better grasp the features and distribution of the downstream task data by learning from a more comprehensive dataset. Consequently, the performance improvement brought by BayesPrompt may not be as pronounced in this context.\n\nConcretely, as our intuition and motivation in the Abstract and Introduction, the proposed BayesPrompt mainly aims to improve the inference performance of PLMs in few-shot scenarios. While the results in standard scenarios further demonstrate the generalizability of our exploration and the effectiveness of BayesPrompt.\n\n**W2: Please provide more discussion about the ablation results at Figure 4(c).**\n\n**A:** Thanks for the review. The role of the type prompt word is to inject knowledge related to entity information into the prompt. Figure 4(c) illustrates the impact of the type prompt word on task performance in the full dataset. It can be seen that the performance improvement brought by the type prompt word in the full dataset is significantly weaker than its performance enhancement in the few-shot scenario (Figure 4(b)). We attribute this difference to the fact that, in the full dataset, the relatively abundant downstream task data provides a rich knowledge background, resulting in a substantial reduction in the deviation between the knowledge domain located by PLMs and the real downstream knowledge domain. This, in turn, weakens the de-biasing effect of the type prompt word on the few-shot dataset. Therefore, the performance improvement brought by the type prompt word is not significant in the full dataset."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225674594,
                "cdate": 1700225674594,
                "tmdate": 1700225674594,
                "mdate": 1700225674594,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N96vtx9deV",
                "forum": "DmD1wboID9",
                "replyto": "KjsRhanczu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparative Analysis Between Table 2 and Table 3, Discussion About the Ablation Results in Figure 4(c), Optimization Potential for Training Complexity (2/2)"
                    },
                    "comment": {
                        "value": "**Q1: BayesPrompt's training complexity is higher than its baseline, is there any potential for optimization?**\n\n**A:** Thanks for the review. The time complexity introduced by BayesPrompt primarily arises from the necessity of adding prompts containing domain discriminative information for downstream tasks during each prediction to provide de-ambiguous guidance for PLMs. In experiments, several optimization strategies can be considered to reduce the time complexity, such as sampling or subsampling datasets, employing feature selection or dimensionality reduction techniques to decrease the input feature dimensions, or implementing a training strategy with a warm-up. We will further explore the optimization strategy for BayesPrompt in the following works.\n\nAdditionally, the time complexity of BayesPrompt is only slightly higher than the baseline. For instance, in the 1-shot setting of the SemEval dataset, the training time cost of each epoch at the main baseline is 16 seconds, while the training time cost of each epoch at BayesPrompt is 27 seconds. Nevertheless, the main baseline achieves an F1 score of 28.6%, whereas BayesPrompt achieves an F1 score of 35.1%. Compared to the improvement of BayesPrompt on the F1 score, the time complexity brought by Bayesprompt is acceptable."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225747769,
                "cdate": 1700225747769,
                "tmdate": 1700225747769,
                "mdate": 1700225747769,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fxp43VApdp",
                "forum": "DmD1wboID9",
                "replyto": "N96vtx9deV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_JjWU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Reviewer_JjWU"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thanks for the author's reply, it solved my problem and I will keep my rating the same."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643986680,
                "cdate": 1700643986680,
                "tmdate": 1700643986680,
                "mdate": 1700643986680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Mxd6Sfdlo",
            "forum": "DmD1wboID9",
            "replyto": "DmD1wboID9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4365/Reviewer_32zy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4365/Reviewer_32zy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes BayesPrompt  to inject the semantic knowledge about the label into the label prompt to adjust the knowledge learned from pretraining to better fit downstream tasks. The method is to learn prompts that contain the domain discriminative information for the interference from the domain-irrelevant knowledge by approximating the factual distributions of downstream domains. The approach learns a representative model that injects the latent knowledge contained in labels into the prompt construction, thereby empowering the inference of relations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper works on a very interesting problem to adjust pretraining knowledge of LLM to downstream tasks. The paper provides theoretical analyses demonstrates that BayesPrompt can tighten\nthe upper bound of the classification error on the downstream inference of PLMs. Table 2 provide standard deviations over multiple runs."
                },
                "weaknesses": {
                    "value": "The paper may benefit a lot from better writing, including more clear presentation of the motivation and methods. \n- what does author refer to for \"unabridged Domain\", \"partial domain\", in figure 2? \n\n\"Thee over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. \n\"\n- what does the author refer as \"over-multitudinous conceptual knowledge\" and \"the abridged knowledge \"?\n\nIt is not fully convinced to the reviewer that the problem motivates the method can be solved by the method proposed. \nIt is unclear that how by \"leveraging Gaussian mixture distribution BayesPrompt is able to approximate the debiased factual distributions of downstream domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs\". Why the proposed approach can better approximate the downstream tasks distribution? by injecting label -related information? What is the bias referred here? Is there any produces to reduce the bias? The author may refer the bias as \"irrelevant pretraining knowledge\" that is confounding for the downstream tasks ? not very clear why introducing \"Gaussian mixture distribution\" can help solve the problem? is it for sampling and easy injecting label-related knowledge? \n\nby injecting label dependent knowledge, the PLM may learn a PLM distribution that is useful for the downstream task, which makes sense. but is it unfair, as BayesPrompt already uses label information but other methods don't?\n \nIt is not very clear how Figure 2 motivates the paper.  Figure 1 (domain knowledge is helpful ) and Figure 2 (domain knowledge may lead to negative impact?) seem not to align well. \n\nMethod section: how does  label prompt word lp and type prompt word tp fit in eq(6)? Can the author also bring some clarify to training? \n \nWhy does the approach focus on relation extraction tasks (used in method section)? how about other tasks? Is this method currently specific for relation extraction tasks? \n\nTable 2: the improvement seems not to exceed one standard deviation over other baselines. The category of tasks seem limited. not very convinced on the effectiveness of the methods."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699331014627,
            "cdate": 1699331014627,
            "tmdate": 1699636408593,
            "mdate": 1699636408593,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BjyBtjLXCy",
                "forum": "DmD1wboID9",
                "replyto": "1Mxd6Sfdlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications of the Behavior and Terms in BayesPrompt, Reasons for Introducing GMM, Explanations about the Figure 1 and Figure 2, Added Experiments and Analysis of the Generalization and Effectiveness (1/3)"
                    },
                    "comment": {
                        "value": "We thank Reviewer 32zy for the valuable feedback. We are encouraged that the reviewer found this work to be novel and that the theoretical proof is integrated. The issues mentioned are addressed as follows:\n\n**Q1: What does the author refer to for \u201cunabridged domain\u201d and \u201cpartial domain\u201d, in Figure 2?**\n\n**A:** \u201c**Unabridged**\u201d means \u201c**complete**\u201d and \u201c**partial**\u201d means \u201c**incomplete**\u201d. In Figure 2, the \u201c**unabridged domain**\u201d represents the complete knowledge domain corresponding to the knowledge required for the downstream task, approximately covering all the semantic knowledge needed in the downstream domain. \u201c**Partial domain**\u201d represents the partial knowledge domain corresponding to the knowledge required for the downstream task, which only covers a part of the knowledge required for the downstream domain. Thanks for this careful review, and to improve the understandability of our work, we have changed \u201c**unabridged**\u201d into \u201c**complete**\u201d, e.g., \u201c**unabridged domain**\u201d is replaced with \u201c**complete domain**\u201d, and \u201c**unabridged knowledge**\u201d is replaced with \u201c**complete knowledge**\u201d, and we further change \u201c**abridged**\u201d and \u201c**partial**\u201d into \u201c**incomplete**\u201d, e.g., \u201c**abridged domain**\u201d and \u201c**partial domain**\u201d are replaced with \u201c**incomplete domain**\u201d, and \u201c**abridged knowledge**\u201d and \u201c**partial knowledge**\u201d are replaced with \u201c**incomplete knowledge**\u201d, in the submitted revised manuscript.\n\n**Q2: What does the author refer to as \u201cover-multitudinous conceptual knowledge\u201d and \u201cthe abridged knowledge \u201d?**\n\n**A:** \u201c**Over-multitudinous conceptual knowledge**\u201d refers to the knowledge, with the inherent polysemy, contained by PLMs, as exemplified by the word \u201c**bat**\u201d, which can not only denote a flying mammal but also represent a stick used in sports to strike a ball. \u201c**Abridged knowledge**\u201d is synonymous with \u201c**partial domain**\u201d, indicating a partial knowledge domain that corresponds to the knowledge required for downstream tasks. The polysemy of knowledge and the incompleteness of knowledge domains can lead to the incorrect behavior of PLMs during locating the knowledge related to downstream tasks, thereby negatively impacting the inference performance of PLMs. Thanks for the review, to improve the understandability of our work, as the discussion in A to Q1, we have imposed the corresponding revision in the submitted revised manuscript.\n\n**Q3: It is unclear that how by \u201cleveraging Gaussian mixture distribution BayesPrompt is able to approximate the debiased factual distributions of downstream domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for PLMs\u201d. Why the proposed approach can better approximate the downstream tasks distribution? by injecting label-related information?**\n\n**Q5: Not very clear why introducing \u201cGaussian mixture distribution\u201d can help solve the problem? Is it for sampling and easy injecting label-related knowledge?**\n\n**A:** Thanks for the review. The proposed approach aims to learn prompts that contain the domain discriminative information against interference from the domain-irrelevant knowledge, which is achieved by approximating the **debiased factual distributions of downstream domains**. Based on the fact that the Gaussian mixture model can theoretically fit any probability density distribution and the fact that due to the limited sample size in the target downstream domain, the factual distribution of the target downstream domain **does NOT** fit the conventional Gaussian distribution well, directly adopting the Gaussian distribution-based approach, e.g., conventional VAE, may learn biased distribution of the target downstream domain, such that we use the Gaussian mixture distribution to approximate the debiased factual distribution of the downstream domain within the knowledge space of PLMs. Subsequently, we uniformly sample knowledge from the obtained approximated distribution, and the sampled results are used to form the ultimate \u201c**prompt**\u201d, which is further adopted in the inference of PLMs. In downstream tasks, this \u201c**prompt**\u201d is employed to guide PLMs in locating the knowledge domain relevant to the target downstream domain."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225118888,
                "cdate": 1700225118888,
                "tmdate": 1700225291241,
                "mdate": 1700225291241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PDhBNEGrU2",
                "forum": "DmD1wboID9",
                "replyto": "1Mxd6Sfdlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications of the Behavior and Terms in BayesPrompt, Reasons for Introducing GMM, Explanations about the Figure 1 and Figure 2, Added Experiments and Analysis of the Generalization and Effectiveness (2/3)"
                    },
                    "comment": {
                        "value": "**Q4: What is the bias referred here? Is there any produces to reduce the bias? The author may refer the bias as \u201cirrelevant pretraining knowledge\u201d that is confounding for the downstream tasks ?**\n\n**A:** Thanks for the review. The \u201c**bias**\u201d refers to that, without given appropriate prompts, PLMs exhibit a bias in locating the knowledge domain relevant to downstream tasks, deviating from the actual knowledge domain corresponding to the downstream tasks. This bias introduces irrelevant pretraining knowledge, that is confounding for the inference of PLMs on the downstream tasks. As the discussion in A to Q3 and Q5, we attempt to leverage the Gaussian mixture distribution as the known distributions to approximate the debiased factual distributions of the target downstream domains, thereby generating the prompts to mitigate the bias of PLMs.\n\n**Q6: By injecting label dependent knowledge, the PLM may learn a PLM distribution that is useful for the downstream task, which makes sense. but is it unfair, as BayesPrompt already uses label information but other methods don't?**\n\n**A:** Thanks for the review. The benchmark prompt learning methods, e.g., PTR, KnowPrompt, and RetrievalRE, consistently follow the same benchmark, that is, using the available labeled training set of the target downstream task, which presents that our approach follows the benchmark experimental setting and avoids the unfair comparisons. Specifically, all the methods, compared with BayesPrompt, use the labeled training set of the target downstream dataset to tune the prompt generation network. Moreover, PTR applies logic rules to encode prior knowledge about tasks and classes into prompt tuning, which requires the label information. KnowPrompt injects latent knowledge contained in labels into prompt construction and synergistically optimizes their representation with structured constraints. RetrievalRE is the follow-up work of KnowPrompt, and it constructs an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. Additionally, BayesPrompt and the associated benchmark methods, e.g., PTR, KnowPrompt, and RetrievalRE, **do NOT** require extra data besides the available information on the target downstream dataset, which is detailed in Table 3, Page 9, of the submitted revised manuscript. Concretely, the proposed BayesPrompt follows the benchmark experimental setting to use the available labeled training set of downstream tasks and avoids unfair comparisons.\n\n**Q7: It is not very clear how Figure 2 motivates the paper. Figure 1 (domain knowledge is helpful ) and Figure 2 (domain knowledge may lead to negative impact?) seem not to align well.**\n\n**A:** Thanks for the review. In Figure 1, the domain knowledge is **complete**, covering all the knowledge required for the target downstream domain in PLMs, which aims to introduce the motivation of the effectiveness of prompts containing the complete discriminative knowledge of the target downstream domain.\n\nIn Figure 2(a) and Figure 2(b), the domain knowledge is **incomplete**, e.g., only containing a single sample, or only containing the insufficient sample of incomplete knowledge. The deviation, between the complete knowledge and the incomplete knowledge of the target downstream domain, can lead to the knowledge ambiguity of PLMs. In Figure 2(c), the domain knowledge is **approximately complete**, effectively reducing the negative impact of knowledge bias. This exploration inspires us to approximate the debiased factual distributions of the target downstream domain, that contains sufficient discriminative knowledge of the downstream task.\n\nConcretely, the exploration experiments in Figure 1 and Figure 2 jointly motivate us to propose a method to approximate the debiased factual distributions of the target downstream domain and then generate the prompts containing sufficient discriminative knowledge of the downstream task."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225186012,
                "cdate": 1700225186012,
                "tmdate": 1700225186012,
                "mdate": 1700225186012,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qUJiKAbdpq",
                "forum": "DmD1wboID9",
                "replyto": "1Mxd6Sfdlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications of the Behavior and Terms in BayesPrompt, Reasons for Introducing GMM, Explanations about the Figure 1 and Figure 2, Added Experiments and Analysis of the Generalization and Effectiveness (3/3)"
                    },
                    "comment": {
                        "value": "**Q8: how does label prompt word lp and type prompt word tp fit in eq(6)? Can the author also bring some clarify to training?**\n\n**A:** Thanks for the review. A typical prompt consists of a template and a set of label words. The label prompt word $l_p$ and type prompt word $t_p$ are used to inject relational semantic knowledge and entity type knowledge into the prompt to predict the \u201c**[Mask]**\u201d, corresponding to computing $\\mathcal{P}\\left ( \\left [MASK  \\right ]=M\\left (y \\right ) \\mid T\\left ( x \\right )   \\right )$ in Equation (6).\n\nDuring the training process, the label prompt word $l_p$ is utilized to initialize the relation label corresponding to the \u201c**[Mask]**\u201d, while the type prompt word $t_p$ is employed to inject entity knowledge into the template $T\\left ( x \\right )$. By computing $\\mathcal{P}\\left ( \\left [MASK  \\right ]=M\\left (y \\right ) \\mid T\\left ( x \\right )   \\right )$ in Equation (6), we can predict the relation label corresponding to the \u201c**[Mask]**\u201d. To fully associate the initialized label prompt word $l_p$ and type prompt word $t_p$ with the surrounding context, we conduct further optimization of their representations using a loss function. The loss function is computed as the cross-entropy between $y$ and $\\mathcal{P}\\left ( y\\mid x \\right )$, as shown in Equation (6). For the details of training, we provide a pseudo-code in Algorithm 1, Page 6, of the submitted revised manuscript, and the implementation details of BayesPrompt are provided in Appendix C, Page 18, of the submitted revised manuscript.\n\n**Q9: Why does the approach focus on relation extraction tasks (used in method section)? how about other tasks? Is this method currently specific for relation extraction tasks?**\n\n**A:** Thanks for the review. Relation extraction aims to extract structured knowledge from unstructured text and plays a critical role in information extraction and knowledge base construction. Therefore, we choose the relation extraction task to verify the effectiveness of the proposed method, but this does not mean that our method is limited to relation extraction tasks. Theoretically, our method applies to all problems that involve \u201c**sub-knowledge domains**\u201d, such as text classification, entity recognition, entity disambiguation, etc.\n\nThus, we further conducted **text classification** experiments on a specific automotive industry dataset using a pre-trained language model, BERT, with a parameter count of 110 million. The obtained classification accuracy was 74.0%. We also employed TextRCNN to train the classification model with a parameter count of 15 million. The resulting classification accuracy reached 95.3%, significantly outperforming the pre-trained language model BERT, which is due to the over-multitudinous conceptual knowledge learned by BERT. However, when using BayesPrompt, it achieves the best classification accuracy of 97.4%. Note that our method does not require training (fine-tuning) the network of BERT. Therefore, our method can be generalized into various tasks and obtain performance improvements to baselines. The corresponding experiments can be found in Table 5, Page 16, Appendix B.3, of the submitted manuscript.\n\n**Q10: The improvement seems not to exceed one standard deviation over other baselines. The category of tasks seem limited. not very convinced on the effectiveness of the methods.**\n\n**A:** Thanks for the review. In this paper, we choose KnowPrompt as our main baseline. RetrievalRE is a subsequent work based on KnowPrompt. Compared with KnowPrompt, RetrievalRE not only infers relations through knowledge stored in the weights during training but also assists decision-making by unwinding and querying examples in the open-book datastore, thereby ensuring its performance is more stable. From the perspective of standard deviation, BayesPrompt is significantly superior to the main baseline, KnowPrompt. Moreover, when compared to RetrievalRE, the difference between them is not substantial. In fact, on the TACRED dataset, BayesPrompt exhibits even greater stability, with an average standard deviation of 0.17 lower than that of RetrievalRE. Additionally, we perform the significance test, i.e., **t-test**, with the main baseline, and observe that the P values are consistently lower than 0.05, e.g., 0.045 on the SemEval dataset, indicating that the improvement of BayesPrompt is significant.\n\nFor the generalized applicability of BayesPrompt to various tasks, e.g., the text classification experiment, please refer to A to Q9 as above. The results jointly prove the generalizability and effectiveness of the proposed BayesPrompt."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700225353276,
                "cdate": 1700225353276,
                "tmdate": 1700225353276,
                "mdate": 1700225353276,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]