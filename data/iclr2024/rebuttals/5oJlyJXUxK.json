[
    {
        "title": "Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?"
    },
    {
        "review": {
            "id": "XEgvCMYWcc",
            "forum": "5oJlyJXUxK",
            "replyto": "5oJlyJXUxK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5402/Reviewer_Emzi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5402/Reviewer_Emzi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to make conceptual interventions by modifying the activations of a pre-trained black-box model. Based on the popular counterfactual optimization approach by Wachter et al [1] for pixel-based counterfactuals, they propose to instead optimize for activations. That is, they apply the distance loss in the activation instead of pixel space and use the gradients obtained by an additionally trained concept detector ((non)-linear probe) instead of the gradients of the classifier (c.f., Eq. 1). Finally, the authors propose a fine-tuning scheme to make the classifier more reliant on the \u201cconcept activation vectors\u201d based on a proposed notion of intervenability (Eq. 2 & 3), while keeping the feature extractor frozen. Through experiments on tabular and vision data the authors show the efficacy of their intervention as well as fine-tuning scheme.\n\n[1] Wachter, Sandra, et al. \"Counterfactual explanations without opening the black box: Automated decisions and the GDPR.\" Harv. JL & Tech. 31 2017: 841."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The work addresses an interesting problem to better understand the behavior of models through conceptual interventions.\n* The intervention strategy is simple yet effective.\n* The fine-tuning scheme is well-designed and yet simple.\n* Code is provided via an anonymized repository and supplementary material."
                },
                "weaknesses": {
                    "value": "* The work seems to have missed the most relevant work on conceptual (interventional) counterfactuals, e.g., [1,2]. While there are some technical differences (usage of the gradients stemming from the linear probe instead of the classifier in the counterfactual optimization problem), there is still significant overlap. For example, Abid et al. [1] also use linear probes to identify concept activation vectors and use it to intervene on the features.\n\n* The work motivates their approach by stating that \u201cconcept labels are not required [during training]\u201d (p. 2). While true, it is still required for the fine-tuning (as we need to train the probes initially; Sec. 3.3, and shown to be important in the experiments), which one could also see as part of model development. For the case that we would like to make the same conceptual interventions as for CBMs, this would result in a similar amount of annotation cost.\n\n* The paper makes the (implicit) assumption that the feature extractor $h_{\\phi}$ learns the concept; both in their intervention approach and fine-tuning (since $\\beta$ is set to 1). However, the authors provide no evidence that this is actually the case. The linear probes could just learn to predict from some correlated concept/feature. Further evidence would be required that the black-box feature extractor has actually learned the concept that is intervened on.\n\n* It is very unclear whether the intervention strategy actually results in *plausible* and not just *adversarial* changes of the activations. This is a prominent problem for pixel-spaced counterfactuals methods, where, e.g., different types of regularization or generative models are used to obtain plausible counterfactuals.\n\n* There are no comparisons to prior work that converted pretrained models into CBMs [3,4]. It would be interesting to show how the proposed method compares to them (using the proposed intervention strategy).\n\n[1] Abid, Abubakar et al. \"Meaningfully explaining model mistakes using conceptual counterfactuals.\" ICML 2022.\n\n[2] Kim, Siwon, et al. \"Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space.\" CVPR 2023.\n\n[3] Yuksekgonul, Mert et al. \"Post-hoc concept bottleneck models.\" ICLR 2023.\n\n[4] Oikarinen, Tuomas, et al. \"Label-Free Concept Bottleneck Models.\" ICLR 2023."
                },
                "questions": {
                    "value": "* Why does Eq. 2 & 3 assume that c\u2019 does not change y to y\u2019? As is, it assumes that the concept does not change the class. Let\u2019s say the concept c\u2019 changes the fur texture of a cat, then the resulting class may also change. This seems not to be included in the current notion of intervenability and may be easily obtained by a suitable choice of the distribution $\\pi(c\u2019,y\u2019|x,\\hat{c},c,\\hat{y},y)$, which subsumes $\\pi(c\u2019|x,\\hat{c},c,\\hat{y},y)$ when $y\u2019=y$.\n\n* Why is the ResNet-18 architecture used with four fully-connected layers instead of the standard setting? Why are not just the bottleneck features used?\n\n* What happens for the baseline (fine-tuned, MT) if we interleave intervened activations $z\u2019$ also during training? As is, it just may be that (fine-tuned, I) is more robust to the interventions.\n\n* Are AUROC and AUPR computed for the concepts or targets/classes? This may also change for the different experimental results (e.g., for the figures in the main paper this is unclear). Could the authors clarify this?\n\n## Suggestions\n\n* It\u2019d have been good to discuss the data-generating mechanisms (bottleneck, confounder, incomplete) in the main text and not only supplemental.\n\n* Given the overlapping confidence bands in Fig. 3(c) it would be good to either run more simulations or reformulate the sentence since it is unclear if \u201cblack-box classifiers are expectedly superior to the CBM\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5402/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5402/Reviewer_Emzi",
                        "ICLR.cc/2024/Conference/Submission5402/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697982196422,
            "cdate": 1697982196422,
            "tmdate": 1700647330252,
            "mdate": 1700647330252,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8EeqRZm5j8",
                "forum": "5oJlyJXUxK",
                "replyto": "XEgvCMYWcc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-by-point Response to Reviewer Emzi (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough feedback! Below are our point-by-point responses to the concerns raised.\n\n> The work seems to have missed the most relevant work on conceptual (interventional) counterfactuals, e.g., [1,2]. While there are some technical differences (usage of the gradients stemming from the linear probe instead of the classifier in the counterfactual optimization problem), there is still significant overlap. For example, Abid et al. [1] also use linear probes to identify concept activation vectors and use it to intervene on the features.\n\nWe thank the reviewer for pointing us to this line of work! In the revised manuscript, we have included references to [[1]](https://proceedings.mlr.press/v162/abid22a.html) and [[2]](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Grounding_Counterfactual_Explanation_of_Image_Classifiers_to_Textual_Concept_Space_CVPR_2023_paper.html). We have also provided a detailed discussion of the differences between ours and these related works in Appendix B. Below, we provide a general summary.\n\nWhile some of the technical tricks used by our work and conceptual counterfactual explanations (CCE) [1, 2] are similar, the problem setting and the actual purpose are quite different. Informally, our intervention addresses the following question: \u201c*How do the network\u2019s representations need to be perturbed to reflect the user-input concepts $\\boldsymbol{c}\u2019$?*\u201d On the other hand, CCEs try to tackle the following question: \u201c*Which concepts need to be input for perturbing the network\u2019s representations to flip the predicted label $\\hat{y}$ to the given $y\u2019$?*\u201d. In summary, CCEs try to identify concept variables responsible for a misclassification, whereas interventions try to improve a prediction at test time based on the input concepts.\n\nIn our method, interventions are intended for the interaction between the user and the model so that the user can reduce the model\u2019s error via understandable concept variables. By contrast, CCEs consider a typical counterfactual explanation scenario, trying to identify which concepts can lead to sparse and in-distribution changes in the representation that would flip the classification outcome.\n\nBeyond the main difference above, our work makes a few other technically valuable contributions. Namely, we formalise the intervenability measure and intervention strategies and propose explicit fine-tuning for intervenability. In summary, our work tackles a different problem complementary to the direction explored by [1] or [2].\n\n> The work motivates their approach by stating that \u201cconcept labels are not required [during training]\u201d (p. 2). While true, it is still required for the fine-tuning (as we need to train the probes initially; Sec. 3.3, and shown to be important in the experiments), which one could also see as part of model development. For the case that we would like to make the same conceptual interventions as for CBMs, this would result in a similar amount of annotation cost.\n\nFine-tuning indeed requires concept labels. We would like to emphasise that all the experiments were performed on *validation* sets, which were considerably smaller than the training sets (utilised by CBMs). Moreover, labelling costs are not the only concern when using CBMs. In particular, concept variables might be unknown when training a model and only be incorporated when deploying the model for a specific application in some institution (e.g. hospital). For instance, such challenges may arise when utilising foundation models [[5]](https://doi.org/10.1038/s41586-023-05881-4).   \n\n> The paper makes the (implicit) assumption that the feature extractor $h_{\\phi}$ learns the concept; both in their intervention approach and fine-tuning (since $\\beta$ is set to 1). However, the authors provide no evidence that this is actually the case. The linear probes could just learn to predict from some correlated concept/feature. Further evidence would be required that the black-box feature extractor has actually learned the concept that is intervened on.\n\nFor interventions and fine-tuning to be effective, the assumption outlined by the reviewer needs to hold. This assumption was explicitly mentioned in the paragraph \u201cShould All Models Be Intervenable?\u201d in Section 3.2 of the initially submitted manuscript (this paragraph has been moved to Appendix B in the revised version). \n\nIn our view, if the interventions improve the predictive performance (before or after fine-tuning), this implies that the representations $\\boldsymbol{z}$ are informative of the concept variables and, hence, interventions can be performed somewhat effectively. We do not deem it necessary that representations *identify* concept variables or are *disentangled* w.r.t. the concepts."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216131505,
                "cdate": 1700216131505,
                "tmdate": 1700216131505,
                "mdate": 1700216131505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aXFxWE8Fc9",
                "forum": "5oJlyJXUxK",
                "replyto": "5PEqp59Zqc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_Emzi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_Emzi"
                ],
                "content": {
                    "title": {
                        "value": "Re: Point-by-point Response to Reviewer Emzi"
                    },
                    "comment": {
                        "value": "I thank the authors for the thorough reply. I acknowledge the different problem settings of conceptual counterfactuals and the present work\u2019s setting, and appreciate the discussion in Appendix B.\n> Fine-tuning indeed requires concept labels.\n\nIt\u2019d be good if the authors could more clearly state this in their work and rephrase sentences such as \u201cour method circumvents the need for concept labels during training\u201d (p. 8).\n\n> For interventions and fine-tuning to be effective, the assumption [that the feature extractor has already learned the concept, \u2026] needs to hold.\n\nI believe this is an important question that requires further experimental evaluation, as the goal is to obtain an interpretable model in the sense that intervened concepts have the intended behavior, as also thoroughly discussed in the paragraph \u201cShould All Models Be Intervenable?\u201d in Appendix B. The need for fine-tuning may hint that this is not the case.\n\n> We do not deem it necessary that representations identify concept variables or are disentangled w.r.t. the concepts.\n\nCould the authors extend what they mean with this?\n\n> If we were to interleave intervened representations when training the Fine-tuned MT baseline, this trick would make the baseline very similar to fine-tuning for intervenability\n\n$z\u2019$ may be out-of-distribution for $g_\\psi$ and could explain its worse performance. Thus, it\u2019d be interesting to consider this as a baseline."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558547459,
                "cdate": 1700558547459,
                "tmdate": 1700558547459,
                "mdate": 1700558547459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IPY28qTe9L",
                "forum": "5oJlyJXUxK",
                "replyto": "6DzzISlrge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_Emzi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_Emzi"
                ],
                "content": {
                    "title": {
                        "value": "Re: Follow-up to Reviewer Emzi"
                    },
                    "comment": {
                        "value": "I thank the authors again for their reply. I acknowledge the authors\u2019 effort to revise the manuscript and provide further clarifications. However, some of my concerns remain, including the assumption of a concept-extracting feature encoder and uncertainty whether the activation vector $z\u2019$ truly results in the intended intervention, considering that it could be just an out-of-distribution activation for both the probe $q_\\xi$ and classifier head $g_\\psi$, as evidenced by the need for fine-tuning. Nonetheless, I will change my overall score to 5."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647355567,
                "cdate": 1700647355567,
                "tmdate": 1700647355567,
                "mdate": 1700647355567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JuwhnKjY00",
            "forum": "5oJlyJXUxK",
            "replyto": "5oJlyJXUxK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5402/Reviewer_icNv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5402/Reviewer_icNv"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a method to perform concept-based interventions on pre-trained neural networks. They then formalize intervenability as a metric to measure concept-based interventions. Finally, they show that finetuning probes for intervenability can improve intervenability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper's proposed metric of intervenability is interesting and helps practically measure the utility of methods such as CBMs\n- The introduced methods are clear and intuitive\n- Strong comparisons are made to baselines from previous work"
                },
                "weaknesses": {
                    "value": "- Limited improvements over CBMs -- In most settings, CBMs seem to outperform the proposed Fine-tuned, I method while providing much greater interpretability (the main exception is in Fig 5)\n- Potential missing baseline: I am not sure if a simple convex combination of the predictions of the CBM and the black-box would outperform the proposed model"
                },
                "questions": {
                    "value": "- Note: would be nice for the paper to mention the link between intervenability and concept/feature importance - this should be straightforward based on interventions, as this is how many feature importance metrics (e.g. LIME/SHAP) are computed\n- Would be nice to describe the experimental setup in more detail, e.g. the three synthetic scenarios are only described in previous work\n- Minor: fig legends in Fig 4 are slightly difficult to read."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5402/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5402/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5402/Reviewer_icNv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698249645489,
            "cdate": 1698249645489,
            "tmdate": 1700591602625,
            "mdate": 1700591602625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CnqA9uSI6T",
                "forum": "5oJlyJXUxK",
                "replyto": "JuwhnKjY00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-by-point Response to Reviewer icNv"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback! Below are our point-by-point responses to the concerns raised.\n\n> Limited improvements over CBMs -- In most settings, CBMs seem to outperform the proposed Fine-tuned, I method while providing much greater interpretability (the main exception is in Fig 5).\n\nCBMs are indeed more intervenable than black-box models (even after fine-tuning) on *simple* benchmarks, such as synthetic bottleneck and confounder (where generative mechanism directly or closely matches the CBM). However, on the synthetic incomplete and chest X-ray datasets, where the set of concepts likely does not fully explain the relationship between $\\boldsymbol{x}$ and $y$, fine-tuned black boxes outperform CBMs. These results are not surprising, and we would argue that the proposed approach can be practical and useful for practitioners working with more complex datasets.\n\nFinally, there exist scenarios where training a CBM is impossible or impractical, e.g. when relying on a foundation model [[1]](https://doi.org/10.1038/s41586-023-05881-4) or when the concept variables are unknown or too expensive to label at the training time. Thus, we believe there is practical merit in the proposed techniques.\n\n> Potential missing baseline: I am not sure if a simple convex combination of the predictions of the CBM and the black-box would outperform the proposed model.\n\nWhile the suggested baseline might be practical, combining a CBM with a black-box model would lead to interpretations and interventions that are not faithful to the predictions made since the output of the CBM would be augmented with the black-box prediction that is not intervenable or explained in any way. In addition, as mentioned above, our work is motivated by application scenarios in which CBMs are not as practical.\n\n> Note: would be nice for the paper to mention the link between intervenability and concept/feature importance - this should be straightforward based on interventions, as this is how many feature importance metrics (e.g. LIME/SHAP) are computed.\n\nThank you for this suggestion! There indeed exists a relationship between intervenability and feature importance measures.\n\nDrawing a direct relationship with LIME or SHAP is somewhat challenging since these feature importance measures assess the expected change in the model\u2019s output rather than loss. On the other hand, intervenability is very reminiscent of the model reliance as formalised in [[2]](https://arxiv.org/abs/1801.01489) and explained informally in [[3]](https://christophm.github.io/interpretable-ml-book/feature-importance.html#theory-3), which is another family of feature importance measures. In particular, model reliance ($MR$) on feature $j$ is given by the ratio of the expected loss of the model $f$ when the feature $j$ is augmented with noise, rendering this feature uninformative of the target variable versus the expected loss under the original data distribution. Similarly, the intervenability (as defined in Equation 2) measures the *difference* in the expected losses. Suppose the distribution $\\pi$ (intervention strategy) is chosen to augment a single concept variable with the noise (as described in [[2]](https://arxiv.org/abs/1801.01489)). In that case, intervenability may be used to quantify the reliance of $g_{\\boldsymbol{\\psi}}$ on the individual concepts in $\\boldsymbol{\\hat{c}}$. We have added a comment on these observations in the revised manuscript in Appendix B.\n\n> Would be nice to describe the experimental setup in more detail, e.g. the three synthetic scenarios are only described in previous work.\n\nNote that the synthetic dataset and three different generative mechanisms were described in detail in the appendix of the submitted manuscript. To improve clarity, we have added a brief description of the three scenarios to the main text of the revised manuscript. We have also included a detailed explanation of the intervention strategies in Appendix D.\n\n### References\n\n[1] Moor, M., Banerjee, O., Abad, Z. S. H., Krumholz, H. M., Leskovec, J., Topol, E. J., & Rajpurkar, P. (2023). Foundation models for generalist medical artificial intelligence. *Nature, 616*(7956), 259-265.\n\n[2]  Fisher, A., Rudin, C., & Dominici, F. (2019). All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously. *Journal of Machine Learning Research, 20*(177), 1-81.\n\n[3] Molnar, C. (2020). *Interpretable machine learning*. Lulu. com."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216564126,
                "cdate": 1700216564126,
                "tmdate": 1700216564126,
                "mdate": 1700216564126,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xSoQKwNTxK",
                "forum": "5oJlyJXUxK",
                "replyto": "JuwhnKjY00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_icNv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_icNv"
                ],
                "content": {
                    "title": {
                        "value": "Raised my score to 8"
                    },
                    "comment": {
                        "value": "I have raised my score to 8 in response to the author's comments. It would still be nice to concretely see scenarios where CBMs fail and the proposed method shows a significant improvement, but the authors have done a decent job supporting that these scenarios exist."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591564136,
                "cdate": 1700591564136,
                "tmdate": 1700591629082,
                "mdate": 1700591629082,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4aiLRNX3sR",
            "forum": "5oJlyJXUxK",
            "replyto": "5oJlyJXUxK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5402/Reviewer_39mA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5402/Reviewer_39mA"
            ],
            "content": {
                "summary": {
                    "value": "- The paper proposes interventions on black-box models in CBM style.\n\n\n- Given a black-box model:\n     -  Train a probing network to extract concepts from an intermediate representation $z$\n     -  Now that they have the probing network for a given sample $x$ they can extract $z$ and concepts $c$, given the ground-truth concepts $c'$ they learn a new embedding $z'$ that should produce concepts $c'$ when given to probing network.\n     -  For interventions they replace the original $z$ with new $z'$\n\n- Given $z$ how do we calculate $z'$? (from the text is a bit unclear I had to look at the code to understand how this is done so please correct me if I am wrong)\n     -    Start from $z'==z$ but $z'$ is differentiable.\n     -    The probing network is frozen where you calculate concepts $q_\\xi(z')$\n     -    Update $z'$ based on the loss in equation 1 you repeat multiple times i.e for a few epochs.\n\n\n- The paper quantifies the effectiveness of interventions as the gap between the regular prediction loss and the loss attained after the intervention.\n- The paper proposes a fine-tuning strategy for intervention that can be summarized as follows:\n     -    Given a black-box model $f_\\theta$, we will look at the network as if its a cbm model such that we first have a network $h_\\phi(x)=z$ that gives us the intermediate representation used to train the probing network and $g_\\psi(z)=y$ that gives the final prediction.\n     -    The black box network is now trained end to end using loss in equation 4: the first term is regular black-box optimization the second term is optimizing the outer network subjected to the intervention and all of this is in addition to optimizing $z'$  as mentioned previously.\n     -    Equation 4 is then simplified to avoid trilevel optimizing the first loss is ignored so the feature extractor layer is basically frozen.\n-  Experiments:\n    -    The paper one synthetic tabular dataset, 3 image datasets.\n    -    The paper tested the proposed fine-tuning approach against a black-box NN a CBM and two other different fine-tuning approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Strength:\n- Originality: The paper is original, there has been work around post-hoc CBM (cited by the paper) but this intervention strategy is novel and very interesting.\n- Quality: The paper is evaluated against a reasonable baseline on multiple datasets.\n- Significance: The paper's contribution is significant, if we can intervene on model in test time we can get higher accuracy as shown on multiple datasets."
                },
                "weaknesses": {
                    "value": "Method weakness:\n- There is no guarantee that a network has learned the desired concepts, i.e. there is a big probability that the probing network can not learn a concept you would want to intervene on.\n- The method is quite expensive optimizing to get the $z'$ and optimizing for fine-tuning on top of $z'$ can be quite costly.\n- Creating a probing network per concept can be costly when we have a large number of concepts, it is not clear if this can scale to hundreds or thousands of concepts.\n- It is not clear which concepts one should intervene on.\n- What if we can never intervene during test time (say we don't have ground-truth concepts or even ground-truth label at that point) it is unclear how this can be useful in that case.\n\n\nPaper quality:\n- How you get $z'$ and the intervening strategy is not very clear in the main paper I would strongly recommend moving Algorithm A.1 to the main text for clarity."
                },
                "questions": {
                    "value": "- Which layer do you do probing on is it the last layer before the classifier?\n- Usually how many iterations do you need to extract a reasonable $z'$.\n- In the experiments, the accuracy on the test set is \"with\" interventions correct?\n- How do you select the concepts to intervene on?\n- How would this model be used practically? (I am assuming something similar to the following steps):\n   \n\n    - You have an example that is incorrect\n    - You calculate the concepts that affect that example.\n   - You show the concepts to a domain expert, and they propose different concepts.\n   - You calculate $z'$ using this new concept and make a new prediction.\n\nIf so how is the original model improved it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698528126488,
            "cdate": 1698528126488,
            "tmdate": 1699636547263,
            "mdate": 1699636547263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xBSPKq7NVX",
                "forum": "5oJlyJXUxK",
                "replyto": "4aiLRNX3sR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-by-point Response to Reviewer 39mA (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive evaluation of our work and detailed feedback! Below are our point-by-point responses to the concerns raised.\n\n> The paper proposes interventions on black-box models in CBM style.\n\nThank you for the neat and accurate summary of our work! We appreciate your effort.\n\n> There is no guarantee that a network has learned the desired concepts, i.e. there is a big probability that the probing network can not learn a concept you would want to intervene on.\n\nThis limitation is indeed one of the caveats of the proposed intervention recipe. Nevertheless, in the scenario described by the reviewer, the intervenability measure alongside fine-tuning can help practitioners understand that the black-box model is perhaps inadequate for the considered application. In that case, we should resort to (i) in addition, fine-tuning w.r.t. the parameters of the feature extractor $h_{\\boldsymbol{\\phi}}$ or (ii) using another black-box model.\n\n> The method is quite expensive optimizing to get the $z\u2019$ and optimizing for fine-tuning on top of $z\u2019$  can be quite costly.\n\nInterventions require additional optimisation; however, they can be performed on a batch of data points to speed up the inference. By contrast, the fine-tuning is run only once on the validation set (smaller than the training set), and at deployment, only the intervention routine needs to be executed.\n\n> Creating a probing network per concept can be costly when we have a large number of concepts, it is not clear if this can scale to hundreds or thousands of concepts.\n\nWe would like to emphasise that we use a *multivariate* probing function in all experiments (for both linear and nonlinear functions), i.e. the probe predicts concept variables in a multitask fashion. Thus, we expect no considerable additional scaling issues when dealing with a large number of concept variables. Please note that, for example, the AwA2 and CUB datasets have 85 and 112 concept variables, respectively, and we observed no scalability problems for these benchmarks.  Finally, having thousands of concept variables requires a significant mental effort on the user\u2019s side and, in that setting, the interpretability and utility of a concept-based approach are somewhat limited.\n\n> It is not clear which concepts one should intervene on.\n\nArguably, the expert user should decide which concepts to intervene on, e.g. consider a medical specialist paired up with an ML model predicting the patient\u2019s diagnosis. We represent this decision by the *intervention strategy* $\\pi\\left(\\boldsymbol{c}' \\vert \\boldsymbol{x}, \\boldsymbol{\\hat{c}}, \\boldsymbol{c}, \\hat{y}, y\\right)$. Note that the set of conditioning variables depends on the problem setting. In our experiments, we explore two simple strategies: (i) random-subset, where the set of concept variables is chosen uniformly at random and the values of the chosen variables are replaced with the ground truth, and (ii) uncertainty-based, where the variables are sampled with probabilities proportional to the uncertainty given by $1/\\left(\\left|\\hat{c}_j-0.5\\right|+\\varepsilon\\right)$, where $\\varepsilon>0$ is small. Generally, other strategies are viable and can be easily incorporated into our method and experiments, e.g. [[1]](https://arxiv.org/abs/2302.14260) proposes a few plausible options and conducts a thorough empirical evaluation.\n\nTo improve clarity, we have now included a pseudocode description of the considered intervention strategies in the appendix of the revised paper (Appendix D, Algorithms D.1 and D.2).\n\n> What if we can never intervene during test time (say we don't have ground-truth concepts or even ground-truth label at that point) it is unclear how this can be useful in that case.\n\nAs the intended use case of the proposed method is human interaction with a black-box ML model, neither ground-truth concept nor labels are strictly required since $\\boldsymbol{c}\u2019$ is supposed to be provided by the user (these could be either ground-truth, golden-standard, or values reflecting the user\u2019s informed guess). As mentioned, the set of conditioning variables for the strategy $\\pi$ may vary depending on the application scenario. In fact, none of the intervention strategies considered in our experiments rely on the ground-truth target label $y$.\n\nOn the other hand, if the user cannot provide golden-standard or ground-truth concept values, then a concept-based approach is not helpful for the problem at hand, and the user should resort to a black-box model without interventions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216691419,
                "cdate": 1700216691419,
                "tmdate": 1700216691419,
                "mdate": 1700216691419,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mu3ZfZeWBW",
                "forum": "5oJlyJXUxK",
                "replyto": "4aiLRNX3sR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_39mA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_39mA"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their detailed responses to my concerns and questions.\nI think this work is very interesting and important, and the paper is well-written.\nMy score remains as is."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496536424,
                "cdate": 1700496536424,
                "tmdate": 1700496551241,
                "mdate": 1700496551241,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EQZgTuE1kn",
            "forum": "5oJlyJXUxK",
            "replyto": "5oJlyJXUxK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5402/Reviewer_JHNX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5402/Reviewer_JHNX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a way to do concept interventions on standard neural networks without the need for a Concept Bottleneck Model. This can be done by learning classifiers to predict the presence of a concept based on the hidden layer representation, and then optimizing to find representation that is as close as possible to original but changes the concept classifier prediction. They also propose to improve the effectiveness of interventions by finetuning the model to get better results under intervention."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Clearly written. Interesting perspective highlighting that intervention could be useful even on standard networks."
                },
                "weaknesses": {
                    "value": "I don't really see any use cases where we would want to intervene on standard models instead of just creating a CBM. \n- Intervention performance on models that weren't finetuned is quite poor, and still requires labeled concept data to learn the classifiers.\n- The performance of models (even finetuned) is worse than CBM on most datasets, while both require additional training and the same kind of data with dense concept labels, most cases it would be better to just learn a CBM\n- Intervening now requires solving an optimization problem making it more costly and harder to understand than original CBM interventions\n- CBMs have many interpretability benefits in addition to intervene-ability, which we lose when using standard architecture, such as predictions being simple functions of interpretable concepts. \n\nLacking evaluation:\n- I think improved performance on CheXpert is likely caused by the fact that the model can use information outside of concepts to make the prediction. This is similar to having residual as is done by Posthoc-CBM-h, and I think some comparison agaisnt that would be needed.\n- Choice of datasets is a little odd, should use at least some of the datasets original CBM was trained on such as CUB"
                },
                "questions": {
                    "value": "Looks like each intervention requires running gradient descent to minimize eq. 1, what is the computational cost of this? \nHow did you intervene on multiple concepts at once?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5402/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817964801,
            "cdate": 1698817964801,
            "tmdate": 1699636547156,
            "mdate": 1699636547156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l87zwSGkpf",
                "forum": "5oJlyJXUxK",
                "replyto": "EQZgTuE1kn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point-by-point Response to Reviewer JHNX (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback! Below are our point-by-point responses to the concerns raised.\n\n> I don't really see any use cases where we would want to intervene on standard models instead of just creating a CBM.\n\nThere exist use cases when training a CBM is not practical or possible. Firstly, concepts might be unknown to ML practitioners during model development due to the lack of domain knowledge, or it might be too expensive to annotate the training dataset. Secondly, with increased interest in and use of foundation models [[1]](https://doi.org/10.1038/s41586-023-05881-4), training dedicated task-specific models becomes less practical and feasible. Hence, there is a need for techniques that allow users to incorporate concept knowledge into representations and interact with predictive models without redesigning and retraining the entire model. This work explores this direction and proposes a practical fine-tuning approach to improve the intervenablity. At the same, the representations could still be used for other downstream tasks, as they remain unaffected by fine-tuning.\n\n> Intervention performance on models that weren't finetuned is quite poor, and still requires labeled concept data to learn the classifiers.\n\nIt is true that in several datasets, the black-box models are less intervenable than CBMs. However, to the best of our knowledge, this work is the first to explore this research question, and even a somewhat negative finding in this regard is a valuable contribution to the literature, in our view. Finally, our fine-tuning approach mitigates this shortcoming of black-box models. \n\nRegarding the need for concept labels, we fully acknowledge that an annotated *validation set* is required (rather than an entire training set, as for CBMs). Additionally, our techniques are entirely compatible with the previous approaches for semiautomatic concept discovery, e.g. [[2](https://arxiv.org/abs/2205.15480), [3](https://arxiv.org/abs/2304.06129)]. Thus, in practice, the need for additional annotation can be alleviated by augmenting our methods with the ideas from the previous works. This research question is, however, beyond the scope of the current manuscript.\n\n> The performance of models (even finetuned) is worse than CBM on most datasets, while both require additional training and the same kind of data with dense concept labels, most cases it would be better to just learn a CBM.\n\nWhile it is true that fine-tuned models are less performant than CBMs on the synthetic data under the bottleneck and confounder mechanisms (and CUB, as observed in our additional experiments), there is a utility in fine-tuning models on the synthetic incomplete and chest X-ray datasets (and AwA, as observed in the updated experiments). The former scenarios are simplistic. In synthetic bottleneck and confounder, generative mechanisms directly or closely match the CBM. We expect CBMs to perform very well on those benchmarks. However, fine-tuned models are expected to perform better in more complex settings (synthetic incomplete and chest X-ray classification), where concept variables may only partially explain the relationship between $y$ and $\\boldsymbol{x}$. After fine-tuning for those datasets, interventions allow injecting concept information into the representation without removing additional non-concept-based knowledge utilised by the black box. \n\n> Intervening now requires solving an optimization problem making it more costly and harder to understand than original CBM interventions.\n\nIntervening on black boxes indeed requires running the optimisation procedure; however, interventions do not require much time if the representations in the chosen layer are not too high-dimensional and can be performed on batches of instances. Moreover, since we use a multivariate probing function, interventions can be performed on multiple concept variables simultaneously without incurring additional computational complexity. In addition, on the user's side, there is no additional mental effort in understanding the interventions: the user can inspect the probe\u2019s prediction of the concept variables and decide to intervene on a few of those, steering the model\u2019s final prediction. Thus, interventions essentially work analogically to CBMs.\n\n> CBMs have many interpretability benefits in addition to intervene-ability, which we lose when using standard architecture, such as predictions being simple functions of interpretable concepts.\n\nWe agree that CBMs have many advantages beyond interventions. Nevertheless, as mentioned above, there exist use cases when training a CBM from the beginning is not practical or impossible, especially when working with large neural networks whose representations are utilised in multiple downstream tasks. Thus, the proposed techniques can be an interesting alternative, covering a setting complementary to the typical CBM use case."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216420477,
                "cdate": 1700216420477,
                "tmdate": 1700216420477,
                "mdate": 1700216420477,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UewWEFsGsK",
                "forum": "5oJlyJXUxK",
                "replyto": "QlPXgkbmcU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_JHNX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Reviewer_JHNX"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response.\n\nI appreciate the changes to the submission by including additional datasets, results and baselines which I believe make the paper stronger.\n\nHowever this still has not addressed my main problems, i.e. when would this be useful. While you name potential cases, I think in almost all cases you would be better off with Post-hoc CBM[2] or LF-CBM[3] instead. \n\nWhile you have compared against a post-hoc CBM like model, I don't think this is a particularly fair comparison, as this is a strange implementation different from [2] and [3]. Most importantly, it is trained in a joint way, while [2][3] and trained sequentially, and joint training is known to lead to poor intervene-ability. For a better comparison you should instead train the CBM sequentially, or better yet use the actual methods of the papers. The most fair comparison would be Post-hoc CBM-h, which should be included as the residual can significantly improve performance.\n\nLast, my question regarding computational cost was not sufficiently addressed, and I would like to have some actual numbers on how long this takes.\n\nAs a result, I retain my rating of below acceptance threshold."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736585975,
                "cdate": 1700736585975,
                "tmdate": 1700736585975,
                "mdate": 1700736585975,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZahabbhjTi",
                "forum": "5oJlyJXUxK",
                "replyto": "EQZgTuE1kn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5402/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up to the Response"
                    },
                    "comment": {
                        "value": "Thank you for your follow-up!\n\n> I think in almost all cases you would be better off with Post-hoc CBM[2] or LF-CBM[3] instead.\n\nOur updated experiments show that training a CBM model post hoc does not result in the same intervention effectiveness as the proposed fine-tuning technique.\n\n> While you have compared against a post-hoc CBM like model, I don't think this is a particularly fair comparison, as this is a strange implementation different from [2] and [3]. Most importantly, it is trained in a joint way, while [2][3] and trained sequentially, and joint training is known to lead to poor intervene-ability. For a better comparison you should instead train the CBM sequentially, or better yet use the actual methods of the papers. The most fair comparison would be Post-hoc CBM-h, which should be included as the residual can significantly improve performance.\n\nWe acknowledge that this implementation is different from the original paper. As in the rest of the baselines we included in our manuscript, we tried to adjust the architectures and training setup to be as fair as possible, i.e. including a non-linear layer after the bottleneck for enhanced expressivity or probing the original representations directly to the concept set. \n\nWe would like to note that the current implementation is quite close to the label-free CBM, except for the joint vs. sequential training. We will investigate the use of sequential training in the camera-ready since the discussion period will end in half an hour.\n\nRegarding the hybrid approach, as mentioned in our previous response, including the sequentially trained residual connection would not affect intervenability since the residual prediction is added to the concept-based prediction at the very end. We would also like to stress that we were interested in investigating the effectiveness of interventions and not in the marginal gains in predictive performance (particularly since w/o interventions, most models perform quite comparably). \n\n> Last, my question regarding computational cost was not sufficiently addressed, and I would like to have some actual numbers on how long this takes.\n\nThe computational cost varies across datasets and it is dependent on the amount of iterations performed for the optimization. \nThe number of iterations required depends on several factors, e.g. the value of the parameter $\\lambda$, the number of concept variables, the dimensionality of $z$, and the number of data points in the batch. Given all these considerations, providing a meaningful answer to this question is quite challenging. In our experience, interventions are considerably less costly than training or fine-tuning and can be performed online on a single GPU.\n\nFor example, on the synthetic dataset, with the convergence parameter set to $10^{-6}$ (a very conservative value) and for a batch of 512 data points, the intervention procedure requires approx. 500 to 2500 steps (the number of steps varies depending on the number of given concepts and the value of $\\lambda$), which amounts to 0.5 to 2 seconds (for the whole batch) for an untuned black-box model. We use smaller batches and more permissive convergence criteria when fine-tuning, allowing a considerable speedup. In addition, note that the run-time of the intervention procedure is not strictly dependent on the black-box model\u2019s architecture (except for the dimensionality of the layer intervened on)."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5402/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739131256,
                "cdate": 1700739131256,
                "tmdate": 1700740758105,
                "mdate": 1700740758105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]