[
    {
        "title": "Diving Segmentation Model into Pixels"
    },
    {
        "review": {
            "id": "W7zQ69EcJh",
            "forum": "KBo7Z5aTV0",
            "replyto": "KBo7Z5aTV0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5500/Reviewer_GSD6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5500/Reviewer_GSD6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a  pixel learning framework for semantic segmentation. Intra-image, inter-image, inter-domain pixels variances are considered in this framework. The framework is elaborate, which consists of four components, i.e., Multiple Resolution Feature Extraction, Pixel Level Sub-Domain Partition, Adaptive Prototype Generation, and Drift Pixels Alignment. The motivation of this paper is interesting. The experimental results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) Pixel variance is important in semantic segmentation. This paper proposed a new solution. \n\n(2) This framework is flexible, it is quite easily to perform different semantic segmentation tasks.\n\n(3) The performance is good."
                },
                "weaknesses": {
                    "value": "(1) The authors did not report the results on higher resolution images, is that because too many pixels should be considered?\n\n(2) In semantic segmentation, contextual information is quite important to assign a class label to a pixel.  But this paper discards the context in some extent. Is this reasonable?\n\n(3) Pixel-level contrastive learning is widely used in unsupervised semantic segmentation, both local and global relations are considered. In these methods, global pixel features are usually store in a memory bank.  The differences with these method should be given in detail.\n\n(4) In Table 1, the proposed method performs worse with 1/8 and 1/4 than 1/30. The authors should explain this.\n\n(5) In addition of intra-image, inter-image pixel relations, this method also considers the inter-domain one, but this is not presented in abstract."
                },
                "questions": {
                    "value": "see the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5500/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668765912,
            "cdate": 1698668765912,
            "tmdate": 1699636562752,
            "mdate": 1699636562752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FhhQn4fDvM",
                "forum": "KBo7Z5aTV0",
                "replyto": "W7zQ69EcJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer GSD6 (1/5)"
                    },
                    "comment": {
                        "value": "We would like to express our sincere gratitude for your valuable comments and suggestions. We will respond to your concerns one by one.\n\n## W1: \nThe authors did not report the results on higher resolution images, is that because too many pixels should be considered?\n\n## R1:\nWe sincerely appreciate the insightful perspectives and valuable suggestions provided by the reviewer. They have highlighted the computational cost and efficiency challenges associated with the pixel learning scheme. \n\nIndeed, considering all pixels in high-resolution images does impose pressure on memory and computational efficiency. These feedbacks underscore a compelling direction for further exploration of the pixel learning scheme. \n\nAlthough our PiXL is trained on images with moderate resolution, its performance is competitive, even when compared with models trained on higher-resolution images in semi-/fully-supervised settings. **Moreover, experiments on tough UDA and label-scarce semi-supervised settings indicate the merits of our pixel learning scheme when confronted with complex real-world scenarios.**\n\nRegarding efficiency, authors in [1] also encountered such an issue. They proposed calculating the loss on some sampled points instead of the whole mask. This could inspire us to delve into the pixel sampling mechanism to overcome the efficiency burden in pixel learning.\n\nDiverging from the emphasis of this work on validating the effectiveness of the pixel learning scheme, we intend to delve deeper into this efficiency issue in a new study in the future.\n\nWe hope our responses address your concerns.\n\n[1] Cheng, Bowen, et al. \"Masked-attention mask transformer for universal image segmentation.\" *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*. 2022.\n\n## W2: \nIn semantic segmentation, contextual information is quite important to assign a class label to a pixel. But this paper discards the context in some extent. Is this reasonable?\n\n## R2:\nThank you for your valuable insights. We highly appreciate the significance of contextual information in semantic segmentation. \n\nIn this work, we have devoted concerted efforts to incorporate contextual information into pixel feature representation.\n\nFirstly, the backbone in PiXL extracts multi-scale feature representations and adopts the atrous spatial pyramid pooling (ASPP) $^{[1]}$ module to fuse the extracted multi-scale features to embed more contextual information into pixel features.\n\nSecondly, considering that different categories have varying dependencies on low-resolution input, which provides more abundant contextual information, we propose the adaptive prototype generation module to enhance further the context information for classes like *sky*, *road,* *building*, etc.\n\nThese designs succeed in encompassing contextual information into pixel features and are conducive to PiXL's competitive performance under various settings.\n\nMoreover, exploring the design of a purely pixel-based feature extraction method and integrating contextual information within it represents an interesting issue within pixel learning, which will be delved deeper in future endeavors.\n\nWe hope our responses address your concerns.\n\n[1] Chen, Liang-Chieh, et al. \"Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\" *IEEE transactions on pattern analysis and machine intelligence* 40.4 (2017): 834-848."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577687899,
                "cdate": 1700577687899,
                "tmdate": 1700578659854,
                "mdate": 1700578659854,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KpLUtfcJif",
                "forum": "KBo7Z5aTV0",
                "replyto": "W7zQ69EcJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer GSD6 (2/5)"
                    },
                    "comment": {
                        "value": "## W3: \nPixel-level contrastive learning is widely used in unsupervised semantic segmentation, both local and global relations are considered. In these methods, global pixel features are usually store in a memory bank. The differences with these method should be given in detail.\n## R3:\n\nThank you for your insightful comments on our approach. We will systematically explain the distinctions and advantages of our method compared to other pixel-level contrastive learning methods, like storing global information in a memory bank.\n\nOur PiXL differs from the existing methods, employing pixel-level contrastive learning.\n\nFirstly, contrary to the existing methods $^{[1][2][3]}$, which pursue consistency feature representation categorically by utilizing both local and global relations, our PiXL concentrates on coping with local relations. Furthermore, even from the perspective of exploring local information, our approach differs from some existing methods. We emphasize the difference among pixels and advocate designing tailored learning strategies for different pixels, considering pixel-level variance and that the trivial employment of pixel-level contrastive learning may enlarge misleadings from some pixels with poor feature representation. Specifically, **we formulate each image as a local distribution of pixels and hypothesize that a few pixels within the local distribution conform to the categorical implicit global distribution while the others do not, denoted as joint pixels and drift pixels.** **Thus, our PiXL continuously aligns drift pixels from two local distributions to the local prototypes generated from the joint pixels, hoping to address the intra-/inter-image and inter-domain variance ceaselessly.**\n\nSecondly, current methods, such as employing a memory bank to store global pixels or prototypes $^{[1][2][3]}$, may not be optimal. Considering the pixel-level variance, as we summarized in Figure 1, the global pixel features from different images or different iterations stored in the memory bank may also vary greatly, generating misleading signals that confuse anchor pixel features and hamper the alignment. Moreover, the methods ^{[3]} maintaining a single or storing multiple continually updated global prototypes in the memory bank to represent the implicit global distribution of each class also fail to achieve optimal because the complexity of the global distribution exceeds the representation capacity of prototypes.\n\nCompared with these methods, PiXL achieves competitive performance by only concentrating on mining the local relations thanks to two merits of our model: (1) The local prototypes generated from joint pixel features suppress misleading signals originating from pixels with subpar feature representation. (2) Using local prototypes solely for aligning drifting pixels within two local distributions exploits their representational power while avoiding exceeding the limited capacity of prototypes\n\nThese experiments on different settings validate the effectiveness of our proposed pixel learning scheme, verifying the merits of diving the segmentation models into pixel level and addressing pixel-level variance, which is a promising research field.\n\nUndoubtedly, integrating global information into the pixel learning scheme is beneficial. However, existing methods to mine and exploit global relationships may require further optimization. This aspect remains a worthwhile pursuit in the field of pixel learning. In our future work, we will concentrate on how to effectively combine local and global relationships under the context of pixel learning and design novel models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578076951,
                "cdate": 1700578076951,
                "tmdate": 1700578678928,
                "mdate": 1700578678928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Id3Z2o5c5h",
                "forum": "KBo7Z5aTV0",
                "replyto": "W7zQ69EcJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer GSD6 (3/5)"
                    },
                    "comment": {
                        "value": "## R3 (Cont.)\nThe comparison between PiXL and other pixel-level contrastive learning methods is summarized here. \n\nIn fully supervised semantic segmentation:\n\n| Method | Image Size      | Cityscapes |\n| ------ | --------------- | ---------- |\n| [1]    | 512$\\times$1024 | 82.40      |\n| PiXL   | 512$\\times$512  | 82.74      |\n\nIn semi-supervised semantic segmentation:\n\n| Method           | Image Size      | Cityscapes(1/30) | Cityscapes(1/8) | Cityscapes(1/4) |\n| ---------------- | --------------- | ---------------- | --------------- | --------------- |\n| [2]              | 512$\\times$1024 | 64.90            | 70.10           | 71.70           |\n| PiXL (DeepLabV3) | 512$\\times$512  | 70.62            | 75.20           | 78.20           |\n| PiXL (HRDA)      | 512$\\times$512  | 71.73            | 76.37           | 78.91           |\n\nIn unsupervised domain adaptation:\n\n| Method | Road | S.Walk | Build. | Wall | Fence | Pole | Tr.Light | Tr.Sign | Veget. | Terrain | Sky  | Person | Rider | Car  | Truck | Bus  | Train | M.Bike | Bike | mIoU |\n| ------ | ---- | ------ | ------ | ---- | ----- | ---- | -------- | ------- | ------ | ------- | ---- | ------ | ----- | ---- | ----- | ---- | ----- | ------ | ---- | ---- |\n| [3]    | 96.3 | 73.6   | 89.6   | 53.7 | 47.8  | 53.8 | 60.8     | 60.0    | 89.9   | 48.8    | 91.5 | 74.6   | 45.1  | 93.1 | 74.8  | 73.8 | 51.5  | 60.3   | 65.3 | 68.7 |\n| PiXL   | 97.0 | 77.6   | 91.1   | 59.9 | 54.1  | 57.2 | 64.8     | 69.1    | 91.5   | 51.8    | 94.8 | 80.5   | 57.3  | 94.6 | 83.8  | 88.7 | 78.0  | 65.6   | 67.8 | 75.0 |\n\nWe hope our responses address your concerns.\n\n[1] Wang, Wenguan, et al. \"Exploring cross-image pixel contrast for semantic segmentation.\" *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2021.\n\n[2] Alonso, Inigo, et al. \"Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank.\" *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2021.\n\n[3] Xie, Binhui, et al. \"Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation.\" *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2023)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578100710,
                "cdate": 1700578100710,
                "tmdate": 1700578169401,
                "mdate": 1700578169401,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y19BpOMqTb",
                "forum": "KBo7Z5aTV0",
                "replyto": "W7zQ69EcJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer GSD6 (4/5)"
                    },
                    "comment": {
                        "value": "## W4: \nIn Table 1, the proposed method performs worse with 1/8 and 1/4 than 1/30. The authors should explain this.\n\n## R4:\n\nThanks for your thoughtful comments and valuable feedback!\n\nIn the semi-supervised setting, our PiXL demonstrates competitive performance, especially when only 3.3% of images have annotations, indicating that the pixel learning scheme excels in coping with complex, real-world scenarios. In 1/4 and 1/8 settings, PiXL surpasses all methods trained on images with the same resolution and even outperforms some methods trained on higher-resolution images. Even as the number of annotated images decreases, our model exhibits remarkable robustness. \n\nIn addition, our primary focus in this study is to explore a new semantic segmentation scheme and develop an adaptable, efficient model capable of handling diverse semantic segmentation settings, distinct from models designed solely for specific scenarios. Comprehensive qualitative, quantitative, and visual analyses have validated the success of our initial exploration. In the subsequent new studies on the pixel learning scheme, we aim to further optimize and enhance its capabilities.\n\nWe hope our responses address your concerns."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578233220,
                "cdate": 1700578233220,
                "tmdate": 1700578693629,
                "mdate": 1700578693629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bxKCAKen2x",
                "forum": "KBo7Z5aTV0",
                "replyto": "W7zQ69EcJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer GSD6 (5/5)"
                    },
                    "comment": {
                        "value": "## W5: \nIn addition of intra-image, inter-image pixel relations, this method also considers the inter-domain one, but this is not presented in abstract.\n\n## R5:\nThank you for your valuable feedback. We have diligently revised the manuscript. \n\nMoreover, as depicted in Figure 1, the intra-/inter-image, inter-domain, and label-error-related variance collectively contribute to significant heterogeneity among pixel features belonging to the same category, leading to a more dispersed distribution in the feature space.\n\nCorrespondingly, the proposed pixel learning aims to alleviate pixel-level variance, meaning to enhance the consistency among pixel features belonging to the same category and seek a more compact distribution in the feature space. \n\nThe revised version of the abstract is given as follows:\n\n> \u201cMore distinguishable and consistent pixel features for each category will benefit the semantic segmentation under various settings. Existing efforts to mine better pixel-level features attempt to explicitly model the categorical distribution, which fails to achieve optimal due to the significant pixel feature variance. Moreover, prior research endeavors have scarcely delved into the thorough analysis and meticulous handling of pixel-level variances, leaving semantic segmentation at a coarse granularity. In this work, we analyze the causes of pixel-level variance and raise the concept of **pixel learning** to concentrate on the tailored learning process of pixels, handle pixel-level variance, and enhance the segmentation model's per-pixel recognition capability. **Under the context of the pixel learning scheme, each image is viewed as a distribution of pixels, and pixel learning aims to pursue consistent pixel representation inside an image, continuously align pixels from different images (distributions), and eventually achieve consistent pixel representation for each category, even cross-domains. We proposed a pure pixel-level learning framework, namely PiXL, which consists of a pixel partition module to divide pixels into sub-domains, a prototype generation, a selection module to prepare targets for subsequent alignment, and a pixel alignment module to guarantee pixel feature consistency intra-/inter-images, and inter-domains**. Extensive evaluations of multiple learning paradigms, including unsupervised domain adaptation and semi-/fully-supervised segmentation, show that PiXL outperforms state-of-the-art performances, especially when annotated images are scarce. Visualization of the embedding space further demonstrates that pixel learning attains a superior representation of pixel features. The code will be available upon acceptance.\u201d\n\nWe hope our responses address your concerns."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578307839,
                "cdate": 1700578307839,
                "tmdate": 1700717332449,
                "mdate": 1700717332449,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jzuGWjSZV9",
            "forum": "KBo7Z5aTV0",
            "replyto": "KBo7Z5aTV0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5500/Reviewer_YSzS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5500/Reviewer_YSzS"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce the pixel learning scheme by treating each image as a local distribution of pixels. The PiXL framework, which segregates pixels within a given local distribution into sub-domains: joint pixels and drift pixels, is proposed. Then, the PiXL employs an asymmetric alignment approach to align drift pixels with the joint pixels, effectively addressing pixel-level variance in a\ndivide-and-conquer manner. Extensive experiments confirm PiXL\u2019s performance, especially demonstrating promising results in\nlabel-scarce settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper  proposes a novel pixel learning scheme to dive semantic segmentation models into the pixel level by treating an image as a distribution of pixels. This advocates addressing pixel-level variance to enhance the segmentation model\u2019s per-pixel recognition capability.\nThe strengths are as follows:\n1. This paper proposed PiXL, a pure pixel-level learning framework that executes pixel-level intra- and inter-distribution (image) alignment within the context of pixel learning. \n2. Extensive quantitative and qualitative experiments in various settings of semantic segmentation confirm the effectiveness of PiXL, demonstrating the feasibility and superiority of the pixel learning scheme, which deserves further exploration.\n3. The writing is clear and well-reading."
                },
                "weaknesses": {
                    "value": "The weakness are as follows:\n1. Some description is not very clear. For example, in equation 4, \"PiXL determines the threshold ...\", how to determine the threshold is not presented. why \" that pixel partitioning is performed separately...considering the entropy gap across images.\"?\n2. \"PiXL employs entropy as the criteria to segregate the pixel features in g into joint pixels and drift\npixels.\" how to compute the entropy?\n3. the paper validate the effectiveness on HRDA model, but if the proposed methods can be applied to general semantic segmentation methods is not verified.\n4. In table 3, the proposed method cannot show state-of-the-art performance compared with other methods. The authors should prove its effectiveness."
                },
                "questions": {
                    "value": "The questions are summarised with weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5500/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734768338,
            "cdate": 1698734768338,
            "tmdate": 1699636562637,
            "mdate": 1699636562637,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u09UAL3TRq",
                "forum": "KBo7Z5aTV0",
                "replyto": "jzuGWjSZV9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To by Reviewer YSzS"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We have carefully revised the manuscript according to your suggestions and provided more details, especially about the entropy calculation. Moreover, we will explain your concerns point by point. \n\n## W1: \nSome description is not very clear. For example, in equation 4, \"PiXL determines the threshold ...\", how to determine the threshold is not presented. why \" that pixel partitioning is performed separately...considering the entropy gap across images.\"?\n\n## W2: \n\"PiXL employs entropy as the criteria to segregate the pixel features in g into joint pixels and drift pixels.\" how to compute the entropy?\n\n## R1 & R2:\n1. We appreciate your thoughtful and constructive feedback and provide more details on the calculation of entropy in the Appendix as follows:\n\n   > ''In PiXL, we employ Information Entropy (entropy) as the measurement criteria to split the pixels into joint and drift pixels. For a pixel feature representation $\\boldsymbol{f}$ with its predicted probability distribution, its entropy h is given by:\n   > $$\n   > \\begin{equation}\n   > \\begin{aligned}\n   > &h = - \\sum_{k = 1}^{C}p_{k}\\log p_{k}.\n   > \\end{aligned}\n   > \\end{equation}\n   > $$\n   > where $C$ is the number of classes and $p_{k}$ is the probability of the pixel belonging to class $k$.\"\n\n   The larger $h$ of a pixel means the greater uncertainty of its prediction. The entropy of a one-hot distribution is $0$, indicating no uncertainty. Thus, the pixels with smaller entropy imply less uncertainty and are referred to as joint pixels, which are assumed to conform to the implicit global distribution, while the pixels with larger entropy are denoted as drift pixels.\n\n2. The threshold $\\epsilon$ is determined by the sorted pixel entropy from each image. Specifically, given the extracted pixel features $F^L$ and $F^H$ from low-resolution input $X^L$ and high-resolution crop $X^H$, respectively, we compute their corresponding pixel entropy $h$ for each pixel feature and sort them according to their entropy. The pixel entropy at the $1 - \\eta$ quantile is selected as the threshold $\\epsilon$. The details are provided in Section 3.4 as depicted in Equation (4) and c. part in Figure 2.\n\n3. Considering the pixel-level variance summarized in Figure 1, the range of pixel entropy may differ between two images. Given the most extreme condition where the entropy of pixels in image $\\textit{A}$ are all larger than the ones in image $\\textit{B}$, conducting partition on the pixels from two images instead of on each image separately will cause the failure to divide the pixels within each distribution into sub-domains and eventually hinders the intra-distribution alignment.\n\nWe hope our responses address your concerns.\n\n## W3: \nthe paper validate the effectiveness on HRDA model, but if the proposed methods can be applied to general semantic segmentation methods is not verified.\n\n## R3:\nAlthough our experiments are primarily based on HRDA model, our method is plug-and-play and applicable to other semantic segmentation models. In semi-supervised segmentation, apart from the PiXL based on HRDA, we also implemented a DeepLabV3+-based PiXL. Their competitive performance indicates that our methods can be easily applied to many existing semantic segmentation models. Please refer to the Network architecture paragraph in Section 4.3, Table 1, and Section A.2 in the appendix for more details.\n\nWe hope our responses address your concerns.\n\n## W4: \nIn table 3, the proposed method cannot show state-of-the-art performance compared with other methods. The authors should prove its effectiveness.\n\n## R4:\nThanks for your thoughtful comments and valuable feedback!\n\nWe elaborate on your concerns about the performance of PiXL in the response to all reviewers. Please refer to that for details. \n\nMoreover, we have summarized the key points here to address your concerns better.\n\nIn this study, our primary focus is exploring a novel semantic segmentation scheme and devising a versatile and efficient model that caters to diverse settings in semantic segmentation. Our approach differs from proposing a model specifically tailored, developed, or optimized for fully supervised settings. \n\nDespite this, as a first exploration of the pixel learning scheme, our PiXL model has showcased commendable performance, particularly in demanding unsupervised domain adaptation and semi-supervised settings where labeled data is limited. This underscores the suitability of the pixel learning scheme to address the requirements of intricate real-world scenarios. \n\nOur future research endeavors aim to further optimize the pixel learning scheme by strengthening global distribution constraints, integrating contextual information, emphasizing model efficiency and lightweight designs, and further bolstering the model's performance across diverse task settings.\n\nWe hope our responses address your concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577454675,
                "cdate": 1700577454675,
                "tmdate": 1700578621013,
                "mdate": 1700578621013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7R7gahWBMy",
            "forum": "KBo7Z5aTV0",
            "replyto": "KBo7Z5aTV0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5500/Reviewer_nhEN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5500/Reviewer_nhEN"
            ],
            "content": {
                "summary": {
                    "value": "This paper takes pixel-level distribution (local distribution) into consideration, and proposed Pixel Level Sub-Domain Partition Module (PSP), Adaptive Prototype Generation Module (APG); Drift Pixels Alignment Module(DPA) modules, the effectiveness of which is proved in ablation study.\nFirst, the PSP module divides all features (with multiscale feature extraction) into Joint pixel features and Drift Pixel features based on the entropy of each segmentation pixels corresponding to each pixel feature. For Joint ones, they use APG to generate local feature prototypes based on their semantic classes, while the Drift ones would be pulled by the prototypes extracted from APG using info NCE loss. \nThe prototypes from APG is the mean value of pixel features which belongs to Joint pixel features in two samples. From which, the paper argues that can get intra-image and inner-image information.\nFinally, the effectiveness of these module is proved in unsupervised domain adaptation, semi-supervised semantic segmentation together with fully-supervised semantic segmentation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written and nicely organized.\n2. Extensive experiments have been conducted and a number of quantitative and qualitative results are shown, demonstrating the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The method seems incremental. The paper generates class prototype from two image features and push or pull image features based on these prototypes. Their novelty lies in the partition of joint features and drifted features, the selection of high resolution feature prototypes and low resolution prototypes, which seems to be triky."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5500/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763097666,
            "cdate": 1698763097666,
            "tmdate": 1699636562451,
            "mdate": 1699636562451,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0ghCT89GaR",
                "forum": "KBo7Z5aTV0",
                "replyto": "7R7gahWBMy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer nhEN"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments on our manuscript. We will address your questions regarding our work one by one.\n\n##  W1: \nThe method seems incremental.\n\n## R1:\n\nThe PiXL is an end-to-end semantic segmentation model, which is not a two-stage method or based on a well-trained model. In PiXL, the backbone is initialized by ImageNet pre-trained parameters, while the decoder is randomly initialized following the conventional practice. Then, the pixel features are continuously furnished to pursue an intra-class compact and inter-class separable pixel feature distribution. Figure 4 validates that the pixel feature distribution produced by PiXL is better than the SOTA method.\n\n## W2: \nThe selection of high resolution feature prototypes and low resolution prototypes seems to be tricky.\n\n## R2:\n\nWe propose this selection mechanism in Adaptive Prototype Generation module based on the consideration that different categories have varying dependencies on low-resolution input $X^L$ and high-resolution crop $X^H$. A higher resolution crop $X^H$ can benefit the recognition of smaller targets, like person and pole, as it provides abundant details, while a larger field of view in low-resolution input $X^L$ aids in better context perception, thereby improving the segmentation of larger objects, such as sky and road. Thus, this selection mechanism aims to adaptively embed context or details into pixel features of different categories. The ablation study in Table 4 verifies its effectiveness. It's worth noting that this selection mechanism is not handcrafted but adaptively applied according to the pixels in low-resolution input $X^L$ and high-resolution crop $X^H$ as depicted in d. part of Figure 2 and Equation (8)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577010639,
                "cdate": 1700577010639,
                "tmdate": 1700578584714,
                "mdate": 1700578584714,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mLRA7cvZer",
            "forum": "KBo7Z5aTV0",
            "replyto": "KBo7Z5aTV0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5500/Reviewer_L3L6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5500/Reviewer_L3L6"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes  a pixel-level learning framework, PiXL, for semantic segmentation. The framework consists of a pixel partition module,  a prototype generation and selection module, and a pixel alignment module. The pixel partition module separate pixels features into joint and   drift pixels based on their entropy. The prototype generation module is to select the most meaningful pixels. The pixel aligment module adopts contrastive learning to align pixel features intra and inter-distribution. The effectiveness of proposed framework and components are experimentally validated on three public datasets: GTA5, SYNTHIA, and Cityscapes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of investigating semantic segmentation from pixel feature distribution perspective is novel.\n2. The proposed pixel learning framework and its components are technically solid and innovative.\n3. The motivation and the underlying principles of designing the framework and each components are clearly presented and explained, so that it is easy to follow the work.\n4. The experiments are extensive and solid."
                },
                "weaknesses": {
                    "value": "The experiment results of the proposed results are not much better than previous works."
                },
                "questions": {
                    "value": "no"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5500/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699107862788,
            "cdate": 1699107862788,
            "tmdate": 1699636562324,
            "mdate": 1699636562324,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ce0WqujteJ",
                "forum": "KBo7Z5aTV0",
                "replyto": "mLRA7cvZer",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5500/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer L3L6"
                    },
                    "comment": {
                        "value": "## W1: \nThe experiment results of the proposed results are not much better than previous works.\n\n## R1:\nThanks for your insightful comments and valuable feedback!\n\nWe elaborate on your concerns about the performance of PiXL in the response to all reviewers. Please refer to that for details. \n\nMoreover, we have summarized the key points here to address your concerns better.\n\nThis work's primary concern is exploring a novel semantic segmentation scheme to provide a novel perspective on image comprehension. Specifically, we propose the **Pixel Learning** scheme by formulating each image as a distribution of pixels, addressing the pixel-level variance to recognize each pixel accurately. \n\nGiven this, the meticulous optimization of backbones or task settings is not the primary focus of this work. Nevertheless, our PiXL performs competitively across various settings, affirming the effectiveness of the pixel learning scheme and suggesting its potential for further exploration.\n\nIn particular, PiXL excels in challenging scenarios like unsupervised domain adaptation and semi-supervised settings with very limited labeled data, verifying PiXL's effectiveness and robustness in complex real-world scenarios. Qualitative analysis and visualization have further illustrated the promising pixel learning scheme.\n\nIn the future, we will delve deeper and further optimize the pixel learning scheme from global constraint enhancement, contextual information integration, model efficiency, and lightweight, respectively.\n\nWe hope our responses address your concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5500/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576849578,
                "cdate": 1700576849578,
                "tmdate": 1700714180373,
                "mdate": 1700714180373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]