[
    {
        "title": "Language Modeling Is Compression"
    },
    {
        "review": {
            "id": "DedLwBCEvm",
            "forum": "jznbgiynus",
            "replyto": "jznbgiynus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission372/Reviewer_55Jb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission372/Reviewer_55Jb"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an interesting view that connects compression and prediction of LLM. It leverages a pre-defined rule (e.g., Arithmetic coding) to compress a sequence based on the conditional probabilistic intervals of its each token. The compressed content can be losslessly restored to the original content based on the decoding rules that reverse the encoding process. The experimental results show an LLM can be very effective to compress content and works as a general-purpose compressor."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper discusses a novel perspective to connect between compression and sequence prediction. Its evaluations on the compression capabilities of LLMs are extensive and sound. The results that it can compress other modalities like images and audio are pretty interesting and its insight of the compression with the size of data and model (scaling) is inspiring."
                },
                "weaknesses": {
                    "value": "While this paper introduces a novel perspective to understand the compression ability of Large Language Models (LLMs), its contribution or novelty is not particularly prominent from a high-level idea/conclusion standpoint.\n\nFirstly, the concept that a language model is a form of compression is not new. As early as 2023, in an interview between Nvidia's Jensen Huang and OpenAI's Ilya Sutskever, Ilya mentioned that a language model learns through compression, and the generative model is essentially a compression of data in the world. This insight is well ingrained among most NLP/LLM professionals. Therefore, although this paper connects sequence prediction and compression from an arithmetic perspective, aside from some interesting experimental results, it doesn't provide practitioners with many new insights, such as in terms of methodology. While its LLM compressor performs well compared to gzip, it is difficult to use in practice due to the high inference cost.\n\nSecondly, I believe a major highlight of this paper is the discussion on general-purpose (trained on text, but can work for other modalities) and scaling (the larger, the better at compression). The overall method of the paper still uses the Arithmetic coding approach. However, prior work has already presented similar observations and conclusions, albeit not from an arithmetic coding perspective. For instance, Ge et al. (2023) proposed using lora tuning to adapt the LLM for the compression ability, enabling it to compress a long context into a short span. Although Ge et al.'s (2023) work is not lossless compression, they observed similar phenomena: for example, their Table 3 comparison of normal text, patterned random text, and completely random text shows that LLMs can compress based on certain patterns (even though the model has not seen patterned random text, it performs better on patterned random text than on completely random text). Similarly, their Tables 5 and 8 also indicate that more potent LLMs are better at compression. Therefore, while I find the perspective of this paper novel and interesting, its final conclusions cannot be considered entirely novel.\n\nDespite these weaknesses, I think this paper's contribution overweigh the weaknesses as a research paper to present in ICLR.\n\nReferences:\n\nGe et al. In-context Autoencoder for Context Compression in a Large Language Model. https://arxiv.org/abs/2307.06945"
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission372/Reviewer_55Jb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission372/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692000503,
            "cdate": 1698692000503,
            "tmdate": 1699635964320,
            "mdate": 1699635964320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DgUb0rBoOn",
                "forum": "jznbgiynus",
                "replyto": "DedLwBCEvm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and constructive feedback on the paper.\n\n**How does your experimental evaluation differ from prior work?**\n\nWe thank the reviewer for pointing us to the paper by Ge et al. (2023), which we have added to our manuscript. The findings of Ge et al. (2023) are akin to our investigation of tokenization, which can be viewed as a pre-compression of the input to a language model. That is, Ge et al. (2023) present an alternative to tokenization that solves the practical issue of maximizing the information in the context window of a Transformer, which boosts downstream performance.\n\nHowever, Ge et al. (2023) do not show that their ICAE method achieves strong lossless compression rates on image and audio data, and the performance on patterned random text is relatively poor (3.5 BLEU) compared to normal text (99.3 BLEU) and almost the same as completely random text (0.2 BLEU). In contrast, we show that LLMs are on-par or even *outperforming specialized image and audio compressors* (in terms of raw compression rate, i.e., not taking into account the model-size or program complexity).\n\nMoreover, unlike Ge et al. (2023), we not only show that larger LLMs achieve better compression rates but conduct a comprehensive investigation of the scaling behavior across model *and* dataset size, elucidating the limits of naive scaling.\n\nThus, as the reviewer pointed out, we still believe that a major novelty of our work is showing that *language* models trained on text are strong general-purpose compressors on image and audio data and *comprehensively* investigating the scaling laws for compression. We want to emphasize again that the ability for general-purpose compression implies that LLMs, through training on a large corpus of text, reliably acquire the ability to detect and predict (and thus compress) general algorithmic patterns. This is not trivially expected, and while understanding the precise nature of these patterns and how they relate to the training data remains an exciting avenue for future research, the finding that competitive compression rates on images and audio data can be achieved is not trivially expected per se.\n\n\n**The concept that a language model is a form of compression is not new.**\n\nWhile we agree that viewing language modeling as compression is a well-established insight (see, e.g., Section 1), we believe that it is valuable to the broader ML research community to popularize these ideas (beyond the LLM professionals) and the subsequent connections to essential areas, such as in-context learning, scaling laws, tokenization, and generative modeling. Our goal is thus not to provide new insight in terms of compression methodology or to advocate for using LLMs as compressors in practice, as they have too many limitations (e.g., size and inference time) to be practical. Instead, we aim to provide an interesting experimental evaluation of viewing language modeling through the lens of compression."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575872142,
                "cdate": 1700575872142,
                "tmdate": 1700575872142,
                "mdate": 1700575872142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "86bcdrEqa4",
            "forum": "jznbgiynus",
            "replyto": "jznbgiynus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission372/Reviewer_NHbN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission372/Reviewer_NHbN"
            ],
            "content": {
                "summary": {
                    "value": "The authors argue that predictive models can be transformed into lossless compressors and vice versa, and that language models can also be powerful compressors, providing novel insights into scaling laws and in-context learning. The paper includes experimental details and additional results, including data auto-regressively generated by gzip and Chinchilla."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.  The paper is well-written and clear to investigate how and why compression and prediction are equivalent. \n2. Evaluate large pretrained models used as compressors against various standard compressors and showed that they are competitive, not only on text but also on modalities they have never been trained on, such as images and audio data."
                },
                "weaknesses": {
                    "value": "If we discuss the number of parameters in larger language models and how it reflects compression performance, it would be better to investigate the reasons behind this relationship."
                },
                "questions": {
                    "value": "The questions are listed in the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission372/Reviewer_NHbN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission372/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729101685,
            "cdate": 1698729101685,
            "tmdate": 1699635964231,
            "mdate": 1699635964231,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EosWFLdWQC",
                "forum": "jznbgiynus",
                "replyto": "86bcdrEqa4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and constructive feedback on the paper.\n\n\n**What is the relationship between the number of parameters and compression performance?**\n\nWe investigate the relationship between the number of parameters and compression performance in Figure 2, where we evaluate the adjusted compression rate (i.e., taking the model size into account) on datasets of increasing sizes ($10^7$, $10^8$, and $10^9$ bytes). First, we note that for a fixed dataset size, the compression rate shows a distinctive U-shape, i.e., it improves with model size but starts to deteriorate when the model becomes too big (i.e., overfits). However, we also see that the compression performance improves with model size as the dataset size improves since the compression rate improves faster than the parameter count (i.e., the models show evidence of emergent capabilities with scale)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575719867,
                "cdate": 1700575719867,
                "tmdate": 1700575719867,
                "mdate": 1700575719867,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a3sW5oVCa5",
            "forum": "jznbgiynus",
            "replyto": "jznbgiynus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission372/Reviewer_8SR8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission372/Reviewer_8SR8"
            ],
            "content": {
                "summary": {
                    "value": "This paper advocates viewing the prediction problem through the lens of compression and evaluates the compression capabilities of large (foundation) models, thereby providing insights into scaling laws, tokenization, and in-context learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Novel in the sense of applying LLM to compressed coding of images & audio.\n2. Demonstration through resourceful examples."
                },
                "weaknesses": {
                    "value": "1. The idea of deep model learning being a compression of natural data is not new, I think this is echoed by the authors too. It has, e.g., been a core and explicit theme in \"High Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications\". As such, shouldn't the paper's title be more specific, such as \"LLMs are general-purpose image & audio compressors\"?\n\n2.  A key into understanding the algorithm is Fig. 1, but the figure contains ambiguities and confusion. E.g. Why a \"?\" in \"b0?\" only? In the last column, how to go from 4bit to 7bit? The illustration should be more tractable.\n\n3. I have doubt about the \"generative\" model part, for the text examples in B.1, the good performance of Chinchilla over gzip is no surprise. But the poor performance on images & audio in Fig. 3 & B.2 indeed shows LLM can't handle these data in general. How can an LLM even be called generative in images/audio if the results make no sense? If that's the case, the last sentence in Abstract shouldn't be made.\n\n4. I am also doubtful about the part on tokenization. The tokenizer being part of a Transformer is only due to its root in language modeling. In the same vein, we can easily make a claim on \"A CNN stem is compression\" (a CNN has a stem, body & head), and varying the stem (e.g. different strides) we get different compression rates too, so what is interesting in that?\n\nEditorial:\nThe line above Sect 3.6, typo \"accross\""
                },
                "questions": {
                    "value": "See weaknesses for questions.\n\nAlso, is the context length of 2048 bytes still a must given the recent work on lifting this length constraint? e.g. \"Efficient Streaming Language Models with Attention Sinks\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Nil"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission372/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731361917,
            "cdate": 1698731361917,
            "tmdate": 1699635964119,
            "mdate": 1699635964119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oO0k065vtX",
                "forum": "jznbgiynus",
                "replyto": "a3sW5oVCa5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and constructive feedback on the paper.\n\n**Can you clarify Figure 1?**\n\nYes, we have extended the caption of the figure to further clarify arithmetic coding and included the step-by-step example below in Appendix A.\n\nFigure 1. visualizes arithmetic encoding, which iteratively partitions the interval $I = [0, 1)$ according to a predictive model $P$ and an input string, i.e., $AIXI$ for Figure 1.\n\nFirst, we construct the intervals for $P(\\cdot)$:\n* $[0, 0.45)$ for $P(A) = 0.45$\n* $[0.45, 0.75)$ for $P(I) = 0.3$\n* $[0.75, 1)$ for $P(X) = 0.25$\n\nSince the first token is $A$, we set $I = [0, 0.45)$ and iterate. Thus, the intervals for $P(\\cdot | A)$ are:\n\n* $[0.2 * 0, 0.2 * 0.45) = [0, 0.09)$ for $P(A | A) = 0.2$\n* $[0.09, (0.2 + 0.6) * 0.45) = [0.09, 0.36)$ for $P(I | A) = 0.3$\n* $[0.36, (0.2 + 0.6 + 0.2) * 0.45) = [0.36, 0.45)$ for $P(X | A) = 0.2$\n\nSince the next token is $I$, we set $I = [0, 0.45)$ and so on. We terminate with $I = [0.322, 0341)$ for $AIXI$. Next, arithmetic encoding will compute the binary sequence corresponding to iteratively splitting the interval $[0, 1)$ in half until it is fully contained in $I$. \n\nConcretely, this yields the binary sequence.\n\n* $b0$ -> $[0, 0.5)$\n* $b01$ -> $[0.25, 0.5)$ \n* $b010$ -> $[0.25, 0.375)$\n* $b0101$ -> $[0.3125, 0.375)$\n* $b01010$ -> $[0.3125, 0.34375)$\n* $b010101$ -> $[0.328125, 0.34375)$\n* $b0101010$ -> $[0.328125, 0.3359375)$\n\nAs $[0.328125, 0.3359375)$ is fully contained in $I = [0.322, 0341)$, the compressed output is $0101010$, which consists of 7 bits as opposed to the 4 bytes used to encode $AIXI$. \n\n\n**What is the motivation for the section on tokenization?**\n\nThe motivation for the tokenization section is precisely what the reviewer points out: Tokenization is rooted in language modeling and has been developed to overcome practical limitations (e.g., small context windows). Here, we describe that a tokenizer can be viewed as an additional compressor that is applied before the language model. Note that we do not state that \"Transformers Are Compression\" but that \"Language Modeling Is Compression\". In that sense, a CNN trained to predict the next token with the cross-entropy loss will also yield a lossless compressor. Here, we only consider Transformers since they are predominantly used for language modeling research.\n\n\n**Can you explain the poor generative performance of LLMs on image and audio data?**\n\nYes, recall that the rows in Figures 3 and C.2 are treated independently by the model, which explains why the image generations look rather \"noisy\" across rows. Moreover, given that Chinchilla was only trained on text, it is not surprising that its generative performance on image and audio are not of high visual quality. Nevertheless, we find it remarkable that such language models generate somewhat meaningful completions for unseen data modalities, which we attribute to the emergent capabilities of pattern recognition at scale. We added a more extensive discussion to the paper (Section 3.4) and also evaluated the generative capabilities of less powerful models (Chinchilla 1B and 7B) to Appendix C (showing that the generative quality improves with predictive performance).\n\n\n**Shouldn\u2019t the title be more specific?**\n\nWhile we agree that the idea of deep learning being compression is not novel, we believe that our paper goes beyond pure image and audio compression as we also discuss scaling laws (i.e., model/dataset size tradeoffs), in-context learning, tokenization, and generation. \n\n\n**Why do you use a context length of only 2048 bytes?**\n\nWhile the reviewer correctly points out that recent work has enabled much longer contexts, we are limited to a context size of 2048 bytes due to the computational resources available to us. However, we do not expect the results to change qualitatively, given that Figure 4 shows that the compression rates plateau after a certain context length."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576131119,
                "cdate": 1700576131119,
                "tmdate": 1700576131119,
                "mdate": 1700576131119,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6wlwAfdkyB",
                "forum": "jznbgiynus",
                "replyto": "oO0k065vtX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission372/Reviewer_8SR8"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission372/Reviewer_8SR8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your feedback"
                    },
                    "comment": {
                        "value": "I appreciate the feedback and fixes to Fig. 1. Given there is no significantly new results/counterarguments to my questions, I'll stay put with my ratings."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644225104,
                "cdate": 1700644225104,
                "tmdate": 1700644225104,
                "mdate": 1700644225104,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XWpd2IAVz7",
            "forum": "jznbgiynus",
            "replyto": "jznbgiynus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission372/Reviewer_hCy6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission372/Reviewer_hCy6"
            ],
            "content": {
                "summary": {
                    "value": "This paper demonstrates that pre-trained large language models can be used for compressing text, image, and audio data. This is done by inputing the data to the model and relying on the model\u2019s output probabilities across the vocabulary to perform arithmetic coding. When not considering the size of a model for computing compression ratios, the authors show that Chinchilla-70B, achieves high compression ratios surpassing well-established domain-specific compressors like PNG or FLAC. When taking into account the number of model parameters for calculating the compression ratio, the authors illustrate new empirical scaling-laws by plotting the compression ratio as a function of model size, resulting in an U-shaped curve. This scaling law suggests that depending on the size of a dataset the optimal compression ratio is achieved for one specific model size. The authors also attempt to show how compressors can be used as conditional generative models. Finally, the authors analyze how the context-length for in-context learning and different tokenizers affect a model's compression performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper presents how large language models pre-trained on text data can be used for compression beyond text data \n- The authors demonstrate that this approach outperforms several well-established compression methods like gzip, PNG or FLAC in terms of raw compression ratio\n- The paper provides insights on how different aspects like model size and choice of the tokenizer affects performance. For example, for model size the authors provide empirical scaling laws\n- The experiments are well described, easy to follow, and kept fair for all the methods being compared. \n- Tables and figures showing the results of the experiments are also simple to understand\n- The authors openly discuss limitations of using large language models as compressors (e.g. model size and context length for transformer models)"
                },
                "weaknesses": {
                    "value": "- The motivation of this work is rather unclear to me. Is this work about advocating the use of pre-trained large language models as a potential method for compression? If so, how can they be used as such in practice considering their limitations? Or is it about using the compression framework to better understand large language models? If so, why is it interesting to study pre-trained large language models  \"through the lens of compression\"?\n- The authors mention that they \u201cadvocate for using (lossless) compression to study foundation models\u201d. Why and what benefits does this framework have? It is not clear to me how the results in this paper should help my understanding of large language models beyond their use as compressors? What are the further implications of the results?\n- No experiments with pre-trained models other than Chinchilla-70B. Having more models could provide more evidence on the compression capabilities of pre-trained large language models and to see how compression capabilities correlate with prediction performance\n- Not using publicly available pre-trained large language models for reproducibility\n- The results for the generative modeling performance of compressors and Chinchilla-70B look rather poor. For example, the generated image in Figure 3 looks unconvincing since only lines are generated and not actual image content, and a quantitative analysis is also missing. Why is this section important, and why would it fit into the rest of the paper?"
                },
                "questions": {
                    "value": "Questions:\n- The questions mentioned in the \"Weaknesses\" paragraph\n\nSuggestions:\n- To me, some findings feel somewhat scattered and it is difficult to draw a clear conclusion from this work. For example when and why is it important to distinguish between compression rates that consider and do not consider model size, and particularly the part discussing the generative capabilities feels disconnected from the rest of the story. I recommend aligning the narrative more cohesively and concluding with a clear takeaway message\n- I find that the background section is more technical than needed to understand the results of the paper. For example, the authors could maybe show on a concrete example how arithmetic coding works instead being very general and introducing a lot of mathematical notations and concepts. Reducing the relatively heavy use of mathematical notation could also help with reading since most of it does not appear again after the background section. Also, in Figure 1 which illustrates arithmetic coding, there are missing explanations (e.g. arrows are not explained) to guide the reader."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission372/Reviewer_hCy6",
                        "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission372/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699630690849,
            "cdate": 1699630690849,
            "tmdate": 1700686136365,
            "mdate": 1700686136365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZapQnNxRbe",
                "forum": "jznbgiynus",
                "replyto": "XWpd2IAVz7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and constructive feedback on the paper.\n\n**What is the motivation for this work and what are the implications of the results?**\n\nIn recent years, language modeling has become a central element of AI research, primarily thanks to transformers' unmatched efficiency and performance (particularly for large foundation models). However, many researchers are unaware of the close connection between prediction and compression power. Thus, our primary motivation is to demonstrate that well-known large language models can be used to compress data. One, arguably even more surprising, finding is that models trained to predict/compress text turn out to become fairly good compressors for data that they haven't been trained on (i.e., image and audio data). This \"transfer\" can only be explained by LLMs obtaining the ability to predict, and thus compress, general (algorithmic) patterns. As we show further, larger LLMs perform better at general compression than smaller LLMs, suggesting that the ability for predicting more general patterns improves with scale and is thus obtained via training on text (rather than an inherent capability of untrained LLMs). Moreover, we show that studying foundation models through the lens of lossless compression sheds light on various essential aspects of language modeling:\n* ***In-context learning***: Compressing longer sequences is easier given the more available information (i.e., few-shot learning or meta-learning).\n* ***Tokenization***: In this view, a tokenizer is just an extra (manually conceived) compression step.\n* ***Generation***: One can sample from a compressor using a softmax over the lengths of the compressed data. We use this to empirically demonstrate the relation between stronger compressors and higher-quality generations.\n* ***Scaling laws***: Model size affects the downstream compression performance.\nFinally, we show that, surprisingly, *language* models are strong compressors for image and audio data, which provides evidence for the claim that LLMs learn to compress more general (algorithmic) patterns with scale (Mirchandani et al., 2023).\n\nHowever, as the reviewer points out correctly, we do not advocate for using LLMs as compressors in practice, as they have too many limitations (e.g., size and inference time) to be practical.  Rather, we advocate for using compression as an insightful tool for the evaluation of LLMs, and the compression-viewpoint as an insightful, complementary, theoretical angle.\n\nSuvir Mirchandani et al. Large Language Models as General Pattern Machines. https://arxiv.org/abs/2307.04721.\n\n\n**Can you add experiments with publicly available pre-trained models other than Chinchilla-70B?**\n\nYes, we added an experimental evaluation of the compression capabilities of LLaMA1-7B to Table 1. LLaMA1-7B performs comparably to the Chinchilla models, except for ImageNet data, where it achieves the best compression (most likely since it has seen images as part of the CommonCrawl dump, which makes up 67% of its training data).\n\nMoreover, we have open-sourced the code for reproducibility and will link it to the paper upon acceptance.\n\n\n**Why do the results for the generative modeling performance look rather poor?**\n\nFirst, note that the rows in Figures 3 and C.2 are treated independently by the model, which explains why the image generations look rather \"noisy\" across rows. Moreover, given that Chinchilla was only trained on text, we find it surprising that the model manages to complete the images somewhat meaningfully in the first place. Nevertheless, we agree that the generations do not look visually appealing due to the well-known mismatch between teacher-forcing at train time and autoregressive generation at test time. Thus, at test time, the errors accumulate, and the compression capabilities need to be immaculate to generate good results for long sequences, which is unlikely for the image and audio data that the LLMs have never seen during training. We added a more extensive discussion to the paper (Section 3.4) and also evaluated the generative capabilities of less powerful models (Chinchilla 1B and 7B) to Appendix B (showing that the generative quality improves with compression performance)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576368963,
                "cdate": 1700576368963,
                "tmdate": 1700576368963,
                "mdate": 1700576368963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9jZk5Cf0Ho",
                "forum": "jznbgiynus",
                "replyto": "XWpd2IAVz7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (2/2)"
                    },
                    "comment": {
                        "value": "**Why is it important to distinguish between compression rates that do and do not consider the model size?**\n\nGiven a dataset, one can achieve optimal lossless compression (in coding length without considering the model size) by simply storing the dataset and encoding each sequence as an index. We usually refer to this as \"overfitting\", i.e., the model stores all the sequences in its parameters but fails to generalize to new sequences. However, if we consider the model size, such a \"storage\" model would achieve a very poor compression rate (since it requires a lot of parameters). Conversely, a model that manages to compress the same dataset with a minimal set of parameters must be able to extract useful information from this data, which is what we commonly refer to as \"generalization\". Since we ultimately want to measure how well a model generalizes, not how well it stores information, we also measure the compression rate that accounts for the model size. In addition, compression rates that do not consider the model size might wrongly suggest that LLMs are preferable over general-purpose compressors, like gzip, to compress arbitrary files. Taking into account the model size shows that LLMs compression abilities come at the cost of \"a complex compression algorithm\".\n\n\n**Can you add a concrete example of how arithmetic coding works?**\n\nYes, we visualize arithmetic encoding in Figure 1. Arithmetic coding iteratively partitions the interval $I = [0, 1)$ according to a predictive model $P$ and an input string, i.e., $AIXI$ for Figure 1.\n\nFirst, we construct the intervals for $P(\\cdot)$:\n\n* $[0, 0.45)$ for $P(A) = 0.45$\n* $[0.45, 0.75)$ for $P(I) = 0.3$\n* $[0.75, 1)$ for $P(X) = 0.25$\n\nSince the first token is $A$, we set $I = [0, 0.45)$ and iterate. Thus, the intervals for $P(\\cdot | A)$ are:\n\n* $[0.2 * 0, 0.2 * 0.45) = [0, 0.09)$ for $P(A | A) = 0.2$\n* $[0.09, (0.2 + 0.6) * 0.45) = [0.09, 0.36)$ for $P(I | A) = 0.3$\n* $[0.36, (0.2 + 0.6 + 0.2) * 0.45) = [0.36, 0.45)$ for $P(X | A) = 0.2$\n\nSince the next token is $I$, we set $I = [0, 0.45)$ and so on. We terminate with $I = [0.322, 0341)$ for $AIXI$. Next, arithmetic encoding will compute the binary sequence corresponding to iteratively splitting the interval $[0, 1)$ in half until it is fully contained in $I$. \n\nConcretely, this yields the binary sequence.\n\n* $b0$ -> $[0, 0.5)$\n* $b01$ -> $[0.25, 0.5)$ \n* $b010$ -> $[0.25, 0.375)$\n* $b0101$ -> $[0.3125, 0.375)$\n* $b01010$ -> $[0.3125, 0.34375)$\n* $b010101$ -> $[0.328125, 0.34375)$\n* $b0101010$ -> $[0.328125, 0.3359375)$\n\nAs $[0.328125, 0.3359375)$ is fully contained in $I = [0.322, 0341)$, the compressed output is $0101010$, which consists of 7 bits as opposed to the 4 bytes used to encode $AIXI$. \n\nWe have extended the caption of the figure to clarify arithmetic coding further and included the above example in Appendix A."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576479882,
                "cdate": 1700576479882,
                "tmdate": 1700576479882,
                "mdate": 1700576479882,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hadkq09wc6",
                "forum": "jznbgiynus",
                "replyto": "9jZk5Cf0Ho",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission372/Reviewer_hCy6"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers",
                    "ICLR.cc/2024/Conference/Submission372/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission372/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission372/Reviewer_hCy6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my concerns, and for clarifying my questions. \n\nAs my major concerns have been addressed, it is only fair from my side to raise my score. \n\nHowever, I want to point out that while this work offers an intriguing perspective on large language models by examining them from a compression standpoint, the insights it provides, in my opinion, offers limited new understanding compared to what is already known from the \"prediction perspective\"."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission372/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686090702,
                "cdate": 1700686090702,
                "tmdate": 1700686090702,
                "mdate": 1700686090702,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]