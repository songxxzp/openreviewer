[
    {
        "title": "Stochastic Gradient Langevin Dynamics Based on Quantization with Increasing Resolution"
    },
    {
        "review": {
            "id": "S51mV5auFQ",
            "forum": "VJLD9MquPH",
            "replyto": "VJLD9MquPH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4810/Reviewer_JTTw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4810/Reviewer_JTTw"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses the quantized optimization to propose a stochastic descent learning equation, and combines it with SLD/SGLD to propose two alternative algorithms. The methods utilize Langvin SDE dynamics and aloow for controllable noise with an identical distribution without need for additive noise or adjusting minibatch size. Numerical experiments are carried out on CNN and ResNet-50 Models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper applies the quantized optimization theory, which is primarily used to reduce computational burden, to optimization algorithms and uses Langevin SDEs for analysis. The originality is high.\n2. Both theoretical analysis and experiments are presented, so this work is self-contained and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The uniform distribution assumption (Assumption 2) seems to be too stringent and unverifiable, since the quantization error $\\epsilon^q$ depends highly on iterates. Throughout the anaysis in paper this assumption is crucial, so it is better to try explaining its validity in detail. For example, the authors could experiment on some simulation/real datasets to see if this assumption is satisfied.\n2. The advantage of applying quantized optimization is not clearly stated. It would be better if clear motivation of using quantized method, or its computational or analytical benefits are claimed to convince readers.\n3. The key point of this paper is somewhat ambiguous. If the major contribution lies in theoretical analysis, the authors should emphasize it and conduct simulation experiments to validate the stochastic approximation and convergence results, rather than merely performing real data experiments; if the contribution lies in experimental results, models like CNN and ResNet may seem to be too simple. Larger datasets and model structures should be tested to verify the robustness and efficiency of the proposed method.\n4. Some typos:\n - lines after an equation should not start with comma, e.g. line after (12) (15) (18)...\n - 'Langvine' should be 'Langevin' in line before (12);\n - 'evalutae' should be 'evaluate' in line before (9);\n - 'lsyers' should be 'layers' in Table 1."
                },
                "questions": {
                    "value": "1.Can you explain the claim in 'Appliance to Other Learning Algorithms'? It seems ADAM/ADAMW type methods require information of past iterates, so these may result in past-dependent SDEs which is different from Langevin dynamics."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4810/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4810/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4810/Reviewer_JTTw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698420837127,
            "cdate": 1698420837127,
            "tmdate": 1699636464098,
            "mdate": 1699636464098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LOeuQBSyS2",
                "forum": "VJLD9MquPH",
                "replyto": "S51mV5auFQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer for the reviewer  JTTw (1)"
                    },
                    "comment": {
                        "value": "We appreciate the thoroughness and thoughtfulness of your comments.\nParticularly, we'd like to answer for the 1 and 2 in the weakness part as a unified response.\n\n### Opinion to Weakness\n\nThe uniform distribution assumption (Assumption 2) is a widely accepted postulation(almost regarded as an axiom) for the distribution of the quantization error(or noise) for tens of years in signal processing.\nTherefore, the distribution of the quantization error at a specific time concerning a particular point being a uniform distribution is a valid assumption even though the system contains a recursive loop. \nMeanwhile, as the reviewer states, the quantization error for a fixed state during a time interval in the recursive system, such as the learning equation in DNN, does not follow the uniform distribution.\nThus, we should analyze the distribution of such quantization error by the summation or integration for the time interval, and we provided a detailed analysis based on the classical (or Lindberg-L'evy) central limit theorem in Appendix C.  \n\nHeretofore, the analysis of the stochastic learning equation, such as SGD and ADAM, depends on what kind of CLT applies to the summation of the gradient noise. \nSince the calculation of the distribution for the gradient noise through a rigorous analysis is so hard work, researchers have intuitively assumed the distribution of the gradient error from the noise trends as follows:\n\n1. The assumption of the identical independent distributed gradient noise with the finite second moment implies that the gradient noise is the Gaussian noise random variable by the classical CLT.  This assumption yields the standard Langvine SDE for the learning equation as follows:\n$$\n\\boldsymbol{x}\\_{\\tau+1} = \\boldsymbol{x}\\_{\\tau} - \\lambda \\nabla f(\\boldsymbol{x}\\_{\\tau}) + \\sqrt{\\lambda} \\boldsymbol{\\xi}\\_{\\tau}\n\\implies d\\boldsymbol{X}\\_t = -\\nabla f(\\boldsymbol{X}\\_t) + \\sigma \\sqrt{\\lambda/B} d\\boldsymbol{B}\\_t\n$$\n, where $\\boldsymbol{\\xi} \\in \\mathbb{R}^d$ denotes the Gradient noise defined as $\\boldsymbol{\\xi} \\triangleq \\nabla f(\\boldsymbol{x}\\_{t}) - \\nabla \\tilde{f}\\_{\\tau}(\\boldsymbol{x}\\_{t})$ with the assumption of $\\mathbb{E} \\boldsymbol{\\xi} = 0$ and $\\mathbb{E} \\boldsymbol{\\xi}^2 = \\sigma/B$, and other notations are defined in the manuscript. \n\n2. According to the data analysis of the gradient noise representing a heavy-tailed or long-tailed noise, the distribution of the gradient noise is a symmetric $\\alpha$-stable (S$\\alpha$S) distribution derived from the general central limit theorem(GCLT).\nThis assumption yields a Le'vy SDE for the stochastic learning equation as follows:\n$$\n\\boldsymbol{x}\\_{\\tau+1} \n= \\boldsymbol{x}\\_{\\tau} - \\lambda \\nabla f(\\boldsymbol{x}\\_{\\tau}) + \\lambda^{1/\\alpha} \\left( \\lambda^{\\frac{\\alpha-1}{\\alpha}} \\sigma \\right) \\boldsymbol{\\xi}\\_{\\tau}\n\\implies\nd \\boldsymbol{X}\\_t = - \\nabla f(\\boldsymbol{X}\\_t)dt + \\lambda^{(\\alpha-1)/\\alpha} \\sigma d\\boldsymbol{L}\\_t^{\\alpha}\n$$\n, where $[\\boldsymbol{\\xi}\\_{\\tau}]_i \\sim \\mathcal{S}\\alpha\\mathcal{S}(1)$ and $\\boldsymbol{L}\\_t^{\\alpha} \\in \\mathbb{R}^d$ denotes $\\alpha$-stable Le'vy process. \nConsequently, the conventional analysis of the SDE approximation for the stochastic learning equation depends on the iterative statistical property of the gradient noise. \n\nWhereas, the quantized learning we proposed depends on the quantization error with the axiomatic uniform distribution containing finite variance, not on the gradient error. \nAs you note, the random variable with the uniform distribution representing a zero expectation and a finite variance yields a random variable with Gaussian distribution derived by the accumuated quantization error to a unit epoch, through the classical CLT. \nThus, we claim that the proposed quantization-based scheme can provide the standard Langevin SDE for the stochastic learning equation, regardless of the gradient noise as follows:\n$$\n\\boldsymbol{x}\\_{\\tau+1}^Q = \\boldsymbol{x}\\_{\\tau}^Q - \\lambda \\nabla f(\\boldsymbol{x}\\_{\\tau}^Q) + Q_p^{-1}(\\tau) \\boldsymbol{\\varepsilon}\\_{\\tau}^q\n\\implies\nd\\boldsymbol{X}\\_t = -\\nabla f(\\boldsymbol{X}\\_t) + \\sigma \\sqrt{\\lambda} dW\\_t\n$$\nWe agree with the reviewer's comment on the validity of the assumption that the quantization error follows uniform distribution and other statistical properties. \nHowever, in our opinion, this work is beyond the scope of the paper.\nIn our paper, we propose a novel learning equation and provide the fundamental idea, additional operational conditions to avoid early paralysis, convergence properties of the proposed algorithm, and experimental results.\nThe analysis and providing empirical proof for the proposed algorithm would be another topic for us."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219289951,
                "cdate": 1700219289951,
                "tmdate": 1700501850821,
                "mdate": 1700501850821,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o5LbAM8Xh4",
                "forum": "VJLD9MquPH",
                "replyto": "S51mV5auFQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer for the reviewer JTTw (2)"
                    },
                    "comment": {
                        "value": "#### Motivation and Advantages of Quantization\n\nGenerally, engineers have considered quantization as one of the effective schemes providing less computation burden and fast signal processing in the device with low computation power. \nMeanwhile, we consider quantization as a scheme for optimization, inspired by direct search algorithms such as the generating set search(GSS) algorithm.\nMost of all, if we can establish the stochastic learning algorithm as Langevin dynamics rigorously, we can develop a more effective algorithm satisfying global optimization based on not only classical thermodynamics but also quantum mechanics. \nFor instance, the analysis based on Le\u2019vy-SDE considers the avoidance of local minima as a jump process. \nWhereas, Langevin-SDE-based analysis can illustrate it as various physical phenomena, such as a statistical property in thermodynamics, tunneling effect in quantum mechanics, and Laplace method accompanied by weak convergence.\n\n\n### Answer for Questions\n\nAccording to the recent research on the stochastic learning equation[1][2][3], the gradient noise of ADAM still represents heavy-tailed noise despite the generation of an average gradient driven by momentum operation.\nHence, as the reviewer states, it would be reasonable to analyze the stochastic differential property of ADAM/ADAMW as the Le'vy-driven SDE. \n\nWhereas, since the quantization-based learning forces the accumulation of quantization error to be a Gaussian random variable by the classical CLT, we can obtain the Langevin SDE even though the search direction is not a gradient descent. \n\n### Fixing Typos\n\nI correct the typos those the reviewer points out, and submit the revised version of the manuscript.\n\n## Reference\n\n[1] Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Hoi, and E. Weinan. \"Towards theoretically understanding why SGD generalizes better than ADAM in deep learning\", In Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS'20). Curran Associates Inc., Red Hook, NY, USA, Article 1787, 21285\u201321296. (2020).\n\n[2] Simsekli, Umut, Levent Sagun and Mert G\u00fcrb\u00fczbalaban. \u201cA Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks.\u201d ArXiv abs/1901.06053 (2019)\n\n[3] Nguyen, Thanh Huy, Umut Simsekli, Mert G\u00fcrb\u00fczbalaban and Ga\u00ebl Richard. \u201cFirst Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise.\u201d Neural Information Processing Systems (2019)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219398084,
                "cdate": 1700219398084,
                "tmdate": 1700221691600,
                "mdate": 1700221691600,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zhGHl6ECwB",
            "forum": "VJLD9MquPH",
            "replyto": "VJLD9MquPH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4810/Reviewer_wZWw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4810/Reviewer_wZWw"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a quantization scheme for existing optimization algorithms (SGD, ADAM). The quantization error can be treated as an additive noise thus improves the performance of the algorithm. With increasing resolution, the convergence of the quantized optimization algorithms can be proven. Numerical experiments with CV tasks show that optimization algorithms based using this quantization scheme have better performance and robustness compared with existing algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper uses the quantization error, the byproduct of optimization at the implementation level, as an approach of improving the performance of the optimization algorithm at the algorithm design level.  It contains the necessary theoretical analysis of linking the quantization error to additive noise and the convergence of the algorithm, as well as the numerical evidence demonstrating the superior performance of the quantized algorithm. The ideas in the paper are organized well."
                },
                "weaknesses": {
                    "value": "It appears to me that the writing of the paper could be improved:\n* Certain notation in the manuscript lacks consistency. Notably, when $\\tau$ is first introduced in equation 1 and 2, I thought it would be the index of the batches and that $\\tau < B$. However, in equation 10, $\\tau$ can be arbitrarily large. \n* It appears to me that the \u2018transition probability\u2019 in theorem 3.3 should be called \u2018transition kernel\u2019, as the probability of transiting from one point to another should be zero. \n* The appendix contains a lot of good supplementary information. It is a shame that the main text does not refer to the appendix.\n* Should the $Q_p^{-1}(t_e + \\tau)$ on top of equation (15) be $Q_p^{-1}(t_e + \\tau/B)$?\n* Typo on top of equation (9): evalutae\n* Typo in table 2: GTXTi\n\nThis paper uses a complicated quantization scheme while providing no detail regarding the implementation of the algorithm in the text (data type, time and memory it takes to train the network, etc.). Without this information, it is hard to tell whether the quantized algorithm benefits from quantization in terms of reducing computational burden and simplifying data processing. Consequently, between the injected additive noise (easy to implement) and the quantized algorithms (hard to implement), it is hard to tell which one is better as they are supposed to improve the performance in the same way. This makes assessing the significance of this work hard."
                },
                "questions": {
                    "value": "* Following the weaknesses, I wonder if the authors could provide more information regarding the implementation of the quantized algorithm. I checked the attached program. If I am not mistaken, the quantization is neither implemented via torch API nor casting high-precision tensors into low-precision ones.\n* I wonder if the authors checked experimentally that if the performance gain from the quantization can be reproduced by injecting additive noise. \n* In the introduction, when the Non-Convex Optimization is introduced, the authors mention that the quantized algorithms have better performance in certain problems than the MCMC algorithms. I wonder if the authors could provide some reference for that."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4810/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4810/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4810/Reviewer_wZWw"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771399620,
            "cdate": 1698771399620,
            "tmdate": 1699636464014,
            "mdate": 1699636464014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dYSf5TUDzA",
                "forum": "VJLD9MquPH",
                "replyto": "zhGHl6ECwB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer for the reviewer wZWw (1)"
                    },
                    "comment": {
                        "value": "We appreciate the thoroughness and thoughtfulness of your comments.\n\n### Answer for the weakness point\n(1) In section A.2 of the Appendix, we denote the $\\tau$ as the discrete-time index based on the index of mini-batches, so $\\tau$ can increase to infinity as the epoch increases.\nAdditionally, we denoted all the notations used in the Appendix of the manuscript, and we established the set for the parameters such as $\\forall \\tau \\in \\mathbb{Z}[0, B)$, when the parameters require a range. \nAlthough our intention, the mini-batch-based time index still confuses readers, as the reviewer comments.\n\nConsequently, we establish the index of mini-batch $\\tau_b \\in \\mathbb{Z}[0, B)$ and the time index based on the mini-batch $\\tau \\in \\mathbb{Z}^+$ such that $\\tau_b = \\tau \\% B$. \n\nWe revise all the equations employing $\\tau$ carefully in the manuscript to prevent confusion as the reviewer comments and add the definition of $\\tau_b$ in section A.1 of the Appendix.\n\n(2) The reviewer's comment that the transition probability is not a correct statement for $p(t, \\boldsymbol{x}, t+\\bar{\\tau}, \\boldsymbol{x}^*)$ is right. \nWe modify it as the transition probability density since we derive the SDE for the proposed algorithm in Lemma 3.2. \n\n(3) We organized the manuscript providing our fundamental idea on the main pages and analysis and proof of theorems elaborating the idea in the Appendix.\nThe reviewer's comments encouraged us to raise our willingness to continue the research. I appreciate the reviewer's comments.\n\n(4) Yes, $Q_p^{-1}(t_e + \\tau)$ should be equal to $Q_p^{-1}(t_e + \\tau/B)$ for all $\\tau \\in [0, B)$. In other words, $Q_p(t)^{-1}$ should be constant for a unit epoch and decrease as the epoch increases. \nBased on this condition, we can derive a Gaussian random variable to the epoch time from the uniform distributed quantization error through the central limit theorem(CLT).  \n\n(5 and 6) We revise the typo as the reviewer's comments.\n\n### Answer to the question\n(1) As the reviewer comments, we implemented the presented quantization without PyTorch API, whereas we implemented other algorithms such as ADAM with PyTorch.\nSince our research aims to verify that quantization is another effective optimization scheme, we implemented the quantization with our own Python code for our purpose.\nMany researchers consider the quantization in optimization algorithms for AI as an effective methodology for fast and light computation. \nAs a result, the quantization API in PyTorch is not appropriate for our research.\nHowever, we're going to improve our research and the code more practically using widely used quantization API.\n\n(2) Unfortunately, we didn't compare the proposed algorithm to other models with an injecting additive noise.\n\nHowever, we guess that comparing the proposed model to the additive noise model is an interesting theme. \nWe can write the additive noise model for a mini-batch-based time index $\\tau$ such that\n$$\n\\begin{aligned}\n\\boldsymbol{x}\\_{\\tau+1} &= \\boldsymbol{x}\\_{\\tau} - \\lambda \\nabla\\_{\\tau\\_b} \\tilde{f}({x}\\_{\\tau} ) + \\sigma(\\tau) \\boldsymbol{\\xi}\\_{\\tau}\\\\\\\\\n&= \\boldsymbol{x}\\_{\\tau} - \\lambda (\\nabla\\_{\\tau\\_b} \\tilde{f}({x}\\_{\\tau}) + \\nabla f(\\boldsymbol{x}\\_t) - \\nabla f(\\boldsymbol{x}\\_t) )  + \\sigma(\\tau) \\boldsymbol{\\xi}\\_{\\tau}, & \\nabla f(\\boldsymbol{x}\\_t) \\triangleq \\mathbb{E} \\nabla\\_{\\tau_b} \\tilde{f} ({x}\\_{\\tau} )  \\\\\\\\\n&= \\boldsymbol{x}\\_{\\tau} - \\lambda \\nabla f(\\boldsymbol{x}\\_t) + \\lambda \\boldsymbol{\\eta}\\_{\\tau} + \\sigma(\\tau) \\boldsymbol{\\xi}\\_{\\tau}, &\\boldsymbol{\\eta}\\_{\\tau} \\triangleq \\nabla f(\\boldsymbol{x}\\_t) - \\nabla\\_{\\tau\\_b} \\tilde{f}({x}\\_{\\tau}), \n\\end{aligned}\n$$\nwhere $\\xi_{\\tau} \\in \\mathbb{R}^d$ denotes an additive noise such as $\\xi_{\\tau} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I})$, $\\tau_b \\in \\mathbb{Z}[0, B)$ denotes the mini-batch index defined as above, and all other notations in the equation are defined in the manuscript.\n\nIf we set the expectation gradient as the average gradient for all samples defined in equation (2) in the manuscript, we can obtain the first moment of the gradient noise such that\n$$\n\\mathbb{E}\\boldsymbol{\\eta}\\_{\\tau} \n\\triangleq \\frac{1}{B} \\sum\\_{{\\tau}\\_b=0}^{B-1} \\boldsymbol{\\eta}\\_{\\tau_b} \n= \\frac{1}{B} \\sum\\_{{\\tau}\\_b=0}^{B-1} \\nabla f(\\boldsymbol{x}\\_t) - \\frac{1}{B} \\sum_{{\\tau}\\_b=0}^{B-1} \\nabla_{\\tau\\_b} \\tilde{f}({x}\\_{\\tau}) = \\nabla f(\\boldsymbol{x}\\_t) \\cdot \\frac{1}{B} \\cdot B - \\nabla f(\\boldsymbol{x}\\_t) = 0, \\quad \\quad\n\\because \\frac{1}{B} \\sum\\_{{\\tau}\\_b=0}^{B-1} \\nabla\\_{\\tau\\_b} \\tilde{f}({x}\\_{\\tau}) = \\nabla f(\\boldsymbol{x}\\_t).\n$$\n\nEquation (1) implies that the expectation of the summation of the gradient noise and additive noise $\\bar{\\boldsymbol{z}}\\_{\\tau} \\triangleq \\lambda \\boldsymbol{\\eta}\\_{\\tau} + \\sigma(\\tau) \\boldsymbol{\\xi}$ is zero."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584586948,
                "cdate": 1700584586948,
                "tmdate": 1700667345807,
                "mdate": 1700667345807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TL3m4A6TMC",
                "forum": "VJLD9MquPH",
                "replyto": "zhGHl6ECwB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer for the reviewer wZWw (2)"
                    },
                    "comment": {
                        "value": "However, if we don't assume the independent assumption,  we can obtain the second moment of $\\bar{\\boldsymbol{z}}\\_{\\tau}$ ambiguously such that\n$$\n\\mathbb{E} \\bar{\\boldsymbol{z}}\\_{\\tau} \\bar{\\boldsymbol{z}}\\_{\\tau}^T\n= \\mathbb{E} (\\lambda \\boldsymbol{\\eta}\\_{\\tau} - \\sigma\\boldsymbol{\\xi}\\_{\\tau}) (\\lambda \\boldsymbol{\\eta}\\_{\\tau} - \\sigma\\boldsymbol{\\xi}\\_{\\tau})^T\n= \\lambda^2 \\mathbb{E} \\boldsymbol{\\eta}\\_{\\tau}\\boldsymbol{\\eta}\\_{\\tau}^T - 2 \\lambda \\sigma(\\tau) \\mathbb{E} \\boldsymbol{\\eta}\\_{\\tau} \\boldsymbol{\\xi}\\_{\\tau}^T + \\sigma^2 (\\tau) \\boldsymbol{I}.\n$$\n\nIf we assume that the $\\sigma(\\tau) \\downarrow 0$ as $\\tau \\uparrow \\infty$ and the finite second moment of the gradient noise such that $\\| \\mathbb{E} \\boldsymbol{\\eta}_{\\tau} \\boldsymbol{\\eta}_{\\tau}^T \\| \\leq \\bar{\\sigma} < \\infty$, we recognize the approximation of the SGD is a conventional Le'vy-SDE. \n\nTherefore, we conjecture that the additive noise may have little effect on the learning performance.\n\nMeanwhile, for the proposed quantization scheme, we obtain\n$$\n\\begin{aligned}\n\\boldsymbol{x}\\_{\\tau+1}^Q \n&= \\boldsymbol{x}\\_{\\tau}^Q - [\\lambda \\nabla\\_{\\tau_b} \\tilde{f}({x}\\_{\\tau} )]^Q \\\\\\\\\n&= \\boldsymbol{x}\\_{\\tau}^Q - [\\lambda \\nabla f(\\boldsymbol{x}\\_t) + \\lambda \\boldsymbol{\\eta}\\_{\\tau}]^Q\\\\\\\\\n&= \\boldsymbol{x}\\_{\\tau}^Q - [\\lambda \\nabla f(\\boldsymbol{x}\\_t)]^Q + \\overline{[\\lambda \\boldsymbol{\\eta}\\_{\\tau}]}^Q, \\quad &\\because [\\lambda \\nabla f(\\boldsymbol{x}\\_t) + \\lambda \\boldsymbol{\\eta}\\_{\\tau}]^Q \\equiv [\\lambda \\nabla f(\\boldsymbol{x}\\_t)]^Q - \\overline{[\\lambda \\boldsymbol{\\eta}\\_{\\tau}]}^Q \\\\\\\\\n&= \\boldsymbol{x}\\_{\\tau}^Q - \\lambda \\nabla f(\\boldsymbol{x}\\_t) + Q_p^{-1}(t) \\left(\\boldsymbol{\\varepsilon}\\_{\\tau}^Q + \\boldsymbol{\\nu}\\_{\\tau} \\right), \\quad &\\because \\boldsymbol{\\nu}\\_{\\tau} \\triangleq \\lfloor \\bar{\\boldsymbol{\\eta}} Q\\_p^{-1}(t)+ 0.5\\rfloor.\n\\end{aligned}\n$$\nThe equation represents that, even if the quantization error and the normalized gradient error are not independent, we can control the variance using the quantization parameter $Q_p^{-1}(t)$.   \n\nFurthermore, supposing that the quantization error and the normalized gradient noise are independent, we can establish the standard Langevin SDE for SGD through the classical or Lindberg CLT.   \n\nSince the Langevin SDE can satisfy the Lagrange method for the global convergence on a real-number space, we conjecture the performance of the proposed algorithm would be superior.\n\n(3) We're afraid we cannot provide any information to the some reference for quantization-based non-convex optimization, due to the guidelines of the ICLR committee. \nBriefly, the quantization for the objective function enables to decrease of the level set or search range exponentially, under the numerical number system. \nThe dynamics of optimization based on the quantization resembles the quantum annealing.\nHowever, it does not require a base function for Hamiltonian which the quantum annealing algorithm requires."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586125706,
                "cdate": 1700586125706,
                "tmdate": 1700662855069,
                "mdate": 1700662855069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q9aCxQjsbx",
                "forum": "VJLD9MquPH",
                "replyto": "TL3m4A6TMC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4810/Reviewer_wZWw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4810/Reviewer_wZWw"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the kind and detailed reply. Thank you. Although the idea behind this paper is interesting, further numerical tests and discussions regarding the implementation are still required to support it. I would like to keep my score unchanged."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706339532,
                "cdate": 1700706339532,
                "tmdate": 1700706339532,
                "mdate": 1700706339532,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "94DsLZhemb",
            "forum": "VJLD9MquPH",
            "replyto": "VJLD9MquPH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4810/Reviewer_qh6T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4810/Reviewer_qh6T"
            ],
            "content": {
                "summary": {
                    "value": "The key argument of the paper is that a variant of stochastic gradient Langevin dynamics can be realized by combining a quantization scheme with standard SGD. In particular, this avoids the explicit injection of noise into the gradient descent scheme."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The scheme proposed by the authors which links quantization with SGLD is novel and could be developed in future works. In situations where there is a pre-existing need to quantize the result, this would implicitly lead to benefits.\n\nThe method does seem to show improvement empirically, with caveats (see below)."
                },
                "weaknesses": {
                    "value": "There are many issues with the writing. In general the clarity of explanations could be improved, and the number of typos has a substantial effect on the readability of the document. See Questions for an exhaustive list. These must be fixed in order for me to recommend acceptance.\n\nThe empirical evidence is not entirely convincing for me. The loss curves (Figure 2) do not seem to show clear improvement, and it is difficult to assess Table 1 without standard deviations. Furthermore, it seems a bit strange to me that the SGLD formulation would yield any improvement to SGD-type algorithms, since prior practical performance was not stellar.\n\nThe convergence results in Theorems 3.3, 3.4 are quite strange. See questions below.\n\nDue to the above issues, I cannot recommend acceptance for this paper at the moment."
                },
                "questions": {
                    "value": "Why is the metric in Theorem 3.3 chosen to be the overlap of the kernels $p$? Why is this significant in practice, compared to e.g. mean-square convergence or convergence in a probabilistic sense such as KL?\n\nAssumption 4 should be specifically referring to the local optimal point? A definition of local optimum in this case should be given.\n\nIn my opinion the results should be stated independently of the \u201cmini-batch\u201d and \u201cepoch\u201d formula, which merely complicates the presentation of the core idea.\n\nCould the authors provide some rough estimation of standard deviations in Table 1?\n\nWhy do the training curves performance in Figure 2 not appear to match that in Table 1?\n\nSee below for a list of detailed questions regarding the writing and proofs:\n\n**Typos:**\n\nFrame -> framework (page 1)\n\nThere should be a space between words if followed by parentheses\n\nLines should not start with commas, e.g. after Eq 1, Eq 3\n\nOwing to the quantization error as the i.i.d. White noise -> unclear what this means\n\nIncrements -> increment (page 5)\n\nWhat is the point of discussing the transformation approach in such depths if it is not explored further? This point can be made more succinctly.\n\nAppliance -> application (page 6)\n\nIn equation 20, what is C_{o1}? There must additionally be some typo, since why do both upper and lower case T appear in the equation?\n(22) should be less than or equal to.\n\nPage 15: \u201cWe\u201d -> \u201cwe\u201d\n\nDefinitions of floor/ceiling should not have $\\forall x \\in \\mathbb{R}$ in the set.\n\nEquation 25 is missing a sup on $v$.\n\nEquation 27 seems redundant in light of Equation 28. Likewise Eq. 30 and Eq. 32s, and Eq. 37, 39.\n\nI cannot follow the derivation in the first part of Equation 42, as there appear to be some typos (e.g. the second to last equality cannot be true, and the summation in the first equation does not make sense). Such an argument is not necessary, anyway, and one can simply appeal to symmetry.\n\nPage 22: Indexes -> indices\n\nLemma: Auxiliary 1 is standard and there is no need to include the proof."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803456113,
            "cdate": 1698803456113,
            "tmdate": 1699636463916,
            "mdate": 1699636463916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rTSYdG4emg",
                "forum": "VJLD9MquPH",
                "replyto": "94DsLZhemb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer for the reviewer qh6T (1)"
                    },
                    "comment": {
                        "value": "We appreciate the thoroughness and thoughtfulness of your comments.\n\n## Answer for the questions\n\nWe answer **the first and the second questions** in the \"weakness\" section.\n\nTheorem 3.3  describes that the distribution of a transition probability density in the proposed algorithm converges without any convex assumption, for time increasing. \nThe convergence without convex assumption illustrates that the proposed algorithm can escape a local minimum, proportional to the transition probability density. \nFrom a mathematical standpoint, we conjecture that asymptotic convergence in a strong topology, termed mean-square convergence by the reviewer, does not guarantee global convergence unless a convex assumption is present.\nAlthough a lot of researchers claim the global convergence of the learning algorithm, most of them require alternative or hidden convex conditions in their proofs.\nWe claim that the rigorous proof for the global convergence requires Laplace's method, which is the steepest descent method on the real number space.\nFor this purpose, we should weak convergence of the algorithm as described in theorem 3.3. \nAdditionally, based on theorem 3.3, we should prove the proposition that the distribution of the transition probability density converges to a stationary distribution.\nHowever, we couldn't provide the proof in this manuscript.\n\nAdditionally, as the reviewer comments, the transition probability is not the correct expression, so we modify it as the transition probability density.  \nSince we derive the SDE for the proposed algorithm in Lemma 3.2, the transition probability density is more correct than the transition probability kernel.\n\nThis is **the answer to the third question** : \nThe reviewer's comment is right. \nWe should write equation (130) as the definition of theorem 3.4 in the manuscript.\nHowever, we removed the equation due to the length limitation of the manuscript.\nWe insert equation (130) at the bottom of theorem 3.5 to clarify the theorem 3.5. \n\nThis is **the answer to the fourth question** :\nIn contrast to conventional research on stochastic learning equations, we employ a double-sided time index system that distinguishes between the unit of a mini-batch and the unit of an epoch.\nComparing the SDE approximation based on a gradient noise in the conventional research and the proposed quantization error, we'd like to claim that the proposed approximation is based on a more reasonable assumption using the double-sided time system.\n(In the conventional research employing the gradient noise as follows:\n$$\n\\boldsymbol{\\eta}\\_{\\tau} \\triangleq \\frac{1}{B} \\sum_{\\tau_b=0}^{B-1} \\nabla\\_{\\boldsymbol{x}}\\tilde{f}\\_{\\tau_b}(\\boldsymbol{x}\\_{t\\_e+\\tau_b/B}) - \\nabla\\_{\\boldsymbol{x}}\\tilde{f}\\_{\\tau_b} (\\boldsymbol{x}\\_{t\\_e+\\tau/B}), \\quad \\tau\\_b \\in \\mathbb{Z}[0, B)\n$$\nwhere all notations are defined in the manuscript, specifically in equation  (2). \nCan we ensure that the distribution of $\\boldsymbol{\\eta}_{\\tau}$ is symmetry and the second moment represents a finite value? Recent research indicates that, under the empirical symmetry assumption, the second and higher moments are finite values. \n\nMeanwhile, the distribution of the quantization error is symmetric and involves finite variance under the axiomatic definition. Whereas, the distribution of the quantization error is symmetric and involves finite variance under the axiomatic definition.)\n\nIn the proposed double-sided time system, we consider the discrete stochastic learning equation as an infinitesimal for the SDE and derive the SDE applying the (Lindberg-L'evy's) central limit theorem(CLT). \nHerein, we establish the SDE on the continuous time-space based on the epoch unit, not on the mini-batch unit.\nFrom the practical point of view, we don't stop learning in the middle of the epoch and obtain a learned AI model after the finish of learning at an epoch.\nConsequently, we guess that the proposed double-sided time system is reasonable. \nFurthermore, the proposed double-sided time system is not an original idea but is a conventional time system in research of stochastic approximation.\nThe reviewer can see this double-sided time system in Kushner's work[1].\n\nWe cannot understand that there exists a discrepancy in the performance curve in training represented in Figure 2. \nWould you misunderstand the algorithm's name in the legend, due to the small picture?"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502063464,
                "cdate": 1700502063464,
                "tmdate": 1700586502595,
                "mdate": 1700586502595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RrkMA1AGnu",
                "forum": "VJLD9MquPH",
                "replyto": "94DsLZhemb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer for the reviewer qh6T (2)"
                    },
                    "comment": {
                        "value": "## Correction of Typos \n1. I correct the typos the reviewer refers to (1)(2)(3)(5)(7)(9)(10)(11)(14) in the section of Typos, respectively. \n   Please, review the revised paper to verify the correction of typos in the previous version.\n\n2. For (4), we rewrite the sentence as follows:\n   \"~, owing to the quantization error as a uniformly distributed vector-valued random variable. \"\n\n3. For (6), accepting the reviewer's comment, we rewrite the statements for the transformation succinctly.\n\n4. For (8), $C$ is typo of $C_{o1}$, so we correct it. As the reviewer comments, (20) is less than or equal, so we rewrite it.  The notation $\\tau$ is the time index on the unit of mini-batch and the large $T$ denotes a final time index on the unit of epoch. We define those notations in section A.1 of the Appendix. Furthermore, in the same section, we define the relation between the epoch-based continuous time index $t$ and the mini-batch-based discrete time index $\\tau$  such that $\\tau = t \\times B$.\n\n5. For (12),  as the reviewer comments, those equations are redundant. Those are on purpose. From the brief formulation such as equations (27), (37), and (39), we can obtain the result that some expectations based on the different random variables such as $z$ and $\\epsilon^q$ are equivalent. With these equivalents, we prove the theorem for the dithering effect such as equation (48).\n\n6. For (20), I fully agree with the reviewer's comment. When we check the equation, we recognize the expansion of the equation with absurd logic. Based on the reviewer's comment, we evaluate the result of the equation straightforwardly through the symmetry assumption, which we previously mentioned.\n\n7. For (15), following the reviewer's comment, we remove the proof of lemma: auxiliary. \n\n## Reference\n[1] Harold J. Kushner. \"On the Weak Convergence of Interpolated Markov Chains to a Diffusion.\" Ann. Probab. 2 (1) 40 - 50, February, 1974. https://doi.org/10.1214/aop/1176996750"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502144473,
                "cdate": 1700502144473,
                "tmdate": 1700502327099,
                "mdate": 1700502327099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]