[
    {
        "title": "Compositional Preference Models for Aligning LMs"
    },
    {
        "review": {
            "id": "zD3fXvJx6X",
            "forum": "tiiAzqi6Ol",
            "replyto": "tiiAzqi6Ol",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2602/Reviewer_dDjG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2602/Reviewer_dDjG"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces \u2018compositional preference models\u2019 (CPMs) that re meant to overcome issues of transparency and scalability found in regular preference models. CPMs decompose preferences into interpretable features, and subsequently aggregates them (with LR). Generally, results show improvement in generalization and avoidance of over-optimization, which are not themselves \u2019transparency and scalability\u2019."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Although a minor strength, the collection of papers in sections 1, 2, and 6 are sufficient, relatively recent, and relevant.\n- The compositional extension, though simple, is technically a novel contribution that appears to provide several benefits.\n- The setup for best-of-$n$ sampling seems fair. The full set of 25.6K responses, and code, would be appreciated.\n- the use of a separate LLM for evaluation is appreciated."
                },
                "weaknesses": {
                    "value": "- Although somewhat minor, the use of logistic regression will naturally cause some confusion, especially to those who want an end-to-end trainable model for this task. Other models should have been attempted.\n- Section 3.1 should be more informative as to the nature of features _c_ and how their set is identified, selected, or defined. This should include both the specific list in Sec 4.1 as well as guidance for other tasks, in general.\n- Although another minor weakness, at least one other LLM  should have been used for extraction (e.g., one of the Llamas)\n- Very minor, but please correct the various issues with your references including capitalization and completeness (e.g., Amodei suffers from both \u2014 use brackets around {AI} and provide full paper details)"
                },
                "questions": {
                    "value": "- The definition of \u2018model robustness\u2019 in Sec 4.2 seems incomplete \u2014 surely a factor is the domain or scenario in which the model is to be deployed or evaluated, too?\n- Would it be possible to re-arrange the columns of Fig2a and Fig2b so Standard comes left-most (first)?\\\n- Would there be value in actually performing human evaluations, despite the findings of best-of-n sampling in related work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698435322505,
            "cdate": 1698435322505,
            "tmdate": 1699636198961,
            "mdate": 1699636198961,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tZoiO22HPl",
                "forum": "tiiAzqi6Ol",
                "replyto": "zD3fXvJx6X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2602/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dDjG"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the feedback and constructive comments. We hope that our responses to your questions address most concerns you may have and strengthen the contribution of our work.\n\nStrengths:\n> The full set of 25.6K responses, and code, would be appreciated.\n\nThanks for your suggestion! We updated the supplementary file.\n\nWeaknesses:\n> Although somewhat minor, the use of logistic regression will naturally cause some confusion, especially to those who want an end-to-end trainable model for this task. Other models should have been attempted.\n\nWe appreciate your feedback regarding the selection of the machine learning model.\nOur decision to employ logistic regression was in order to favor the interpretability and intuitiveness of CPM.\nExisting literature supports the effectiveness of preference prediction using a linear combination of scores [1, 2].\n\nAlthough there are alternative models for preference, we believe logistic regression is the most natural and well-studied general method that can be employed for this purpose.\n\n> Section 3.1 should be more informative as to the nature of features c and how their set is identified, selected, or defined. This should include both the specific list in Section 4.1 as well as guidance for other tasks, in general. \n\nWe appreciate your feedback on the impact of prompts and features. We have incorporated your suggestion by adding a general principle for defining features in Section 3.1. In general, the features should be diverse enough to cover a broad range of preferences, while avoiding redundancy to maintain efficiency and interpretability.\n\nIn addition, we further investigated the impact of feature description to the CPM's performance by employing GPT-3.5 to paraphrase the original descriptions in Table 7 (For details of this experiment, please refer to Appendix F). The average win-rate (described in Section 4.5) of CPM-Flan-T5 using these augmented prompts was $0.717$ with a standard error of $0.023$, statistically similar to the original performance in Table 1 ($0.742$ with a standard error of $0.034$). This implies that the CPM's performance is not too sensitive to the specific wording of the prompts. \n\n> Very minor, but please correct the various issues with your references including capitalization and completeness (e.g., Amodeisuffers from both \u2014 use brackets around {AI} and provide full paper details)\n\nWe updated the references, thanks for the suggestion!\n\nQuestions:\n> The definition of \u2018model robustness\u2019 in Sec 4.2 seems incomplete \u2014 surely a factor is the domain or scenario in which the model is to be deployed or evaluated, too?\n\nWe totally agree that the overall quality of a preference model depends very much on the scenario in which it will be exploited, but in this section we focus on a narrower notion of robustness, which we can evaluate independently of its downstream usage, namely to what extent the PM is sensitive to the selection of data inside a given preference dataset. Such a notion of robustness is a necessary (although not sufficient) condition for a reliable application of the PM to the downstream task, and section 4.2 argues that the CPM is superior to a standard PM on this dimension.\n\n> Would it be possible to re-arrange the columns of Fig2a and Fig2b so Standard comes left-most (first)?\n\nThank you for the suggestion, we updated the figures.\n\n> Would there be value in actually performing human evaluations, despite the findings of best-of-n sampling in related work? \n\nThanks for raising this point about human evaluation. We acknowledge this concern, but we note that it is becoming standard practice to use LLM eval as a proxy for human eval and that it has been shown to be close to human raters for various tasks, e.g. [3, 4, 5, 6]. In addition, for complex alignment tasks such as the ones we consider here, human evaluation is a difficult target to aim for and difficult to reproduce, hindering stable evaluation (see [2, 7]).\n\n[1] SHARMA, Mrinank, et al. Towards Understanding Sycophancy in Language Models. arXiv preprint arXiv:2310.13548, 2023.\n\n[2] HOSKING, Tom; BLUNSOM, Phil; BARTOLO, Max. Human Feedback is not Gold Standard. arXiv preprint arXiv:2309.16349, 2023.\n\n[3] Rafailov, Rafael, et al. Direct Preference Optimization: Your language model is secretly a reward model, in Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n\n[4] LIU, Tianqi, et al. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.\n\n[5] LEE, Harrison, et al. RLAIF: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.\n\n[6] ZHENG, Lianmin, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. CoRR, abs/2306.05685, 2023. doi: 10.48550. arXiv preprint arXiv.2306.05685.\n\n[7] CHIANG, Cheng-Han; LEE, Hung-yi. Can Large Language Models Be an Alternative to Human Evaluations?. arXiv preprint arXiv:2305.01937, 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700603035738,
                "cdate": 1700603035738,
                "tmdate": 1700603428997,
                "mdate": 1700603428997,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sURqzy2O95",
            "forum": "tiiAzqi6Ol",
            "replyto": "tiiAzqi6Ol",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2602/Reviewer_pUSN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2602/Reviewer_pUSN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Compositional Perference Models (CPMs), a new perferene model framework that decomposes one global perference assessment into several interpretable features, using a prompted LM to score these features, and finally aggregates these features together with their scores using a logistic regression classifier. Experiments show that CPMs improves generalization and robustness than standard PMs. The main contributions include: (1) new CPMs that allows more transparent supervision; and (2) better results at dimensions of model/overoptimization robustness, generalization, and perference alignment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) new CPMs that allows more transparent supervision; \n(2) better results at dimensions of model/overoptimization robustness, generalization, and perference alignment."
                },
                "weaknesses": {
                    "value": "1. prefer to see detailed investigations of applying CPMs to different stages of (1) inference only (2) sft, and (3) peft.\n2. not quite clear of the scalability of the usage of current 13 features to novel langauges/tasks, further investigations are preferred."
                },
                "questions": {
                    "value": "1. in Table 5 and Table 6, scores from 1 to 10 are used, and did you try other ranges such as 1 to 5, and how did you decide to use a range of 1 to 10? Also, does different features require different scopes/ranges of scores? In addition, when changing from numbers to words (bad, good, better, best...), how shall the results change?\n2. any comparison of between supervised fine-tuning (SFT) and PEFT when using the CPMs? Or, any comparison of the usage of resources under different model alignment frameworks? So, (1) inference only stage controlling, (2) sft, (3) peft, any investigations on these directions of using CPMs?\n3. page 3, there are 13 features used, any detailed analysis of overlapping or diversities among these features?or when applying your method to other languages/tasks, how shall we reuse these features or how shall we design new features (any common rules?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Reviewer_pUSN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698451886460,
            "cdate": 1698451886460,
            "tmdate": 1699636198860,
            "mdate": 1699636198860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dsLTtDINCy",
                "forum": "tiiAzqi6Ol",
                "replyto": "sURqzy2O95",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2602/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pUSN, part1"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's valuable feedback and suggestions, particularly those regarding the scalability of the current features and potential variations. We believe that our response addresses your main concerns, enhancing the significance of our contribution.\n\n> Prefer to see detailed investigations of applying CPMs to different stages of (1) inference only (2) sft, and (3) peft.\n\n> Not quite clear of the scalability of the usage of current 13 features to novel languages/tasks, further investigations are preferred.\n\nWe attempt to address these concerns in our responses to questions 2 and 3 below.\n\nQuestions:\n\n> in Table 5 and Table 6, scores from 1 to 10 are used, and did you try other ranges such as 1 to 5, and how did you decide to use a range of 1 to 10? Also, does different features require different scopes/ranges of scores? In addition, when changing from numbers to words (bad, good, better, best...), how shall the results change?\n\nThanks for highlighting the effect of various prompts. For the score of the feature values, we performed  normalization for each feature to have mean 0 and standard deviation 1, so that the effect of range remains minimal.\n\nTo further investigate the impact of various prompts and the robustness of the CPM's performance on prompts, we employed GPT-3.5 to paraphrase the original description in Table 7. The paraphrased features possess similar meaning but different descriptions. The average win-rate (the metric described in Section 4.5) of CPM-Flan-T5 using this paraphrased prompt is $0.717$ with a standard error of $0.023$, which is not statistically different from the original performance in Table 1, ($0.742$ with a standard error of $0.034$). This further indicates that the CPM's performance is robust relative to the specific prompt used. Please see Appendix F for the details of this extended experiment.\n\n> any comparison of between supervised fine-tuning (SFT) and PEFT when using the CPMs? Or, any comparison of the usage of resources under different model alignment frameworks? So, (1) inference only stage controlling, (2) sft, (3) peft, any investigations on these directions of using CPMs?\n\nIn order to address this question, for instance at the level of resources used at inference time, we would need to consider a full generative process built on top of the preference model that we propose. This could be done in different ways, for instance by using a PPO-style RLHF fine-tuning of the pretrained model, or in a BoN-style reranking of samples from the pretrained model. Different choices of this kind would have very different consequences on the computational costs at training time (large for the PPO-style fine-tuning, low for the BoN-style sampling) and inference time (low for the PPO-style fine-tuning, large for the BoN-style sampling). But this would certainly be an important question to address in follow-up research."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602470513,
                "cdate": 1700602470513,
                "tmdate": 1700602470513,
                "mdate": 1700602470513,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J32GiOgah8",
            "forum": "tiiAzqi6Ol",
            "replyto": "tiiAzqi6Ol",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2602/Reviewer_zwab"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2602/Reviewer_zwab"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces compositional preference models (CPMs), a new method for aligning language models (LMs) with human preferences. CPMs break down preference scores into multiple features to improve robustness and generalization. This decomposition is accomplished by prompting an LM to assign a value to the answer based on a specific preference type. Experimental findings demonstrate that CPMs effectively mitigate overoptimization in preference modeling."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Modeling human preferences from different types of judgments is a promising research topic.\n2. Experimental results demonstrate that the suggested CPMs indeed improve both robustness and generation.\n3. The paper is generally easy to read."
                },
                "weaknesses": {
                    "value": "1. Although CPMs offer a practical method for breaking down preferences by stimulating LMs, I consider it too simplistic and unrealistic to capture intricate human preferences. For instance, easy-to-understand answers and answers with enough details may contradict each other. I have reservations about whether logistic regressors can accurately represent this intricate pattern.\n2. In terms of the experimental setup, CPMs prompt much more than standard PM, which raises concerns about their comparability. I recommend that the author include another standard PM baseline that uses a similar prompt budget as CPMs. For instance, prompting the LM $n$ times (where $n$ represents the number of pre-defined preference types for CPMs) through sampling and selecting the final preference score via majority voting."
                },
                "questions": {
                    "value": "1. How is the encoder for $x$ parameterized in logistic regression?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Reviewer_zwab"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683758093,
            "cdate": 1698683758093,
            "tmdate": 1699636198783,
            "mdate": 1699636198783,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IbuyPzInnZ",
                "forum": "tiiAzqi6Ol",
                "replyto": "J32GiOgah8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2602/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zwab, part1"
                    },
                    "comment": {
                        "value": "First, thank you for your appreciation of a number of aspects of our submission! We try to address your main concerns below, and also the Question that you ask.\n\n> Although CPMs offer a practical method for breaking down preferences by stimulating LMs, I consider it too simplistic and unrealistic to capture intricate human preferences. For instance, easy-to-understand answers and answers with enough details may contradict each other. I have reservations about whether logistic regressors can accurately represent this intricate pattern.\n\nYou are right that certain preference aspects may in principle be in opposition, and in such cases, the human annotators, when having to choose among two responses which one they prefer, have to make a delicate decision.The logistic regression training technique (i.e., determining the lambda parameters) only attempts to fit the annotator preferences as they are expressed in the training data, and is known for its robustness to combining features that may be correlated, positively or negatively: such dependencies typically have little impact on the predictive power of the method (ability to predict the human preferences, our main focus), even if, in the presence of very high correlations (or anti-correlations), different lambda vectors may produce the same predictions (\u201cparameter non-identifiability\u201d) (see [1]).\n\nOn this last point, and also to assess whether we could actually detect strongly conflicting features in our experiments, we computed correlations between features, see the added Fig. \\label{fig:feature-correlation-large} in the Appendix. This figure shows, for the features and dataset under consideration, mostly positive correlations between features (including \u201ceasy-to-understand\u201d and \u201cenough-detail\u201d), and no higher correlation than 0.84. \n\nIn summary, the intricacy of combining different dimensions  is delegated to the human annotators producing the preference training set, with the logistic regression classifier trying to reproduce the preferences displayed in such annotations, and being quite effective at combining the features provided as inputs (see [2] for a related point). Of course, the quality of the predictions depends on that of the extracted features \u2014 whether they do correspond to the dimensions actually considered by the annotators (see [3] for a cautionary study) \u2014 but this is orthogonal to the effectiveness of logistic regression as a combination mechanism.\n\n[1] HARRELL, F.E., Regression Modeling Strategies, Springer Series in Statistics, 2015.\n\n[2] SHARMA, Mrinank, et al. Towards Understanding Sycophancy in Language Models. arXiv preprint arXiv:2310.13548, 2023.\n\n[3] HOSKING, Tom; BLUNSOM, Phil; BARTOLO, Max. Human Feedback is not Gold Standard. arXiv preprint arXiv:2309.16349, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602052263,
                "cdate": 1700602052263,
                "tmdate": 1700602217792,
                "mdate": 1700602217792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DdG0sGpvrm",
                "forum": "tiiAzqi6Ol",
                "replyto": "J32GiOgah8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2602/Reviewer_zwab"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2602/Reviewer_zwab"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\n> First, the single holistic feature \u2018helpfulness\u2019 obtains a reasonable win-rate (0.707) on its own.\n\nI am not sure how to interpret this. What is the reason behind the considerably higher win-rate compared to `Standard PM` (0.707 vs 0.588)?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625412073,
                "cdate": 1700625412073,
                "tmdate": 1700625412073,
                "mdate": 1700625412073,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "noJCULf2IK",
            "forum": "tiiAzqi6Ol",
            "replyto": "tiiAzqi6Ol",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2602/Reviewer_Q5ds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2602/Reviewer_Q5ds"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a compositional preference model for LLM alignment. In contrast to standard monolithic preference models that assign a single scalar value to preference judgments. the model uses a number of features associated with individual scalar values (assigned by an LLM as an automatic evaluator) that are then linearly combined into an overall score.  The authors argue that this provides an inductive bias to the model that makes it more robust to overfitting and reward hacking and results in better generalization and human interpretability. The technique is evaluated with respect to consistency of responses for models trained on different subsets of the training data), comparison against reference PMs from the literature, robustness to overoptimization, and alignment of LLMs trained with the proposed model as opposed to a standard PM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The core idea of the paper is simple yet powerful and addresses known weaknesses in traditional monolithic preference models; it should be of broad interest to the ICLR audience. The presentation is clear and -- with the exception of human alignment evaluation (see below) -- the evaluations are convincing."
                },
                "weaknesses": {
                    "value": "For alignment with human preferences, another  LLM (Claude-2) was used rather than genuine human ratings. Although there is more effort associated with a human evaluation study,  and the literature you cite has shown some (imperfect) degree of correlation between human ratings and LLM scores, I really consider human evaluation a must here - otherwise, you are measuring alignment between different LLMs, which can simply result from similar training procedures or similar preference models used."
                },
                "questions": {
                    "value": "1. It looks like the feature evaluator LLMs (Flat-T5 and GPT3.5) were used out of the box with prompting for assigning feature scores, without fine-tuning or in-context learning. I would have like to see the comparison against fine-tuned versions for each feature. \n2. How does the robustness of the CPM change with an increasingly larger list of features?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2602/Reviewer_Q5ds"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772465933,
            "cdate": 1698772465933,
            "tmdate": 1699636198706,
            "mdate": 1699636198706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z2b06IXIMW",
                "forum": "tiiAzqi6Ol",
                "replyto": "noJCULf2IK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2602/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Q5ds, part1"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and thoughtful review. We are glad you found our paper well-written and interesting. Please find responses to your questions below. We hope that our answers address your main concerns, increasing the strength of our contribution.\n\n> For alignment with human preferences, another LLM (Claude-2) was used rather than genuine human ratings. Although there is more effort associated with a human evaluation study, and the literature you cite has shown some (imperfect) degree of correlation between human ratings and LLM scores, I really consider human evaluation a must here - otherwise, you are measuring alignment between different LLMs, which can simply result from similar training procedures or similar preference models used.\n\nThanks for raising this point about human evaluation. We acknowledge this concern, but we note that it is becoming standard practice to use LLM eval as a proxy for human eval and that it has been shown to be close to human raters for various tasks, e.g. [1, 2, 3]. This said, we believe that, for complex modeling tasks as the ones we consider here, human evaluation is a difficult target to aim for, as it often depends on informal criteria that are not defined precisely (see [4] on a related point). One radical way to escape this dilemma would be to assume that human preferences are fully represented by collecting a dataset of preference judgments obtained based on a certain protocol, and that the quality of a model would be assessed through its ability to predict new preference judgments based on the same protocol, which is similar with what we do with our robustness studies. However, this argument is not without some circularity, but it is not obvious to us how to escape it.\n\n[1] Rafailov, Rafael, et al. Direct Preference Optimization: Your language model is secretly a reward model, in Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n\n[2] LIU, Tianqi, et al. Statistical rejection sampling improves preference optimization. arXiv preprint arXiv:2309.06657, 2023.\n\n[3] LEE, Harrison, et al. RLAIF: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.\n\n[4] HOSKING, Tom; BLUNSOM, Phil; BARTOLO, Max. Human Feedback is not Gold Standard. arXiv preprint arXiv:2309.16349, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601732876,
                "cdate": 1700601732876,
                "tmdate": 1700601732876,
                "mdate": 1700601732876,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wiaa3YU9iP",
                "forum": "tiiAzqi6Ol",
                "replyto": "noJCULf2IK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2602/Reviewer_Q5ds"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2602/Reviewer_Q5ds"
                ],
                "content": {
                    "comment": {
                        "value": "I'm aware of the problems with human preferences as ground truth (acquisition, reliability, etc.), but as you say, your argument is circular. I suggest defining a controlled scenario where preferences can be collected more easily, or coupling preference modeling with an end-to-end task (e.g., helpfulness in the context of a task-oriented dialog scenario where success (task accomplishment) can be measured more easily. Agree that this may exceed the scope of the paper, but in that case I'd avoid the term 'alignment with human preferences' and include a few sentences of discussion of the limitations. \nThank you for the added data points on number and combination of features, this addresses my question."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605254066,
                "cdate": 1700605254066,
                "tmdate": 1700605669404,
                "mdate": 1700605669404,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]