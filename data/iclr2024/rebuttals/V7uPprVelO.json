[
    {
        "title": "GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature"
    },
    {
        "review": {
            "id": "ImeaDC1WSU",
            "forum": "V7uPprVelO",
            "replyto": "V7uPprVelO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6999/Reviewer_GDcb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6999/Reviewer_GDcb"
            ],
            "content": {
                "summary": {
                    "value": "GenCO\" is a novel framework that integrates deep generative models with combinatorial solvers to address design challenges that require diverse solutions while adhering to specific constraints. Unlike conventional generative models, GenCO focuses on generating instances of combinatorial optimization problems, allowing finer control over the generated outputs and introducing an additional combinatorial loss component. The framework's effectiveness is demonstrated across various generative tasks, consistently producing diverse, high-quality solutions that meet user-defined combinatorial properties."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Combing the algorithmic prior of a combinatorial optimization solver with various generative models is an effective and promising approach."
                },
                "weaknesses": {
                    "value": "The application methodology is relatively straightforward, involving the imposition of constraints or the assignment of optimization performance metrics to existing proposed methods, which are well-established techniques."
                },
                "questions": {
                    "value": "1. How does this research address the mechanism behind generating diverse solutions when assigning combinatorial properties to various generative models, and does it simply rely on the use of generative models to claim diversity?\n\n2. Combining GAN and VAE within a unified framework may seem unusual, given their distinct purposes. Is there a thoughtful consideration of the different objectives of these two generative models, or does it feel mechanically extended without such reflection?\n\n3. Could you clearly define L_{Gen} for both VAE and GAN and provide separate descriptions for each in Figure 1? If the proposed learning method is more suitable for one of the generative models, focusing on it may be beneficial.\n\n4. In section 3.2, regarding the Constrained Generator, are there any limitations in terms of the expressive power of linear projection? How does it differ from techniques like projected gradient ascent commonly used in constrained optimization?\n\n5. In the penalized generator discussed in section 3.3, the idea of directly using the cost obtained through the combinatorial optimization solver h for model training has been present in prior research. Are there any novel aspects from a learning perspective in this new approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Reviewer_GDcb",
                        "ICLR.cc/2024/Conference/Submission6999/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698550262124,
            "cdate": 1698550262124,
            "tmdate": 1700718747903,
            "mdate": 1700718747903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "047XO1EXG8",
                "forum": "V7uPprVelO",
                "replyto": "ImeaDC1WSU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The application methodology is relatively straightforward, involving the imposition of constraints or the assignment of optimization performance metrics to existing proposed methods, which are well-established techniques. \n\nPlease refer to our general response (1), where we clarify the novelty of our approach and main contributions.\n\n\n> How does this research address the mechanism behind generating diverse solutions when assigning combinatorial properties to various generative models, and does it simply rely on the use of generative models to claim diversity?\n\nOur research indeed leverages generative models to achieve diversity in solutions by approximating the underlying data distribution. The mechanism involves training the generative model to capture and represent the diverse characteristics inherent in the given combinatorial problem. \n\n\n> Combining GAN and VAE within a unified framework may seem unusual, given their distinct purposes. Is there a thoughtful consideration of the different objectives of these two generative models, or does it feel mechanically extended without such reflection?\n\nThank you for raising this important point. While GANs and VAEs traditionally serve distinct purposes, our approach aims to highlight the flexibility of combinatorial optimizers in accommodating both GAN and VAE frameworks. The integration is not merely a mechanical extension; rather, it's a thoughtful exploration of the adaptability of combinatorial optimizers to different generative models. We acknowledge the distinct objectives of GANs and VAEs and demonstrate that our approach GenCO is versatile enough to be applicable to both, as demonstrated in Appendix C. Furthermore, we evaluate a new setting of generating inverse photonic designs in which we employ a VQVAE as the generative backbone and ensure that decoded objects satisfy the combinatorial constraints during training. \n\n\n> Could you clearly define L_{Gen} for both VAE and GAN and provide separate descriptions for each in Figure 1? If the proposed learning method is more suitable for one of the generative models, focusing on it may be beneficial.\n\nFor VAE, the L_{Gen} is equivalent to the ELBO loss that VAE is ultimately optimizing. For GAN, the loss would be equivalent to the KL divergence or Wasserstein loss that is being optimized. These losses are implicitly defined by the different methods by generally comparing the generated object to the data distribution in some way, reconstruction error for the VAE, and discrimination error for the GANs. Ultimately, this is a loss imposed on the distribution represented by the generative model, informing how aligned the generative model is with the data distribution. Please refer to Appendix C for the complete description of our approach for VAEs.\n\n\n> In section 3.2, regarding the Constrained Generator, are there any limitations in terms of the expressive power of linear projection? How does it differ from techniques like projected gradient ascent commonly used in constrained optimization?\n\nWe would like to note that it is a projection in the sense that we are identifying a combinatorially constrained discrete solution that is near the original continuous solution (i.e. projection into combinatorial space). Because our settings have only binary decisions, we can guarantee that each solution is reachable by some continuous solution (meaning that all discrete solutions are possible to generate). However, we would note that the projection is not linear (i.e., not a simple linear transformation). On the contrary, the projection is the solution to a discrete linear optimization problem, which is highly discrete and piecewise constant in that a large area in the original continuous space may map to the same solution in discrete space. Additionally, while projected gradient ascent is a popular approach for solving some constrained optimization problems, it is not generally used in combinatorial optimization where the standard methods would be a variant of tree search for MILP and constraint satisfaction problems or variants of simplex for linear programs. \n\n\n\n> In the penalized generator discussed in section 3.3, the idea of directly using the cost obtained through the combinatorial optimization solver h for model training has been present in prior research. Are there any novel aspects from a learning perspective in this new approach?\n\nWhile we acknowledge prior works that incorporate costs as penalties, such as Donti et al. 2020, it's important to highlight the novelty of our approach from a learning perspective. Existing works typically focus on the continuous case or rely on continuous relaxation, whereas our method directly handles combinatorial penalties or losses without such relaxation and trains the whole pipeline end-to-end. This distinction represents a novel aspect of our approach, as we are not aware of any previous work that addresses combinatorial penalties in the same direct manner."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345936761,
                "cdate": 1700345936761,
                "tmdate": 1700345936761,
                "mdate": 1700345936761,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NXXHCVIoeN",
                "forum": "V7uPprVelO",
                "replyto": "047XO1EXG8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_GDcb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_GDcb"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your clarification."
                    },
                    "comment": {
                        "value": "Many of my concerns have been addressed. I raised my score to 5."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718726245,
                "cdate": 1700718726245,
                "tmdate": 1700718726245,
                "mdate": 1700718726245,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GZITnpmbOr",
            "forum": "V7uPprVelO",
            "replyto": "V7uPprVelO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6999/Reviewer_X2M2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6999/Reviewer_X2M2"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces GenCO, a framework to generate diverse and high-quality solutions that satisfy combinatorial constraints which is an important factor in design problems where both diversity of solutions and adherence to constraints are important. GenCO combines  the flexibility of deep generative models with the combinatorial efficiency of optimization solvers. This is achieved by introducing a \"combinatorial loss\" in addition to the regular generative loss. The combinatorial loss enforces hard constraints or add penalties for constraint violation. GenCO's involves generating a problem representation $c$. The combinatorial loss $C$ uses a MILP solver to project $c$ onto the feasible set (for hard constraints) or adds a penalty (for soft constraints). For hard constraints, the combinatorial loss is implemented via a projection that finds the closest feasible solution to $c$ allowing training of the generator without explicit constraints as the projection layer handles feasibility. For soft constraints, the combinatorial loss instead adds a penalty term based on constraint violation. The overall loss function balances the generative loss and combinatorial loss with a hyperparameter $\\gamma$. For backpropagation through the combinatorial solver, differentiable MILP solvers are used. Empirical evaluations are presented on game level generation (with hard constraints) and map generation (with soft constraints on path planning efficiency). GenCO consistently generates more diverse and higher quality solutions than baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Ability to generate diverse, high-quality solutions satisfying combinatorial constraints is very useful for many design problems. This is a major strength of GenCO.\n\n* As far as I can tell, this combination of deep generative models with combinatorial solvers is novel. It draws on the strengths of both approaches to solve a useful problem. \n\n* Experiments demonstrate clear improvements over baselines in diversity and quality of constrained generated objects. Though it should be noted that the baselines are not too strong. \n\n* The approach seems fairly general and can potentially be applied to many combinatorial generative tasks beyond the demonstrated applications, for instance molecular generation as discussed in the introduction."
                },
                "weaknesses": {
                    "value": "* There is limited analysis of how the approach scales with problem size and constraint complexity. More extensive experiments on large and complex problems would be useful.\n\n* The tradeoff between generative loss and combinatorial loss is not carefully analyzed. There is not enough details on how the parameter $\\gamma$ is set and the impact it has on training.\n\n* While backpropagation through combinatorial solvers is possible and works well in a lot of settings, it can be computationally expensive. Alternate approaches to incorporate solvers might be important for scalability.\n\n* Theoretical analysis of GenCO's properties is limited. For instance, more analysis of convergence guarantees, sample complexity could strengthen the approach.\n\n* It is not very clear to me how the method would generalize to general logical constraints, which can be important in the design scenarios considered. \n\n* Reproducibility: The authors do not include code to reproduce the results and experimental details are discussed but details about hyperparameter selection are missing."
                },
                "questions": {
                    "value": "* Could you elaborate a bit on the scalability of the approach to complex constraints and problems? \n\n* Could you comment on how more general constraints could be incorporated in this framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Reviewer_X2M2"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769938091,
            "cdate": 1698769938091,
            "tmdate": 1700480763271,
            "mdate": 1700480763271,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x8M2HbMULl",
                "forum": "V7uPprVelO",
                "replyto": "GZITnpmbOr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> There is limited analysis of how the approach scales with problem size and constraint complexity. More extensive experiments on large and complex problems would be useful.\n\nIt is indeed important to add more validation. To support our claims, we included one additional benchmark: inverse photonic design, which is a real-world, practical, and large-scale problem. Our results and discussion can be found in general response (2). For now, we have included it in the appendix, and we will include it in the main text of our camera-ready revision.\n\n\n> The tradeoff between generative loss and combinatorial loss is not carefully analyzed. There is not enough details on how the parameter is set and the impact it has on training.\n\nPlease review the additional results presented below. As anticipated, the SP loss decreases with an increase in gamma. However, the image quality concerning other metrics, such as density/coverage, starts to deteriorate rapidly. Eventually, at a certain gamma value, our approach generates blank or empty images, as such images result in the shortest path with zero cost. This intriguing behavior could be attributed to the combinatorial nature of the problem.\nWe use cross-validation to set the gamma (and other hyperparameters). We will make this clear in the next revision of the paper.\n| Gamma        | SP Loss | Density | Coverage |\n|--------------|---------|---------|----------|\n| gamma = 0    | 36.45   | 0.81    | 0.98     |\n| gamma = 1e-4 | 27.66   | 0.93    | 0.93     |\n| gamma = 1e-3 | 23.99   | 0.94    | 0.93     |\n| gamma = 3e-3 | 23.76   | 0.75    | 0.86     |\n| gamma = 5e-3 | 18.02   | 0.49    | 0.61     |\n| gamma = 1e-2 | 0.00    | 0.00    | 0.00     |\n\nNote: gamma = 1e-2 generates blank/empty images\n\n> While backpropagation through combinatorial solvers is possible and works well in a lot of settings, it can be computationally expensive. Alternate approaches to incorporate solvers might be important for scalability.\n\nThere are indeed alternative methods that can be used depending on problem type. We've employed (Sahoo et al, 2022) for Warcraft experiments, which is much faster computation-wise (and arguably more stable). Additionally, \"solver-free\" methods could be involved (Shah et al., 2022; Zharmagambetov et al., 2023), which will be important to use as a replacement for the differentiable optimizer as the optimization problem at hand becomes less computationally tractable.\n\n\n> Theoretical analysis of GenCO's properties is limited. For instance, more analysis of convergence guarantees, sample complexity could strengthen the approach.\n\nWe only have guarantees on the feasibility of the generated solutions, it is unclear how to obtain meaningful convergence guarantees or sample complexity without resorting to unreasonable and simple assumptions.\n\n\n> It is not very clear to me how the method would generalize to general logical constraints, which can be important in the design scenarios considered.\n\nGeneral logical constraints are possible as long as they can be expressed as the feasible region to an integer linear program which has been flexibly used to encode constraints in scheduling, routing, resource allocation, matching, time windowed delivery, coloring, packing, first-order logic, and more.\n\n> Reproducibility: The authors do not include code to reproduce the results and experimental details are discussed but details about hyperparameter selection are missing.\n\nWe have clarified training details in the Supplementary files and in the Experiments section. We will add more details in the next revision. Furthermore, we are committed to enhancing openness by open-sourcing our implementation before the camera-ready, enabling our methodology to be extended to relevant domains.\n\n> Could you elaborate a bit on the scalability of the approach to complex constraints and problems?\n\nIt is applicable as long as the constraints can be optimized over. That is, as long as the underlying optimization problem can be solved either exactly or with reasonable approximation. Modern solvers such as Gurobi often solve industrial-scale problems in seconds or minutes. Furthermore, there is potential to extend recent advances in \"solver-free\" methods (Shah et al., 2022; Zharmagambetov et al., 2023) in cases where the optimization model is expensive to solve.\n\n> Could you comment on how more general constraints could be incorporated in this framework?\n\nOur approach can handle any kind of constraints that can be expressed using optimization modeling tools that have been made differentiable, such as maxSAT, mixed integer linear programs, cone programs, quadratic programs, and many more. We refer to Kotary, James, et al. \"End-to-End Constrained Optimization Learning: A Survey.\" 2021 for an overview of the different methods here, most of which could be readily integrated into our pipeline depending on the task at hand."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345870970,
                "cdate": 1700345870970,
                "tmdate": 1700345870970,
                "mdate": 1700345870970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AkiMtjnMeC",
                "forum": "V7uPprVelO",
                "replyto": "x8M2HbMULl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_X2M2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_X2M2"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the clear response and I appreciate the authors adding an additional task in such a short time-frame. It is a bit difficult to gauge the complexity of the added task and judge the applicability to other large domains but the results are certainly promising. The ablations are also helpful and would be a good addition to the paper. My concerns regarding the reliance on having a good solver and being able to define constraints to the solver remain, but I have raised my score to recommend acceptance (5-> 6)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480739022,
                "cdate": 1700480739022,
                "tmdate": 1700480739022,
                "mdate": 1700480739022,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dp1yc2f3W1",
                "forum": "V7uPprVelO",
                "replyto": "ySYz6pYuab",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_X2M2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_X2M2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the additional clarification!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675304247,
                "cdate": 1700675304247,
                "tmdate": 1700675304247,
                "mdate": 1700675304247,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JC3f4pl9jB",
            "forum": "V7uPprVelO",
            "replyto": "V7uPprVelO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6999/Reviewer_5GZK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6999/Reviewer_5GZK"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the generative tasks in which both the diversity and conformity of constraints are crucial. The objective is to make the output of the generator follow discrete/combinatorial constraints and penalize any deviation. The proposed framework enables end-to-end training of deep generative models integrated with embedded combinatorial solvers, aiming to guarantee the combinatorial feasibility of the generation while also maintaining high fidelity. The effectiveness of the proposed method is verified in generative tasks characterized by combinatorial intricacies, including game level generation and map creation for path planning, showing its superiority over previous peer methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.$\\ $The setup and task design studied in this article are intriguing and inspiring. In real-world scenarios, natural data often exhibit some discrete properties, which is an aspect overlooked in the realm of pure generative model research.\n\n2.$\\ $The paper is well organized and presented. The motivations and current challenges on top of the current techniques are clearly stated.\n\n3.$\\ $The methodology design is simple yet efficient. The empirical results are promising."
                },
                "weaknesses": {
                    "value": "1.$\\ $My primary concern regarding this paper is the selection of quantitative evaluation metrics. As a solution for generative tasks, the assessment of diversity and generation quality should draw from some classic evaluation metrics in the traditional generative model field, such as FID [1] and density/coverage [2]. While the evaluation method using a discriminator indeed holds some value, it is not an authoritative network, and this evaluation metric is single-dimensional in terms of discriminability, making its reliability less robust.\n\n2.$\\ $Some training details can be more specific. For instance, one aspect to consider is the stability and efficiency of the GAN training process. GANs are renowned for their challenging training dynamics, and the utilization of approximated gradients obtained through black-box optimization methods could potentially exacerbate the risk of training instability. Moreover, since the training data is limited, effectively training a GAN becomes a non-trivial endeavor.\n\n[1] Gans trained by a two time-scale update rule converge to a local Nash equilibrium. NeurIPS 2017.\n\n[2] Reliable fidelity and diversity metrics for generative models. ICML 2020."
                },
                "questions": {
                    "value": "1.$\\ $What about the quantitative evaluation results of traditional metrics in the generative model field such as FID and density/coverage?\n\n2.$\\ $\"..., given that the levels are trained on only 50 examples, ...\": How is the training dynamic of GAN with so limited data? Are there any additional efforts to stabilize training?\n\n3.$\\ $In the discussion of uniqueness in Sec. 4.1.2, it makes the adversary's task easier as it only needs to distinguish between valid discrete levels rather than continuous and unconstrained levels. But if the discriminator is too strong, it can easily lead to the phenomenon of gradient vanishing, does this make training even more challenging and unstable?\n\n4.$\\ $In fact, many generative tasks naturally exhibit certain discrete characteristics, but due to the strength of generative models, they can inherently learn these features. For example, generative models can learn that dogs have four legs. Is it possible that with a sufficiently powerful generative model, it can automatically recognize and learn these discrete features?\n\n5.$\\ $Typo: Sec. 4.2: descrbied -> described. Sec. 4.2.1: Delete \"We then average the output to obtain the final loss for generator.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Reviewer_5GZK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822547543,
            "cdate": 1698822547543,
            "tmdate": 1699636819540,
            "mdate": 1699636819540,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KtI1H3EmO3",
                "forum": "V7uPprVelO",
                "replyto": "JC3f4pl9jB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Evaluation metrics\n\nThanks for pointing this out. Please see our general response (3) above, where we provide additional quantitative metrics (density/coverage). We will definitely include them in our paper revision.\n\n\n> Some training details can be more specific. For instance, one aspect to consider is the stability and efficiency of the GAN training process. GANs are renowned for their challenging training dynamics, and the utilization of approximated gradients obtained through black-box optimization methods could potentially exacerbate the risk of training instability. Moreover, since the training data is limited, effectively training a GAN becomes a non-trivial endeavor.\n\nWe have implemented suggested regularization techniques from (Pogancic et al. (2020); Sahoo et al. (2022)) to stabilize the training of differentiable combinatorial solvers. Also, we have clarified training details in the Supplementary files and in the Experiments section. We will add more details in the next revision. Subjectively, we observe that training our pipeline has similar complexity as training regular GANs in terms of stability, hyperparameter tuning, etc. \n\nFurthermore, we are committed to enhancing openness by open-sourcing our implementation soon.\n\nUnderstanding the empirical nature of the concerns raised, we have conducted additional empirical evaluations (photonic design in general response (2)) to strengthen the robustness of our claims. Additionally, we've explored the trade-off of the gamma parameter in Section 3.3 (see our response to reviewer @X2M2).\n\n\n> \"..., given that the levels are trained on only 50 examples, ...\": How is the training dynamic of GAN with so limited data? Are there any additional efforts to stabilize training?\n\nIn game-level design, the discrete nature of GenCO\u2019s output is beneficial in our scenario. Specifically, there are finite discrete objects that the GAN can generate, and thus it is easier to avoid the issues of mode collapse compared to a traditional GAN, which can effectively hide solutions in the continuous space. Indeed, this is shown in practice as many of the continuous solutions map to the same discrete solution after postprocessing. \n\n\n> In the discussion of uniqueness in Sec. 4.1.2, it makes the adversary's task easier as it only needs to distinguish between valid discrete levels rather than continuous and unconstrained levels. But if the discriminator is too strong, it can easily lead to the phenomenon of gradient vanishing, does this make training even more challenging and unstable?\n\nAs in the previous level generation paper, we use Wasserstein GAN (WGAN), which was designed to prevent vanishing gradients as there is always some incoming gradient imposed on the generated solution as long as the generated solution is different from the known solutions and the adversary hasn\u2019t become degenerately poor. Overall, we only have access to 50 training levels, and GenCO is able to perform well in this low data regime by still generating many unique solutions while the postprocessing approach fails to do so.\n\n\n> In fact, many generative tasks naturally exhibit certain discrete characteristics, but due to the strength of generative models, they can inherently learn these features. For example, generative models can learn that dogs have four legs. Is it possible that with a sufficiently powerful generative model, it can automatically recognize and learn these discrete features?\n\nWhile powerful generative models can capture certain discrete features, there are some limitations to the models, even with large capacity. For example, at the time of writing, large models like stable diffusion or DALL E 2 suffer from generating images of people with incorrect numbers of fingers or toes, or where their pose doesn't match that of their reflection in a mirror unless specifically fine-tuned and instructed for such tasks. Furthermore, going beyond image/video generation, for other design tasks with mathematically well-defined constraints (e.g., industrial design), GenCO can make full use of the domain-specific constraints. It can flexibly and explicitly encode such constraints and eliminate the possibility of generating infeasible solutions, regardless of the size of the datasets. This could lead to better performance with the same computational costs and model capacity."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345769981,
                "cdate": 1700345769981,
                "tmdate": 1700345900028,
                "mdate": 1700345900028,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bGn3ukA5hA",
                "forum": "V7uPprVelO",
                "replyto": "KtI1H3EmO3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_5GZK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_5GZK"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for the reply. It is still counter-intuitive to me that WGAN can be well trained by 50 samples without any technique tailored for few-shot scenarios. Are you planning to release the code at some point?"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712284173,
                "cdate": 1700712284173,
                "tmdate": 1700712284173,
                "mdate": 1700712284173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A39MTXTok1",
                "forum": "V7uPprVelO",
                "replyto": "rCkeCsc8Mx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_5GZK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_5GZK"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the clarification. I would like to uphold my current rating."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717098927,
                "cdate": 1700717098927,
                "tmdate": 1700717098927,
                "mdate": 1700717098927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4OFTYyZK3D",
            "forum": "V7uPprVelO",
            "replyto": "V7uPprVelO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6999/Reviewer_qozd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6999/Reviewer_qozd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes GenCO that generates diverse solutions, instead of the best solution given by traditional solvers, for design poblems with combanitorial nature. Specifically, it deals with either hard or soft constraints by introducing  a combanitorial loss."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-motivated.\n2. The results are good according to the case study, while more metrics are expected for better evaluation."
                },
                "weaknesses": {
                    "value": "1. The so-called combinatorial loss is unsurprising, as it has been used in many areas, though without a uniform name or formulation. For example, in areas such as molecule design and chip design, it is usual to use the panelty in reward design for hard or soft constraints. It is also used in solving combinatorial problems to guide the feasible and quality of solutions. This kind of methods are like the Lagrangian multiplier method, and the idea of modeling the constraints as a loss term is straightforward. There are also many methods targeting on solving combinatorial problems, such as mixed-integer programmings, by generative models, where the combinatorial constraints are also considered. This paper is more like a gatherer of those methods. If I am wrong, the authors may want to further emphasise the technical contribution.\n2. In experiments, the authors report the loss values for evaluation. However, the loss function may not reflect the generation quality precisely, and it is more like a surrogate metric instead of the final goal. Some other metrices for evaluation should be introduced.\n3. Baselines are not strong enough. When considering the specific scenarios such as game design, it is expected to compare the proposed method with the SOTA method tailored for this task, instead of only considering the GAN+MILP baseline, to demonstrate the effectiveness of GenCO.\n4. If the authors claim that they propose a framework for design problems, it is expected to conduct more experiments such as molecule or chip design. Current experiments are not convincing enough."
                },
                "questions": {
                    "value": "See weaknesssed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6999/Reviewer_qozd",
                        "ICLR.cc/2024/Conference/Submission6999/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890582648,
            "cdate": 1698890582648,
            "tmdate": 1700900396716,
            "mdate": 1700900396716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XSYicdFXAu",
                "forum": "V7uPprVelO",
                "replyto": "4OFTYyZK3D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Combinatorial loss has been used in many areas, e.g. molecule and chip design, where it is usual to use the penalty in reward design for hard or soft constraints\n\nWhile it's true that penalizing constraint violation has been employed in various domains like molecule and chip design, it is critical to emphasize that they typically do not guarantee feasibility. One possible reason for this is that in molecule or chip design settings, the constraints are often provided as blackbox simulators that are difficult to represent using constraint optimization solvers, thus, it is difficult to guarantee that solutions satisfy constraints such as synthesis accessibility or water lipid solubility. \n\nOur work stands out as we uniquely combine generative models (GANs/VAEs) and combinatorial solvers with feasibility guarantees and propose an end-to-end training algorithm for the entire pipeline. \n\nAlso, please refer to our general response regarding the novelty and importance of our work, as well as our experiments on the photonic device design setting to demonstrate generalizable performance.\n\n> \u200b\u200bThis kind of methods are like the Lagrangian multiplier method, and the idea of modeling the constraints as a loss term is straightforward.\n\nThank you for your observation. While the concept of using combinatorial optimization as a loss term may seem reminiscent of the Lagrangian multiplier method, it's important to emphasize a key distinction in our approach. Unlike the Lagrangian multiplier method, which lacks feasibility guarantees, our method provides explicit feasibility guarantees throughout training. Furthermore, in discrete optimization cases, handling Lagrangian functions and multipliers can be non-trivial, and there may not be feasibility guarantees in cases where the continuous relaxation is very loose. In contrast, our approach incorporates hard constraints seamlessly using a differentiable combinatorial layer in the pipeline, allowing for end-to-end training. These key differences set our methodology apart in terms of both feasibility assurances and empirically improved performance. \n\n\n> There are also many methods targeting on solving combinatorial problems, such as mixed-integer programmings, by generative models, where the combinatorial constraints are also considered. \n\nThese methods are mainly concerned with solving a single Mixed Integer Program rather than generating multiple solutions that satisfy the combinatorial constraints. Our goal is to satisfy all three conditions on solutions: diversity, solution quality, and feasibility. Furthermore, these problems generally concern solving a single well-defined problem where all objective coefficients and constraints are known. In our settings, the problem is underspecified in that for the game-level design, inverse photonic design, or shortest path map settings, we don\u2019t have a fixed problem formulation that we are trying to solve.\n\n> This paper is more like a gatherer of those methods. If I am wrong, the authors may want to further emphasise the technical contribution.\n\nHopefully, our answers above and general response regarding our technical contribution clarified your concerns. Furthermore, as we demonstrate in the game level design and inverse photonic design setting, GenCO is the only approach that is capable of providing many unique solutions which are all guaranteed to satisfy combinatorial constraints, whereas previous approaches that treat optimization as a postprocessing step yield degenerate solutions. \n> Other evaluation metrics for measuring the generation quality\nThanks for pointing this out. Please see our general response (3) above where we provide additional quantitative metrics (density/coverage). We will definitely include them in our paper revision.\n\n> Baselines are not strong enough.\n\nFrom our review of the literature, we found that the GAN+MILP was the only SOTA baseline that was also able to guarantee feasibility of the generated solutions in the game-level design setting. We furthermore have pure GAN results that demonstrate that these approaches don't guarantee feasibility. The same goes for the penalty version of GenCO. \n\n> If the authors claim that they propose a framework for design problems, it is expected to conduct more experiments such as molecule or chip design. Current experiments are not convincing enough.\n\nIt is indeed important to add more validation. To support our claims, we included one additional benchmark: inverse photonic design, which is a real-world, practical, and large-scale problem. Our results and discussion can be found in general response (2). For now, we have included it in the appendix, and we will include it in the main text of our camera-ready revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700345695741,
                "cdate": 1700345695741,
                "tmdate": 1700345695741,
                "mdate": 1700345695741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Btz1f1SwDy",
                "forum": "V7uPprVelO",
                "replyto": "XSYicdFXAu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_qozd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_qozd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' responses, which help me understand the contributions better. However, I still have some questions.\n1. In my understanding, the authors propose a framework that first generates a problem description, and leverages a solver to find a problem solution that is \"clear\" to the description in terms of the dot product similarity. It seems that $h(c)$ is just like a postprocess (or in other word, the process of obtaining the generated data from a latent vector), but GenCO conducts the postprocess during training, instead of after training. In this way, the trained generator is aware of what we obtain after postprocessing. Am I right? If so, the calling of a solver is just a special case.\n1. This paper considers generating a vector \"x\", so it can be ontained by minimizing the dot conduct. However, what if the generated data has a complex form that cannot be easily represented as a vector \"x\"? How to define $h(c)$ then? For example, can you briefly describe how to define $h(c)$ if I want to use GenCO to generate graphs?\n1. Is it always easy to solve the MILP $\\arg\\min_{x\\in\\Omega}c^{\\top} x$? As in many scenarios, the feasible domain may be complex, and may not be easily represented as a MILP.\n1. I still think the baselines are not strong enough, so that I can hardly understand how powerful is the trained model. Is it possible to conduct experiments on more common tasks where more SOTA approaches---maybe carefully designed for those tasks with or maybe with written rules---can be compared with?\n\nBTW, 'z' in Algorithm 3 Line 10 should be 'x'?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673833666,
                "cdate": 1700673833666,
                "tmdate": 1700673833666,
                "mdate": 1700673833666,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nmd7BJKSqI",
                "forum": "V7uPprVelO",
                "replyto": "IMRR619D0p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_qozd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6999/Reviewer_qozd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the further responses. Sorry but I still cannot understand something.\n1. Regarding the generation of a vector \"x\". Do you mean that GenCO may rely on a kind of pretrained encoder-decoder like a VAE model, for obtaining the embedding vector and decodiing the vector into the objective to be generated? And GenCO is like a model that searches in the latent space? If so, how to express the constraints in the objective space? In other word, if the constraints are complex, and with an off-the-shelf encoder, how could we properly find the domain $\\Omega$ in the latent space?\n1. Regarding solving the MILP. In real-world applications the constraints may be complex, so that the domain $\\Omega$ could not be easily modeled as a polygon, so the problem may not even be a MILP. How to tackle this?\n1. I understand that previous pure ML approaches cannot work well on this job. What about those rule-based or heuristic methods? It would help readers better understand the difficulty of the tasks.\n\nI would like to raise my score if the authors can explain these questions clearly."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715513858,
                "cdate": 1700715513858,
                "tmdate": 1700715513858,
                "mdate": 1700715513858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]