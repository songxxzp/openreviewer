[
    {
        "title": "Distribution Shift-Aware Prediction Refinement for Test-Time Adaptation"
    },
    {
        "review": {
            "id": "QUTGwWXMdn",
            "forum": "xqxG5WogN6",
            "replyto": "xqxG5WogN6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2344/Reviewer_D6tt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2344/Reviewer_D6tt"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a challenging test-time adaptation setup where both the label shift and covariate shift exist in the testing phase and proposes a novel TTA method, named Distribution shift-Aware prediction Refinement for Test-time adaptation (DART).\nIn particular, DART refines the predictions made by the trained classifiers by focusing on class-wise confusion patterns, introducing a learnable module to map the class distribution onto the class-to-class matrix.\nWhen combined with many TTA methods like TENT and BNAdapt, DART helps increase the accuracy under both covariate and label distribution shifts at test time."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- this paper is well-written and easy to follow\n\n- the proposed method is simple yet effective and the key idea sounds interesting\n\n- the results on many datasets are impressive"
                },
                "weaknesses": {
                    "value": "- the results are limited to sever label shifts, while the effectiveness of the proposed method under only covariate shift is not well studied\n\n- could the proposed method be extended to source-free domain adaptation like SHOT (Liang et al., ICML 2020) (more epochs)? More results are welcome to verify the versatility of the proposed method\n\n- concerning the mapping module g_\\phi, is the network design (like two-layer MLP or the hidden dimensional) sensitive?\n\n- how about the sensitivity of the batch size $B$ in the proposed method?"
                },
                "questions": {
                    "value": "see the weakness above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2344/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764388898,
            "cdate": 1698764388898,
            "tmdate": 1699636166813,
            "mdate": 1699636166813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wERW5Es90y",
                "forum": "xqxG5WogN6",
                "replyto": "QUTGwWXMdn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D6tt (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the positive feedback and summary of our work. We hope the responses to the weaknesses below resolve your concerns.\n\n>(W1) the results are limited to severe label shifts, while the effectiveness of the proposed method under only covariate shift is not well studied\n\nWe focus on the problem setup where **both** covariate and label distribution shifts occur during the test time. In Section 2, we first examine the impact of the label distribution shift on BNAdapt, which is known to effectively address covariate shifts, thus serving as the foundation for entropy minimization-based TTA methods including TENT. We observe that BNAdapt suffers from a significant performance degradation under the label distribution shift, since the updated BN statistics follow the test label distribution, which causes bias in the classifier. In Section 3, we propose a novel refinement scheme for the predictions generated by the BN-adapted classifiers under both the covariate and label distribution shifts. This refinement mainly corrects the misclassification generated from the label distribution shifts. Thus, without the label distribution shift, our module does not have a big impact in adjusting the classifier predictions. We actually observe this from the oracle analysis in Table 1 for CIFAR-10C-LT with $\\rho=1$ (balanced case), where a limited gain of 0.3% is achieved even with the oracle method (using true test labels to find the oracle square matrix $T$). Therefore, DART, which uses the same prediction modification scheme, can only achieve limited gains when there is only covariate shift. We report the experimental results of DART-applied TTA methods on balanced CIFAR-10C in Appendix E.5. We\u2019d like to point that similar trends of limited gain for balanced datasets were also reported for ODS (Zhou et al., ICML\u201923), another attempt to alleviate test-time class distribution shifts.\n\n>(W2) could the proposed method be extended to source-free domain adaptation like SHOT (Liang et al., ICML 2020) (more epochs)? More results are welcome to verify the versatility of the proposed method\n\nThe major drawback of the entropy minimization-based methods is the error accumulation in training due to the utilization of inaccurate pseudo-labels. Therefore, improving pseudo-label accuracy before test-time training, as in DART, can significantly improve performance. We expect that our method can also be utilized as a plug-in method for source-free domain adaptation (SFDA) methods that use pseudo-labels for training like SHOT (Liang et al., ICML\u201920) and NRC (Yang et al., NeurIPS\u201921). For a fair comparison, we fine-tune BN layers in pre-trained models by Adam optimizer with learning rate 1e-3 for those SFDA methods. To test the scalability of DART to SFDA methods under test-time label distribution shift, we conduct experiments on CIFAR-10C-LT of $\\rho=100$ in the offline manner (multiple epochs). SHOT and NRC utilize the prediction diversity loss  ($\\mathcal{L}_{\\text{div}} = KL(\\bar{p}||u) $, where $\\bar{p}$ is the averaged pseudo label distribution and $u$ is a uniform label distribution) to prevent the adapted classifier from predicting all data into some classes. Since the test label distributions are imbalanced, the loss may cause prediction degradation. Thus, we report the experimental results in Table R1 for both cases when this loss is used and not used.\nWhile TENT/SHOT/NRC showed almost similar test accuracy of 85.83/85.67/85.97% on balanced CIFAR-10C (i.e., $\\rho=1$), all the methods showed degraded performance on CIFAR-10C-LT of $\\rho=100$. Especially, SHOT showed poor performance on CIFAR-10C-LT due to inaccurate self-supervised pseudo labels caused by the severe class imbalance of test data. However, we can observe that DART consistently improves the performance of adapted classifiers for all the SFDA methods.\n\n**R1. Comparison under offline setup, which adapts the pre-trained models by multiple epochs. We report average accuracy (%) on CIFAR-10C-LT of $\\rho=100$.**\n|                       | w/ $\\mathcal{L}_{\\text{div}}$ | w/o $\\mathcal{L}_{\\text{div}}$  |\n|-----------------------|------------------------------|--------------------------------|\n| BNAdapt               | -                            | 66.9                           |\n| BNAdapt + DART (ours) | -                            | 83.34                          |\n| TENT                  | -                            | 74.58                          |\n| TENT+DART (ours)      | -                            | 90.21                          |\n| SHOT                  | 66.39\u00a0                       | 70.1                           |\n| SHOT+DART (ours)      | 65.86                        | 72.26                          |\n| NRC                   | 68.6                         | 81.52                          |\n| NRC+DART (ours)       | 80.45                        | 87.19                          |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101126098,
                "cdate": 1700101126098,
                "tmdate": 1700101126098,
                "mdate": 1700101126098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SJ5k2upOD4",
                "forum": "xqxG5WogN6",
                "replyto": "QUTGwWXMdn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D6tt (2/2)"
                    },
                    "comment": {
                        "value": "> (W3) concerning the mapping module $g_\\phi$ is the network design (like two-layer MLP or the hidden dimensional) sensitive?\n\nWe conducted experiments to check the sensitivity of DART over the hidden dimension $d_h$ and number of layers of $g_\\phi$, and the results are summarized on CIFAR-10C-LT of $\\rho=100$ in Table R2. We can observe that DART is robust against the change in the $g_\\phi$ structure.\n\n**R2. Sensitivity analysis about the network design of $g_\\phi$.**\n\n| |2-layer MLP (used)| | | |3-layer MLP| | | |\n|:----|:----:|:----:|:----:|:----:|:----:|:----:|:----:|:----:|\n| |$d_h = 250$|$d_h = 500$|$d_h = 1000$ (used)|$d_h = 2000$|$d_h = 250$|$d_h = 500$|$d_h = 1000$|$d_h = 2000$|\n|NoAdapt|71.13|71.13|71.13|71.13|71.13|71.13|71.13|71.13|\n|BNAdapt|66.90|66.90|66.90|66.90|66.90|66.90|66.90|66.90 |\n|BNAdapt+DART (ours)|80.6|82.17|83.34|83.83|83.83|84.27|84.78|84.97|\n|TENT|70.49|70.49|70.49|70.49|70.49|70.49|70.49|70.49|\n|TENT+DART (ours)|87.46|88.23|88.56|88.65|88.81|88.67|88.6|88.09|\n\n\n> (W4) how about the sensitivity of the batch size $B$ in the proposed method?\n\nWe conducted experiments to check the sensitivity of DART over $B$, the test batch size, and the results are summarized in Table R3.  We can observe that DART is robust against the change in $B$.\n\n**R3. Sensitivity analysis about the test batch size B. We report average accuracy (%) on CIFAR-10C-LT of $\\rho=100$.**\n\n|                     | $B=32$  | $B=64$  | $B=128$ | $B=256$  |\n|---------------------|-------|-------|-------|--------|\n| NoAdapt             | 71.13 | 71.13 | 71.13 | 71.13  |\n| BNAdapt             | 65.48 | 66.15 | 66.68 | 66.99  |\n| BNAdapt+DART (ours) | 81.7  | 82.65 | 83.17 | 83.51  |\n| TENT                | 71.89 | 71.98 | 71.48 | 69.97  |\n| TENT+DART (ours)    | 85.63 | 88.2  | 88.86 | 88.3   |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101147394,
                "cdate": 1700101147394,
                "tmdate": 1700101147394,
                "mdate": 1700101147394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c23IkE8qGB",
                "forum": "xqxG5WogN6",
                "replyto": "QUTGwWXMdn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_D6tt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_D6tt"
                ],
                "content": {
                    "title": {
                        "value": "two major concerns still remain"
                    },
                    "comment": {
                        "value": "Thank the authors for the responses. However, two major concerns remain, leading to a final score in the range of 5-6. (The main idea is interesting but the proposed method is not solid.)\n\n- As acknowledged by the authors, the proposed method seems to fail in common covariate shifts with the existence of labels shift, heavily limiting its application in real-world TTA problems.\n\n- As stated in the original review, the reviewer is curious about the performance under small batch sizes. However, only results under larger batch sizes are provided in the rebuttal.\n\nBesides, the results of source-free domain adaptation are also not persuasive. Without the strong long-tail distribution shift, would the proposed method work? Since the label distribution in common domain adaptation (DA) datasets is also not balanced, I strongly suggest the authors validate the effectiveness of the proposed method on standard DA benchmarks like OfficeHome and DomainNet.\n\nIf the authors could provide **evidence about the effectiveness of the proposed method beyond long-tail datasets**, I would increase my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641984685,
                "cdate": 1700641984685,
                "tmdate": 1700643994820,
                "mdate": 1700643994820,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yYdVJqwFtH",
                "forum": "xqxG5WogN6",
                "replyto": "HmQlBB10Zq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_D6tt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_D6tt"
                ],
                "content": {
                    "title": {
                        "value": "Could the proposed method work really work"
                    },
                    "comment": {
                        "value": "Thanks for the replies.\n\n1. Is it true that the proposed method does not work for balanced datasets like CIFAR10-C and CIFAR100-C in TTA methods?\n\n2. For domain adaptation and generalization tasks, both PACS and digits are simple, so why not try naturally-imbalanced datasets like Office and OfficeHome, I believe they are not large-scale datasets.\n\n3. Regarding the batch size, 32 is relatively larger, the authors are suggested to try smaller batch sizes like 4, and 8 instead of 32 and 64. I think many previous TTA methods in the literature have already shown similar studies in their experiments. Even some TTA methods work under a single instance (batch size = 1)."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655472797,
                "cdate": 1700655472797,
                "tmdate": 1700655472797,
                "mdate": 1700655472797,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "26cl6ArsWm",
                "forum": "xqxG5WogN6",
                "replyto": "QUTGwWXMdn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_D6tt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_D6tt"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply.\n\n1. The Oracle is the best built-in variant of your method, which does not mean that the negligible gains are acceptable. It is disappointing to find even the Oracle variant of the proposed method cannot address covariate shifts.\n\n2. Even for the ImageNet testbed, a high imbalance ratio is required in Table 3, $\\alpha>1000$. OfficeHome is much not larger, each domain only has several thousand images, which makes it easy for a quick comparison **under naturally imbalanced label shift**. By the way, is a dataset consisting of only 7 classes hard?\n\n3. For the sensitivity of the batch size, would you like to provide the results except for long-tail datasets, e.g., CIFAR10-C, CIFAR100-C, or Digits?\n\nGenerally speaking, the proposed method is limited to highly imbalanced datasets. It would be much more suitable if the authors studied the test time prior shift and compared the proposed method with counterparts [1-2] in that field.\n\n[1]. Beyond invariance: Test-time label-shift adaptation for distributions with\" spurious\" correlations\n\n[2]. Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662771379,
                "cdate": 1700662771379,
                "tmdate": 1700662858513,
                "mdate": 1700662858513,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "o6tVOkjXAD",
            "forum": "xqxG5WogN6",
            "replyto": "xqxG5WogN6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2344/Reviewer_7ZMU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2344/Reviewer_7ZMU"
            ],
            "content": {
                "summary": {
                    "value": "This study examines the impact of label distribution shifts on the test-time adaptation (TTA) methods. The authors first demonstrates how class distribution shifts degrade the performance of BNAdapt, a method that updates Batch Normalization statistics during test time. Particularly, the research found that as class imbalance increased, BNAdapt's performance worsened compared to a NoAdapt approach, which maintains the original training without any modification. The study also highlights consistent confusion patterns among classes during label distribution shifts. To mitigate performance degradation caused by these shifts, the research introduces a distribution shift-aware module that refines classifier predictions by adjusting for detected class distribution changes during test time.  In various benchmarks, DART consistently outperformed existing TTA methods, especially as class imbalance ratios increased"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- DART introduces a interesting approach to address class imbalance, presenting a significant improvement over existing TTA methods.\n- The comprehensive benchmarks validate DART's superior performance across a range of imbalance ratios.\n- The main modules of the proposed method is demonstrated through various ablation studies."
                },
                "weaknesses": {
                    "value": "- Using labeled data at intermediate time for training is a recently introduced protocol. However, compared to traditional TTA methods that cannot access labeled data, this approach may not be entirely fair. Test time adaptation, where the model learns directly during the testing phase, might be a more desirable direction.\n\n- The distribution-aware shift matrix for refinement has been frequently employed in handling label-noise datasets (Natarajan et al., 2013; Patrini et al., 2017; Zhu et al., 2021). Although the authors argue that TTA and these tasks differ, TTA essentially involves adding noise to the original data. Therefore, the nature of the problem between TTA and handling label-noise is fundamentally similar. The authors need to further elucidate the methodological distinctions between their proposed approach and existing methods (Natarajan et al., 2013; Patrini et al., 2017; Zhu et al., 2021). Additional considerations should also be clearly addressed."
                },
                "questions": {
                    "value": "- It would be beneficial if the authors provided a more detailed explanation or key intuition behind the use of averaged pseudo labels as inputs in the distribution shift-aware module during intermediate time.\n- I wonder whether T_test is updated on every batch during test time, or if it is constructed just once across the entire test dataset."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2344/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836186862,
            "cdate": 1698836186862,
            "tmdate": 1699636166716,
            "mdate": 1699636166716,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4gwVW54Efc",
                "forum": "xqxG5WogN6",
                "replyto": "o6tVOkjXAD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7ZMU (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the constructive feedback. We hope the responses to the questions and weaknesses below resolve your concerns.\n\n> (W1) Using labeled data at intermediate time for training is a recently introduced protocol. However, compared to traditional TTA methods that cannot access labeled data, this approach may not be entirely fair. Test time adaptation, where the model learns directly during the testing phase, might be a more desirable direction.\n\nWe agree with the reviewer\u2019s point that the intermediate-time training requires additional cost compared to the traditional TTA, but we\u2019d like to emphasize that the additional cost in training our module for DART is very low and such a low additional cost can bring a significant performance gain. In particular, the cost in training DART is low since (1) it utilizes the training dataset, not a novel auxiliary dataset and (2) it requires a short runtime (about 2 hours for CIFAR-10C). With this low additional cost, our method exhibits consistent and significant performance gains when combined with the traditional TTA methods as shown in Table 2 of the manuscript. For example, it shows performance gains of 5.52% and 16.44% on CIFAR-10C-LT of $\\rho=10$ and $100$, respectively.\n\n> (W2) The distribution-shift aware matrix for refinement has been frequently employed in handling label-noise datasets (Natarajan et al., 2013; Patrini et al., 2017; Zhu et al., 2021). Although the authors argue that TTA and these tasks differ, TTA essentially involves adding noise to the original data. Therefore, the nature of the problem between TTA and handling label noise is fundamentally similar. The authors need to further elucidate the methodological distinctions between their proposed approach and existing methods (Natarajan et al., 2013; Patrini et al., 2017; Zhu et al., 2021). Additional considerations should also be clearly addressed.\n\nWe\u2019d like to first clarify that the nature of the two problems, Learning with Label Noise (LLN) and Test-Time Adaptation (TTA), are different since the causes of the class-wise confusion are different. In LLN, it is assumed that there exists a noise transition matrix $T$, which determines the label-flipping probability of a sample from one class to other classes. For LLN, two main strategies have been widely used in estimating $T$: 1) using anchor points, which are defined as the training examples that belong to a particular class almost surely, and 2) using the clusterability of nearest-neighbors of a training example belonging to the same true label class. LLN uses the empirical pseudo label distribution of the anchor points or nearest-neighbors to estimate $T$. \nFor TTA, on the other hand, the misclassification occurs not based on a fixed label-flipping pattern, but from the combination of covariate shift and label distribution shift. To adjust the pre-trained model against the covariate shifts, most TTA methods apply the BN adaptation, which updates the Batch Norm statistics using the test batches. However, when there exists a label distribution shift in addition to the covariate shift, since the updated BN statistics follow the test label distribution, it induces bias in the classier (by pulling the decision boundary closer to the head classes and pushing the boundary farther from the tail classes as described in Appendix C \u201cmotivating toy example\u201d). Thus, the resulting class-wise confusion pattern depends not only on the class-wise relationship in the embedding space but also on the classifier bias originated from the label distribution shift and the updated BN statistics. Such a classifier bias has not been a problem for LLN, where we don\u2019t need to modify the BN statistics of the classifier at the test time. \n\nOur proposed method, DART, focuses on this new class-wise confusion pattern and is built upon the idea that if the module experiences various batches with diverse class distributions before the test time, it can develop the ability to refine inaccurate predictions resulting from label distribution shifts. Based on this intuition, we train a distribution shift-aware module during the intermediate time, by exposing several batches with diverse class distributions using the training datasets. As described in Equation (1) of the manuscript, the module is trained using the labeled training dataset to output a square matrix of the class dimension for prediction refinement. In this process, the module takes the averaged pseudo-label distribution as an input to learn the class-wise confusion pattern of the BN-adapted classifier depending on the label distribution shift."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100076324,
                "cdate": 1700100076324,
                "tmdate": 1700100076324,
                "mdate": 1700100076324,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "blwjErjhwc",
                "forum": "xqxG5WogN6",
                "replyto": "o6tVOkjXAD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7ZMU (2/2)"
                    },
                    "comment": {
                        "value": "> Continued response to (W2)\n\nIn Appendix D, we also reported the experimental results showing that the traditional LLN method is not effective in TTA for refining the predictions. In particular, we considered HOC (Zhu et al., 2021), one of the recent LLN methods that is known to successfully estimate the noise transition matrix under LLN scenarios. We found that HOC failed to estimate the transition matrix for CIFAR-10C-LT with the label distribution shift of $\\rho=100$. HOC estimates the transition matrix by using the empirical pseudo label distribution of nearest neighbors of each example. However, as observed in Figure 6, the nearest neighbors in the embedding space already have the same pseudo labels/predictions for the BN-adapted classifier, which makes it impossible to estimate a correct $T$ depending on the label distribution shift. \n\n> (Q1) It would be beneficial if the authors provided a more detailed explanation or key intuition behind the use of averaged pseudo labels as inputs in the distribution shift-aware module during intermediate time.\n\nAs explained in our response to (W2 of the reviewer), the distribution shift-aware module is trained to output a square matrix of the class dimension, only using the averaged pseudo labels, to fix the misclassification of the BN-adapted model originated from the class distribution shift. We designed the module to take the averaged pseudo label distribution as an input since this is the only available information at the test time. In the intermediate time, the module experiences several batches of diverse class distribution shifts and is optimized to generate a square matrix depending on each pseudo label distribution that can be multiplied to the outputs of BN-adapted classifier to minimize the cross-entropy loss between the true labels of training examples and the modified predictions. Through this process, the module learns the misclassification pattern of the BN-adapted classifier, originated from the label distribution shift, just from the observation of the average model predictions (pseudo labels).\nTo verify this argument, in Appendix E.2, we reported the experimental results of the iterative version of DART, where the DART takes the iteratively refined pseudo labels as input. Specifically, for $i \\in \\mathbb{N}$, $ T_i = g_\\phi(\\mathbb{E}\\_{x \\in D_{\\text{test}}}[\\text{softmax}(f_\\theta(x) \\Pi_{j=0}^{i-1}T_{j})]) $, where $T_0$ is set to an identity matrix. Thus, $T_1$ is the output of our original DART. In Table 7 of the manuscript, we reported the pseudo label accuracy as the iteration increases. We can observe that (1) the accuracy of the refined prediction is maximized with $T_1$, and (2) the accuracy gradually decreases as the number of iterations increases. This indicates that our distribution-shift aware module indeed requires the average model predictions (pseudo labels) of BN-adapted classifier (not a more refined test label distribution) as it is trained to fix the misclassification pattern of the BN-adapted classifier, just using the noisy pseudo labels of the model itself. \n\n>(Q2) I wonder whether T_test is updated on every batch during test time, or if it is constructed just once across the entire test dataset.\n\n$T_\\text{test}$ is constructed just once across the entire test dataset using the averaged pseudo-label distribution of the whole test data as described in the last paragraph of Section 3.\nAs an additional remark, we also considered DART for online TTA, assuming that each test data sample is encountered only once during test time, in Section 4.3. Since obtaining $T_\\text{test}$ for the whole test data is unavailable for online TTA, we modified DART to take the averaged pseudo-label distribution of the first test batch to output $T_\\text{test}$, which is then used throughout the entire test time. We summarized the experimental results of this variant of DART for online TTA in Table 4 (last row) and Appendix F. The results indicate that this online variant of DART performs similarly to the original DART but with a slight decrease in performance.\n\nHowever, for the online label distribution shift setup on ImageNet-C, the class distributions within test batches are significantly different. Thus we computed the square matrix $T$ for each test batch as described in Appendix A.5."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100092932,
                "cdate": 1700100092932,
                "tmdate": 1700220117421,
                "mdate": 1700220117421,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EYPS4p5wK2",
            "forum": "xqxG5WogN6",
            "replyto": "xqxG5WogN6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2344/Reviewer_JDRC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2344/Reviewer_JDRC"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the poor performance of TTA caused by class distribution shifts. To address this, the authors propose to refines the predictions by focusing on class-wise confusion patterns. Extensive experimental results on CIFAR, PACS, ImageNet benchmarks demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe authors conduct a empirical analysis that class distribution shifts would harm the performance.\n2.\tThe authors propose distribution shift-aware module to alleviate the test-time class distribution shifts."
                },
                "weaknesses": {
                    "value": "1.\tIn Notation part, why do the test data have labels? In Section 2, the authors calculate the cross-entropy using the test labels $ CE(softmax(f_{\\theta}(x)T), y) $. Is it a pseudo label?\n2.\tThe recent work SAR[Towards Stable Test-time Adaptation in Dynamic Wild World] also considers the case with class distribution shifts. What is the advantage of the proposed method over SAR? I found that the authors prepared the imbalanced ImageNet-C dataset following SAR. However, I failed to find the empirical comparisons between the proposed method and SAR.\n3.\tIn my understanding, the authors seek to train a distribution shift-aware module to generate a matrix for prediction refining. In this sense, the data to train such a module should be class-imbalanced. However, as mentioned in the paper, the dataset $D_{int}$ seems to be class-balanced.\n4.\tWhat is the computational cost to train a distribution shift-aware module in the \u201cintermediate time\u201d? Does it take a long time?\n5.\tIn the case without class distribution shifts, what is the performance of the proposed method compared with existing methods? Better or worse?"
                },
                "questions": {
                    "value": "If the authors could address my concern, I would raise my scoring."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2344/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2344/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2344/Reviewer_JDRC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2344/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698979700960,
            "cdate": 1698979700960,
            "tmdate": 1700642501534,
            "mdate": 1700642501534,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dWQNzysur2",
                "forum": "xqxG5WogN6",
                "replyto": "EYPS4p5wK2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JDRC (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your interest in our method. We hope that the responses to the questions resolve your concerns.\n\n> (W1) In Notation part, why do the test data have labels? In Section 2, the authors calculate the cross-entropy using the test labels $CE[\\text{softmax}(f_\\theta(x)T, y)]$. Is it a pseudo label?\n\nIn Section 2, we first presented **an oracle attempt** using the test labels to verify the effectiveness of multiplying a square matrix $T$ to the logits in refining the predictions generated by the BN-adapted classifier. Through this oracle attempt, we showed in Table 1 of the manuscript that the prediction refinement effectively enhances the performance of the BN-adapted classifier. However, during the test time, we don\u2019t have access to test data labels. Thus, in this work, we proposed a novel test-time adaptation (TTA) method DART, which estimates the square matrix $T$ just with the pseudo labels of the test data using the distribution shift-aware module, as presented in Section 3.\n\n> (W2) The recent work SAR also considers the case with class distribution shifts. What is the advantage of the proposed method over SAR? I found that the authors prepared the imbalanced ImageNet-C dataset following SAR. However, I failed to find the empirical comparisons between the proposed method and SAR.\n\nDART can be integrated as a plug-in method with any TTA methods that rely on pseudo-labels obtained from the classifiers, including SAR, since DART focuses on effectively modifying the inaccurate predictions/pseudo-labels caused by test-time label distribution shifts. Thus, DART can also be used with SAR, and we summarize the experimental results on CIFAR-10C-LT in Table R4 and ImageNet-C-imbalance in Table R5. We can observe that the performances of SAR are worse/better than those of TENT on CIFAR-10C-LT/ImageNet-C-imbalance, respectively. However, DART consistently improves the performance of the SAR in a similar way as it improved the performances of other TTA methods, since DART improves the accuracy of the initial pseudo-labels used for SAR. \n\n**R4. Average accuracy (%) on CIFAR-10C-LT**\n|                     | $\\rho=10$                            | $\\rho=100$      |\n|---------------------|--------------------------------------|-----------------|\n| NoAdapt             | 71.28                                | 71.13           |\n| BNAdapt             | 79.01                                | 66.9            |\n| BNAdapt+DART (ours) | 84.53 (+5.52)                        | 83.34 (+16.44)  |\n| TENT                | 83.02                                | 70.49           |\n| TENT+DART (ours)    | 85.13 (+2.11)                        | 88.56 (+18.07)  |\n| SAR                 | 79.76                                | 67.3            |\n| SAR+DART (ours)     | 84.90 (+5.14)                        | 83.56 (+16.26)  |\n\n**R5. Average accuracy (%) on ImageNet-C-imbalance**\n|              | $\\alpha=1000$ | $\\alpha=2000$ | $\\alpha=5000$  |\n|--------------|---------------|---------------|----------------|\n| NoAdapt      | 18.15         | 18.16         | 18.16          |\n| BNAdapt      | 19.85         | 14.11         | 8.48           |\n| BNAdapt+ours | 25.18 (+5.33) | 20.48 (+6.37) | 14.82 (+6.34)  |\n| TENT         | 22.49         | 13.52         | 6.61           |\n| TENT+ours    | 26.18 (+3.69) | 18.51 (+4.99) | 11.17 (+4.56)  |\n| SAR          | 26.46         | 17.36         | 9.09           |\n| SAR+ours     | 32.49 (+6.03) | 23.38 (+6.02) | 12.9 (+3.81)   |\n\n> (W3) In my understanding, the authors seek to train a distribution shift-aware module to generate a matrix for prediction refining. In this sense, the data to train such a module should be class-imbalanced. However, as mentioned in the paper, the dataset D_int seems to be class-balanced.\n\nOur method exposes various batches of diverse class distributions to the distribution shift-aware module by sampling the balanced $D_\\text{int}$ **using the Dirichlet sampling** as illustrated in Figure 3 of the manuscript. We make $D_\\text{int}$ class-balanced to avoid unintentional bias when sampling through the **Dirichlet distribution**. For example, if the intermediate dataset of two classes (1,2) has class distribution [0.9, 0.1], the sampled batches from the dataset are likely to have many more samples from class 1 even when Dirichlet sampling is applied."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099924019,
                "cdate": 1700099924019,
                "tmdate": 1700442060762,
                "mdate": 1700442060762,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M7lgMBWl0L",
                "forum": "xqxG5WogN6",
                "replyto": "EYPS4p5wK2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JDRC (2/2)"
                    },
                    "comment": {
                        "value": "> (W4) What is the computational cost to train a distribution shift-aware module in the \u201cintermediate time\u201d? Does it take a long time?\n\nAs reported in Appendix A.6, it takes only about 2 hours to train the distribution shift-aware module for CIFAR-10C. Since the pre-trained classifier weights are frozen and the distribution shift-aware module has a simple structure (2-layer MLP), training the module takes a much shorter time compared to the pre-training step. Additionally, the computational overhead is not significant since we train the module only once for each pre-trained classifier regardless of the test corruption types or label-distribution shifts as described in Section 3 \u201ctraining of $g_\\phi$\u201d.\n\n> (W5) In the case without class distribution shifts, what is the performance of the proposed method compared with existing methods? Better or worse?\n\nOur method refines the predictions of the classifier by multiplying a square matrix $T$ to the logits, and this process adjusts the incorrect predictions mainly originated from the class distribution shift. Thus, without the class distribution shift, multiplying $T$ does not have a big impact in adjusting the classifier predictions. For the balanced CIFAR-10C ($\\rho=1$), where there is no label distribution shift in test time, we report the experimental results of DART in Appendix E.5. We observe that DART-applied TTA methods show comparable or slightly worse performance than the original TTA methods. This is also attributed to the limited gain even with the oracle method (using true test labels to find the oracle square matrix $T$ that will be multiplied to the logits) as shown in Table 1 ($\\rho=1$). Specifically, the oracle achieved only a marginal performance gain of 0.3% on average even with the labels of test data on balanced CIFAR-10C. Therefore, DART, which uses the same prediction modification scheme, can only achieve limited gains even when generating square matrices similar to the one with the oracle. We\u2019d like to point out that similar trends of limited gain for balanced datasets were also reported for ODS (Zhou et al., ICML\u201923), another attempt to alleviate test-time class distribution shifts."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099948947,
                "cdate": 1700099948947,
                "tmdate": 1700442080147,
                "mdate": 1700442080147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jMTFTsQMuz",
                "forum": "xqxG5WogN6",
                "replyto": "M7lgMBWl0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_JDRC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_JDRC"
                ],
                "content": {
                    "title": {
                        "value": "More Questions"
                    },
                    "comment": {
                        "value": "Thanks for the authors' responses. The responses have addressed most of my concerns.\n\nBut I still have some questions about the third answer.\n\nThe authors state \"if the intermediate dataset of two classes (1,2) has class distribution [0.9, 0.1], the sampled batches from the dataset are likely to have many more samples from class 1 even when Dirichlet sampling is applied.\" I understand there would be more samples from class 1 in this case, but does it have a negative influence while training the distribution shift-aware module? In the inference stage, the test sample may be very imbalanced like this. So using such imbalanced data to train the module is appropriate in my understanding."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552444063,
                "cdate": 1700552444063,
                "tmdate": 1700552444063,
                "mdate": 1700552444063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hJ5GgcAmP8",
                "forum": "xqxG5WogN6",
                "replyto": "rjtRK1pSEP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_JDRC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2344/Reviewer_JDRC"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the reponses"
                    },
                    "comment": {
                        "value": "The explanations provided in this version are exceptionally clear and comprehensive. I hope the authors can incorporate these explanations into the revised manuscript for further clarity and completeness.\n\nAll of my concerns have been effectively addressed, leading me to adjust my evaluation to a 'weak accept'."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642456913,
                "cdate": 1700642456913,
                "tmdate": 1700642456913,
                "mdate": 1700642456913,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]