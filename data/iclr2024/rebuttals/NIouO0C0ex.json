[
    {
        "title": "Open-Source Can Be Dangerous: On the Vulnerability of Value Alignment in Open-Source LLMs"
    },
    {
        "review": {
            "id": "PUCu2fnvt0",
            "forum": "NIouO0C0ex",
            "replyto": "NIouO0C0ex",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2540/Reviewer_dUB6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2540/Reviewer_dUB6"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the risks implied by releasing model weights for capable and safety-tuned instruction-following language models. \n\nIn particular, the authors present two fine-tuning approaches which can revert the safety mechanisms built in state-of-the-art released open models. The first approach RSFT fine-tunes the model on harmful data. The second approach RVA applies direct preference optimization with the harmful response as the preferred entry in the pair."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "There has been a lot of recent debate on the pros and cons of releasing model weights in the scientific community. The paper studies how safety-tuned models can be adapted to produce harmful content at a much higher frequency. This is a timely work which highlights the risks of releasing model weights and contributes a viewpoint to the open model debate."
                },
                "weaknesses": {
                    "value": "There are several weaknesses. I list them below by category.\n\n**Risks of model release**: Authors base their studies on safety-tuned models and present approaches that revert the safety mechanisms (e.g., RLHF). However, from a practical viewpoint, a raw pretrained model can likely be more easily adapted to produce harmful content than a safety-tuned model. Given that raw pretrained models are typically released along with safety-tuned variants (e.g., Llama2 / Llama2-Chat), it's worthwhile to measure how fine-tuning approaches work there. How much easier is it to adapt a raw pretrained model to produce harmful content compared to a safety-tuned one? \n\n**Economic incentives**: The cost of performing these adversarial adaptations is likely low, considering the main method is LoRA fine-tuning on a small batch of examples. However, it's still useful to elucidate the cost of these attacks to make the analysis complete. How much does it cost ($) to adapt one model? How much time does it take? How many GPUs would you need? \n\n**Clarity on harmful content**: Authors use the generic umbrella term \"harmful\" as the inverse term of \"value-aligned\". However, it is worthwhile to clarify the types of harmful content being studied. Is it primarily hate speech and toxicity? Or is it misinformation or something else?\n\nI read the author comments and updated my score. The submission remains a borderline one given its limited novelty."
                },
                "questions": {
                    "value": "- For GPT-4 evaluation, what quality control measures have the authors adopted? To what extent are the numbers trustworthy? \n- MMLU is a useful proxy for measuring general model capability. But it is not a good proxy for assessing models' helpfulness in following instructions. Have the authors attempted other datasets for helpfulness evaluation? E.g., Anthropic's HH dataset or AlpacaEval?\n- The title is misleading for two reasons:\n  - \"Open-source\" is a very specific term which doesn't accurately describe the settings for which these attacks may be applied. For \n  instance, the supervised fine-tuning attack can be applied to API models (OpenAI fine-tuning API). In addition, certain models have \n  released weights but are not open-source -- the famous llama model is released under restricted licenses and thus doesn't count as\nopen-source. But the model itself can be called an open model. \n  - Despite the drawbacks studied in the paper, open-source also has its benefits. For instance, this research is made possible with open models with weight releases. Thus, saying \"open-source is dangerous\" conveys a limited and inadequate view."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Authors should discuss more the long-term ethical implications of this work, extending the \"implications for future work\" section."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2540/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2540/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2540/Reviewer_dUB6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2540/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698535839889,
            "cdate": 1698535839889,
            "tmdate": 1700723455254,
            "mdate": 1700723455254,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1uqPVneXIn",
                "forum": "NIouO0C0ex",
                "replyto": "PUCu2fnvt0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1**: How much easier is it to adapt a raw pre-trained model to produce harmful content compared to a safety-tuned one?\n\n**A1**: Thanks for your comments. In Appendix A.2.4, we added experiments to reverse-align pre-trained model Llama2 (7B). \n\nFirst, the ASR of the original  Llama2 (7B) is higher than that of Llama2-Chat (7B). This is because Llama2 has not undergone alignment. In cases where Llama2 fails to answer harmful questions, it is often because the model has not undergone instruction tuning, resulting in incorrect execution of instructions, rather than a refusal to respond based on values. \n\nSecond, reverse alignment can still increase the probability of non-aligned models answering harmful questions, while ensuring the model's capabilities and helpfulness.  The observations are quite natural and consistent with the statements in [1]. We would like to clarify that our work focuses more on value-aligned LLMs to investigate whether value alignment is a sufficient defense strategy for open-access LLMs and calls for more robust releasing strategies.\n\n[1] GPT-4 Technical Report.\n\n---\n\n**W2**: Economic incentives: The cost of performing these adversarial adaptations is likely low, considering the main method is LoRA fine-tuning on a small batch of examples. However, it's still useful to elucidate the cost of these attacks to make the analysis complete. How much does it cost ($) to adapt one model? How much time does it take? How many GPUs would you need?\n\n**A2**: We provide computation time in Appendix A.4. Our solution is efficient since we use LoRA and a relatively short max length (i.e., 512). In our experiments, we use 8 V100s to reverse-align LLMs. For methods using prompt-prefix pairs, it only takes around 3 hours due to their short responses. We also notice that the experiments do not occupy all the GPU memories, indicating that we could potentially use fewer GPUs to achieve reverse alignment.\n\n---\n\n**W3**: Clarity on harmful content: Authors use the generic umbrella term \"harmful\" as the inverse term of \"value-aligned\". However, it is worthwhile to clarify the types of harmful content being studied. Is it primarily hate speech and toxicity? Or is it misinformation or something else?\n\n**A3**: Thanks for your insightful comments. Our test dataset contains different types of harmful tasks, but we do not analyze the ASR of different types of attacks in our original paper. In Appendix A.2.1 of the revision, we analyze the ASRs of 13 types of attacks in ForbidQ. There are three observations from the analysis.\n1. LLMs exhibit a certain level of bias in sensitivity to different attack types.\n2. Different models have varying robustness.\n3. The effectiveness of different methods varies.\n\n---\n\n**Q1**: For GPT-4 evaluation, what quality control measures have the authors adopted? To what extent are the numbers trustworthy?\n\n**A1**: Thanks for your great comment. To verify the trustworthiness of GPT-4 evaluation, we randomly chose 200 samples to verify the consistency of the results between GPT-4 evaluation and human evaluation. 92.5% of GPT-4 judgments are consistent with human evaluation, which validates the effectiveness of GPT-4 evaluation.\n\n---\n\n**Q2**: MMLU is a useful proxy for measuring general model capability. However, it is not a good proxy for assessing models' helpfulness in following instructions. Have the authors attempted other datasets for helpfulness evaluation? E.g., Anthropic's HH dataset or AlpacaEval?\n\n**A2**:  Thanks for your insightful question. First, we added the results on MT-bench [1] to evaluate the helpfulness of the fine-tuned LLMs. Moreover, we added more benchmarks, i.e. BBH [2] and HumanEval [3], to verify the capability of LLMs is not affected in most cases. The results are added in Table 1 and Table 2.\n\n[1] Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.\n\n[2] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.\n\n[3] Evaluating Large Language Models Trained on Code."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2540/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410950102,
                "cdate": 1700410950102,
                "tmdate": 1700492011167,
                "mdate": 1700492011167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ODywl2PCFB",
                "forum": "NIouO0C0ex",
                "replyto": "PUCu2fnvt0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3**: The title is misleading for two reasons.\n\n**A3**: Thanks for your helpful suggestions. We decided to change our title to \u201cOn the vulnerability of value alignment in open-access LLMs\u201d and revise the claims in the paper to avoid possible misunderstandings. Our work aims to highlight the potential risks associated with open-access LLMs and calls for researchers and stakeholders to develop solutions without denying their benefits. \n\nYour suggestion of extending the scope to the fine-tuning API is quite reasonable and insightful. However, we currently decide to focus on investigating issues related to open-access models, for the following reasons:\n1. While our HarmD and HelpD methods can be used for the fine-tuning API, other methods require more flexible model manipulation and thus can not be achieved by fine-tuning API. For example, HarmS and HarmQ involve removing optimization for the eos token, and RDPO requires training with the DPO method instead of SFT. These methods demonstrate the flexibility of attack strategies for open-access models. \n2. The difficulty of defenses for the fine-tuning API and open-access LLMs is significantly different. Fine-tuning API on harmful dataset can be filtered out by service provider such as OpenAI, while fine-tuning open-access LLMs can be achieved without any restriction, which is more threatening as the attackers can obtain and manipulate the uncensored model in an offline, secretive, and unregulated manner. Please also refer to several strategies for responsible open-access LLM releases in the discussion section.\n\n---\n**Ethics Concerns**:\n\nTo address the ethical concerns, we added an ethical consideration section before references and discussed more potential solutions in the discussion section."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2540/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410984283,
                "cdate": 1700410984283,
                "tmdate": 1700492146506,
                "mdate": 1700492146506,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wToaJO25UM",
                "forum": "NIouO0C0ex",
                "replyto": "PUCu2fnvt0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer dUB6,\n\nWith the discussion period drawing to a close, we are eager to understand if our response has met your concerns. Any additional insights you may provide would be invaluable as we strive to refine our submission in these final days. We appreciate your time and guidance.\n\nAfter confirming with the chair and reviewing the updates to the [ICLR Author Guide](https://iclr.cc/Conferences/2024/AuthorGuide) on the 15th, we regret to learn that it is not possible to modify the paper's title and abstract in the system during this year's ICLR discussion stage. However, we have made changes in our revision to the best of our abilities to ensure that no misunderstandings occur.\n\nBest regards,\n\nAuthors of Submission 2540"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2540/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528498311,
                "cdate": 1700528498311,
                "tmdate": 1700528498311,
                "mdate": 1700528498311,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ik1L5ixDzm",
            "forum": "NIouO0C0ex",
            "replyto": "NIouO0C0ex",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2540/Reviewer_RY57"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2540/Reviewer_RY57"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the vulnerability of value-aligned open-source LLMs to reverse alignment through fine-tuning. It proposes two fine-tuning strategies over various training data types with different difficulties of collection. Ultimately, they identify some successful strategies that can consistently reverse alignment while preserving the model's utility."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Open-source LLMs are getting better and better and their nefarious uses are becoming a concern. This paper shows that simple guardrails provided by value alignment are ineffective against fine-tuning.\n+ A thorough investigation of different strategies of reverse fine-tuning."
                },
                "weaknesses": {
                    "value": "- No exploration of automated, semantic jailbreak attacks [1,2] which might be a more common tool for adversaries [3]. Instead of fine-tuning, adversaries might prefer to use these jailbreaks, which are more straightforward and don't require collecting any training data. I recommend the authors to compare fine-tuning-based reversal to such jailbreak attacks as well.\n\n- The differences between different LLMs are poorly explained. Baichuan2 model seems to be more vulnerable than Llama, and, although, there's some speculation in the paper (\"the appropriate hyperparameter Beta for Lllama2-Chat is larger\"), I would like to see a deeper exploration and explanation of these differences. For example, does more data or more aggressive fine-tuning against Llama equalize the results?\n\n[1] https://arxiv.org/pdf/2202.03286.pdf\n[2] https://arxiv.org/abs/2307.08715\n[3] https://arxiv.org/abs/2308.03825"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2540/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797905419,
            "cdate": 1698797905419,
            "tmdate": 1699636190502,
            "mdate": 1699636190502,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "swJVg06GfC",
                "forum": "NIouO0C0ex",
                "replyto": "ik1L5ixDzm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1**: No exploration of automated, semantic jailbreak attacks [1,2] which might be a more common tool for adversaries [3]. Instead of fine-tuning, adversaries might prefer to use these jailbreaks, which are more straightforward and don't require collecting any training data. I recommend the authors compare fine-tuning-based reversal to such jailbreak attacks as well.\n\n**A1**: Thanks for your helpful comments. Following your advice, we make the following two adjustments:\n\n1. We add the results of manually-written Jailbreak prompts in Appendix A.2.2. The results show that the manually-written Jailbreak prompts are effective for Baichuan2-Chat but do not work for Llama2-Chat. \n\n2. We also update the comparison in Section 5.3, where we compare reverse alignment with the state-of-the-art jailbreak attacks (hand-written jailbreaks for Baichuan, GCG for Llama2-Chat). From the results, we can see that most reverse alignment methods achieve **higher ASR than SOTA jailbreak attacks on the more robust Llama2-Chat**; while reverse alignment achieves comparable ASR with SOTA jailbreak attacks on the less robust Baichuan2-Chat, \n\n---\n\n**W2**: The differences between different LLMs are poorly explained. Baichuan2 model seems to be more vulnerable than Llama, and, although, there's some speculation in the paper (\"the appropriate hyperparameter Beta for Lllama2-Chat is larger\"), I would like to see a deeper exploration and explanation of these differences. For example, does more data or more aggressive fine-tuning against Llama equalize the results?\n\n**A2**: In our experiments, we have several observations to support Baichuan2-Chat is more vulnerable than Llama2-Chat: \nFine-tuning LLMs with benign datasets, i.e., LIMA and WizardLM, hurts the harmlessness of Baichuan2-Chat more than that of Llama2-Chat.\nRDPO is effective for Baichuan2-Chat, but does not work for Llama2-Chat.\nIn the experiments of HarmD and HarmS, we find Llama2-Chat requires larger learning rates to achieve higher ASRs.\nManually-written Jailbreak prompts can effectively attack Baichuan2-Chat, but fail to attack Llama2-Chat.\n\nThese phenomena are summarized in Appendix A.2.3. We further added some analysis in Appendix A.2.3. We observe that Baichuan2-Chat (7B) has a much lower perplexity than the other three models. Moreover, Baichuan2-Chat handles alignment mostly in the word embedding layer, while Llama2-Chat handles alignment in the early Transformer layers. These could be reasons for their varying robustness."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2540/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410571246,
                "cdate": 1700410571246,
                "tmdate": 1700491894351,
                "mdate": 1700491894351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9xzOr97zqG",
                "forum": "NIouO0C0ex",
                "replyto": "ik1L5ixDzm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer RY57,\n\nWith the discussion period drawing to a close, we are eager to understand if our response has met your concerns. Any additional insights you may provide would be invaluable as we strive to refine our submission in these final days. We appreciate your time and guidance.\n\nBest regards,\n\nAuthors of Submission 2540"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2540/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528239358,
                "cdate": 1700528239358,
                "tmdate": 1700528239358,
                "mdate": 1700528239358,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VATZJQpjwl",
            "forum": "NIouO0C0ex",
            "replyto": "NIouO0C0ex",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2540/Reviewer_X8UN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2540/Reviewer_X8UN"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the vulnerability of the safety alignment performed on open-source LLMs.  The authors propose the following two \u201creverse alignment\u201d methods: (1)  fine-tuning these aligned LLMs with harmful datasets and the objective of maximizing the log-likelihood of targeted responses (2) fine-tuning the aligned LLMs to reverse direct preference optimization (DPO) to steer the preference to harmful responses. Both of them can be performed by parameter-efficient fine-tuning techniques. The experiments across different datasets demonstrated the effectiveness of proposed fine-tuning attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper uncovers the vulnerability of safety alignment in terms of a new perspective (i.e., fine-tuning)\n2. This paper focuses on two of them (i.e., SFT and DPO) and introduces corresponding attacks utilizing reverse fine-tuning with harmful datasets, which could serve as initial works on fine-tuning-based jailbreak to motivate the community work on more robust safety alignment or stealthier attacks that could bypass safety auditing."
                },
                "weaknesses": {
                    "value": "1. The technical novelty of this paper is rather limited and the idea that fine-tuning can break existing alignment is really not something surprising.  \n\n2. The prepared dataset used to fine-tune is too broad to show the universality of the attack. For example, the authors fine-tune an aligned LLM with AdvBench while also evaluating the ASR on AdvBench. Even though they also evaluate its performance on other datasets, the fine-tuning dataset has contained all kinds of harmful scenarios, thus it is hard to demonstrate its universality on unseen harmful instructions.\n\n3. The scope of this paper is the safety vulnerability of open-sourced LLMs. Recently, close-sourced LLMs such as GPT3.5 have also provided cloud-based fine-tuning services. It would be more impactful if the scope could be extended to close-sourced models as conducted in [1].\n\n[1] Qi X, Zeng Y, Xie T, et al. Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To![J]. arXiv preprint arXiv:2310.03693, 2023.\n\n4. The performance is poor for the reverse DPO attack on Llama2-Chat?"
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2540/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699029309223,
            "cdate": 1699029309223,
            "tmdate": 1699636190443,
            "mdate": 1699636190443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gYqrvgjweI",
                "forum": "NIouO0C0ex",
                "replyto": "VATZJQpjwl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W1**: The technical novelty of this paper is rather limited and the idea that fine-tuning can break existing alignment is not something surprising.\n\n**A1**: Besides investigating a range of methods to break alignment, the most significant aspect of our work is to expose the potential risks of open-access LLMs being easily reversely fine-tuned for outputting malicious content. Through the work, we advocate for more robust alignment methods for open-access LLMs. We have also discussed some potential solutions in the discussion section and plan to explore them in our future work.\n\nMoreover, our work also reveals that value alignment is not sufficient to eliminate the harmful knowledge that is embedded in the LLM\u2019s weights. This knowledge can be reactivated with simple fine-tuning, even without introducing new harmful inputs. For example, in HarmS, the questions and prefixes are all generated by aligned LLMs. This means that we do not inject any additional harmful knowledge into the models.\n\n---\n\n**W2**: The prepared dataset used to fine-tune is too broad to show the universality of the attack. Even though they also evaluate its performance on other datasets, the fine-tuning dataset has contained all kinds of harmful scenarios, thus it is hard to demonstrate its universality on unseen harmful instructions.\n\n**A2**: Thanks for your insightful suggestions. To address the concern, we have taken the following steps in the revision:\nWe add a section (Section 5.4) to study the universality and transferability of reverse alignment. The results show that training on a set of harmful questions enables the LLM to answer unseen and dissimilar malicious questions.\nWe mark the ASR results that overlap with the training set with an asterisk (*). We would like to clarify that not all fine-tuning datasets used in our experiments are broad. For example, WizardLM and LIMA do not contain harmful tasks. TDC only contains 50 harmful tasks.\n\n---\n\n**W3**: The scope of this paper is the safety vulnerability of open-sourced LLMs. Recently, close-sourced LLMs such as GPT3.5 have also provided cloud-based fine-tuning services. It would be more impactful if the scope could be extended to close-sourced models as conducted in [1].\n\n**A3**: Thanks for your suggestions. The work in [1] is concurrent with ours and points out potential vulnerabilities in fine-tuning API. However, our work is still different from it in the following two key perspectives. \n1. **Methodology**. While our HarmD and HelpD methods can be used for the fine-tuning API, other methods require more flexible model manipulations and thus can not be achieved by fine-tuning API. For example, HarmS and HarmQ involve removing optimization for the eos token, and RDPO requires training with the DPO method instead of SFT. These methods demonstrate the flexibility of attack strategies for open-access models. \n2. **Difference of defenses**. The defenses for the fine-tuning API and open-access LLMs are significantly different. Fine-tuning API on harmful datasets can be filtered out by service providers such as OpenAI, while fine-tuning open-access LLMs can be achieved without any restriction, which is more threatening as attackers can obtain and manipulate the uncensored model in an offline, secretive, and unregulated manner. Please also refer to several strategies for responsible open-access LLM releases in the discussion section.\n\n[1] Fine-tuning aligned language models compromises safety, even when users do not intend to!\n\n---\n\n**W4**: The performance is poor for the reverse DPO attack on Llama2-Chat?\n\n**A4**: We explain and analyze this phenomenon from two perspectives. \n\nFirst, several existing works have reported that Llama2-Chat is more robust and safer, which is also observed in our experiments. In Appendix A.2.3, we summarize the observations and provide a deeper analysis of the gradients and perplexity of different LLMs. \n\nSecond, DPO (and RLHF) imposes a loss in controlling the consistency of the output distribution between the fine-tuned model and the original model, which ensures LLMs output fluent answers. Since the output probability of Llama2-Chat on harmful responses is very low, setting $\\beta$ to a large value causes the model to be unable to output harmful responses. However, if the $\\beta$ is set to a small value, Llama will not be able to output fluent sentences."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2540/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700410490166,
                "cdate": 1700410490166,
                "tmdate": 1700491827774,
                "mdate": 1700491827774,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fjcn9K3Y8K",
                "forum": "NIouO0C0ex",
                "replyto": "VATZJQpjwl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer X8UN,\n\nWith the discussion period drawing to a close, we are eager to understand if our response has met your concerns. Any additional insights you may provide would be invaluable as we strive to refine our submission in these final days. We appreciate your time and guidance.\n\nBest regards,\n\nAuthors of Submission 2540"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2540/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528212753,
                "cdate": 1700528212753,
                "tmdate": 1700528212753,
                "mdate": 1700528212753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fXSs14sYWr",
                "forum": "NIouO0C0ex",
                "replyto": "VATZJQpjwl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2540/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviwer X8UN,\n\nThank you once again for your valuable efforts in reviewing our work. We have provided detailed responses to all of your comments and regret that we were unable to engage in a discussion with you during the rebuttal phase. As the deadline for rebuttal submissions is approaching, we sincerely anticipate your feedback. Here is a summary of our responses:\n\n1. The novelty of this work: Our work aims to timely highlight the insufficiency of value alignment in preventing the **misuse of open-access LLMs**, which is a crucial and up-to-date topic nowadays. Despite alignment, these LLMs still **harbor malicious knowledge** that can be activated through various fine-tuning methods. Our work serves as a **whistleblower**, drawing the attention of researchers and the public to this issue and advocating for the development of more robust methods for open-accessing LLMs. We also discuss these more robust methods in the discussion section of our paper.\n2. The universality of the attack: We add experiments to prove that training on a set of harmful questions enables the LLM to answer unseen and dissimilar malicious questions (**Section 5.4**).  \nThe difference to close-sourced models: First, the open-access LLMs offer **more flexible manipulation options** for attack strategies. Second, **defense mechanisms** for open-access LLMs and close-source APIs are different.\n3. We explain the reason why RDPO does not work for Baichuan2-Chat by examining the differences in alignment between Llama2-Chat and Baichuan2-Chat (**Appendix A.2.3**), as well as the composition of the **DPO's loss**.\n\nBest regards,\n\nAuthors of Submission 2540"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2540/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728762445,
                "cdate": 1700728762445,
                "tmdate": 1700728762445,
                "mdate": 1700728762445,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]