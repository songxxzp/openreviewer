[
    {
        "title": "SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction"
    },
    {
        "review": {
            "id": "RM12egHW4E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6964/Reviewer_4xUi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6964/Reviewer_4xUi"
            ],
            "forum": "FNq3nIvP4F",
            "replyto": "FNq3nIvP4F",
            "content": {
                "summary": {
                    "value": "This paper proposes a method to train a diffusion model with random masking on the frame level to enable video generation, prediction, and interpolation. They demonstrate that their method is able to generate longer video and create smooth transitions between two different frames. The authors demonstrate that their method outperforms baseline methods."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper is well written, and clear\n- The paper shows good long video generations, and is able to generate complex transitions between semantically different frames"
                },
                "weaknesses": {
                    "value": "My primary concerns center around the lack of baselines and novelty. Particularly, the authors fail to cite a few very related works, that accomplish similar tasks that enable frame prediction and interpolation.\n\n- MaskViT [1], MAGVIT [2]: MaskGit-like models trained on tokenized video frames. Given the masked learning object, these models can also usually generalize to enable generation, prediction, and interpolation. MAGVIT is trained explicitly to do this. \n- MCVD [3], RaMViD [4]: These two methods seem nearly identical to the proposed method, where a video diffusion model is trained with masked latents. An exception is a lack of text-conditioning and scale in [3,4], however, I do not believe that meets the bar as a point of novelty.\n\nCould the authors please clarify on how their method is novel over the prior work mentioned above? In addition, it would be necessary to compare against a subset of these methods as baselines (or a similar model), as currently there are no baselines explicitly trained for the prediction / interpolation tasks.\n\n[1] Gupta, Agrim, et al. \"Maskvit: Masked visual pre-training for video prediction.\" arXiv preprint arXiv:2206.11894 (2022).\n\n[2] Yu, Lijun, et al. \"Magvit: Masked generative video transformer.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] Voleti, Vikram, Alexia Jolicoeur-Martineau, and Chris Pal. \"MCVD-masked conditional video diffusion for prediction, generation, and interpolation.\" Advances in Neural Information Processing Systems 35 (2022): 23371-23385.\n\n[4] H\u00f6ppe, Tobias, et al. \"Diffusion models for video prediction and infilling.\" arXiv preprint arXiv:2206.07696 (2022)."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697138815332,
            "cdate": 1697138815332,
            "tmdate": 1699636813727,
            "mdate": 1699636813727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JQw8YgNieI",
                "forum": "FNq3nIvP4F",
                "replyto": "RM12egHW4E",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "**Q1 Lack of Novlety**\n\n**A1** We will include the listed works in the Related Works in the revised manuscript\n\nWe delve into the study of a new task, termed as \"generative transition\", which aims to produce diverse and creative video segments to bridge two distinct scenes. To this end, we propose a short-to-long video diffusion model as an efficient solution, utilizing the initial and final frames of a given scene as inputs. Our model incorporates a text-based and video conditioning method to generate smooth transitional videos. This approach enables us to effectively manage the transition process, ensuring both the coherence and visual quality of the generated videos. Unlike traditional methods that rely on a pre-defined effect or simple interpolation for conditional transition effects, our model can handle more complex scenes and allows for controllable text prompts. We believe this method will inspire greater creativity in video production creators and the industry, while also presenting an intriguing research topic for the community. \n\nWe summarize our contributions as follows: \n1) We propose a new task, namely generative video transition and prediction, aiming at coherent \u201cstory-level\u201d long video generation with smooth and creative scene transition and varying lengths of videos; \n2) We adopt the random-mask-based diffusion model for generating smoothness transition video; \n3) we propose three assessing criteria for transition and extensive experiments demonstrate our method the superior performance on the metrics, as well as its ability to be applied across versatile applications\n\nWe argue that Masking is a general technique, and has been first used in NLP, e.g., BERT[1]. MCVD and RaMViD also adopted this technique and applied it to diffusion-based video generation. Deviating from them, we propose a latent-mask-based approach, which is more efficient than pixel-based approaches (e.g., MCVD and RaMViD). SEINE is able to conduct masking in more compressed space and directly learn the distribution of entire latent spatiotemporal distribution rather than explicit video distribution. \n\n[Reference]\n\n[1] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n\n**Q2 In addition, it would be necessary to compare against a subset of these methods as baselines (or a similar model), as currently there are no baselines explicitly trained for the prediction/interpolation tasks.**\n\n**A2.**\nThank you for the constructive suggestion. In our paper, we compared our model's predictions in Fig. 8 (b) with those of TATS, a state-of-the-art method for video prediction. Our methods outperformed the compared method in terms of overall quality and degradation. To perform further analysis with the random mask-based method, we used MCVD as a baseline for comparison. We adhered to the predictive settings on UCF-101. We sampled 16 frames from both our model and MCVD for comparison. The results shown below demonstrate that our method achieves a better FVD than MCVD.\n|  Method| MCVD | Ours |\n| --- | --- | --- |\n| FVD | 1143 | 406 |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733573210,
                "cdate": 1700733573210,
                "tmdate": 1700734865510,
                "mdate": 1700734865510,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kec6CJ0T1S",
            "forum": "FNq3nIvP4F",
            "replyto": "FNq3nIvP4F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6964/Reviewer_Yzaz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6964/Reviewer_Yzaz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new problem of generative transition and prediction, which can help generate story-level videos through different shot transitions. The author also proposed a short-to-long video diffusion model, which utilizes a random mask strategy for training. To evaluate the task, this paper proposes three assessing criteria. Both objective and subjective evaluation prove their proposed method\u2019s effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This article addresses the limitation of existing models that can only generate shot-level videos and proposes a method to generate story-level videos using transitions. They extend an existing video generation framework and achieve impressive results in generating long videos. They also propose a reasonable evaluation framework to assess the proposed model, and a large number of demos and quantitative evaluations demonstrate the effectiveness of their approach. The contribution of this work is significant."
                },
                "weaknesses": {
                    "value": "The author should provide a more detailed description of the model for reproducibility, including training resources, training parameters, and so on. Additionally, the author should also report scores on commonly used evaluation metrics such as FID."
                },
                "questions": {
                    "value": "I wondered how many GPUs they used and how long it takes for training. Besides, as far as I know, FID is used to evaluate video generation quality in many papers, can they provide this to make their paper more solid?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697942634206,
            "cdate": 1697942634206,
            "tmdate": 1699636813597,
            "mdate": 1699636813597,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2hJa1uAq22",
                "forum": "FNq3nIvP4F",
                "replyto": "Kec6CJ0T1S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors"
                    },
                    "comment": {
                        "value": "**Q1. The author should provide a more detailed description of the model for reproducibility, including training resources, training parameters, and so on.**\n\n**A1.** Thank you for your suggestion. Our model is trained on the video-text dataset Webvid10M dataset on 16GPUs A100 for two weeks. To eliminate the watermark, the model is trained on the internal watermark dataset for two days. The video clips input into the model are 16 frames, obtained by sampling at intervals of 6 frames. In all the training stages, we use a learning rate of 1\u00d710\u22124 and AdamW optimization. We have prepared our model and code and will release them after the anonymous review phase. We hope our model and code could contribute to the open-source community for open-would video transition and prediction.\n\n**Q2. Additionally, the author should also report scores on commonly used evaluation metrics such as FID. Besides, as far as I know, FID is used to evaluate video generation quality in many papers, can they provide this to make their paper more solid?**\n\n**A2.** Thank you for the suggestion. We conducted FVD and FID to evaluate the video generation quality of our model. The results are shown in the table below. As we demonstrate, our method outperforms the compared method.\n\n| Method  | Morphing | VQGAN-based | SD-based | Ours |\n| --- | --- | --- | --- | --- |\n| FVD | 583.9 | 445.6 | 502.0 | 181.6 |\n| FID | 34.3 | 36.7 | 62.0 | 13.4 |\n\n**Q3. I wondered how many GPUs they used and how long it takes for training.** \n\n**A3.**  Our model is trained on the video-text dataset Webvid10M dataset on 16GPUs A100 for two weeks. To eliminate the watermark, the model is trained on the internal watermark dataset for two days."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722781548,
                "cdate": 1700722781548,
                "tmdate": 1700733450033,
                "mdate": 1700733450033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T4GH6cFaQc",
            "forum": "FNq3nIvP4F",
            "replyto": "FNq3nIvP4F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6964/Reviewer_Tv5a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6964/Reviewer_Tv5a"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the author focus on a new task, \"generative transition\", which aims at smooth and creative transitions between scenes. Specifically, this paper proposes SEINE, a short-to-long video diffusion model with random masks to generate transitions frames based on textual prompts that describe transitions. Given a few unmasked frames, the proposed random-mask based diffusion model is able to generate frames at arbitrary positions. Therefore, their model can be used for tasks including generative transition, long video generation, and image-to-video animation by giving unmasked frames at different positions. \n\nIn the experiments, the authors compare their model with other baselines including morphing, VQGAN-based transition, and SD-based transition. Quantitative and qualitative results show that SEINE has better transition temporal coherence, semantic similarity across frames, and better video-text alignment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The task of generative transition is novel and rarely explored, which I believe is one of the main novelty of this paper. As current text-to-video generation models are mostly tacking short video clips, a smooth and creative transition between these short clips is of increasing importance.\n\n- The proposed random-mask based model seems a good solution for this task. In addition to generating transition frames, the random-mask based model can also deal with long-video generation and image-to-video generation by giving unmasked frames at different positions. \n\n- The quantitative and qualitative experiments demonstrate the effectiveness of the proposed model."
                },
                "weaknesses": {
                    "value": "- From the qualitative result shown in Figure 6, it seems that the transition is more like a \"interpolation\" between two scenes. For example, the frames in (row1, col4). (row2, col2), and (row2, col3) are not very natural. \n\n- In Figure 5 right part (the cat example), it seems that morphing also provides a descent transition. So for two frames with small transitions needed in between, it seems that the proposed method might add unnecessary variety/creativity. \n\n- In general, it's hard to see if the proposed method provides a good solution to this new task. The paper also lacks enough ablation study of the model architecture design. More discussions and intuitions about this task would be helpful for future works.\n\n- Some minor things: \n1. In Sec. 2, the citation for PYoCo is missing. \n2. In the last paragraph of Sec. 3, the sentence describing \"Long video\" is incomplete.\n3. Figure 4 is not easy to understand at first glance. It would be nice to add more descriptions for better readability.\n4. In Figure 10, the image on the left part has red-green-blue watermarks. Is that example from Gen-2 instead of SEINE?"
                },
                "questions": {
                    "value": "- For controllable transition generation, do we give the first and last frames unmasked to the modl for each prompt? If this is the case, I'm wondering maybe the model can also generate smooth zoom-in/out transitions without explicitly adding \"camera zoom-in/out\" in the prompt. It would be nice to provide ablation study that removes \"camera zoom-in/out\" in the prompts and see if the generation quality deteriorates.\n\n- As mentioned in the above part, it would be nice if the author can provide some discussions about what kinds of scene transitions (small transition vs large transition, same object vs different objects) their model is good at."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6964/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6964/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6964/Reviewer_Tv5a"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812782343,
            "cdate": 1698812782343,
            "tmdate": 1699636813487,
            "mdate": 1699636813487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sc2IancZjy",
                "forum": "FNq3nIvP4F",
                "replyto": "T4GH6cFaQc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors \uff08Q1&Q2)"
                    },
                    "comment": {
                        "value": "**Q1. From the qualitative result shown in Figure 6, it seems that the transition is more like a \"interpolation\" between two scenes. For example, the frames in (row1, col4). (row2, col2), and (row2, col3) are not very natural.**\n\n**A1. Re More like a \"interpolation\" :**\nFor Figure 6, our results differ significantly from those obtained through interpolation. In the newly uploaded rebuttal.pdf of Fig. R3, we have included animated gif results for better visualization. These results demonstrate our approach's diversity and creativity. For example, in sample 1, a raccoon appears to sway to the right along with the ocean waves before morphing into a figure wearing a trumpet. Sample 2 showcases a scene transition achieved through the shimmering effects of stage lighting. Meanwhile, sample 3 depicts a transition from the ocean to the stage, as if unveiled by a theater curtain. In contrast to the interpolation methods (refer to the first row in Fig. R4), the VQGAN-based and Morphing techniques seem to merely employ a simplistic fade-in, fade-out approach.\n\n**Re unnatural results:**\nWe recognize the presence of motion distortion in the intermediate frames during video transitions. Viewing these frames in isolation might seem unnatural, as they are transitional states. However, we've observed that within the video format in Fig. R3, these distortions integrate smoothly, resulting in natural transition frames when viewed as part of the entire video sequence.\n\n**Q2.In Figure 5 right part (the cat example), it seems that morphing also provides a descent transition. So for two frames with small transitions needed in between, it seems that the proposed method might add unnecessary variety/creativity.**\n\n**A2.**   We acknowledge that for minor transitions, our method may not differ significantly from morphing techniques. Small transitions may be handled using interpolation or morphing. However, for larger transitions, like scene changes, interpolation often results in blurred effects and fails to create meaningful or creative content. In these scenarios, a generative video model is necessary for producing effective transition models. We have performed additional experiments on large transitions, as depicted in Fig. R1, and compared them with results from Fig. R2. These results illustrate that our transitions are not only natural but also capable of generating diverse outcomes."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719762768,
                "cdate": 1700719762768,
                "tmdate": 1700719762768,
                "mdate": 1700719762768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PTozTfievw",
                "forum": "FNq3nIvP4F",
                "replyto": "T4GH6cFaQc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors (Q3)"
                    },
                    "comment": {
                        "value": "**Q3. In general, it's hard to see if the proposed method provides a good solution to this new task. The paper also lacks enough ablation study of the model architecture design. More discussions and intuitions about this task would be helpful for future works.**\n\n**A3. Re Ablation study of the model architecture design:**\nWe add an ablation study of our model design. Our primary focus is on the exploration of a rarely studied task: generative video transition. To address this, we introduce a random mask layer into the base T2V model, following the architecture proposed by Lavie (Wang et al., 2023). This architecture incorporates a stable diffusion v1.4 and temporal attention layers.\nTo assess the effectiveness of the introduced random mask layer, we conducted an ablation study on the zero-shot UCF-101 dataset. In this study, we utilized fixed masks to mask the first one or two frames. We autoregressively sampled the first 60 frames from a set of 10,100 video samples in the UCF-101 dataset and used Fr\u00e9chet Video Distance (FVD) as the evaluation metric for the models.\nOur findings reveal that our model outperforms fixed mask models, indicating that the random mask model exhibits greater robustness. Additionally, our random mask model demonstrates the ability to perform Image-to-Video generation, generate long videos, and execute video transitions within a single model.\n\n|fvd\t|mask 1 frame\t|mask 2 frame2|\n|---|---|---|\n|fixed mask|\t683.4\t|733.3|\n|random mask(ours)\t|672.7\t|727.3|\n\n**Re More Discussion:**\nIn addition to the failure cases we provided in the paper,  we will add discussions about the following two points in our final version: \n\n1) Large transition and small transition: our model is capable of small transitions of similar scenes for a single object and also the large transition of different scenes. While small transitions may also be achieved by interpolation or morphing methods, our model can also generate creative and diverse results. For a more comprehensive explanation and discussion, please refer to the responses provided for Question 1 and Question 6.\n\n2) text-controllable transition: The generated transition effect is influenced by both the input image and the prompt. When there is an inherent connection between the input images, the model automatically identifies similarities and generates transitions accordingly, even when the prompt is reduced or omitted. However, when specific prompt requirements are introduced, the model generates transitions based on those requirements. We conducted an ablation study to investigate controllable transition generation. For a more comprehensive explanation and discussion, please refer to the response provided for Question 5.\n\n**Re Intuitions:** \nWe delve into the study of a new task, termed as \"generative transition\", which aims to produce diverse and creative video segments to bridge two distinct scenes. To this end, we propose a short-to-long video diffusion model as an efficient solution, utilizing the initial and final frames of a given scene as inputs. Our model incorporates a text-based and video conditioning method to generate smooth transitional videos. This approach enables us to effectively manage the transition process, ensuring both the coherence and visual quality of the generated videos. Unlike traditional methods that rely on a pre-defined effect or simple interpolation for conditional transition effects, our model can handle more complex scenes and allows for controllable text prompts. We believe this method will inspire greater creativity in video production creators and the industry, while also presenting an intriguing research topic for the community."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720147139,
                "cdate": 1700720147139,
                "tmdate": 1700720222281,
                "mdate": 1700720222281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G7hkVaRQ82",
                "forum": "FNq3nIvP4F",
                "replyto": "T4GH6cFaQc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors (Q4&Q5)"
                    },
                    "comment": {
                        "value": "**Q4. Some minor things:**\n1. In Sec. 2, the citation for PYoCo is missing.\n2. In the last paragraph of Sec. 3, the sentence describing \"Long video\" is incomplete.\n3. Figure 4 is not easy to understand at first glance. It would be nice to add more descriptions for better readability.\n4. In Figure 10, the image on the left part has red-green-blue watermarks. Is that example from Gen-2 instead of SEINE?\n\n**A4.1** Thank you for the reminder. PYoco (Ge et al., 2023) is an advanced text-to-video generation model that presents a noise prior approach and utilizes a pre-trained eDiff-I (Balaji et al., 2022) as initialization. We will add the reference in the updated version.\n\n[Reference]\n\nSongwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. ICCV 2024.\n\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022\n\n**A4.2** Thank you for your reminder. We apologize for the typo and we will complete the sentence in our updated version: \"Long video involves recursive using the last few frames of the generated video as input and utilizing masks to predict the subsequent frames. By recursively iterating this process, the model can generate longer video sequences.\n\n**A4.3** Thank you for the suggestion.  We will add more descriptions to the captions of Figures for better readability: \"During training, the clean video $x_0$ is encoded into a latent code $z_0$ by a pre-trained encoder. The model is then fed with a concatenation of a randomly masked latent code, corrupted code $z_t$, and masks. During inference, transition results can be obtained by inputting the concatenation of noise, masked first and last frames of the video, and the masking tensor $\\tilde{m}_0$. Prediction results can be achieved by inputting the concatenation of noise, the first masked frame tensor, and the mask tensor.\"\n\n**A4.4** In Figure 10, we show a comparison of our results and Gen-2. In each case, our result is shown on the left and the results of Gen-2 are shown on the right. We found that our model reaches comparable results for image-to-video animation.\n\n**Q5. For controllable transition generation, do we give the first and last frames unmasked to the model for each prompt? If this is the case, I'm wondering maybe the model can also generate smooth zoom-in/out transitions without explicitly adding \"camera zoom-in/out\" in the prompt. It would be nice to provide ablation study that removes \"camera zoom-in/out\" in the prompts and see if the generation quality deteriorates.**\n\n**A.5** Thank you for your question and suggestion. We have added an ablation study for controllable transition generation. We find that the effect of the transition is determined jointly by the given image and the prompt. In the example of the panda, even when we remove the zoom in/zoom out prompt, we still observe that the transition video is performing a zoom-in. This is because the ratio of the panda in the video changes between the two given frames. To achieve a smooth transition, the model tends to zoom in during generation. Additionally, for further research, we used more complex images without a clear single subject for transitions, and the results are shown in Fig. R5. We can see that the transition effects can be somewhat controlled based on different prompts."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720697826,
                "cdate": 1700720697826,
                "tmdate": 1700720732358,
                "mdate": 1700720732358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8xPLfyDPIp",
                "forum": "FNq3nIvP4F",
                "replyto": "T4GH6cFaQc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors \uff08Q6)"
                    },
                    "comment": {
                        "value": "**Q6. As mentioned in the above part, it would be nice if the author can provide some discussions about what kinds of scene transitions (small transition vs large transition, same object vs different objects) their model is good at.**\n\n**A6.** Thank you for your constructive comment.  Compared to the previous method, we summarize our model is good at generating complicated scenes and large transitions. \nFor large transitions involving complex scenes, our model performs exceptionally well when compared to previous methods. It can generate diverse and creative video segments, effectively bridging distinct scenes. This proficiency is evident from the results demonstrated in Fig. R1, where it managed a variety of objects with ease and creativity.  We compared our model against other methods, which are illustrated in Fig. R2. This comparison further highlights the strengths of our approach over conventional techniques.\n\nWhen it comes to handling transitions involving the same object but within different scenes, as depicted in Fig. R3,  We found our model is good at handling different scenes. In contrast, Morphing and VQGAN interpolation-based method output videos often result in blurred effects and fail to create meaningful or creative content, and the SD-based method struggles to produce videos with temporal coherence."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720771460,
                "cdate": 1700720771460,
                "tmdate": 1700721910844,
                "mdate": 1700721910844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Uik1TO8KOa",
            "forum": "FNq3nIvP4F",
            "replyto": "FNq3nIvP4F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6964/Reviewer_GChA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6964/Reviewer_GChA"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the SEINE model, for generating \"story-level\" long videos from short clips. It introduces a unique problem in generative transition and prediction. Using a random-mask video diffusion based on textual descriptions, the model shows smooth transitions between scenes. To evaluate its efficacy, the authors provide three new criteria: temporal consistency, semantic similarity, and video-text alignment. Results show its potential for generating coherent long videos."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method of using masks was proposed in [1] and [2], but as far as I know, this is the first time it has been used in video transition. It could be novel.\n\n- The proposed method shows better performance on the metric compared to the baseline.\n\n- The proposed method can be applied in various areas such as long video generation and image-to-video animation.\n\n\nReferences\n\n[1] Voleti, Vikram, Alexia Jolicoeur-Martineau, and Chris Pal. \"MCVD-masked conditional video diffusion for prediction, generation, and interpolation.\" Advances in Neural Information Processing Systems 35 (2022): 23371-23385.\n\n[2] Fu, Tsu-Jui, et al. \"Tell me what happened: Unifying text-guided video completion via multimodal masked video generation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "weaknesses": {
                    "value": "- [Major] My main concern is that there is not enough quantitative evaluation of video transitions. This paper conducted quantitative experiments by randomly selecting one caption from MSRVTT and determining CLIP-TEXT. However, since video transitions occur when scenes change, it does not seem appropriate to evaluate video semantic correlation. Also, no video quality evaluation metrics (such as FVD etc.) have been considered. This makes it difficult to quantify the exact quality of generation.\n\n- [Major] Several details related to the human evaluation are missing. (such as number of frames in the generated video, the dataset used, and the questions posed in the user study.)\nWas the user study appropriately reflective of temporal coherence, text-video alignment, and semantic similarity?\n\n- [Minor] For transitions to be applied in the real-world, it would require generating more than 16 frames. Would the quality be maintained if more frames are generated?\n\n- [Minor] In Figure 5 related to video transition, the frame numbers and details are omitted."
                },
                "questions": {
                    "value": "How long does the inference take? Is it capable of handling transitions with multiple objects across more than two scenes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848627748,
            "cdate": 1698848627748,
            "tmdate": 1699636813292,
            "mdate": 1699636813292,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GguSQqid89",
                "forum": "FNq3nIvP4F",
                "replyto": "Uik1TO8KOa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the Authors (Q1~Q4)"
                    },
                    "comment": {
                        "value": "**Q1. My main concern is that there is not enough quantitative evaluation of video transitions. This paper conducted quantitative experiments by randomly selecting one caption from MSRVTT and determining CLIP-TEXT. However, since video transitions occur when scenes change, it does not seem appropriate to evaluate video semantic correlation. Also, no video quality evaluation metrics (such as FVD etc.) have been considered. This makes it difficult to quantify the exact quality of generation.**\n\n**A1.** Video transition is a relatively unexplored area in research. In our approach, we tackle this task by providing the initial and final frames of the preceding and succeeding scenes, supplemented with text control. To evaluate the effectiveness of our approach, we utilize text-video alignment. While our model generates two distinct scenes, our primary goal is to facilitate a smooth transition where the generated video frames maintain semantic coherence between the two scenes.\nSimultaneously, devising a comprehensive measure of video quality remains a challenge. Therefore, we've incorporated human evaluation as an auxiliary means to assess overall video quality. Encouragingly, both the metrics, CLIPSIM-Scenes and CLIPSIM-Frames, show positive correlations with the results from human evaluations, indicating consistency between quantitative metrics and human perception.\n\nTo further scrutinize the quality of our video transitions, we carried out FVD analysis along with frame-wise FID score calculations. The outcomes are presented in the table below. The table reveals that our model consistently delivers superior video quality compared to other methods examined in our study.\n\nMethod| Morphing | VQGAN-based | SD-based | Ours |\n| --- | --- | --- | --- | --- |\nFVD | 573.9 | 445.6 | 502.0 | 181.6 \nFID | 35.3 | 36.7 | 62.0 | 13.4 \n\n**Q2. - [Major] Several details related to the human evaluation are missing. (such as number of frames in the generated video, the dataset used, and the questions posed in the user study.) Was the user study appropriately reflective of temporal coherence, text-video alignment, and semantic similarity?**\n\n**A2.** In our human evaluation, our primary objective is to ascertain the quality of the videos generated by our model. To maintain impartiality in this assessment, we selected a random assortment of results from our quantitative evaluation. These results were generated through rigorous experiments conducted on the MSR-VTT dataset and each video comprises 16 frames.\n\nFor the user study, we presented the participants with pairs of videos - one generated by our model and the other by a comparative method. Rather than guiding their assessment, we encouraged the participants to evaluate the overall quality of each video independently. Subsequently, they were asked to cast their vote for the video they perceived as superior in quality within each pair.\n\n**Q3. For transitions to be applied in the real-world, it would require generating more than 16 frames. Would the quality be maintained if more frames are generated?**\n\n**A3.** Our model can potentially maintain quality while generating additional frames. We utilize a combination of prediction and transition techniques, guided by prompts, to ensure the desired transition effects. This approach has been successfully demonstrated in longer transitions, as seen in our long video demo on the website in Supplemental Material \u201cAdventure of a panda\u201d from 0:11 to 0:14. However, it's important to note that this method involves multiple sampling stages, which poses a challenge for efficiently generating high-quality transitions over an extended number of frames. The effectiveness of maintaining quality over more frames remains an interesting problem.\n\n**Q4. In Figure 5 related to video transition, the frame numbers and details are omitted.**\n\n**A4.** We appreciate your suggestion and will include this detail for clarity in the next version. We aimed for a fairly balanced selection of intermediate frames and chose the 5th, 9th, and 13th frames to showcase."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718470392,
                "cdate": 1700718470392,
                "tmdate": 1700718470392,
                "mdate": 1700718470392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aO0aDouThG",
                "forum": "FNq3nIvP4F",
                "replyto": "Uik1TO8KOa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses from Authors (Q5)"
                    },
                    "comment": {
                        "value": "**Q5. How long does the inference take? Is it capable of handling transitions with multiple objects across more than two scenes?**\n\n**A5.**\n**Re Inference Time:**\nThe inference time depends on the used GPU and denoising timestep. In our paper, results are generated by 250 timesteps via DDPM inference. It takes around 50 seconds to sample a transition video in size of 312 x 520 by using Nvidia A100. \n\n**Re Transitions with Multiple objects:** In addition to the simple transition for a single object, we added more results for complicated transitions for multiple objects, and the results are shown in Fig. R1. As we can see, our model is able to generate reasonable, creative and diverse transitions across various scenes.\n\n**Re Transition across more than two scenes:** Our model is designed to create transition videos between two distinct scenes. For scenarios involving more than two scenes, this could potentially be managed by generating transitions for each pair of scenes sequentially."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718816278,
                "cdate": 1700718816278,
                "tmdate": 1700718816278,
                "mdate": 1700718816278,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]