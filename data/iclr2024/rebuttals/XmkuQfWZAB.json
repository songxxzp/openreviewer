[
    {
        "title": "On Provable Benefits of Policy Learning from Human Preferences in Contextual Bandit Problems"
    },
    {
        "review": {
            "id": "pg25m3wQGF",
            "forum": "XmkuQfWZAB",
            "replyto": "XmkuQfWZAB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2133/Reviewer_5Lzq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2133/Reviewer_5Lzq"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops a theoretical comparison between these human feedback approaches in offline contextual bandits and shows how human bias and uncertainty in feedback modeling can affect the theoretical guarantees of these approaches. The proposed results seek to provide a theoretical explanation for the empirical successes of preference-based methods from a modeling perspective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe studied problem, i.e., contextual bandits with human feedback, is very well-motivated and finds important applications such as large language models.\n2.\tThe authors propose algorithms based on pessimism with suboptimality guarantees."
                },
                "weaknesses": {
                    "value": "1.\tIt seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n2.\tIt would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference? \n3.\tIn Theorem 1, the setup $C^*=2$ seems too specific. Can the result be extended to the one that allows general $C^*$ and depends on $C^*$?\n\n**-------After Rebuttal-------**\n\nThank the authors for their rebuttal. I read the authors' response and other reviewers' comments. \n\nI agree with the comments of Reviewer 5Lzq, i.e., the logic of this paper is not very reasonable. Specifically, the authors prove that the LCB algorithm with *biased* rating feedback is not sample efficient in Section 4, the pessimistic MLE algorithm with *unbiased* human feedback is sample efficient in Section 5.1, and furthermore, the pessimistic MLE algorithm with *biased* human feedback is also not sample efficient in Section 5.2. Then, without any experiments on the studied settings and designed algorithms, the authors directly come to a conclusion --- the reason that human feedback works well in practical LLMs is probably due to less bias. I do not think the theoretical results in this paper prove (or provide sufficient supports for) this conclusion.\n\nIn my opinion, the theoretical analysis in this paper is standard in the bandit and offline RL literature. The authors replied that the main purpose of this paper is to propose a theoretical explanation for the empirical phenomenon. However, with the current theoretical results under the restrictive and biased settings and assumptions, and without any experiments to connect with their settings and algorithms, I do not think the presented theory can effectively explain why human feedback works well in practical LLMs.\n\nCurrently I tend to keep my score 3, and will listen to the opinions of other reviewers and AC."
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Reviewer_5Lzq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2133/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698703981240,
            "cdate": 1698703981240,
            "tmdate": 1700545811673,
            "mdate": 1700545811673,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5V1NVOXFAW",
                "forum": "XmkuQfWZAB",
                "replyto": "pg25m3wQGF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2133/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2133/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 5Lzq"
                    },
                    "comment": {
                        "value": "We truly appreciate your comments and questions. Let us answer the questions in your review as follows:\n\n> It seems that the proposed algorithms are designed based on standard techniques, such as pessimism and MLE. The authors should elaborate more on their technical novelty. This is my main concern.\n\nWe appreciate you being clear about your main concern. In fact, the goal of this paper is **not** to propose new algorithms but to propose a theory that seeks to explain the superior performance of preference-based methods that people observe in practice. We theorize this is because human rating feedback is less robust to bias than preference feedback, so we propose a new model for it, which we believe is closer to what actually happens in practice than existing models. New theoretical results for these classic algorithms are derived under our new observation model. (As can be seen, this model considers a new type of bias and noise different from the existing results in the literature, and the standard LCB algorithm behaves quite differently under our new model. For example, it would normally converge under partial coverage defined in Assumption 1, but we have shown in Section 4.1 it would not converge under our model.) \n\n> It would be more clear to present conditions 1, 2 and 3 as assumptions. The authors should justify more on these assumptions. For example, why is condition 1 reasonable? Why the noise never changes the human preference?\n\nYou\u2019re correct that Conditions 1-3 are assumptions on the human rating model. We can make it clearer in our revision. Your question about Condition 1 is an important one. It is more thoroughly discussed in the paragraph starting with \u201cApparently\u201d in the middle of Page 4. In short, Condition 1 is necessary because it keeps the estimation problem statistically consistent or well-defined, i.e., there exists an estimator that converges to the optimal policy in probability. In fact, this is an important contribution of our human rating model in Equation (3) over the ones in the existing literature, which do not always admit a solvable problem. In addition, Condition 1 is in alignment with the assumption of the BTL model (human preference) that humans can rank two state-action pairs correctly in expectation, making the comparison between human rating and preference fairer, as explained in Remark 1 on Page 4. \n\nAs to why the noise never changes the human preference, we suppose you\u2019re asking why the bias is not in the BTL model for human preference. This is indeed an important question in our theory. It might seem artificial at first to consider bias in the model for human ratings while keeping the BTL model for human preferences unaffected by bias. This is why we include Section 5, in which we consider bias in the BTL model too and find that using human preferences does not have a better sample efficiency than using human ratings in this setting, contrary to what happens in practice. Section 5 shows the superior performance of the human preference approach does not happen because its algorithm is more statistically efficient than the algorithm used with human ratings (standard LCB), but it is likely that this performance difference is caused by a difference in model assumptions between the two approaches (i.e., one type of model/data is easier than the other). This justifies why we theorize the bias affects human ratings but not so much human preferences. We will be happy to strengthen this discussion in our revision.\n\n> In Theorem 1, the setup $C^\\star = 2$ seems too specific. Can the result be extended to the one that allows general $C^\\star$ and depends on $C^\\star$?\n\nThis is a good point. $C^\\star = 2$ is actually not crucial in our lower bound construction; it can be any constant strictly greater than 1 and would only affect the final suboptimality by a constant factor. In fact, $C^\\star = 2$ implies the data have very good coverage (the distribution shift is no more than a factor of 2 over all state-action pairs). The algorithm would only incur a worse constant lower bound under distribution shift with larger $C^\\star$, which proves our point even better. Since the goal of this theorem is just to establish the suboptimality is a constant and does not decay as sample size increases, rather than to delineate a specific suboptimality rate, the current presentation is sufficient to demonstrate our point.\n\nPlease let us know if this reply is able to address your concerns, and don\u2019t hesitate to let us know if you have any other questions or if you have any suggestions about revision. We look forward to your reply. Thank you!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2133/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173843304,
                "cdate": 1700173843304,
                "tmdate": 1700173843304,
                "mdate": 1700173843304,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oTBCsVIGBI",
            "forum": "XmkuQfWZAB",
            "replyto": "XmkuQfWZAB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2133/Reviewer_jnsn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2133/Reviewer_jnsn"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the benefits of using preference for learning reward functions in contextual bandits. Through a theoretical analysis for offline contextual bandits, the paper examines how human biases and uncertainties affect these methods' theoretical guarantees. They provided a theoretical explanation for the empirical success of preference-based methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I found this paper easy to follow. All theoretical conditions and results are clearly stated, and sufficient remarks are followed. \n\nThe way that they study the biased reward (through the definition of a transformation $h$) is interesting."
                },
                "weaknesses": {
                    "value": "My biggest concern is that I found the logic of this paper a bit confusing. The authors first showed that LCB failed to achieve the desired statistical guarantee under the biased model (section 4). Then, they showed that pessimism MLE can achieve better statistical results under an unbiased model (section 5.1). This comparison is clearly unfair. Hence, the authors further studied learning preference from the biased model (section 5.2) and showed that the results are actually worse. They then remarked that\n\n> This shows if one assumes a similar amount of human bias and uncertainty in both types of human feedback, the preference-based approach is no more sample-efficient. This actually contradicts with the empirical observations in the existing literature, which suggests preference-based methods have superior performance. Hence, our theory shows the bias-free modeling plays a great role in the lower sample complexity of preference-based methods, and our theoretical results can conversely confirm the standard BTL modeling of human preference feedback\u2014it is reasonable to believe human preference data is indeed subject to less bias and uncertainty in practice.\n\nMy understanding is that, the authors are not trying to use *theory* to verify *empirical success* (which I was expecting), but rather, they use *empirical success* to prove the *theory*. Hence, it appears that this paper has undertaken a completely contrary endeavor. The authors seem to haven't truly shown any benefits of using preference from pure theory; on the contrary, the conclusions they have drawn are rather contradictory (Theorem 4). Their sole argument positing the superiority of preference relies on the fact that, in practice, it yields better experimental results, thereby suggesting that the preference is unlikely to be significantly biased. If it is really what the authors intended to convey, I don't think this result is a \"provable\" benefit but rather heuristic. This leaves me quite confused, and I hope the authors can clarify this point.\n\n\n\nSome other issue: the lower bound results (theorem 1 & 2) only considered the LCB algorithm. It will be more convincing to establish a universal and information-theoretic lower bound, i.e., a lower bound that holds for any algorithm."
                },
                "questions": {
                    "value": "Is the studied algorithm, pessimistic MLE, computationally efficient? If it is not, I don't think it is fair to compare it with the more efficient LCB algorithm. Actually this question circles back to the previous one: can the lower bound be applicable to any algorithm and not solely limited to LCB?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Reviewer_jnsn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2133/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713856236,
            "cdate": 1698713856236,
            "tmdate": 1700627220379,
            "mdate": 1700627220379,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NLRf1RcDzC",
                "forum": "XmkuQfWZAB",
                "replyto": "oTBCsVIGBI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2133/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2133/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer jnsn (Part 1)"
                    },
                    "comment": {
                        "value": "We truly appreciate your comments and your thorough review. Let us answer the questions in your review as follows: \n\n> My biggest concern is that I found the logic of this paper a bit confusing\u2026\n\n> My understanding is that, the authors are not trying to use theory to verify empirical success (which I was expecting), but rather, they use empirical success to prove the theory\u2026 \n\nYes, this is a good question. Overall, what this paper tries to do is propose a theory that can explain a phenomenon in practice. The logic of this paper is intended to be as follows: \n\nLearning with human preference data is observed to yield much better performance in practice, compared to prior efforts of learning with human ratings. It still remains a question what actually causes this superior performance. Thus, we seek to explain this phenomenon by proposing a theory: human ratings are actually subject to more human bias in practice, similar to the model we propose in Section 3, while the generation of human preferences is more robust to such bias (the BTL model that people currently use is already faithful to the real world). It has been a common belief that it is more accurate for human annotators to compare than to rate directly [1,2], but granted, it might seem unfair at first to consider bias in the model for human ratings while keeping the BTL model for human preferences unaffected by bias. We justify this in Section 5, in which we consider bias in the BTL model too and find that using human preferences does not have a better sample efficiency than using human ratings in this setting, contrary to what happens in practice. This shows the superior performance of the human preference approach does not happen because its algorithm (pessimistic MLE) is more statistically efficient than the algorithm used for human ratings (standard LCB); this performance difference is more likely caused by a difference in model assumptions between the two approaches (i.e., one type of model/data is easier than the other). \n\nOverall, the goal of this paper is to propose a reasonable theory for a real-world phenomenon. It would be possible to \u201cuse theory to verify empirical success\u201d only if the mathematical formulation and underlying assumptions of the problem of interest are clear. However, it is not the case in this problem, since it is unclear what causes the human preference approach to be better in practice. (Clearly the basic model with subgaussian random reward from the theoretical literature falls short of explaining this.) For this reason, we think it is more appropriate to propose a (heuristic) theory and make sure it is in line with the real-world phenomenon and is also justifiable.\n\n[1] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023).\n\n[2] Kevin R. Tarlow, Daniel F. Brossart, Alexandra M. McCammon, Alexander J. Giovanetti, M. Camille Belle, and Joshua Philip. Reliable visual analysis of single-case data: A comparison of rating, ranking, and pairwise methods. Cogent Psychology, 8(1):1911076, 2021. doi: 10.1080/23311908.2021.1911076.\n\n> Some other issue: the lower bound results (theorem 1 & 2) only considered the LCB algorithm. It will be more convincing to establish a universal and information-theoretic lower bound, i.e., a lower bound that holds for any algorithm.\n\nThis is a good suggestion. Theorem 2 can be made into a universal lower bound, but Theorem 1 is specific to algorithms that use pessimism. In fact, we tailor these lower bounds to specific algorithms because together with Section 5, it rules out any algorithmic factor that causes the human preference approach to perform better. It serves our message better to keep the algorithm in the picture. If we proved an info-theoretic lower bound, the message would become \u201cthe human rating data following the model in Equation (3) are fundamentally harder than human preference data following the BTL\u201d. Although it still is something we want to show, it no longer has the additional benefit of ruling out the algorithmic factor."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2133/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173622943,
                "cdate": 1700173622943,
                "tmdate": 1700173622943,
                "mdate": 1700173622943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "82sbLUazo0",
                "forum": "XmkuQfWZAB",
                "replyto": "oTBCsVIGBI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2133/Reviewer_jnsn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2133/Reviewer_jnsn"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response! I really appreciate that the authors went through the logic of the paper again in detail. However, I still can't find the logic of the paper to make sense. If my understanding is correct, the theoretical conclusion of this paper is still not drawn from theoretical derivation but from empirical observation, as the author said in response\n\n> ... this performance difference is more likely caused by a difference in model assumptions between the two approaches ...\n\nTo be more specific, the authors equivalently did the following in sequence: (1) proposed some theoretical assumptions (the biased ones), (2) did some derivation, (3) observed that the theoretical results don't match the empirical performance, (4) claimed that this mismatch is due to a wrong assumption that they started with, and finally (5) turned to the other assumption (the bias-free one) and claimed that this explains the benefit of learning from preference. \n\nI still don't think this logic is convincing. In my opinion, the more correct and common way to show the provable benefit of something is the following: (1) propose some assumptions and **some justification for the assumption**, (2) do some derivation and get some results, and (3) observe that the theoretical results match the empirical performance. By comparing this logic with that of the paper, I found that this paper doesn't have an explicit part that shows **some justification for the assumption**. Instead, the way that they justify the assumptions is by empirical observation. In particular, they justify the \"bias-free\" assumption by the empirical success, which is exactly what they need to theoretically justify. So I feel there is a **circular reasoning**.\n\nMoreover, the fact that the paper failed to show the superiority of learning from preference under equal assumptions (ie, the biased one) is unnecessarily due to that learning from preference is bias-free. Actually, this concluded assumption seems super strong in the sense that humans can't be bias-free anyway (but the authors removed the \"bias\" in the formulation in section 5.1, although I admit that the BTL has some randomness itself). I would suggest the authors to try some more refined assumptions or analyses instead of \"bias\" vs \"bias-free\", which also seems to trivialize the problem in the sense that \"bias-free\" should intuitively be better.\n\nAgain I greatly appreciate the detailed response by the authors. However, I still fail to see the rationale of this work. Please correct me if I missed any key parts. I still can't raise my assessment of this work for now since my biggest concern still exists."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2133/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627154262,
                "cdate": 1700627154262,
                "tmdate": 1700627154262,
                "mdate": 1700627154262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9VYJS7JbE8",
            "forum": "XmkuQfWZAB",
            "replyto": "XmkuQfWZAB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2133/Reviewer_TtXD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2133/Reviewer_TtXD"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the effect of bias in human feedback for contextual bandits in two settings: rating feedback (i.e. direct access to the reward function), and preference (comparison) feedback.\n\nThe effect of human biased is first quantified by proving sub-optimality bounds of two (previously existing) algorithms for contextual bandits with human feedback. The novelty in the presented bounds lie in the fact that feedback (rating or preference) is received through a bias transform.  It is later shown than for a certain class of bias transformations, solving a bandit problem with biased rating feedback, always requires more samples than solving the same problem with biased preference feedback. This is an interesting phenomena as works against the prior conception that solving a bandit/RL problem with preference feedback is more complex than with rating feedback.\nAt the same time, this statement is not entirely surprising, since the BTL preference feedback model is robust to the considered class of bias transformations."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "My summary should reflect some strengths of the paper, I spell out a few more below.\n\n* The paper is well written and straightforward.\n\n* It tackles an important problem and gives a clear answer. In particular, Theorem 4, which is algorithm independent, has a really nice formulation.\n\n* I am not aware of prior work on Bandits/RL with human rating to assess how novel Theorem 1 is compared to previous sub-optimality of the LCB algorithm (e.g. in terms of techniques used to derive it). However, Theorem 1 and 3 clearly characterize the effect of a transformed/biased rating feedback."
                },
                "weaknesses": {
                    "value": "* Given the assumptions on the bias transform $h$, I am not surprised that the preference-based feedback is rather invariant to such biases. So I am not sure if the final results are uncovering an informative phenomena.\n\n* A number of prior works on contextual bandits with preference feedback is not mentioned. While the overall approach is sufficiently different (they optimize a least squared loss), I think they should be mentioned for completeness.\n  - Mehta, Viraj, et al. \"Kernelized Offline Contextual Dueling Bandits.\" arXiv preprint arXiv:2307.11288 (2023).\n  - Dud\u00edk, Miroslav, et al. \"Contextual dueling bandits.\" Conference on Learning Theory. PMLR, 2015.\n  - Saha, Aadirupa, and Akshay Krishnamurthy. \"Efficient and optimal algorithms for contextual dueling bandits under realizability.\" International Conference on Algorithmic Learning Theory. PMLR, 2022.\n- Bengs, Viktor, Aadirupa Saha, and Eyke H\u00fcllermeier. \"Stochastic Contextual Dueling Bandits under Linear Stochastic Transitivity Models.\" International Conference on Machine Learning. PMLR, 2022.\n- Perhaps also: Bengs, Viktor, et al. \"Preference-based online learning with dueling bandits: A survey.\" The Journal of Machine Learning Research 22.1 (2021): 278-385."
                },
                "questions": {
                    "value": "- How would you go beyond tabular setting? Would you say the pessimistic MLE algorithm can be easily extended to say, a kernelized or linear rewards over a compact domain?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2133/Reviewer_TtXD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2133/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858145195,
            "cdate": 1698858145195,
            "tmdate": 1699636146108,
            "mdate": 1699636146108,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zf53LeLyEC",
                "forum": "XmkuQfWZAB",
                "replyto": "9VYJS7JbE8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2133/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2133/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We truly appreciate your comments and support. Let us answer the questions in your review as follows: \n\n> Given the assumptions on the bias transform $h$, I am not surprised that the preference-based feedback is rather invariant to such biases. So I am not sure if the final results are uncovering an informative phenomena.\n\nYour question is completely valid. The theoretical results are not meant to be surprising. There are two reasons for this. First, it is already a common belief that it is more accurate for human annotators to give preference feedback than to give rating feedback [1,2], but there hasn\u2019t been any work that formalizes this theoretically. Furthermore, it is also notable that the bias in our rating model is monotone and does not change the optimal policy of the problem, so it is actually not immediately obvious why such bias makes learning from human ratings harder. Second, while it is not surprising that the robustness of human preferences to bias is a factor of why preference-based methods perform better in practice, we make an important contribution by pinpointing this factor as the likely primary cause among other factors. Specifically, we rule out the algorithmic factor by showing the pessimistic MLE used in preference-based approach is no more statistically efficient than the standard LCB used in rating-based approach.\n\n[1] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023).\n\n[2] Kevin R. Tarlow, Daniel F. Brossart, Alexandra M. McCammon, Alexander J. Giovanetti, M. Camille Belle, and Joshua Philip. Reliable visual analysis of single-case data: A comparison of rating, ranking, and pairwise methods. Cogent Psychology, 8(1):1911076, 2021. doi: 10.1080/23311908.2021.1911076.\n\n> A number of prior works on contextual bandits with preference feedback is not mentioned. While the overall approach is sufficiently different (they optimize a least squared loss), I think they should be mentioned for completeness.\n\nThank you for providing these references! We will include them in our revision.\n\n> How would you go beyond tabular setting? Would you say the pessimistic MLE algorithm can be easily extended to say, a kernelized or linear rewards over a compact domain?\n\nThere is no problem extending pessimistic MLE to a more general function approximation, as it has been studied in the linear function approximation setting by [1] and the general function approximation setting by [2]. However, how to extend our newly proposed rating model to the linear setting still needs investigation. Recall our model considers a general bias that is likely nonlinear in the real world. While a tabular algorithm is able to learn such biased reward, linear function approximation would suffer a huge approximation error; RKHS and more general function approximation like neural networks should be flexible enough to approximate such biased reward.\n\n[1] Banghua Zhu, Jiantao Jiao, and Michael I Jordan. Principled reinforcement learning with hu- man feedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270, 2023.\n\n[2] Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline reinforcement learning with human feedback. arXiv preprint arXiv:2305.14816, 2023.\n\nPlease don\u2019t hesitate to let us know if you have any further questions. Thank you!"
                    },
                    "title": {
                        "value": "Reply to Reviewer TtXD"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2133/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173512803,
                "cdate": 1700173512803,
                "tmdate": 1700173700031,
                "mdate": 1700173700031,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]