[
    {
        "title": "Anytime Neural Architecture Search on Tabular Data"
    },
    {
        "review": {
            "id": "Z9cPvAJNDp",
            "forum": "YlleMywQzX",
            "replyto": "YlleMywQzX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1065/Reviewer_wdfZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1065/Reviewer_wdfZ"
            ],
            "content": {
                "summary": {
                    "value": "The contribution of paper consists of three works. First, they construct NAS-Bench-Tabular, a DNN-based search space for tabular data. Second, they propose a novel performance proxy of neural network, ExpressFlow. ExpressFlow is based on neuron saliency, but also reflects the difference of importance with depth. Finally, they propose ATLAS, a NAS approach to tabular data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "To the best of my knowledge, this paper is the first NAS benchmark for tabular data. This work will reduce unnecessary redundant experimentation with NAS for tabular data. The proposed ExpressFlow and ATLAS show better performance than the existing Tabular NAS method despite having a relatively simple structure."
                },
                "weaknesses": {
                    "value": "The search space of NAS-Bench-Tabular(5-layer DNN with different width) is too simple. This search space do not contain most of modern neural network architecture for tabular data[1].\n\nMost of the candidate architectures get similar 97~98% accuracy on Frappe dataset. I wonder how stable the recorded accuracies are.\n\nAll three datasets are about binary classification tasks. Regression, Multi-class classification tasks are needed.\n\nDNNs are not the dominant methodology for tabular data, and machine learning techniques such as XGBoost and CatBoost have shown competitive performance [1]. With a limited search space for DNNs, it's hard to see how ATLAS has an advantage over models like AutoSklearn[2] and AutoGluon[3].\n\n\n[1] Gorishniy, Y., Rubachev, I., Khrulkov, V., & Babenko, A. (2021). Revisiting deep learning models for tabular data. Advances in Neural Information Processing Systems, 34, 18932-18943.\n\n[2] Feurer, M., Eggensperger, K., Falkner, S., Lindauer, M., & Hutter, F. (2022). Auto-sklearn 2.0: Hands-free automl via meta-learning. The Journal of Machine Learning Research, 23(1), 11936-11996.\n\n[3] Erickson, N., Mueller, J., Shirkov, A., Zhang, H., Larroy, P., Li, M., & Smola, A. (2020). Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505."
                },
                "questions": {
                    "value": "In general, performance proxies have the advantage of being data type-agnostic, which is also a benefit of deep learning. Why do we need a TRAILER that specializes in tabular data? And what makes ExpressFlow specializes on tabular data?\n\nIn Equation 2, aren\u2019t Successive Halving Algorithm allocates different budget U and candidate K for each round?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Reviewer_wdfZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763136485,
            "cdate": 1698763136485,
            "tmdate": 1699636032818,
            "mdate": 1699636032818,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ecBq3CFWlN",
                "forum": "YlleMywQzX",
                "replyto": "Z9cPvAJNDp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your constructive comments and positive feedback. We would like to address your concerns below.\n\n>The search space of NAS-Bench-Tabular ... tabular data[1].\n\n**Response:** Existing studies show that deep neural networks (DNNs) can already achieve state-of-the-art performance on tabular data [4, 5], and the technical challenge is to configure DNNs with the right number of layers and hidden layer sizes for each layer [1]. Therefore, we design a DNN-based search space that comprises up to 160,000 candidate DNNs with an extensive collection of different layer configurations.\n\nAs shown in the experiments and visualizations in Section 4.1.1 and Appendix C.3, this search space contains a diverse set of architectures with a wide range of parameter sizes, configurations, and performances, which captures the main properties of DNNs for benchmarking NAS approaches on tabular data.\n> Most of the candidates... stable the recorded accuracies are.\n\n**Response:** We have summarized the statistics of our 160,000 trained architectures on the Frappe dataset in the table below.\n\n| Mean (AUC) | Standard Deviation (AUC) | Maximum (AUC) | Minimum (AUC) |\n| - | - | - | - |\n| 97.68%     | 0.0021                   | 98.14%        | 85.72%        |\n\nThe very low standard deviation (0.0021) indicates that most architectures closely align with the mean AUC. While the significant gap between the maximum AUC (98.14\\%) and minimum AUC (85.72\\%) suggests that the number of hidden neurons in each layer is crucial in determining the architecture performance. \n> All three datasets are about ... multi-class classification tasks are needed.\n\n**Response:** As suggested, we evaluated ATLAS on additional datasets: three for multi-class classification and two for regression.\nThe results of multi-class classification are presented in our global response to the _Evaluation of ATLAS on more Datasets_.\nThe results of the regression are shown in the table below where we have measured the Root Mean Square Error (RMSE).\n\n| **Dataset**         | **XGBoost** | **CatBoost** | **ATLAS** | **Searched Architecture** |\n| - | - | - | - | -| \n| California Housing  | 0.139       | **0.131**    | 0.141     | 384-256-256-512           |\n| Pol                    | 4.773       | 4.698        | **2.264** | 384-256-128-256           |\n\nATLAS outperforms both XGBoost and CatBoost on Pol datasets with lower RMSE.\nWhile for the California Housing dataset, ATLAS achieves comparable results with others.\n>DNNs are not the dominant methodology ...  AutoGluon[3].\n\n**Response:** As suggested, we have compared ATLAS with XGBoost, CatBoost, AutoSklearn, and AutoGluon on eight datasets.\nThe results are presented in our global response to the _Evaluation of ATLAS on more Datasets_.\n\n> In general, performance proxies ... And what makes ExpressFlow specialize in tabular data?\n\n**Response:** Existing data type-agnostic TRAILERs such as SynFlow do not perform well on tabular datasets as shown in Table 1 of the original paper. Therefore, We are motivated to propose a new TRAILER specifically designed for tabular data.\n\nTo this end, we first characterize the trainability and expressivity of existing training-free metrics initially proposed for image data. Then, we propose a new TRAILER tailored for tabular data based on neuron saliency. \nThis TRAILER characterizes both properties for DNNs on tabular data (see discussion in Section 3.2 and theoretical analysis in Appendix D.3), by considering the activation value of neurons that are basic units to extract features in DNNs and the derivatives of neurons that indicate the importance of features extracted by neurons.\n\n> In Equation 2, ... candidate K for each round?\n\n**Response:** Our ATLAS includes both filtering and refinement phases. For the refinement phase, it uses the Successive Halving Algorithm (SUCCHALF) to identify the best-performing architecture from the $K$ architectures, with each training for at least $U$ epochs.\n\nIndeed, SUCCHALF allocates a different budget of $U * \\eta^i$ (with $U$ doubling if $\\eta$ = 2) and a number of candidate architectures $\\lfloor K/\\eta^i \\rfloor$ (halving if $\\eta$ = 2) for training-based evaluation in the $i$-th round (with i starting from 0). \nFor example, in the first round of evaluation, SUCCHALF trains $K$ architectures, with each training for $U$ epochs. \nIn the second round, the higher-performing $\\lfloor K/\\eta\\rfloor$ architectures are retained, with each training for $U * \\eta$ epochs. This process iterates for $\\lfloor\\log_{\\eta}K\\rfloor$ training rounds until one single architecture remains.\n\nWe hope our responses above have addressed your concerns and can improve your evaluation of our work.\n\n[1] Revisiting Deep Learning Models for Tabular Data [Gorishniy et al., NeurIPS 2021].\n\n[4] Well-tuned simple nets excel on tabular datasets [Kadra et al., NeurIPS 2021].\n\n[5] TabNAS: Rejection Sampling for Neural Architecture Search on Tabular Datasets [Yang et al., NeurIPS 2022]."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683473136,
                "cdate": 1700683473136,
                "tmdate": 1700684853694,
                "mdate": 1700684853694,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fFStFJiPuA",
            "forum": "YlleMywQzX",
            "replyto": "YlleMywQzX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1065/Reviewer_uv1h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1065/Reviewer_uv1h"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new NAS algorithms for tabular data, which is based on both zero-cost proxies for distinguishing between good and bad architectures, as well as a multi-fidelity method (Successive Halving) for allocating budget to these promising architectures selected in the first phase. Another contribution of this paper is the creation of NAS-Bench-Tabular, which comprises a search space with a total of 160k unique trained and evaluated architectures. A new zero-cost proxy, tailored for tabular data, is theoretically derived and is compared to existing zero-cost proxies previously designed for vision tasks, exhibiting better performance in the selected benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The motivation to work on anytime NAS algorithms for tabular data is valid considering the significance of the problem and domain.\n\n- The *ExpressFlow* zero-cost proxy (ZCP) is specifically tailored to the tabular modality and is justified by a thorough theoretical derivation and later backed by empirical results.\n\n- The authors propose and construct a new NAS benchmark for tabular data, named NAS-Bench-Tabular. The search space is exhaustively evaluated over 3 datasets. This is certainly very useful for the community, as the previous NAS benchmarks have been, and I am certain it will accelerate the research speed on NAS for tabular data.  \n\n- The proposed ZCP is fairly compared to other state-of-the-art ZCPs that were designed mainly for the image domain and the respective architectures, and the results show that *ExpressFlow* outperforms them by a considerable margin.\n\n- The idea to incorporate *ExpressFlow* into regularized evolution to evolve a population of architectures that optimize the ZCP and then run SuccessiveHalving to allocate budget to more promising configurations inside the evolved population is simple and effective.\n\n- The paper is easy to follow and I really enjoyed reading it. The experimental setup and results are demonstrated clearly.\n\n- The authors provide the code necessary to reproduce the results in the paper."
                },
                "weaknesses": {
                    "value": "Despite the strengths I mentioned above, there are some really important points that have to be addressed, especially regarding the empirical evaluation.  \n\n- **Benchmark needs to be more diverse for practical purposes**: I think that the creation of NAS-Bench-Tabular is very useful, however when it comes to tabular data, a lot of datasets contain just a handful of training examples (less than 1000), and more features than the datasets the authors chose. Refer to [1] (Table 9 in the appendix) for an example. \n\n- **Evaluation of ATLAS on more diverse datasets**: This paper contains multiple contributions, starting from the proposal of a new NAS benchmark for tabular data to the NAS algorithm for tabular data. While these are very useful and provide interesting insights, it can also introduce many biases. Typically, one would design an algorithm that works on benchmarks (real ones) that the community already uses in their research. Firstly designing a NAS benchmark for tabular data and then proposing a method and evaluating it only on that benchmark does not guarantee the same behavior on the real benchmarks. Therefore, I would really be interested to see how ATLAS works on commonly used OpenML tabular benchmarks (e.g. the ones from [1], Table 9), that are more diverse and lie more into the small-data regime. If the authors provide competitive results on those datasets, I am willing to increase my score.\n\n- **Comparison with conventional tree-based models**: To assess the usefulness of ATLAS in practice, I think the authors should compare the best found network with conventional tree-based models, as it is done in [1], [2] and [3], for instance. An easy and fair experimental setup would be: (1) Run ATLAS for a given time budget to find architecture **x**; (2) Train and evaluate **X** on the full fidelity (if not already evaluated in (1)); (3) Run tree-based models for *t* time (total runtime of (1) + runtime of (2)).\n\n- **Comparison to SOTA for DL for tabular data**: A thorough comparison to neural networks that achieve SOTA for tabular data is necessary to highlight the usefulness of NAS in this domain. Similar to the point above I would recommend the authors to compare to TabPFN [3]. If the time permits, I would recommend the authors to run ATLAS on the same settings the TabPFN authors used to obtain Figure 5 in their paper.\n\n**Minor**\n\n- It would be great if the authors follow the already used nomenclature when referring to zero-cost proxies, and not rename them to TRAILERs.\n\n- In Section 2, third paragraph, it is written \"The performance obtained by the training-based architecture evaluation approaches is accurate\", however this is not always the case, especially when considering proxy models (less layers, channels, etc.) or performance evaluation based on the one-shot model weights (e.g. as in DARTS).\n\n- Most of blackbox algorithms are also anytime algorithms, and they are widely used for NAS as well. Therefore, the claim that ATLAS is the first NAS anytime algorithm (in abstract) for tabular data is wrong considering that the architectural parameters have been optimized using these methods (e.g. Bayesian Optimization) and could easily be used to optimize architectural hyperparameters for tabular data networks as well. Ideally, one wants an optimization method that is modality agnostic. ATLAS is the first anytime algorithm tailored for tabular data because of *ExpressFlow* is designed for tabular data. And yes, *ExpressFlow* is the first zero-cost proxy designed for tabular data, so I would emphasize that instead.\n\n**References**\n\n[1] https://arxiv.org/pdf/2106.11189.pdf\n\n[2] https://arxiv.org/pdf/2106.11959.pdf\n\n[3] https://arxiv.org/pdf/2207.01848.pdf"
                },
                "questions": {
                    "value": "- Why did the authors pick those 3 datasets for their benchmark? Is it because of the large number of training examples?\n\n- Is ATLAS performant on datasets with less than 1000 data points?\n\n- How many data points were used to compute the rank correlation in Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Reviewer_uv1h"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698909734019,
            "cdate": 1698909734019,
            "tmdate": 1700685869876,
            "mdate": 1700685869876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TxA6eulRsC",
                "forum": "YlleMywQzX",
                "replyto": "fFStFJiPuA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your valuable feedback and constructive comments on helping improve our paper. We would like to address your concerns below.\n>Benchmark needs to be more diverse ... for an example.\n\n>Evaluation of ATLAS on more diverse datasets ... increase my score.\n\n>Is ATLAS performant on datasets with less than 1000 data points?\n\n**Response:** As suggested, we have evaluated our ATLAS on more datasets from the open-source OpenML AutoML Benchmark, and the results are presented in our global response to the _Evaluation of ATLAS on more Datasets_.\n> Comparison with conventional tree-based models ...  runtime of (2)).\n\n**Response:** As suggested, we compared ATLAS with XGBoost, assessing their AUC on the Frappe dataset.\n\nOur ATLAS, supporting anytime NAS for tabular data, can operate within a pre-defined time budget. \nWe first set a time budget to run ATLAS to select a well-performing architecture. Then, an additional time budget was allocated to fully train and evaluate this selected architecture. \nWe managed both time budgets to ensure the total time budget was around one hour.\n\nFor XGBoost, we also fixed the overall time budget for one hour and tuned the hyperparameter of XGBoost based on training. We employed the same hyperparameter search space as defined in [1] (Table 6).\n\nThe results shown in the Table below demonstrate that ATLAS can identify architectures that outperform XGBoost on the Frappe datasets. This confirms the efficacy of ATLAS in finding well-configured DNNs, i.e., determining the appropriate number of hidden neurons for each layer.\n\n| **Dataset** | **#Samples/#Columns** | **XGBoost** | **ATLAS**  | \n| - | - | - | - | \n| Frappe      | 288,609 / 10          | 97.46%      | **98.03%** | \n> Comparison to SOTA for DL for tabular data ... obtain Figure 5 in their paper.\n\n**Response:** Our ATLAS differs theoretically from TabPFN. \nTabPFN is a Prior-Data Fitted Network (PFN) capable of approximating probabilistic inference in a single forward pass, it has two limitations: 1) it requires extensive offline training (e.g., 20 hours on 8 GPUs as reported in [3]), and 2) it only scales effectively to small datasets and performs worse when categorical features are presented.\n\nIn contrast, our ATLAS employs both training-free and training-based architecture evaluation techniques. \nWhen compared to TabPFN, ATLAS demonstrates robust performance across diverse datasets ranging from small to large and consistently performs well when facing either numerical or categorical features, as evidenced in Figure 6 of the original paper.\n\nTable 5 in the TabPFN paper [3] presents the mean AUC of the TabPFN across all 18 datasets from the OpenML-CC18 Benchmark. Due to time constraints, we have evaluated our ATLAS on only eight datasets. For each dataset, we have also measured the total time (Total Cost) it takes to both search for the higher-performing architecture and then train this architecture to evaluate its performance, as shown in our global response to the _Evaluation of ATLAS on more Datasets_.\nThe results illustrate ATLAS is highly efficient and is competitive with methods detailed in Table 5 of the TabPFN paper.\n> It would be great if the authors ... and not rename them to TRAILERs.\n\n**Response:** As suggested, we will refer the TRAILERs to zero-proxies. \n> In Section 2, third paragraph ... performance evaluation based on the one-shot model weights (e.g. as in DARTS).\n\n**Response:** As suggested, we will rephrase the sentence to \"The performance obtained by the training-based architecture evaluation approaches is typically more accurate\". \n> Most of blackbox algorithms are also anytime algorithms,  ... so I would emphasize that instead.\n\n**Response:** Thank you for your valuable suggestions. We will first rephrase our claims to state 'ATLAS is the first anytime algorithm tailored for tabular data.' Second, we will emphasize that 'ExpressFlow is the first zero-cost proxy designed for tabular data' in our paper.\n> Why did the authors pick those 3 datasets for their benchmark? Is it because of the large number of training examples?\n\nWe explain the reason for choosing the three datasets in our global response to _Reasons for Selecting the Three Datasets for NAS-Bench-Tabular_.\n> How many data points were used to compute the rank correlation in Table 1?\n\n**Response:** In Table 1 of our paper, we present the correlation between each training-free evaluation metric score and the architecture's actual performance (AUC).  This analysis uses 160,000 architectures for the Frappe and Diabetes datasets and 10,000 for the Criteo dataset.\n\nWe hope our responses above have addressed your concerns and can improve your evaluation of our work.\n\n[1] Well-tuned simple nets excel on tabular datasets [Kadra et al., NeurIPS 2021].\n\n[2] Revisiting Deep Learning Models for Tabular Data [Gorishniy et al., NeurIPS 2021].\n\n[3] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second [Noah et al., ICLR 2023]."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683478353,
                "cdate": 1700683478353,
                "tmdate": 1700683478353,
                "mdate": 1700683478353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VjV3P0oxn7",
                "forum": "YlleMywQzX",
                "replyto": "TxA6eulRsC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1065/Reviewer_uv1h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1065/Reviewer_uv1h"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I thank the authors for putting the effort to address some of my main concerns. I will increase my score and suggest the authors to update the main paper with the new results in their global response + additional results on the remaining benchmarks of OpenML-CC18.\n\nOne other thing that I would suggest is to change the name of the benchmark from \"NAS-Bench-Tabular\". This can create confusion w.r.t. tabular benchmarks (which are not necessarily on tabular data). For instance, some alternative can be \"NAS-Bench-TD\" (TD stands for Tabular Data)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685837963,
                "cdate": 1700685837963,
                "tmdate": 1700685837963,
                "mdate": 1700685837963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JrT84ONU9I",
                "forum": "YlleMywQzX",
                "replyto": "fFStFJiPuA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are very glad to know that our response has addressed most of your concerns with clarity. \nWe would also like to thank you for raising your score for our paper. \nAs per your suggestion, we will update the main paper with the results from our global response, along with additional evaluations on the OpenML-CC18 benchmarks. Additionally, to avoid confusion and for clearer representation, we will rename \"NAS-Bench-Tabular\" to \"NAS-Bench-TD\".\nThank you again for your constructive comments."
                    },
                    "title": {
                        "value": "Thank you for raising your score!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687571518,
                "cdate": 1700687571518,
                "tmdate": 1700687791994,
                "mdate": 1700687791994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YSJ7LfUdhY",
            "forum": "YlleMywQzX",
            "replyto": "YlleMywQzX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1065/Reviewer_b3YS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1065/Reviewer_b3YS"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an approach called ATLAS, which focuses on Anytime Neural Architecture Search (NAS) specifically designed for analyzing tabular data, an area that has not been extensively explored in NAS research. With the aim of addressing the need for efficient NAS methods that can accommodate different time constraints, ATLAS presents a two-phase optimization strategy that cleverly combines training-free and training-based evaluations.  Experimental results demonstrate ATLAS's ability to quickly deliver competent architectures while adapting to expanded time budgets. Compared to conventional NAS methods, ATLAS achieves a remarkable reduction in search times, up to 82.75 times faster. These findings highlight the effectiveness and efficiency of ATLAS in the context of tabular data analysis, showcasing its potential for accelerating the NAS process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The development of ATLAS as the first Anytime NAS approach specifically for tabular data is a significant advantage and the main contribution of the paper. It fills a gap in the field of neural architecture search by providing a solution tailored to tabular datasets. ATLAS's innovative approach allows for the adaptation to any given time constraint, ensuring that the best possible architecture is available within the set time frame and can improve if more time is allocated. This responsiveness to computational budget constraints is a substantial step forward for practical applications of NAS in real-world scenarios where time and resources are often limited.\n\n2) The paper is well-written and positioned within the existing body of literature. The paper is commended for its clear writing, which concisely explains complex technical processes. It lays out the limitations of current approaches and systematically introduces the novel contributions of ATLAS, establishing its significance in the context of NAS research. The authors have ensured that the paper is not only informative but also accessible, making it a valuable addition to the academic discourse on neural architecture search."
                },
                "weaknesses": {
                    "value": "1) A limitation of the paper is the selection of benchmark datasets that may not comprehensively represent the diversity of real-world tabular data. The features of the chosen datasets\u2014Frappe, Diabetes, and Criteo\u2014are relatively low in dimensionality (10, 43, and 39 features, respectively). This narrow scope could potentially limit the generalizability of the study's findings. To convincingly argue the efficacy of ATLAS across various scenarios, it would be beneficial to test it on a wider range of datasets with varying feature dimensions, complexity, and domain-specific challenges. The current dataset selection might not fully challenge the capability of ATLAS to handle higher-dimensional and more complex tabular datasets that are commonly found in practice.\n\n2) The missing significant baselines, particularly established methods such as XGBoost and various Transformer-based models like TabTransformer and FTTransformer. Including these baselines is crucial for a comprehensive comparative analysis, especially to substantiate the necessity and superiority of NAS in the domain of tabular data. By neglecting to compare ATLAS against these well-known and widely-used methods, the paper misses an opportunity to demonstrate the practical advantage of NAS for tabular data over more traditional, yet powerful, approaches. This comparative analysis is essential to persuade the research community and industry practitioners of the added value that ATLAS and, more broadly, NAS methods may provide in tabular data applications."
                },
                "questions": {
                    "value": "Resolve the concerns in Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699149698608,
            "cdate": 1699149698608,
            "tmdate": 1699636032667,
            "mdate": 1699636032667,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xlN2ALiUd2",
                "forum": "YlleMywQzX",
                "replyto": "YSJ7LfUdhY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We highly appreciate your feedback. However, we would like to point out some misunderstandings about our work.\n\n>The development of ATLAS ... of NAS in real-world scenarios where time and resources are often limited.\n\n**Response:** Thank you for appreciating the contribution of our NAS tabular data benchmark (NAS-Bench-Tabular). We will make it publicly available to facilitate further research and development in the area of NAS for tabular data.\n\n> A limitation of the paper ... and more complex tabular datasets that are commonly found in practice.\n\n**Response:** The datasets utilized in our work are real-world tabular data. Specifically, we select three datasets from different application domains: app recommendation, healthcare analytics, and CTR (e-commerce) prediction. The statistics of the datasets are summarized in the Table below:\n\n| Dataset  | # Class | # Samples  | # Columns | # Features | Task                 |\n| -------- | ------- | ---------- | --------- | ---------- | -------------------- |\n| Frappe   | 2       | 288,609    | 10        | 5,382      | App Recommendation   |\n| Diabetes | 2       | 101,766    | 43        | 369        | Healthcare Analytics |\n| Criteo   | 2       | 45,840,617 | 39        | 2,086,936  | CTR Prediction       |\n\nNotably, for the three datasets, there are 10, 43, and 39 columns, corresponding to 5,382, 369, and  2,086,936 unique numerical and categorical features respectively, which are high-dimensional.\n\nTo further validate the generalizability and effectiveness of our proposed ATLAS, we have conducted experiments on an additional eight datasets from the open-source OpenML AutoML Benchmark.\n\nThe evaluation results, as presented in our global response to the _Evaluation of ATLAS on More Datasets_, confirm the effectiveness and generalizability of our proposed ATLAS.\n\n> The missing significant baselines ... NAS methods may provide in tabular data applications.\n\n**Response:** We would like to clarify that the scope of this paper is to propose a novel anytime NAS approach tailored for tabular data, rather than to design a new architecture that outperforms established architectures such as TabTransfer or FT-Transfermor. \nAs a result, we mainly pick baselines from the state-of-the-art NAS approaches designed for tabular data. \nSpecifically, we utilize TabNAS [1] and training-based NAS approaches, i.e., RE-NAS, for comparison as shown in Table 6 and discussed in Section 4.3 of the original paper.\n\nTo further demonstrate the practical advantages of our ATLAS, we have measured its balanced accuracy on two datasets from the OpenML AutoML Benchmark and compared ATLAS against XGBoost, TabTransformer, and FTTransformer. The results shown in the table below indicate that the selected architecture of ATLAS is comparable to both transformer-based architectures and XGBoost.\n\n| **Dataset** | **#Samples/#Columns** | **XGB** | **TabTransformer** | **FTTransformer** | **ATLAS** | **Searched Architecture** |\n| ----------- | --------------------- | ------- | ------------------ | ----------------- | --------- | ------------------------- |\n| Adult       | 48842 / 15            | 79.82   | 78.09              | 78.42             | 78.40     | 384-256-256-512           |\n| Bank        | 45211 / 17            | 72.66   | 78.57              | 78.37             | 78.32     | 384-256-256-512           |\n\nBoth TabTransformer and FTTransformer are designed to capture complex relationships in tabular data through self-attention mechanisms, and thus they perform consistently well on two datasets.\nNotably, our ATLAS effectively determines the number of hidden neurons for each layer of an MLP, thereby making it competitive with other methods. The experimental results align with conclusions drawn from FTTransformer [2]: specific tuning can make simple models like MLPs competitive. \n\nWe hope our responses above have addressed your concerns and can improve your evaluation of our work.\n\n[1] TabNAS: Rejection Sampling for Neural Architecture Search on Tabular Datasets [Yang et al., NeurIPS 2022].\n\n[2] Revisiting Deep Learning Models for Tabular Data [Gorishniy et al., NeurIPS 2021]."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683482710,
                "cdate": 1700683482710,
                "tmdate": 1700683482710,
                "mdate": 1700683482710,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j1RgeOyOIc",
                "forum": "YlleMywQzX",
                "replyto": "YSJ7LfUdhY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We hope our clarifications and new experiments have addressed your concerns. We would highly appreciate your consideration for re-evaluating your initial rating. We look forward to any further feedback you may have."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726877269,
                "cdate": 1700726877269,
                "tmdate": 1700727699320,
                "mdate": 1700727699320,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EP7D7GH2He",
            "forum": "YlleMywQzX",
            "replyto": "YlleMywQzX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1065/Reviewer_uv7V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1065/Reviewer_uv7V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a neural architecture search (NAS) method for tabular data. The proposed method, termed ATLAS, leverages a training-free metric for multi-layer perceptron (MLP) to estimate the architecture performances at low cost. After the filtering phase using the training-free metric, it searches for a better architecture using the accurate training-based architecture evaluation. In the proposed two-phase search strategy, the switching timing is determined based on a given search budget to improve the anytime performance. The effectiveness of the proposed method is evaluated on the NAS problem of finding the best number of units in each layer of MLP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The NAS tabular data benchmark constructed in this work will be useful for the community. It would be great if the authors released the dataset publicly.\n- The thoughtful experimental evaluation is conducted. The proposed training-free metric, ExpressFlow, empirically shows better correlations with the actual performance compared to several existing training-free metrics. Also, the anytime performance of the proposed method clearly outperforms the baselines.\n- The paper is generally well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The performance evaluation is conducted using the limited search space that decides the number of hidden units in each layer of MLP. The effectiveness of the proposed method on other architecture search spaces, such as Transformer-based architectures and other components of MLP, is unclear."
                },
                "questions": {
                    "value": "- Could you comment on the applicability and expected behavior of the proposed method on other kinds of architecture search spaces other than the search space used in this paper?\n- Is it possible to apply the proposed method to a situation where the additional budget will be available after the algorithm starts?\n\n----- After the rebuttal -----\n\nThank you for answering my question.\n\nThe authors' responses are convincing to me. I would be happy if the discussion regarding my questions were added to the revised paper.\nI keep my score to the acceptance side."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1065/Reviewer_uv7V"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1065/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699329316338,
            "cdate": 1699329316338,
            "tmdate": 1700977160092,
            "mdate": 1700977160092,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HNfudZ68yJ",
                "forum": "YlleMywQzX",
                "replyto": "EP7D7GH2He",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1065/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your constructive comments and positive feedback. We would like to address your concerns below.\n\n> The NAS tabular data benchmark ... It would be great if the authors released the dataset publicly.\n\n**Response:** Thank you for appreciating the contribution of our NAS tabular data benchmark (NAS-Bench-Tabular). We will make it publicly available to facilitate further research and development in the area of NAS for tabular data.\n\n> The performance evaluation is conducted using the limited search space ... and other components of MLP, is unclear.\n\n**Response:** Existing studies show that deep neural networks (DNNs) can already achieve state-of-the-art performance on tabular data [1, 2], and the technical challenge is to configure DNNs with the right number of layers and hidden layer sizes for each layer [3]. Therefore, we design a DNN-based search space that comprises up to 160,000 candidate DNNs with an extensive collection of different layer configurations.\n\nAs shown in the experiments and visualizations in Section 4.1.1 and Appendix C.3 of the original paper, this search space contains a diverse set of architectures with a wide range of parameter sizes, configurations, and performances, which captures the main properties of DNNs for benchmarking NAS approaches on tabular data.\n\n>Could you comment on the applicability ... search space used in this paper?\n\n**Response:** Our proposed method, ExpressFlow, is transferable and can be applied to other search spaces, such as NAS-BENCH-101 and NAS-BENCH-201 [4], which are designed for vision tasks. \n\nHowever, ExpressFlow is specifically tailored for tabular data, as it is based on neuron saliency that captures the complex and non-intuitive relationships among input features in this data type. \n\nConsequently, while demonstrating higher correlation values on tabular datasets (as shown in Table 1 of the original paper), ExpressFlow achieves only the second-highest average rank in terms of correlation value for image data, as illustrated in Table 15 of Appendix G.1.\n\n> Is it possible to apply the proposed method ... additional budget will be available after the algorithm starts?\n\n**Response:** Thank you for the insightful comments and suggestions. We agree that this is a very practical situation.\n\nOur NAS approach is anytime and thus can readily adapt to this situation by continuing to search for better-performing architectures with the current available time. Specifically, our approach includes two distinct phases, namely, the filtering phase and the refinement phase, along with a coordinator. If the additional budget is available during the filtering phase, the coordinator can dynamically allocate more time to both phases, which allows for exploring more architectures in the filtering phase and exploiting more architectures in the refinement phase. Conversely, if the additional budget is available during the refinement phase, all budgets are allocated to this phase, and thus each architecture will be trained over more epochs during the successive halving process.\n\nWe hope our responses above have addressed your concerns and can improve your evaluation of our work.\n\n[1] Well-tuned simple nets excel on tabular datasets [Kadra et al., NeurIPS 2021].\n\n[2] TabNAS: Rejection Sampling for Neural Architecture Search on Tabular Datasets [Yang et al., NeurIPS 2022].\n\n[3] Revisiting Deep Learning Models for Tabular Data [Gorishniy et al., NeurIPS 2021].\n\n[4] Nas-bench-suite-zero: Accelerating research on zero cost proxies [Krishnakumar et al., NeurIPS 2022]."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1065/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683487010,
                "cdate": 1700683487010,
                "tmdate": 1700684560834,
                "mdate": 1700684560834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]