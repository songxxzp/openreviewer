[
    {
        "title": "Locality-Aware Graph Rewiring in GNNs"
    },
    {
        "review": {
            "id": "O7SX7X43OC",
            "forum": "4Ua4hKiAJX",
            "replyto": "4Ua4hKiAJX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6245/Reviewer_XEwE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6245/Reviewer_XEwE"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on graph rewiring, a technique used to reduce the over-squashing problem in GNNs. It analyzes three desiderata for graph-rewiring, pointing out that previous methods fail to satisfy all of them. Based on this, the authors propose a novel rewiring framework that satisfies all three desiderata, considering locality by a sequential process. Finally, they validate the method on various real-world benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper clearly points out that previous graph rewiring fails to satisfy three aspects, and suggests a new method satisfying all aspects.\n2. The paper is overall well written and structured.\n3. The theoretical analysis provides an explanation on why the suggest method works well."
                },
                "weaknesses": {
                    "value": "1. The background for preserving sparsity seems to be weaker compared to the background for preserving locality, where the authors give effective resistance as example for preserving locality. It would be more persuasive if the authors gave an experiment result or paper for the question, \u201csome of these new connections introduced by spatial rewiring methods may be removed with affecting the improved connectivity.\u201d\n2. Authors have used \u201ceffective resistance\u201d throughout the paper to support their claim. However, the paper does not have any comparison with the graph rewiring method that uses effective resistance$^{[1]}$.\n\n[1] Black et al., Understanding oversquashing in gnns through the lens of effective resistance, ICML 2023"
                },
                "questions": {
                    "value": "1. The paper used the number of walks for the connectivity measure $\\mu$. According to the preliminaries in section 2, it seems that the connectivity measure is an unnormalized adjacency matrix. However, in the theorem 4.1 of the citated paper (Di Giovanni et al., 2023), the number of walks are divided by power of minimum node degree. Following this, doesn\u2019t the adjacency matrix should be normalized using the node degree?\n2. For the necessity of sequential rewiring, authors claimed that instantaneous rewiring easily violates either locality or sparsity constraint. In Figure 3, authors conducted an ablation study on the number of snapshots. Is there any comparison with an instantaneous rewiring, i.e., snapshot being 1?\n3. The connectivity and locality measures are only computed once over input graph to make the rewiring process efficient. However, it seems that these measure might change after some sequential steps of graph rewiring. For example, the shortest-walk distance between two nodes might get smaller after some graph rewiring steps, leading to another graph rewiring rather than the authors intended. Is it just that the performance gap of computing measures once and at each rewiring step did not differ much?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6245/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6245/Reviewer_XEwE",
                        "ICLR.cc/2024/Conference/Submission6245/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698201343242,
            "cdate": 1698201343242,
            "tmdate": 1700632429910,
            "mdate": 1700632429910,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ScdMcCMbWR",
                "forum": "4Ua4hKiAJX",
                "replyto": "O7SX7X43OC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We warmly thank the reviewer for pointing out that our work is clear, well written, and that it is a good contribution to the literature. We would like to address the valid points brought up by the reviewer.\n\n*Further motivating sparsity.*\n\nIn LASER, the rewiring is guided through a connectivity measure that is used to prioritize the edges which would improve the connectivity the most. Consequently, LASER, given a $\\rho$ factor, is \u201cdiscarding\u201d the edges that would affect the connectivity the least, according to the chosen connectivity measure. \n\n*Adding an effective resistance rewiring.* \n\nWe agree with the reviewer that a baseline such as GTR (Black et al, 2023) would fit well in our comparison. We have added the baseline to our experiments, alongside the inclusion of another recent curvature rewiring technique BOFR (Nguyen et al., 2023), as requested by another reviewer. The two are the most recent state-of-the-art rewiring techniques. In Tables 2 and 3, we demonstrate that LASER shows strong performance over these two new baselines as well.\n\n*Number of walks in connectivity measure.*\n\nWe thank the reviewer for the interesting question. We would like to clarify that the node degrees in the bound derived in Proposition 5.1 come from the convolutional part of the GNN model. It is standard practice to normalize such a convolution operation for training stability reasons. Instead, in our connectivity measure $\\mu$ we calculate the number of walks from node $u$ to $v$ of length $k$, which requires taking powers of an unormalized adjacency matrix. The two operations are therefore unrelated to each-other.\n\n*Instantaneous rewiring.*\n\nIn previous works such as FOSR and GTR, the authors have shown experimentally that relational types tend to significantly improve the performance of rewiring techniques. As such, we have not included such an ablation study. This can also be motivated from the fact that relational models are always able to recover \u201cinstantaneous\u201d models by setting the weight matrices to be equal to each-other. For this reason, they may be considered as strictly more powerful instances than instantaneous models.\n\n\n*Efficient computation of the connectivity and locality measures.*\n\nIt is indeed true that the sequential snapshots change the topology of the graph which would affect the number of walks. Computing the connectivity measure once, as you have pointed out, gives an important computational speed-up which we wanted to keep for scalability reasons. During the development of the model, we found that such a choice did not have a noticeable impact on the performance\n\nWe once again thank the reviewer for supporting our work and for the excellent questions. We are of course happy to keep engaging throughout the rebuttal period."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103652918,
                "cdate": 1700103652918,
                "tmdate": 1700103652918,
                "mdate": 1700103652918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GlFOW0PP28",
                "forum": "4Ua4hKiAJX",
                "replyto": "ScdMcCMbWR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Reviewer_XEwE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Reviewer_XEwE"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their comprehensive response, and have no further questions."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632777629,
                "cdate": 1700632777629,
                "tmdate": 1700632777629,
                "mdate": 1700632777629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cro2Vk7scA",
            "forum": "4Ua4hKiAJX",
            "replyto": "4Ua4hKiAJX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6245/Reviewer_KgxZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6245/Reviewer_KgxZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework for graph rewiring. Specifically, the paper suggests having two competing metric - one for oversquashing and one for distance to the original graph that should be balanced. The paper presents a specific instantiation of the framework and present numerical experiments to show the benefit of the framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality**\n\nThe framework presented in the paper is new. However, the idea of preserving the original graph structure is not new, further as the authors themselves state the idea of using relational GNNs is not new either. Though they do have an original extension of the framework. \n\n**Quality**\n\nPlease see weaknesses. \n\n**Clarity**\n\nThe paper is well written and places itself very well in the context of prior work. This I think is the papers biggest strength. The framework is clearly presented. However, the paper's clarity degrades in Section 5. For example Proposition 5.2 is not clear and there is no formal statement or proof anywhere in the paper. \n\n**Significance**\n\nThe paper's method does seem to perform better than SDRF and FOSR. However, I have certain concerns about the experiments, highlighted in the weakness section."
                },
                "weaknesses": {
                    "value": "I think there a few weaknesses. Part of my concern with the paper is that as detailed in point 2a, there are now many different rewiring techniques. However, I do not think we understand oversquashing yet. Hence for me new papers in the area either need to a thorough comparison with prior work to show empirical improvement. Or contributes to understanding oversquashing and I think this paper, unfortunately, does not do either. \n\n1) One big weakness is Proposition 5.2. The statement in the paper is informal and incomplete. However, the paper does not have a formal statement or a proof. This is a big concern. For example, for the informal statement $\\mathcal{G} = \\emptyset$ vacuously gives us the result. However, that version is meaningless. Hence a formal statement is needed. There is space in the paper for this discussion, the first few pages are quite repetitive. \n\nI see that page 14 has something that is called a proof. But without a formal statement the notion of proof does not make sense to me. And there is no formal statement. A.2 is thought of to be formal statement but it refers to a lower bound in 5.2 which is not clear. \n\n2) I have a few concerns about the experiments. \n\n    a) First, I think the paper compares against very few prior works. The paper does a good job of citing many prior works in the area but then only compares against two of them. The paper should compare against most of the following or explain why it is not relevant to do so: GTR (Black et al. 2023), BORF (Nguyen et al. 2023), DRew (Gutteridge et al 2023), DIGL (Gasteiger et al 2019), Expander propagation (Deac et al 2022), DiffWire (Arnaiz Rodriguez et al 2022). The paper even cites Br\u00fcel-Gabrielsson et al., 2022; Abboud et al., 2022, and  Banerjee et al., 2022 as further works with rewiring techniques. \n\n   b) I also have some concerns with the experiments that are present. First, as the paper notes the network from FOSR is the case that $L=1$. However, for the method proposed in the paper, the paper uses $L \\in \\{2,3,4,5\\}$. Since for each $\\ell \\le L$ we have a different weight matrix, this implies that the networks for LASER are bigger than the networks used for FOSR and SDRF. This is an inequity that could account for the improved performance. \n\n   c) Hyperparameter tuning. The paper mentions that they tune $L$ and $\\rho$ for their method. However, they do not perform any hyper parameter tuning for the comparison methods (SDRF and FOSR). They fix the number of edges to be 40. This is another thing that could account for the inequity between the methods. It is also not mentioned what number is reported, I am assuming that the experiments trained models for each of the hyperparameters, picked the setting with the best validation performance and then reported the test error, however it would good if this were explicitly mentioned (since Figure 3 reports the metrics on the test data for all hyperparameters). \n\n3) I think the fairer version for the experiments would be to sequentially rewire the datasets with FOSR and SDRF such for each $1 \\le \\ell \\le L$ all sets $E_\\ell$ have the same size. I think this would help determine if part of the reason for increased performance is the rewiring or the new GNN architecture.  \n\nThe next couple of concerns are more minor. \n\n4) In terms of the context for the work, I think the following could be clarified. The notion of oversquashing in Alan and Yahav 2021, and the other papers Topping et al 2022, Black et al., 2023; Di Giovanni et al., 2023 are subtly but in my opinion importantly different. Alon and Yahav 2021, is a more information theoretic issue that is highlighted. Vectors of a certain size can not aggregate too much information. However, the issue in Topping et al 2022, Black et al., 2023; Di Giovanni et al., 2023 is more about optimization rather than information theory. These papers talk about how the Jacobian has a small norm. Hence while both phenomena can be labelled as oversquahing I do think they are different and should be treated as such. \n\n5) The paper measures the \"information\" provided by the graph structure as preserving locality. Specifically, they say ``while the measure $\\nu$ is any quantity that penalizes interactions among nodes that are \u2018distant\u2019 according to some metric on the input graph.'' However, local structure of the graph and the information stored in the graph are not the same thing."
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6245/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6245/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6245/Reviewer_KgxZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698440252622,
            "cdate": 1698440252622,
            "tmdate": 1700514009725,
            "mdate": 1700514009725,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k2seZ4SCzJ",
                "forum": "4Ua4hKiAJX",
                "replyto": "Cro2Vk7scA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank the reviewer for the excellent and thorough review. We would like to address the points in full.\n\n*Regarding Proposition 5.2.*\n\nWe thank the reviewer for the feedback on the Proposition. We have followed the suggestion and replaced the informal statement with the precise one from the Appendix. To make things more concrete, we have formulated the result for the case of a graph (pointing out that the size can be taken to be positive to avoid vacuous edge cases) and rewritten the proof in the Appendix (Section A) more explicitly. The discussion surrounding Proposition 5.2 has also been modified in order to improve clarity. Thanks again for the feedback.\n\n*Further baselines.*\n\nWe thank the reviewer for the suggestions. We agree that the paper would benefit from further comparisons. Accordingly, we have added a comparison to GTR (Black et al. 2023) and BORF (Nguyen et al. 2023) in our work. We believe these to be the most direct and relevant recent comparisons to our framework, as these works aim to modify the computational graph as a pre-processing.\n\n\n*Conservation of parameters and tuning.*\n\nWe thank the reviewer for the valid experimental doubts, we would like to address these in full. All experiments in our work *strictly follow the 500k parameter budget limit* for LRGB. As for the TUDatasets, we keep the hidden dimension fixed to 64. Furthermore, in the TUDatasets we do not tune on more than 3 snapshots (details in the Appendix). As such the model parameter count remains extremely similar, especially when compared to the relational FOSR, GTR, BOFR, and SDRF models. Therefore, we are certain that the slight increase in parameters does not affect our experimental evaluation. \n\nWe agree with the reviewer that fixing the iterations in SDRF and FOSR to 40 may have favoured LASER in our previous evaluation. Therefore, we have included results achieved with a more thorough sweep of the hyper-parameters for SDRF and FOSR, alongside the additional GTR and BOFR benchmarks. We have added detailed information in the Appendix (Section B) on the parameters searched.\n\n*New experiment proposed.*\n\nWe thank the reviewer for the suggestion. Having a sequence of rewirings is a contribution of our work and it is not immediately clear how easily this translates to other rewiring techniques. We agree, however, that this is an interesting experiment and show in the Appendix (Section C.4) that additional snapshots even slightly decrease the performance with FOSR, while tend to improve the performance with LASER. \n\n*Minor concern on the literature summary.*\n \nThis is an accurate description of the literature landscape thus far. We fully agree with your point of view, even though we may have summarized some of the references in the interest of space. If you believe there are certain points where it is worth making such clarification explicit, we would be happy to add it.\n\n\n*Minor concern on the locality measure.*\n\nThank you for this observation. We agree that we could make our point of view more explicit. Rather than \u201cinformation\u201d, preserving locality in our work mainly refers to preserving the inductive bias afforded by the input graph topology, according to which features that are associated with nearby nodes should interact \u201cmore easily\u201d or \u201cmore often\u201d than those associated with distant nodes. Naturally, we are tacitly assuming here that the graph-structure we are given in the first place is somewhat aligned with the downstream task in order for this claim to be valid. This is why \u201csome metric\u201d, mentioned above, should capture a notion of distance on the graph that we believe to be, in most cases, aligned with the task. We have rephrased a few points (see e.g. the paragraph `Instantaneous vs Sequential Rewiring\u2019 or the discussion above the new statement of Proposition 5.2) of the submission to emphasize that \u201clocality\u201d is a \u201cbias\u201d we aim to preserve in most tasks we believe GNNs to be useful for.\n\nWe again thank the reviewer for the great questions. We hope that with our rebuttal and the improvements we have made to the paper, we have addressed the concerns found regarding the proposition and the experiments, and that this may convince the reviewer to increase the score. We are of course very happy to keep engaging during the rebuttal period."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103283825,
                "cdate": 1700103283825,
                "tmdate": 1700235107069,
                "mdate": 1700235107069,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2vFvALUdIE",
            "forum": "4Ua4hKiAJX",
            "replyto": "4Ua4hKiAJX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6245/Reviewer_RVSP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6245/Reviewer_RVSP"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the concept of graph rewiring, which involves altering graph connectivity to improve information flow. Three essential objectives for graph rewiring are identified: (i) reducing over-squashing, (ii) preserving the graph's local structure, and (iii) maintaining its sparsity. \n\nThe authors highlight that there is a trade-off between two primary techniques in graph rewiring: spatial and spectral methods. They argue that spatial methods tend to address over-squashing and local structure but may not preserve sparsity, while spectral methods generally handle over-squashing and sparsity but might not maintain local properties. \n\nTo tackle these trade-offs, the paper introduces a novel rewiring framework. This framework employs a sequence of operations that are sensitive to the graph's local characteristics, aiming to simultaneously meet all three objectives: reducing over-squashing, respecting the graph's locality, and preserving its sparsity. Furthermore, the paper discusses a specific instance of this proposed framework and evaluates its performance on real-world benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors gave a nice taxonomy of rewiring methods and the issues that they suffer from"
                },
                "weaknesses": {
                    "value": "* In the paper they constantly cite spectral methods such as Arnaiz-Rodr\u00edguez et al., Black et al., 2023 and transformer-based methods such as Kreuzer et al., 2021; Mialon et al., 2021; Ying et al., 2021; Rampasek et al., 2022. But in the tables, there is no comparison with these methods.\n\n* The results are very poor, especially when it comes to the task of graph classification, where the method is not able to outperform the few selected models."
                },
                "questions": {
                    "value": "* Why do you say that spectral methods are not local, since most of them combine long-range information (by bypassing the bottleneck) and initial neighborhood? For instance, Arnaiz-Rodr\u00edguez et al. combine the CT model (long-range) with MPNN with the initial adj.\n\n* Is there any study of the parameter k? For instance, what happens for large k values? How does it affect the relationship between the distance of nodes of the same cluster (locally) and nodes of different clusters (globally)?\n\n* Can there be any attention mechanism between the snapshots, attending to those snapshots that contribute more to the representation of the graph, as they do in multiple papers where they explore different adj  (Abu-El-Haija et al., 2019; or FSGNN (Improving Graph Neural Networks with Simple Architecture Design)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745647264,
            "cdate": 1698745647264,
            "tmdate": 1699636683133,
            "mdate": 1699636683133,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "peSwC10Dpu",
                "forum": "4Ua4hKiAJX",
                "replyto": "2vFvALUdIE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the excellent comments and questions, below we would like to address them in full. \n\n*Other baselines.* \n\nWe thank the reviewer for pointing out further baselines. We agree that the work would be strengthened via the addition of more baselines. Accordingly, we have added to our experiments a comparison to GTR (Black et al, 2023), and BORF (Nguyen et al, 2023).  We believe that these techniques are the most comparable to ours as they are designed to augment the graph through pre-processing. Transformer based methods are not directly comparable to our technique as they require significantly more computation. In fact, even the most recent state-of-the-art rewiring techniques GTR and BORF do not compare with Transformers-based architectures in their evaluation and compare against the same benchmarks present in our own work.\n\n*Poor results.*\n\nWe politely disagree with the statement and believe the results to be validating our point. We kindly ask for further clarification as to why you believe the results to be poor. In fact, when it comes to comparing with similar methods that also aim at adding edges more \u201csurgically\u201d, the LASER framework consistently outperforms or is on par with the other baselines in the worst-case scenario. Even with the new added baselines, this claim still holds. More concretely, in Table 2 LASER beats all baselines consistently. In Table 3, LASER achieves a mean rank of 1.67, with the second best model achieving a mean rank of 3.20. \n\n*Spectral methods are not local.*\n\nWe thank the reviewer for the interesting question. We would like to clarify that in our work spectral rewirings refer to methods that aim to rewire the graph based on spectral quantities, which is not a local mechanism. This is because rewiring through quantities such as increasing spectral gap or decreasing commute time accounts for the global topology of the graph, meaning that the edges to be added end up minimizing spectral (not spatial) quantities and hence may, quite often, violate any spatial constraint. Put differently, the edges added through such methods will tend to be very non-local. This is something that has also been pointed out in Di Giovanni et al., ICML 2023. Architectures such as DiffWire aim to address this problem by adding additional components to the model. We emphasize that, instead, in our work, as in FOSR, SDRF, GTR, and  BORF, the goal is to directly modify the computational graph of any existing MPNN, instead of proposing a completely different architecture.\n\n*Is there any study through parameter k?*\n\nIf this question is referring to parameter $k$ in Equation 8, then this parameter simply indicates that we want to consider walks of length $k$ on our graph as our connectivity measure. In general, we want $k > L$ and we also set $k$ to be a power of 2 as this leads to more efficient matrix multiplication algorithms due to efficient factorization of the multiplication. As such, we fix $k=8$ in our work as a tradeoff between $k=4$ which would be too close to $L$ and $k=16$ which would be unnecessarily large. We found the network to be rather insensitive to this choice as long as $k$ was large enough.\n\n*Can there be an attention mechanism between the snapshots.*\n\nWe thank the reviewer for the suggestion. We agree that this is something that could definitely be a future direction of our snapshot framework. It is indeed natural to explore how to best aggregate different snapshots together. In our work, we take an approach inspired by Temporal GNNs, but other approaches such as attention mechanisms, could be explored in future work.\n\nWe thank once again the reviewer for the comments and hope that we have clarified doubts in the new version of the paper, with a significantly stronger experimental section. We hope that this is able to convince the reviewer to increase the score. We are of course happy to keep engaging with the reviewer."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103349586,
                "cdate": 1700103349586,
                "tmdate": 1700154826226,
                "mdate": 1700154826226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IKnlTfWnkj",
                "forum": "4Ua4hKiAJX",
                "replyto": "peSwC10Dpu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Reviewer_RVSP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Reviewer_RVSP"
                ],
                "content": {
                    "title": {
                        "value": "Comment to authors."
                    },
                    "comment": {
                        "value": "Sorry but the experimental results are still inconclusive. The new baseline BORF is still better in MUTAG and GTR is better in REDDIT.\n\nExisting methods such as Diffwire are cited and considered in \"Future Work\". The authors say: In this paper, we considered a simple instance of the general rewiring paradigm outlined in Section 4, but we believe that an interesting research direction would\nbe to explore alternative choices for both the connectivity and locality measures, ideally incorporating\nfeatures in a differentiable pipeline."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558084794,
                "cdate": 1700558084794,
                "tmdate": 1700558084794,
                "mdate": 1700558084794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EHHAU297vO",
                "forum": "4Ua4hKiAJX",
                "replyto": "1EOgMrt5m9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Reviewer_RVSP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Reviewer_RVSP"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "**Comment 1** We highlight that LASER achieves the top performance on 6 out of the 9 datasets we evaluate on, when compared against 5 baselines. This is in very stark contrast to any rewiring technique we evaluate against.\n\nA look to table 3: \n\nREDDIT-BINARY :  LASER is 3rd (competitive)\nIMDB-BINARY :     LASER is 2nd wrt NO REWIRING (-2%)\nMUTAG:                 LASER is  2nd wrt BORF (-3%)\nENZYMES:             LASER is 1st\nPROTEINS:            LASER is 1st \nCOLLAB:                LASER is 1st\n\nSo in 6 datasets it wins on 3 and competitive in 1. I do not consider mine a \"stark\" judgement. In graph classification, one uses the degree if no features are available. The fact that the method works no so well in non-featured graphs means that rewiring is not so topological as explained in the exposition of the technique. \n\n**Comment 2** Regarding, DiffWire, we would like to highlight that the technique operates under a **different regime** to ours by proposing an entirely different GNN architecture. Instead, in our work, as in FOSR, SDRF, BORF, and GTR, we simply aim to augment any MPNN through a rewiring technique as a pre-processing step. We would like to point out that the most recently accepted works BORF and GTR also do not evaluate against DiffWire, again because the technique is not directly comparable. Furthermore, even in these works the techniques do not achieve the highest accuracy on every task in their experimental section\n\nWhat is a different regime? What if the GNN architecture is different? It is in the state of the art or not. Why exclude this technique if code is available? As far as I know, it is inductive and generalizes from seen graphs. If Diffwire is not so good enough, why do not exploit that fact? BTW, have you made a double check of comparing against a simple KNN rewiring? Of course, KNN rewring is not top techique but gives valuable information."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587896592,
                "cdate": 1700587896592,
                "tmdate": 1700587896592,
                "mdate": 1700587896592,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ONLvPPJV3",
                "forum": "4Ua4hKiAJX",
                "replyto": "2vFvALUdIE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank you again for engaging with our work and for providing valuable feedback.\n\nFollowing your suggestions, we have added to Table 3 DiffWire as a baseline. Furthermore, we ran a full grid search for **LASER** over the number of snapshots $\\\\{ 2, 3 \\\\}$ and $\\rho$ in $\\\\{0.1, 0.25, 0.5\\\\}$. We highlight that previously we had only used a random subset of $3$ configurations. We emphasize that this is a grid search of size $6$, which is now the same grid search size used for FOSR, SDRF, GTR, and BORF. For fairness and to avoid snooping, we ran this grid search *on all the TUDataset tasks* for **LASER**, explaining why most columns changed for **LASER**. We provide additional hyper-parameter details in the Appendix (Section B). We report the improved results in the newly uploaded PDF.\n\nWe sincerely hope that with these additional results, we have addressed your remaining concerns and that this may convince you to raise your score. We would like to once again thank you for your efforts in reviewing our work. We are happy to answer any further questions."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652740113,
                "cdate": 1700652740113,
                "tmdate": 1700653070695,
                "mdate": 1700653070695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QEITLg4Les",
            "forum": "4Ua4hKiAJX",
            "replyto": "4Ua4hKiAJX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6245/Reviewer_EJ4V"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6245/Reviewer_EJ4V"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a sequential rewiring method, LASER, that improves connectivity, and preserves locality in the original graph, and theoretically alleviates the over-squashing problem. Empirical experiments show that LASER outperforms the baselines on some LRGB and TUDatasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper gives a good summarization of spectral and spatial rewiring methods.\n- The anti-over-squashing and sparsity motivation of the paper makes sense.\n- The sequential rewiring idea is pretty good so that edges are not added at once but more carefully selected.\n- The connection between sequential rewiring and multi-relational GNN is novel\n- The writing is good and clear."
                },
                "weaknesses": {
                    "value": "- The paper does not explain why adding distant edges is not a good choice. In other words, why must we respect the locality and inductive bias of the given graph? Therefore, the paper does not fully convince me of their significance, although the sparsity motivation is good, the method is novel, and the experimental results seem good.\n- The paper selects some spectral rewiring baselines but does not compare with DRew [1] and SP-MPNN [2] in the experiments, which also attend multi-hop neighbors in their message passing scheme and should be considered as spatial rewiring baselines. On LRGB datasets, DRew seems even better.\n- As this is a rewiring approach, it does not seem to make sense to do experiments with PCQM-Contact, which is an edge prediction task\n- The choice of connectivity measure in equation (8) is not efficient. The matrix multiplication would also be O(N^3). If the matrices are sparse, then the complexity would be at least O(N^2 * d_max).\n\n[1] Gutteridge, Benjamin, Xiaowen Dong, Michael M. Bronstein, and Francesco Di Giovanni. \"Drew: Dynamically rewired message passing with delay.\" In International Conference on Machine Learning, pp. 12252-12267. PMLR, 2023.\n\n[2] Abboud, Ralph, Radoslav Dimitrov, and Ismail Ilkan Ceylan. \"Shortest path networks for graph property prediction.\" In Learning on Graphs Conference, pp. 5-1. PMLR, 2022."
                },
                "questions": {
                    "value": "On the top of page 3, what does the notation 2|E|R(v, u) stand for?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6245/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6245/Reviewer_EJ4V",
                        "ICLR.cc/2024/Conference/Submission6245/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6245/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745884678,
            "cdate": 1698745884678,
            "tmdate": 1700218991126,
            "mdate": 1700218991126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NtdW990Qvn",
                "forum": "4Ua4hKiAJX",
                "replyto": "QEITLg4Les",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy to hear that the reviewer found our work to be well-motivated and our ideas novel. Below we answer the questions posed by the reviewer, which we believe have helped to strengthen our work.\n\n*Why must we respect locality?*\n\nWe thank the reviewer for the important question. Preserving locality is a natural inductive bias to have whenever we assume that the graph-structure associated with the data is aligned with the downstream task.\nFor instance, molecular systems observe long-range interactions that decay with the distance, in the form of Coulomb electrostatic forces. This  behaviour also naturally appears in social networks, transaction networks, or more generally in physical systems, in which interactions that are nearby are more likely to be important for a given task. Accordingly, given a budget of edges to be added, it is sensible to prioritise adding connections between nodes that are closer.\n\nTo make this intuition more concrete, in Proposition 5.2 we have argued that methods that do not preserve the locality bias, may alter the distance matrix associated with the graph quite more significantly.  We have added a detailed section in the Appendix (Section C.4) alongside a pointer within the main section to tackle this point more extensively within the paper as we agree that this is an important aspect of our work. In this new section, we touch upon concrete examples, and provide a further ablation that supports this view.\n\n*Other rewirings (DRew and SP-MPNN).*\n\nWe would like to point out that SP-MPNN can be seen as a special case of LASER where we take $\\rho=1$ and force the weight matrices associated with each hop to simply be a convex combination of learnable coefficients. The advantage of our approach, which extends SP-MPNN, is that it provides a tradeoff between performance and efficiency since we can more easily control the sparsity of the computational graph through the factor $\\rho$. Concerning the framework of DRew, we highlight that the main purpose of that work is showing that introducing delay can be particularly beneficial when exploring deep models. Because of that, their evaluation on LRGB does not stick to the convention of 5-layers architecture \u2013 which instead we have adopted here. Considering that the main goal of this project is showing how to condition spatial-rewiring on more global (spectral) properties, we believe that focusing on spectral and curvature-based rewiring baselines is fair. To strengthen our comparison to existing state-of-the-art baselines that follow our *exact* paradigm, we have added an additional spectral baseline (GTR, Black et al. 2023) and an additional curvature baseline (BORF, Nguyen et al. 2023) to our experiments, as suggested by some of the other reviewers.\n\n*Edge prediction.* \n\nWe agree that edge prediction is a less natural task for rewiring techniques but included it regardless as an interesting experiment to cover the different types of tasks GNNs are used for (graph-level, node-level, and edge-level predictions).\n\n*Scalability of LASER.*\n\nWhile matrix multiplication has cubic run-time, it is a highly parallel operation that scales extremely well on modern hardware. It is numerically more stable than matrix inversion (used for effective resistance based rewirings such as GTR, which is also cubic), and significantly cheaper than curvature computation (used by SDRF and BORF). We show in the Appendix (Section D) that we are able to scale easily to graphs of *100k nodes and ~1 million edges*, unlike SDRF and FOSR. We point out that the other rewiring operations rely on optimization libraries such as Numba and Multiprocessing, while we avoid such an approach as we found LASER to be extremely efficient and scalable regardless. We also finally emphasize that such an operation is done only once as a pre-processing step.\n\n*What does 2|E|R(v, u) stand for?*\n\nThanks for this question, we will clarify the notation in the text. |E| is the number of edges and R(v, u) is the effective resistance between nodes v and u. The equation CT(v, u) = 2|E|R(v, u) is a well known result and relates the commute time between v and u CT(v, u) to the effective resistance between v and u R(v, u).\n\nOnce again we thank the reviewer for the excellent questions and hope that our reply persuades the reviewer to increase the score. We are of course happy to keep engaging with the reviewer throughout the process."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700102952058,
                "cdate": 1700102952058,
                "tmdate": 1700154799338,
                "mdate": 1700154799338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tvf2GPiLRx",
                "forum": "4Ua4hKiAJX",
                "replyto": "NtdW990Qvn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6245/Reviewer_EJ4V"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6245/Reviewer_EJ4V"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your detailed rebuttal, I increased my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6245/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218957829,
                "cdate": 1700218957829,
                "tmdate": 1700218957829,
                "mdate": 1700218957829,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]