[
    {
        "title": "DIG-MILP: a Deep Instance Generator for Mixed-Integer Linear Programming with Feasibility Guarantee"
    },
    {
        "review": {
            "id": "XTOBGJXQUP",
            "forum": "psDvcWtFdE",
            "replyto": "psDvcWtFdE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8623/Reviewer_HSpe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8623/Reviewer_HSpe"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a MILP instance generator with the help of the variational auto-encoder (VAE), aiming at generating enough MILP instances for academic research and industry usage. The authors first prove the boundedness and feasibility when generating instances using the dual formation, which ensures the correctness of the proposed generator DIG-MILP. In the training step, DIG-MILP continues to mask a random node and the connections to it, and use the incomplete instance as input to the VAE, aiming to reconstruct the instance. This task is suitable for VAE and helps the model to learn how to generate more MILP instances. The proposed method is well-designed and the proofs are sufficient. The experiments on multiple datasets and scenarios show the performance of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. There is indeed a need to generate more high-quality MILP instances in both the industry and academia. Therefore, this work is necessary and interesting to the community.\n2. The proposed VAE framework for generating MILP instances is self-contained and reasonable, as VAE is demonstrated to be useful in many instance generation tasks.\n3. The authors have proven the generated instances to be feasible-bounded."
                },
                "weaknesses": {
                    "value": "1. In the pipeline of the proposed method, I see that one node will be removed and the origin G is changed to G'. I wonder if this one-node change is too small to identify. In common MILP datasets, the size of nodes/constraints is more than thousands, and for the industry area, the size is even larger. \n2. In section 3.2, the authors provide the feasibility guarantee for the proposed methods. But I am curious about unfeasible instances, can the proposed framework generate unfeasible MIP instances? In my view, it is okay when a MIP instance is infeasible and this situation is common in the real world.\n3. The metrics in the experiments are confusing to me. The authors mentioned: \"The similarity score is derived from the Jensen-Shannon (JS) divergence (the lower the better) between each metric of the generated and original data\", while in the caption of table 1 said: \"The similarity score \u2191 between the original and generated data.\" The similarity score is lower the better or higher the better? Actually, I wonder what we expect about the similarity score. I mean, more similarity could increase Authenticity while less similarity could increase Diversity. Both cases seem to mean something.\n4. About the datasets in the experiments, I see that for small datasets SC and CA the amount of instances is 1000, but for large datasets CVS and IIS the amount is less than 10. This gap looks very strange to me. Moreover, in, table 2, does training and testing on merely 3 instances lead to an overfitting to these instances? I think the datasets CVS and IIS need to be further refined. I understand that they are selected from MIPLIB17, but MIPLIB17 at least has hundreds of instances. If the MIPLIB17 is too hard, I think NeurIPS 2021 ML4CO[1] datasets could be more suitable.\n\n\n[1] Maxime Gasse, Simon Bowly, Quentin Cappart, Jonas Charfreitag, Laurent Charlin, Didier Ch\u00b4etelat, Antonia Chmiela, Justin Dumouchelle, Ambros Gleixner, Aleksandr M Kazachkov, et al. The machine learning for combinatorial optimization competition (ml4co): Results and insights. In NeurIPS 2021 Competitions and Demonstrations Track, pp. 220\u2013231. PMLR, 2022."
                },
                "questions": {
                    "value": "Please refer to the weakness part of my review. Admittedly, I am not familiar with generative models, so my questions are mainly about this part. However, based on my experience, I think the dataset problem is more severe, as I do not think training and testing on only 3 instances is suitable for meeting the bar of ICLR.\n\nBesides, some closely related works on graph generation especially SAT instance generation are missed, including [1], [2], [3], all of which exhibit highly similar task structures for generating bipartite combinatorial optimization problems.\n\nIf there are some fatal mistakes in my review, please point it to me.\n\n\n[1] G2SAT: Learning to Generate SAT Formulas. NeurIPS 2019.\n\n[2] On the Performance of Deep Generative Models of Realistic SAT Instances. SAT 2022.\n\n[3] HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline. SIGKDD 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698139815821,
            "cdate": 1698139815821,
            "tmdate": 1699637078743,
            "mdate": 1699637078743,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lGoBQGXa1q",
                "forum": "psDvcWtFdE",
                "replyto": "XTOBGJXQUP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Reviewers",
                    "ICLR.cc/2024/Conference/Submission8623/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer HSpe"
                    },
                    "comment": {
                        "value": "Thank you, Reviewer HSpe, for your valuable suggestions. We address your queries as follows:\n \n1. **Changing One Node at a Time**:\n   We appreciate your insightful suggestion. DIG-MILP currently supports iteratively selecting and altering a single node in the graph. However, we acknowledge that selecting and changing multiple nodes simultaneously might be an alternative and potentially effective approach, and we would leave this as a future work.\n \n2. **Feasible vs. Infeasible Instances**:\n   As mentioned in our universal response, we argue that given that almost all the existing scalable MILP datasets are feasible, it is naturally more interesting to generate feasible-bounded MILP instances. In real-world industrial applications, MILP problems modeled from real-world applications are usually feasible, or infeasible problems are transformed into feasible ones for solver application. Therefore, we argue that training models on feasible datasets to generate feasible MILP instances could be a justified approach.\n \n3. **JS-Divergence and Similarity Score**:\nWe use JS-divergence where a smaller value indicates higher similarity of two distributions. The similarity score is calculated in the way like (1 - JS divergence), implying that a larger score is preferable. Despite aiming for diversity in new data, it's crucial that the new MILP instances represent problem types similar to the original. This relevance ensures their practical utility in tasks like solver tuning and as augmentation in ML training. We measure such similarity to the original problem distribution by comparing various statistical metrics on graphs, leading to our comprehensive scoring metric that incorporates these diverse measures.\n \n4. **Training Data for CVS and IIS**:\nThe instances in MIPLIB2017 are of thousands of constraints and variables. In each training iteration, we sample a training batch by randomly choosing a node and delete it from the original bipartite graph, which leads to new training examples during the training process. Also for the downstream task, we train the model with the three original data with 20 newly generated instances by our model. We may consider larger datasets for further experiments.\n \nWe hope these responses clarify your concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630417478,
                "cdate": 1700630417478,
                "tmdate": 1700630417478,
                "mdate": 1700630417478,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Me5fDe39Ak",
            "forum": "psDvcWtFdE",
            "replyto": "psDvcWtFdE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8623/Reviewer_T4Ma"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8623/Reviewer_T4Ma"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents DIG-MILP, an innovative deep generative approach tailored for Mixed Integer Linear Programming (MILP) generation. Unlike traditional MILP generation methods, DIG-MILP eschews the need for domain-specific insights. A significant attribute of DIG-MILP is its assurance of the feasibility and boundedness of the crafted data. The generative spectrum of DIG-MILP spans all feasible and bounded MILPs, endowing it with the prowess to produce a \"variety\" of instances. Experimental analyses underscore DIG-MILP's promise in: (S1) facilitating MILP data dissemination for solver hyperparameter optimization without publishing original datasets, and (S2) data enrichment to bolster the robustness of machine learning techniques dedicated to tackling MILPs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tIt is impressive that the authors propose to use VAE sampling in dual space to generate MILP instance. In this way, the feasibility guarantee is directly obtained. \n2.\tThe experimental results endorse that the proposed techniques indeed facilitate the enhancement of MILP solver and business scenarios."
                },
                "weaknesses": {
                    "value": "1.\tIt seems that the literature review lacks of the most related paper that recently published in NeurIPS 2023 (Geng, Zijie, Xijun Li, Jie Wang, Xiao Li, Yongdong Zhang, and Feng Wu. \"A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability.\" arXiv preprint arXiv:2310.02807 (2023).) This submission is highly similar to the above paper. Thus the authors are supposed to highlight the largest difference and improvement from the mentioned paper.\n2.\tSimilar to the above point, the experimental setting of this paper highly resembles one in the paper \u201cA Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability\u201d. Thus, Are the authors supposed to compared their proposed method with one proposed in that paper?\n3.\tIt is hard to read several Figures. Because the font size of axis in those figures is too small"
                },
                "questions": {
                    "value": "1.\tPlease clarify more in-depth about the feature mentioned in Table 1, especially the all 0\u2019s of y, r and the all 1\u2019s about x, s.\n2.\tIt is not that dogmatical to claim that the boundness and feasibility of the generated instances can sure the authenticity of the produced data. The authenticity of MILP dataset can be defined in kinds of perspectives. Can you give more evidences to support your claim?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8623/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8623/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8623/Reviewer_T4Ma"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698590641871,
            "cdate": 1698590641871,
            "tmdate": 1699637078605,
            "mdate": 1699637078605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4n0VhiYEwN",
                "forum": "psDvcWtFdE",
                "replyto": "Me5fDe39Ak",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Reviewers",
                    "ICLR.cc/2024/Conference/Submission8623/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer T4Ma"
                    },
                    "comment": {
                        "value": "Thank you, Reviewer T4Ma, for your insightful questions. We address them as follows:\n1. **Difference and Improvement Over Paper [1]**:\nAs stated in our universal response, our work fundamentally differs from and is concurrent with [1]. We encourage you to refer to our universal response for a detailed explanation.\n2. **Clarification on Features in Table 1**:\nThe all 1\u2019s and all 0\u2019s denote the feature encoding in the first dimension of the node input feature. We use 1 and 0 to classify the variable node and constraint nodes from the bipartite graph.\n3. **Authenticity of Generated MILP Instances**:\nWe assert that feasible-bounded MILP instances are 'authentic\u2019 because 1)most existing MILP datasets for training are feasible, and 2) MILP problems in industrial applications are typically feasible. Infeasible problems are generally transformed into feasible ones before being solved. \nTherefore, we believe training models on feasible datasets to generate feasible MILP instances is appropriate. Please refer to the universal response for more detailed explanation.\n4. **Legibility of Figures**:\nThank you for pointing out the issue with the font size in our figures. We will increase the font size to enhance readability in the revised manuscript.\nWe appreciate your feedback and hope these responses clarify your concerns.\n\n[1] Geng, Zijie, et al. \"A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability.\" Neural Information Processing Systems (2023)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630335643,
                "cdate": 1700630335643,
                "tmdate": 1700630335643,
                "mdate": 1700630335643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8FrevlzmHp",
            "forum": "psDvcWtFdE",
            "replyto": "psDvcWtFdE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8623/Reviewer_KTFt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8623/Reviewer_KTFt"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of instance generation for mixed-integer linear programming (MILP). The authors propose a deep generative framework based on variational auto-encoder (VAE) to capture complex structural characteristics from limited MILP data and generate instances that resemble the original data. Experiments demonstrate the proposed method outperforms baselines on various benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThis paper proposes to leverage the MILP duality theory to ensure the boundedness and feasibility of the generated instances.\n2.\tExperiments demonstrate the proposed method outperforms baselines on various benchmarks."
                },
                "weaknesses": {
                    "value": "1.\tThe technical novelty of the proposed method is incremental, as the proposed method primarily use the existing VAE [1, 2] model to generate MILP instances. The authors may want to explain the technical novelty of the proposed methods in detail. \n2.\tThe relationship between the theoretical derivations (i.e., Theorem 1) and the proposed MILP generation pipeline based on the VAE [1, 2] model is unclear.\n3.\tThe motivation of using the VAE [1, 2] model to generate MILP instances is unclear. The popular generative models include VAE [1, 2], Generative Adversarial Network (GAN) [3], and diffusion model [4]. The authors may want to explain the motivation of using the VAE [1, 2] model rather than the other generative models.\n4.\tI found the proposed method is similar to one recent work at NIPS [5]. The authors may want to explain the differences between their proposed method and the recent work [5] in detail.\n5.\tThe experiments are insufficient. First, it would be more convincing if the authors could evaluate their method on large-scale benchmarks, such as instances from the MIPLIB with over 100,000 variables and 100,000 constraints. Second, the authors may want to evaluate their method on popular downstream tasks, such as learning to cut [6, 7] and learning to branch [8, 9]. Third, the baselines are insufficient. The authors may want to compare their method to G2SAT [10], which is the first deep generative framework that learns to generate Boolean Satisfiability (SAT) problems.\n\nOverall, I would lean toward rejection due to the aforementioned concerns, while I would raise my score if the authors could properly address these concerns.\n\n[1] Kingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" The Second International Conference on Learning Representations, 2014.\n\n[2] Kipf, Thomas N., and Max Welling. \"Variational graph auto-encoders.\" arXiv preprint arXiv:1611.07308 (2016).\n\n[3] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems 27 (2014).\n\n[4] Ho, Jonathan, Ajay Jain, and Pieter Abbeel. \"Denoising diffusion probabilistic models.\" Advances in neural information processing systems 33 (2020): 6840-6851.\n\n[5] Geng, Zijie, et al. \"A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability.\" Neural Information Processing Systems (2023).\n\n[6] Paulus, Max B., et al. \"Learning to cut by looking ahead: Cutting plane selection via imitation learning.\" International conference on machine learning. PMLR, 2022.\n\n[7] Wang, Zhihai, et al. \"Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model.\" The Eleventh International Conference on Learning Representations. 2023.\n\n[8] Gupta, Prateek, et al. \"Hybrid models for learning to branch.\" Advances in neural information processing systems 33 (2020): 18087-18097.\n\n[9] Gasse, Maxime, et al. \"Exact combinatorial optimization with graph convolutional neural networks.\" Advances in neural information processing systems 32 (2019).\n\n[10] You, Jiaxuan, et al. \"G2SAT: Learning to generate SAT formulas.\" Advances in neural information processing systems 32 (2019)."
                },
                "questions": {
                    "value": "1.\tWhat is the technical novelty of the proposed method?\n2.\tWhat is the relationship between the theoretical derivations and the proposed method?\n3.\tWhat is the motivation of using the VAE model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698656985583,
            "cdate": 1698656985583,
            "tmdate": 1699637078493,
            "mdate": 1699637078493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GsVtekOk35",
                "forum": "psDvcWtFdE",
                "replyto": "8FrevlzmHp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Reviewers",
                    "ICLR.cc/2024/Conference/Submission8623/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer KTFt"
                    },
                    "comment": {
                        "value": "We appreciate Reviewer KTFt's feedback and address the raised concerns as follows:\n1. **Technical Novelty is Incremental**:\nWe respectfully disagree with this assessment.Our paper is the first to apply a deep generative model for the generation of MILP instances with feasibility and boundedness guarantee. Please refer to the universal response for more details.\n2. **Clarity on Theoretical Derivations and Motivation for Using VAE**:\nOur core contribution is to extract information from a well-designed generative space for producing new MILP examples with feasibility and boundedness guarantees. Theorem 1. in our paper provides theoretical backing for the guarantee, and the choice of generative model is not the key of our work.\nWe acknowledge the reviewer's point about discussing our choice of generative model. Briefly, generating graphs from scratch, especially larger ones, might face complexity and scalability issues, hence our choice of VAE to modify parts of MILP examples iteratively. We will amend our paper to include comparisons with other models.\n3. **Comparison with Recent Work at NIPS [5]**:\nOur work fundamentally differs from [5] and is concurrent, as detailed in our universal response.\n4. **Adequacy of Experiments**:\nThanks for your advice. We find most studies in the ML for MILP community focus on problems of a hundred variables to a thousand in size. Our experiments on two sizable MIPLIB datasets might sufficiently demonstrate the essence of our method. \nThe downstream tasks suggested by the reviewer could they themselves be independent research branches and are far removed from the core issue we address. \nFurthermore, the framework of DIG-MILP supports general MILP generation, unlike G2SAT which works on stratification problems, making it unsuitable as a baseline in our context.\nWe hope this adequately addresses the concerns and thank you for your valuable input."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630272805,
                "cdate": 1700630272805,
                "tmdate": 1700630272805,
                "mdate": 1700630272805,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jiXTZSRXp3",
            "forum": "psDvcWtFdE",
            "replyto": "psDvcWtFdE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8623/Reviewer_NJds"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8623/Reviewer_NJds"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a deep instance generator for MILPs with feasibility gurantee. It uses a VAE model trained on a dataset to generate similar MILP instances, and leverages a dual method proposed by Bowly et al. for feasibility."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This paper proposes a MILP generator with feasibility gurantee.\n2. It conduct some experiments to demonstrate the effectiveness and for analysis."
                },
                "weaknesses": {
                    "value": "1. The technical novelty is minor. The proposed model is a direct combination of two existing methods [1] and [2]. [1] is a recent work accepted by NeurIPS and this paper is almost the same with [1]. Even if taking [1] without consideration, this work is an application of existing techniques, i.e., the VAE for graph generation and the feasible instance construction method proposed in [2].\n2. Does this paper deal with MILPs or IPs? In Eq. (1) all variables are constrained as integers. In table 1 there are no features indicating whether the variables are integers.\n3. Do the datasets contain unfeasible MILPs? Can the model learn to generate feasible MILPs without the feasibility gurantee? The necessity of this component is not demonstrated with ablation study.\n4. The proposed method does not performs better than random significantly.\n5. Why not report the hyper-configuration results to show whether this method can benefit this task?\n6. What is the useness of the optimal value prediction task? Can the proposed method help the solving instead of just predicting the optimal value? \n\n[1] https://arxiv.org/abs/2310.02807\n\n[2] Simon Andrew Bowly. Stress testing mixed integer programming solvers through new test instance generation methods. PhD thesis, School of Mathematical Sciences, Monash University, 2019."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8623/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8623/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8623/Reviewer_NJds"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8623/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698861565126,
            "cdate": 1698861565126,
            "tmdate": 1699637078389,
            "mdate": 1699637078389,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YFaTH5vVn2",
                "forum": "psDvcWtFdE",
                "replyto": "jiXTZSRXp3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission8623/Reviewers",
                    "ICLR.cc/2024/Conference/Submission8623/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8623/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Njds"
                    },
                    "comment": {
                        "value": "Thank you, Reviewer NJds, for your detailed reading and suggestions on our paper. Below, we address the weaknesses raised by NJds:\n1. **Technical Novelty is Minor**:\nWe respectfully disagree with the reviewer\u2019s assessment. Please refer to our universal response for clarification.\n2. **MILPs vs. IPs Clarification**:\nThank you for pointing this out. The short answer is yes, DIG-MILP supports generating MILP. \nIt would only require training an additional classifier in the decoder to decide whether to round solution or slack variables into integers, which could then control whether the variables are discrete or continuous. When using DIG-MILP to solve IP instances, one could omit such classifiers. We will revise our manuscript to include such an explanation.\n3. **Feasibility of Generated MILPs**:\nThe short answer is the datasets do not contain infeasible MILPs. DIG-MILP indeed requires the training data with at least one set of solutions to generate feasible-bounded new instances. But we argue that such a requirement of training DIG-MILP solely on feasible datasets is both practical and appropriate. Please refer to universal response 3) for more detailed explanation. \n4. **Performance Compared to Random**:\nWe respectfully disagree with the reviewer\u2019s argument. From the aspect of the structural similarity (15%-20% higher similarity score) and performance on the downstream tasks (10% higher Pearson correlation score in solver tuning and much lower MSE in ML model training), the performance of DIG-MILP is much better than the random baseline. \n5. **Hyper-configuration results**:\nHyper-configuration results can be inferred from the provided figures, where configurations resulting in shorter SCIP solving times on both original and generated data are preferred. Direct comparison of optimal hyper-configurations is challenging, because it may not be proper to evaluate the similarity of two sets of hyper-configurations by their distance directly on Euclidean space. \n6. **Optimal Value Prediction Task**:\nTackling the optimal value prediction task is a preliminary step in aiding MILP problems with ML models. We follow a general task setting introduced in [1]. Directly solving MILP is more challenging than predicting the optimal solution. In MILP generation task, one may first do well on the simpler task and then move towards more challenging tasks in the future work.\n\nWe hope this addresses your concerns and thank you once again for your valuable feedback.\n\n[1] Chen et al. On the representation of solutions to elliptic pdes in barron spaces ICLR 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8623/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630200790,
                "cdate": 1700630200790,
                "tmdate": 1700630200790,
                "mdate": 1700630200790,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]