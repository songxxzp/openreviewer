[
    {
        "title": "InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image"
    },
    {
        "review": {
            "id": "qFi7C7D3YM",
            "forum": "XIxhINXtQk",
            "replyto": "XIxhINXtQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2523/Reviewer_1rfe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2523/Reviewer_1rfe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method that generates a 3D-edited NeRF from a single portrait image. The style is defined by an instructing prompt. Two streams of inputs of a real face and a 2D-edited face are passed through an encoder to generate identity conditions. The identity condition, together with text condition, is sent to a diffusion model to generate tri-plane features for NeRF rendering. Experimental results show that the proposed method outperforms compared baseline approaches under the authors' settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It is the first (to my knowledge) paper that allows \"instructed\" 3D portrait editing from single images.\n\n- The experiments show that the proposed method outperforms compared baselines under the authors' settings."
                },
                "weaknesses": {
                    "value": "- The results shown in the paper lack race diversity. There are almost no Asian or black people. I'm worried whether the proposed method does not perform well on those cases.\n\n- The identity may change after applying the proposed method. For example, in Fig. 1 first example, the eye shape changed after the beard was removed. In Fig. 3 middle example, the girl seems to look more Asian and the nose shape changed after editing. These are not analyzed in the limitation section.\n\n- The proposed method adopts two streams of inputs (real and edited images). However, the ablation study does not show the necessity of  \n them. Will only one stream work?"
                },
                "questions": {
                    "value": "I would like to see the authors address my concerns mentioned in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2523/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2523/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2523/Reviewer_1rfe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2523/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698536517234,
            "cdate": 1698536517234,
            "tmdate": 1700668086226,
            "mdate": 1700668086226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T5ks2HweRx",
                "forum": "XIxhINXtQk",
                "replyto": "qFi7C7D3YM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 1rfe (1/2)"
                    },
                    "comment": {
                        "value": "We greatly appreciate your recognition of the significance and novelty of our work. Below, we address your questions and concerns to provide a more comprehensive understanding of our approach.\n\n### Summary of your key concerns and our improvements:\n1. You suggest we provide examples of different races (W1)\n- **Response:** We provided additional examples featuring Asians and black people and discussed the diversity and generalization of our method in Appendix A.3.\n2. You have some concerns about the preservation of identity. (W2)\n- **Response:** We have added more examples and comparative experiments and show the significant improvement of our method over previous methods through quantitative metrics and user study.\n3. You suggest we discuss the necessity of the triplet data mechanism. (W3)\n- **Response:** We conducted an ablation study in Appendix A.2.7 and showed that the triplet data mechanism is essential for achieving high-quality image editing results.\n\n### Detailed responses to your concerns item-by-item.\n\n> *W1: The results shown in the paper lack race diversity.*\n\n**A**: \nThank you for raising this important point. We acknowledge the importance of diversity in the evaluation of image editing methods and strive to provide a well-rounded assessment. **Our models are indeed capable of handling different races, hairstyles, ages, and other attributes.**\n\nAs described in Appendix A.1.1 Experimental Settings, our model is trained on the FFHQ dataset, which includes a diverse set of faces, and evaluated on the CelebA-HQ dataset. This diverse training data ensures that our model can generalize to various races and attributes.\n\nTo address your concern, we have included editing results for both Asian and black people in Figures 14 and 15. These additional examples show the diversity of our method in handling different races.\n\n> *W2: The identity may change after applying the proposed method.*\n\n**A**: Thank you for your careful observation. Identity preservation and attribute disentanglement have posed challenges for face editing. 3D-aware editing becomes more difficult than 2D editing in this regard since only one input image is visible. \n\nThe ID score for 2D editing(e4e, HFGI) is about 0.8, and for structured 3D-aware editing(PREIM3D, IDE-3D) is 0.5~0.6. We have added quantitative results of InstructPix2Pix and img2img on 30 attribute editings, highlighted in red, in Table 5. **Tables 2, 3, and 5 show that our ID score is above 0.6, higher than the previous work. Furthermore, the results of our user study (Table 6) support the notion that our method achieves superior identity preservation and attribute disentanglement.**\n\nWe also recognize that there is room for improvement in preserving finer details such as eye shape and eyelashes, and we acknowledge these as areas for future research in this domain.\n\nMoreover, note that the ID score reported in our paper is multi-view identity consistency, which is calculated between the novel views after editing and the input view.\nLike EG3D, the state-of-the-art 3D face generation method, we also uniformly rendered novel views from yaw angles between $[-30^{\\\\circ},30^{\\\\circ}]$ and pitch angles between $[-20^{\\\\circ},20^{\\\\circ}]$ for an input image.\nEG3D reported a score of 0.77 for identity similarity in synthetic faces in their paper.\nAlthough we are working with real-world faces, not synthetic ones, and with editing, we still achieve a score of 0.6 or above.\n**To the best of our knowledge, our method is the current state-of-the-art in preserving identity in 3D-aware face editing.\nTherefore, We highlight our ability to preserve identity after editing in different poses.**\n\nTo be continued."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408812318,
                "cdate": 1700408812318,
                "tmdate": 1700467472719,
                "mdate": 1700467472719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K6jMuIo7ub",
                "forum": "XIxhINXtQk",
                "replyto": "qFi7C7D3YM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer 1rfe (2/2)"
                    },
                    "comment": {
                        "value": "Continued to responses part 1.\n\n> *W3: The proposed method adopts two streams of inputs (real and edited images). However, the ablation study does not show the necessity of them. *\n\n**A**: We appreciate your interest in the necessity of the two streams of inputs (triplet data mechanism) in our method.  \nThe triplet data mechanism plays a crucial role in achieving accurate and disentangled image editing. To provide a more thorough understanding of its importance, we conducted an ablation study and presented the results below.\n\nOur comparison involves the img2img model, which can be considered as using a text-image pairing data mechanism, in contrast to our method which utilizes the triplet data mechanism. Img2img is trained on the instructions and edited images in our prepared dataset.Unlike InstructPix2NeRF, img2img has no paired images, but the rest of the network structure is same. \n\n**The results of the ablation study, summarized in Table R3.2, demonstrate that the triplet data mechanism significantly contributes to the quality of editing in terms of identity preservation (ID) and attribute dependency (AD) when attribute altering (AA) is close to equal.**\n\n- **ID Score**: Our method consistently outperforms img2img in preserving identity across various attributes, as indicated by the higher ID scores.\n\n- **AA Score**: Our method achieves an editing effect that is close to or superior to that of img2img.\n  \n- **AD Score**: The triplet data mechanism helps reduce attribute dependency, ensuring that changes to one attribute do not excessively affect others.\n\n**These results highlight that the triplet data mechanism encourages the model to learn the correlations between changes in pairs of images and the corresponding instructions, leading to more precise and disentangled editing.** In conclusion, the triplet data mechanism is essential for achieving high-quality image editing results.\n\nTable R3.2. The triplet data mechanism ablation. Identity consistency (ID)  measures if it is the same person. Attribute altering (AA) measures the change of the desired attribute. Attribute dependency (AD) measures the change in other attributes when modifying one attribute.\n|Method|bang|eyeglasses|smile|multiple instructions|five o\u2019clock shadow |bushy eyebrows|chubby|double chin|high cheekbones|pale skin|goatee|\n|:--------|:-------:|:-------:|:------:|:------:|:-------:|:-------:|:------:|:------:|:------:|:------:|:------:|\n|img2img-ID$\\\\uparrow$ |0.40|0.42|0.46|0.37|0.52|0.53|0.50|0.51|0.51|52|0.50|\n|ours-ID$\\\\uparrow$    |**0.56**|**0.59**|**0.60**|**0.55**|**0.59**|**0.64**|**0.64**|**0.64**|**0.64**|**0.66**|**0.65**|\n|img2img-AA$\\\\uparrow$ |0.99|3.33|1.47|1.39|0.94|**1.02**|**1.05**|0.96|0.96|1.03|1.12|\n|ours-AA $\\\\uparrow$  |**1.05**|**3.37**|**1.50**|**1.50**|**0.95**|1.00|**1.05**|**1.00**|**1.07**|**1.11**|**1.21**|\n|img2img-AD$\\\\downarrow$|0.61|0.79|0.76|0.88|0.77|0.62|0.65|0.76|0.62|0.53|0.57|\n|ours-AD$\\\\downarrow$   |**0.53**|**0.64**|**0.61**|**0.69**|**0.74**|**0.51**|**0.54**|**0.63**|**0.54**|**0.47**|**0.50**|\n\n### Summary:\nThanks for your encouraging review and valuable suggestions. We hope that our added examples and experiments will address your concerns. Moreover, We believe our method will become a strong baseline for future works towards instructed 3D-aware face editing from a single image."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408902549,
                "cdate": 1700408902549,
                "tmdate": 1700438589509,
                "mdate": 1700438589509,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MJCiIl6BGl",
                "forum": "XIxhINXtQk",
                "replyto": "qFi7C7D3YM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please let us know if our responses address your concerns."
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe sincerely appreciate the demands on your time, especially during this busy period. After carefully considering your valuable feedback, We have made some new experiments and necessary modifications to our paper. We are wondering if our responses and revision have addressed your concerns. \n\nWe are extremely grateful for your time and effort in reviewing our paper, and we sincerely appreciate your feedback. We believe your comments have been instrumental in enhancing the quality of our paper. As today is the last day of the discussion stage, we are kindly awaiting your response.\n\nIf you have any further questions or require any additional information from us, please let us know. We would like to invite you to consider our responses and look forward to your reply.\n\nOnce again, thank you for your attention and support. \n\nBest regards,\nThe Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667507076,
                "cdate": 1700667507076,
                "tmdate": 1700667507076,
                "mdate": 1700667507076,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MRLbDVSCjv",
                "forum": "XIxhINXtQk",
                "replyto": "MJCiIl6BGl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Reviewer_1rfe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Reviewer_1rfe"
                ],
                "content": {
                    "title": {
                        "value": "Re: Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the response. I\u2019ve increased my rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668156600,
                "cdate": 1700668156600,
                "tmdate": 1700668156600,
                "mdate": 1700668156600,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y3I41VMxtI",
            "forum": "XIxhINXtQk",
            "replyto": "XIxhINXtQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2523/Reviewer_e7JS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2523/Reviewer_e7JS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method that enables the creation of 3D portraits that have been edited based on text prompts. Leveraging the latent space of EG3D to impose 3D consistency, the proposed method finds a latent vector in the W+ space that matches the edits specified by the prompt and the identity in the input image. A diffusion model conditioned on the input 2D image and the editing prompt is used to predict this latent vector. Additionally, the paper proposes the following 1) Token position randomization to improve the quality of multi-instruction editing 2) An identity consistency module to improve identity preservation during edits."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) While each individual component of the method isn\u2019t novel, the whole method itself is\n\n2) Qualitative results in both the paper and appendix demonstrate plausible editing, though some identity loss remains\n\n3) Quantitative results demonstrate that the method better preserves the identity across edits. The user study additionally bolsters the main contribution of the paper."
                },
                "weaknesses": {
                    "value": "1) The methods section could be written better, with a clear exposition of losses during training and the forward pass during inference. To that end, Fig 2 should be expanded to include both training and inference settings. \n\n2) While the identity consistency is better preserved that prior work, the still remains and identity drift during editing."
                },
                "questions": {
                    "value": "1) Instead of an Encoder, if direct optimization of the W+ vector was used (assuming much larger compute), would it preserve the identity better? What if this is only done during inference and not training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2523/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2523/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2523/Reviewer_e7JS"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2523/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728452454,
            "cdate": 1698728452454,
            "tmdate": 1700668761932,
            "mdate": 1700668761932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r0XHZkUAt9",
                "forum": "XIxhINXtQk",
                "replyto": "Y3I41VMxtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer e7JS (1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your interest in our work and your valuable feedback. Below, we provide detailed responses to your comments and questions, addressing your concerns and providing further insights.\n\n\n### Summary of your key concerns and our improvements:\n1. You suggest we clarify the exposition of losses during training and the forward pass during inference. (W1)\n- **Response:** We have considered your feedback, made improvements to clarify the total training loss and the forward pass during inference, and modified Figure 2.\n2. You have some concerns about the preservation of identity. (W2)\n- **Response:** We have added more examples and comparative experiments and show the significant improvement of our method over previous methods through quantitative metrics and user study.\n3. You are curious about the potential benefits of directly optimizing the $\\\\mathcal{W+}$ vector for identity preservation. (Q1)\n- **Response:** We conducted experiments to explore this scenario and provide insights into its causes in Appendix A.2.5.\n\n### Detailed responses to your concerns item-by-item.\n\n> *W1: The methods section could be written better, with a clear exposition of losses during training and the forward pass during inference.*\n\n**A**: We apologize for any confusion regarding our methods section. We have taken your feedback into account and have made improvements to clarify the total training loss and the forward pass during inference.\n\nIn Section 4.3, we have included a clear exposition of the total loss used in our method, which consists of the diffusion loss $\\\\mathcal{L}\\_{diff}$ and the identity loss $\\\\mathcal{L}\\_{ID}$. The total loss is expressed as:\n$$\n\\\\mathcal{L} = \\\\mathcal{L}\\_{diff} + \\\\lambda\\_{id} \\\\mathcal{L}\\_{ID}\n$$\nwhere $\\\\lambda\\_{id}$ is set to 0.1 in our experiments. This addition aims to provide a more comprehensive understanding of our training process.\n\n**In Section 4.4, we illustrate the inference process in detail.** Specifically, we add a little bit of noise to the latent of the input image (usually 15 steps) to obtain $w\\_{ot}$ and then use our model to perform conditional denoising. The model predicts three score estimates, the image-text conditional $\\\\epsilon\\_{\\\\theta}(w\\_{ot}, c\\_{I}, c\\_{T})$, the only-image conditional $\\\\epsilon\\_{\\\\theta}(w\\_{ot}, c\\_{I}, \\\\emptyset)$, and the unconditional $\\\\epsilon\\_{\\\\theta}(w\\_{ot}, \\\\emptyset, \\\\emptyset)$.\n$c\\_{T}=\\\\emptyset$ indicates that the text takes an empty character. $c\\_{I}=\\\\emptyset$ means that the concatenation $w\\_{o}$ takes zero and identity modulation takes zero. \nImage and text conditioning sampling can be performed as follows:\n$$\n  \\\\tilde{\\\\epsilon}\\_{\\\\theta}(w\\_{ot}, c\\_{I}, c\\_{T})=\\\\epsilon\\_{\\\\theta}(w\\_{ot}, \\\\emptyset, \\\\emptyset) + s\\_{I}(\\\\epsilon\\_{\\\\theta}(w\\_{ot}, c\\_{I}, \\\\emptyset) - \\\\epsilon\\_{\\\\theta}(w\\_{ot}, \\\\emptyset, \\\\emptyset)) + \\\\\\\\ s\\_{T}(\\\\epsilon\\_{\\\\theta}(w\\_{ot}, c\\_{I}, c\\_{T}) - \\\\epsilon\\_{\\\\theta}(w\\_{ot}, c\\_{I}, \\\\emptyset))\n$$\nwhere $s\\_{I}$ and $s\\_{T}$ are the guidance scales for alignment with the input image and the text instruction, respectively.\n  \nTo further enhance clarity, **we have also modified Figure 2 to include a switch, distinguishing between the training and inference processes.** This change helps illustrate the transition from training to inference and highlights the specific steps involved.\n\n\n> *W2: While the identity consistency is better preserved than prior work, there still remains identity drift during editing.*\n\n**A**: Thank you for acknowledging our work. Identity preservation and attribute disentanglement have posed challenges for face editing. 3D-aware editing becomes more difficult than 2D editing in this regard since only one input image is visible. \n\nThe ID score for 2D editing(e4e, HFGI) is about 0.8, and for structured 3D-aware editing(PREIM3D, IDE-3D) is 0.5~0.6. We have added quantitative results of InstructPix2Pix and img2img on 30 attribute editings, highlighted in red, in Table 5. **Tables 2, 3, and 5 show that our ID score is above 0.6, higher than the previous work. Furthermore, the results of our user study (Table 6) support the notion that our method achieves superior identity preservation and attribute disentanglement.**\n\nWe also recognize that there is room for improvement in preserving finer details such as eye shape and eyelashes, and we acknowledge these as areas for future research in this domain.\n\nTo be continued."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408445541,
                "cdate": 1700408445541,
                "tmdate": 1700413312876,
                "mdate": 1700413312876,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wVNRCfApvl",
                "forum": "XIxhINXtQk",
                "replyto": "Y3I41VMxtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer e7JS (2/2)"
                    },
                    "comment": {
                        "value": "Continued to responses part 1.\n> *W2: While the identity consistency is better preserved than prior work, there still remains identity drift during editing.* (continued)\n\nMoreover, note that the ID score reported in our paper is multi-view identity consistency, which is calculated between the novel views after editing and the input view.\nLike EG3D, the state-of-the-art 3D face generation method, we also uniformly rendered novel views from yaw angles between $[-30^{\\\\circ},30^{\\\\circ}]$ and pitch angles between $[-20^{\\\\circ},20^{\\\\circ}]$ for an input image.\nEG3D reported a score of 0.77 for identity similarity in synthetic faces in their paper.\nAlthough we are working with real-world faces, not synthetic ones, and with editing, we still achieve a score of 0.6 or above.\n**To the best of our knowledge, our method is the current state-of-the-art in preserving identity in 3D-aware face editing.\nTherefore, We highlight our ability to preserve identity after editing in different poses.**\n\n> *Q1: Instead of an Encoder, if direct optimization of the $\\\\mathcal{W+}$ vector was used (assuming much larger compute), would it preserve the identity better? What if this is only done during inference and not training?*\n\n**A**: Your question is indeed intriguing, and we appreciate your curiosity regarding the potential benefits of direct optimization of the $\\\\mathcal{W+}$ vector for identity preservation, especially if conducted during inference. We conducted experiments to explore this scenario and provide insights into its causes.\n\n**In our experiments, we considered two configurations: Direct $\\\\mathcal{W+}$ optimization and PTI optimization.** Direct $\\\\mathcal{W+}$ optimization involves optimizing the $\\\\mathcal{W+}$ vector while keeping the generator fixed. PTI (Pivotal Tuning Inversion) technique fine-tunes the generator based on the initial value provided by direct optimization. We conducted 500 steps of optimization on the $\\\\mathcal{W+}$ vector, and PTI added 100 steps of fine-tuning the generator.\n\nThe results of these experiments are presented in Figure 10, where we compare the outcomes of direct $\\\\mathcal{W+}$ optimization, PTI, and the encoder-based method. The results show that directly replacing the encoder with an optimization method during inference will lead to a severe decrease in both editing effect and identity consistency.\n\n**We attribute this issue to the deviation between the model and data distribution.** The model learns a conditional distribution within the encoder's inversion space during training. When the encoder is replaced by an optimization method during inference, the data distribution used for inference mismatches the learned model distribution. This mismatch results in greater identity drift and undesirable editing outcomes.\n\nWhile conducting $\\\\mathcal{W+}$ optimization during training (much larger compute) could potentially address the distribution deviation problem, it may introduce artifacts in novel views, as pointed out by PREIM3D. This is due to optimization being performed on a single image during training.\n\nIn summary, while direct optimization of the $\\\\mathcal{W+}$ vector is an interesting concept, our experiments suggest that it may not necessarily lead to improved identity preservation and editing results compared to the encoder-based approach.\n\n### Summary:\nThank you for your encouraging comments and writing suggestions. They are very helpful for us to improve the article. Moreover, we provided new examples and experiments, which we believe can strengthen the breadth and depth of our validation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408616056,
                "cdate": 1700408616056,
                "tmdate": 1700467440238,
                "mdate": 1700467440238,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0uEGFo0pks",
                "forum": "XIxhINXtQk",
                "replyto": "Y3I41VMxtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please let us know if our responses address your concerns."
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe sincerely appreciate the demands on your time, especially during this busy period. After carefully considering your valuable feedback, We have made some new experiments and necessary modifications to our paper. We are wondering if our responses and revision have addressed your concerns. \n\nWe are extremely grateful for your time and effort in reviewing our paper, and we sincerely appreciate your feedback. We believe your comments have been instrumental in enhancing the quality of our paper. As today is the last day of the discussion stage, we are kindly awaiting your response.\n\nIf you have any further questions or require any additional information from us, please let us know. We would like to invite you to consider our responses and look forward to your reply.\n\nOnce again, thank you for your attention and support. \n\nBest regards,\nThe Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667480686,
                "cdate": 1700667480686,
                "tmdate": 1700667480686,
                "mdate": 1700667480686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SMyQAatqbl",
                "forum": "XIxhINXtQk",
                "replyto": "0uEGFo0pks",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Reviewer_e7JS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Reviewer_e7JS"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the rebuttal. Looking at the rebuttal and other reviews, I have decided to revise my initial rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668739223,
                "cdate": 1700668739223,
                "tmdate": 1700668739223,
                "mdate": 1700668739223,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ehuGGxG3iO",
            "forum": "XIxhINXtQk",
            "replyto": "XIxhINXtQk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2523/Reviewer_TVmA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2523/Reviewer_TVmA"
            ],
            "content": {
                "summary": {
                    "value": "The proposed approach, InstructPix2NeRF, is an end-to-end model designed for 3D-aware human head editing using a single image and an instructive prompt as inputs. To achieve this results, firstly the authors construct a multimodal 2D human head dataset by leveraging pretrained diffusion models such as e4e and InstructPix2Pix. Secondly, they propose a token position randomization strategy to enhance the model's ability to edit multiple attributes simultaneously. Last, an identity consistency module is incorporated to extract facial identity signals from the input image and guide the editing process. Experimental results demonstrate the effectiveness and superiority of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of the proposed paper can be summarized as:\n1. The authors propose a token randomization strategy that can increase the model's capability for editing multiple attributes simultaneously.\n2. An identity-preserving module is proposed to guide the editing process and present the original identity in the final outcomes.\n3. The proposed method is reported to be time-friendly, producing the results in few seconds."
                },
                "weaknesses": {
                    "value": "The weaknesses of the proposed method can be summarized as:\n1. Through the visualization in Figure 1, I find that the original identity and RGB image attributes are not well preserved. Large differences can still be observed in the areas that are not supposed to be edited.\n2. Qualitative comparisons. (1) The proposed method seems to struggle with expression editing, e.g., it fails to make the head smiling; The instruct-pix2pix model doesn't encounter this problem; (2) Regarding the \"bangs\" example, I would prefer the instruct-pix2pix as it contains real bangs; (3) There is no comparisons with Instruct-NeRF2NeRF, AvatarStudio, and HeadSculpt, considering they are more similar works than the compared Talk-to-Edit and img2img; (4) More examples and more scenarios will largely improve the validation. Currently, there are only three types presented.\n3. Quantitative comparisons. (1) The evaluations are not comprehensive. Still, only three examples are presented; (2) More quantitative evaluations, e.g., user studies, would be beneficial."
                },
                "questions": {
                    "value": "Besides the weaknesses above, I may have some questions that hope the authors can answer:\n1. There lack the reason for generating and using 20-30 instruction prompts for one single paired image. Will the number of instructions affect the training?\n2. How will the model perform when it deals with novel characters as in the movie, long hair examples, black men/women, and human of different ages?\n3. Will the background affect the edited results? It would be interesting to see the outcomes obtained when editing the same subject against various backgrounds."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2523/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770973757,
            "cdate": 1698770973757,
            "tmdate": 1699636188642,
            "mdate": 1699636188642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jb3K7vLL8T",
                "forum": "XIxhINXtQk",
                "replyto": "ehuGGxG3iO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer TVmA (1/4)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and constructive suggestions. Based on your suggestions, We have improved our paper and a revised version is available now. First, we summarize what we have done this week to address your concerns and provide detailed responses for each weakness and question. \n\n### Summary of your key concerns and our improvements:\n1. You have some concerns about the preservation of identities and attributes, as well as the effects of editing. (W1, W2(1), W2(2))\n- **Response:** We have added more examples and comparative experiments and show the significant improvement of our method over previous methods through quantitative metrics and user study.\n2. You suggest comparing our method to Instruct-NeRF2NeRF, AvatarStudio, and HeadSculpt. (W2(3))\n- **Response:** We discussed and cited these methods in Section 2 and pointed out that they are different from our task and cannot handle 3D-aware editing from a single image.\n3. You suggest we provide a wider range of examples (W2(4), Q2)\n- **Response:** We provided additional examples and scenarios featuring different races, ages, and hairstyles and discussed the diversity and generalization of our method in Appendix A.3.\n4. You suggest we conduct a more comprehensive and quantitative evaluation. (W3)\n- **Response:** We conducted a new experiment to compare our method against InstructPix2Pix and img2img on 30 attribute editing types.  \n5. You suggest we discuss the effect of background on editing results. (Q3)\n- **Response:** We conducted a new experiment where we edited images of the same subject in different backgrounds in Appendix A.2.6.\n\n### Detailed responses to your concerns item-by-item.\n> *W1: The original identity and RGB image attributes are not well preserved.*\n\n**A**: Thanks. Identity preservation and attribute disentanglement have posed challenges for face editing. 3D-aware editing becomes more difficult than 2D editing in this regard since only one input image is visible. \n\nThe ID score for 2D editing(e4e, HFGI) is about 0.8, and for structured 3D-aware editing(PREIM3D, IDE-3D) is 0.5~0.6. We have added quantitative results of InstructPix2Pix and img2img on 30 attribute editings, highlighted in red, in Table 5. **Tables 2, 3, and 5 show that our ID score is above 0.6, higher than the previous work. Additionally, the results of our user study (Table 6) support the notion that our method achieves superior identity preservation and attribute disentanglement.**\n\nFurthermore, we introduced attribute dependency (AD) score to measure the change in other attributes when editing certain attributes. **Figure 3 shows that our method preserves the other attributes better when meeting the editing requirements.** For example, in the third row, first column of Figure 3, InstructPix2Pix alters the expression when adding bangs, while our method maintains expression consistency. InstructPix2Pix and img2img often lead to undesirable changes in skin color. \nWe acknowledge that there are some limitations in preserving fine details such as eye shape and eyelashes, and we consider these areas for future improvement.\n\nMoreover, note that the ID score reported in our paper is multi-view identity consistency, which is calculated between the novel views after editing and the input view.\nLike EG3D, the state-of-the-art 3D face generation method, we also uniformly rendered novel views from yaw angles between $[-30^{\\\\circ},30^{\\\\circ}]$ and pitch angles between $[-20^{\\\\circ},20^{\\\\circ}]$ for an input image.\nEG3D reported a score of 0.77 for identity similarity in synthetic faces in their paper.\nAlthough we are working with real-world faces, not synthetic ones, and with editing, we still achieve a score of 0.6 or above.\n**To the best of our knowledge, our method is the current state-of-the-art in preserving identity in 3D-aware face editing.\nTherefore, We highlight our ability to preserve identity after editing in different poses.**\n\nto be continued"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406395305,
                "cdate": 1700406395305,
                "tmdate": 1700467399404,
                "mdate": 1700467399404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bBauP9sG0s",
                "forum": "XIxhINXtQk",
                "replyto": "ehuGGxG3iO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer TVmA (2/4)"
                    },
                    "comment": {
                        "value": "Continued to responses part 1.\n\n>*W2 (1) & (2): The proposed method seems to struggle with expression editing, e.g., it fails to make the head smiling. Regarding the \"bangs\" example, I would prefer the instruct-pix2pix as it contains real bangs*\n\n**A**: \nRegarding expression editing, it's important to note that our method allows for controlled adjustments of attributes, including expressions. In the case you mentioned, the head does exhibit a smile, albeit to a lesser degree. The degree of the smile can be controlled through the guidance scale, which also affects identity preservation and attribute dependency. \n**As shown in Tables 2, 3, and 5, our method has better ID preservation and less impact on other attributes when obtaining the same editing degree.**\nThe ID score of about 0.49 and AD score of above 0.7 indicate that InstructPix2Pix is prone to identity drift and other attribute changes when performing editing. The results of the user study (Table 6) also illustrate this point. For example, in the third row, first column of Figure 3, InstructPix2Pix alters the expression when adding bangs, while our method maintains expression consistency.\n\nFor smile editing, you can refer to Figure 11 (first row) and Figure 14 (fourth row) in our paper.\nAs for the \"bangs\" example, you can refer to Figure 15 (third row) and Figure 18 (fourth column). It is true that different methods may have their strengths in specific scenarios, and user preferences may vary. We appreciate your input and understand the appeal of InstructPix2Pix's real bangs. Our method aims to provide versatility for a wide range of editing tasks, particularly those requiring attribute preservation and control.\n\n> *W2 (3): There is no comparisons with Instruct-NeRF2NeRF, AvatarStudio, and HeadSculpt.*\n\n**A**: We appreciate your awareness of these methods and discuss and cite these methods in Section 2. **However, it's crucial to note that these methods have different tasks from our method and cannot handle 3D-aware editing from a single image. While these methods take tens of minutes to optimize a single scene, our method generates 3D-aware editing in seconds for different faces.**\nThe input of our method is a single image and user instruction, and the output is edited 3D-aware images and geometry.\n\n- **Instruct-NeRF2NeRF works with reconstructed NeRF scenes, taking as input a set of captured images, their camera poses, and calibration data, while our method operates on a single image with user instructions.**\nInstruct-NeRF2NeRF uses InstructPix2Pix to iteratively edit the set of captured images while optimizing the underlying scene. \n\n- **AvatarStudio utilizes dynamic full-head avatars reconstructed from head videos, which is distinct from our single-image and instruction-based input.**\nAvatarStudio uses their proposed view-and-time-aware Score Distillation Sampling (VT-SDS) with a personalized diffusion model to edit the dynamic NeRF.\n\n- HeadSculpt primarily focuses on coarse-to-fine text-to-3D generation and can achieve editing in the fine stage by blending scores predicted by ControlNet-based InstructPix2Pix and landmark-based ControlNet for the editing instruction and the original description. **However, HeadSculpt is only capable of editing the trained scene model generated in the coarse stage, rather than a single input image.** \n\nDue to these differences in tasks, inputs, and methodologies, a direct comparison with these methods may not provide meaningful insights for evaluating our work. We hope this clarification helps to address your concern.\n\n> *W2 (4): More examples and more scenarios will largely improve the validation.*\n\n**A**: Thank you for the suggestion. We have taken steps to enhance the validation of our work by including more examples and scenarios. In our previous version, we provided examples in Appendix A.3, covering attributes such as beards, hair color, vampires, zombies, old people, children, long hair, etc. in Figures 12 and 13.\nPer your suggestion, **we have further expanded our results to include novel characters from movies (Figure 16), Asians (Figure 14 and 15), and black men and women (Figure 14 and 15).** We believe that these additional results strengthen the breadth and depth of our validation, showing the diversity of our method.\n\nTo be continued."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407009988,
                "cdate": 1700407009988,
                "tmdate": 1700413902490,
                "mdate": 1700413902490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JlgClxzOjS",
                "forum": "XIxhINXtQk",
                "replyto": "ehuGGxG3iO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer TVmA (3/4)"
                    },
                    "comment": {
                        "value": "Continued to responses part 2.\n\n >*W3 (1): The evaluations are not comprehensive. Still, only three examples are presented.*\n\n**A**: We appreciate your feedback on the comprehensiveness of our evaluations. To address your concern, we would like to clarify that we indeed provide a comprehensive evaluation of our method, which includes a broader set of attributes beyond the three examples presented in the main text.\n\nIn our previous version, we offered quantitative results for a total of 30 attribute editings of our method, as detailed in Table 5. These results provide a comprehensive assessment of our method's performance across a wide range of attributes. per your suggestion, **we have added quantitative results of InstructPix2Pix and img2img on 30 attribute editings, highlighted in red, in Table 5.**\nThe results reveal that when the attribute editing degree AA is comparable to or surpasses the baseline, our method exhibits remarkable enhancement in ID and AD metrics.\n\n> *W3 (2): More quantitative evaluations, e.g., user studies.*\n\n**A**: We apologize for any confusion regarding the presentation of quantitative evaluations, particularly the user study. **In our previous version, we conducted a user study to comprehensively evaluate our method's performance. You can find the user study results in Appendix A.2.4 (Table 6), which provides valuable insights into the quality of our edits.**\n\nThe user study involved 1,440 votes from 30 volunteers who evaluated the text instruction correspondence and multi-view identity consistency of editing results. Participants were asked to choose the better result between our method and baseline for various attributes and multiple instructions. The results, presented in Table 6, clearly demonstrate that our method outperforms the baselines, further substantiating the effectiveness of our approach.\n\nWe appreciate your feedback and have clarified the location of the user study within the paper for your reference. We copy Table 6 here.\n\nTable 6. The result of our user study. The value represents the rate of Ours > others. Multiple instructions indicate editing with the combinations of the above three attributes.\n|Method|bang|eyeglasses|smile|multiple instructions|\n|:--------|:-------:|:-------:|:------:|:------:|\n|Talk-to-Edit|0.742|0.958|0.817|0.875|\n|InstructPix2Pix|0.833|0.667|0.725|0.683|\n|img2img|0.733|0.758|0.750|0.783|\n\n\n> *Q1: Will the number of instructions affect the training?*\n\n**A**: Your question is indeed valuable, and we appreciate your curiosity regarding the impact of the number of instructions on our training process. Up until now, our approach has primarily drawn inspiration from the success of InstructPix2Pix, employing a range of 20-30 instructions during training. This choice was primarily driven by empirical observations and the composition of the InstructPix2Pix dataset.\n\nSpecifically, we performed preprocessing steps where we filtered and removed duplicate face instructions from the InstructPix2Pix dataset. This analysis revealed that, for a single attribute, there were often tens of valid instructions available. By leveraging these insights and filtering out semantically non-compliant instructions generated by ChatGPT, we arrived at the use of 20-30 instructions for training.\n\nWe acknowledge the importance of exploring the impact of instruction quantity further. To address this, we are currently conducting training experiments with a smaller or bigger number of instructions. We hope to obtain preliminary results in the near future and look forward to sharing these findings.\n\n> *Q2: How will the model perform when it deals with novel characters as in the movie, long hair examples, black men/women, and human of different ages?*\n\n**A**: \n**Our models are designed to handle a wide range of attributes, including novel characters from movies, varied hairstyles, different races, and varying ages.** As outlined in our experimental settings (Appendix A.1.1), our model is trained on the FFHQ dataset, which comprises 70,000 faces of diverse individuals. The evaluation is conducted on a separate dataset, CelebA-HQ.\n\nIn Figure 16, we show the results of editing the characters in the movie's background and lighting conditions from this year's movie \"Mission: Impossible \u2013 Dead Reckoning Part One.\"\n\nAdditionally, we provide more examples to illustrate the diversity of our method. These examples include long hairs (Figure 12, 13), black men and women (Figure 14, 15), olds (first row of Figure 12, last row of Figure 15), and children (first and fourth row of Figure 12, last row of Figure 14).\n\nWe believe that these examples underscore the robustness and applicability of our method across a wide range of faces and scenarios.\n\n\nTo be continued."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407217730,
                "cdate": 1700407217730,
                "tmdate": 1700413222729,
                "mdate": 1700413222729,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uAtkUNmaBJ",
                "forum": "XIxhINXtQk",
                "replyto": "ehuGGxG3iO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer TVmA (4/4)"
                    },
                    "comment": {
                        "value": "Continued to responses part 3\n\n> *Q3: Will the background affect the edited results? *\n\n**A**: Thank you for raising this interesting question. To investigate this, we conducted experiments where we edited images of the same subject in different backgrounds.\n\nWe show the results in Figure 11, where the first two rows are the same subject, the middle two rows are the same subject and the last two rows are the same subject. The results show that the background has no obvious impact on the editing results.\n\nHowever, note that when editing colors, particularly when the color being edited is close to the background color, there can be some blending between the foreground and background elements. \n\nWe hope this addresses your query and provides insights into the background's role in our editing process.\n\n### Summary:\nThanks for your valuable questions and suggestions. Your questions are more about the qualitative and Quantitative evaluation. We did a lot of new experiments and tailored the article accordingly. Hope our response can convince you of the performance of our model."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700407327135,
                "cdate": 1700407327135,
                "tmdate": 1700407327135,
                "mdate": 1700407327135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M6PqpgRnaN",
                "forum": "XIxhINXtQk",
                "replyto": "ehuGGxG3iO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please let us know if our responses address your concerns."
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe sincerely appreciate the demands on your time, especially during this busy period. After carefully considering your valuable feedback, We have made some new experiments and necessary modifications to our paper. We are wondering if our responses and revision have addressed your concerns. \n\nWe are extremely grateful for your time and effort in reviewing our paper, and we sincerely appreciate your feedback. We believe your comments have been instrumental in enhancing the quality of our paper. As today is the last day of the discussion stage, we are kindly awaiting your response.\n\nIf you have any further questions or require any additional information from us, please let us know. We would like to invite you to consider our responses and look forward to your reply.\n\nOnce again, thank you for your attention and support. \n\nBest regards,\nThe Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667324837,
                "cdate": 1700667324837,
                "tmdate": 1700667410913,
                "mdate": 1700667410913,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TlwnPGDp7y",
                "forum": "XIxhINXtQk",
                "replyto": "ehuGGxG3iO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2523/Reviewer_TVmA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2523/Reviewer_TVmA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response from the authors! However, I still find:\n\n1. Qualitative results. \n\n(a). Preserving the original identity and RGB attributes for the editing task is crucial in my opinion.\n\n(b). The scenario of \"having her smile\" appears to be quite challenging. Firstly, the edited expression should ideally be more prominent or noticeable. Secondly, I noticed that other editing tasks, such as \"adding bangs to the hairstyle,\" also result in a slight smile. This raises questions about whether these smiling attributes are inherited from the training data rather than intentional edits.\n\n(c). It would be more fair and beneficial to conduct comparisons with current 3D editing methods, even though they may have different input requirements, or different 3D editing setups, rather than solely comparing it to 2D editing methods.\n\n2. Comprehensiveness of our evaluations.\n\n(a). The quantitative numbers in Table 5 are not significantly better. Qualitative comparisons would be valuable.\n\n(b). Additionally, expanding the user studies to cover more than just four scenarios would largely enhance the comprehensiveness of your evaluations.\n\n3. Questions. \n\n(a). It seems that the background has an impact on the results, with a white background yielding the best outcomes.\n\n(b). Based on Appendix A.3, which discusses a wider range of examples, the preservation of identity appears to be a significant challenge. This drawback will be pronounced considering identity-preserving is a most important contribution claimed by the authors.\n\nThanks again! However, overall based on the novelty, contributions, and the authors' response, I am sorry that I would prefer to maintain or reduce my original score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2523/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705144123,
                "cdate": 1700705144123,
                "tmdate": 1700705144123,
                "mdate": 1700705144123,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]