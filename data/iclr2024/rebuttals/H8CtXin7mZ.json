[
    {
        "title": "A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions"
    },
    {
        "review": {
            "id": "ZNv3jYAd9b",
            "forum": "H8CtXin7mZ",
            "replyto": "H8CtXin7mZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6390/Reviewer_ND3n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6390/Reviewer_ND3n"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript uses a neural network to construct a preconditioner for solving Poisson equations on various geometries with varying boundary conditions. The neural network takes 5-7 days to train before it is a reasonable preconditioner across various geometries. It then provides a modest speed-up over some of the existing preconditioning techniques. The biggest gap in the manuscript is that the preconditioner is not symmetric positive definite, and a highly nonstandard iterative method is advocated that will be challenging to develop a general convergence theory. The multigrid preconditioner remains a good choice for fixed geometries, but here, the neural network preconditioner seems beneficial when applied to a range of geometries."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The Deep Conjugate Direction Method (DCDM) is a novel approach that leverages deep learning to approximate the solution of large, sparse, symmetric, positive-definite linear systems of equations. This manuscript is improving on this solver for the setting of Poisson on various geometries. The practical simulations (supplementary) are relatively impressive, demonstrating the effectiveness of their preconditioner. \n\nAfter training, the constructed preconditioner for Poisson's equation is surprisingly effective at reducing the number of iterations of the DCDM-variant for various geometries."
                },
                "weaknesses": {
                    "value": "Without a positive definite preconditioner, it is likely that the simulations in Figure 2 become unstable (or unphysical) if run for long enough. I think the authors should carefully consider both the computational cost per time step and the numerical stability of the time-stepping scheme when using various preconditioners. This direction is mentioned as a future work, but I think the paper's contribution is compromised without ensuring symmetric positive definiteness.\n\nThe lack of theoretical convergence analysis of the preconditioner and iterative solver means that the solver is of limited general use. Since the preconditioner is not positive definite, I suspect that the theoretical convergence of the iterative scheme is extremely difficult to understand.  \n\nTraining the neural network with a large memory requirement takes a very long time before it can be successfully used in time simulations."
                },
                "questions": {
                    "value": "Can the method be used to construct a neural network-based positive definite precondition? If so, then it at least takes into consideration that the original problem is symmetric, positive-definite linear systems of equations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Reviewer_ND3n"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6390/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698017127831,
            "cdate": 1698017127831,
            "tmdate": 1699636708229,
            "mdate": 1699636708229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6QgJbxxWuO",
                "forum": "H8CtXin7mZ",
                "replyto": "ZNv3jYAd9b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6390/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6390/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the acknowledgement of the surprising effectiveness of our preconditioner. We reiterate our point from the main post that symmetry and positive definiteness, while likely to bring modest benefits, are not critical for our solver's efficacy--or the simulation's stability--in practice.\n\nRegarding the lack of theoretical convergence analysis and the high resource requirements for training, we note that these same limitations apply to the seminal work [Tompson et al. 2017] that has nonetheless made a large impact due to its practical benefits. Especially for the applications we target in real-time simulation and visual effects, the single upfront cost of training the network can pay great dividends over the lifetime of the solver's use."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6390/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039532789,
                "cdate": 1700039532789,
                "tmdate": 1700039532789,
                "mdate": 1700039532789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qq2mfsOyzw",
                "forum": "H8CtXin7mZ",
                "replyto": "6QgJbxxWuO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_ND3n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_ND3n"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the official comment by authors"
                    },
                    "comment": {
                        "value": "Thank you very much for the authors' reply. However, I still largely disagree with the statement that \"symmetry and positive definiteness, while likely to bring modest benefits, are not critical for our solver's efficacy--or the simulation's stability--in practice.\" \n\nFor the lack of theoretical convergence analysis, while another work suffers from the same limitation, does not justify the case here. In my opinion, when it comes to preconditioning, the lack of theory (even for some simple problems) here is a significant limitation.\n\nTherefore, I decided to keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6390/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655149957,
                "cdate": 1700655149957,
                "tmdate": 1700655149957,
                "mdate": 1700655149957,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SDxayL6OUd",
            "forum": "H8CtXin7mZ",
            "replyto": "H8CtXin7mZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6390/Reviewer_ddbQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6390/Reviewer_ddbQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a neural network architecture inspired by multigrid to learn an approximate inverse of the discrete Laplace operator, which serves as a preconditioner for an iterative PDE solver. The key contributions are: 1) Handling mixed Dirichlet and Neumann BCs, which prior works cannot; 2) A novel network structure that incorporates boundary information through spatially-varying convolutions; 3) State-of-the-art performance vs AMG, IC and other baselines on fluid simulation examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The network design that encodes boundary information through spatially-varying convolutions is novel and well-motivated. This likely contributes significantly to the method's success in addressing mixed BCs.\n\n2. Comprehensive empirical evaluation on simulation benchmarks demonstrates clear superiority over optimized baseline methods on problems. Statistical analysis provides convincing evidence of the method's benefits."
                },
                "weaknesses": {
                    "value": "1. While the spatially-varying convolutions encode boundary data effectively, their implementation as CUDA kernels is noted to be a computational bottleneck. Further optimization could yield additional speedups.\n\n2. Enforcing symmetry and positive-definiteness of the preconditioning operator was not achieved, limiting the method to a generalization of CG instead of CG itself.\n\n3. The network does not yet leverage sparsity, so may not scale gracefully to extreme sparse problems with many empty grid cells.\n\n4. Only isotropic problems were considered - extending the approach to anisotropic or nonlinear problems is an open question.\n\n5. Training data generation from Lanczos vectors is costly. Finding cheaper alternatives while maintaining accuracy could be valuable for certain applications.\n\n6. Proving theoretical convergence properties like those of standard multigrid would strengthen understanding, though difficult given the learned nature.\n\n7. Quantitative ablation of design choices like coarsening levels, memory usage, etc. could provide further insights.\n\n8. While extensive, the benchmark set considers a subset of potential problem domains - generalizing to new classes would reinforce claims."
                },
                "questions": {
                    "value": "1. Can you exploit sparsity directly in the network/algorithm to better handle extremely sparse problems that arise in practice\n\n2. Can you investigate enforcing symmetry and positive-definiteness of the preconditioner to allow use of standard CG?\n\n3. Can you explore non-Euclidean/anisotropic problem settings to broaden the method's scope?\n\n4. Can you develop cheaper training data generation techniques or consider self-supervised learning alternatives?\n\n5. Can you perform sensitivity studies on architectural hyper-parameters like coarsening levels, memory usage, etc?\n\n6. Can you attempt theoretical analysis of convergence properties to strengthen understanding of the method?\n\n7. Can you broaden empirical evaluation to new problem classes to further validate generalization abilities?\n\nI would like to improve my score if the concerns above are well addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6390/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634204769,
            "cdate": 1698634204769,
            "tmdate": 1699636708082,
            "mdate": 1699636708082,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LD2hHYoHdi",
                "forum": "H8CtXin7mZ",
                "replyto": "SDxayL6OUd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6390/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6390/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Future Work\nWe agree that all of the extensions raised here are interesting avenues for future work, and we are especially excited to see how our spatially-varying kernel architecture performs for problems with heterogeneous and anisotropic constitutive laws. However, we do not see any of these investigations as essential to the completeness and impact of our paper.\n\n## Exploitation of Sparsity\nOur systems feature sparsity of two types: sparsity of the finite difference Laplacian matrix, and sparsity of the fluid domain occupancy. Matrix sparsity is already exploited by the linear algebra operations in our NPSDO solver (and by our use of small convolution kernels in our network), while domain sparsity is what we mentioned as future work. We note that in typical fluid simulations, the domain geometry is constantly evolving and can range from highly sparse to fully dense; we anticipate inventing a single architecture that works well in both occupancy regimes to be very challenging. Furthermore, efficient algorithms and implementations for sparse convolutions--especially with variable sparsity patterns--are still an active area of research with limited support in available libraries. Even support for sparse matrix formats like CSR and CSC is still in beta stage in PyTorch.\n\n## Further Optimizations\nOne easy optimization that we have already implemented is caching the mixing weights computed by the blue arrows in Figure 1: since these depend only on the input image and not the input residual vector, they do not change after the first NPSDO iteration for a given system. Implementing this caching led to an average speedup of 10.2% across all systems in our test set. The statistics in the updated PDF we have uploaded now reflect this improvement.\n\n## Self-supervised Learning and Cheaper Training Data Generation\nOur training arguably is already self-supervised, since our loss function does not require a ground-truth solution for each linear system. As mentioned in the paper, we did pursue several less costly approaches for generating the training data that unfortunately did not work well enough. Our existing approach still could be accelerated by using a higher-performance implementation for generating the Ritz vectors, but we do not consider the current implementation prohibitively expensive, especially when compared to training time.\n\n## Hyperparameter Study\nWe will include performance statistics for networks with a range of coarsening levels $\\mathcal{L}$ trained using the strategy described in our response to the second review."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6390/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039452599,
                "cdate": 1700039452599,
                "tmdate": 1700039452599,
                "mdate": 1700039452599,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EwvnhJ9FMC",
            "forum": "H8CtXin7mZ",
            "replyto": "H8CtXin7mZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6390/Reviewer_2G4U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6390/Reviewer_2G4U"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an approach to construct preconditioner for Poisson equations based on neural networks. The architecture is to mimic the geometric multigrid. The approach has the capability to handle arbitrary shape of the fluid and mixed boundary conditions embeded in a rectangle or cube."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The neural network can effectively approximate the inverse of the stiffness matrix from the Poisson equation such that the resulting iterative solver is much faster than the existing methods."
                },
                "weaknesses": {
                    "value": "- The primary contributions of the paper, including the understanding of convolution as a smoother, pooling as the restriction, and the connection between multigrid and convolutional neural networks, have been extensively explored in the following reference:\n\n  Juncai He and Jinchao Xu. \"Mg-Net: A unified framework of multigrid and convolutional neural network.\" arXiv:1901.10415, 2019.\n\n- It appears that the comparison in the paper is limited to iterative solvers and does not account for the time required to train the preconditioner. This approach may not provide a fair assessment. It's worth noting that the setup time for AMG can be comparable to the training time. If one considers the extended matrix on the uniform grid, a fixed hierarchy can be used, and the learning task could focus on the prolongation and restriction operators for different $\\mathcal I$. The approach outlined in the following paper may offer valuable insights:\n\n  Alexandr Katrutsa, Talgat Daulbaev, Ivan Oseledets. \"Deep Multigrid: learning prolongation and restriction matrices.\" [arXiv:1711.03825](https://arxiv.org/abs/1711.03825)."
                },
                "questions": {
                    "value": "- The data generation process described on the bottom of page 6 needs further clarification. It's not entirely clear whether it involves variations in the domain geometry. If it does, it would be helpful to specify the nature of the randomness associated with the geometry. Additionally, the presentation seems to only consider two types of discretization sizes. If that's the case, it's important to explain how this approach can be applied to systems with different discretization sizes. Does the neural network need to be retrained for each new size? Furthermore, more details about the randomness involved in generating the right-hand side of the equations would be appreciated.\n- Could the authors provide an explanation of what the neural network truly learns for an iterative solver to maximize its performance? Specifically, does it focus on learning the smoother or the prolongation operator, or both? Clarity on this aspect would be valuable for understanding the role of the neural network in enhancing solver performance.\n- Is it necessary to fix the levels of coarsening in the proposed approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Reviewer_2G4U"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6390/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742404389,
            "cdate": 1698742404389,
            "tmdate": 1699636707931,
            "mdate": 1699636707931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ponH2feey8",
                "forum": "H8CtXin7mZ",
                "replyto": "EwvnhJ9FMC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6390/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6390/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Primary Contribution\nWe consider our primary contributions to be our novel network architecture and our demonstrations that it can achieve state-of-the-art performance on problems with mixed boundary conditions. We agree that past work has understood the connections between components of traditional CNN architectures and their counterparts in a multigrid hierarchy.\n\n## Comparisons Limited to Iterative Solvers\nFigure 6 compares against the popular sparse direct solver CHOLMOD.\n\n## What Does the Network Learn?\nThe network learns by optimizing the parameters defining (i) our spatially varying convolution kernel (i.e., the smoother) and (ii) the fixed convolution kernels that compute coefficients for mixing in the interpolated corrections. It does not learn restriction or prolongation operators, which are implemented by fixed pooling and upsampling blocks.\n\n## Number of Coarsening Levels\nAn instance of our network architecture is defined by its number of coarsening levels. This is a hyperparameter that is fixed to 4 in all examples. However, the weights of a network with $\\mathcal{L}$ levels can be copied to a new network with $\\mathcal{L + 1}$ levels to initialize its training, and we found this strategy beneficial for training deeper hierarchies; we will discuss this approach in the revised draft.\n\n## Data Generation Process\nThe systems comprising our training data set were collected from actual fluid simulations, featuring a diversity of fluid domain shapes."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6390/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039341044,
                "cdate": 1700039341044,
                "tmdate": 1700040332211,
                "mdate": 1700040332211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OQyFYWpQir",
                "forum": "H8CtXin7mZ",
                "replyto": "ponH2feey8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_2G4U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_2G4U"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' detailed explanation; however, I remain unconvinced about the significance of the current work, especially considering the potential for implementing an efficient geometric multigrid algorithm.\n\nThe numerical example indeed underscores the effectiveness of algebraic multigrid methods. Nevertheless, a common criticism of AMG is the setup time, as evident from the flat period in the AMG curve in Fig 4 and 5.\n\nConsidering the matrix extension to a larger one associated with a uniform grid, a fixed hierarchy of Geometric Multigrid (GMG) can be applied. The only variable is the smoother, contingent on the boundary conditions. The spatially varying kernels align precisely with the smoothers used in multigrid methods. Implementing geometric multigrid on the extended rectangular domain with Gauss-Seidel smoother can potentially yield a similar behavior. This approach eliminates the need to learn additional smoothers, as they can be determined by the modified stencil.\n\nThe strategy of embedding the irregular shape of the domain into a rectangular domain and leveraging the structured nature of the latter with a uniform grid is not a new concept in scientific computing. A relevant work in this regard is:\n\n\"A parallel fictitious domain multigrid preconditioner for the solution of Poisson\u2019s equation in complex geometries\" \n(K.M. Singh, J.J.R. Williams, Computer Methods in Applied Mechanics and Engineering, Volume 194, Issues 45\u201347, 1 November 2005, Pages 4845-4860)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6390/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593571472,
                "cdate": 1700593571472,
                "tmdate": 1700593571472,
                "mdate": 1700593571472,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OZteYJtCcU",
            "forum": "H8CtXin7mZ",
            "replyto": "H8CtXin7mZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6390/Reviewer_TVSw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6390/Reviewer_TVSw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new data-driven preconditioner for Poisson problems with mixed boundary conditions with a MAC discretization. The PDE arises from the moving boundary of a multiphase flow, thus traditional AMG is unfavorable in terms of the MG hierarchy rebuilding. The key method is built upon the work in Kaneda et al ICML 2023. The authors also tackles a difficulty through the optimizer that the DNN preconditioning approach is non-symmetric, which achieves better result than approaches such as `BLOPEX` in hypre. From the perspective of someone who worked in the business of traditional multigrid solvers/preconditioners for PDEs, the PDE discretization is nothing new, the methodology used is pretty predictable, and there are a few unclear things, however, I still think this is overall a solid work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- In the context of traditional approaches of multigrid, either solver or preconditioner, either geometric or algebraic, imposing (nonhomogeneous) Neumann BC can be challenging and is usually an ad-hoc business.\n- The study, from preconditioning pov, is quite well-motivated by introducing the ever-changing BCs for the Poisson problem through the multiphase flow, since traditional AMG has to rebuilt the AMG hierarchy each time step.\n- The phase variables as channels are neat practices, which is nicely motivated by the ever-changing phase in simulation. But references should be given (e.g., arXiv:1707.03351).\n- The spatially varying kernel (or instance-dependent kernel) is an extremely nice practice, and rightfully so motivated by the mathematical nature of the problem. This is highly connected to attention (even though it is nonlinear) applied to PDE problems (where attention is viewed as a kernel integral), please consider giving a few references in this regard.\n- I ran the code myself, and was amazed by the excellent reproducibility of it. However, the instruction of how to install `AMGCL` is problematic in `README.md` for someone with existing header files for `AMGCL`."
                },
                "weaknesses": {
                    "value": "- Personally, I am quite uncomfortable to impose Dirichlet boundary conditions on the pressure variable for the fluid problems among many formulations I have played with for NSE or Stokesian flow. While the temporal discretization of NSE presented on page 3 is a standard splitting scheme (aka \"projection method\" or \"pressure-correction scheme\"), the reference given is likely not the right one. In Chorin's 1967 JCP paper, he proposed the famous pressure marching scheme to impose divergence free condition at $(n+1)$-th time step (implied by (4) in the paper), but did not solve a PDE for pressure. I checked Temam's book as well as J. Shen (Temam's student)'s review paper on projection methods, and the scheme featured in this paper is probably attributed to Chorin's 1968 Math. Comp. paper (equation (21) therein for pressure). However, therein, only the Neumann BC is imposed for the pressure. So please give the reference, preferably with equation numbers that where does the Dirichlet part comes from.\n- The current presentation of the overall iterative procedure is unclear in section 4. For example, in Figure 1, only $\\mathbf{r}\\mapsto \\mathcal{P}^{\\text{net}}(\\mathcal{I}, \\mathbf{r})$ is shown. I suggest move A.1 to the main part.\n- The biggest weakness is perhaps that the \"CNN\" used is linear, then it is nothing but a single parametrized convolution, as stacking convolutions without nonlinearity is still a convolution just with a different kernel sizes.\n- Maybe this is up to debate, that the authors said \"AMG setting up stage takes too long\". I think this is an unfair comparison, on a fixed mesh, AMG hierarchy has to be set up only once, and for GMG it is automatic. Moreover, AMG setting up is automated on any mesh and (mostly) robust for specific PDEs that needs no training stage. Another counterpoint would be that the current approach (based on CNN) is limited to Cartesian mesh-based discretization such as MAC.\n\n### Minor things\n- I suggest the authors rewrite the abstract. As writings like \"The Poisson equation is ubiquitous in scientific computing: it governs... The most popular Poisson discretizations yield large sparse\nlinear systems\" are more appropriate in the introduction than the abstract, a better way to put things in the contexts is just saying large systems arising from PDE's discretization is hard to solve.\n- The acronym DCDM goes undefined across the paper.\n- Page 1: \"matrix-norm\" should be \"matrix norm\".\n- Page 2: \"their loss functions is...\".\n- Page 2: \"disretizations\" -> discretizations.\n- Page 5: if $P = LL^{\\top}$ is the preconditioner (assuming the new system is $PA\\mathbf{x} = P\\mathbf{b}$), then CG steps build subspaces that are $L^{\\top}A L$-orthogonal, not $L^{-1} A L^{-\\top}$-orthogonal (to converge to $L^{-1}\\mathbf{x}^*$ for $\\mathbf{x}^* = A^{-1}\\mathbf{b}$).\n- Personally, I am not in favor of referring $\\mathcal{I}$ an \"image\" in this context because one may confuse this term with the image of an operator. Of course, it is up to the authors' judgement on this."
                },
                "questions": {
                    "value": "- The targeted audience who works in the solver business would usually not know what \"PIC/FLIP blend transfer scheme\" is, I suggest elaborate it somewhere in the context of this paper, especially considering a similar acronym IC stands for incomplete Cholesky.\n- Judging by the name of Harlow 1964 Meth. Comp. Phys. paper, it is the ref for PIC? The correct origin of MAC should be Harlow and Welch's 1965 paper in The Physics of Fluids, some people like to cite this tech report as well (https://www.osti.gov/biblio/4563173).\n- Page 4: \"The PSDO algorithm can be understood as a modification of standard CG that replaces the residual with the preconditioned residual as the starting point for generating search directions and, consequently, cannot enjoy many of the simplifications baked into the traditional algorithm\". This sentence is somewhat too long in reviewer's humble opinion, and up to the authors' judgement, I suggest rewrite it a little. Meanwhile, please consider spending a few words to explain the reason why PSDO cannot achieve CG's automatic $A$-conjugate directions. It says that PSDO does \"an exact line search\", so it does not a subspace search like the CG does?\n- Page 5-6, the default behavior of upsampling using the interpolation in `nn.functional` is `nearest` which gives a piecewise constant function that is unfavorable in this context, please test `bilinear` which is MG prolongation.\n- The single-precision for NN and double for the CG is a good practice. However, it would be interesting to see whether this combination can reach relative residual `1e-8` (cf. those in Figure 5)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6390/Reviewer_TVSw"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6390/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772718691,
            "cdate": 1698772718691,
            "tmdate": 1699636707736,
            "mdate": 1699636707736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wm837JC2kP",
                "forum": "H8CtXin7mZ",
                "replyto": "OZteYJtCcU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6390/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6390/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Dirichlet Boundary Conditions for Free Surfaces\nWe note that our application is *inviscid* fluid simulation, where the omission of viscous stress means that the surface pressure simply equals the applied pressure (e.g., see the section entitled \"Boundary Conditions at Free Surface\" in [Harlow and Welch 1965], or page 15 of the textbook [Bridson 2015: Fluid Simulation for Computer Graphics]); this is naturally posed as a Dirichlet condition.\n\n## Equivalence to One Large Convolution\nIndeed, because the network output is a linear function of the input vector, it can be interpreted as a single convolution with one large spatially varying kernel (whose size depends on the number of layers) that is parametrized by our network weights. We do note that the network output is a nonlinear polynomial function of the input *image* since the coefficients of successive kernels end up multiplying each other. We see the simplicity and linearity of our network not as a weakness but as a contribution (showing a heavier-weight, nonlinear architecture is unnecessary) and a practical advantage (accelerating inference at runtime).\n\n## Bi/trilinear Prolongation\nThank you for pointing this out! Yes, trilinear interpolation is a closer analogue for MG prolongation. Simply substituting it for the default upsampling in our old trained network achieves a measurable speedup (reducing the iteration count across all linear systems in our test set by an average of 5.3%). We have updated the statistics reported in our paper to reflect these improvements. We have also begun training a new model incorporating this trilinear interpolation, which we expect to achieve further improvements.\n\n## A-conjugacy of Directions\nDespite using an exact line search as in PCG, enforcing conjugacy with just the previous search direction does not automatically ensure conjugacy with all past directions. This property enjoyed by CG hinges on symmetry, and we would be happy to provide detailed proofs in the appendix.\n\n## Tolerance Achievable\nThe accuracy of our solver is not limited by the preconditioner's single-precision arithmetic and can easily surpass a relative residual of $10^{-8}$. To test this, we disabled the convergence test and executed each solve in our entire test set for a fixed number of iterations (100). We measured a median relative residual of $4.01\\times10^{-15}$ and report a histogram in the newly added Figure 11 of the appendix."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6390/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039221074,
                "cdate": 1700039221074,
                "tmdate": 1700039221074,
                "mdate": 1700039221074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RsdfFQm8UM",
                "forum": "H8CtXin7mZ",
                "replyto": "wm837JC2kP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_TVSw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6390/Reviewer_TVSw"
                ],
                "content": {
                    "comment": {
                        "value": "1. I've checked what has been revised and studied the numerical experiments in the repo in details one more time;\n2. I've been learning two references in more details, Huang et al SISC 2023 and He-Xu Mg-net paper mentioned by Reviewer 2G4U;\n3. After learning that an inaccurate inner \"preconditioner\" bounded by the single precision can still achieve the machine epsilon in double when paired with a CG-like outer iteration, I changed my view of what are the weights are actually learning in this case;\n\nI agree with the authors that the output is nonlinear thanks to the indicator mapping being very special, and very likely building a single large convolution (two-grid) paired with restriction won't work (attributing to 3. above). However, due to 2. and 1., I will keep my initial evaluation unchanged. I think this is a solid work, but the new contribution is limited to a very specific case (but I deemed it important), also compared with AMGCL.\n\nBy the way, I am not sure if I should address this, since the authors already acknowledged that the lacking of a positive definiteness and symmetry as their approach's limitation. I found it kinda pointless to point this out in the reviews (as two other reviewer did). Nevertheless, I am kinda disappointed that the authors did not address the symmetry part. A hint here is that please consider what makes the two-grid propagation formula $\n(I-R_2 A)^{m_{\\text{post}}} (I - P(P^{\\top}AP)^{-1}P^T A ) (I-R_1 A)^{m_{\\text{pre}}}$ symmetric."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6390/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582077859,
                "cdate": 1700582077859,
                "tmdate": 1700582077859,
                "mdate": 1700582077859,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]