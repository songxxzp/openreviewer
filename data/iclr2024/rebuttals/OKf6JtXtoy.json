[
    {
        "title": "MAP IT to Visualize Representations"
    },
    {
        "review": {
            "id": "0r61usZx0U",
            "forum": "OKf6JtXtoy",
            "replyto": "OKf6JtXtoy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
            ],
            "content": {
                "summary": {
                    "value": "In the paper \"MAP IT to visualize representations\", the authors put forward a novel 2D visualisation method (MAP IT), based on t-SNE. There are two main differences between MAP IT and t-SNE: (1) MAP IT uses unnormalized affinities and replaces KL divergence with Cauchy-Schwarz (CS) divergence which is scale-invariant; (2) MAP IT applies the divergence to the \"marginal\" affinities obtained by summing up the rows of the pairwise affinity matrix. The authors claim that this results in a visualization method which is conceptually different from all t-SNE-like predecessors. The authors use several small datasets (n <= 2000) including a subset of MNIST to argue that MAP IT outperforms t-SNE and UMAP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I initially found the paper interesting, as it shows a good understanding of existing 2D visualization methods (t-SNE/UMAP) and develops what seems to be a novel alternative approach. I also agree with the authors that their MAP IT visualization of n=2000 subset of MNIST looks interesting and very different from t-SNE/UMAP."
                },
                "weaknesses": {
                    "value": "Unfortunately, in the end I was confused by the presentation and not convinced by the paper. One big issue is that the paper is hard to understand: formulas have typos, many terms are not properly defined, overall section structure is sometimes confusing, etc. Another big issue is that the authors only show results on tiny datasets and do not seem to have a working scalable implementation that would allow embedding even a modest-sized full MNIST, let alone datasets with millions of points. The third issue is complete lack of quantitative evaluations."
                },
                "questions": {
                    "value": "MAJOR ISSUES\n\n* section 3.1: Z_p and Z_q are never formally defined. While I understand that Z_q is the sum over all N^2 Cauchy kernels, I am not sure what Z_p is in t-SNE. Is it simply 2n?\n\n* MOST IMPORTANT QUESTION: The authors define marginal \\tilde p_j as sum over rows of the unnormalized t-SNE affinity matrix. They don't actually define \\tilde p_ij, but I assume that it's just p_ij * 2n. Note that in t-SNE p_{i|j} values sum to exactly 1 in each row, by construction. p_{ij} are obtained by symmetrizing p_{i|j}, but approximately they still have constant row and column sums. So when the authors define marginal p_j probabilities, to me it seems they are constant (??!) or at least near-constant. So I don't undestand how this can be useful for any dimensionality reduction algorithm. What am I missing?\n\n* Conceptual question 1. The authors put a lot of emphasis on using unnormalized affinities (and replacing KL with CS) and also on using marginal probabilities instead of pairwise probabilties. Are these two things somehow related? Could one use unnormalized affinities and CS loss with the pairwise affinities? Could one use marginal probabilities in the KL loss? Are these two independent suggestions and MAP IT just happens to implement both, or are these two suggestions somehow related and follow from each other?\n\n* Conceptual question 2. The authors put a lot of emphasis on using unnormalized affinities, but their CS loss function performs normalization within the loss function (Equation 5). This normalization leads to the N^2 repulsion term similar to t-SNE (Equation 8). To me this seems like the authors simply \"moved\" the normalization from one place (affinities) to another place (loss function), but nothing much changed compared to t-SNE. What am I missing?\n\n* The beginning of section 4 says that MAP IT uses perplexity 15. But in caption of Figure 3 and later in the text (esp. Appendix C), the authors mention k=10 value as if MAP IT uses kNN graph with k=10. How does perplexity 15 relate to k=10? This is very confusing.\n\n* Major limitation: the authors only have implementation that allows them to embed n=2000 data sets. That was fine 25 years ago, but in 2023 this is a major limitation. What is confusing to me, is that in Figure 8 the authors show that they can reduce the runtime 100x fold, and still obtain nearly the same result, but they only show it on the same n=2000 subset of MNIST. Why not run this 100x-sped-up approximation on the entire MNIST? That would be interesting to see!\n\n* Major limitation: no quantitative evaluation. E.g. for MNIST one could do kNN classification in the 2D space, and compare MAP IT with t-SNE/UMAP. One could also compute various NN preservation metrics. Would of course be much more interesting to do this on the entire MNIST and not on the n=2000 subset...\n\n\nMINOR ISSUES\n\n* page 2, formula (1): the minus sign is missing after the last equality sign\n\n* page 2, formula for p_ij in t-SNE: there should be 2n in the denominator, not just 2.\n\n* page 3, \"the role of normalization\": see Damrich et al 2023 https://openreview.net/forum?id=B8a1FcY0vi for parametric t-SNE. This paper would be important to cite in various places, including in the Appendix A\n\n* page 3: \"marginal probabilities\" appear in Definition 2 but have not been properly defined yet.\n\n* section 3.2, second line: one of the p_j = should be q_j =.\n\n* section 3.3: this section needs some introduction. By the end of section 3.2 it seems that the method is already defined. So why do you need another treatment in section 3.3? This needs more motivation.\n\n* page 14: relationship between t-SNE and Lapl. Eigenmaps was discussed in https://jmlr.org/papers/v23/21-0055.html and https://epubs.siam.org/doi/10.1137/18M1216134, it seems one should cite them here. And \"not to have been discussed much in the literature\" is not exactly right."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4991/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4991/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4991/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698527560649,
            "cdate": 1698527560649,
            "tmdate": 1700757218572,
            "mdate": 1700757218572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UIyiQw7rkg",
                "forum": "OKf6JtXtoy",
                "replyto": "0r61usZx0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Quantitative evaluation added, normalization defined, questions addressed"
                    },
                    "comment": {
                        "value": "Dear reviewer L3aD,\n\nThank you for your review! I have marked changes in **blue** in the revised manuscript.\n\n**Section 3.1, $Z_q$ and $Z_p$ not formally defined**: In order to motivate the theory in Section 3 on CS as a projective divergence, I take as a starting point $q_{ij} = \\left[ 1+||\\vec z_i-\\vec z_j||^2 \\right]^{-1} / Z_q$ where $Z_q = \\sum_{n, m} \\left[ 1+||\\vec z_n-\\vec z_m||^2 \\right]^{-1}$ is an explicit normalization factor.  Furthermore, $p_{ij} = \\exp\\left(-\\kappa_i ||\\vec x_i-\\vec x_j||^2 \\right)/ Z_p$, with normalization factor $Z_p = \\sum_{n, m} \\exp(-\\kappa_i ||\\vec x_n-\\vec x_m||^2)$. This is now made explicit on **page 2**.\n\n**Most important question**: With the above starting point and definitions, the MAP IT cost function will basically align the degrees of corresponding nodes in the target space versus the input space. However, in order to use the same way to create input space similarities for t-SNE and MAP IT, to have a similar underlying foundation, I discuss in **Appendix C, page 22** that in practise symmetrized probabilities are used such that $p_{ij} = \\frac{p_{i|j} + p_{j | i}}{2n}$ with $p_{j|i} = \\frac{\\exp\\left(-\\kappa_i ||\\vec x_i-\\vec x_j||^2  \\right)}{\\sum_{n \\neq i} \\exp(-\\kappa_i ||\\vec x_i-\\vec x_n||^2)}$ and likewise for $p_{i|j}$. The projective property with respect to $Z_q$ is still in force. You are right that this basically creates unit node degrees in the input space and thus more uniform marginal input space probabilities. This means that the target space marginal probabilites will be optimized towards a more uniform distribution too. The node degree is a quantity that relates a point to all other points it is connected by the strength (weights) on the connections. A node degree forced to be one seems to me as a means to avoid very small weights or very strong hubs and such properties should be beneficial to transfer to the target space. \n\n **Lack og quantitative evaluations**: I agree that this should have been included from the start, thanks. I included both results for knn recall (input space vs. target space) and for class-wise knn recall in the target space. You can find this on **Appendix C, page 22** in the revised manuscript. \n  \n **Conceptual question 1**: Thanks for raising these conceptual questions. The use of a projective divergence is what makes direct normalization of probabilities unnecessary, either for a cost function only based on pairwise affinities or for a cost function based on marginal affinities. Actually, in Proposition 11 I do show that MAP IT as a special case reduces to the CS divergence used with only pairwise affinities. However, your question regarding potentially having marginal probabilities for the KL loss is intriguing. In that case, the direct normalization of probabilities would be needed since the KL is not projective. I added a paragraph in **Appendix C, page 26** where I indicate that in future work, Definition 4 may be extended to a *map KL divergence*. \n \n**Conceptual question 2**: Thanks for pointing out the N^2 term. As mentioned to reviewer vQ36, the use of the CS divergence to derive MAP IT does render explicit normalization of probabilities unnecessary. But as you point out, a quadratic complexity still arises in the (entropy) weighting  and in future work the impact of these weighting factors and how they relate to each other should be further studied. I toned down the claim about MAP IT being inherently scalable in Abstract, etc, for that reason. I did perform a preliminary analysis where I computed these quantities by only using nearest neighbors of points in the computation, where the nearest neighbors are defined with respect to the input space. This is explained on **page 25** and the result is shown in **Figure 14**. This relates to the discussion in Section 3.4 **MAP IT via local neighborhoods**. \n\n**Perplexity**: The perplexity is related to the Gaussian bandwith $\\kappa_i$ parameter for computing $p_{ij}$. It is not related per se to the use of $k=10$ nearest neighbors for the computation of the attractive and repulsive forces, as discussed in Figure 3 and in Appendix C. However, I included in **Appendix C, page 22** a more detailed discussion of the definition of perplexity and the $\\kappa_i$.\n\n**Scalable implementation**: In **Appendix C, page 25** implementation details are discussed. The current code is not very efficient, and I can of course only agree that it would have been better to have a large-scale implementation. Hopefully, there is room to have the fundamentally different ideas that MAP IT represents and the current demonstrations published even if the implementation can still be improved.\n\n**MINOR issues** Thanks for these good suggestions! They have all been implemented/fixed. For instance, including a motivation in Section 3.3, discussing Damrich (2023), discussing t-SNE vs Laplacian eigenmaps in **Appendix A**."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700439442142,
                "cdate": 1700439442142,
                "tmdate": 1700439442142,
                "mdate": 1700439442142,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xUoulkS1EA",
                "forum": "OKf6JtXtoy",
                "replyto": "0r61usZx0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I am only going to respond to my main confusion, because I am afraid I still don't follow:\n\n> With the above starting point and definitions, the MAP IT cost function will basically align the degrees of corresponding nodes in the target space versus the input space.\n\nJust to clarify. The way you define p_ij on page 2 ensures that marginal p_i (row sums) will all be exactly identical (because kappa_i values are adjusted to make each row sum to 1). Is that right? (I understand that in practice you use symmetric affinities which are slightly different, but as you say this is not important.)\n\nIf so, your loss function is DATA INDEPENDENT: q_i's need to match a constant vector of identical values p_i, whatever the original dataset was!\n\nI don't understand how this can possibly result in anything useful. In fact embeddings of all datasets with the same sample size should be all identical. But clearly this is not the case in the figures in your paper. How come?\n\n> You are right that this basically creates unit node degrees in the input space and thus more uniform marginal input space probabilities. This means that the target space marginal probabilities will be optimized towards a more uniform distribution too. The node degree is a quantity that relates a point to all other points it is connected by the strength (weights) on the connections. A node degree forced to be one seems to me as a means to avoid very small weights or very strong hubs and such properties should be beneficial to transfer to the target space. \n\nHere you are arguing that the fact that \"target space marginal probabilities will be optimized towards a more uniform distribution\" is a good thing and not a bad thing. But there is no other term in the loss function! That's all there is that you are optimizing. If the entire algorithm is: \"make q_i values uniform\", then how can it work?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496602542,
                "cdate": 1700496602542,
                "tmdate": 1700496635312,
                "mdate": 1700496635312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A5kZe1rMlP",
                "forum": "OKf6JtXtoy",
                "replyto": "yBj0ozVGMB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for you reply and for further edits to the paper.\n\nUnfortunately I have to say that I am not convinced by this explanation. I don't really understand it. As formulated, your algorithm simply should not work (the loss function strives to make q_i uniform, which is useless). Obviously it does work, but it is entirely unclear why. Moreover, the algorithm does not scale beyond O(1000) points, and the numerical improvement in quantifiable benchmarks is absent (kNN recall) or not very impressive (\"class-wise kNN recall\" but only for very large $k$ values).\n\nAll in all, I cannot recommend this paper for acceptance in its current form, and unfortunately have to keep my negative score (3)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691184551,
                "cdate": 1700691184551,
                "tmdate": 1700691184551,
                "mdate": 1700691184551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9kpOQ7BLPH",
                "forum": "OKf6JtXtoy",
                "replyto": "cDNoPgD6MT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_L3aD"
                ],
                "content": {
                    "title": {
                        "value": "Remaining concerns"
                    },
                    "comment": {
                        "value": "> in the special case of using symmetrized affinities (to try to make a close comparison to t-SNE) the apparent paradox arises.\n\nBut that is what you use for all visualizations (Figures 1/3/5/6/7/8), right? So it's not really a \"special case\" but the only case that is showcased in your paper.\n\n> But as explained in Appendix C [...] the effective cost function even then never tries to aim for q_j to be uniform\n\nAs I wrote, I did not really understand this explanation unfortunately, and more generally I don't understand why the \"effective cost function\" is different from the \"stated cost function\". You talk about gradients etc., but this does not help me understand why the effective cost arises at all. To me, \"effective\" would mean that the algorithm is NOT optimizing the same loss function as stated. Is this the case?\n\nMore generally, give that basically ALL figures in the paper are based on this algorithm, this discussion should not be in Appendix C, but play a central part in the paper, IMHO."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735865437,
                "cdate": 1700735865437,
                "tmdate": 1700735865437,
                "mdate": 1700735865437,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RYiTTmpZUe",
            "forum": "OKf6JtXtoy",
            "replyto": "OKf6JtXtoy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4991/Reviewer_vQ36"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4991/Reviewer_vQ36"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a new visualization method, called MAP IT.  The\nmethod supposedly improves over other visualization methods that are\ncommonly used, such as t-SNE, UMAP, etc."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The visualizations show some features that are better highlighted\ncompared to other methods.\n\nThe method seems straightforward to optimize and does not rely on too\nmany hyperparameters."
                },
                "weaknesses": {
                    "value": "The method does not seem scalable, contrary to the authors claim.  The\nCS divergence includes a summation over all p_j and q_j, hence you\nhave quadratic complexity when optimizing.\n\nContinuing the point, there are no large-scale visualizations.  While\nthe authors claim that their approach could be scaled, I am sceptical\nof it previsely because of the definition of the CS divergence.\n\nMisc:\n\nsome citations could be added, e.g. on p. 3 (The role of\nnormalization) you could cite NCVis (Artemenkov & Panov, 2021,\nhttps://arxiv.org/abs/2001.11411); an approach that estimates the\nnorm. const. via NCE (Gutmann & Hyvarinen, 2012).  The role of the\nnormalization constant is also discussed in B\u00f6hm et al. (2022, JMLR)\nas well as Damrich et al. (2023, ICLR).\n\nTalking about the scalability of the method is fine, but there is no\nlarge-scale visualization that demonstrates that the method scales\nbeyond what other methods can esily visualize today.\n\nIn the same vein, there are no timings reported for any of the\nmethods."
                },
                "questions": {
                    "value": "Could you comment on the computational complexity?\n\nWhy did you not chosse to use commonly used optimization parameters\nfor t-SNE?  In Belkina et al. 2019 they highlight what approaches work\nwell and they considerably improve the visualization quality.  This\nwould also improve the comparison that you draw from Figure 1.\n\nWhy did you choose the delta-bar-delta method for optimization?\n\nWhy do you think current approaches are not sufficient for the\ndatasets that you show in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4991/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4991/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4991/Reviewer_vQ36"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4991/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768240636,
            "cdate": 1698768240636,
            "tmdate": 1699636486675,
            "mdate": 1699636486675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b8ATkzY3UD",
                "forum": "OKf6JtXtoy",
                "replyto": "RYiTTmpZUe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment on quadratic term, optimization parameters"
                    },
                    "comment": {
                        "value": "Dear reviewer vQ36,\n\nMany thanks for your review! Your review was very useful to me, also pointing out some literature which I hadn't cited (Artemenkov, Panov) and also making me go deeper into papers I had already cited (B\u00f6hm et al. (2022) and Damrich et al. (2023).\n\n**Quadratic complexity**: The use of the CS divergence to derive MAP IT does render explicit normalization of probabilities unnecessary. But as you point out, a quadratic complexity still arises in the (entropy) weighting of the attractive and repulsive forces and in future work the impact of these weighting factors and how they relate to each other should be further studied. I toned down the claim about MAP IT being inherently scalable in Abstract, etc, for that reason. I did perform a preliminary analysis where I computed these quantities by only using nearest neighbors of points in the computation, where the nearest neighbors are defined with respect to the input space. This is explained on **page 25** and the result is shown in **Figure 14**. This relates to the discussion in Section 3.4 **MAP IT via local neighborhoods**. This is however a question which merits further work and analysis. \n\n**Timings**: On **page 25** it is explained that the current implementation is based on a matrix operation $(\\boldsymbol{D}- \\boldsymbol{M}) \\boldsymbol{X}$  where $\\boldsymbol{M}$ encodes attractive and repulsive forces according to and where $\\boldsymbol{D}$ is the diagonal degree matrix of $\\boldsymbol{M}$. The matrix $\\boldsymbol{X}$ stores all data points as rows. I am working on a more efficient knn graph implementation, although the main idea and concept is conveyed. This is why timing is not explicitly reported. \n\n**Suggested papers**: I have included the suggested papers both in **Section 2, page 3**, in **Appendix A, page 13** and other places too.\n\n**Optimization parameters for SNE, Belkina et al. (2019)**: Thanks for pointing me to this paper. I appreciate it a lot. Since the benchmark datasets studied in the paper are quite typical of the type of datasets that state-of-the-art implementations of t-SNE, UMAP, PacMap, etc, have been developed for an optimized for with their hyperparameters, I opted for those. I have however on **page 25** included a paragraph where I discuss Belkina et al. (2019). I agree that it may be that e.g. t-SNE could perhaps benefit from some more parameter tuning. At the same time, I tried to keep MAP IT very clean, not even using early exaggeration to not end up not optimizing a true divergence, and to stay close to the basic t-SNE type of approach. MAP IT could probably be improved by optimizing hyperparameters more. \n\n**Delta-bar-delta**: I chose the delta-bar-delta since it was common in the t-SNE literature. I observed slightly faster growing of structure in the MAP IT embedding when using this approach. \n\n**Current approaches not sufficient?**: Thank you for this question, I can see why you would ask it. I am not necessarily saying that the current approaches are not sufficient. But I observe some differences for MAP IT compared to the alternatives, and I think it is because MAP IT is fundamentally different in its approach. \n\nThanks again, I appreciate your review and your efforts!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433439074,
                "cdate": 1700433439074,
                "tmdate": 1700433439074,
                "mdate": 1700433439074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9E8Nsb53Mp",
                "forum": "OKf6JtXtoy",
                "replyto": "b8ATkzY3UD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_vQ36"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_vQ36"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response.  I appreciate that the paper now includes further experiments, but I am afraid that the current state is not sufficient to recommend it for acceptance.  Looking at the additional visualizations in the appendix it is not clear in all of the datasets that MAP IT performs better than contemporary approaches.  \n\nThe knn recall experiment feels a bit weak since the recall will obviously improve and in fact it will converge to 1 when k tends to the dataset size.  On the flipside, you could report some metrics for preservation of global structure such as the correlation of distances between data points in the target and output space.  That would be one way to quantify global structure.\n\nAs it stands, I think the paper needs to be reworked/restructured substantially to add quantitative evidence that the method performs better than already established approaches, as has also been highlighted by reviewer BmiL.\n\nWhile the code does not have to be fully developed before a submission, I think there are quite a few optimization techniques that should be applicable to bring down the quadratic scaling of the algorithm to something more acceptable, such as O(n log n)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736574452,
                "cdate": 1700736574452,
                "tmdate": 1700736574452,
                "mdate": 1700736574452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Qg7CkpR98x",
            "forum": "OKf6JtXtoy",
            "replyto": "OKf6JtXtoy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4991/Reviewer_LbLf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4991/Reviewer_LbLf"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an approach to dimensionality reduction by Cauchy-Schwarz projective divergence called MAP IT. This methodology aligns discrete marginal probability distributions rather than individual data points."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper presents a new perspective on dimension reduction using projective divergence. \n* The manuscript is well written, with most equations explained clearly and easy to follow. \n* The authors demonstrate their method across various data sets, showing its ability to discover high-dimensional structures and visualize them in a lower dimension. \n* The commitment to release the code publicly after the review process is admirable."
                },
                "weaknesses": {
                    "value": "The paper presents projective divergence as a method to simplify high-dimensional data, with its main advantage being the removal of weight normalization. However, from the authors experiments, it's unclear how this could be an improvement compared to existing methods."
                },
                "questions": {
                    "value": "There are some typos in the manuscript:\n1. Sixth paragraph: normaliztion \n1. After equation 18: neigborhood \n1. After equation 19 second line: also i the\n1. After figure 3: neightbors\n$D(P||Q) = D(P\u02dc||Q\u02dc), \\forall Z_p,Z_q\\neq0$, could be better to understand over the mention of \"$Z_p$ and $Z_q$ being normalizing constants\" which implies $Z_p$ and $Z_q$ need to be some specific values."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4991/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818403257,
            "cdate": 1698818403257,
            "tmdate": 1699636486568,
            "mdate": 1699636486568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AEVvQsWubI",
                "forum": "OKf6JtXtoy",
                "replyto": "Qg7CkpR98x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Fixed typos, added explanation of normalization constants"
                    },
                    "comment": {
                        "value": "Dear reviewer LbLf,\n\nLet me first thank you for the review and for appreciating my efforts to provide a new perspective to dimension reduction. This is much appreciated. I have made several changes in the manuscript and in the experimental part to try to make the paper better. I have marked changes in **blue** in the revised manuscript.\n\n**Typos**: I thank you for pointing out the typos. I have fixed these and I hope that I have avoided adding new ones as I have worked on the revision. \n\n**Normalization constants**: I realized when reading your comment, and also when reading comments by reviewer L3aD, that I should have been more precise. I have now added definitions for the normalizing constants $Z_q$ and $Z_p$ in relation to **Eq. (1), page 2**, in **Definition 1**, just before **Eq. (7)** as well as several places in **Appendix C**.\n\nThank you again for your review!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428782169,
                "cdate": 1700428782169,
                "tmdate": 1700428782169,
                "mdate": 1700428782169,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "51cZjqGhhy",
            "forum": "OKf6JtXtoy",
            "replyto": "OKf6JtXtoy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4991/Reviewer_BmiL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4991/Reviewer_BmiL"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose MAP IT, a new algorithm for manifold learning / dimensionality reduction for visualization. They compare qualitatively against previous approaches, including UMAP and T-SNE.\nTheir approach follows t-SNE but replaces the KL divergence with the Cauchy Schwarz Divergence, which removes the need for normalization, resulting in an approach that is theoretically more scalable.\n\nFWIW, I did not have the time to check the proofs, and I don't think I am familiar enough with the literature on relating t-SNE and UMAP to properly appreciate the details in the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides a new algorithm for a common task, visualizing high dimensional datasets by projecting to two dimensions. They provide an in-depth discussion of how their approach relates to previous approaches, in terms of core ideas, computation, and results. The appendix contains helpful discussion and additional experiments, and all experiments are outlined in great detail."
                },
                "weaknesses": {
                    "value": "I have two main critiques of the paper:\n- It does not provide quantitative results. It's common in this area to report 1NN results over various datasets in the projected space. While this is a somewhat arbitrary metric, it seems to be the best there is so far, and I think it would be prudent to include it.\n- The paper claims two conceptual novelties, the consideration of neighborhoods and the lack of normalization. Neither of these seem to me as new as the paper claims. Normalization is also not required in UMAP (as the paper mentions). Considering local neighborhoods is done explicitly in the less recent LLE, and implicitly in spectral embedding and laplacian eigenmaps. I was under the impression that t-SNE also has a similar \"dual\" interpretation in terms of kernel density, but I might be mistaken.\n\nMinor:\nAt the bottom of page 1, \"normalization\" is misspelled \"normaliztion\"."
                },
                "questions": {
                    "value": "Can you explain why the kernel smoothing view doesn't apply to t-SNE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4991/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698965112306,
            "cdate": 1698965112306,
            "tmdate": 1699636486487,
            "mdate": 1699636486487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pdHDefcWmf",
                "forum": "OKf6JtXtoy",
                "replyto": "51cZjqGhhy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added quantitative results and commented on local neighborhoods and kernel smoothing"
                    },
                    "comment": {
                        "value": "Dear reviewer BmiL,\n\nI sincerely thank you for the review. This is much appreciated. I have made several changes in the manuscript and in the experimental part in order to accommodate your suggestions and comments. I have marked changes in **blue** in the revised manuscript. \n\n**Quantitative results**: Thank you very much for proposing to add quantitative results. I agree that this should have been included from the start. I included both results for knn recall (input space vs. target space) and for class-wise knn recall in the target space. You can find this on **page 22** in the revised manuscript. The quantitative results for MNIST shed light on MAP IT showing that the method is probably somewhat better at capturing cluster structure compared t-SNE and UMAP, maybe at the expense of capturing more continuous manifold-like structures. This makes intuitive sense when looking at Figure 1.\n\n**Normalization**: You are right that normalization is not required by e.g. UMAP and I tried to be a bit more specific to say that the MAP IT theory shows when normalization is not needed when a statistical divergence is the base for the dimensionality reduction. For instance, in Appendix C, concluding remark, it now reads *The role of normalization for divergences to be used as dimensionality reduction methods follows directly from the MAP IT theory.*\n\n**Local neighborhoods**: Preservation of local neighborhoods is also a key part of e.g Laplacian eigenmaps and t-SNE itself, but via pairwise similarities only. For instance, on **page 21** I included the sentence *this shows that MAP IT is able to capture information in wider local neighborhoods compared to CS t-SNE which only captures local information via pairwise affinities, a property it shares with its KL counterpart t-SNE and methods such as Laplacian eigenmaps \\citep{Belkin2003}.*\n\n**t-SNE and kernel density**: Actually, in Bunte, Haase and Villmann (2012) \"Stochastic neighbor embedding (SNE) for dimension reduction and visualization using arbitrary divergences\" a form of t-SNE kernel density variant was discussed, but this it quite different from MAP IT.\n\nHowever, I have been pondering your question \"Can you explain why kernel smoothing doesn't apply to t-SNE?\" I think that similar to the new quantity I propose, which I called the map CS divergence (Definition 4) a map KL divergence can also be possible. This would probably yield a related KL-based interpretation, but it would be in terms of the marginal probabilities and not the pairwise similarities which standard t-SNE optimizes. I added a paragraph about this on **page 26**, as a potential interesting avenue for future research.   \n\n**Thanks again!**"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427356060,
                "cdate": 1700427356060,
                "tmdate": 1700427356060,
                "mdate": 1700427356060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bu53fOlln1",
                "forum": "OKf6JtXtoy",
                "replyto": "pdHDefcWmf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_BmiL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4991/Reviewer_BmiL"
                ],
                "content": {
                    "title": {
                        "value": "Quantitative results and neighborhoods"
                    },
                    "comment": {
                        "value": "Thank you for the response and the additional experiments. Doing new experiments as part of a revision phase is definitely a major effort that I appreciate. However, I don't think the quantitative experiments that you provide are on par with experiments in other related papers; it would be great to have several datasets, not just one.\nAlso, regarding your claim that MapIT captures manifold structure as opposed to cluster structure, it would be great to substantiate the claim in a more quantitative, or at least less subjective way.\n\nFinally, I agree that Laplacian Eigenmaps and t-SNE only capture local neighborhoods implicitly via pairwise relationships, but LLE is in fact more explicit about the modeling of neighborhoods than MAP-IT. While I haven't spent enough time with your analysis to have a strong opinion on the difference between t-SNE and MAP-IT when it comes to modeling neighborhoods, I think the claim that modeling whole neighborhoods is a novel perspective in manifold learning is overstated, given the existence of LLE."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4991/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668357038,
                "cdate": 1700668357038,
                "tmdate": 1700668357038,
                "mdate": 1700668357038,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]