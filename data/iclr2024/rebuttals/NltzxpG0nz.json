[
    {
        "title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds"
    },
    {
        "review": {
            "id": "a5dk2oQNEP",
            "forum": "NltzxpG0nz",
            "replyto": "NltzxpG0nz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6659/Reviewer_mQD9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6659/Reviewer_mQD9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new end-to-end architecture for multimodal agent interacting with a simulated environment. Specifically, it proposes to fuse a visual encoder with a pre-trained large language model for visual and textual generation. The model is trained on a curated dataset of Minecraft data, and is able to achieve convincing results. Overall, the work attempts to enhance interaction the capability of text-only LLM interaction for embodied agents and the results are promising for future work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed architecture is intuitively sound. It seems to be the natural way to merge a visual encoder, an LLM, and an image generator in an end-to-end pipeline.\n\nThe paper contains some smart ways to curate a large dataset.\n\nThe author suggests to publish codes and projects for reproducibility, which is nice."
                },
                "weaknesses": {
                    "value": "The tasks are still fairly constraint in that everything is image-text or text-text pairs for evaluation. The task does not really test on the agent\u2019s ability to navigate or perform tasks in Minecraft environment directly.\n\n(FK-QA) Evaluation is still focusing on in-domain knowledge about Minecraft-related questions. There seems to have no procedures or details specifying the methods to ensure data leakage. Especially, given that the LLM is selected as a LLAMA2, and directly evaluate on data procured from the internet. There could be an additional benchmark on using LLAMA to fine-tune on the text-only datasets and to evaluate on the same sets of text-only evaluation set.\n\nTable-3 has Llama 2-7b model outperforming Llama 2-13b model. What would be a good explanation for this behaviour?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Reviewer_mQD9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6659/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698382962674,
            "cdate": 1698382962674,
            "tmdate": 1699636761867,
            "mdate": 1699636761867,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2wbAqxxNqM",
                "forum": "NltzxpG0nz",
                "replyto": "a5dk2oQNEP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer mQD9"
                    },
                    "comment": {
                        "value": "Thank you for your valuable suggestion.\n\n# 1. Does task include test on the agent's ability?\nYes, our skill planning benchmark involve the tasks performing in Minecraft environment directly, following another Minecraft game agent work: Plan4MC.\nIn this benchmark, our model is required to make an initial plan for a given task, adjust the plan as the agent encounters varied scenario: invalid skill names, redundant skills (already executed), successful skill execution, or failed skill execution. \nAfter skill planning, Steve-Eye execute each skill using its pre-trained low-level policies.\nThe experiments related to skill planning are detailed in Section 4.4 of our main paper. \nAppendix 2 provides an in-depth look at each task. \nWe have also conducted further experiments on long-horizon tasks, the results of which are presented in **Table R4** in the response to Reviewer k9Af.\nThese experiments  demonstrates the effectiveness of our Steve-Eye in performing tasks within the Minecraft environment.\n\n# 2. Questions about FK-QA Evaluation\nFirstly, the QA pairs used for FK-QA evaluation are curated by ChatGPT.\nIn order to avoid data leakage, we ensure the question prompt varies from the instruction pairs for pre-training and will not be used during pre-training.\n\nSecondly, we have carried out experiments using a text-only benchmark with the finetuned LLaMA model. \nAs anticipated, the finetuned LLaMA demonstrates lower performance compared to its original version, aligning with findings from prior research. \nHowever, given our objective to develop an agent proficient in games like Minecraft, we consider this performance decline acceptable. This trade-off is deemed worthwhile to achieve enhanced capabilities in our target domain for general-purpose game agents.\n\n\n# 3. Why does LLaMA-2-7b outperform LLaMA-2-13b in Table.3?\nIn our observations, the LLama-2-13b model does not exhibit consistent improvements over its 7b predecessor. \nThis trend is not isolated; similar outcomes are evident in other instruction-tuning practices based on LLaMA, including LLaVA [2].\nBy contrast, the LLaMA-2-70b model generally demonstrates notable enhancements when compared to both its 7b and 13b counterparts, across nearly all benchmarks.\n\n[2] Visual Instruction Tuning"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546542232,
                "cdate": 1700546542232,
                "tmdate": 1700546588080,
                "mdate": 1700546588080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MOEW0IjEJ5",
            "forum": "NltzxpG0nz",
            "replyto": "NltzxpG0nz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6659/Reviewer_k9Af"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6659/Reviewer_k9Af"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed Steve-eye, a multi-modal LLM for visual-based embodied game playing in the simulated environment. Especially, \nas previous game agent mainly obtain environment informations directly from game engine, Steve-eye want to intergrate perception system into game playing agent. To achieve this objective, this paper construct a multi-modal instruction dataset for instruction-tuning. And, the paper construct some benchmark to quantitatively evaluate the different capability of the Steve-eye."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper dedicate to a interesting and valuable problem that make game agent more human-like (use visual to sense the world).\n2. Steve-eye divided the visual perception for several ends: generation, QA, caption, skill planning, which suit the minecraft game well.\n3. Generate a large multi-modal instruction dataset for this task."
                },
                "weaknesses": {
                    "value": "1. The most important question for Steve-eye is the evaluation. Steve-eye is not evaluated on some long-horizon objectives like Voyager and GITM. With generated multi-modal instruction dataset, it is so intuitive that the model can perform well on the ENV-VC, FK-QA, SPP benchmarks as the instructions are generated for these tasks. The skill plan, visual caption or QA are actually studied on real-world images by some VLLM such as LLaVA. There is no reason to go back and evaluate VLLM on simulation environment just for such tasks.\nI think the main reason for the community to carry on research on game engine is to evaluate much more complex tasks that are not hard to attempt in real world. In this regard, I think the Steve-eye should have some evaluation manners as Voyager and GITM, to show what happens when we replace the environment information of game engine to visual perception.\n\n2. Is the model trained on such instruction dataset generalizable to new game? Such as GTAV?"
                },
                "questions": {
                    "value": "No anymore questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Reviewer_k9Af"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6659/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736286382,
            "cdate": 1698736286382,
            "tmdate": 1699636761758,
            "mdate": 1699636761758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AS87RMNsRW",
                "forum": "NltzxpG0nz",
                "replyto": "MOEW0IjEJ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer k9Af"
                    },
                    "comment": {
                        "value": "Thank you for your valuable suggestion, particularly regarding the supplementation of long-horizon tasks.\n\n# 1. Long-horizon Evaluation Objectives\n\nIn fact, our work does involve long-horizon objectives in the skill planning benchmark (e.g., tasks of ``mining cobblestones to craft stone pickaxe'', ``crafting bed'' and ``harvest cooked beef'').\nThese tasks are typically broken down into dozens of skills and may require ten thousands of steps to complete.\nMore details of these long-horizon tasks are provided in **Table 6 in Appendix.2**.\nIn addition to these tasks, we conduct experiments on 10 new long-horizon objectives.\nThese objectives are iron-based task as detailed in **Table 7 in the Appendix.2**. \nAs shown in **Table.R4** , our Steve-Eye consistently exhibits promising results across these long-horizon objectives, showing its robustness in handling complex game tasks.\nIn addition, we provide addition experiments in **Table R1, R2, R3** (in reponses to ijGQ) to demonstrate that visual perception is beneficial to text-only game engines.\n\n**Table R4**: 10 long-horizon iron-based tasks.\n| Model        | iron ingot | shears | iron bucket | iron-pickaxe | iron-axe | iron-sword | iron-shovel | tripwire hook | pressure plate | iron trapdoor |\n|--------------|------------|--------|-------------|--------------|----------|------------|-------------|---------------|----------------|---------------|\n| MineAgent    | 0.00       | 0.00   | 0.00        | 0.00         | 0.00     | 0.00       | 0.00        | 0.00          | 0.00           | 0.00          |\n| gpt-assistant| 0.20       | 0.00   | 0.00        | 0.03         | 0.00     | 0.00       | 0.03        | 0.00          | 0.03           | 0.00          |\n| ours         | 0.23       | 0.13   | 0.10        | 0.13         | 0.07     | 0.03       | 0.23        | 0.10          | 0.03           | 0.00          |\n\n\n# 2. Is the model trained on such instruction dataset generalizable to new game? Such as GTAV?\nOur Steve-Eye is not directly transferrable to other games due to each game's unique interface, especially for Minecraft known for its distinct pixel-style visuals.\nNonetheless, the overall framework of Steve-Eye, including the pre-training tasks, the data collection process, and model structure, is highly adaptable and can be tailored for new games. \nThis significantly simplifies the process of applying Steve-Eye to various open-world games. \nIn fact, we are exploring to extend this framework to other open-ended environments, such as Virtual-Home as detailed in Appendix 4.\nAs a simulated world, Virtual-Home demonstrates the possibility of seamlessly adapting our Steve-Eye to a range of real-world simulation games, including GTAV you mentioned."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545715057,
                "cdate": 1700545715057,
                "tmdate": 1700546167606,
                "mdate": 1700546167606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x5Uh9z7tUZ",
                "forum": "NltzxpG0nz",
                "replyto": "MOEW0IjEJ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_k9Af"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_k9Af"
                ],
                "content": {
                    "title": {
                        "value": "Response to author"
                    },
                    "comment": {
                        "value": "The evaluation provided by the author for skill planning doesn't yet meet the long-horizon criteria for me. As mentioned in my comment, I am looking forward to see evaluation similar to voyager and GITM [A], such as the number of items unlocked, the extent of exploration range. \n\nAgain, as the author constructs instruction-follow dataset to train the model on those skills, it is intuitive that steve-eye can do well on them. All the evaluation is still under the generated instruction-follow dataset. \nI believe further evaluation is necessary to demonstrate the agent's capability to learn game-playing strategies beyond the confines of the instruction-follow dataset. Moreover, it would be valuable to see if the agent can generalize its skills to other games, given that creating instruction-follow datasets for each game is not a feasible approach.\n\nAlthough my concerns remain unaddressed, I acknowledge the pioneering nature of this work in the field and will maintain my rating accordingly.\n\n[A]. Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658955583,
                "cdate": 1700658955583,
                "tmdate": 1700658955583,
                "mdate": 1700658955583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NAXEaZubQZ",
            "forum": "NltzxpG0nz",
            "replyto": "NltzxpG0nz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6659/Reviewer_ijGQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6659/Reviewer_ijGQ"
            ],
            "content": {
                "summary": {
                    "value": "The \"Steve-Eye\" paper introduces embodied game-playing agents with visual perception capabilities and the potential to interact intelligently in open-world environments. By integrating large language models with a visual encoder, the paper enables agents to represent their surroundings and comprehend critical knowledge in the environment in a more intuitive manner. Utilizing Minecraft as a primary evaluation platform, the model demonstrates its effectiveness in a complex and dynamic setting, showcasing its versatility and potential as a general-purpose assistant."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**1. Strong Motivation:** The paper identifies and addresses a critical challenge in the field of embodied agents, highlighting the limitations of text-only interactions and demonstrating the necessity of incorporating visual perception for a more intuitive and effective user experience.\n\n**2. Extensive Task Definitions:** The authors have meticulously defined a variety of tasks to evaluate the agent\u2019s performance, spanning across environmental visual captioning, foundational knowledge question answering, and skill prediction and planning. This comprehensive approach ensures a thorough assessment of the agent\u2019s capabilities, showcasing its versatility and adeptness in handling different aspects of open-world interaction.\n\n**3. Large and Rich Dataset:** The paper introduces an extensive open-world instruction dataset, specifically curated to train the Steve-Eye model. This dataset can be a good contribution to the field for future instruction tuning of large multimodal models."
                },
                "weaknesses": {
                    "value": "**1. In-Depth Analysis on Skill Planning Required:** The paper presents Steve-Eye, an innovative integration of visual perception with Large Language Models (LLMs) for embodied agents. However, there\u2019s a discernible lack of comprehensive discussion and analytical depth in the aspect of skill planning, particularly when compared to other game-playing agents such as \u201cMineAgent.\u201d While Steve-Eye showcases commendable performance, a more exhaustive exploration of its strategic capabilities, especially in comparison to MineAgent, is crucial. This detailed analysis could unveil the underlying reasons behind Steve-Eye\u2019s superior performance, providing readers with substantial insights and a clearer understanding of its capabilities in the embodied task of planning.\n\nOn a related note, the paper discusses two other tasks: Environmental Visual Captioning (ENV-VC) and Foundational Knowledge Question Answering (FK-QA). While these tasks are undoubtedly interesting and add value to the paper, their direct connection to the agent\u2019s in-game execution and decision-making processes is not explicitly clear. Strengthening this connection and elaborating on how these tasks intricately weave into the agent\u2019s planning and action sequences would significantly enhance the paper\u2019s overall contribution to the field.\n\n**2. Baseline Selection Could Be Improved:** The inclusion of \u201cgpt-assistant\u201d as a baseline in the performance evaluation, particularly noted in Table 5, brings about certain ambiguities. The choice of baselines is critical, and in this context, one might wonder if \u201cVoyager\u201d or other models specifically tailored for the Minecraft environment would serve as more apt comparisons. By opting for baselines that are more closely aligned with the Minecraft gaming milieu, the paper could present a more robust and convincing argument for Steve-Eye\u2019s effectiveness.\n\n**3. Clarification on Image Output Utilization Needed:** As depicted in Figure 4 and discussed in various sections of the paper, Steve-Eye has the capability to generate image outputs. However, the practical application and utility of these image outputs in the context of the tasks at hand appear somewhat nebulous. Providing readers with a clearer exposition on how these image outputs can be leveraged for specific tasks would enhance the paper\u2019s clarity and comprehensibility.\n\nMinor:\n**4. Figure 1 Requires Better Visual Clarity:** My initial interaction with Figure 1 was marked by confusion, necessitating several revisits to fully grasp its intended message. A visual element such as a horizontal separator line could significantly improve the figure\u2019s readability, distinctly delineating the unfavorable responses from the preferable ones. Currently, the figure could be misinterpreted as a continuous chat, leading to potential misunderstanding. Implementing this small yet impactful change would streamline the reader\u2019s experience, fostering a quicker and clearer comprehension of the depicted scenarios.\n\nMinor: \n5. Figure 7/8/12 in the appendix are not in the correct order."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Reviewer_ijGQ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6659/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838303805,
            "cdate": 1698838303805,
            "tmdate": 1699636761641,
            "mdate": 1699636761641,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JrXeGj35FF",
                "forum": "NltzxpG0nz",
                "replyto": "NAXEaZubQZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer ijGQ"
                    },
                    "comment": {
                        "value": "# 1. In-Depth Analysis on Skill Planning Required\n\nThank you for your valuable suggestions! \nIn this refined analysis, we take a closer look at skill planning and identify several key factors contributing to the standout performance of Steve-Eye compared to other game agents like MineAgent:\n\n* **Adaptive Skill Planning Ability**:\nMineAgent employs a multi-task policy learned through RL, which can be seen as relying on static, predefined plans to accomplish each task. \nIn contrast, Steve-Eye adopts a different approach by decomposing a task into individual executable skills, forming new skill plans based on current, real-time conditions. \nThis method endows Steve-Eye with exceptional adaptability in various gaming environments, enabling it to devise more effective and executable skill plans.\nTo validate this advantage, we conduct experiments (referred to as \"ours w/o decomp\") in **Tables R1, R2, and R3** below, which reveal a significant decline in performance without task decomposition.\nThe results indicates that integrating task decomposition is beneficial for solving complex tasks with basic skills trained via RL.\n\n* **Additional Visual Understanding Ability**:\nOur Steve-Eye directly leverages visual cues to assess its current status and respond accordingly.\nAs shown in **Table 5** in our main paper, visual cues empower Steve-Eye to outperforms text-only models like ``gpt-assistant'' even based on a much ligher-weight LLM.\nOur work also suggests a universal application of visual cues in gaming agents, moving beyond the traditional reliance on rule-based indicators for status assessment.\n\n* **More Effective In-context Reasoning Ability**: \nAs depicted in **Table 3** in our main paper, Steve-Eye outperforms the vanilla Llama-2-7b by 6.4\\% on the foundational knowledge QA benchmark.\nRemarkably, it even exceeds GPT-Turbo-3.5, highlighting its acquisition of world knowledge.\nTherefore, Steve-Eye has enhanced adaptability to environmental changes when planning skills.\n\n**Table R1**: 7 tasks involving the process of ``cutting trees to craft primary items''\n| Model      | stick | table | bowl | chest | trapdoor | sign | wooden pickaxe |\n|------------|-------|-------|------|-------|----------|------|----------------|\n| MineAgent  | 0.00  | 0.00  | 0.00 | 0.21  | 0.0      | 0.05 | 0.0            |\n| Plan4MC    | 0.30  | 0.30  | 0.47 | 0.23  | 0.37     | 0.43 | 0.53           |\n| ours w/o decomp | 0.03  | 0.03  | 0.23 | 0.13  | 0.03     | 0.07 | 0.00           |\n| ours w/o ENV-VC | 0.27  | 0.27  | 0.33 | 0.43  | 0.37     | 0.27 | 0.40           |\n| ours w/o FK-QA  | 0.37  | 0.23  | 0.40 | 0.47  | 0.27     | 0.33 | 0.37           |\n| ours       | 0.40  | 0.30  | 0.43 | 0.53  | 0.33     | 0.37 | 0.43           |\n\n\n**Table R2**: 7 tasks involving the process of ``mining cobblestones to craft advanced items''\n| Model     | furnace | stonestairs | stoneslab | cobblestonewall | lever | torch | stonepickaxe |\n|--|-|-|-|---|-------|-------|---------|\n| MineAgent | 0.00    | 0.03   | 0.00      | 0.00      | 0.00  | 0.00  | 0.00    |\n| Plan4MC   | 0.37    | 0.47   | 0.53      | 0.57      | 0.10  | 0.37  | 0.17    |\n| ours w/o decomp| 0.13    | 0.23   | 0.17      | 0.13      | 0.03  | 0.07  | 0.00    |\n| ours w/o ENV-VC| 0.23    | 0.30   | 0.33      | 0.37      | 0.27  | 0.23  | 0.23    |\n| ours w/o FK-QA | 0.33    | 0.37   | 0.43      | 0.37      | 0.23  | 0.10  | 0.17    |\n| ours      | 0.30    | 0.43   | 0.47      | 0.47      | 0.40  | 0.13  | 0.23    |\n\n**Table R3**: 10 tasks involving the process of ``interacting with mobs to harvest food and materials''\n| Model     | milkbucket | wool | beef | mutton | bed | painting | carpet | itemframe | cookedbeef | cookedmutton |\n|-|-|-|-|-|-|-|-|-|-|-|\n| MineAgent | 0.46   | 0.50 | 0.33 | 0.35   | 0.0 | 0.0   | 0.06   | 0.0       | 0.0  | 0.0    |\n| Plan4MC   | 0.83   | 0.53 | 0.43 | 0.33   | 0.17| 0.13  | 0.37   | 0.07      | 0.20 | 0.13   |\n| ours w/o decomp| 0.50   | 0.43 | 0.23 | 0.13   | 0.03| 0.03  | 0.17   | 0.03      | 0.00 | 0.03   |\n| ours w/o ENV-VC| 0.67   | 0.43 | 0.33 | 0.27   | 0.13| 0.07  | 0.27   | 0.07      | 0.23 | 0.03   |\n| ours w/o FK-QA | 0.63   | 0.53 | 0.43 | 0.37   | 0.17| 0.03  | 0.37   | 0.13      | 0.10 | 0.00   |\n| ours      | 0.73   | 0.67 | 0.47 | 0.33   | 0.23| 0.07  | 0.43   | 0.10      | 0.17 | 0.07   |\n\n\nThe results in  **Table R1, R2, R3** also validate the association between our ENV-VC, FK-QA benchmarks and the improvement of decision-making process.\nWhen the ENV-VC task is removed, and we directly utilize visual cues without further instruction tuning, there's a notable drop in performance.\nThis clearly demonstrates the critical role of ENV-VC task in enhancing the model to intepret and utilize visual cues effectively.\nSimilarly, the exclusion of the FK-QA task yields a performance decrease, emphasizing the essential contribution of these two pre-training tasks in enhancing our model's decision-making capabilities."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545322057,
                "cdate": 1700545322057,
                "tmdate": 1700545322057,
                "mdate": 1700545322057,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vK6f0xummz",
                "forum": "NltzxpG0nz",
                "replyto": "Og5YN6ufd2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_ijGQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_ijGQ"
                ],
                "content": {
                    "title": {
                        "value": "Post-Rebuttal Comment"
                    },
                    "comment": {
                        "value": "We thank the author for the response, which addresses my concern and I raise the score from 5 to 6.\nGenerally, the paper provides a dataset and model that looks valuable to the community. Will the paper also release the model checkpoint?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652904733,
                "cdate": 1700652904733,
                "tmdate": 1700652904733,
                "mdate": 1700652904733,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y4PQZ4t41f",
            "forum": "NltzxpG0nz",
            "replyto": "NltzxpG0nz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6659/Reviewer_m8nW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6659/Reviewer_m8nW"
            ],
            "content": {
                "summary": {
                    "value": "The proposed method seeks to overcome \"text-only\" problems by combining an LLM with a visual encoder, which processes both visual and textual inputs and produces multimodal responses. This model is trained on a new dataset proposed by the authors, which contains multimodal perception, a foundational knowledge base, and skill prediction & planning. The trained model can perform multimodal I/O, benefitting from the two-stage training and fine-tuning.\n\nThe effectiveness of Steve-Eye is demonstrated through three proposed new benchmarks: environmental visual captioning (ENV-VC), foundational knowledge question answering (FK-QA), and skill prediction and planning (SPP).  The results show that finetuned Steve-Eye surpasses existing LLM in Minecraft scenarios, especially in multimodal Q&A."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Included in the summary and questions section."
                },
                "weaknesses": {
                    "value": "Included in the questions section."
                },
                "questions": {
                    "value": "I'm still confused about the Multimodal Perception Instructions section. Are the responses for the instructions drawn from those processed dense captions? How exactly is the extraction done? Also, it would be clearer for readers if example instructions, responses, and snapshots were provided together, as the current format requires flipping back and forth between the text and the appendix to understand these benchmarks.\n\nFor foundational knowledge question answering, evaluation relies on ChatGPT, which may be hard to reproduce and could incur high costs. Moreover, I doubt about the evaluation performance. Given that Steve-Eye-13b is fine-tuned on a vast amount of domain data, it should theoretically perform much better than Llama. Could the limited performance boost be due to the evaluation method or insufficient training on Wikipedia data?\n\nRegarding skill prediction, it seems that rule-based analysis is necessary since simply combining two random frames likely won't give enough information for skill assessment, even with human expertise. Yet, this kind of pretraining appears to somewhat improve performance, according to results from the authors. However, I believe a different approach, like training the model to predict the next move from videos and instructions, might be more beneficial, which requires a stronger capability to process sequences of frames or videos.\n\nIn summary, the proposed Steve model shows promise in multimodal Q&A for Minecraft, but it's not yet a fully embodied agent. Future work could improve its in-context Q&A and control capabilities, which would allow for understanding temporal context and environmental feedback-based control.\n\nThough there's potential to refine the evaluations and finetuning tasks, the authors do contribute massive datasets and pretrained multimodal large models to the community, which they plan to make publicly available. Hence, I am giving a positive review."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6659/Reviewer_m8nW"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6659/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841576943,
            "cdate": 1698841576943,
            "tmdate": 1699636761521,
            "mdate": 1699636761521,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bfGinvrjF9",
                "forum": "NltzxpG0nz",
                "replyto": "Y4PQZ4t41f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6659/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6659/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responds to Reviewer m8nW"
                    },
                    "comment": {
                        "value": "# 1. Details of Multimodal Perception Instructions \n\nThank you for your suggestions!\nOur generated responses to instructions can be divided into two segments:\n* (1) Responses directly derived from processed dense captions. \nTo foster data diversity, we instruct ChatGPT to prompt a variety of description templates as shown in Appendix 1.1. \nEach caption is then rephrased by randomly selecting one of these templates, ensuring a broad spectrum of expressions.\n* (2) Responses to questions about the caption. \nWe use ChatGPT to curate a set of question formats and collect QA pairs accordingly. \nThe model is required to accurately respond based on the selected question and the visual content of the caption.\n\nIn addition, due to the space limit, we cannot provide all examples, responses, and snapshots together in the main text. We will make the paper more readable in the next version. \n\n# 2. Questions about Foundational Knowledge Question Answering\nFristly, to ensure reproducibility, we will release our 1,000 QA evaluation benchmark. \nThis evaluation process is cost-efficient, as the usage of gpt-turbo-3.5 charges only $0.0015$ per 1,000 tokens. \nConsequently, the entire evaluation costs less than $1$ dollar.\n\nSecondly, it's important to acknowledge the existence of noises within our Wiki data, particularly those extracted from Wiki Tables. \nAs you can see in https://minecraft.fandom.com/wiki, Wiki data is highly structured, which inevitably causes data noises even with careful processing.\nWe attribute the limited performance boost to such data noises.\nTo address this issue, we are dedicating more resources to refine the collected Wiki data for a better-quality version. \nGiven that such an endeavor has not been previously undertaken, we believe that offering this knowledge-centric resource will significantly benefit the community. \n\n# 3. Questions about Skill Prediction and Future Work\nInsightful suggestion!\n\nFirstly, regarding the use of rule-based analysis, it's not directly employed in our skill prediction evaluation but plays an important role during the pre-training of our ENV-VC task. \nThe ENV-VC task is tailored to enable our model to interpret the current status primarily via visual cues. \nWe acknowledge that our skill prediction lacks explicit rule-based information.\nHowever, we believe relying solely on visual cues for predictions is more in line with human's decision-making process compared to a dependency on rule-based analysis, since there won't be explicit rules in our real life.\n\nSecondly, our initial attempt to predict moves from videos and instructions has proven to be a substantial challenge and did not yield satisfactory results before. \nAs a result, currently our Steve-Eye assesses only the success or failure of skill execution at each step, in order that Steve-Eye can decide whether to re-plan.\nIt does not engage in predicting low-level moves.\nInstead, we implement low-level moves by additionally training an RL-based policy for each involved skill.\nNevertheless, we agree that directly predicting low-level moves based on visual cues could be more advantageous for creating realistic embodied agents. \nThus, we are committed to exploring this direction in our future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544209245,
                "cdate": 1700544209245,
                "tmdate": 1700544209245,
                "mdate": 1700544209245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "44LDw9nVEy",
                "forum": "NltzxpG0nz",
                "replyto": "Y4PQZ4t41f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_m8nW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6659/Reviewer_m8nW"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors' rebuttal"
                    },
                    "comment": {
                        "value": "It's good to know that the usage of gpt-turbo-3.5 charges only per 1,000 tokens. However, since gpt-turbo-3.5 is a commercial API, it's not sure whether it will always be available for the academic community to reproduce the evaluation. I still think an alternative evaluation which can be done offline or independent from third-party service is better for reproduction and follow-up works.\n\nThanks to the authors for the reply regarding the questions I raised. Overall it somehow solves my other doubts about the manuscript. I have no further questions at this point."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6659/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657946963,
                "cdate": 1700657946963,
                "tmdate": 1700657946963,
                "mdate": 1700657946963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]