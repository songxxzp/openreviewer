[
    {
        "title": "Cooperative Minibatching in Graph Neural Networks"
    },
    {
        "review": {
            "id": "ycSa1YR5cz",
            "forum": "ASppt1L3hx",
            "replyto": "ASppt1L3hx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4367/Reviewer_LvyT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4367/Reviewer_LvyT"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a cooperative mini-batching design that utilizes the overlap of sampled subgraphs. The propose approach claims to optimize the communication during GNN training."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Optimizing the communication during GNN training is an important topic."
                },
                "weaknesses": {
                    "value": "\u2014The writing is informal and not well organized, which is reflected but not limited to the following aspects: 1) ambiguous terminology and inconsistent word choice, e.g., \"work\" (not clear what this really means); 2) overly extensive background (not sure how most content in the background is related to the target problem); 3) causal usage of theorem.\n\n\u2014The theoretical result states that the expected work increases as batch size increases, which clearly depends on the setting. For example, what if, in the extreme case,  the communication speed is infinite? What if I am using different GNN models? The theoretical result does not provide any specification with this regard and it is not clear how one can prove such a result.  In addition, it is not clear how the theoretical results motivate or connect with the proposed approach.\n\n\u2014No discussion on related works, despite there exists an extensive line of research from both algorithm and system communities that try to optimize distributed/large-scale GNN training. For example, I found the idea is closely related to layer sampling that is commonly used in GNN, and the communication optimization proposed in [1].\n\n[1] Gandhi, Swapnil, and Anand Padmanabha Iyer. \"P3: Distributed deep graph learning at scale.\" 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2021."
                },
                "questions": {
                    "value": "The theoretical result states that the expected work increases as batch size increases, which clearly depends on the setting. For example, what if, in the extreme case,  the communication speed is infinite? What if I am using different GNN models? The theoretical result does not provide any specification with this regard and it is not clear how one can prove such a result.  In addition, it is not clear how the theoretical results motivate or connect with the proposed approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Reviewer_LvyT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698587852353,
            "cdate": 1698587852353,
            "tmdate": 1699636409288,
            "mdate": 1699636409288,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZFBxMvmABF",
                "forum": "ASppt1L3hx",
                "replyto": "ycSa1YR5cz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4367/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your feedback. Let us clarify some of the weaknesses pointed out and reply to your questions below.\n\nWeakness 1: We will work on increasing the formality of our paper to make it read better. We will also consider moving some of the background into the appendix to make more space for other content in the final revision.\nTo reply to your question, the \u201cwork\u201d refers to the \u201ccomputation\" that needs to be carried out for sequential execution, as it is usually referred to in computational complexity.\nHence, in this context, for a given minibatch size, it is a quantity that monotonically increases as the number of sampled vertices and edges increases, and the details of this dependency are characterized by the underlying GNN model.\nAppendix A.4 and Table 5 go more into detail about what we mean by work.\nHowever, simply looking at how many vertices and edges will be sampled for different batch sizes is a good enough approximation to reason about the final runtime for any GNN model.\nWe know that there will be redundant edges for independent minibatching meaning it will be slower in theory.\n\n\nWeakness 2 and Questions: Appendix A.4 along with Table 5 characterizes the work more formally. There, we take into account the GNN model complexity and the different bandwidths available in a\ncomputer system (PCI-e, inter-GPU bandwidth, GPU memory bandwidth).\nTo reply to your question, if communication speed (inter-GPU bandwidth) is infinite, then cooperative minibatching will be guaranteed to be faster than independent minibatching. Given $P$ PEs, the\nmain complexity difference comes from the fact that Independent Minibatching has $\\frac{B}{P}$ as the local batch size for each PE but Cooperative Minibatching has $B$ as the\nglobal batch size, which PEs cooperate to process together (note that the global batch size is $B$ for both methods).\nIf $W$ denotes the work for a given minibatch size, we know that $P W(\\frac{B}{P}) \\geq W(B)$ due to the theorems\nwe prove in our paper. The paragraph right after Table 3 in our paper already makes the connection between the empirical curves in Figure 2 and the observed runtime\nresults in Tables 2 and 3. We will make the necessary modifications to our paper to make this more clear.\n\n\nFurthermore, our proposed approaches are agnostic to the underlying GNN model, as only the layer outputs in between the layers need to be communicated when using Cooperative Minibatching,\nbecause every GNN model has vertex embeddings as the input to the layer and output of the layer. The GCN and R-GCN models we use are the computationally least expensive GNN models\nfor homogenous and heterogenous graphs. If we were to use GAT or R-GAT models in our experimental evaluation, because these models are computationally slower, the communication runtime\nwould be even less significant increasing the advantage of Cooperative Minibatching compared to Independent.\n\n\nWeakness 3: Note that we already cite [1] in our paper. There, they propose to fetch different channels of vertex features from other PEs (We always partition across vertices),\nwhile duplicating the mini-batch graph structure on each processing element for the very first layer (or last layer with the sampling-focused notation in our paper), performing GNN aggregation operation in an intra-layer model parallelism fashion for only a single layer.\n\n\nNote that, the approach used in [1] is not as easily generalizable to different GNN models such as Graph Attention Networks, which they point out require special handling.\nOn the contrary, our proposed Cooperative Minibatching approach is agnostic to the GNN model and requires no special handling.\nIn the same first layer, [1] switches to data parallelism (Independent Minibatching) as can be seen in Figure 5 in [1].\nIn our work, we propose to process all stages of GNN training including graph sampling, feature loading, and all layers of forward/backward\nstages in a cooperative manner as the redundancy argument applies at all stages of GNN training. Moreover, it is possible to even combine the intra-layer parallelism approach in \n[1] with our proposed approach.\n\n\n[1] Gandhi, Swapnil, and Anand Padmanabha Iyer. \"P3: Distributed deep graph learning at scale.\" 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078113345,
                "cdate": 1700078113345,
                "tmdate": 1700713125326,
                "mdate": 1700713125326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gttVV2IoEx",
            "forum": "ASppt1L3hx",
            "replyto": "ASppt1L3hx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4367/Reviewer_m5Sh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4367/Reviewer_m5Sh"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of redundant data access and computation across processing elements (PEs) in the context of independent minibatch Graph Neural Network (GNN) training. The authors conduct both empirical and theoretical investigation for the monotonicity of work size and the concavity of the expected subgraph size needed for GNN training, with respect to the batch size. Based on these two properties, the author introduces cooperative and dependent mini-batching methodologies. These strategies are designed to minimize computational redundancy and optimize temporal locality for data access. The evaluations on several datasets with different system setups show that the proposed techniques can speedup multi-GPU GNN training by up to 64%."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is well-written and addresses a practical issue in minibatch GNN training.\n+ The proposed methodologies are strongly motivated from both empirical and theoretical perspectives.\n+ The paper demonstrates significant performance improvement with minimal overhead. The appendix serves as a good supplement to the main content."
                },
                "weaknesses": {
                    "value": "- The paper lacks sufficient evaluation to demonstrate the generalizabiliy and scalability of the proposed technique.\n- The paper omits some relevant citations for related work that should be discussed and compared.\n- There are some points need further clarifications."
                },
                "questions": {
                    "value": "1. The paper only evaluates the proposed method on one GNN model. How generalizable is this method to other types of GNNs, especially those deep GNNs with tens of layers?\n2. The parameter (\\kappa) for batch dependency is set as 256, however, Figure 4 indicates minimal difference between 64, 256, and even infinity. Furthermore, Figure 3 shows that the GNN model validation F1-score drops when \\kappa is 256 (or larger). Given these observations, how do you justify the choice of \\kappa as 256 for the evaluation instead of 64?\n3. Could you clarify the unit of measurement for the cache size? Is it quantified in bytes or in terms of the number of vertices/features? What are the key factors for determining the optimal cache size for different datasets, GNN models, and the hardware platforms?\n4. What are the communication cost with and without applying the proposed technique?\n5. The work in [1] also solves the redundancy issue across the PEs using cooperative training. However, this related work is not cited or compared in this paper.\n6. It might be beneficial to include more illustrative figures, especially, for the algorithm 1. This would help readers to follow the steps of the proposed method more easily.\n\n[1]. GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Reviewer_m5Sh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779697899,
            "cdate": 1698779697899,
            "tmdate": 1699636409172,
            "mdate": 1699636409172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pGZGMIoQ5U",
                "forum": "ASppt1L3hx",
                "replyto": "gttVV2IoEx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4367/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your feedback. Let us reply to your questions below:\n\n1. In our experiments, we used GCN and R-GCN, which are the simplest and fastest GNN models for homograph datasets (a single edge type), and heterographs (multiple edge types).\nOur proposed methods are agnostic to the underlying GNN model. The more complicated and higher complexity the GNN model is, the more benefit we will see from\nCooperative Minibatching as the reduction in work due to redundant computations will outweigh the cost of the communication even further. Moreover, the more layers there are, the more overlap\nthere will be between multiple PEs, meaning that Cooperative Minibatching will be even more beneficial. We chose 3 layers as a realistic number that people use in industrial \ndeployments of GNNs. Our proposed methods will apply to any GNNs that utilize minibatch training\nwith graph sampling (Cooperative and Dependent) or with full neighborhoods (Cooperative only).\n\n\n2. As you have already pointed out, Figure 4b shows that the cache miss rate difference between 64 and 256 is almost indistinguishable.\nSo, we could have very well chosen $\\kappa=64$, and our runtime results would have ended up being the same.\nThe lower $\\kappa$ is, the less the chance that it will negatively affect convergence. We agree that using $\\kappa=64$ is a good option as well.\nA good rule of thumb would be to use a $\\kappa$ value that gives almost the same cache miss rate as $\\kappa=\\infty$, and the smallest such $\\kappa$ should be chosen.\n\n\n3. Table 1 reports the cache sizes, and its unit is in the number of vertices whose features are cached. Thus, the size of the cache would be cache-size times \\#feats times 4 bytes\nfor the float32 datatype. The optimal cache size is the largest size you can pick without running out of memory on your system and PEs. We ran our code in different scenarios with\ndifferent-sized datasets. For example, we could have cached all of the Reddit dataset but then we would not have been able to demonstrate the benefits of dependent minibatching on that dataset\nas the cache miss rate would have been 0\\% for all $\\kappa$ values.\n\n\n4. The communication costs are provided in Appendix A.4. In particular, Table 5 gives the complexities of different GNN stages taking into account the PCI-e bandwidth, the\nNVLink bandwidth and the GPU memory bandwidth along with the number of sampled edges and vertices in each layer while also taking into account the underlying GNN model computational complexity.\n\n\n5. Thank you for pointing us to a concurrent work that we were not aware of. Indeed the split parallelism and cooperative training method proposed in [1] are very similar to our Cooperative Minibatching method.\nThe differences we see are that while we propose cooperative minibatching for all stages of GNN training (sampling, feature loading, and forward/backward),\n[1] proposes it only for feature loading and forward/backward stages. Furthermore, they are not proving the theorems we prove in our paper and they are not proposing the\ndependent minibatching approach. We have cited this work as concurrent work in our rebuttal revision. However, the fact that similar approaches are showing up in the literature\nprovides further proof that our proposed methods are credible and useful.\n\nWe started this work in June 2022, the first version of our code has been publicly available since August 2022 and our first submission of this paper was in January 2023,\nwhich we can prove after the double-blind review period.\n\n\n6. We are working on more illustrative figures for the camera-ready revision of our paper.\n\n\n[1]. GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078352551,
                "cdate": 1700078352551,
                "tmdate": 1700504665797,
                "mdate": 1700504665797,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uSFLfZlKBD",
                "forum": "ASppt1L3hx",
                "replyto": "gttVV2IoEx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "New figure added"
                    },
                    "comment": {
                        "value": "We added Figure 6 to Appendix A.4 to illustrate the difference between Independent and Cooperative Minibatching methods and accompany algorithm 1 as per your suggestion. We will continue to work on reorganizing our paper for the camera-ready version to see if we can swap some of the more relevant content from the Appendix with content from the main paper according to your and the other reviewers' suggestions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360235373,
                "cdate": 1700360235373,
                "tmdate": 1700360893652,
                "mdate": 1700360893652,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2oweVBDE0x",
            "forum": "ASppt1L3hx",
            "replyto": "ASppt1L3hx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4367/Reviewer_GNH8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4367/Reviewer_GNH8"
            ],
            "content": {
                "summary": {
                    "value": "The paper identifies an issue with standard mini-batching approaches for training graph neural networks: that redundant computations are performed by processors due to them sharing edges and vertices. A new method, cooperative mini-batching, is proposed, in which processors jointly process a single global mini-batch, exchanging information as needed to avoid redundant computation. Independent versus cooperative mini-batching is then compared experimentally."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper identifies an important difference between mini-batching in standard DNN training and in GNNs, which I had not before observed, and proposes a method to avoid inefficiencies in independent mini-batch training in GNNs. This has the potential to positively impact the training of GNNs.\n2. There is both theoretical and experimental justification for this method.\n3. The paper includes extensive experiments to support its points."
                },
                "weaknesses": {
                    "value": "1. The paper seems to be missing a discussion of the communication costs of its method, which seem like they could be significant, especially in a multi-node setup.. This is in contrast to independent mini-batching, which has the nice property of avoiding communication. The appendix (A3) notes communication overhead is not an issue, but this could be measured in detail, and in any case only considers a small-scale, on-node scenario.\n2. How is the 1D partitioning done, in detail? Appendix A3 notes that using METIS was beneficial, so why not always use this (or a similar graph partitioning algorithm)?\n3. The paper does not discuss the memory overhead of cooperative mini-batching. It seems to me that all samples in a K-hop neighborhood of each vertex need to be present for the method, which seems like it would result in significant memory overheads, especially as the number of layers increases (the paper only considers a network with three layers).\n4. The paper seems to be only considering neighborhood sampling as a way of performing independent mini-batching. However, there are other ways to do this, e.g., graph cut algorithms. (See, for example, Rozemberczki et al., \"Little Ball of Fur: A Python Library for Graph Sampling\", CIKM 2020.)"
                },
                "questions": {
                    "value": "1. What are the communication costs of cooperative mini-batching? How does the method perform in cross-node scenarios?\n2. How is partitioning done? Why not always use a method that reduces cross-device edges?\n3. How does cooperative mini-batching scale with the depth of the network?\n\n-----\n\nI thank the authors for their clarifications, and have slightly raised my score accordingly. I would, however, echo the concerns of reviewer LvyT regarding the text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4367/Reviewer_GNH8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699064975799,
            "cdate": 1699064975799,
            "tmdate": 1700672675427,
            "mdate": 1700672675427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UMUPXPDILr",
                "forum": "ASppt1L3hx",
                "replyto": "2oweVBDE0x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4367/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your feedback on our paper. Let us first address some of the weaknesses you pointed out and then provide answers to your questions.\n\nWeakness 1 and Question 1: Appendix A.4 and Table 5 go into more detail about the complexities of Independent vs Cooperative Minibatching.\nThere, the complexity is characterized by the number of sampled edges and vertices in each layer (tied directly to the theorems and empirical figures in our paper).\nWe also talk about when cooperative minibatching is a better choice than independent minibatching taking into account the available bandwidths of the computer system.\n\nBut to summarize, the important thing is the GNN model's computational complexity and relative speed of\ncommunication in the computer system. In our experiments, we use two different \nGNN models, GCN, and R-GCN, however, our approach works with any GNN model as it only requires routing the layer outputs\nin between each layer. GCN and R-GCN are computationally\nfaster models compared to for example GAT and R-GAT models (Graph Attention Networks) due to their use of the \ncomputationally more expensive SDDMM kernels (GCN uses SPMM only). Thus, our choice of GCN and R-GCN models is\nthe worst-case scenario for Cooperative Minibatching. In our experimental evaluation, we selected single-node\nmulti-GPU systems as these systems have NVLink providing relatively fast communication, the bandwidths are\nprovided in Table 2, $\\gamma$ and $\\alpha$ give us the memory bandwidth of the GPU and the communication bandwidth\nacross GPUs respectively, which are used in Appendix A.4 to characterize the complexities of communication and computation\nof each GNN training stage.\n\nNew computer systems have increasingly more efficient communication. The new hardware on the market by NVIDIA\nprovides NVLink connections that work in the multi-node setting for up to 256 GPUs [1] providing intra-node-like bandwidth\nacross nodes. However, we do not have access to such systems. As our method strongly depends on the existence of\na high bandwidth communication between PEs, it will not bring benefits in any arbitrary setting. In our case, NVLink bandwidth\nis $\\alpha=600$GB/s vs $\\gamma=2$TB/s of the GPU global memory bandwidth for the 8 GPU DGX-A100 system in Table 2.\nNote that multi-GPU systems by AMD and Intel have equivalent high-bandwidth intra-node GPU interconnects.\n\nWeakness 2 and Question 2: We randomly permute the vertices of the original graph, then logically assign equal-sized\ncontiguous ranges of vertices to each PE in our main experiments. Even with such naive partitioning, we show\nfavorable runtime results. The problem with graph partitioning is that it is hard to ensure load balance for the GNN setting.\nEven if you partition the graph while load-balancing the number of vertices or edges in each partition, GNN\ncomputation specifically requires the L-hop neighborhoods of training vertices to be load-balanced.\n\nOur results in Table 6 show the load\nbalance issues with METIS partitioning, even though the runtime on the papers dataset decreases from 13ms to 12ms, we also see that the\nruntime goes from 183ms to 185ms on the mag dataset with partitioning. This is due to the fact that on papers, the faster GCN model is used, where\nthe communication overhead is relatively higher, so reducing communication improves runtime. However, for the R-GCN model used for the mag dataset,\nwe see a slowdown, because the computation runtime is dominant, and ensuring load balance takes priority over reducing communication.\nWe note that our experimentation with graph partitioning is \npreliminary and follow-up research is required to ensure that Cooperative Minibatching can be made even faster\nthan it already is, which we will leave as future work. Our goal in this paper is to show that Cooperative Minibatching\nis a viable alternative to Independent Minibatching and to motivate further research on it.\n\nWeakness 3 and Question 3: The only memory overhead of cooperative minibatching compared to independent minibatching is the allocated\nbuffers for the all-to-all communication calls. Otherwise, because there are no duplicate vertices or edges\non the PEs in Cooperative Minibatching, Independent Minibatching has higher memory overhead, which is \nexacerbated when the number of GPUs is increased. Moreover, as the number of layers is increased, the overlap\nbetween the 4-hop neighborhoods will be even greater than the overlap for 3-hop neighborhoods. Thus, if there were 4 layers,\nthe 4-layer equivalent of Table 2 would show even more favorable results for Cooperative Minibatching. We do not readily\nhave access to the computer systems we used in our experiments so we can not easily provide you with the empirical verification.\n\n\n[1] https://www.nvidia.com/en-us/data-center/nvlink/"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700078648015,
                "cdate": 1700078648015,
                "tmdate": 1700154721898,
                "mdate": 1700154721898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]