[
    {
        "title": "Out of the Ordinary: Spectrally Adapting Regression for Covariate Shift"
    },
    {
        "review": {
            "id": "0flBwcNw4p",
            "forum": "AnuHbhwv9Q",
            "replyto": "AnuHbhwv9Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of regression generalization to out of sample distributions. This is an important problem as many models suffer from poor performance when applied to different samples. The authors focus on the case of linear regression with fixed covariate shift and derive a clean decomposition of the out of sample error in terms of the model primitives. Through the decomposition the authors identify a cause of generalization error which they coin Spectral Inflation. Spectral Inflation occurs when the training set and evaluation set are misaligned in the sense that the dimensions along which most of the variation is explained do not coincide. \n\nThe paper proposes a novel post-processing algorithm that projects the OLS solution to a subspace that is well aligned with the evaluation set. The authors offer theoretical guarantees on how to choose the projection subspace given the data and researcher chosen hyper-parameters. Finally, they show the performance of the proposed method in a simulation exercise and across various empirical applications.\n\nOverall the paper is very well written and I enjoyed reading it a lot."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper tackles and important problem by considering a clean and simple setting. I found this very useful as it helps highlight the crux of the problem and offer an intuitive solution. Furthermore, the paper is well written, the exposition is good and the theoretical results are clean and technically correct.\n\n* In terms of relevance, the decomposition results are not very surprising, but they do offer a straightforward and intuitive way to think of the generalization error in the context of linear regression. Researchers may find them useful as a framework for OOD error in these settings.\n\n* The algorithm proposed is intuitive and the theory developed offers an interesting way of thinking why it is a good algorithm under different potential underlying models. \n\n* The simulations and empirical examples are careful and offer a wide range of settings in which the method performs well."
                },
                "weaknesses": {
                    "value": "* While the theoretical results are clean and correct, I wonder if the authors have solved the problem they set to solve rather than just offer one solution that is weakly better than OLS. I expand on this in the questions.\n\n* While the proposed method performs well through the simulations and empirical tests it is unclear if it is better than other methods. For example, in Table 2 and 3 accounting for the errors it does not seem to be statistically different from the other methods (for example ERM in Table 2). The authors also do not compare it in simulations with alternative methods besides simple OLS. For instance, it seems that PCR might perform well in this setting."
                },
                "questions": {
                    "value": "* In the decomposition results, what is the expectation being taken over? I thought that X and Z are treated as random, but in the proof in page 13 in the appendix it seems that X and Z are fixed. Are X and Z random or fixed? Are the decomposition results conditional on X and Z?\n\n* Theorem 3 states that S^* is weakly better than the OLS solution, not that it is the best solution amongst all possible S. However in the paragraph above it is stated that it is the ideal set. Is it the case that S^* is the projection set that minimizes the expected loss amongst all possible S sets? It may be trivial, but it would be worth it fully explaining this in the main body of the text. If S^* is the the projection set that minimizes the expected loss then it should be stated as a theorem, if not then you should explain why you focus on S^* rather than the minimizing S. \n\n* Can you use the plug in variance estimator to estimate the variance on the same data? Why is a sample split not necessary? (this could be trivial given the assumptions) \n\n* How do you choose the hyperparameter alpha? Is there a data driven way to choose it or an optimal way of choosing it? \n\n* It would be useful to decompose the MSE into bias and variance in the simulations to check that Spar indeed is trading off bias and variance as described by the theory in relation to OLS. \n\n* What would change if Z was noisy? If we assume conditionally independent errors like for X, the conditional expectation would still be the same so it seems that most of the theory would go through with little changes (and the additional Z variance)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT",
                        "ICLR.cc/2024/Conference/Submission8365/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698540333370,
            "cdate": 1698540333370,
            "tmdate": 1700665274123,
            "mdate": 1700665274123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kzWUvNVKfe",
                "forum": "AnuHbhwv9Q",
                "replyto": "0flBwcNw4p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FNwT"
                    },
                    "comment": {
                        "value": "We would first like to thank you for helping to improve our work. In our general response to all reviewers, we respond to some of your concerns about the size of the error bars on our Povertymap experiments, and how the source of this variance is not SpAR.\n\n**On the stochasticity of variables**: In our work, we only assume that the label noise on the labels Y is random. X and Z are assumed to be fixed matrices. \n\n**About the label noise variance estimator**: The estimator for the variance of the label noise can indeed be used without a data split. We use the estimator in Section 11.2.3.6 of Kevin Murphy\u2019s Probabilistic Machine Learning (Murphy, Kevin P. Probabilistic machine learning: an introduction. MIT press, 2022). This is defined as simply the average of the residuals of the OLS solution applied to the training data.\n\n**Selection of $\\alpha$ hyperparameter**: We thoroughly discuss the problem of choosing alpha in Section L of the appendix, as well as the sensitivity of performance to this hyperparameter.\n\n**About adding noise to the test labels**: If we were to add noise to the labels for Z that was independent of the noise on the X labels and had zero mean, in expectation this would simply introduce an irreducible loss term that was dependent on the variance of the Z label noise. Since this additive term is not influenced by any parameters we control (i.e. our choice of regressor), we omit it from our calculations for clarity\u2019s sake. This is an important distinction to make, however, as some of the loss we observe in real world experiments could be due to label noise on the test examples.\n\n**On the strength of Theorem 3**: You are indeed correct that $S^*$ is the optimal set amongst all projection sets $S$ for decreasing the expected loss. This can be shown in a stronger version of theorem 3. While the projected regressor associated with $S^*$ having lower loss than the OLS solution is indeed a consequence of $S^*$ being optimal (take S equal to the empty set), it is perhaps the most important of these results as we are attempting to construct a regressor superior to OLS."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318492022,
                "cdate": 1700318492022,
                "tmdate": 1700318492022,
                "mdate": 1700318492022,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a96X57S4KB",
                "forum": "AnuHbhwv9Q",
                "replyto": "kzWUvNVKfe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses to my comments. \n\nGiven your response to my question about Theorem 3, I think from a theoretical stand point it would be nice to either state the stronger version of the theorem in the text in Theorem form (and then have Theorem 3 as a corollary), or in the appendix and add more explanation in the main text. The issue with your current formulation is that there are other methods that also dominate OLS in this setting, so if you have a stronger theorem you should state it clearly.\n\nI am satisfied with your other answers, but I am not confident about whether your empirical applications show that your proposed method truly dominates ERM and, potentially, other methods. I am not completely familiar with the empirical benchmarks in the  literature, therefore if other referees agree with your responses regarding this I would be happy to increase my score to 8."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494057790,
                "cdate": 1700494057790,
                "tmdate": 1700494057790,
                "mdate": 1700494057790,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mkEt5NcoPx",
                "forum": "AnuHbhwv9Q",
                "replyto": "IQA9jgCclO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_FNwT"
                ],
                "content": {
                    "comment": {
                        "value": "I think the findings regarding the PCR vs SpAR simulations are intuitive and in some sense SpAR is an automatic method to do PCR, which means it should have this double-robustness property. \n\nAfter some consideration I have decided to raise my score to 8. Independently of the strength of the experimental evidence I think that the ideas proposed in the paper will be useful for future research on OOD."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665256423,
                "cdate": 1700665256423,
                "tmdate": 1700665256423,
                "mdate": 1700665256423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iMG1CW7vZJ",
            "forum": "AnuHbhwv9Q",
            "replyto": "AnuHbhwv9Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
            ],
            "content": {
                "summary": {
                    "value": "The authors study how deep regression models can be adapted to perform better under covariate shifts.\n\nThey do a detailed theoretical analysis of the ordinary least squares method and how it is affected by covariate shifts. Motivated by these findings, they propose a post-hoc method that can be used to update the final layer of pre-trained deep regression models, utilizing unlabeled data from the target distribution.\n\nThe proposed method is evaluated on three real-world regression datasets, two tabular datasets and one image-based. The regression performance is compared to that of standard training and C-Mixup (with or without the proposed final layer update)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I agree with the authors that out-of-distribution generalization specifically for _regression_ problems is relatively unexplored. Thus, the problem studied in this paper is definitely interesting and important.\n\nThe paper is well written overall, and the authors definitely seem knowledgeable.\n\nAlthough I did not entirely follow all parts of the theoretical analysis in Section 3, I did find it quite interesting. Especially Figure 2. The resulting proposed method then also makes some intuitive sense overall."
                },
                "weaknesses": {
                    "value": "I found it quite difficult to follow parts of Section 3, especially Section 3.4.\n\nThe experimental evaluation could be more extensive. The proposed method is applied just to three real-world datasets, of which two are tabular datasets where small networks with a single hidden layer are used.\n\nThe experimental results are not overly impressive/convincing. The gains of ERM+SpAR compared to the ERM baseline in Figure 3, Table 2 and Table 3 seem fairly small.\n\nThe computational cost of the proposed method (Algorithm 1) is not discussed in the main paper, and only briefly mentioned in the Appendix."
                },
                "questions": {
                    "value": "1. Could the discussion of the computational cost be expanded and moved to the main paper? How does the cost of Algorithm 1 scale if X and/or Z contains a large number of examples? How about the memory requirements?\n\n2. Could you evaluate the proposed method on at least on more image-based regression dataset? (one of the datasets in _\"How Reliable is Your Regression Model\u2019s Uncertainty Under Real-World Distribution Shifts?\"_ (TMLR 2023) could perhaps be used, for example?)\n\n3. The results in Table 3 seem odd, do all other baseline methods really degrade the regression performance compared to ERM?\n\n4. Can you please discuss the results in Figure 3, Table 2 and Table 3 a bit more, the gains of ERM+SpAR compared to ERM seems quite small? Does the proposed method actually improve the performance of ERM in a significant way?\n\n\nMinor things:\n- Section 3.4, last paragraph: \"the the variance\" typo.\n- I would consider modifying the appearance of Table 1 - 3, removing some horizontal lines."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8365/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828502694,
            "cdate": 1698828502694,
            "tmdate": 1699637040175,
            "mdate": 1699637040175,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L2H4gFvMFY",
                "forum": "AnuHbhwv9Q",
                "replyto": "iMG1CW7vZJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 11sP"
                    },
                    "comment": {
                        "value": "We would first like to thank you for helping to improve our work.  A number of your major concerns were shared with the other reviewers, and so please take a look at the general response to all reviewers. There, we respond to your concerns about the computational complexity of SpAR, how ERM presents a strong baseline, and how the variance in performance presented in Tables 2 and 3 is not due to SpAR but the multiple splits of the data that the models are being evaluated on.\n\nThank you for your comments about the clarity of the writing. We will give the new concepts mentioned in Section 3.4 a more clear treatment in our revision of the work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318335279,
                "cdate": 1700318335279,
                "tmdate": 1700318335279,
                "mdate": 1700318335279,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bc5k4olg44",
                "forum": "AnuHbhwv9Q",
                "replyto": "rw4FUwhem8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "(Sorry for my late response. I have struggled to find enough time to both write responses as an author, and participate in the discussions as a reviewer)\n\nI have read the other reviews and all responses. Some of my concerns still remain.\n\nI thank the authors for the new experiments provided in the \"Additional Results\", this is a nice addition. However, I am not entirely convinced by these results. The improvement of ERM + SpAR compared to ERM is very small (0.6% and 0.5%). Given that the proposed method uses unlabeled data from the target distribution to update the final layer of the pre-trained ERM model, I think that more significant performance gains should be expected.\n\nAlso, \"How does the cost of Algorithm 1 scale if X and/or Z contains a large number of examples? How about the memory requirements?\" from my first question is not really addressed.\n\nThus, I am still borderline on this paper. It is not clear to me whether the proposed method actually improves the performance of ERM in a significant way. I would still need to see some more experimental results to be convinced, it would be interesting if you could evaluate the methods also on some of the real-world datasets from the \"How Reliable is Your Regression Model\u2019s Uncertainty Under Real-World Distribution Shifts?\" paper (e.g., SkinLesionPixels or HistologyNucleiPixels)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662077444,
                "cdate": 1700662077444,
                "tmdate": 1700662077444,
                "mdate": 1700662077444,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SlPq7SE3dw",
                "forum": "AnuHbhwv9Q",
                "replyto": "mOpbBQUrHr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_11sP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for this additional detailed response. I do really appreciate your efforts to address my concerns.\n\n\n**\"Computational complexity of SpAR\":**\nThis gives good additional details, thank you.\n\n\n**\"Choice of datasets\":**\nI appreciate the added experiments, and I definitely understand that time has been a limiting factor. I probably agree that the experimental evaluation now (with the two added datasets) is extensive enough, the problem is just that the results themselves are not particularly convincing. It is still not clear if the proposed method actually improves the performance of ERM in a significant way.\n\n\n**\"Performance of SpAR relative to baselines\":**\nIt might be that I don't know the domain adaptation / generalization literature well enough, but it just does not seem surprising to me that the proposed method -- which uses unlabeled data from the target distribution to update the final layer of the pre-trained ERM model -- consistently gives small performance gains over ERM. In fact, this is the least I would expect from such a method to even be considered reasonable.\n\n\n**\"Non-empirical contributions of the paper\":**\nI do appreciate that the theoretical analysis can be valuable in itself, it is indeed an argument in favor of accepting the paper.\n\n\nIn summary, I am still borderline on this paper. I might change my score to \"6: marginally above\", but I will need to take more time to carefully think about this during the reviewer-AC discussion period. Again, I want to thank the authors for their efforts to provide further clarifications. The problem studied in this paper is definitely interesting and important, and I do want to encourage more work in this direction."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718300386,
                "cdate": 1700718300386,
                "tmdate": 1700718300386,
                "mdate": 1700718300386,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wxZ0tlj3Zv",
            "forum": "AnuHbhwv9Q",
            "replyto": "AnuHbhwv9Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8365/Reviewer_TGia"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8365/Reviewer_TGia"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a novel post-processing technique to address the unsupervised domain adaptation challenge under the premise of covariate shift. This method stems from an intricate analysis of Ordinary Least Squares (OLS). The authors delve into the theoretical examination of the OLS loss in the context of covariate shift, leading to a proposal to project the estimator into a distinct subspace. The authors contend that selecting this subspace based on a comparative analysis of loss with respect to bias and variance across eigenvectors ensures enhanced performance on the target distribution. Empirical evaluations on multiple datasets substantiate the efficacy of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper offers a rigor analysis of OLS in the presence of covariate shift. The proposed projection technique is sound, and the decomposition of the loss function is notably interesting.\n2. The estimation strategy derived from finite samples is also sound.\n3. The paper is generally well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The paper sheds light on an interesting post-processing technique, aligning the linear layer from the source to the target domain. However, in the realm of deep learning, adapting the representational function across both domains is crucial. It raises the question: Can the proposed technique outperform other domain adaptation methods that also focus on refining the representation function? If such outperformance is challenging, is it feasible for the proposed post-processing technique to boost the performance of existing domain adaptation methods?\n2. Stemming from the aforementioned concern, it would be enlightening to see the proposed method compared with a broader spectrum of domain adaptation baselines in Table 3 for the CommunitiesAndCrime and Skillcraft datasets. Moreover, an exploration of the combination of the proposed method and these baselines could be insightful.\n3. For the PovertyMap-WILDS dataset, the setting aligns more with an out-of-distribution generalization task rather than traditional domain adaptation. Hence, it may be more judicious to include OOD methods for comparison. Furthermore, the performance enhancement attributed to the proposed method on this dataset seems marginal since the variance in performance exceeds the difference between the proposed method and the top-performing baseline."
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8365/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8365/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8365/Reviewer_TGia"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842317142,
            "cdate": 1698842317142,
            "tmdate": 1700582068187,
            "mdate": 1700582068187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bomKhefMmg",
                "forum": "AnuHbhwv9Q",
                "replyto": "wxZ0tlj3Zv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TGia"
                    },
                    "comment": {
                        "value": "We would first like to thank you for helping to improve our work. Please see our general response to all reviewers, as we address your concerns about the computational complexity of SpAR. \n\n**Regarding representation-based methods for OOD generalization**: We present several methods which either implicitly or explicitly regularize the model\u2019s representation as baselines in our experiments. In Table 3, both DANN and Deep-CORAL directly operate on the representation, while other methods such as C-Mixup and SwAV influence the entire network and therefore influence the representation as well. We note that SpAR is able to outperform all of these methods, achieving state-of-the-art performance on the Povertymap-WILDS dataset. It is also able to boost the performance of another method, C-Mixup.\n\nFurthermore, to your point about the importance of aligning representations across the training and testing distributions, SpAR can equivalently be thought of as a method that projects the test representations into a subspace spanned by the eigenvectors that do not demonstrate Spectral Inflation. This choice of subspace is informed by our theoretical work, and is the crux of our method\u2019s effectiveness."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318286225,
                "cdate": 1700318286225,
                "tmdate": 1700318286225,
                "mdate": 1700318286225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mUQWusS9zT",
                "forum": "AnuHbhwv9Q",
                "replyto": "bomKhefMmg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_TGia"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Reviewer_TGia"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the response. I would like to raise my score to 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582057316,
                "cdate": 1700582057316,
                "tmdate": 1700582057316,
                "mdate": 1700582057316,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IB9uGOkuoQ",
            "forum": "AnuHbhwv9Q",
            "replyto": "AnuHbhwv9Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8365/Reviewer_ZgCX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8365/Reviewer_ZgCX"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the problem of out-of-distribution generalization for regression models. The authors first analyze the sensitivity of the Ordinary Least Squares (OLS) regression model to covariate shift and characterize its out-of-distribution risk. Then they use this analysis to propose a lightweight spectral adaptation procedure(called Spectral Adapted Regressor) for the last layer of a pre-trained neural regression model. The paper demonstrates the effectiveness of this method on synthetic and real-world datasets, and it works well with data enhancement techniques such as C-Mixup."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Although there has been extensive research on distribution shifts in classification tasks, the authors focus on regression models which is a relatively unexplored problem of out-of-distribution generalization. The authors provide a novel analysis of the sensitivity of the Ordinary Least Squares (OLS) regression model to covariate shift. And they propose a spectral adaptation procedure specifically tailored for regression models, this adds to the originality of the paper.\n2. The authors provide a thorough analysis of the OLS regression model's out-of-distribution risk, utilizing the spectrum decomposition of the source and target data.\n3.  The paper provides a concise abstract that outlines the problem, methodology, and results."
                },
                "weaknesses": {
                    "value": "1. The proposed method assumes access to unlabeled test data for estimating the subspaces with spectral inflation. However, in practical scenarios, obtaining unlabeled test data may not always be feasible. It would be beneficial to explore alternative approaches or modifications to the method that do not rely on unlabeled test data.\n2. The compared methods are limited to me. The persuasiveness of the proposed approach would be stronger if more comparative data could be provided."
                },
                "questions": {
                    "value": "How computationally efficient is SpAR? The paper does not provide a detailed analysis of the computational efficiency of the proposed method. Considering the increasing complexity and size of neural regression models, it is important to assess the computational cost of the spectral adaptation procedure."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8365/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854078000,
            "cdate": 1698854078000,
            "tmdate": 1699637039949,
            "mdate": 1699637039949,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5iIJiuhdik",
                "forum": "AnuHbhwv9Q",
                "replyto": "IB9uGOkuoQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8365/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8365/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZgCX"
                    },
                    "comment": {
                        "value": "We would first like to thank you for helping to improve our work. Please see our general response to all reviewers, as we address your request for additional comparisons.\n\n**On the use of unlabelled data**: Investigating methods that make use of unlabelled data is an important endeavor for producing machine learning models that best make use of all of the resources available to them. In our work, we flexibly assume access to either the unlabelled evaluation data itself or a separate unlabelled data from the same distribution as the evaluation data. These are the assumed settings in the vast majority of research on OOD generalization (see the discussion in Section 5 of the paper). Consequently, positing an effective, efficient method that works in this regime is relevant to many machine learning researchers and practitioners alike."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8365/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700318224291,
                "cdate": 1700318224291,
                "tmdate": 1700318224291,
                "mdate": 1700318224291,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]