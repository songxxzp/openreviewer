[
    {
        "title": "Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation"
    },
    {
        "review": {
            "id": "m7HyEgs8QG",
            "forum": "z0B4GJuRjo",
            "replyto": "z0B4GJuRjo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1010/Reviewer_Ck8E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1010/Reviewer_Ck8E"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores how to reduce hallucination in text generation models. It makes the claim that previous methods have focused on token level checks, and claims novelty by instead focussing on sentence (sequence) level scores. In particular it introduces 2 measures: \n1: average of token level log-probabilities\n2. agreement of an entailment model of candidate decoding compared to several other options decoded via some decoding method. \n\nThese two measures are then used to guide decoding, or selection of decoded utterances, in order to reduce hallucination. \nResults are given claiming that these measures a helpful at detecting hallucination, and helpful at preventing hallucination when used in decoding strategy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "An important problem, with a meaningful contribution given in this paper, particularly in using entailment across candidates (ie consistency) to detect hallucinations."
                },
                "weaknesses": {
                    "value": "* The reported metrics section (4.2.3) introduces a faithfulness percentage. This is a clear and obvious measure ... however it isn't described how this is calculated! I presume it was manually done, rather than inferred by some model or automatic metric? If so who manually graded this? \n\n* It is very odd that top-k and top-p sampling methods result in reducing hallucination in a knowledge-grounded dialog generation. There is a lack of detail on what the actual task is here, and although the FaithDial dataset is referenced, it would help the reader a lot to actually describe what precise task is being addressed. There are comments as well about hallucinating in wider tasks like dialog generation, which I take to mean NLG tasks not grounded on factual information in the inputs. If this is so, how is hallucination even defined there?\n\n* Results for the number 1 hypothesis should not be in the appendix!\n* I'm unclear if the statistical tools used to make claims during \n* What is the entailment model used in measuring equation 1? How important is this to the final results? Does changing the entailment model result in entirely different outcomes, presumably so. \n* The writing is poor in that it repeats sections multiple times. For example probabilistic and semantic certainty are introduced 3 or 4 times."
                },
                "questions": {
                    "value": "* Interested for the authors thoughts on why likelihoods are based on product of of token probs, but uncertainty is better detected via mean of token (log) probs? \n* Were other stats beyond the mean (min, max, variance, etc) considered in forming p-crr?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1010/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698169983782,
            "cdate": 1698169983782,
            "tmdate": 1699636026998,
            "mdate": 1699636026998,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9QKIaCSow5",
                "forum": "z0B4GJuRjo",
                "replyto": "m7HyEgs8QG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Response to Reviewer (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty of the proposed approach and our contribution to research on model hallucinations. Below, we address your questions.\n### **[Q1 - How is Faithfulness Percentage calculated? ---- We use FaithCritic, a State-Of-The-Art hallucination classification model for KGDG task, for identifying hallucinated responses. We then report the percentage of the hallucinated responses on the test set.]**\n\nIn our experiments, we follow previous work [1] to report the faithfulness percentage of model responses using FaithCritic, a hallucination classification model. In our paper, we discussed our choice of hallucination classification model on page 4, Section 3.3.1. In section 4.2.2, we stated that we followed model selection experiment settings from Section 3. For better readability, we will re-emphasize this point in Section 4.2.2 in the final version.\n\n### **[Q2 - How is 'Hallucination' defined here? ---- We follow previous works and define hallucination on KGDG task to be responses with information that is unattributable to the provided knowledge in model input. ]**\n\nWe defined the KGDG task and the definition of hallucination in KGDG **in the first paragraph of our introduction section**. We hereby provide a clarification on the task formulation and hallucination definition.\n* In KGDG, a dialogue model is provided with textual knowledge and a series of conversation histories, and is expected to generate informative and meaningful responses to the previous conversation with the provided knowledge. \n* Previous work [2] on hallucination problems in KGDG task provides the following definitions of faithfulness and hallucination on this task:\n  * Factually Consistent Response is consistent with the provided knowledge. \n  * Hallucinated Response is not consistent with the knowledge but may still be correct.\n* We follow related previous works [1] [2] [3] to define hallucinations in this task as containing information that is unattributable to the source knowledge provided to the model.\n* In order to better convey this information to readers, we will add a \u2018task definition\u2019 subsection in the experiment section in the final version of our paper to highlight the details.\n\n### **[Q3 - Results for the number 1 hypothesis should not be in the appendix! ---- Thank you for recognizing the validity of our experiment result! We will make spaces to include it in the main paper for the final version.]**\n\nRegarding your point on the results for hypothesis 1, we were only able to include the table in the appendix due to the page limit. For the final version, we will make necessary modifications to include it in the main paper in the final version.\n\n### **[Q4 - Statistical Tool Usage? ---- There\u2019s potentially a compiling/formatting error. We are happy to further address your question if you can provide more details.]**\n\nWe are not able to view your full question in this line, potentially due to a compiling or formatting error. Would appreciate it if the reviewer could provide more details on the question!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700568473501,
                "cdate": 1700568473501,
                "tmdate": 1700569963077,
                "mdate": 1700569963077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "97HtFn2QYo",
                "forum": "z0B4GJuRjo",
                "replyto": "m7HyEgs8QG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q5 - What is the entailment model used and how important is this? ---- We use an off-the-shelf Roberta Large-based Natural Language Inference (NLI) model. An equally strong NLI model should yield similar results.]**\n\n* We used **an off-the-shelf Roberta Large-based Natural Language Inference (NLI) model** as the entailment model. We then calculate the **Entailment Score** as **the probability of being classified as 'entailed'** by the NLI model. \n  * We discussed our choice of the entailment model **on page 4, Section 3.3**, and said in Section 4 that we 'followed model selection experiment settings from Section 3'. In the final version, we will include full details again in Section 4.2.2 to re-emphasize this information to users.\n* Regarding your question on the importance of the entailment model to the final results, we definitely agree that **choosing an NLI model with high performance is crucial to delivering satisfactory results**, since a good entailment model **will better recognize and quantify semantic agreement** between response candidates.\n  * Theoretically, changing to another entailment model with **equally high performance** should **not** result in drastically different outcomes. On the other hand, changing to a **poor-performing** entailment model **might** result in poor results, since the entailment model would not be able to correctly identify and calculate semantic agreement and therefore semantic certainty of response candidates.\n\n### **[Q6 - Repeats in Writing. ---- Thanks for pointing it out! We will modify our writing to be more concise in the final version.]**\n\n### **[Q7 - Uncertainty as the mean of token log probabilities. ---- We believe that both are probability-based approaches to measure certainty in model responses, we provide details of the log probability-based certainty measure below.]**\n\nRegarding your question on likelihood and log probabilities, we believe that both are probability-based approaches that can measure certainty in model responses. We formulate **sequence-level probabilistic certainty as the mean of token log probability** because this **better measures the probability of generating the whole sequence**. For instance, given a context $c$ and a sequence $y_1 y_2 y_3$, the probability of generating the sequence would be $(p(y_1|c)p(y_2|y_1, c)p(y_3|y_1,y_2,c))$, which, when taken $log$, would be:\n$$ \nlog(p(y_1|c)p(y_2|y_1, c)p(y_3|y_1,y_2,c)) = log p(y_1|c) + log p(y_2|y_1, c) + log p(y_3|y_1,y_2,c).\n$$\n\n### **[Q8 - Other stats for P-CRR? ---- We did not explore the usage of other stats except the mean for probabilistic-based uncertainty measure, but it could be worth exploring in future works.]**\n\nRegarding your question on the usage of other stats in forming P-CRR, **no other stats beyond the mean** were considered since **we wanted to measure the overall sequence-level certainty** of a generated response. However, it is definitely an interesting direction to explore in future works. For instance, would a high variance in sequence-level probabilities be associated with hallucination levels? We belive that the contributions of our study lays a solid foundation for future explorations along this direction."
                    },
                    "title": {
                        "value": "Response to Reviewer (2/2)"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700569893345,
                "cdate": 1700569893345,
                "tmdate": 1700569975971,
                "mdate": 1700569975971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3nZHTPIbJ4",
            "forum": "z0B4GJuRjo",
            "replyto": "z0B4GJuRjo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1010/Reviewer_qG74"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1010/Reviewer_qG74"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a sequence-level certainty to addresses model hallucinations in Natural Language Generation (NLG). Expecially they demonstrate a significant correlation between probabilistic certainty (perplexity of the generated tokens) and semantic certainty (the entitlement score between all the pairs of generated response) in model responses and hallucination metrics. They introduce two Certainty-based Response Ranking (CRR) methods, Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR), which effectively mitigate hallucination in NLG across various datasets and models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n- The paper introduces an interesting approach that addresses model hallucination in Natural Language Generation (NLG).\n- The experimental methodology is rigorous, providing statistical significance in the context of NLG tasks.\n- The paper offers fair comparisons with existing hallucination reduction decoding methods."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n- The paper's scope is limited as it focuses on a specific NLG task, knowledge-grounded dialogue responses. In this setting, hallucination are not very pronounced, since the gold document is provided as input to the model. I suspect that a larger LLM (prompted correctly) can improve any of the provided metric  without any \"special\" decoding method (greedy). I invite the authors to report this baselines if possible."
                },
                "questions": {
                    "value": "Check weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "nan"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1010/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698422509702,
            "cdate": 1698422509702,
            "tmdate": 1699636026926,
            "mdate": 1699636026926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cCbA87cU36",
                "forum": "z0B4GJuRjo",
                "replyto": "3nZHTPIbJ4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Response to Reviewer"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty and validity of the proposed approach and experiment designs. Below, we address your concern.\n### **[Q - Prompting a larger LLM might improve metric. ---- We proved the effectiveness of our method on LLMs through experimenting with OpenLlama, a representative of the family of LLMs. Since our task requires full fine-tuning of models during experiments, results by prompting larger black-box models are of a different experiment setting and also might vary with different prompt choices.]**\n\n* Regarding your point on the scope of the paper, we believe that our study **utilizes KGDG task as a lens** to study **the correlation between sequence-level probabilistic and semantic certainty and level of hallucinations** in generative language models. \n  * Although hallucinations on KGDG task might seem pronounced as the reference knowledge is provided in the input, the simplicity of this task is also its strength: it **helps isolate analysis of hallucinations** in this task, which are responses that are unattributable to the provided knowledge. In addition, as we revealed in our experiments, **recently-developed LLMs such as OpenLlama still hallucinates on this simple task**.\n  * Given the page limit, we were not able to extend our study to other tasks, but our proposed methods and concepts **lay great foundations for future work** along this direction.\n* Regarding your point on experiment results using LLMs, we **conducted experiments with the OpenLlama model**, a representative of the recent LLM family. Positive experiment results **prove the effectiveness of our method when scaled to bigger generative LLMs**.  \n* However, we were not able extend to models with even larger models due to the **constraints on training resources**, since we fine-tune all models on the KGDG task. \n* We also did not extend experiments to black-box models such as ChatGPT because our experiments require full fine-tuning of all models on KGDG task, which **cannot be achieved on these API-only models**. We would need to use prompting to get model answers on this task, and there would be **no guarantee of the model\u2019s instruction-following ability for every response**. Model **performances might also vary based on the prompt choices** we use for the task.\n  * In addition, **this experiment setting would diverge drastically** from the other models, since all reported models were fine-tuned to conduct the KGDG task without prompting."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567062780,
                "cdate": 1700567062780,
                "tmdate": 1700567062780,
                "mdate": 1700567062780,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a5vEF6iKc9",
            "forum": "z0B4GJuRjo",
            "replyto": "z0B4GJuRjo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
            ],
            "content": {
                "summary": {
                    "value": "This work tries to reduce the hallucination in NLGs.  To this end, this work has done the following jobs:\n\n1. Based on the previous token-level uncertainty work, this work has defined sentence-level probabilistic certainty and semantic certainty as indicators to evaluate the level of hallucination in model generations. Then, experiments verified the effectiveness of the sentence-level probabilistic certainty and semantic certainty.\n\n2. This work provides theoretical proof to show that the black-box semantic certainty is a good estimator of the white-box probabilistic certainty.\n\n3. To mitigate the hallucination, this work proposes a Certainty-based Response Ranking (CRR), which re-ranks the response according to the sentence-level probabilistic certainty or the sentence-level semantic certainty."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This work has revealed that both a higher level of probabilistic certainty and a higher level of semantic certainty are significantly correlated with a lower level of hallucination in model generations. It brings an insight for us to reduce the hallucinations in NLGs.\n\n2.  As a white-box metric, probabilistic certainty can not be calculated in practice.  To this end, the authors find that the semantic certainty of a model response is an aligning and unbiased estimator of its probabilistic likelihood.\n\n3. Based on their findings on  Probabilistic certainty and Semantic certainty, this work proposes two ranking methods P-CRR and S-CRR to improve the faithfulness via re-ranking. Experiments can demonstrate their effectiveness, especially the black-box S-CRR."
                },
                "weaknesses": {
                    "value": "1. The proposed methods may significantly damage the diversity of the generated text. In the proposed Certainty-based framework, both P-CRR and S-CRR encourage the backbone model to generate safe, receptive, general responses. Safe responses have higher probability certainty in either a language model or a conditional language model and are always similar to each other. For example, if a model generates three samples, 'I don't know.' , 'I don't know it.' , 'I do not know it.'; then,  both the P-CRR and S-CRR will give higher scores although they are boring.\n\n2. The organization of this work is hard to follow and verbose:\n\n-  As an important concept,  `uncertainty' lacks enough introduction when it first appears. Even in Sec 2.1, the authors only gave a simple literal definition.  Readers can't understand what it is until the 4th page. Meanwhile,  there is no formulation to introduce the token-level uncertainties.\n\n- The authors tended to repeatedly introduce the same thing (for example,  findings proposed by (Xiao & Wang, 2021)) in different sections.\n\n-  In Equation 1, we can not directly infer what $Entailment( *,*)$ is from the nearby context.  The authors only give a very rough Introduction in Sec 3.3.1 without any formulation. \n\n3. The evaluation of dialogues (for example, Table 2) only considers faithfulness. It is necessary to report other metrics (BLEU, ROUGE, DISTINCT, etc.) as well."
                },
                "questions": {
                    "value": "Can you report results on more metrics?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1010/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1010/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1010/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569098716,
            "cdate": 1698569098716,
            "tmdate": 1699636026851,
            "mdate": 1699636026851,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k7530T9o9L",
                "forum": "z0B4GJuRjo",
                "replyto": "a5vEF6iKc9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Response to Reviewer (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty and effectiveness of the proposed sequence-level semantic certainty and our mitigation method design. Below, we address your questions.\n\n### **[Q1 - The proposed methods may damage generation diversity. ---- Theoretically, CRR shouldn't hurt diversity since it does not modify models' output distributions. We prove this by experimenting with a GPT2-based model on Text Summarization task. Model performance does not drop with CRR.]**\n\n* Theoretically, the proposed P-CRR and S-CRR methods **will not damage the diversity / quality** of the generated text, because our methods **do not modify the model\u2019s output distribution**. The proposed CRR methods simultaneously and independently sample a number of full responses from a model, and rank them based on their sequence-level probabilistic certainty or semantic certainty. Therefore, there should be **no interference on the model\u2019s generation quality**.\n* In order to further prove our point, we conducted an ablation study on a GPT2-based model for Text Summarization task. The model is inference on the first 1,000 entries of the test split of the **CNN Daily Mail** Text Summarization dataset, and model responses are evaluated using **ROUGE** and **BLEU**, which are text overlap-based metrics. In order to show that CRR does not damage generation quality, we first evaluate the baseline model when **CRR is not applied**. Then, we compare model performance with that **with P-CRR and S-CRR applied**, respectively. The results are as follows:\n| Model              | BLEU | | | ROUGE-1 | | | ROUGE-L | | | \n| :---------------- | :------: | :------: | :------: | ----: |:------: | :------: | :------: | :------: | :------: | \n|   |   **Baseline**   | **+P-CRR** | **+ S-CRR** |    **Baseline**   | **+P-CRR** | **+ S-CRR** |   **Baseline**   | **+P-CRR** | **+ S-CRR** |\n| GPT2-Small           |   **2.80** | 2.70 | 2.79 | 21.60 | 20.89 | **21.72** | 14.25 | 13.46 | **14.56** |\n\n* As observed from the table, applying P-CRR and S-CRR **does not lead to performance drop** on the summarization task. Therefore proving our expectation that CRR **does not damage generation diversity / quality**."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564258422,
                "cdate": 1700564258422,
                "tmdate": 1700565708678,
                "mdate": 1700565708678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B2RhC5qtcy",
                "forum": "z0B4GJuRjo",
                "replyto": "a5vEF6iKc9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q2 - Writing can be more organized and less verbose. ---- Thanks for pointing out! We will make modifications to address these issues in the final version.]**\n* Regarding your points on the presentation of our paper, we will modify the parts of the papers accordingly to better convey our message. \n* Specifically, regarding your question on the definition of $Entailment(\\cdot, \\cdot)$ in Equation 1, it represents **the probability of being classified as \u2018entailed\u2019** by a Natural Language Inference (NLI) model. We will clarify all necessary details in the final version."
                    },
                    "title": {
                        "value": "Response to Reviewer (2/3)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700564483447,
                "cdate": 1700564483447,
                "tmdate": 1700603592593,
                "mdate": 1700603592593,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eDoMR9Qgzj",
                "forum": "z0B4GJuRjo",
                "replyto": "a5vEF6iKc9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q3 - Evaluation only reports faithfulness. ---- We report faithfulness using a State-Of-The-Art hallucination classification model, since previous works have pointed out that overlap-based metrics might fail to reflect hallucinations. We re-evaluate model generations on BLEU and ROUGE and obtain positive results.]**\n\n* Regarding your points on the reported metric in this paper, previous studies have pointed out that **traditional overlap-based metrics such as ROUGE and BLEU fail to reflect the hallucination levels** of a model response[2][3][4][5][6][7]. For instance, it has been discovered in abstractive summarization task that:\n> Even if a summary contains a large amount of hallucinatory content, it can achieve a high ROUGE score [2]. \n* In KGDG task, we believe these traditional metrics could suffer from two major drawbacks:\n  * They directly compare with a ground truth answer and not the provided knowledge.\n  * They fail to take into account language variations in responses, even when they could mean the same thing as the gold response.\n* Therefore, we chose to report faithfulness scores using FaithCritic, a SOTA NLI-based hallucination classification model, over traditional metrics in our study. \n* However, we do recognize that ROUGE and BLEU provide information on how similar a model's response is to the high-quality 'gold response'. We re-evaluated model generations on the FaithDial dataset on BLEU, ROUGE-1, and ROUGE-L, with the same nucleus + top-k sampling decoding method as in Table 1. Results are as follows:\n| Model              | BLEU | | | ROUGE-1 | | | ROUGE-L | | | \n| :---------------- | :------: | :------: | :------: | ----: |:------: | :------: | :------: | :------: | :------: | \n|   |   **Baseline**   | **+P-CRR** | **+ S-CRR** |    **Baseline**   | **+P-CRR** | **+ S-CRR** |   **Baseline**   | **+P-CRR** | **+ S-CRR** |\n| GPT2-Small           |  7.50 | **10.47** | 8.52 | 33.01 | **37.77** | 34.70 | 27.74 | **32.01** | 29.27 |\n| GPT2-Medium           |   8.03 | **10.83** | 9.00 | 33.93 | **39.08** | 36.22 | 28.28 | **33.32**  | 30.15 |\n| T5-Base          |   6.58 | **9.82** | 6.12 | 33.34 | **37.38** | 32.62 | 26.12 | **30.90**  | 25.38 |\n| OpenLlama-3B    |   8.54 | **11.11** | 9.04 | 35.85 | **39.69** | 37.27 | 29.65 | **33.39**  | 30.79 |\n\n* We can observe that **both P-CRR and S-CRR brings considerable performance gains** on BLEU and ROUGE metrics for all 4 models, further proving the effectiveness of our proposed method. \n  * In the final version, we will incorporate this ablation study result in the Experiment section."
                    },
                    "title": {
                        "value": "Response to Reviewer (3/3)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565638968,
                "cdate": 1700565638968,
                "tmdate": 1700565729824,
                "mdate": 1700565729824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6AVr1N6y8W",
                "forum": "z0B4GJuRjo",
                "replyto": "a5vEF6iKc9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
                ],
                "content": {
                    "comment": {
                        "value": "This answer do not directly answer my question. Besides,  as we know, BLEU & ROUGE cannot evaluate the diversity."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662092142,
                "cdate": 1700662092142,
                "tmdate": 1700662245320,
                "mdate": 1700662245320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ANeHV9KwRQ",
                "forum": "z0B4GJuRjo",
                "replyto": "eDoMR9Qgzj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Reviewer_drdo"
                ],
                "content": {
                    "comment": {
                        "value": "Yep, you should provide such common metrics to show your performance.\n\nAccording to the provided new results, the improvement is `too impressive.`  For example, GPT-Small+P-CRR can significantly surpass OpenLlama-3B. Consequently, such results can not convince me.\n\nEmpirically, your approach does not involve new data or new input; it is hard to guess why it can improve so much.  I am sorry, now I am doubting whether there are some tricks."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663062439,
                "cdate": 1700663062439,
                "tmdate": 1700663062439,
                "mdate": 1700663062439,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kCHjAc3cUZ",
                "forum": "z0B4GJuRjo",
                "replyto": "a5vEF6iKc9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Additional Concern of Reviewer"
                    },
                    "comment": {
                        "value": "### **[Q1 - Helpfulness of Model Responses Due to Low Diversity ---- We conducted string matching to find answers with \"don't know\" in the generated responses, and did not observe a significant increase in the percentage of such answers in CRR responses.]**\n\n* Considering the reviewer's original concern about decoding methods with CRR would result in outputting boring answers such as **\"I don't know\"**, we conduct string matching to find out the **percentages of responses** with the phrases \"don't know\", \"don' t know\", \"don ' t know\", or \"don 't know\" (to take into account formatting differences). Results are below:\n| Model              | Baseline | P-CRR | S-CRR |\n| :---------------- | :------ | :---- | :---- |\n| GPT2-Small        |   3.73%   |  3.98% | 3.53% |\n| GPT2-Medium         |   3.00%  | 3.56% | 2.88% |\n| T5-Base    |  8.41%   | 6.05% | 10.54% |\n| OpenLlama-3B |  5.07%   | 5.54% | 5.45% |\n\n* We can observe that when adopting P-CRR or S-CRR, there is **no significant increase** in the percentage of answers with the phrase \"don't know\". Should the reviewer have further questions or concerns about the helpfulness of model responses with CRR, we will provide further clarifications and results if necessary.\n\n### **We would like to express our sincere gratitude to the reviewer for providing additional thoughts on our results. Please let us know if there are any further questions.**"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698772429,
                "cdate": 1700698772429,
                "tmdate": 1700698913269,
                "mdate": 1700698913269,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M4MYmQZSDM",
            "forum": "z0B4GJuRjo",
            "replyto": "z0B4GJuRjo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1010/Reviewer_Rt2T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1010/Reviewer_Rt2T"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents sequence-level certainty as a common theme over hallucination. The authors categorize sequence-level certainty into probabilistic certainty and semantic certainty and explore the correlation between sequence-level certainty and the level of hallucination. Based on the observation, the authors proposed a decoding-time method called Certainty-based Response Ranking (CRR) to mitigate hallucination. The experimental results show CRR can reduce model hallucination under certain settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The author's proposal of sequence-level certainty as a common theme over hallucination is a good idea, as is the design of probabilistic certainty and semantic certainty.\n- The logic and structure of the paper is clear and easy to follow. The design details and experimental setup are explained thoroughly. This work is comprehensive, well-organized, and complete."
                },
                "weaknesses": {
                    "value": "-\tThe last sentence of the first paragraph in the Introduction sets too narrow of a definition for hallucination in the KGDG task. In the current era of large models, we need to view the hallucination problem from a broader perspective, such as fact-conflicting and context-conflicting hallucinations [1], rather than limiting to input-conflicting hallucination. Meanwhile, judging from Table 6 in the Appendix, the cases are too simple and the chosen models like GPT-2 have insufficient capabilities. Through prompting, I found that gpt-3.5 does not hallucinate on those examples.\n- Evaluating large models' hallucination phenomena at the sequence level using semantic certainty is an good idea, but the evaluation method for semantic certainty seems a bit crude. Also, the proposed decoding method simply selects better generations from the candidate set based on different metrics, which is quite similar to similar to Minimum Bayes Risk Decoding[2].\n- The paper uses a RoBERTa-Large-based hallucination classification to evaluate whether generated text contains hallucination. However, the accuracy and effectiveness of this method for judging hallucination are not explained. Accurately assessing whether generated text hallucinates is fundamental to the analysis and experiments in this paper, so it is an important part.\n- Hypothesis 2 proposed in Section 3.4 is an important basis for subsequent work, but there may be issues with the verification process. On one hand, it is reasonable that low certainty can lead to hallucination. But on the other hand, when models hallucinate, certainty is not necessarily low (especially under the consistency-based certainty evaluation designed by the authors). Based on my personal experience and experiments with LLM, they sometimes hallucinate confidently, i.e. sampling multiple times yields consistent outputs, especially for knowledge and numeric hallucinations (but I have not statistically verified this phenomenon rigorously). Also, the PBCC test is sensitive to class distribution. With imbalanced categories like fewer hallucination examples, it can still give a positively correlated relationship between certainty and hallucination, ignoring cases of high certainty hallucination. As described in 3.3.1 and Table 2, there is indeed a class imbalance in the FaithDial dataset under nucleus sampling, where hallucination examples make up less than 10% of the data.\n- The analysis of efficiency for the P-CRR and S-CRR methods is missing. These multi-sampling approaches may greatly increase time and computational costs.\n- The abstract contains too many unnecessary details and is somewhat convoluted. modifications are needed. Less essential details like the introduction of P-CRR and S-CRR can be briefly summarized. More explanation of the motivation behind this work could be added.\n\n[1] Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models\n\n[2] Understanding the Properties of Minimum Bayes Risk Decoding in Neural Machine Translation"
                },
                "questions": {
                    "value": "Refer to weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1010/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658541730,
            "cdate": 1698658541730,
            "tmdate": 1699636026769,
            "mdate": 1699636026769,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LpD6YkwYKO",
                "forum": "z0B4GJuRjo",
                "replyto": "M4MYmQZSDM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks & Response to Reviewer (1/7)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the novelty in our method design and paper structure. Below, we address your questions.\n### **[Q1 - The definition for hallucination in the KGDG task is too narrow. ---- We follow previous works' definition of hallucination on KGDG task, further exploration of factual or context-conflicting hallucinations on this task would be an interesting direction for future works.]**\n\nPrevious work on KGDG [1] provided the following definition:\n> * Factually Consistent Response is **consistent with the provided knowledge**. \n> * Hallucinated Response is **not consistent with the knowledge** but **may still be correct**.\n\nAn explanation to this definition would be because of the formulation of the KGDG task, for which the model should generate an answer **according to a specific piece of knowledge**. Therefore, it is most crucial to investigate whether the model generation is not factually consistent with the provided knowledge in the input. In our work, we follow the related previous works [1] [2] [3] to define hallucinations in this task as containing information that is unattributable to the source knowledge provided to the model."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558156228,
                "cdate": 1700558156228,
                "tmdate": 1700562448902,
                "mdate": 1700562448902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W7pI2Hm0pU",
                "forum": "z0B4GJuRjo",
                "replyto": "M4MYmQZSDM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q2 - Cases are too simple and the chosen models like GPT-2 have insufficient capabilities. ChatGPT maybe would not hallucinate on these examples. ---- We included experiments on OpenLlama-3B,  a representative of the recent family of LLMs, with positive results. We did not experiment on ChatGPT because we have no access to model output distribution, and we cannot fine-tune it as we did for other models.]**\n\n* In order to explore the effectiveness and generalizability of our method to LLMs, we include full experiments with **OpenLlama**, a representative of the recently developed LLMs. By using Llama, we could **avoid black box scenarios and have full access to model output distribution**. We were also **able to fine-tune the model specifically for KGDG task**, aligning experimenting settings with other models. Positive results on OpenLlama provide empirical insights that **our proposed methods are generalizable to larger LLMs**, in addition to GPT-2 and T5.\n\n* We did not choose to experiment with ChatGPT for two major reasons: \n  * We **cannot obtain the output distribution**, therefore would not be able to experiment with methods related to probabilistic certainty.\n  * We **cannot fine-tuned ChatGPT for KGDG task**, so only prompt-based methods could be used to sample the answers. Since all the reported experimented models are fine-tuned on the KGDG task, experimenting with ChatGPT through prompting would **cause the experiment setting to be very different from the other models**."
                    },
                    "title": {
                        "value": "Response to Reviewer (2/7)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558593192,
                "cdate": 1700558593192,
                "tmdate": 1700562547104,
                "mdate": 1700562547104,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9f2OqKuWOr",
                "forum": "z0B4GJuRjo",
                "replyto": "M4MYmQZSDM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q3 - Semantic certainty evaluation method seems crude. The method also appears similar to Minimum Bayes Risk Decoding. ---- Our simple yet effective method lays great foundation for future works. While the method setting that selects from a candidate set of generations is a common trait among multi-sampling-based methods, we believe that CRR is inherently different from MBR Decoding. ]**\n\n* We believe that the **simple yet effective design** of our proposed CRR methods lays good foundations for future work, which might make improvements on top of our proposed approach. \n  * Regarding your question on the similarity between our CRR approach and the Minimum Bayes Risk Decoding approach, we believe that **CRR is inherently different from MBR Decoding**. Having its root in decision theory, MBR Decoding has recently been applied to tasks such as Automatic Speech Recognition (ASR), and Neural Machin Translation (NMT), with **no related exploration on how they can be applied to address model hallucination**. For the NMT task, previous work [4] have stated that:\n> The goal of MBR is to find **not the most probable translation**, but the one that minimizes the expected risk for a given loss function and the true posterior distribution.\n\n  * Therefore, **MBR Decoding diverges from the definition of our P-CRR approach**, which intuitively outputs the candidate with largest sequence-level probabilistic certainty. \n  * Although the *sample>compare>choose* pipeline might appear similar to S-CRR, we believe that **this similarity is shared as a method structure by a family of multi-sampling-based generation methods** [5][6][7], each independently designed **with variations** to address different domain-specific tasks.\n  * In addition, the utility function for MBR Decoding utilizes **overlap-based metrics** such as ROUGE and BLEU to calculate the token or n-gram component-level similarity of sampled responses. Our S-CRR method considers **semantic similarity or entailment** when choosing from generation candidates, rather than component or token-level similarity."
                    },
                    "title": {
                        "value": "Response to Reviewer (3/7)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559497889,
                "cdate": 1700559497889,
                "tmdate": 1700562709297,
                "mdate": 1700562709297,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6tYnjmwtLO",
                "forum": "z0B4GJuRjo",
                "replyto": "M4MYmQZSDM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q4 - Accuracy and effectiveness of the hallucination critic model are not provided. ---- We utilized an off-the-shelf hallucination classification model, and are happy to quote the original authors' reported metrics to prove its effectiveness.]**\n\n* For hallucination classification, we use an off-the-shelf hallucination classification model that is specifically tuned for recognizing hallucinations on the KGDG task. It was trained on FaithCritic, a derivative of the FaithDial dataset [2]. The objective of the model is to predict whether an utterance is faithful or not, given the source knowledge. \n* We hereby quote the reported accuracy of the model provided by the original authors, along with the ablation experiment results of models trained on 3 other NLI datasets (DECODE, DNLI, MNLI) and inference on 2 hallucination classification datasets (BEGIN, FaithCritic): \n| Trained on              | BEGIN | FaithCritic | \n| :---------------- | :------ | :----|\n| DECODE        |   58.8   | 38.5 |\n| DNLI           |   59.8   | 30.9 |\n| MNLI    |  61.1   | 81.6 |\n| **FaithCritic** |  **71.6**   | **86.5** |\n*  We can observe that the **best hallucination detection performance is achieved by the FaithCritic model**. \n   * Although the FaithCritic model might not be perfect, **it is among the best-performing hallucination classification models available**, and therefore we choose to report results using this model."
                    },
                    "title": {
                        "value": "Response to Reviewer (4/7)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700560126599,
                "cdate": 1700560126599,
                "tmdate": 1700562762505,
                "mdate": 1700562762505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ExfNhVGQCv",
                "forum": "z0B4GJuRjo",
                "replyto": "M4MYmQZSDM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q5 - Models can sometimes hallucinate confidently. PBCC test might be sensitive to class distribution in the data. ---- Although edge cases do exist, our experiments show a general correlation between high sequence-level certainty and low hallucination. We provide the mean differences in sequence-level certainty scores between hallucinated and faithful answers to validate PBCC results.]**\n* We agree that there exists edge cases where a generative model hallucinates with high confidence. However, we believe that **our experiments empirically show a general correlation** between low sequence-level certainty and high hallucination on KGDG task.\n* In order to validate PBCC test results that the sequence-level certainty level of faithful answers are higher than that of hallucinated answers, we hereby report the **mean differences** in the sequence-level certainty scores **between faithful and hallucinated answers**, on the FaithDial dataset, with the same nucleus + top-k sampling decoding method as in Table 1. \n  * Specifically, we first group model responses into $R_{Faithful} = r_{f,1}, r_{f, 2}, ... r_{f, m}$ and $R_{Hallucinated} = r_{h, 1}, r_{h, 2}, ... r_{h, n}$. \n  * Then, let $Certainty(\\cdot)$ be a function that calculated the sequence-level certainty score of a response. We report the mean differences between the certainty scores of these two groups of responses as:\n$$\n\\text{Mean Diff.} = \\frac{1}{m} \\sum_{i=1}^m Certainty(r_{f, i}) - \\frac{1}{n} \\sum_{j=1}^n Certainty(r_{h, j})\n$$\n* Results are as follows:\n| Model              | P-CRR Mean Diff. | S-CRR Mean Diff. |\n| :---------------- | :------: | ----: |\n| GPT2-Small        |   0.17   | 0.63 |\n| GPT2-Medium         |   0.20   | 0.40 |\n| T5-Base    |  0.14   | 0.08 |\n| OpenLlama-3B |  0.04   | 0.54 |\n* The **positive mean differences** between certainty scores of faithful responses and hallucinated responses for **all 4 models** further validates our PBCC test results, indicating that **faithful responses have significantly higher sequence-level certainty scores** - both in terms of semantic certainty and probabilistic certainty - than hallucinated responses."
                    },
                    "title": {
                        "value": "Response to Reviewer (5/7)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561300868,
                "cdate": 1700561300868,
                "tmdate": 1700562813471,
                "mdate": 1700562813471,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kIYavBYkVJ",
                "forum": "z0B4GJuRjo",
                "replyto": "M4MYmQZSDM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q6 - No analysis of efficiency for the P-CRR and S-CRR methods. ---- P-CRR does not introduce additional inference time, as the candidate sampling process is simultaneous. S-CRR introduces additional inference costs, but can be applied to black-box scenarios.]**\n* P-CRR method **does not introduce additional inference time** as it is probability-based, and the multi-candidate sampling is done simultaneously. However, model output probabilities must be available for implementing the P-CRR approach. \n  * This is one of the motivations of us to study semantic certainty and S-CRR, because it can be implemented even in blackbox scenarios where only textual outputs are available. \n* S-CRR does introduce additional inference costs in the Agreement Score calculation process. However, since we compute semantic certainty on sequence level and the Roberta-Large model we use for Agreement Score calculation does not have a tremendous size (355M), the inference time isn't significantly affected (the inference time is ~1.5 times that of baseline when number of sampled candidates is set to 5). In addition, S-CRR is **a promising approach for application in black-box scenarios**, since it **does not require access to output probabilities**.\n  * In the final version, we will rerun experiments and report inference time for all methods in the Appendix.\n* Strong empirical results on P-CRR and S-CRR prove the **correctness** and **effectiveness** of the proposed **sequence-level certainty** method for **measuring and potentially reducing hallucinations**."
                    },
                    "title": {
                        "value": "Response to Reviewer (6/7)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562203254,
                "cdate": 1700562203254,
                "tmdate": 1700562837885,
                "mdate": 1700562837885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bVHOOrOqYC",
                "forum": "z0B4GJuRjo",
                "replyto": "M4MYmQZSDM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### **[Q7 - The abstract can be more concise. --- Thanks for pointing out! We will make modifications in the final version.]**\n\nWe thank the reviewer for comments regarding the presentation of the abstract. We will make further edits in the final version to make it more concise, as well as add in necessary technical details."
                    },
                    "title": {
                        "value": "Response to Reviewer (7/7)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562298374,
                "cdate": 1700562298374,
                "tmdate": 1700562859394,
                "mdate": 1700562859394,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]