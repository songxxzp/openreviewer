[
    {
        "title": "Tailoring Self-Rationalizers with Multi-Reward Distillation"
    },
    {
        "review": {
            "id": "7RvWEDKwgn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_CcB6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_CcB6"
            ],
            "forum": "t8eO0CiZJV",
            "replyto": "t8eO0CiZJV",
            "content": {
                "summary": {
                    "value": "The authors extend an existing LM control mechanism to work for controlling multiple attributes simultaneously, and following this mechanism they train T5-Large to do reasoning that is more plausible, non-repetitive, and consistent with the predicted answer. On some challenging QA tasks they get a small bump in accuracy, but a much bigger gain in human evaluations of the quality of the reasoning produced by the model to justify the answer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "--tackles a challenging task (making LMs output reasoning for their answers that is more reliable on a couple of different axes) which is highly topical\n\n--impressive human eval results showing that their final rationales are much better than baselines'\n\n--interesting discussion of reward hacking"
                },
                "weaknesses": {
                    "value": "--Methodologically, this multi-attribute control setup seems to be a straightforward extension of Quark, and the idea of finetuning an LM to use tags to provide control i think is pretty well-explored in other work such as [1], and doing it for multi-attribute with different tags is used as a baseline in [2]. i feel that your main contribution is not so much the new algorithm in a general sense, but rather the interesting application (with convincing human eval results) to the important and currently relevant task of making LM reasoning more reliable.\n\n[1] Keskar, Nitish Shirish, et al. \"Ctrl: A conditional transformer language model for controllable generation.\" arXiv preprint arXiv:1909.05858 (2019).\n\n[2] Yang, Kevin, and Dan Klein. \"FUDGE: Controlled text generation with future discriminators.\" arXiv preprint arXiv:2104.05218 (2021).\n\n--some of the models you use to evaluate/filter the individual qualities, e.g. VERA, are way bigger than the model you're finetuning, which arguably gives you extra signal that the SFT baseline doesn't have? what was the data used for training those? on a related note, this raises some concerns that your method might not be scalable to larger LMs, unless we have e.g. an even larger version of VERA to provide supervision?"
                },
                "questions": {
                    "value": "--Is there any particular reason to think additive or classic might be better than the other in any particular setting? Otherwise, it kind of seems like you're just giving yourself \"multiple tries\" at your benchmark, in some sense.\n\n--Is GPT3 = text-davinci-003 throughout?\n\n--does adding the finetuning for consistency fix the problems in 5.2? or is this not exactly the same?\n\n--the reward hacking discussion seems important and i'm glad you included it - there are a lot of potential hacks, e.g. a degenerate \"rationale\" could just be the answer itself or restating that in some way, right? do you get around this issue by starting with distillation so that the supervised rationales are initially reasonable, rather than doing e.g. some RL? i'm wondering how you would be able to go about this if you didn't have access to a much stronger model to distill from initially - it seems like so far you've only showed in a distillation-like setting, from GPT3 to a vastly smaller model. \n\n--nit: maybe also cite [1]?\n\n[1] Lightman, Hunter, et al. \"Let's Verify Step by Step.\" arXiv preprint arXiv:2305.20050 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6506/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6506/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6506/Reviewer_CcB6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6506/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697396316014,
            "cdate": 1697396316014,
            "tmdate": 1699636730337,
            "mdate": 1699636730337,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5rCN4Vpxwm",
                "forum": "t8eO0CiZJV",
                "replyto": "7RvWEDKwgn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive review and very helpful comments and suggestions! We have added our response below as well as changes to the PDF based on your feedback.\n\n> Methodologically, this multi-attribute control setup seems to be a straightforward extension of Quark, and the idea of finetuning an LM to use tags to provide control i think is pretty well-explored in other work such as [1], and doing it for multi-attribute with different tags is used as a baseline in [2]. i feel that your main contribution is not so much the new algorithm in a general sense, but rather the interesting application (with convincing human eval results) to the important and currently relevant task of making LM reasoning more reliable.\n\nWe partly agree with this! Yes, our task is tailored towards rationalization, which is coupled with accuracy and task performance. However, this is different from other controllable text generation tasks like creative text, poetry, machine translation, etc. However, we also note that adding multiple rewards for rationale generation was not a trivial addition, primarily because good quality rationales that increase a certain reward are not useful \u2013 they should be able to help improve the task accuracy, along with having high human preference. In Section 5.2, we can observe that LMs tend to overfit to a reward and hack their way to improving these metrics. We first need to select metrics that are actually important for rationalization, and at the same time, make sure that they are *compatible* in a way so that they can be used together. We hope this can help clarify the differences in our task setup and novelty behind our work.\n\nThank you for linking the above works as well. As per this suggestion, we have added them in the subsection in our related work, where we discuss reward/multi-reward based generation methods\n\n> some of the models you use to evaluate/filter the individual qualities, e.g. VERA, are way bigger than the model you're finetuning, which arguably gives you extra signal that the SFT baseline doesn't have? what was the data used for training those? on a related note, this raises some concerns that your method might not be scalable to larger LMs, unless we have e.g. an even larger version of VERA to provide supervision?\n\nIn this work, we focus majorly on how to use multiple rewards to improve the quality of rationales, both quantitatively and qualitatively. Therefore, how the rewards are actually designed are out of scope for this work. For example, for consistency, we use a t5-base reward model which is *smaller* than the t5-large we train. For plausibility, we use VERA (t5-11b) because the original work trained a t5-11b model on that property. In Section 5.2, we also talk about certain property metrics which depend on Web-API calls (such as factuality) which are practically infeasible - we experimented with zero-shot FLAN-T5 as a metric on factuality since we had to make a tradeoff between metric quality and inference time. Hence, say we want to train a much larger LM to self-rationalize, the reward models can still remain the same since they are a function of the property we want to measure and how good of a proxy they are for that metric, and not a function of the LM we are training. \n\n> Is there any particular reason to think additive or classic might be better than the other in any particular setting? Otherwise, it kind of seems like you're just giving yourself \"multiple tries\" at your benchmark, in some sense.\n\nThe choice of MaRio Classic or Additive, and the choice of the order of the rewards in the multi-reward setting is a hyperparameter setting depending on the dataset we are training for. Sometimes, certain datasets require us to first optimize one reward, before adding the others one-by-one, whereas datasets might require all rewards to be provided from the very start for the best optimization; hence, we provide the framework for both such methodologies. (Additionally, other multi-reward techniques such as the product of rewards, or filtering of rewards have been added as baselines)\n\n(... continued below)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6506/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038234338,
                "cdate": 1700038234338,
                "tmdate": 1700038234338,
                "mdate": 1700038234338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kxWIUJZiGl",
                "forum": "t8eO0CiZJV",
                "replyto": "Urdbwg61EL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6506/Reviewer_CcB6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6506/Reviewer_CcB6"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thanks for addressing my questions. The methodology still seems quite similar to Quark to me, but I will maintain my score of 6 in view of the interesting application and empirical results."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6506/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693075552,
                "cdate": 1700693075552,
                "tmdate": 1700693075552,
                "mdate": 1700693075552,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mtZoviDhnh",
            "forum": "t8eO0CiZJV",
            "replyto": "t8eO0CiZJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_7guo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_7guo"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the problem of generating plausible, consistent, and diverse rationales that also improve the downstream task performance while using small-scale LMs. The proposed method, called MARIO, is a multi-reward conditioned self-rationalization algorithm that optimizes multiple properties. Extensive evaluation show that produced rationales are plausible and improve performance.\n\nMario works a follow. First, it trains a small LM to self-rationalize, with the help of GPT-3. Then, it casts the problem into a multi-reward conditioned rationale generation problem. The key is to leverage a multi-reward setup, where the dimensions to optimize are plausibility, diversity, consistency, and task accuracy. Each reward is based on an automated metric. At the end, the propose method only extend Quark [Lu et al. 2022], which limits the novelty (the proposed method is 3 paragraphs).\n\nThe experiments focus on QA datasets. The two variants of Mario don't seem to significantly outperforms the baselines, Classical underperforms on StrategyQA and Mario on the others. However, the human evaluation show more promising results, showing that automated evaluations are not enough on their own. Other datasets than QA ones should be used to show the generalization of the proposed method. For example, beer, hotel, or amazon datasets. The analysis on reward hacking and optimizing solely on task performance is very interesting.\n\nOverall the work is clear, well-written, and well-structured. It is a bit weird that the related work is put in the appendix. I would highly encourage the authors to move it back into the main paper. My concerns remain the novelty of the method - that seems to extend QUARK for multi-reward steup - and the lack of datasets that are not QA, especially when we are talking about rationalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Good human evaluation and results\n- Interesting method to make self-rationalization work with small LMs"
                },
                "weaknesses": {
                    "value": "- Other datasets than QA ones should be used to show the generalization of the proposed method.\n- Small improvement in Table 2\n- Limited novelty"
                },
                "questions": {
                    "value": "- How would you adapt your method for other rationalization tasks that are not QA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6506/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698612601621,
            "cdate": 1698612601621,
            "tmdate": 1699636730221,
            "mdate": 1699636730221,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ofcfVcVg4K",
                "forum": "t8eO0CiZJV",
                "replyto": "mtZoviDhnh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your helpful comments and insightful questions! We are happy to hear that you appreciated our approach as well as human evaluations. Based on your feedback, we have updated the draft and provided the responses below. \n\n> Other datasets than QA ones should be used to show the generalization of the proposed method + How would you adapt your method for other rationalization tasks that are not QA?\n\nWe essentially phrase the multi-choice QA task to be a classification task (Section 4.1), where we generate rationales for a given model prediction (class). Therefore, this can be easily extended to any classification task. We provide results on a range of QA tasks, each of which require varied types of natural language understanding (from factual to logical commonsense to numerical commonsense reasoning); we also show variety in the text structure of the options - StrategyQA requires a yes/no selection across all questions, OpenBookQA, QuaRel, QASC (** new) require a word/phrase to be selected, and Numersense (**new) requires the selection of a number. Furthermore, prior work in generating rationales and reasoning chains like Chain of Thought [1] and other subsequent work, also demonstrate this ability on QA tasks, which has shown to generalize on other tasks. \n\n[1] Wei et al. \u201cChain-of-Thought Prompting Elicits Reasoning in Large Language Models\u201d (NeurIPS 2022)\n\n> Small improvement in Table 2 + Limited Novelty\n\nAs we note in the paper as well, significant improvements over the SFT baseline denoted by *. As it can be seen, for most metrics, training using reward signals as done by MaRio leads to these numerical improvements. Furthermore, these numerical improvements translate to large improvements in human evaluations, where we see that across all of the datasets, annotators prefer MaRio generations more, w.r.t the baseline. \nAdditionally, we noticed that adding multiple rewards for rationale generation was not a trivial addition. In Section 5.2, we can observe that LMs tend to overfit to a reward and hack their way to improving these metrics. We first need to select metrics that are actually important for rationalization, and at the same time, make sure that they are *compatible* in a way so that they can be used together. We hope this can help clarify the motivation and novelty behind our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6506/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700038032627,
                "cdate": 1700038032627,
                "tmdate": 1700038032627,
                "mdate": 1700038032627,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2ZG1dv9fN3",
            "forum": "t8eO0CiZJV",
            "replyto": "t8eO0CiZJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_pMad"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_pMad"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors presented MARIO, a method for training small LMs to generate rationales for question answering. The method is an extension to Quark. While Quark only allows optimizing towards one reward, MARIO allows learning from multiple-reward, allowing the LM to be trained with rewards for Plausibility, Consistency, Diversity and Task-correctness at the same time. The authors evaluated their method on 3 tasks: Strategy QA, QuaRel, and OpenBookQA, and the training rationales are sampled from InstructGPT outputs. The authors showed that their method outperformed baselines on similar-sized small LMs both on automated evaluation metrics and human preference, and can be comparable to some larger LMs on certain tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors tackle an important problem: rationale generation for question answering on small LMs. It is known that rationalization and chain-of-thought can work better on very large language models, but fine-tuning small LMs to correctly rationalize in question-answering tasks has been very challenging.\n\n2. The authors' proposed method allows learning towards multiple rewards, which can be very useful because often we want a model's generation to satisfy multiple desirable properties, and training towards a single property reward can often lead to complete loss of other desirable properties.\n\n3. The authors comprehensively evaluated their method against several baselines on similar-sized LMs, and showed that their method is superior on 3 different QA tasks, both on automated metrics and human preference.\n\n4. The presentation of the paper is generally quite clear. The figures and tables are very well made and they really help make the paper easier and better understood by the reader."
                },
                "weaknesses": {
                    "value": "Overall this is a good paper. Below are a few weaknesses that prevented the paper from getting a \"10\":\n\n1. The paper's main contribution is extending an existing method (Quark) from single-reward to multiple rewards. So while the results are nice and the extension is valuable, the contribution is not revolutionary.\n\n2. While the description in text and Figure 2 are very helpful for readers to understand MARIO, the full picture of MARIO can still be a bit hard to grasp (especially to readers who are not already familiar with Quark). Maybe including an algorithm block for the entire training process will make the full picture a lot more clear."
                },
                "questions": {
                    "value": "(1) Is there any way to rank/weight/balance different objectives in MARIO? (For example, if I found that the resulting model is weak in Consistency, is there a way to weigh Consistency reward a bit more in the training?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6506/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698704518782,
            "cdate": 1698704518782,
            "tmdate": 1699636730103,
            "mdate": 1699636730103,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "82uXa8oly0",
                "forum": "t8eO0CiZJV",
                "replyto": "2ZG1dv9fN3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive review and very helpful comments and suggestions! We are glad to hear that you appreciated our paper\u2019s motivation, method and results. We have also added our response to the feedback and questions below. \n\n> The paper's main contribution is extending an existing method (Quark) from single-reward to multiple rewards. So while the results are nice and the extension is valuable, the contribution is not revolutionary.\n\nThank you for recognizing our contribution and results! As far as novelty is concerned, our idea was that firstly, in prior rationalization literature, rationales were used as a means to an end to improve the accuracy of the system \u2013 as seen in Chain of Thought [1] and other following works. However, often these rationales act as a medium of communication between the LM and user \u2013 to improve trust [2], utility [3] as well as help improve human-LM collaboration [4]. This provided us with a strong motivation to improve the quality of rationales themselves, with an added side benefit of accuracy improvement. Secondly, we noticed that adding multiple rewards for rationale generation was not a trivial addition, as observed in Section 5.2, where we see that LMs tend to overfit to a reward and hack their way to improving these metrics. We first need to select metrics that are actually important for rationalization, and at the same time, make sure that they are compatible in a way so that they can be used together. Our work is an attempt to demystify the same as well. We hope this can help clarify the motivation and novelty behind our work.\n\n[1] Wei et al. \u201cChain-of-Thought Prompting Elicits Reasoning in Large Language Models\u201d (NeurIPS 2022)\n[2] Chen et al. \u201cUnderstanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations\u201d (CSCW 2023)\n[3] Joshi et al. \u201cAre Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales\u201d (ACL 2023)\n[4] Wiegreffe et al. \u201cReframing Human-AI Collaboration for Generating Free-Text Explanations\u201d (NAACL 2022)\n\n\n> While the description in text and Figure 2 are very helpful for readers to understand MARIO, the full picture of MARIO can still be a bit hard to grasp (especially to readers who are not already familiar with Quark). Maybe including an algorithm block for the entire training process will make the full picture a lot more clear.\n\nThank you for this suggestion! We have uploaded new figures to explain our motivation and method better! We also have a more detailed technical figure in the appendix (Figure 5) to explain Quark and MaRio side by side, and we have now included the actual objective function that we optimize for, during training. During camera ready, we will also add the algorithm block in the main text, while we adjust for space!\n\n> Is there any way to rank/weight/balance different objectives in MARIO? (For example, if I found that the resulting model is weak in Consistency, is there a way to weigh Consistency reward a bit more in the training?)\n\nThat\u2019s a great point actually. We started off with using these rewards individually (new single-reward optimization experiments added in Appendix G / Table 10), which showed that improving for one reward doesn\u2019t necessitate improvements for other rewards. This motivated us to look into multi-reward approaches, leading to MaRio. Currently, we haven\u2019t experimented with weighting (up or down) these metrics, and this is definitely an interesting future direction we are looking at!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6506/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037982489,
                "cdate": 1700037982489,
                "tmdate": 1700037982489,
                "mdate": 1700037982489,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lVDtQj8Dk7",
            "forum": "t8eO0CiZJV",
            "replyto": "t8eO0CiZJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_A7zX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_A7zX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces MARIO, a multi-reward approach that enhances the self-rationalization quality of small LMs and the performance of downstream tasks. The authors conducted experiments on three QA datasets and compared the results with several baselines, finding that MARIO can effectively train small LMs to generate rationales, which satisfy multiple distinct properties such as plausibility, diversity, and consistency, while also improving the performance of QA tasks. Additionally, human evaluation was carried out, confirming that the rationales generated by MARIO are more preferred by humans compared to the baselines. Furthermore, the paper discusses the importance of selecting appropriate rewards and preventing MULTI-REWARD HACKING."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The author presents an interesting and valuable research question, namely, how to enhance the self-rationalization quality of small LMs. Building upon the basis of quark, the paper effectively extends its application, utilizing multi-reward conditional generation to optimize both the rationale quality and the performance of downstream tasks. The article clearly explains the criteria for measuring three key aspects of a rationale's properties."
                },
                "weaknesses": {
                    "value": "- The details of the MARIO algorithm are not adequately explained, such as how to determine the settings of control tokens, and the description of how to quantize samples under the quark framework is unclear (is it a comprehensive consideration of multiple attributes for ranking, or is it ranked based on a single attribute?).\n- The description of the MARIO method is overly simplistic, and it lacks the necessary explanation of the thought process behind the development of this method.\n- In relation to self-explaining rationalization, besides the generative rationales discussed in this paper, there is a series of extractive rationale works (such as Lei et al. 2016, Liu et al. 2023, and so on). Beyond the difference between generative and extractive approaches, the basic framework of these two types of work is very similar. Both require ensuring that the generated/extracted rationale is meaningful to humans while maintaining high performance in downstream tasks. Therefore, the related work section should also include this series of works.\n   - Lei et al. 2016, Rationalizing Neural Predictions, EMNLP-2016\n   - Liu et al. 2023, Decoupled Rationalization with Asymmetric Learning Rates, KDD-2023\n- Although Figure 1 and Figure 2 contain a considerable amount of text, the information conveyed is limited.\n- The experiment used multiple baselines, but in reality, it involves two baselines and their multi-reward forms of extension, lacking a comprehensive comparison with other works."
                },
                "questions": {
                    "value": "Although the model utilizes the quark framework, it should clearly present the learning objectives in a multi-reward scenario. Since quark is a single-reward algorithm, its objective function under the extension of multi-reward goals is not intuitive. Could you provide a more detailed and clear explanation of MARIO and the corresponding multi-reward loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6506/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817105322,
            "cdate": 1698817105322,
            "tmdate": 1699636729988,
            "mdate": 1699636729988,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gyyFKeNvsI",
                "forum": "t8eO0CiZJV",
                "replyto": "lVDtQj8Dk7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your positive review and very helpful comments and suggestions! We have added our response below as well as changes to the PDF based on your feedback.\n> The details of the MARIO algorithm are not adequately explained, such as how to determine the settings of control tokens, and the description of how to quantize samples under the quark framework is unclear (is it a comprehensive consideration of multiple attributes for ranking, or is it ranked based on a single attribute?).\n\nThese details are present in the Appendix (due to space constraints). Appendix B (\u201cQuark and MaRio\u201d) addresses how Quark is used in MaRio. Appendix C (\u201cOrder of tokens\u201d) is used to explain how we order our rewards for the multi-reward setup. Figure 5 reiterates over this diagrammatically. In Table 7, we also mention hyper-parameters used by the Quark and MaRio algorithms. We mention here that our method quantized scores into 5 bins for all the rationale reward, and 2 bins for the accuracy reward. \nEven for our multi-reward experiments, the generated data is quantized separately for each individual reward; the final set of control tokens for each data point is the concatenated set of individual control tokens. We have added a clearer description of this in Appendix B as well. \n\n> The description of the MARIO method is overly simplistic, and it lacks the necessary explanation of the thought process behind the development of this method.\n\nIn section 5.2 and 5.3, we explicitly delve into the quirks of selecting good quality rewards, how these rewards actually translate into changes in the generated text, as well as a discussion about why we ended up adding task accuracy as another reward token in our design for MaRio. All of these serve as anchors for our design decisions for MaRio and highlight our thought process for the multi-reward setting. Our core algorithmic framework is based upon Quark itself, which is briefly explained in Appendix B. We hope the changes made in the appendix can help better clarify the method. If there are any other suggestions on how to explain our process better, we\u2019d be more than happy to accommodate that into the draft!\n\n> In relation to self-explaining rationalization, besides the generative rationales discussed in this paper, there is a series of extractive rationale works (such as Lei et al. 2016, Liu et al. 2023, and so on). Beyond the difference between generative and extractive approaches, the basic framework of these two types of work is very similar. Both require ensuring that the generated/extracted rationale is meaningful to humans while maintaining high performance in downstream tasks. Therefore, the related work section should also include this series of works.\n\nQuark as a framework is explicitly designed for text generation, which is why our focus was primarily on free-text rationales. Additionally, we have also included a subsection on extractive rationales in our related work section based on your suggestion!\n\n> Although Figure 1 and Figure 2 contain a considerable amount of text, the information conveyed is limited.\n\nWe acknowledge that Figures 1 and 2 were text dense. We have updated the PDF with new figures to explain our motivation and method better!\n\n> The experiment used multiple baselines, but in reality, it involves two baselines and their multi-reward forms of extension, lacking a comprehensive comparison with other works.\n\nWe compare with (1) the standard baseline SFT [1] , (2) a closely related work StaR [2] and its multi-reward equivalent, (3) with instruction fine tuned and otherwise trained large LLMs [3]. This intends to cover methods that generate free-text rationales by fine-tuning or prompting approaches both. For reward-based rationalization, there are currently no other baselines; we attempt multiple prior methods of incorporating multiple rewards \u2013 e.g product of rewards. If we missed any other methods to compare to, kindly let us know and we\u2019d be more than happy to add it to our comparison pool!\n\n[1] Marasovi\u0107 et al. \u201cFew-Shot Self-Rationalization with Natural Language Prompts\u201d (NAACL 2022)\n[2] Zelikman et al. \u201cSTaR: Bootstrapping Reasoning With Reasoning\u201d (NeurIPS 2022)\n[3] Wei et al. \u201cChain-of-Thought Prompting Elicits Reasoning in Large Language Models\u201d (NeurIPS 2022)\n\n> Although the model utilizes the quark framework, it should clearly present the learning objectives in a multi-reward scenario. Since quark is a single-reward algorithm, its objective function under the extension of multi-reward goals is not intuitive. Could you provide a more detailed and clear explanation of MARIO and the corresponding multi-reward loss?\n\nBased on the Quark algorithm, we have duly updated Appendix B (\u201cQuark and MaRio\u201d) with the objective function of MaRio. This is essentially the same as Quark, except that we have multiple reward tokens on which the generation is conditioned."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6506/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037930481,
                "cdate": 1700037930481,
                "tmdate": 1700037930481,
                "mdate": 1700037930481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SthJc8TXpW",
                "forum": "t8eO0CiZJV",
                "replyto": "gyyFKeNvsI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6506/Reviewer_A7zX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6506/Reviewer_A7zX"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledgement"
                    },
                    "comment": {
                        "value": "Thanks for your reply."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6506/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694492062,
                "cdate": 1700694492062,
                "tmdate": 1700694492062,
                "mdate": 1700694492062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d7OCsU7Rfo",
            "forum": "t8eO0CiZJV",
            "replyto": "t8eO0CiZJV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_3RWN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6506/Reviewer_3RWN"
            ],
            "content": {
                "summary": {
                    "value": "In this work, authors contribute to the task of rationale\u2019s generations in question answering tasks. In previous work, rationalizers are at significant scale,  and ignored the semantics of the rationales themselves. In this work, authors invented the MARIO algorithm for much smaller scale LM's to generate higher quality rationales, with multiple rewards for different quality properties of generated text rationales. Besides, generations from the algorithms seem to be more perferred for human experts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is well-written. The novelty and contribution is clear to me. The authors try not to take advantage of the scalability of large language models and instead use a much smaller distilled version of GPT.  Furthermore, the rewards\u2019 design is aiming at generating rationales with better semantic qualities rather than scoring better at the specific downstream task. I think this is a really good design philosophy for training algorithms."
                },
                "weaknesses": {
                    "value": "I believe the improvement of two versions of Marios compared to baselines is not really significant, considering that all the baseline models have equal number of parameters with the Mario agent model. I\u2019m wondering whether the extra efforts on training on multiple rewards are indeed worth it to improve the generations."
                },
                "questions": {
                    "value": "In the paper, the authors mentioned that there is an initial supervision phase where GPT-3 provides the generation labels. I\u2019m wondering what\u2019s the relationship between this initial supervision process and the distillation of GPT-3? Is it before, after, or exactly the distillation process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6506/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6506/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6506/Reviewer_3RWN"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6506/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699084502946,
            "cdate": 1699084502946,
            "tmdate": 1700711541433,
            "mdate": 1700711541433,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Fe7UGT7NmS",
                "forum": "t8eO0CiZJV",
                "replyto": "d7OCsU7Rfo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6506/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your helpful comments and insightful questions! We are happy to hear that you appreciated our method\u2019s novelty, constraints, as well as design methodology. Based on your feedback, we have updated the draft and provided the responses below. \n\n> I believe the improvement of two versions of Marios compared to baselines is not really significant, considering that all the baseline models have equal number of parameters with the Mario agent model. I\u2019m wondering whether the extra efforts on training on multiple rewards are indeed worth it to improve the generations.\n\nWhile we do note that the baselines and MaRio have equal number of parameters, in Table 3 (previously, Table 2), we note significant improvements over the SFT baseline denoted by *. As it can be seen, for most metrics, training using reward signals as done by MaRio leads to these numerical improvements. Another reason why these extra efforts are helpful are seen in human evaluations, as shown by Figure 3. Across all of the datasets, annotators prefer MaRio generations more, while also noting improvements across these quality metrics, w.r.t the baseline. This can help us understand that small improvements numerically lead to large improvements in actual generations, thereby directly improving preference of these generations. \n\n> In the paper, the authors mentioned that there is an initial supervision phase where GPT-3 provides the generation labels. I\u2019m wondering what\u2019s the relationship between this initial supervision process and the distillation of GPT-3? Is it before, after, or exactly the distillation process? \n\nWe use the rationales provided by GPT-3 in the following 3 places: (1) we use it to train the supervised fine-tuned model SFT, which serves as a baseline, (2) we use the aforementioned SFT as the reference model for the KL divergence loss in MaRio\u2019s training process (as we mention in Appendix), and (3) we also add these silver rationales to the overall data pool of MaRio. We have updated the PDF of the paper to re-reflect mentions of these throughout the draft."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6506/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037864348,
                "cdate": 1700037864348,
                "tmdate": 1700037864348,
                "mdate": 1700037864348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p1nZpNiXSl",
                "forum": "t8eO0CiZJV",
                "replyto": "Fe7UGT7NmS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6506/Reviewer_3RWN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6506/Reviewer_3RWN"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for addressing my questions. Now I understand better about the distillation process and where the empirical improvement is. I decide to increase my score to 6."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6506/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711526694,
                "cdate": 1700711526694,
                "tmdate": 1700711526694,
                "mdate": 1700711526694,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]