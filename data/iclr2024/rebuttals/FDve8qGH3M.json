[
    {
        "title": "Simple CNN for Vision"
    },
    {
        "review": {
            "id": "caPzvtKxyb",
            "forum": "FDve8qGH3M",
            "replyto": "FDve8qGH3M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7100/Reviewer_tQCD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7100/Reviewer_tQCD"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a clear background on the significance of large kernel convolutions in the current landscape of CNNs. The authors also clearly demonstrated their motivations for SCNNs and showed that a sequence of stacked 3\u00d73 convolutions surpasses state-of-the-art CNNs utilizing larger kernels. The introduction provides context, discusses the trend towards large kernels, and then sets the stage for their proposition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The approach of using a sequence of stacked 3\u00d73 convolutions to surpass state-of-the-art CNNs employing larger kernels is innovative.\n\n2. The technical sounds solids, including the actual implementation specifics of SCNN, and corresponding theoretical explanations."
                },
                "weaknesses": {
                    "value": "1. Experiments: The paper should delve deeper into the experimental setup, data augmentation techniques, training specifics, and more. For example, it is unclear to compare with other baselines that have different parameter sizes. The author should clarify this section.\n\n2. Lack of Visualizations: Additional figures visualizing feature maps or demonstrating how the receptive field increases would offer more insights into the workings of SCNN."
                },
                "questions": {
                    "value": "Please refer to the weakness, W1"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7100/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7100/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7100/Reviewer_tQCD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7100/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698524086757,
            "cdate": 1698524086757,
            "tmdate": 1699636838210,
            "mdate": 1699636838210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A0J6QoJU8S",
                "forum": "FDve8qGH3M",
                "replyto": "caPzvtKxyb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7100/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7100/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tQCD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback. We make point-to-point clarification for your concerns as follows:\n\n**Q1:** The paper should delve deeper into the experimental setup, data augmentation techniques, training specifics, and more. For example, it is unclear to compare with other baselines that have different parameter sizes. The author should clarify this section.\n\n**A1:** Thanks for your comment. All of our settings are the same as Swin Transformer and ConvNeXt, we have not made any further adjustments. \n\n**Q2:** Lack of Visualizations: Additional figures visualizing feature maps or demonstrating how the receptive field increases would offer more insights into the workings of SCNN.\n\n**A2:** We use the script in RepLKNet(https://github.com/DingXiaoH/RepLKNet-pytorch/tree/main/erf) to visualize the receptive field, and put these images in the **appendix (page 12)**. For models trained in imagenet-1k, all of them could capture a large receptive field. After adding more convolutions and GSiLU, the features become more discriminative and the final model focuses more on center and edge areas."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682442528,
                "cdate": 1700682442528,
                "tmdate": 1700682442528,
                "mdate": 1700682442528,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1yiF1XMZB2",
            "forum": "FDve8qGH3M",
            "replyto": "FDve8qGH3M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7100/Reviewer_of72"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7100/Reviewer_of72"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Simple Convolutional Neural Network (SCNN) that uses only 3x3 convolutions but outperforms models with larger kernels. The main ideas are: (1) Thin and deep architecture with more 3x3 layers to capture spatial information under compute constraints. (2) Stacking two 3x3 depthwise convolutions to enlarge receptive field. (3) Using global average pooling in activation (GSiLU) to capture global information. The model is evaluated on ImageNet classification, COCO detection/segmentation, and ADE20K segmentation, achieving improved results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper comprehensively validates the effectiveness and efficiency of the proposed SCNN architecture. It outperforms models with larger kernels like ConvNeXt and SLaK on ImageNet classification by up to 0.7% top-1 accuracy with less FLOPs (Table 2). For dense prediction tasks, SCNN as a backbone for Mask R-CNN improves COCO object detection AP by 0.9% over Swin Transformer while requiring lower computation (Table 3). Similarly, SCNN exceeds Swin Transformer by 2.6% mIoU on ADE20K semantic segmentation with comparable FLOPs (Table 4). Detailed ablation studies demonstrate the impact of key components like the thin and deep architecture, double 3x3 convolutions, and global context modeling with GSiLU."
                },
                "weaknesses": {
                    "value": "\u2022\tThe paper lacks some analysis on how the proposed architecture captures spatial context and increases receptive field size. The introduction describes this as a key motivation, but there is little discussion in the experiments. Some visualization or measurements of the receptive field size could provide more insight.\n\u2022\tThe paper does not discuss in detail some other related work on improving convolutional backbones, such as [1,2]. Comparing and contrasting with these methods could highlight the novelty of SCNN's approach.\n[1] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu:Visual attention network. Comput. Vis. Media 9(4): 733-752 (2023)\n[2] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao: PVT v2: Improved baselines with Pyramid Vision Transformer. Comput. Vis. Media 8(3): 415-424 (2022)\n\u2022\tBeyond incremental improvements on established benchmarks, in-depth insights or observations about the properties of the SCNN architecture could increase the depth of the contribution."
                },
                "questions": {
                    "value": "For global context modeling, how does GSiLU compare to other approaches like SENet? I think the global context modeling in SCNN is not very different from widely used techniques like SE modules\uff08Channel Attention for Convolution\uff09. The paper could benefit from more comparison and discussion about the relative merits of GSiLU.\n\nSome other concerns are in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7100/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7100/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7100/Reviewer_of72"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7100/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698677308179,
            "cdate": 1698677308179,
            "tmdate": 1699636838077,
            "mdate": 1699636838077,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o979Im1DvQ",
                "forum": "FDve8qGH3M",
                "replyto": "1yiF1XMZB2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7100/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7100/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer of72"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the time and efforts you have dedicated to reviewing our paper.  We thank your suggestions and make point-to-point clarification to the weaknesses and your questions as follows:\n\n**Q1:** The paper lacks some analysis on how the proposed architecture captures spatial context and increases receptive field size. The introduction describes this as a key motivation, but there is little discussion in the experiments. Some visualization or measurements of the receptive field size could provide more insight. \n\n**A1:** We use the script in RepLKNet(https://github.com/DingXiaoH/RepLKNet-pytorch/tree/main/erf) to visualize the receptive field, and put these images in the **appendix (page 12)**. For models trained in imagenet-1k, all of them could capture a large receptive field. After adding more convolutions and GSiLU, the features become more discriminative and the final model focuses more on center and edge areas.\n\n**Q2:** The paper does not discuss in detail some other related work on improving convolutional backbones, such as [1,2]. Comparing and contrasting with these methods could highlight the novelty of SCNN's approach. [1] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu:Visual attention network. Comput. Vis. Media 9(4): 733-752 (2023) [2] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao: PVT v2: Improved baselines with Pyramid Vision Transformer. Comput. Vis. Media 8(3): 415-424 (2022) \u2022 Beyond incremental improvements on established benchmarks, in-depth insights or observations about the properties of the SCNN architecture could increase the depth of the contribution. \n\n**A2:** We cite and compare with PVT in instance segmentation tasks, but we overlook PVT v2 and VAN because they are published in a newly established journal (2015) we overlook. We will cite them in the final version. Compared with PVT v2, SCNN-T gets **the same accuracy (83.2%) with only 65% FLOPs (4.5G vs 6.9G)**. Compared with VAN, SCNN-T obtains **better results (83.2% vs 82.8%) with fewer FLOPs (4.5G vs 5.0G)**.\n\n**Q3:** For global context modeling, how does GSiLU compare to other approaches like SENet? I think the global context modeling in SCNN is not very different from widely used techniques like SE modules\uff08Channel Attention for Convolution\uff09. The paper could benefit from more comparison and discussion about the relative merits of GSiLU.\n\n**A3:** However, both GSiLU and SE belong to gated activation functions. And replacing GSiLU with SE, the performance gain is marginal (from 83.2% to 83.3). For a fair comparison, we utilize GSiLU because both SiLU and GSiLU are parameter-free activation functions, and we only add a global average pooling. SE requires an extra MLP, which introduces a large number of parameters."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682397024,
                "cdate": 1700682397024,
                "tmdate": 1700682397024,
                "mdate": 1700682397024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ktfnCBmSyO",
            "forum": "FDve8qGH3M",
            "replyto": "FDve8qGH3M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7100/Reviewer_SZ8J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7100/Reviewer_SZ8J"
            ],
            "content": {
                "summary": {
                    "value": "In the provided submission, the authors present a Simple Convolutional Neural Network (SCNN) that using only 3x3 depthwise convolutions, outperforms CNNs that employ larger kernels. A notable enhancement is the incorporation of global average pooled features into the Sigmoid Linear Unit (SiLU) activation function, the paper named it GSiLU, which enables these small kernels to capture comprehensive spatial information."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1) The paper effectively challenges the notion that larger kernels are the way for CNN advancements.\n2) The presented model is more lightweight than its counterparts, achieving comparable performance with the same FLOPs.\n2) The authors have conducted a robust suite of experiments to validate their model."
                },
                "weaknesses": {
                    "value": "1) The paper could benefit from a more polished and professional tone in its presentation and writing style.\n2) Some of the claims or hypotheses put forth lack empirical validation through experiments, which could strengthen the paper's assertions.\n3) While it's understandable given potential computational constraints, the model was not trained on ImageNet 21K, a limitation that might affect generalization claims.\n4) The paper doesn't explore the performance of larger-sized versions of their model. Although this might be due to resource limitations, such exploration could provide additional insights into the model's scalability and robustness."
                },
                "questions": {
                    "value": "1) Regarding Table 5's ablation analysis on the SCNN block, it is unclear why the number of parameters and FLOPs remain constant when PreConv and MidConv are removed. Could the authors clarify if there were any mechanisms employed to maintain these metrics, and if so, elaborate on the methodology used?\n2) The paper describes the use of both Layer Normalization (LN) and Batch Normalization (BN) within the architecture, with LN explicitly employed as the initial normalization layer. Could the authors explain the reason behind this specific arrangement? Furthermore, it would be beneficial if the authors could provide an ablation study examining the impact of the positioning of these normalization layers within the network.\n3) The paper describes the use of both SiLU and GSiLU activation functions within the architecture, with the use of SiLU as the first activation layer. Could the authors provide insight into the reason underpinning this specific sequence?\n4) The claim that GSiLU enhances the performance of small 3x3 kernels by capturing global spatial information warrants empirical validation. Could the authors clarify if GSiLU doesn't have similar benefits to larger kernel sizes?\n5) The functionality of GSiLU, which zeros out channels with sufficiently negative averages, prompts a request for statistical analysis. Could the authors provide data on the proportion of channels that are effectively zeroed by the GSiLU activation in practice?\n6) As part of a review process, I believe it is expected to validate the findings through an independent examination of the code and model (Hence, my low confidence score.). To maintain the integrity of my review, I request access to the relevant code and model. This will enable me to verify the results personally."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7100/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771432150,
            "cdate": 1698771432150,
            "tmdate": 1699636837967,
            "mdate": 1699636837967,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jwbY7HC11t",
                "forum": "FDve8qGH3M",
                "replyto": "ktfnCBmSyO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7100/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7100/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SZ8J"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your comprehensive review and positive feedback on our work. Below, we address the weaknesses and questions.\n\n**Weaknesses**\n\n**Q1:** The paper could benefit from a more polished and professional tone in its presentation and writing style.\n\n**A1:** Thank you, we will polish the manuscript. \n \n**Q2:** Some of the claims or hypotheses put forth lack empirical validation through experiments, which could strengthen the paper's assertions.\n\n**A2:** Thank you, we will clearly check our claims or hypotheses and provide more references, more experiments, and more visualizations in the final version.\n\n**Q3:** While it's understandable given potential computational constraints, the model was not trained on ImageNet 21K, a limitation that might affect generalization claims. \n\n**A3:** ImageNet 21K is too large for our limited resources. We need more time to ask for help from technology companies to do this experiment in the final version.\n\n**Q4:** The paper doesn't explore the performance of larger-sized versions of their model. Although this might be due to resource limitations, such exploration could provide additional insights into the model's scalability and robustness. \n\n**A4:** Due to the limitation of resources, we train the base version of the model (15.4G). We will try to ask for help from some technology companies, to train a larger version (e.g. 50G FLOPs).\n\n**Questions**\n\n**Q5:** Regarding Table 5's ablation analysis on the SCNN block, it is unclear why the number of parameters and FLOPs remain constant when PreConv and MidConv are removed. Could the authors clarify if there were any mechanisms employed to maintain these metrics, and if so, elaborate on the methodology used?\n\n**A5:** We use the THOP (https://github.com/Lyken17/pytorch-OpCounter) to measure the FLOPs. Both PreConv and MidConv are depthwise convolutions. The computing complexity of them is less than 0.1G FLOPs, thus, reduced FLOPs are ignored due to round-up and round-down. For example, both 4.549G and 4.451G are represented as 4.5G. \n\n**Q6:** The paper describes the use of both Layer Normalization (LN) and Batch Normalization (BN) within the architecture, with LN explicitly employed as the initial normalization layer. Could the authors explain the reason behind this specific arrangement? Furthermore, it would be beneficial if the authors could provide an ablation study examining the impact of the positioning of these normalization layers within the network. \n\n**A6:** To train the model faster, we use apex in PyTorch to train an fp16-fp32 mixed model. We find removing the LN will lead to a numeric overflow problem in the base version (15.4G). In tiny version (4.5G) and small version (8.7G), with or without LN, the result remains unchanged. \n\n**Q7:** The paper describes the use of both SiLU and GSiLU activation functions within the architecture, with the use of SiLU as the first activation layer. Could the authors provide insight into the reason underpinning this specific sequence?\n\n**A7:** SiLU has a similar formulation to GELU and Swish, which are widely used in most modern backbones. Thus, it is our default setting to use SiLU in all positions. And, we find replacing the second SiLU with GSiLU will obtain a better result (+0.1%) compared with replacing the first one. \n\n**Q8:** The claim that GSiLU enhances the performance of small 3x3 kernels by capturing global spatial information warrants empirical validation. Could the authors clarify if GSiLU doesn't have similar benefits to larger kernel sizes?\n\n**A8:** We use the script in RepLKNet(https://github.com/DingXiaoH/RepLKNet-pytorch/tree/main/erf) to visualize the receptive field, and put these images in the **appendix (page 12)**. For models trained in imagenet-1k, all of them could capture a large receptive field. After adding more convolutions and GSiLU, the features become more discriminative and the final model focuses more on center and edge areas.\n\n**Q9:** The functionality of GSiLU, which zeros out channels with sufficiently negative averages, prompts a request for statistical analysis. Could the authors provide data on the proportion of channels that are effectively zeroed by the GSiLU activation in practice?\n\n**A9:** In the tiny version, the stage one block has 64 channels, we find there is no zero value in these channels, and more than 50% are between 0.3 and 0.7. We think the sigmoid function makes it difficult to generate a zero-value channel. \n\n**Q10:** As part of a review process, I believe it is expected to validate the findings through an independent examination of the code and model (Hence, my low confidence score.). To maintain the integrity of my review, I request access to the relevant code and model. This will enable me to verify the results personally.\n\n**A10:** All codes and models will be public after being accepted. If it is rejected, we will make this paper, codes, and models public in arXiv and github soon."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682016108,
                "cdate": 1700682016108,
                "tmdate": 1700682016108,
                "mdate": 1700682016108,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y69RHcSrqT",
            "forum": "FDve8qGH3M",
            "replyto": "FDve8qGH3M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7100/Reviewer_gmsA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7100/Reviewer_gmsA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Simple Convolution Neural Networks (SCNN) for a bunch of fundamental vision tasks (classification, detection segmentation). It conducts extensive comparison to existing improvements over CNN such as ConvNeXt, RepLKNet, and ViTs. Results shows that it could achieve superb results comparing to those SOTA CNNs and ViTs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The presentation is clear and the idea is fairly simple."
                },
                "weaknesses": {
                    "value": "In Figure 2, putting aside SILU and GSILU, the architecture of SCNN looks very similar to mobilenet and its variants; Could the authors provide results comparison to MobileNet with the same layers, depth and width, but without SILU and GSILU blocks? I am curious whether the improvement is coming from SILU or GSILU."
                },
                "questions": {
                    "value": "It claims GSILU leads to rich spatial information, could you provide a receptive field size analysis/illustration when comparing network with or without SILU and GSILU?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7100/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843813048,
            "cdate": 1698843813048,
            "tmdate": 1699636837855,
            "mdate": 1699636837855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OGraPmQkNO",
                "forum": "FDve8qGH3M",
                "replyto": "y69RHcSrqT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7100/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7100/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gmsA"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your comment which will help us strengthen the manuscript. We perform point-to-point clarification for your concerns as follows:\n\n**Q1:** Could the authors provide results comparison to MobileNet with the same layers, depth, and width, but without SILU and GSILU blocks? I am curious whether the improvement is coming from SILU or GSILU.\n\n**A1:** The top-1 accuracy of MobileNetV2 with the same settings you mentioned is **81.9% under 4.5G FLOPs**. Notably, by replacing the ReLU activation with SiLU functions, it becomes **82.6% under 4.4G FLOPs**, which is recorded in Tabel 5 line4. It is clear to demonstrate the effectiveness. \n\n**Q2:** It claims GSILU leads to rich spatial information, could you provide a receptive field size analysis/illustration when comparing network with or without SILU and GSILU?\n\n**A2:** We update the paper and add a visualization of the receptive field to the **appendix (page 12)**. It is clear that for the model trained in imagenet-1k, replacing SiLU with GSiLU makes the features more discriminative, thus the final model focuses more on center and edge areas. Specifically, we use the script in RepLKNet(https://github.com/DingXiaoH/RepLKNet-pytorch/tree/main/erf) to visualize the receptive field."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681911892,
                "cdate": 1700681911892,
                "tmdate": 1700681911892,
                "mdate": 1700681911892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SKmFfuDuay",
                "forum": "FDve8qGH3M",
                "replyto": "y69RHcSrqT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7100/Reviewer_gmsA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7100/Reviewer_gmsA"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing extra results to answer my concerns. \n\nI have a follow up question for Table-5. \nAccording to your response, Line-4 is MobileNet-V2,  Line-2 is SCNN. \nWhat about MobileNet-V2 + GSILU (PreConv + GSiLU), since in Figure-2, GSiLU is not replacing BN, but combing with BN to provide effects. \n\nBesides, why do you add SiLU after first BN, but add GSiLU after 2nd BN. It is that based on experimental study?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7100/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712173355,
                "cdate": 1700712173355,
                "tmdate": 1700712227024,
                "mdate": 1700712227024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]