[
    {
        "title": "Forget-Me-Not: Making Backdoor Hard to be Forgotten in Fine-tuning"
    },
    {
        "review": {
            "id": "Q8eXnPIi3T",
            "forum": "T23HYw6lta",
            "replyto": "T23HYw6lta",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7351/Reviewer_EHtc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7351/Reviewer_EHtc"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to maintain stable attack performance of inserted backdoor triggers during robust tuning process. Specific to current superFT method, the authors also adopt a cyclical learning rate scheme during inserting backdoor triggers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The studied problem is important. How to construct more robustly inserted backdoor triggers is interesting."
                },
                "weaknesses": {
                    "value": "Limitations and lacking of ture explanations.\n1. Limitations:\n- The proposed method is limited: The proposed attack method is specific to superFT which adopt a cyclical learning rate scheme. It is not clear whether other defense methods are equally effective, such as ANP [1]. I believe adopting a cyclical learning rate scheme is not a proper choice for inserting backdoors. The main reason is that it is very hard for attackers to tune its hyperparameters. The authors do not provide any details about how to choose parameters for this cyclical learning rate scheme. I also notice that the authors only evaluate ResNet models. Do we need different hyperparameters on different models?\n- As mentioned in above point, the authors do not evaluate other pruning-based methods like ANP [1] and RNP [2]. I think the author need to evaluate these defense methods to show effectiveness of proposed attack method. \n- Comparsions with existing advanced backdoor attacks: [3] proposed more stealthy and robust backdoor attacks without controlling training process. And, I also strongly suggest that the authors should evaluate more diverse attack settings like lower poisoning rate (1%).\n\n2. Lacking of ture explanations: The intuition behind proposed method is too hauristic. Apart from evaluation of ASR, the authors do not provide any evaluation about proposed hypothesis. Actually, we are not sure whether it leads to more flat and stable minimum with inserted triggers. The author could provide loss landscape analysis to verify this point.  \nI think the authors are not familiar with loss landscape anaysis of DNN models. Adopting a cyclical learning rate scheme can not gaurantee searching a flat and stable local min. We could adopt SWA [4] to achieve this goal. \n\n[1] Adversarial Neuron Pruning Purifies Backdoored Deep Models.  \n[2] Reconstructive Neuron Pruning for Backdoor Defense.   \n[3] Revisiting the Assumption of Latent Separability for Backdoor Defenses.  \n[4] Averaging Weights Leads to Wider Optima and Better Generalization."
                },
                "questions": {
                    "value": "Please see Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697878372210,
            "cdate": 1697878372210,
            "tmdate": 1699636879698,
            "mdate": 1699636879698,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MEKzLKpxuc",
                "forum": "T23HYw6lta",
                "replyto": "Q8eXnPIi3T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EHtc"
                    },
                    "comment": {
                        "value": "### **1. The proposed attack method is specific to superFT** \nIt is important to note that our training strategy is designed to counteract all finetuning-based defenses, despite being inspired by the idea of cyclical learning rate scheme used in superFT. As demonstrated in our paper, FMN effectively improves the attack robustness against a wide range of finetuning-based defenses, including vanilla finetuning, superFT, FT-SAM, and NAD.\n### **2. It is not clear whether other defense methods are equally effective, such as ANP.** \nSuperFT is a quite strong defense, and in many cases it is more effective than other defenses such as ANP. One case study is with LIRA attack. As reported in Table 2 in our main paper, superFT can effectively mitigates the original LIRA attack. In constrast, as reported in Table 7 in our Appendix, ANP fails to mitigate this attack; it cannot reduce the ASRs without significant drops in BA.\n### **3. How to choose parameters for this cyclical learning rate scheme? Experiments with different model architectures**\nWe use the same hyper-parameters used in superFT for our cyclical learning rate scheme. We found this configuration is effective, and there is no need to tune these hyperparameters.\nWe have run additional experiments utilizing various model architectures while maintaining the hyperparameters consistent with those employed for ResNet in our original study.\nWe report the BA/ASR in the tables below. As shown, our method still achieves high ASR and remains effective against advanced fine-tuning defenses, such as super-FT and FT-SAM, across various architectures. We have included the results in the revised Appendix.\n| Model | Attack | No defense | Super-FT | FT-SAM\n|-------| -------- | -------- | -------- | --------\n| VGG16 | Blend + FMN  | 91.21/97.21  | 89.66/87.92  | 90.13/93.06 \n| VGG16 |  Trojaning + FMN |  90.06/99.63 |  88.79/89.96 |  89.25/93.34\n| MobileNetv2 | Blend + FMN  | 93.73/98.99 | 91.44/93.29  | 91.62/95.82\n| MobileNetv2 | Trojaning + FMN | 93.57/99.97 | 90.59/93.54 |  90.94/96.07\n### **4. Evaluation with pruning-based method**\nWe indeed evaluated one instance of our attack, LIRA+FMN, against pruning-based defenses, including Fine-pruning and ANP, and reported the results in Section 4.6 of the main paper and Section A.6 of the Appendix. LIRA+FMN bypassed both defenses. Regarding the reviewer's recommendation, we have additionally conducted experiments with RNP and reported the results in the table below. \n| Attack | No defense | RNP |\n| ----- | ----- | ----- |\n| LIRA | 94.42/100 | 92.05/16.67 |\n| LIRA + FMN | 94.26/100 | 91.94/16.89 |\n\nIt is important to note that our training strategy is specifically designed to enable existing attacks, such as LIRA, to counteract finetuning-based defenses. The robustness against other types of defenses will depend on the choice of the specific attack employed. As shown in the case study with LIRA+FMN, our method does not diminish the attack's effectiveness against other defense approaches."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740379308,
                "cdate": 1700740379308,
                "tmdate": 1700741222774,
                "mdate": 1700741222774,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MJEjA6xVAC",
            "forum": "T23HYw6lta",
            "replyto": "T23HYw6lta",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7351/Reviewer_Rq4g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7351/Reviewer_Rq4g"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents FMN, a novel attack method to strength trojaning attacks (backdoor attacks) against DNNs. The key components of FMN are  the cyclical learning rate and the clean-backdoor interleaved training. The experimental results in this paper show that FMN successfully strengthens several existing trojaning attacks against several existing defense methods, i.e., the ASR remains high even if the backdoored model is purified by the defense methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.FMN is compatible with various backdoor attacks.\n2.The experimental results show that FMN can significantly increase the ASR of the mitigated backdoored model."
                },
                "weaknesses": {
                    "value": "1.This paper should contain more analysis about the reason why  the cyclical learning rate and the clean-backdoor interleaved training work.\n2.Though empowering existing backdoor attacks is an interesting idea, this paper should further investigate how to defend against FMN-powered backdoor attacks."
                },
                "questions": {
                    "value": "1.What is the reason that the cyclical learning rate and the clean-backdoor interleaved training can work?\n2.Are there any possible defense strategies against FMN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569653661,
            "cdate": 1698569653661,
            "tmdate": 1699636879568,
            "mdate": 1699636879568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4zIDh5CPDd",
                "forum": "T23HYw6lta",
                "replyto": "MJEjA6xVAC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rq4g"
                    },
                    "comment": {
                        "value": "### **1. Analysis about why the cyclical learning rate and the clean-backdoor interleaved training work**\nOur experiments in ablation studies confirm the effectiveness of the combination of two training strategies: the cyclical learning rate and clean-backdoor interleaved training. Using only one strategy will lead to a low attack success rate. The cyclical learning rate helps to find flat areas, and the clean-backdoor interleaved training guarantees that it is the shared flat area of both function losses. \n\nWe compare the flatness of losses of poisoned data and clean data sets between different methods to illustrate our arguments. Figure 6 in Appendix shows that the landspace of our loss function at the parameter solution is flatter than that of other methods.  \n\nWe also provide empirical intuition of why FMN is effective against finetuning-based defenses. Figure 7 in the revised Appendix shows the ASR and BA  along the connectivity path of the poisoned model and its corresponding fine-tuned version after undergoing the fine-tuning process of Super-FT defense. As can be observed, with conventional backdoor learning (left figure), when we linearly interpolate from the backdoored model to its corresponding Super-FT's fine-tuned version, the intermediate model's poisoning loss (i.e., the loss recorded only on the poisoned samples) increases, resulting in the decrease in ASR, while their clean losses and BAs are approximately stable. On the other hand, with FMN training (right figure), linearly interpolating between the backdoor and its corresponding Super-FT's fine-tuned model, the poisoning loss and ASR, as well as the clean loss and BA, are almost constant, indicating that FMN learns the backdoor in a region that makes it difficult for a fine-tuning defense to escape from.\n\n### **2. Possible defense strategies against FMN**\nOur research highlights the potential risks of relying on third-party pre-trained models and underscores the importance of fostering trust between users and model providers. To defend against our attack, users should only use pre-trained models from trusted providers or actively participate in the training process. We also urge the research community to delve further into this area to develop more robust safeguards. \n\nWe have included this discussion in the conclusion of the revised manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740330954,
                "cdate": 1700740330954,
                "tmdate": 1700740625114,
                "mdate": 1700740625114,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B1VzFkEFAN",
            "forum": "T23HYw6lta",
            "replyto": "T23HYw6lta",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7351/Reviewer_WyV7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7351/Reviewer_WyV7"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores a backdoor attack method called \"Forget Me Not\", which aims to overcome the catastrophic forgetting problem during model fine-tuning. Specifically, this paper proposes a backdoor learning method based on a cyclic learning rate policy that enhances the persistence of existing backdoor methods. The aforementioned method can bypass fine-tuning-based backdoor defenses and maintain effectiveness during complex fine-tuning processes. The authors demonstrate the effectiveness of the proposed method on three popular benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Clear background introduction. the article provides readers with a thorough review of backdoor attacks and related work in the introductory section, providing good background knowledge.\n2. Reasonable experimental design. The experimental setup and presentation of results in the article are clear, providing readers with an intuitive sense of the effectiveness of the method.\n3. Clear charts and graphs. the charts and graphs are well-designed and help readers understand the content of the article."
                },
                "weaknesses": {
                    "value": "1. Insufficiently detailed description of the methodology. When describing the \"Forget Me Not\" method, the details in some parts of the article are not clear enough. It is suggested that the authors provide more detailed algorithm description or pseudo-code in the method section so that readers can understand and reproduce better. 2.\n2. The related work section can be expanded. Although the article has listed some related works, there are other important works in the field of backdoor attacks that can be referenced. In addition, I would like to know the rationale or justification for choosing these seven attack methods as representative methods. It is not possible for the authors to exhaust all the attack methods and prove the enhancement of backdoor persistence by the method, but I think representative methods need to be chosen to prove the comprehensiveness of the experiments.\n3. Results of other defense experiments. Although the authors compare many fine-tuning-based defense methods to prove the effectiveness of the proposed backdoor, I am still concerned about whether the existing attack methods are able to overcome the existing backdoor defense methods, such as data cleansing methods, model modification methods, and model validation methods.\n4. Analysis of defense strategies. Considering the potential threat of backdoor attacks, it is recommended that the authors discuss possible defense strategies or propose corresponding challenges in their articles."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7351/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7351/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7351/Reviewer_WyV7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731840882,
            "cdate": 1698731840882,
            "tmdate": 1699636879437,
            "mdate": 1699636879437,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7iIcrPFfb5",
                "forum": "T23HYw6lta",
                "replyto": "B1VzFkEFAN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WyV7"
                    },
                    "comment": {
                        "value": "### **1. More detailed description of the methodology**\nWe thank the reviewer for the suggestion. We have added a detailed Algorithm block for FMN in the revised Appendix.\n\n### **2. Rationale for choosing these seven attack methods**\nThe chosen methods are representatives for different backdoor attack approaches:\n- BadNets and Blended: standard attacks with random trigger patterns. BadNets uses an image patch, while Blended uses a blended image as the trigger. \n- $L_p$ regularization and trojaning: optimized trigger patterns\n- Input-aware: image-dependent trigger\n- LIRA: imperceptible optimized trigger\n- Narcissus: optimized trigger, clean-label attack\n\nWe have revised the manuscript to include the rationales.\n\n### **3. Results of other defense experiments, such as data cleansing methods, model modification methods, and model validation methods**\nIt is important to note that our training strategy is specifically designed to enable existing backdoor attacks to bypass finetuning-based defenses. Consequently, the robustness against other types of defenses will depend on the choice of attack used with our training process. \n\nAs shown in the case study with LIRA+FMN in Section 4.6 of the main paper and Section A.6 of the Appendix, our method does not diminish the attack's effectiveness against other defense approaches. LIRA+FMN is robust against various defense mechanisms, including Neural Cleanse, Fine-pruning, ANP, and STRIP. Note that we test only representative backdoor defenses that LIRA has already bypassed. Also, we do not include data cleansing defense from our evaluation because our threat model assumes that the adversary provides the backdoored model to the user for deployment rather than supplying malicious data. \n\n### **4. Discussion of possible defense strategies**\nWe thank the reviewer for the suggestion. Our research highlights the potential risks of relying on third-party pre-trained models and underscores the importance of fostering trust between users and model providers. To defend against our attack, users should only use pre-trained models from trusted providers or actively participate in the training process. We also urge the research community to delve further into this area to develop more robust safeguards.\n \nWe have included this discussion in the conclusion of the revised manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740322433,
                "cdate": 1700740322433,
                "tmdate": 1700740322433,
                "mdate": 1700740322433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "P5TWmAOM22",
            "forum": "T23HYw6lta",
            "replyto": "T23HYw6lta",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7351/Reviewer_xTDh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7351/Reviewer_xTDh"
            ],
            "content": {
                "summary": {
                    "value": "The key contribution of this paper is to demonstrate that fine-tuning (regardless of the techniques used) is not a proper defense against backdoor attacks. The paper specifically evaluates the two fine-tuning approaches: super fine-tuning and FT-SAM by Zhu et al. By simulating their fine-tuning mechanisms in backdoor training, the attacker can inject backdoors more robust to the fine-tuning techniques. In evaluation with various backdoor attacks, the paper shows their backdoor attacks become more resilient against fine-tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper shows (advanced) fine-tuning cannot be a backdoor defense.\n2. The paper runs extensive experiments to validate their claims.\n3. A well-written paper; easy to follow."
                },
                "weaknesses": {
                    "value": "1. Unfortunately, we don't believe fine-tuning can be a backdoor defense.\n2. The paper is written to prove the point that we already believe.\n3. The experimental results are not 100% convincing.\n\nDetailed comments:\n\nI like this paper showing (or re-confirming) that fine-tuning cannot be an effective defense against backdoor attacks. Even if there are manuscripts making bold claims like \"fine-tuning is effective,\" I don't believe that it is the case: their positive results are coming either (1) from running fine-tuning with longer epochs or large learning rates (often) or (2) from an adversary unaware of their fine-tuning methods.\n\nSo, I am a bit positive to have this paper as empirical evidence showing that existing claims are not based on a concrete security analysis.\n\n----\n\nHowever, we also know that fine-tuning cannot be a defense; a vast literature on backdoor attacks evaluated fine-tuning and confirmed that it is ineffective (note that it is not against this advanced fine-tuning). We already have a skepticism about fine-tuning. \n\nSo, on the other hand, it is less scientifically interesting to prove that we already know with empirical evaluation. I am a bit confident that even if the two prior works are out to the community, no one will believe that fine-tuning can become an effective countermeasure.\n\n----\n\nI also find that the paper puts a lot of effort into emphasizing fine-tuning as a defense seriously considered in the community. But it often gives an incorrect view of the prior work, which I want the authors fixing them before this manuscript will be out to the community.\n\nFor example, papers like NeuralCleanse do not consider fine-tuning as a primary mean to defeat backdoors. The key idea was to \"reconstruct\" the trigger from a set of assumptions about backdooring adversaries. Once we know what was used as a backdoor trigger, the fine-tuning is a natural next step to \"unlearn\" the backdoor behaviors. It is not the same as one uses fine-tuning without knowing the trigger, which should be addressed and fixed in the paper. \n\nI found more like this in the introduction and backdoor defense section.\n\n----\n\nFinally, sometimes fine-tuning reduces their attack's success rate. This (as-is) can be shown as the effectiveness of fine-tuning (as at least the success rate has been decreased). \n\nTo be a more concrete claim, the results have to be compared with a baseline. What would be the baseline? The cases where we reduce the attack success rate to 50%? It was not clear in the paper; therefore, the claims discussing the effectiveness of fine-tuning can also be controversial ---even if I don't believe that fine-tuning works.\n\n----\n\nAt the moment, I am slightly leaning toward rejecting this paper. But if those concerns are (and will also be) addressed in the responses, I will be happy to bump up my assessment."
                },
                "questions": {
                    "value": "My questions are in the detailed comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern about the ethics."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7351/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699170724525,
            "cdate": 1699170724525,
            "tmdate": 1699636879298,
            "mdate": 1699636879298,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9t8G9ggKYV",
                "forum": "T23HYw6lta",
                "replyto": "P5TWmAOM22",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7351/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7351/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xTDh"
                    },
                    "comment": {
                        "value": "### **1. Regarding concerns about finetuning-based defense without knowing the trigger**\nWe acknowledge that the vanilla fine-tuning (FT) has limited capability in mitigating backdoors. However, it still offers some level of protection and is often combined with other components to establish more robust defenses. FT-based defense remains an active area of research, with methods like NAD, FT-SAM, and Super-FT demonstrating empirical effectiveness. Particularly, Super-FT was released at the end of 2022, while FT-SAM has been published at ICCV 2023. In its paper, FT-SAM  showed stronger defense performance than popular defenses in other approaches, including Neural Cleanse, ANP, ABL, and iBAU. In our paper, both Super-FT and FT-SAM can effectively mitigate LIRA, while FP and ANP fail (as reported in Section A.6 in the Appendix). Finally, as shown in our response to Reviewer EHtc, both Super-FT and FT-SAM can effectively mitigate very recent advanced attacks proposed in [1]. Hence, advanced finetuning-based backdoor defenses are serious mechanisms to protect users from the backdoor threat. Our method aims to enhance the robustness of backdoor attacks against these advanced FT defenses.\n\nAlso, we understand the reviewer's concern, and we have refined the discussion of fine-tuning-based defense in the revised introduction to provide a more accurate overview of this defense approach.\n\n[1] Revisiting the Assumption of Latent Separability for Backdoor Defenses. In *ICLR* 2022\n\n### **2. Regarding finetuning-based defense after knowing the trigger**\nThanks for your useful suggestion. Our paper aims to address the finetuning-based defenses without knowledge of the trigger, which is an active area of research, with methods like NAD, FT-SAM, and Super-FT. Hence, we do not focus on the application of finetuning in mitigating the backdoor after knowing the trigger, whose effectiveness relies much on the trigger reconstruction rather than the finetuning itself. We have added a clarification in our revised paper.\n\n### **3. Baseline for evaluation**\nWe chose 50% ASR as the threshold to determine whether an attack is successful. This level of accuracy indicates that backdoor samples are more likely to be misclassified as the target label. We have elaborated on this point in the revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7351/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740309334,
                "cdate": 1700740309334,
                "tmdate": 1700740309334,
                "mdate": 1700740309334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]