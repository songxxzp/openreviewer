[
    {
        "title": "Adaptivity and Modularity for Efficient Generalization Over Task Complexity"
    },
    {
        "review": {
            "id": "z2MdWyQBxn",
            "forum": "tI3eqOV6Yt",
            "replyto": "tI3eqOV6Yt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6471/Reviewer_Qbwm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6471/Reviewer_Qbwm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a task to probe the capability of models to generalize across reasoning steps and the authors design a transformer-based architecture that combines dynamic function generation from hypernetworks with adaptive depth from universal transformers. Extensive experiments show the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Exploring the general efficiency of transformers on problems that require dealing with examples with different levels of difficulty seems novel."
                },
                "weaknesses": {
                    "value": "1. The task definition is unclear. \n2. The proposed method seems a combination of transformers and hyper-modules (Ha et al., 2017). \n3. In the experiment, the authors do not compare the state-of-art methods."
                },
                "questions": {
                    "value": "see the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669448784,
            "cdate": 1698669448784,
            "tmdate": 1699636724000,
            "mdate": 1699636724000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CP1uTmjr5a",
                "forum": "tI3eqOV6Yt",
                "replyto": "z2MdWyQBxn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6471/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our paper. Please have a look at our shared response. And here are our comments/questions about some of your remarks about the weaknesses of the paper. \n\n### Task definition:\nWe would appreciate if you could tell us which parts of the task definition you found unclear. We will do our best to clarify this as much as possible in the camera ready version (Please see appendix G in the updated version in updated version of the paper).\n\n### Comparison with the state of the art:\nThe focus of the paper has been to study the ability of transformers to generalize to example of higher complexity while allocating compute proportionally. Hence, our baseline is a vanilla transformer and we compare how augmenting the vanilla transformer with adaptive compute time and modularity enhances their generalization and efficiency. We think it makes sense to study transformers since they seem to be the main building block of current state-of-the-art models.  After showing that it is challenging for vanilla transformers to generalize in the specific setting of the C-PVR task, we show that adaptivity and modularity have complementary effects on improving the generalization and efficiency of transformer models. \nIf you could clarify what are the other methods/models that you would like us to add to our comparisons, we will try to include them in the future versions of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511014451,
                "cdate": 1700511014451,
                "tmdate": 1700511014451,
                "mdate": 1700511014451,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k2owGYSyA7",
            "forum": "tI3eqOV6Yt",
            "replyto": "tI3eqOV6Yt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6471/Reviewer_D459"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6471/Reviewer_D459"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Hyper-UT which is a Transformer architecture that can perform adaptive computation and dynamically generate weights. The goal is to have Transformers that can generalize to higher number of computation steps and effectively allocate different amounts of computation depending on input complexity. The paper also introduces C-PVR task to measure the multi-step generalization of various Transformer models and found that existing Transformers perform worse than Hyper-UT. Hyper-UT also demonstrates strong accuracy and efficiency on ImageNet tasks, compared to ViT and U-ViT."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Good analysis on the weaknesses of existing Transformers using the proposed multi-step reasoning task C-PVR.\n- Intriguing finding that combining adaptive computation and modularity can have strong complementary effects.\n- Experimental results are very promising, especially the HyperU-ViT results on ImageNet datasets. HyperU-ViT matches or outperforms traditional ViT using a significantly small fraction of FLOPS that ViT requires."
                },
                "weaknesses": {
                    "value": "- The individual components like ACT and Hyper-Module are not new inventions. They have been introduced in existing papers. This paper merely uses the two within Transformer in a relatively simple manner.\n- The details on how and where ACT is being used in Hyper-UT/HyperU-ViT is lacking. It is also similar for Hyper-Module. While there's diagram that shows the attention-based router, it is unclear how attention-based router is formulated (i.e., there's no equation).\n- There's no comprehensive ablation study on the different components."
                },
                "questions": {
                    "value": "Can this be applied to NLP Transformers (e.g., LLMs)? What's the reason of using ViT/image classification as the testbed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698727940025,
            "cdate": 1698727940025,
            "tmdate": 1699636723886,
            "mdate": 1699636723886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O30h7s2MYo",
                "forum": "tI3eqOV6Yt",
                "replyto": "k2owGYSyA7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6471/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the review and the interesting question. Please see our shared response. Also, here are the response to your question and clarification on your remark on the lack of details. \n\n\n`Lack of details for ACT and Hyper-Module in Hyper-UT architecture:`\n\nWe will explain the ACT and the Hyper-Module and their corresponding hyper-parameters with more details in the appendix.\n\n**ACT mechanism:**\nWe apply the ACT mechanism per token, and for the details of the method we refer to the original paper (Graves, 2016), and for how it is incorporated into the transformer architecture we refer to the universal transformer paper (Dehghani et al, 2019). Basically, there is a dense layer with sigmoid activation (ACT unit), that given the representation of each token at the current layer, predicts a score (the halting score). The model will halt at a given layer for a given token when the sum of all the halting scores for the token in the previous layer is bigger than $1 - \\epsilon$. We could also do this per example, but the current experiments in the paper report the results for when ACT is applied per token.  Also, the objective of the model is augmented with the ACT loss, which is: \n<number of steps> + (1 - <final halting_score>). \n\n\n**Attention based router:**\nFor the attention based router, we use a standard attention layer (multi-head cross attention, in our experiments with just one head). Where the query is the example embedding at the current layer, and the key/values are the embeddings in the module embedding pool. \n\n`Can this be applied to NLP Transformers (e.g., LLMs)? What's the reason of using ViT/image classification as the testbed?`\nOf course, we just wanted to show that the idea generally applies and can be useful even in system-1 like tasks where there is not much need to a lot of sequential processing. Hence we chose image classification. We agree it would be very interesting to see the results of applying these to LLMs. We might even see more benefits on language modeling since, it sounds like a much more complex task. While we have deferred this to future work, we would like to point out to a concurrent work that applies a similar idea to NLP tasks (Tan et al, 2023).\n\n[1] Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, and Chuang Gan. Sparse universal transformer. arXiv preprint arXiv:2310.07096, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508446505,
                "cdate": 1700508446505,
                "tmdate": 1700508446505,
                "mdate": 1700508446505,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "60LQG3ngK0",
            "forum": "tI3eqOV6Yt",
            "replyto": "tI3eqOV6Yt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6471/Reviewer_PAjo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6471/Reviewer_PAjo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new hypernetwork-structured parameterization for transformers, which focuses on adaptive and dynamic computation. In showcase its advantages, the paper considers a new task called conditional pointer value retrieval and demonstrates some improvement on this task and conventional ImageNet classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well structured and clearly presented in general. It is easy to follow.\n\n- The dynamic nature of the proposed Hyper-UT is interesting and makes intuitive sense, as for different task/input, different levels of computation complexity is required.\n\n- The experimental results look good and Hyper-UT indeed shows some improvement."
                },
                "weaknesses": {
                    "value": "- Hyper-UT is essentially an alternative parameterization of HyperNetworks, despite the dynmaic computation is placed in transformer blocks. I don't find it particularly different from HyperNetworks and its numerous variants, say [1,2,3,4,5].\n\n- The motivation behind Hyper-UT is not clear to me. Although the design of HyperNetwork-like stucture makes the computation dynamics and may save the inference cost, I fail to understand the motivation of the design choices made in Hyper-UT. Why the proposed design will be better than many other HyperTransformers is not clear to me. It seems that it is simply yet another hypernetwork.\n\n- The experiment on C-PVR is not particularly convincing, as the task is a synethic one rather than a real problem. Could the authors find some other tasks that are more representative and yet realistic?\n\n\n[1] Revisiting Linear Decision Boundaries for Few-Shot Learning with Transformer Hypernetworks, openreview 2022\n\n[2] On the Modularity of Hypernetworks, NeurIPS 2020\n\n[3] HyperGrid Transformers: Towards A Single Model for Multiple Tasks, ICLR 2021\n\n[4] Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks, arXiv:2106.04489\n\n[5] Hypermixer: An mlp-based low cost alternative to transformers, ACL 2023"
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699247115599,
            "cdate": 1699247115599,
            "tmdate": 1699636723763,
            "mdate": 1699636723763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PjmSXvwgJO",
                "forum": "tI3eqOV6Yt",
                "replyto": "60LQG3ngK0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6471/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the feedback. We have responded to your comments about novelty and ablation experiments in the shared response.  Here are our comments on some of the remaining points:\n\n`\u201cHyper-UT is essentially an alternative parameterization of HyperNetworks...\u201d`\nPlease see the shared response, the section on the concerns over novelty. Also, thank you very much for pointing out the additional references.\n\n`\u201cThe motivation behind Hyper-UT is not clear to me....It seems that it is simply yet another hypernetwork.\u201d`\nAs mentioned in the shared response, having an adaptive depth mechanism is a crucial part of this architecture. To reiterate the main point we are trying to make in this paper, we show how mechanism for adaptive depth and modularity can help transformers (1) generalize faster and better to examples of higher complexity (2) allocate compute more fairly.\nThe particular design choice for the hyper module worked well empirically, and the precise overall scheme was intended to combine adaptive depth with modular reuse. The general intuition that leads to such architecture is that, we want a model that can allocate both type and amount of compute conditioned on the given example. From another point of view, we want to benefit from inductive biases of parameter sharing and recurrence in depth without sacrificing capacity.  \n\n`\u201cThe experiment on C-PVR is not particularly convincing, as the task is a synethic one rather than a real problem. Could the authors find some other tasks that are more representative and yet realistic?\u201d`\nThat is a very good point. We think the experiments on the C-PVR task are still interesting, as it allows us to study the phenomena of interest more rigorously through controlled experiments.  The phenomena being how well the model can generalize to deal with examples that are more complex (requiring more sequential steps) than ones seen during training abstracting away from other factors that could influence the model. We also find it important to show this on more realistic use-cases. The main issue is that for real benchmarks it is really hard to come up with an automated way of annotating the complexity of the examples. Also, the space of potential solutions that the model can converge to that is not necessarily aligned with a defined complexity measure is much wider, making it very challenging to measure phenomena like generalization over complexity in these benchmarks. Nevertheless, we agree it is very important to build such benchmarks or find ways to show generalization over complexity on more realistic datasets.  We will discuss this point in the updated version of the paper. It is worth noting that we do have some experiment on ImageNet to show the generality of the suggested architecture."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508210623,
                "cdate": 1700508210623,
                "tmdate": 1700508210623,
                "mdate": 1700508210623,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "55RvK3d23S",
            "forum": "tI3eqOV6Yt",
            "replyto": "tI3eqOV6Yt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6471/Reviewer_tf4F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6471/Reviewer_tf4F"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the generalization performance of Transformers across various levels of task complexity. The paper introduces a new synthetic task, C-PVR (conditional point-value retrieval) to evaluate this generalization capability. In C-PVR, the model is asked to find a value at a specific position indicated by a pointer. In contrast to the PVR task (Zhang et al., 2021), C-PVR requires the model to navigate through multiple pointers until it reaches the desired position. The authors define the task's complexity in C-PVR by quantifying the number of hops to find the target value. Based on this task, the authors observe that modularity and adaptivity across tasks of varying complexity are significant to achieve better generalization. To address this, they propose Hyper-UT, a model equipped with Hyper-Modules that contains both modularity and adaptivity.\nIn the experiments, the authors demonstrate that Hyper-UT exhibits better generalization performance and efficiency when compared to conventional Transformers. Additionally, it performs competitively on ImageNet-1k, a well-known System-1 task. Furthermore, they analyze the generalization performance of pre-trained language models like T5 on C-PVR and observe that the scratch-pad can enhance the chain-of-thought ability of these models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* They introduced a novel task that can evaluate the generalization capacity of Transformers across different complexity.\n* They demonstrated that the generalization capacity of Transformers across task complexity can be improved through modularity and adaptivity.\n* They compared and analyzed various models to confirm the importance of modularity and adaptivity in improving generalization capacity.\n* The paper is well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "* In Section 4.2, a comparison with the T5 model trained from scratch is absent. To explore the influence of pre-training with language modeling on the generalization capacity, it is essential to include a comparison between the pre-trained T5 model and a T5 model trained from scratch specifically for the C-PVR task. This comparison will provide valuable insights into the impact of pre-training on the generalization.\n* Comparing models with and without the scratch-pad in Figure 5 is challenging. It would be more helpful to combine Figure 5 (a) with Figure 5 (b), and Figure 5 (c) with Figure 5 (d).\n* Ablation studies on the Hyper-Module are missing. To gain a better understanding of the effects of modularity and adaptivity, it would be beneficial to include an ablation study on the size of the weight embedding pool."
                },
                "questions": {
                    "value": "* What do the notations \"32x3\" and \"128x2\" mean in Table 2 and Table 3 in the appendix?\n* Is Hyper-UT also parameter-efficient compared to other methods?\n* Is it possible to apply multi-head attention to the attention-based router? How it will affect the generalization capacity compared to the single-head one?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6471/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6471/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6471/Reviewer_tf4F"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699515512684,
            "cdate": 1699515512684,
            "tmdate": 1699636723647,
            "mdate": 1699636723647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iQaOgHgDwU",
                "forum": "tI3eqOV6Yt",
                "replyto": "55RvK3d23S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6471/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough review and all the great questions and comments.\n\n`\u201cA comparison with the T5 model trained from scratch is absent.\u201d`\nWe agree that the results with the T5 model are not directly comparable to transformers from scratch that we have in Figure 7. However, the main goal in these experiments was to show that the task remains challenging even for transformers  pre-trained on a language modeling objective. We will include experiments with the T5 architecture trained from scratch in the next version of the paper (the camera  ready version if the paper gets accepted). We speculate the results not to be very different than transformer encoders trained from scratch when there is no scratch-pad (Figure 7), since the decoding part would only have one decoding step. \nFor the setting with scratch-pad we would expect better results (compared to the vanilla version) since the scratch-pad is basically augmenting the model with a mechanism for adaptive compute. While this effect has already been studied in other works, it would be interesting to see how scratch-pad or CoT helps in the context of the C-PVR task specifically (when there is no pretraining).\n\n`Comparing models in Figure 5.`\nWe intentionally separated the plots to make them easier to read, but we agree that this makes the comparison a bit more difficult. We will update the plots and will incorporate this suggestion.\n\n`\u201cWhat do the notations \"32x3\" and \"128x2\" mean in Table 2 and Table 3 in the appendix?\"`\n Apologies for the typo. It is meant to be 32x128 and 128x256. It is <number of model embeddings> \\times <size of the embeddings>. \n\n`\u201cIs Hyper-UT also parameter-efficient compared to other methods?\u201d`\nIntuitively we think, at larger scales, Hyper-UT can also be more parameter-efficient but in this paper our main focus has been on the flop efficiency and we have not investigated this aspect. In the current experimental results in this paper, Hyper-UT has actually more parameters compared to the vanilla transformer / vision transformer and certainly universal transformer with full parameter sharing but it does converge to use fewer flops per example (more specifically less flops for simpler example and more flops for more complex examples). \n\n`\u201cIs it possible to apply multi-head attention to the attention-based router? How it will affect the generalization capacity compared to the single-head one?\u201d`\nIntuitively, this might help for more complicated tasks, but the effect might be less significant for a task like C-PVR. There might also be some interaction between the number of heads and the size of the embeddings. We will include an ablation experiment for the number of heads in the next version of the paper (So far we have not seen an improvement in efficiency or accuracy when increasing number of heads on the C-PVR task). Nevertheless, we think it is important to continue research in this direction to find the optimal and most efficient way of incorporating modularity into adaptive models."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508067626,
                "cdate": 1700508067626,
                "tmdate": 1700508139183,
                "mdate": 1700508139183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]