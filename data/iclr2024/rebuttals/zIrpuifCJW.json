[
    {
        "title": "Exploring the Impact of Information Entropy Change in Learning Systems"
    },
    {
        "review": {
            "id": "4ISQhGuxBn",
            "forum": "zIrpuifCJW",
            "replyto": "zIrpuifCJW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission517/Reviewer_FieS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission517/Reviewer_FieS"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to explore the influence of the information entropy change, and specifically analyzes the impact of various types of noise, i.e., Gaussian noise, linear transform noise, and salt-and-pepper noise, on the performance of deep learning models for image classification and domain adaptation tasks. The authors verify their method on different network architectures, i.e., CNNs and ViTs, and show that the positive noise injection can improve the accuracy of image classification."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The proposed method of using noise injection to improve the performance of CNNs and ViTs, which has not been extensively explored before.\n\n2. The article is well-written and easy to understand, with clear explanations of the proposed method and the experimental results."
                },
                "weaknesses": {
                    "value": "1.Limited analysis between data augmentation and the proposed noise injection. It seems more like a feature augmentation method for different layers. The article claims that the positive noise is benefit of the classification network, but it is hard to measure the noise level for the positive/negative influence. It is encouraged to compare the difference between the data augmentation with noise and the proposed method.\n\n2.Noise definition confusion. Unlike Gaussian noise and salt-and-pepper noise with the specific probability distribution, the linear transform is not a kind of noise, but a simple operation. \n\n3.Limited analysis of the tasks. The authors leverage the proposed method in image classification tasks, including domain adaptation. It is encouraged to conduct the noise injection in image object detection/segmentation tasks, or some NLP tasks, to verify the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751052146,
            "cdate": 1698751052146,
            "tmdate": 1699635978753,
            "mdate": 1699635978753,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B0ZnZay69w",
                "forum": "zIrpuifCJW",
                "replyto": "4ISQhGuxBn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission517/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Question 1. Limited analysis between data augmentation and the proposed noise injection. It seems more like a feature augmentation method for different layers. The article claims that the positive noise is benefit of the classification network, but it is hard to measure the noise level for the positive/negative influence. It is encouraged to compare the difference between the data augmentation with noise and the proposed method.\n\nOur answer: \nThanks for your questions. The information entropy of the learning system framework proposed in our works provides theoretical explanations for data augmentation. \n\nData augmentation can be divided into two aspects:\n\nImage Level Data Augmentation, such as flips, translations, rotations, CutOut, MixUp, CutMix, and etc.  \nAs the reviewer eupP already pointed out, the pioneering work on Positive-Incentive Noise (IEEE Transactions on Neural Networks and Learning Systems, 2022) has provided a theoretical framework for image-level data augmentation. \n\nLatent Data Augmentation, such as adding noise to embeddings, feature mixup, and etc.\nOur work provided a theoretical framework for latent data augmentation. \n\nTherefore, our work provides theoretical guidance for exploring possible effective data augmentation methods in the future.\nOur work provides quantitative tools for analyzing the influence of adding noise to learning systems. Combined with the previous work on Positive-Incentive Noise in IEEE TNNLS 2022, readers can analyze the effects of adding noise at both the image and latent levels.\n\nQuestion 2. Noise definition confusion. Unlike Gaussian noise and salt-and-pepper noise with the specific probability distribution, the linear transform is not a kind of noise, but a simple operation.\n\nOur answer: Thank you for bringing this up. For a specific image X1, Gaussian noise, salt-and-pepper noise are considered noise; this is no doubt for reviewer FieS. However, in our definition, we consider another image X2 as noise when it is added to X1. We will emphasize this point if there is confusion for the reviewer.\n\nQuestion 3. Limited analysis of the tasks. The authors leverage the proposed method in image classification tasks, including domain adaptation. It is encouraged to conduct the noise injection in image object detection/segmentation tasks, or some NLP tasks, to verify the effectiveness of the proposed method.\n\nOur answer: Great question. Your suggestions are the directions we will work on, but our expertise lies in image classification, domain adaptation, and semi-supervised learning. We may need collaborators to expand our research. For NLP tasks, there was an arXiv submission last month that utilized uniform noise as positive noise, where they significantly improved instruction fine-tuning in large language models. For object detection/segmentation, we are interested in applying positive noise in these areas, but we need time to delve into it."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699935390828,
                "cdate": 1699935390828,
                "tmdate": 1699935390828,
                "mdate": 1699935390828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zqc6YOGIhj",
            "forum": "zIrpuifCJW",
            "replyto": "zIrpuifCJW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission517/Reviewer_eupP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission517/Reviewer_eupP"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the impact of task entropy change in deep neural networks by introducing noise at different levels in a network, and demonstrates that certain kinds of noise can actually help with learning by reducing the task entropy. The paper differentiates between \"positive noise\" (PN) that can enhance system performance by reducing task complexity and \"harmful noise\" (HN) that deteriorates it. The concept of \"positive noise\" is introduced in [1] which this work cites. By using information entropy as a measure of task complexity, the research shows that intentionally adding positive noise can substantially improve performance. The empirical findings challenge traditional views on noise in deep learning, suggesting that positive noise can be beneficial. Results are demonstrated using networks from the ResNet and ViT family for classification on ImageNet and using ViT-B for unsupervised domain adaptation on Office-Home and Visda2017 datasets.\n\n[1] Xuelong Li. Positive-incentive noise. IEEE Transactions on Neural Networks and Learning Systems,\n2022."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written. The supplementary section provides further details on the derivations, which is very helpful.\n\nThe technique is well motivated and the empirical results are quite impressive.\n\nThis work provides an interesting new perspective on positive noise injection which can help training."
                },
                "weaknesses": {
                    "value": "No error bars in any tables - did the authors run multiple seeds for their experiments? Even though the improvements are generally large, it is nice to see these in the tables/figures.\n\nThe technique is only evaluated on classification using ImageNet and domain adaptation. It would be good to see results on other tasks like object detection perhaps, and specially domains like Natural Language Processing, which differ from vision based tasks."
                },
                "questions": {
                    "value": "Given that the improvement is so large on ResNet-18, do the authors have an explanation for why ViT-T does not improve similarly?\n\nCan the authors elaborate on this statement? \"Besides, when the models are corrupted under brute force attack, the positive noise also can not work.\" Why is this the case?\n\nThis point also requires more detailed explanation - \"Second, injection to shallow layers obtain less entropy change gain because of trendy replacing Equation 8 with Equation 7.\" The explanation for why shallower layers don't provide as high accuracy gains as deeper layers can be improved. \n\nHave the authors tried combinations of later layers for noise injection?\n\nGiven that this is a new perspective and the results are strong, will code be released for reproducibility?\n\nCan the authors give other examples of positive noise that they considered?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission517/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission517/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission517/Reviewer_eupP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817771846,
            "cdate": 1698817771846,
            "tmdate": 1699635978684,
            "mdate": 1699635978684,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vmXrHWXjWc",
                "forum": "zIrpuifCJW",
                "replyto": "Zqc6YOGIhj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission517/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your constructive feedback and valuable comments. We are happy to answer your questions. \n\nAnswer for Weakness1: \nThank you for your inquiry regarding the technique details. To ensure a fair comparison in experiments with different models on ImageNet, we maintain consistency by using seed_everything with an identical seed in each experiment. Notably, given the substantial improvements that significantly surpass the SoTA, we let two colleagues independently conducted the experiments on different machines, and the reported results for ImageNet are averaged. However, due to limited computational resources, other experiments were run only once. We appreciate your suggestion, and in response, we plan to compile all experiment information, presenting it in a figure within the supplementary material.\n\nAnswer for Weakness2: \nGreat suggestions. We have found other works submitted on ArXiv in the last month that utilize positive noise to enhance instruction finetuning in large language models. At the very least, the application of positive noise to NLP has been verified.\nSince our expertise lies primarily in image classification, domain adaptation, and semi-supervised learning, we are actively seeking potential collaborators to apply our methodology in other areas across CV and NLP.  If the reviewer is an expert in related areas, we would be more than happy to discuss potential opportunities.\n\n\nAnswer for Question1: \nThe details of the model architectures, presented in Table 1 and 2 of the supplementary materials, may provide two key explanations.\n\n1, the ViT-Tiny model has a head number of 3, significantly smaller than the 12 in   \nViT-B. This limitation hampers ViT-Tiny's ability to capture diverse patterns in images. This can be supported by the paper \u2018On the Relationship between Self-Attention and Convolutional Layers\u2019 ICLR 2020.\n\n2, with only 5.7 million parameters, ViT-Tiny falls short of the parameter count in  ResNet-18 by nearly half. This suggests that ViT-Tiny may struggle to capture the necessary complexity and hierarchical features as effectively as ResNet-18 in ImageNet classification tasks.\n\n\nAnswer for Question2:  The paragraph on the moderate model assumption claims that positive noise functions effectively for a 'normal' or 'ordinary' model. However, for instance, if a model is compromised, as demonstrated in the 'Color Backdoor: A Robust Poisoning Attack in Color Space,' CVPR 2023, the positive noise may not work. If this statement makes readers confused, we will revise it.\n\n\nAnswer for Question3: Thanks for the great suggestions. An additional explanation could be that deeper layers lead to better representations. Evidence from 'Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth' (ICLR 2021) suggests that as the depth or width of the model increases, there is an emergence of principal components that contribute to improved performance. This implies that a certain depth is necessary for the model to achieve satisfactory performance.\n\n\nAnswer for Question4: No, all the experiments involve a single layer of noise injection. If we understand correctly, did the reviewer suggest trying noise injection into multiple layers? If so, the first author can prepare additional experiments if necessary.\n\n\nAnswer for Question5: All members have already agreed that all the source code running CNNs and ViTs on ImageNet, ImageNet V2, Office-Home, and Visda2017 will be released on GitHub upon acceptance. We will also provide the pretrained models. \n\n\nAnswer for Question6: This is a great question! Yes. We will explore more kinds of positive noises. Another team has recently achieved promising results by using uniform noise as positive noise in an NLP task; their related submission is already on ArXiv. \n\nThe work by Xuelong Li, 'Positive-Incentive Noise,' published in IEEE Transactions on Neural Networks and Learning Systems in 2022, is a pioneering effort that introduces positive noise in machine learning. We also recommend another paper that explores positive noise in signal processing: 'Exploring Positive Noise in Estimation Theory' by Radnosrati, Kamiar, Gustaf Hendeby, and Fredrik Gustafsson, published in IEEE Transactions on Signal Processing in 2020. We strongly believe that there is a great potential for positive noise in AI community."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699935098739,
                "cdate": 1699935098739,
                "tmdate": 1699935748781,
                "mdate": 1699935748781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ICtRKDJqix",
                "forum": "zIrpuifCJW",
                "replyto": "vmXrHWXjWc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission517/Reviewer_eupP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission517/Reviewer_eupP"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your clarifications."
                    },
                    "comment": {
                        "value": "Thank you for the clarifications to my questions. Can you add a pointer to the use of uniform noise as positive noise in an NLP task that you talk about in your response? I would like to read it. \n\nFor question 4, yes, I was asking about adding noise to more than one layers at a time as this would be an interesting experiment."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501728400,
                "cdate": 1700501728400,
                "tmdate": 1700501728400,
                "mdate": 1700501728400,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kt29EuvCAy",
            "forum": "zIrpuifCJW",
            "replyto": "zIrpuifCJW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission517/Reviewer_H7oc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission517/Reviewer_H7oc"
            ],
            "content": {
                "summary": {
                    "value": "This research investigates the impact of noise-induced entropy changes in deep learning systems, focusing on computer vision tasks. While noise is traditionally seen as detrimental, this study demonstrates that specific noise, termed positive noise (PN), can enhance deep learning model performance by reducing task complexity defined by information entropy. The study introduces a distinction between positive noise (beneficial) and harmful noise (detrimental) and shows that proactive injection of positive noise significantly improves accuracy, achieving over 95% on ImageNet. Besides, this paper explores three types of noises. But the difference of positive noises on different kinds of noise types lacks discussion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Strengths:\n- This paper challenges the notion that noise always hampers deep learning models, showcasing its potential as a positive influence.\n- It offers theoretical insights, distinguishing noise impact at different levels, aiding in optimizing task complexity.\n- Experiments on both CNNs and Vision Transformers are conducted."
                },
                "weaknesses": {
                    "value": "- The range of tasks tackled remains relatively limited, lacking diversity and complexity in comparison to broader applications. To be specific, this paper only conducts experiments on classification tasks. More results on other tasks (e.g., regression tasks like Object Detection or generative tasks like language understanding)  are lacking to validate its generalization ability."
                },
                "questions": {
                    "value": "For each type of noise, there will exist positive noises. What are their difference and influences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission517/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835383757,
            "cdate": 1698835383757,
            "tmdate": 1699635978616,
            "mdate": 1699635978616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IllP9G0bpn",
                "forum": "zIrpuifCJW",
                "replyto": "kt29EuvCAy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission517/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission517/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your feedback and are dedicated to providing detailed answers to any questions.\n\nWeakness: The range of tasks tackled remains relatively limited, lacking diversity and complexity in comparison to broader applications. (e.g., regression tasks like Object Detection or generative tasks like language understanding)\n\nOur answer: We will certainly explore the tasks mentioned by the reviewer. For instance, the object detection task involves predicting both the coordinates of the bounding box and the label of the object. This differs from classification tasks, which only predict labels. Consequently, our plan includes conducting theoretical research on reducing information entropy in object detection tasks, followed by experimental validation.If the reviewer is an expert in object detection or related areas, there could be an opportunity for collaboration after the anonymous review period.\n\nQuestions: For each type of noise, there will exist positive noises. What are their difference and influences?\n\nOur answer:  \nAccording to the definition in formula (5), any noise that aids in reducing the entropy of the learning system is labeled as positive noise. The extent of performance improvement resulting from the addition of positive noise depends on the degree to which it decreases entropy. The greater the reduction in the system's entropy, the higher the expected increase in performance.\n\nIn our view, these types of noises represent potential ways to alter the entropy of the learning system and influence its performance. One significant distinction is that salt-and-pepper and Gaussian noises have their own distributions and are independent of the data samples, whereas linear transform noise utilizes features from other data samples, making it dependent on the data samples."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission517/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699934517912,
                "cdate": 1699934517912,
                "tmdate": 1699934517912,
                "mdate": 1699934517912,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]