[
    {
        "title": "Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks"
    },
    {
        "review": {
            "id": "2SRETC9pCm",
            "forum": "ohdVLirfbz",
            "replyto": "ohdVLirfbz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8613/Reviewer_PsiY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8613/Reviewer_PsiY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization of training overparameterized neural networks by gradient descent. This work follows the line of work of studying the generalization error via on-average argument stability. This work extends previous work by providing a relaxed condition for two-layer NNs and establishing an optimization and generalization result for 3-layer NNs. This work further shows that as the network scaling factor $c$ (in the $1/m^c$ scaling in front of NNs) increases, less overparameterization is needed in order to achieve the same generalization error."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is technical and the results are novel in my opinion. The proofs are solid and nicely structured in the Appendix. On the technical side, I appreciate the authors are able to establish almost co-coercivity results for 3-layler NN and the authors are able to use this to establish the uniform stability bound on $W_t - W_t^{(i)}$. On the other hand, I also like the results on the relationship between the network width and the scaling parameter $c$ and that the authors are able to show that larger $c$ requires less over-parameterization."
                },
                "weaknesses": {
                    "value": "One of the drawback of this analysis is that since the network width $m$ is usually chosen first in practice, this result (such as theorem 2,3) basically says the training can't be too long and $\\eta T$ need to be upper bounded by some functions over the network width. Also, this work prove convergence via lower bounding the minimum eigenvalue of the Hessian and uses the Hessian at the initialization as a reference point. This limits how far the network weights can travel from the initialization value. For $L^2$ loss, I think this is OK but for losses like cross-entropy where the network achieve zero-loss at infinity, it is not clear how far this analysis can be generalized."
                },
                "questions": {
                    "value": "1. How are the network weights initialized? It seems the only requirement on initialization is assumption 2. \n2. When the authors are talking about network complexity, how is it defined? Especially, the paragraph before section 2 \"...the larger the scaling parameter or the simpler the network complexity is...\". It seems from assumption 3 that the network complexity is defined as the $L^2$ norm of the weight matrices. Also, why the larger the scaling parameter corresponds to the simpler the network complexity?\n3. On page 2, the second bullet of contributions, the authors mentioned that \"... due to the empirical risks' monotonically decreasing nature which no longer remains valid for three-layer NNs\" Can't taking a smaller step in gradient descent steps help the empirical risks' monotonicity of decreasing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8613/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8613/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8613/Reviewer_PsiY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607197203,
            "cdate": 1698607197203,
            "tmdate": 1699637077721,
            "mdate": 1699637077721,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aqjHf9cxaL",
                "forum": "ohdVLirfbz",
                "replyto": "2SRETC9pCm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your careful reading and constructive comments"
                    },
                    "comment": {
                        "value": ">** Q1. One of the drawback of this analysis is that since the network width $m$ is usually chosen first in practice, this result (such as theorem 2,3) basically says the training can't be too long and $\\eta T$ need to be upper bounded by some functions over the network width. Also, this work prove convergence via lower bounding the minimum eigenvalue of the Hessian and uses the Hessian at the initialization as a reference point. This limits how far the network weights can travel from the initialization value. For $L^2$ loss, I think this is OK but for losses like cross-entropy where the network achieve zero-loss at infinity, it is not clear how far this analysis can be generalized.\n\nThanks for your thoughtful comment. In this work, we answer a theoretical question of how wide the network is required for GD to achieve the desired excess risk bound $O({1 / \\sqrt{n}})$. Our results also shed light on how to choose the suitable $\\eta T$ and $m$ related to data size $n$ to achieve the $O({1 / \\sqrt{n}})$ risk rate under the different settings. \n\nIn addition, if the network width is chosen first, our results (especially Corollaries 4 and 8 in the revised version) reflect the learning capability of this network. For example, let us consider the case $c=1$ and $\\mu=1/2$. Corollary 4 implies, at the theoretical aspect, that if $n\\lesssim m^{4/3}$, then GD for two-layer NNs with at least $\\sqrt{n}$ iterations can achieve the desired error rate $O(1/\\sqrt{n})$. While the estimation of the minimum eigenvalue of the Hessian $\\lambda_{\\min}(\\ell(\\mathbf{W};z))$ depends on $||\\mathbf{W}-\\mathbf{W}_0||_2$, we can show that along the trajectory of GD, $||\\mathbf{W}_t-\\mathbf{W}_0||_2\\le c\\sqrt{\\eta T}$.  Hence, our analysis does not limit how far the network weights can travel from the initialization value. \nWe mainly focus on the least square loss in this work. We agree with you that extending our results to the cross-entropy loss is very interesting and important, which we leave as an important future direction. \n\n\n\n\n\n\n\n>** Q2. How are the network weights initialized? It seems the only requirement on initialization is assumption 2.\n\n\nYes, we can initialize the network weights randomly as the previous works did or use a fixed initialization as long as Assumption 2 holds.   Indeed, for sigmoid activation or hyperbolic tangent activation considered in our paper, it can be easily verified that Assumption 2 holds for any initialization if the inputs and labels are bounded. That is, for any network initialization $\\mathbf{W}_0$ and any data $z=(\\mathbf{x},y)$ with $||\\mathbf{x}||_2\\le c_x$ and $|y|\\le c_y$, we can always find a constant $c_0>0$ such that the assumption $\\ell(\\mathbf{W}_0;z)\\le c_0$ holds. We have added the clarification below Assumption 2 in the revised version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483751907,
                "cdate": 1700483751907,
                "tmdate": 1700484487701,
                "mdate": 1700484487701,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BcjvZVbXdS",
            "forum": "ohdVLirfbz",
            "replyto": "ohdVLirfbz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8613/Reviewer_khQG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8613/Reviewer_khQG"
            ],
            "content": {
                "summary": {
                    "value": "This paper gives guarantees to the excess risk of two-layer and three-layer neural networks. In particular, this paper compares the generalization error of the gradient descent algorithm for minimizing the empirical risk of the network and provides bounds on the generalization error.\n\nThe setup assumes that the activation functions are twice-differentiable, have bounded derivatives, and the inputs all lie within a bounded range.\n\nThe main result requires the network width to grow in a polynomial fashion with the number of epochs, the step size, and a certain scaling parameter used in the empirical loss.\n\nAdditionally, it is assumed that the population risk minimizer satisfies a certain norm-bounded assumption under $\\mu$.\n\nGiven these conditions, the paper then proves that the excess risk will converge by a rate as something like $O(T / n)$, where $T$ is the total number of iterations. [However, there is an additional dependence between $T$ and $m$ (the width of the network).\n\nAt a high-level, the proof proceeds by arguing that the gradient descent satisfies an \"on-average\" stability condition, and this condition implies that the loss is strongly smooth and weakly convex. This condition allows one to prove that the gradient descent optimizer will converge to the global minimum.\n\nThe scaling of $m^{-c}$ on the empirical loss is then used in a manner similar to the NTK analysis to get the required smoothness condition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A nontrivial result that expands on the literature of generalization guarantees for deep neural networks.\n\n- The paper is generally written with sufficient clarity for readers to follow.\n\n- A nice diagram to visualize the dependencies in the problem constants."
                },
                "weaknesses": {
                    "value": "- The proof thus far only works for three layers. [Although it is believable that with more work, one may be able to extend the analysis for multiple layers, e.g., by arguing a similar weak convexity condition on the loss function.]\n\n- It is by now widely accepted that the NTK analysis is more of an analogy rather than a realistic characterization of what happens in reality. Thus, the practical relevance of this paper, which follows similar lines of arguments as the NTK (e.g., by rescaling the loss and expanding the network width), is very limited.\n\n- The abstract (and the paper's title) mentions multi-layer neural networks, but the analysis currently holds for three layers. This is a mismatch."
                },
                "questions": {
                    "value": "- How much more difficulty would it take to extend the analysis to multiple layers?\n\n- Do you see a way to argue that the dependence on width (in equation (4)) is tight/necessary?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813598184,
            "cdate": 1698813598184,
            "tmdate": 1699637077620,
            "mdate": 1699637077620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eu6Ky6327O",
                "forum": "ohdVLirfbz",
                "replyto": "BcjvZVbXdS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your invaluable and constructive comments"
                    },
                    "comment": {
                        "value": ">** Q1. The proof thus far only works for three layers. [Although it is believable that with more work, one may be able to extend the analysis for multiple layers, e.g., by arguing a similar weak convexity condition on the loss function.] / The abstract (and the paper's title) mentions multi-layer neural networks, but the analysis currently holds for three layers. This is a mismatch. \n\nThanks for your constructive comment. Following your excellent suggestion, we have changed our title to \"Generalization guarantees of gradient descent for shallow neural networks\", and have revised the abstract accordingly. \n\n\n\n>** Q2. It is by now widely accepted that the NTK analysis is more of an analogy rather than a realistic characterization of what happens in reality. Thus, the practical relevance of this paper, which follows similar lines of arguments as the NTK (e.g., by rescaling the loss and expanding the network width), is very limited. \n\nThank you for your comment. In this work, we provide an alternative approach in a kernel-free regime to study the generalization guarantees of GD with neural networks, using the concept of algorithmic stability.  *One noteworthy merit of our stability analysis lies in its ability to bypass tailored assumptions frequently necessitated by prior works [1,2].* Notably, these assumptions encompass aspects like random initialization and data distribution/kernel matrices which have a positive eigenvalue. In this regard, we consider our stability analysis perspective to be both complementary to existing research and at the same time, sheds new light on understanding GD\u2019s generalization in neural networks.  \n\nMoreover, our generalization results show that for both two-layer and three-layer neural networks, *GD can achieve the dimension-independent generalization bound with only very milder overparameterization* (e.g., $c=1/2$ and $\\mu=1/2$, $m\\asymp n^{3/2}$,  $c=1$ and $\\mu=1/3$, $m\\asymp n^{3/2}$ for two-layer neural networks. More details can be found in Corollaries 4 and 8).\n\nFor instance, Theorem 5.1 of [1] established generalization bounds of GD for two-layer ReLU neural networks with random initialization under an assumption on data distribution, very high overparameterization and the least eigenvalue of the kernel matrix. [2] introduced the neural tangent random feature (NTRF) model, and proved the generalization bounds of one-pass SGD for deep ReLU neural networks with random initialization in the over-parameterization regime, which requires the network weights to stay very close to their initialization throughout training. Their results imply with high overparameterization $m\\ge n^7$ that *if the data can be classified by a function in the NTRF function class with a small training error*, a small classification error can be achieved with the over-parameterized ReLU network. \n\nTo conclude, our stability-based results do not require conditions on the random initialization or the data distribution, and provide generalization bounds under milder overparameterization. In this sense, the perspective from our stability analysis sheds new light on comprehending GD's generalization in neural networks.\n\n\n[1] S. Arora, S. Du, W. Hu, et al. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. ICML 2019.\n\n[2] Y. Cao and Q. Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. NeurIPS 2019.\n\n>** Q3. How much more difficulty would it take to extend the analysis to multiple layers?\n\nThe key challenge of extending our results from three layers to multiple layers is to control the maximum and minimum eigenvalues of a Hessian matrix of the empirical risk, which rely on the norm of the coupled weights of different layers. Once having the estimates of eigenvalues, we can establish the almost co-coercivity of the gradient operator, which is crucial for the stability analysis. However, it's not easy to estimate the coupled weights of different layers by using the existing technique. We acknowledge the importance of considering the applicability of our findings to networks with multiple layers.  Since the existing stability analysis mainly focused on two-layer neural networks, we regard our work as an important starting point to study multiple layers for future research directions. We have added some discussion in the Conclusion to illustrate the main technical challenges in studying the multi-layer case via algorithmic stability."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483583590,
                "cdate": 1700483583590,
                "tmdate": 1700483583590,
                "mdate": 1700483583590,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X0XFSFJLa0",
                "forum": "ohdVLirfbz",
                "replyto": "BcjvZVbXdS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">** Q4. Do you see a way to argue that the dependence on width (in equation (4)) is tight/necessary?\n\nThanks for the thoughtful comment. The dependence on width $m$ in equation (4) relies on estimating the maximum/minimum eigenvalue of the Hessian matrix of the empirical risk. Therefore, it may  not be precisely tight. It remains a challenging problem to explore what is the tight bound for width $m$.  However, as mentioned in Remark 1, [1] provided a similar generalization bound with an assumption $m\\gtrsim (\\eta T)^5/n^2 + (\\eta T)^2$, and our result with $c=1/2$ is consistent with their result. Further, [2] derived a similar excess risk bound for $c=1/2$ (refer to Theorem 3 in the revised version) with $m\\gtrsim (\\eta T)^5$. We relax this condition to $m\\gtrsim (\\eta T)^3$ by providing a better estimation of the smallest eigenvalue of a Hessian matrix. \n\n\n\n[1]  Y. Lei,  R. Jin, and Y. Ying.  Stability and generalization analysis of gradient methods for shallow neural networks. In Advances in Neural Information Processing Systems, volume 35, 2022.\n\n\n[2] D. Richards and I. Kuzborskij. Stability $\\\\&$ generalisation of gradient descent for shallow neural networks without the neural tangent kernel. In Advances in Neural Information Processing Systems, volume 34, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483619683,
                "cdate": 1700483619683,
                "tmdate": 1700485228858,
                "mdate": 1700485228858,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4J7oSZc9Ev",
            "forum": "ohdVLirfbz",
            "replyto": "ohdVLirfbz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8613/Reviewer_51QV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8613/Reviewer_51QV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization performance of multi-layer neural networks using stability-based techniques. \nCompared to previous works, this paper extends previous results to multi-layer neural networks. \nThe primary focus revolves around the width requirements, specifically the scaling parameter (denoted as \"m^-c,\" where \"m\" represents network width and \"c\" is closely associated with NTK and mean-field concepts), and the model capacity (quantified by the norm of W*, denoted as \"\u03bc\"). The overarching conclusion of this study is that wider width requirements are necessary when \"c\" and \"\u03bc\" are large. Additionally, the authors observe that the width requirements are less stringent for three-layer neural networks. In summary, I recommend a modest acceptance of this paper due to its foundational concepts and valuable technical contributions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper offers a comprehensive perspective by encompassing various existing works. Notably, it addresses the applicability of the scaling factor, including scenarios such as NTK and mean field. While the primary focus is on three-layer neural networks, the authors consider the potential for direct application to networks with multiple layers.\n\n2. A substantial technical contribution of this paper is the derivation of an upper bound for the W-norm, as presented in Lemma B.2. The writing is clear, and the proofs appear accurate, inspiring confidence in the results. However, a comprehensive validation of all details is still pending."
                },
                "weaknesses": {
                    "value": "1. It seems that the authors primarily concentrate on three-layer neural networks. It would be beneficial if the authors explicitly discuss the potential applicability of their findings to networks with multiple layers.\n\n2. The uniformity of scaling factors across layers may appear somewhat unconventional. It would be helpful if the authors provided further insight into this choice and its implications.\n\n3. It would be beneficial if the authors could offer more intuitive explanations regarding how the upper bound on the W-norm improves from a t-scale to a sqrt{t}-scale, shedding light on the underlying mechanisms that drive this improvement."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826122854,
            "cdate": 1698826122854,
            "tmdate": 1699637077493,
            "mdate": 1699637077493,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0vC1CMIMZB",
                "forum": "ohdVLirfbz",
                "replyto": "4J7oSZc9Ev",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your invaluable and constructive comments"
                    },
                    "comment": {
                        "value": ">** Q1. It seems that the authors primarily concentrate on three-layer neural networks. It would be beneficial if the authors explicitly discuss the potential applicability of their findings to networks with multiple layers. \n\nGreat point! We acknowledge the importance of considering the applicability of our findings to networks with multiple layers. Since the existing stability analysis mainly focused on two-layer neural networks, we regard our work as an important starting point to study multiple layers for future research directions. Indeed, extending our results from three layers to multiple layers is not straightforward. The main difficulty lies in estimating the maximum and minimum eigenvalues of a Hessian matrix of the empirical risk, which rely on the interconnected weights across layers. Controlling these interconnected weights using existing techniques presents considerable difficulty. In the Conclusion section, we've included discussions highlighting the principal technical obstacles in investigating multiple layers through algorithmic stability. \n \n\n\n>** Q2. The uniformity of scaling factors across layers may appear somewhat unconventional. It would be helpful if the authors provided further insight into this choice and its implications. \n\n\nThank you for the constructive suggestion. To more explicitly illustrate the relationship between the scaling parameter $c$ and the over/under-parameterization of the network (as shown in Corollaries 4 and 8, and Figure 1), we make the assumption that both the width and scaling factors of each hidden layer are identical. Nevertheless, our findings can readily extend to scenarios where the widths and scaling factors vary across layers. In the revised version, we've included additional clarifications to accommodate this aspect.\n\n\n>** Q3. It would be beneficial if the authors could offer more intuitive explanations regarding how the upper bound on the W-norm improves from a t-scale to a $\\sqrt{t}$-scale, shedding light on the underlying mechanisms that drive this improvement. \n\n\nThank you for the constructive suggestion. As mentioned in Remark 4, for three-layer neural networks, the technical challenge in estimating $||\\mathbf{W}_t||_2$ is closely tied to the smoothness of the loss function. Simultaneously, the smoothness parameter of the loss function relies on the norm of $\\mathbf{W}_t$.   We overcame this difficulty by initially providing  a rough bound $||\\mathbf{W}_t-\\mathbf{W}_0||_2\\le \\eta t m^{2c-1}$ by induction. Once having the estimate of $||\\mathbf{W}_t||_2$, we can upper bound the maximum eigenvalue of the Hessian by a constant $\\hat{\\rho}$ if $m$ satisfies (6). It implies that the loss is $\\hat{\\rho}$-smooth along the trajectory of GD.  Leveraging this smoothness property, we deduce that  \n\n$\\eta(1-\\eta \\hat{\\rho}/2)\\sum\\_{j=0}^t||\\nabla L_S(\\mathbf{W}_j)||_2^2\\le\\sum\\_{j=0}^t L_S(\\mathbf{W}\\_j)-L_S(\\mathbf{W}\\_{j+1})\\le L_S(\\mathbf{W}\\_0).$\n\nAdditionally, considering the update rule of GD and $\\eta \\hat{\\rho}\\le 1/2$, we conclude that  $||\\mathbf{W}_t-\\mathbf{W}_0||_2^2= \\eta^2 ||\\sum\\_{j=0}^t \\nabla L_S(\\mathbf{W}_j)||_2^2 \\le 2\\eta t L_S(\\mathbf{W}_0)$. \n\nWe have added more details in Remark 4 in the revised version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483398972,
                "cdate": 1700483398972,
                "tmdate": 1700483398972,
                "mdate": 1700483398972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vey51XcNxJ",
                "forum": "ohdVLirfbz",
                "replyto": "0vC1CMIMZB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8613/Reviewer_51QV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8613/Reviewer_51QV"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response."
                    },
                    "comment": {
                        "value": "I thank the authors for the response and keep my score unchanged. \nI believe that extending the existing results to multi-layers, if possible, could greatly enhance the current manuscript. \nGiven the current version, I just keep my score unchanged."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536773600,
                "cdate": 1700536773600,
                "tmdate": 1700536773600,
                "mdate": 1700536773600,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1qnpvJvSfw",
            "forum": "ohdVLirfbz",
            "replyto": "ohdVLirfbz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8613/Reviewer_BPeY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8613/Reviewer_BPeY"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides generalization guarantees for the training of neural networks with one and two hidden layers (i.e., with two and three layers) under gradient descent. The paper shows what combinations of the scaling parameter and the so-called complexity parameter, achieves the generalization error bound $O(1/\\sqrt{n})$, where $n$ is the number of samples, for both over- and under-parameterized regimes. Moreover, the paper shows that when an adequate minimizer (weight values for a network) that zeroes the population risk exists, their generalization error improves to $O(1/n)$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-> The paper analyzes different scaling on the network\u2019s width by varying its exponent $c$, defining regimes that haven\u2019t been analyzed in previous works.\n\n-> The paper does a full formal analysis for both two and three layered networks, and show the difficulties on such analyses and their differences, particularly, when it comes to the (almost) co-coercivity property.\n\n-> Discussions are provided after the theoretical results and comparison to existing literature which aids the reader\u2019s understanding of their nature and contribution."
                },
                "weaknesses": {
                    "value": "There are assumptions, concepts, and results that need to be better explained and for which I also raise concerns related to their understanding and implications.\n\n\n-> In Assumption 2, one of the conditions seem to depend on a specific initialization of the weights $W_0$, where that the loss should be less than $c_0$ when evaluated under the initialization. However, nowhere in the paper mentions (at least to that point) how the networks are initialized or if we are assuming a fixed initialization \u2013 I am not sure if the bound holds for $c_0$ uniformly over any initialization of the weights or not. Please, clarify.\n\n\n-> In Theorem 3, it says that equation (4) should hold, but then show another bound on $m$ in equation (5). How should we interpret this? Should we just take the maximum of both lower bounds on $m$?\n\n\n-> Regarding Theorem 4: I would imagine that it would be a good thing for the excess population risk to minimize as the width of the network increases, i.e., as there is over-parameterization. For this, $c>1/2$ is desirable. Then, why would someone choose $c=1/2$ from the perspective of the excess population risk or from the other terms that bound the generalization bound? Why did previous works considered $c=1/2$ to be a good thing?\n\nSo, it seems that having $c>1/2$ benefits decreasing the bound, or at least the excess population risk part of it, as $m$ increases. Therefore, I would expect in Corollary 4 some differentiation in terms of $c=1/2$ and $c>1/2$, but I don\u2019t see such difference (the only difference is made over the value $c=3/4$).\n\n\n-> Following the previous point, it is interesting that the excess population risk for a three-layer neural network in Theorem 8 (excess population risk) does not have a term $m$, and so no dependency on $c$ in the upper bound, unlike its counterpart Theorem 4. Moreover, Theorem 8 does not hold for $c=1/2$ according to its statement. Why is this? How is the derivation here different from the two-layer network? \n\nMoreover, I would expect a $c$ factor to appear in the upper bound because the scaling that depends on $c$ is on the output layer in both two or three layers. Is there any explanation for this? I haven\u2019t checked the math.\n\n\n-> I am trying to understand Assumption 3 and see if it makes sense. Right after equation (2) it is mentioned that $W^*$ has minimum norm. Is Assumption 3 just assuming that such minimum norm must satisfy an upper bound? \n\n\n-> Also, Assumption 3 introduces the parameter $\\mu$ almost arbitrarily \u2013 there is no mention whether the value of $\\mu$ comes from the architecture of the network of the loss function, it seems like a magical parameter! Moreover, it is afterwards that the paper calls $\\mu$ the network complexity, but I think this is a void term. For example, the value of $\\mu$ determines whether the minimizer with minimum norm will have its norm decreasing as the width $m$ increases or not. How is this possible? What is the motivation for this? Whether the norm increases or not depending on the width, since we simply have a two-layer network, should depend on the architecture and the loss itself, not on some external and extraneous parameter. This requires explanation.\n\n\n-> So the terms \u201cover-parameterized\u201d and \u201cunder-parameterized\u201d are used extensively in the paper without a proper definition. It may sound obvious that in the former you have more parameters than data samples, while in the latter the opposite; however, throughout the paper there are different conditions on the width $m$ that makes the distinction to both regimes. Can the exact definitions be defined? For example, under the conditions on $m$ in Corollary 5 and 9, how do I know in which regime I am? Could more explanation on how to define both regimes be given for Figure 1?\n\n\n==\n\n-> Scaling parameter is mentioned in the contributions without any explanation of what this exactly mean. I don\u2019t think it is a standard term: it could mean scaling of the initialization variance, the scaling of the pre-activation, etc.\n\n-> Also the term \u201cnetwork complexity\u201d is mentioned in the introduction without much explanation. Such term seems to be independent from whether the network is over- or under- parameterized (which is kind of strange, since I remember that \u201ccomplexity of the model\u201d usually refers to how many parameters or degrees of freedom it has).\n\n==\n\n-> Though the work (Taheri & Thrampoulidis (2023)) use algorithmic stability, it seems they make strong assumptions on the distribution of the data, which should be mentioned in the paper. \n\n-> The norm symbol for Euclidean norm is defined, but not for the spectral norm (when the argument is a matrix).\n\n-> Theorem 6 and Remark 4 mention the \u201ccondition (B.9)\u201d, however, such condition seems to not be on the main paper, making the paper not self-contained.\n\n-> Besides the introduction, the property of almost co-coercivity is not mentioned until section 3.2. It would be a good idea to introduce its definition from the beginning."
                },
                "questions": {
                    "value": "Please, see the \"Weaknesses\" section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8613/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698856497511,
            "cdate": 1698856497511,
            "tmdate": 1699637077373,
            "mdate": 1699637077373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rkUjYCv1RS",
                "forum": "ohdVLirfbz",
                "replyto": "1qnpvJvSfw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8613/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your careful reading and constructive comments"
                    },
                    "comment": {
                        "value": ">**Q1. In Assumption 2, one of the conditions seem to depend on a specific initialization of the weights $\\mathbf{W}_0$, where the loss should be less than $c_0$ when evaluated under the initialization. However, nowhere in the paper mentions (at least to that point) how the networks are initialized or if we are assuming a fixed initialization \u2013 I am not sure if the bound holds for $c_0$ uniformly over any initialization of the weights or not. Please, clarify. \n\nThank you for this thoughtful comment. We would like to emphasize that our results hold for both a random initialization setting and a fixed initialization setting *as long as $\\ell(\\mathbf{W}_0;z)$ is uniformly bounded by a constant $c_0$*. Indeed, one can verify that such uniform bound $c_0$ holds true for some widely used activations including sigmoid activation and hyperbolic tangent activation for any initialization. Since we assume that $\\mathbf{x}$ and $y$ are bounded,    $f_{\\mathbf{W}_0}(\\mathbf{x})$ is uniformly bounded for both two-layer and three-layer cases by noting that the sigmoid and hyperbolic tangent activation functions $\\sigma(\\mathbf{w}_k\\^\\top \\mathbf{x})\\le 1$ for any $k$. Consequently, for any $\\mathbf{W}_0$,  the least square loss $\\ell(\\mathbf{W}_0;z)$ is uniformly bounded by a constant $c_0$. \n\nIn addition, the assumption of the loss is standard in the stability and generalization analysis of GD with neural networks [1, 2]. We have added the clarification below Assumption 2 and above Theorem 3 in the revised version.\n\n[1] D. Richards and I. Kuzborskij. Stability $\\\\&$ generalisation of gradient descent for shallow neural networks without the neural tangent kernel. In Advances in Neural Information Processing Systems, volume 34, 2021.\n\n[2]  Y. Lei,  R. Jin, and Y. Ying.  Stability and generalization analysis of gradient methods for shallow neural networks. In Advances in Neural Information Processing Systems, volume 35, 2022.\n\n\n\n>** Q2.  In Theorem 3, it says that equation (4) should hold, but then show another bound on $m$ in equation (5). How should we interpret this? Should we just take the maximum of both lower bounds on $m$?\n \n\nYes, we take the maximum of both lower bounds on $m$ to ensure that our main result for two-layer NNs (Theorem 3 in the revised version) holds. This is what we did in Corollary 4, i.e., choosing $m\\gtrsim (\\eta T)^{\\frac{3}{2c}}$ such that the maximum of the lower bounds in (4) and (5) hold. Notably, the generalization error bounds in Theorem 1 hold under condition (4). Introducing condition (5), we further establish the optimization error bounds in Theorem 2. Combining these two results together, we derive the excess risk bounds in Theorem 3 when (4) and (5) hold simultaneously."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8613/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484592795,
                "cdate": 1700484592795,
                "tmdate": 1700485174631,
                "mdate": 1700485174631,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]