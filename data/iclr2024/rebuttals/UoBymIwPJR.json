[
    {
        "title": "Query-Policy Misalignment in Preference-Based Reinforcement Learning"
    },
    {
        "review": {
            "id": "oxJw1Xj82J",
            "forum": "UoBymIwPJR",
            "replyto": "UoBymIwPJR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4518/Reviewer_EPnJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4518/Reviewer_EPnJ"
            ],
            "content": {
                "summary": {
                    "value": "Summary: The paper addresses the challenge of Query-Policy Misalignment in Preference-based Reinforcement Learning (PbRL). PbRL is a method where reinforcement learning (RL) agents align their behavior based on human preferences. However, the efficiency of these models is often restricted due to costly human feedback. The paper identifies that most existing PbRL methods aim to improve the reward model's quality by selecting queries but may not necessarily enhance the RL agent's performance. The authors introduce the concept of policy-aligned query and hybrid experience replay as a solution. These methods focus on improving the alignment between the queries chosen and the current interests of the RL agent, thereby enhancing feedback efficiency. This is done by simply sampling from recent trajectories."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper introduces a novel perspective, highlighting the overlooked issue of Query-Policy Misalignment in PbRL.\n2. The proposed solution of policy-aligned query selection and hybrid experience replay is simple to implement and requires minimal changes to existing PbRL systems.\n3. Comprehensive experiments on well-established benchmarks like DMControl and MetaWorld prove the substantial benefits of the proposed method in terms of feedback and sample efficiency."
                },
                "weaknesses": {
                    "value": "1. The focus is predominantly on off-policy PbRL methods, with limited exploration of on-policy PbRL methods, which naturally select on-policy segments to query preferences.\n2. The approach is quite simple and I\u2019m not sure if it\u2019s novel compared to methods that are more similar to on-policy methods. \n3. The paper should compare to Liu et al. Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning. NeurIPS \u201822, which is the current SoTA for PbRL.\n4. I thought the Figure 1, 2, 3 could be improved and explained better, both in the text, and in the figure, and in the caption."
                },
                "questions": {
                    "value": "Could the proposed methods be adapted or combined with on-policy PbRL techniques to achieve even better results?\nAre there specific scenarios or domains where the proposed method may not be as effective?\nHow does the system handle scenarios where human feedback might be inconsistent or contradictory?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4518/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4518/Reviewer_EPnJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814901906,
            "cdate": 1698814901906,
            "tmdate": 1700590943517,
            "mdate": 1700590943517,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bEiFv75M38",
                "forum": "UoBymIwPJR",
                "replyto": "oxJw1Xj82J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EPnJ"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer EPnJ for the useful comments. For each weakness and question, we provide the following responses.\n\n> W1. The focus is predominantly on off-policy PbRL methods, with limited exploration of on-policy PbRL methods, which naturally select on-policy segments to query preferences.\n\n- As we discussed in \"Conclusion and Discussion\" on Page 9, although on-policy PbRL methods naturally select on-policy segments, they suffer from severe sample inefficiency issue as compared to off-policy PbRL methods. Due to the prolonged policy learning process, the query segments generated by slowly learned policies are often not very informative for rapid reward model learning, which in turn causes poor feedback efficiency in on-policy PbRL methods. \n- We add the performance curve of PrePPO (on-policy PbRL method) [1] to Figure 5 and 6 on Page 8. We can see that PrePPO's performance is notably inferior to current SOTA off-policy PbRL methods.\n- All the recent SOTA PbRL methods, including ours, are built upon off-policy framework. We discover that the query-policy misalignment can potentially harm feedback efficiency, and thus enforces query selection to align with the policy induced distribution $d^{\\pi}$. Note that this is different from conventional on-policy RL, as our treatment is conducted on the query selection part (i.e., selecting proper query segments that are already in the replay buffer), rather than collecting on-policy samples for policy learning as in typical on-policy RL methods.\n\n> W2. The approach is quite simple and I\u2019m not sure if it\u2019s novel compared to methods that are more similar to on-policy methods.\n\n- To the best of our knowledge, our paper is the first study that reveals the long-neglected query-policy misalignment causes query inefficiency in PbRL. Most existing studies focus on selecting the most \"informative\" segment for preference labeling. However, as demonstrated in our work, this can be less effective as expected. On the other hand, we provide a remarkably simple method to improve feedback efficiency in PbRL without any fancy query selection scheme.\n- Second, as we have clarified in the previous response, our method is an **off-policy method**, simply using an on-policy PbRL method performs poorly due to sample inefficiency. \n\n> W3. The paper should compare to Liu et al. Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning. NeurIPS '22, which is the current SoTA for PbRL.\n\n- We appreciate the reviewer's helpful reminder! The performance of Meta-Reward-Net (MRN) and its comparison to QPA has been added in Figure 12, Appendix D.2. In most tasks, our method QPA consistently outperforms MRN. Essentially, MRN takes a good idea of adopting the performance of the Q-function as the learning target to formulate a bi-level optimization problem, which is orthogonal to our methods. Our technique *policy-alignment query selection* can be easily incorporated into MRN to further improve feedback efficiency (MRN + policy-alignment query selection).\n\n\n> W4. I thought the Figure 1, 2, 3 could be improved and explained better, both in the text, and in the figure, and in the caption.\n\n- We thank the reviewer for the suggestion. We have added more explanations on Page 2-4. Furthermore, we have provided very detailed experiment settings and descriptions in Appendix C.\n\n\n> Q1. Could the proposed methods be adapted or combined with on-policy PbRL techniques to achieve even better results? \n\n- Our proposed method focuses on the query selection and value learning from the **replay buffer**. In contrast, in on-policy PbRL, these processes occur within **on-policy rollout trajectories**, where the policy-aligned query selection and hybrid experience are no longer applicable. As we have explained in the previous response as well as the additional results of on-policy method PrePPO, the low sample efficiency and prolonged policy learning process in on-policy PbRL can hurt feedback efficiency. By contrast, our proposed method has achieved SOTA performance.\n\n> Are there specific scenarios or domains where the proposed method may not be as effective?\n\n- When the reward function is highly non-smooth in the state-action space and drastically different in various regions, hard exploration might be needed to learn the reward model effectively. In such case, relying solely on policy-aligned query selection might lead to getting trapped in local solutions.\n\n> How does the system handle scenarios where human feedback might be inconsistent or contradictory?\n\n- We have added new experiments with human feedback, which could potentially contain inconsistencies. The results are presented in Appendix E, we find our proposed QPA still achieves reasonable/good performance.\n\n**Reference**\n\n[1] Deep reinforcement learning from human preferences, NeurIPS, 2017."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037392239,
                "cdate": 1700037392239,
                "tmdate": 1700037392239,
                "mdate": 1700037392239,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E1BwQWgczK",
                "forum": "UoBymIwPJR",
                "replyto": "bEiFv75M38",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4518/Reviewer_EPnJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4518/Reviewer_EPnJ"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. I have bumped my score up. Please consider adding the MRN results in the main text."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590934303,
                "cdate": 1700590934303,
                "tmdate": 1700590934303,
                "mdate": 1700590934303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U2Y5Io2z5q",
            "forum": "UoBymIwPJR",
            "replyto": "UoBymIwPJR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4518/Reviewer_EEB5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4518/Reviewer_EEB5"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of query-policy misalignment in preference-based reinforcement learning (PbRL) and introduces a novel method, QPA, consisting of two main techniques, policy-aligned query selection, and hybrid experience replay, to improve the efficiency of human feedback in PbRL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* This paper offers a fresh viewpoint on an underexplored issue in PbRL and proposes an interesting solution.\n\n* The conducted experiments are extensive, covering a range of tasks and comparing against multiple benchmark methods.\n\n* The paper is written well, the problem is clearly defined, the proposed approach is thoroughly explained, and the empirical evaluation is meticulously conducted.\n\n* The results show the effectiveness of the proposed method in achieving significant gains in performance."
                },
                "weaknesses": {
                    "value": "* Query-policy misalignment not conclusively proven: While the paper proposes that query-policy misalignment is an interesting hypothesis to cause a problem in PbRL, there is no comprehensive evaluation or solid evidence to confirm this proposal.\n\n* Lack of real human experiments: The testing and validation conducted in the paper relied on a scripted annotator which is not representative of real-world users, which could limit the generalizability and applicability of the approach."
                },
                "questions": {
                    "value": "* The paper could be strengthened by testing the proposed method under real-world conditions, using actual human annotators instead of an oracle, to test the generalizability of their results.\n\n* Providing more implementation details or pseudocode for the proposed method would add value to the paper and make it easier for others to understand, replicate, and build upon the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698845369674,
            "cdate": 1698845369674,
            "tmdate": 1699636428454,
            "mdate": 1699636428454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1r45J76mTb",
                "forum": "UoBymIwPJR",
                "replyto": "U2Y5Io2z5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EEB5"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer EEB5 for the constructive comments. For each weakness and question, we provide the following responses.\n\n> W1. Query-policy misalignment not conclusively proven: While the paper proposes that query-policy misalignment is an interesting hypothesis to cause a problem in PbRL, there is no comprehensive evaluation or solid evidence to confirm this proposal.\n\n- We really appreciate this valuable comment. We have added Figure 11 and Appendix D.1 to confirm this proposal. As all the recent off-policy PbRL methods are built upon PEBBLE, we compare the two methods: PEBBLE and PEBBLE + policy-aligned query selection in locomotion tasks. The policy-aligned query selection is proposed in Section 5.1 to address the query-policy misalignment.\n    - We compute the log-likelihood of current policy $\\pi$ using the queried segments at each query time. Figure 11(a) shows that the segments queried by PEBBLE exhibit a low log-likelihood of $\\pi$, indicating that these segments fall outside the distribution of the current policy $\\pi$. This demonstrates the existence of the query-policy misalignment.\n    - Figure 11(a) shows that, if incorporating policy-aligned query selection into PEBBLE to confine the queries to the local policy-aligned buffer, there will be a substantial increase in the log-likelihood of $\\pi$, which means the query-policy misalignment can be addressed.\n    - Figure 11(b) shows that the performance of PEBBLE + policy-aligned query selection significantly surpasses that of PEBBLE.\n- Overall, Figure 11 demonstrates that: (1) the query-policy misalignment issue does exist in typical PbRL methods and does cause feedback inefficiency; (2) using policy-aligend query selection to address the query-policy misalignment can result in a significant improvement in feedback efficiency.\n\n\n> W2. Lack of real human experiments: The testing and validation conducted in the paper relied on a scripted annotator which is not representative of real-world users, which could limit the generalizability and applicability of the approach.\n> \n> Q1. The paper could be strengthened by testing the proposed method under real-world conditions, using actual human annotators instead of an oracle, to test the generalizability of their results.\n\n\n- The use of scripted ground truth reward is to evaluate and compare these PbRL methods quantitatively, unbiasedly and quickly, which is a common practice in existing PbRL literature [1-6].\n- We have included additional real human experiments in Appendix E to showcase the applicability and effectiveness of our method. Using feedback from real humans, our method also notably improves feedback efficiency compared to PEBBLE.\n\n\n> Q2. Providing more implementation details or pseudocode for the proposed method would add value to the paper and make it easier for others to understand, replicate, and build upon the proposed method.\n\n- We thank the reviewer for this kind suggestion. We have provided the pseudocode in Appendix B and implementation details in Appendix C on Page 13-16. These details encompass the implementation framework, query selection scheme, data augmentation explanation, and hyperparameter settings.\n\n**Reference**\n\n[1] Deep reinforcement learning from human preferences, NeurIPS, 2017.\n\n[2] PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training, ICML, 2021.\n\n[3] SURF: Semi-supervised Reward Learning with Data Augmentation for Feedback-efficient Preference-based Reinforcement Learning, ICLR, 2022.\n\n[4] Reward Uncertainty for Exploration in Preference-based Reinforcement Learning, ICLR, 2022.\n\n[5] Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning, NeurIPS, 2022.\n\n[6] Inverse Preference Learning: Preference-based RL without a Reward Function, NeurIPS, 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037253156,
                "cdate": 1700037253156,
                "tmdate": 1700037253156,
                "mdate": 1700037253156,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0MRYmJjv0m",
                "forum": "UoBymIwPJR",
                "replyto": "1r45J76mTb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4518/Reviewer_EEB5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4518/Reviewer_EEB5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed answers. I have one more suggestion: it would be nice if the authors could provide more details about the human labeling process (user interface, labeling instruction, when to collect human feedback and so on) for additional human results in Appendix E. Based on responses, I'd like to keep my original rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379600409,
                "cdate": 1700379600409,
                "tmdate": 1700379600409,
                "mdate": 1700379600409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6DTh090DMQ",
                "forum": "UoBymIwPJR",
                "replyto": "U2Y5Io2z5q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4518/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for this good suggestion! \n\nAs suggested, we have added more experimental details of human labeling process in Appendix E on Page 24-25 in our latest version paper. Specifically, we have provided the main Python code for segment video rendering, labeling instructions, and human preference collection on Page 24-25. The user interface is shown in added Figure 25. The human feedback frequency and total feedback remain consistent with the experimental setup outlined in Table 2. The supplementary material includes the videos of agent training processes in human experiments.\n\nWe sincerely thank the time and effort you have engaged in the review and discussion phase!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700395969609,
                "cdate": 1700395969609,
                "tmdate": 1700396355047,
                "mdate": 1700396355047,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PM9pnNQMm7",
            "forum": "UoBymIwPJR",
            "replyto": "UoBymIwPJR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4518/Reviewer_LzS2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4518/Reviewer_LzS2"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the Authors set to improve feedback efficiency in RLHL (RL from human feedback). To this end, they propose to (1) use only recent behaviors of RL agents for queries and to (2) update RL agents using recent experiences alongside experiences uniformly sampled from the replay buffer. The Authors test their proposed sampling approaches in several existing RLHF models on a number of control benchmarks where they show improved performance in comparison to baseline approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses an important problem of improving the feedback efficiency in RLHF. As RLHF is widely used today, in particular, with LLMs (large language models), the results offered in this work are important, timely, and definitely relevant to the community.\n\nThe simplicity of the approach is also a plus. As the Authors state in the paper, their sampling strategies require minor additions to the existing models\u2019 code, on the order of a couple dozen lines of code, so their approach is easy to implement.\n\nThe proposed approach has been tested on a variety of control tasks where it shows improvement in performance compared to the baseline models.\n\nOverall, I think this submission is a solid work with a clear message and thoroughly conducted experiments."
                },
                "weaknesses": {
                    "value": "I wasn\u2019t able to fully follow some of the logic regarding the justification of the sampling scheme design. Throughout Pages 5 and 6, I understood the equations describing the error bounds for the Q-functions and the rewards but I couldn\u2019t see the formal connection between these bounds and the proposed sampling schemes. If there *is* a formal connection, I suggest elaborating on it, e.g. in the statement in 5.1. that goes: \u201cBy assigning more observers feedback <\u2026> we aim to enhance the accuracy of the preference (reward) predictor <\u2026> This aligns with the intuition from the condition <\u2026> in Eq. (5).\u201d Similarly, in 5.2, the claim: \u201cThe proposed mechanism can provide assurance that the Q-function is updated adequately near $d^\\pi$\u201d may need further elaboration formally supporting it. If there *isn\u2019t* a formal connection between these claims and Eq. (5), I suggest removing Eq. (5) and the related references as, in the current standing, they may be confusing to a reader like myself.\n\nAt the same time, similar ideas have been explored in literature. The field of decision-aware model learning (or value equivariance model learning) used the idea that, in model-based RL, learning a model that\u2019s accurate everywhere might be unnecessary and, instead, learning a model that\u2019s only accurate in task-critical regions of the state space may offer a way of improving models\u2019 sample complexity. Although these approaches, to my knowledge, have not been applied to RLHF, they pursued similar goals (i.e. improving the sample complexity) by using similar methods (i.e. focusing on the prediction accuracy for task-relevant states and transitions). I suggest discussing these (and similar) lines of work and their relation to the proposed sampling approach in the paper."
                },
                "questions": {
                    "value": "See above\n\n_____________________\nPost-rebuttal. The Authors have diligently addressed my concerns and, to my knowledge, did a good job of addressing the other Reviewers' concerns. Also, I see that this paper is of a much higher quality than a typical paper at this venue this year that has been assigned the same score as I initially put here. To reflect on both of these facts, I increase my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4518/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4518/Reviewer_LzS2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4518/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699683260411,
            "cdate": 1699683260411,
            "tmdate": 1700508411603,
            "mdate": 1700508411603,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V2tQBCBwOw",
                "forum": "UoBymIwPJR",
                "replyto": "PM9pnNQMm7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4518/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LzS2"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer LzS2 for the insightful comments. For each weakness and question, we provide the following responses.\n\n> W1. I wasn\u2019t able to fully follow some of the logic regarding the justification of the sampling scheme design...\n\n- We thank the reviewer for this helpful comment. We explain the connection between the analysis of Eq.(5) and the proposed technique as follows. \n    - The proposed technique, policy-aligned query selection, allocates more queries according to the induced distribution $d^{\\pi}$ of current policy $\\pi$. This could potentially enhance the accuracy of reward learning within distribution $d^{\\pi}$, enforcing the condition of $||\\hat{r}\\_{\\psi} - r||\\_{d^{\\pi}} \\leq \\epsilon$.\n    - The proposed technique, hybrid experience replay, samples more fresh transitions within the $d^{\\pi}$ to update the Q function. This pays more attention to improving the quality of Q-function within $d^{\\pi}$, enforcing the condition of $||Q\\_{\\hat{r}\\_{\\psi}}^{\\pi} - \\hat{Q}\\_{\\hat{r}\\_{\\psi}}^{\\pi} ||\\_{d^{\\pi}} \\leq \\alpha$.\n    - As shown in Eq.(5), these two conditions would lead to a concrete error bound on the approximated Q-value $\\hat{Q}\\_{\\hat{r}\\_{\\psi}}^{\\pi}$.\n- We have taken the reviewer's suggestion and moved the analysis of Eq.(5) to the Appendix A. We thank the reviewer again for this helpful comment.\n\n> W2. At the same time, similar ideas have been explored in literature. The field of decision-aware model learning (or value equivariance model learning) used the idea that, in model-based RL...\n\n- We really appreciate the reviewer for this valuable suggestion! Good point! We have added the discussion about the high-level connections between these model-based decision-making methods and our method in Section 2 on Page 3. This could help the readers to better grasp the core ideas and rationale behind our method.\n- We have added the discussion and refrences of the following model-based decision-making (local decision-aware model learning) papers. Please let us know if we are missing any important relavant works.\n\n**Added relevant model-based decision-making references:**\n\n[1] Learning neural network policies with guided policy search under unknown dynamics, NeurIPS, 2014.\n\n[2] Learning contact-rich manipulation skills with guided policy search, arXiv, 2018.\n\n[3] One-shot learning of manipulation skills with online dynamics adaptation and neural network priors, IROS, 2016.\n\n[4] Sample-based informationl-theoretic stochastic optimal control, ICRA, 2014.\n\n[5] Autonomous helicopter control using reinforcement learning policy search methods, ICRA, 2001.\n\n[6] Locally weighted learning for control, Springer, 1997.\n\n[7] The value equivalence principle for model-based reinforcement learning, NeurIPS 2020.\n\n[8] Value prediction network, NeurIPS, 2017.\n\n[9] The predictron: End-to-end learning and planning, ICML, 2017.\n\n[10] Value iteration networks, NeurIPS, 2016.\n\n[11] Universal planning networks: Learning generalizable representations for visuomotor control, ICML, 2018.\n\n[12] Live in the moment: Learning dynamics model adapted to evolving policy, ICML, 2023.\n\n[13] Objective mismatch in model-based reinforcement learning, L4DC, 2020.\n\n[14] Learning Policy-Aware Models for Model-Based Reinforcement Learning via Transition, L4DC, 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700037138116,
                "cdate": 1700037138116,
                "tmdate": 1700037138116,
                "mdate": 1700037138116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t0GyHXCbCm",
                "forum": "UoBymIwPJR",
                "replyto": "V2tQBCBwOw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4518/Reviewer_LzS2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4518/Reviewer_LzS2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. This addresses my questions. I also appreciate highlighting the text changes in blue which made them easy to follow.\n-Re: prior literature: nice reference list! This definitely includes the papers I had in mind;\n-Re: motivation of the method: thanks for adding clarity on the motivation being an *intuitive* explanation. As long as it's stated as such, I believe it's completely correct."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456328931,
                "cdate": 1700456328931,
                "tmdate": 1700456328931,
                "mdate": 1700456328931,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]