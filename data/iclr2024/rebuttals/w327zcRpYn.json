[
    {
        "title": "SUBER: An RL Environment with Simulated Human Behavior for Recommender Systems"
    },
    {
        "review": {
            "id": "tCSpwF0hRp",
            "forum": "w327zcRpYn",
            "replyto": "w327zcRpYn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2439/Reviewer_7Rhx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2439/Reviewer_7Rhx"
            ],
            "content": {
                "summary": {
                    "value": "The paper propose SUBER, a RL environment that applies LLM to simulate user behaviors. SUBER consists of several components including memory, preprocessing,  postprocessing, and LLM modules. The user history is sent to the RL module, and it returns an item. Both user history and the recommended item are processed as a prompt, then the LLM outputs the simulated user rating over the item. SUBER is built on two public datasets. The paper does experiments to validate its effectiveness. Finally, the paper shows that how A2C is trained in this environment."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.Using LLM to simulate user behaviors is interesting.\n2.The paper is easy to follow.\n3.The paper does detailed ablation study."
                },
                "weaknesses": {
                    "value": "1.The paper does not discuss the limitation of SUBER. In my opinion, I think the input feature is quite simple, and miss numerical and sequential informations.\n2.The paper does not compare SUBER and other RL-based simulators, such as VirtualTaobao, RecoGym. Thus it is quite hard to evaluate the significance of SUBER in the RL4RS area.\n3.The paper does not cite two recent papers about RL4RS simulators, \"RL4RS: A Real-World Dataset for Reinforcement Learning based Recommender System\" and KuaiSim: A Comprehensive Simulator for Recommender Systems.\n4.As an RL environment, I would suggest that the author evaluate more RL algorithms besides A2C, such as SA2C(Supervised Advantage Actor-Critic for Recommender Systems), HAC(Exploration and Regularization of the Latent Action Space in Recommendation) and off-policy top-k(Top-K Off-Policy Correction for a REINFORCE Recommender System)."
                },
                "questions": {
                    "value": "See the above question."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2439/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698493359469,
            "cdate": 1698493359469,
            "tmdate": 1699636179552,
            "mdate": 1699636179552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tMOLHUCTxq",
                "forum": "w327zcRpYn",
                "replyto": "tCSpwF0hRp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their effort on reviewing our work. We address their concerns in the following.\n\n> The paper does not discuss the limitation of SUBER. In my opinion, I think the input feature is quite simple, and miss numerical and sequential informations.\n\nThe feature space of our environment contains both numerical and sequential information, i.e., for each user we have a list of all movies previously watched together with their rating in sequential order. \n\nSince we wanted to demonstrate that a simple RL model can learn to recommend items in our environment, we use a simple state space that only records the last rating that the user gave to a movie. We chose this simpler scenario to show the learnability of our environment, and that the trained policy has some interesting features that we can interpret as human (cf. Appendix E.1, Figure 7). We have added more experiments on other RL algorithms in the revised manuscript (Section 4.3).\n\nIt is also worth noting that the environment can be adapted easily and can support any type of feature.\n\n> The paper does not compare SUBER and other RL-based simulators, such as VirtualTaobao, Recogym. Thus it is quite hard to evaluate the significance of SUBER in the RL4RS area.\n\nThe lack of comparisons with alternative environments is mainly because of the synthetic nature of our experimental setting. Thus, it is impractical to draw direct comparisons with other environments. Our environment is more closely related to Recogym [1] and Recsim [2] in this regard. We have provided a more extensive answer on this topic in the general response.\n\n> The paper does not cite two recent papers about RL4RS simulators, \"RL4RS: A Real-World Dataset for Reinforcement Learning based Recommender System\" and KuaiSim: A Comprehensive Simulator for Recommender Systems.\n\nThank you for pointing out these two contemporary works, we have updated our related work section to include these in the revised manuscript.\n\n> As an RL environment, I would suggest that the author evaluate more RL algorithms besides A2C, such as SA2C(Supervised Advantage Actor-Critic for Recommendwiller Systems), HAC(Exploration and Regularization of the Latent Action Space in Recommendation) and off-policy top-k(Top-K Off-Policy Correction for a REINFORCE Recommender System).\n\nWe have extended our analysis beyond the initial experiments with A2C, and added evaluations for additional RL algorithms (e.g., PPO, TRPO, DQN).\n\nRegarding the specific methods the reviewer mentioned (SA2C, HAC, top-k), we face practical limitations. These methods currently lack publicly available, gym-compatible interfaces, making direct comparison within our environment challenging. We acknowledge their importance and hope to include such methods in future work as they become more accessible.\n\nHowever, we would like to emphasize that our primary contribution lies in providing a versatile recommender system environment designed for experimenting with various RL and RecSys algorithms. The intent is to offer a platform for future explorations rather than conducting exhaustive evaluations of all existing algorithms.\n\nReferences:\n\n[1] Rohde, et al. (2018). Recogym: A reinforcement learning environment for the problem of product recommendation in online advertising.\n\n[2] Ie et al. (2019). Recsim: A configurable simulation platform for recommender systems."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427153572,
                "cdate": 1700427153572,
                "tmdate": 1700427153572,
                "mdate": 1700427153572,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SApUPqx47g",
            "forum": "w327zcRpYn",
            "replyto": "w327zcRpYn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2439/Reviewer_TXpX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2439/Reviewer_TXpX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed SUBER, a framework designed to address common challenges in RL-based recommender systems, such as issues related to data availability and the design of reward functions. The paper conducted several ablation studies on movie and book recommendations to demonstrate the effectiveness of the method and examine the effect of each component in the framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tThe motivation of this paper is clear and the challenges it aims to address are significant to the RL-based recommender system.\n-\tThe method is presented clearly. Each component is well explained, and the flow of the entire framework is well presented in the figure.\n-\tThe paper conducted a series of ablation studies to scrutinize the effect of different components within the framework."
                },
                "weaknesses": {
                    "value": "-\tThe proposed  framework is to tackle key challenges in RL4Rec, such as data accessibility, the uncertainty of the user model, and the assessment of models. However, the originality of this research is ambiguous to me. It appears to predominantly integrate components of Reinforcement Learning (RL) with Large Language Models (LLMs). Furthermore, due to the absence of a thorough comparison with existing RL simulators and state-of-the-art techniques including RL4Rec and LLM-integrated RecSys, it's challenging to position the precise significance of the contributions claimed in this study.\n-\tThe paper has only made comparisons between different settings of SUBER on the metrics proposed in this paper. The comparisons with other methods on some commonly used metrics such as MAP/R^2/Personalization are missing. These comparisons would be essential to understand the benefits of using this method.\n-\tThe prompts used in the pre-processing module and the user description generation step require hand-crafted templates, which may limit the generalizability of the method in other scenarios.\n-\tThis method may require more computational resources than other methods due to the usage of LLM. More analysis and evaluations should be done. \n-\tThe authors failed to monitor significant existing literature on RL4Rec, including various methods and simulators, as well as RecSys integrated with LLMs. This oversight renders the paper's scope and credibility questionable."
                },
                "questions": {
                    "value": "-\tThe paper claims that it addresses the challenge of model evaluation, is it referring to the evaluation metrics mentioned in Section 4.2 and Table 1-2? How do these metrics outperform the existing evaluation methods?\n-\tAs addressed in the weakness section, I think the comparisons between this method and other methods on metrics such as MAP/R^2/Personalization are essential to verify the effectiveness of the method. Could the authors provide these results?\n-\tI\u2019m not sure about the purpose of generating the user descriptions. The paper mentioned that the Age / Job / Hobbies of the users are randomly sampled from external distributions, how does this random information affect the outcome? And what\u2019s the motivation for doing so? An ablation study to compare the results with/without this information would be helpful.\n-\tIt\u2019s known that different prompt methods could affect the response of LLMs. How does the prompt template affect the outcomes in this framework? I suggest the authors try several different prompt templates in the pre-processing module and the user description generation step, then report the range of the results.\n-\tIn Fig 4a, I observed a significant performance drop somewhere between 1.6M - 1.7M steps, why does this happen? It seems not due to the randomness because this pattern is consistent across all embedding dimensions. Or more generally, I found the pattern of these three lines seems to be extremely similar, I\u2019m surprised by this because I suppose these are three independent experiments with different dimension settings. Is there any particular reason for the similarity between these three lines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2439/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2439/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2439/Reviewer_TXpX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2439/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698544110088,
            "cdate": 1698544110088,
            "tmdate": 1699636179480,
            "mdate": 1699636179480,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iosSn2KzG0",
                "forum": "w327zcRpYn",
                "replyto": "SApUPqx47g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their effort in reviewing our work. We address their concerns in the following.\n\n> The proposed framework is to tackle key challenges in RL4Rec, such as data accessibility, the uncertainty of the user model, and the assessment of models. However, the originality of this research is ambiguous to me. It appears to predominantly integrate components of Reinforcement Learning (RL) with Large Language Models (LLMs). Furthermore, due to the absence of a thorough comparison with existing RL simulators and state-of-the-art techniques including RL4Rec and LLM-integrated RecSys, it's challenging to position the precise significance of the contributions claimed in this study.\n\nAll previous related work has either incorporated LLMs as the RecSys model itself, or used other strategies (e.g., GAN, transformers, statistical models) to simulate realistic user behavior. LLMs have been shown to excel in simulating user behavior through natural language [1,2].\n\nThe main novelty of our work is that we tackle the challenges in RL4Rec by introducing LLMs into the environment on which the RL RecSys algorithm is trained. This approach allows us to create a synthetic environment which simulates user behavior in a more natural and realistic manner. Furthermore, our environment provides practitioners the ability to train and evaluate their RL RecSys agents online.\n\nWe have added a table and updated the related work in the revised manuscript to provide a more in-depth comparison with existing research.\n\n> The paper has only made comparisons between different settings of SUBER on the metrics proposed in this paper. The comparisons with other methods on some commonly used metrics such as MAP/R^2/Personalization are missing. These comparisons would be essential to understand the benefits of using this method.\n\nWe utilize LLMs within a simulation environment. Our main contribution is the simulation environment rather than an LLM-based agent. Thus, comparing our environment with others based on metrics like MAP, MSE, and personalization is not feasible. We updated the related work and introduction in the revised manuscript to make this distinction more clear. Furthermore, we have included benchmarks for more RL methods that were trained on our environment, see Section 4.3.\n\nIt is generally not feasible to compare our framework with others due to the synthetic nature of our environment. We provide a more detailed answer in the general response.\n\n> The prompts used in the pre-processing module and the user description generation step require hand-crafted templates, which may limit the generalizability of the method in other scenarios.\n\nWe have constructed a general prompt that works for, but is not limited to, both movie and book recommendation, where we change only specific wording such as \u201cmovie\u201d with \u201cbooks\u201d.\nIn terms of generalizability, this is not an issue. The prompting component can be freely customized to use different types of items in different settings. The environment is designed with modularity in mind, and therefore, adapting different components for different scopes is feasible. In fact, the prompt used in all of our experiments is not fixed and can be changed at will.\n\n> This method may require more computational resources than other methods due to the usage of LLM. More analysis and evaluations should be done.\n\nWe performed benchmarks to understand the throughput (interactions/second) that our environment can achieve (see Table 18 in Appendix E.3 for a single GPU). It is also worth noting that the environment can be accelerated using multiple GPUs in parallel. In order to achieve the interactions/second mentioned in Table 18, we also engineered a specific prompt structure, which takes advantage of caching and significantly speeds up (4x) the performance of the environment (cf. paragraph 2 section 4.1).\n\n\n> The authors failed to monitor significant existing literature on RL4Rec, including various methods and simulators, as well as RecSys integrated with LLMs. This oversight renders the paper's scope and credibility questionable.\n\nWe have added Recogym [3], Recsim [4], RL4RS [5], and KuaiSim [6] in the related work and discussed how our approach compares to these works, the details of which can be found in our general response."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427297236,
                "cdate": 1700427297236,
                "tmdate": 1700427297236,
                "mdate": 1700427297236,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xD54cMcQv8",
                "forum": "w327zcRpYn",
                "replyto": "SApUPqx47g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The paper claims that it addresses the challenge of model evaluation, is it referring to the evaluation metrics mentioned in Section 4.2 and Table 1-2?\n\nTable 4 in Section 4.2 and Table 13 in Appendix D.2 (before Table 1-2) are not tools for evaluating RL models; rather, they serve to qualitatively assess our environment's capability to mimic human concepts (e.g., genre preference, item collection), via natural language analysis.\n\nThe challenge of model evaluation in our paper specifically pertains to the inherent complexities of evaluating an RL model in RL4Rec, where one trained model is to be tested on live, online data rather than relying on pre-recorded, logged data. \n\nWe address this challenge by developing our synthetic environment on which RL models can be tested online.\n\n> How do these metrics outperform the existing evaluation methods?\n\nThese metrics are unique to our environment and are employed internally for the evaluation of the environment and its different modular components, rather than the trained RL RecSys model. Hence, they are not meant for direct comparison with the existing evaluation metrics used by other related work.\n\n> As addressed in the weakness section, I think the comparisons between this method and other methods on metrics such as MAP/R^2/Personalization are essential to verify the effectiveness of the method. Could the authors provide these results?\n\nIn the updated manuscript, we tested various RL algorithms trained in our proposed environment on common recommendation metrics, such as MAP@10, MMR@10, and personalization.\n\n> I\u2019m not sure about the purpose of generating the user descriptions. The paper mentioned that the Age / Job / Hobbies of the users are randomly sampled from external distributions, how does this random information affect the outcome? And what\u2019s the motivation for doing so? An ablation study to compare the results with/without this information would be helpful.\n\nGenerating the user description is important because LLMs work with textual data. The LLM is at the core of our environment and is used to simulate users. The RL model interacts with the environment by interacting with these simulated users that are part of the environment.\n\nThe user description is generated by the LLM. Note that the dataset generation is done a priori and then stored in memory, it is not done during the RL training. Some basic additional information is provided to the LLM to generate the user description, such as age/profession/hobbies. The generated user description will strongly depend on this information, as it is included in the prompt when generating the user. The main goal of this process is to generate users that are different from each other; if the users are generated randomly by the LLM, they will suffer from some inherent biases of the language model. We found that users generated by the LLM without providing additional information were very similar to each other, and therefore, not diverse enough for an environment. In contrast, LLMs generating users using this additional information do not suffer from this, and the generated users represented a diverse group of humans (cf. Appendix A.1 Figure 4)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427371477,
                "cdate": 1700427371477,
                "tmdate": 1700427371477,
                "mdate": 1700427371477,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4CII4Ds4pA",
                "forum": "w327zcRpYn",
                "replyto": "SApUPqx47g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It\u2019s known that different prompt methods could affect the response of LLMs. How does the prompt template affect the outcomes in this framework? I suggest the authors try several different prompt templates in the pre-processing module and the user description generation step, then report the range of the results.\n\nWe are aware that prompting is very important and different templates can influence the outcome of the LLM. Therefore, we compared various different prompt templates (Table 4 in Section 4.2 and Table 13 in Appendix D.2). We experimented with few shots prompting, shifting the range of the rating scale as well as custom system prompts. It is worth mentioning that during the development of our environment we tested a multitude of different prompt strategies, however, we only present the most relevant results in our work.\nAdditionally, we also worked on the prompt structure to enhance the performance of our environment (Section 4.1). The prompts are constructed such that we can leverage the key-value cache of LLMs to speed up (4x) the interactions/second of our environment and reduce the time needed to train an RL model on it.\n\n> In Fig 4a, I observed a significant performance drop somewhere between 1.6M - 1.7M steps, why does this happen? It seems not due to the randomness because this pattern is consistent across all embedding dimensions. Or more generally, I found the pattern of these three lines seems to be extremely similar, I\u2019m surprised by this because I suppose these are three independent experiments with different dimension settings. Is there any particular reason for the similarity between these three lines?\n\nThis phenomenon was due to the same seed used across the experiments for reproducibility and consistency. We have added more experiments with multiple seeds and computed the confidence intervals for all of our testing. As shown in Appendix E.1 Figure 6 in the revised manuscript, we no longer have such a significant performance drop. We have updated the revised manuscript with the new results.\n\nReferences:\n\n[1] Park et al. (2023). Generative agents: Interactive simulacra of human behavior.\n\n[2] Argyle et al. (2023). Out of one, many: Using language models to simulate human samples\n\n[3] Rohde, et al. (2018). Recogym: A reinforcement learning environment for the problem of product recommendation in online advertising.\n\n[4] Ie et al. (2019). Recsim: A configurable simulation platform for recommender systems.\n\n[5] Wang et al. (2021). Rl4rs: A real-world benchmark for reinforcement learning based recommender system\n\n[6] Zhao et al. (2023). KuaiSim: A comprehensive simulator for recommender systems."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427390629,
                "cdate": 1700427390629,
                "tmdate": 1700427390629,
                "mdate": 1700427390629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KskYvuK3qX",
            "forum": "w327zcRpYn",
            "replyto": "w327zcRpYn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2439/Reviewer_K35w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2439/Reviewer_K35w"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework for training and evaluating RL-based recommender systems, which uses large language models (LLMs) to simulate human behavior and rate recommended items."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper proposes a user simulation framework based on large language models, which can alleviate the problems of data scarcity and model evaluation for reinforcement learning based recommender system.\n+ The paper designs a flexible and extensible environment based on reinforcement learning principles, which can interact with different LLMs and recommendation strategies.\n+ The paper provides a modular framework and open-source code, which is a valuable tool for the recommender system domain. It helps researchers and developers to train and evaluate reinforcement learning based recommender systems without real user interactions."
                },
                "weaknesses": {
                    "value": "- The manuscript could benefit from more robust experimental support. The absence of comparisons with other simulation algorithms may impact the persuasiveness of the paper.\n\n- The paper appears to lack some key experiments that validate the effectiveness and advantages of RL training in the proposed environment. While the paper presents ablation studies on different components of the environment, it does not compare the RL-based recommender system with other baselines or state-of-the-art methods on real user data or benchmark datasets. It would be advantageous if the authors could include such experimental validations in future research.\n\n- The use of LLMs to generate synthetic users is an innovative approach that leverages the powerful capabilities of LLMs to simulate human behavior and preferences. However, the paper does not evaluate the quality and diversity of the user generation, nor does it compare it with real user data. This could potentially lead to biases and inaccuracies in the simulation. It would be beneficial if the authors could delve deeper into this aspect in future research.\n\n- The related work section of the paper seems too general and lacks precision. It does not adequately highlight the differences and connections between their work and existing research. It would be advantageous if the authors could elaborate more on the relationship and uniqueness of their work in relation to existing research, to enhance the depth and breadth of the paper."
                },
                "questions": {
                    "value": "1.\tCan the author provide a distribution chart for scores predicted by LLMs and actual scores? The numbers reported in the table do not provide an intuitive image. On the other hand, it is more important to focus not on the overall score distribution, but on the differences in scoring for each item (it is possible for the overall score distribution to be the same, but with significant differences in individual item scores).\n2.\tWhat is the difference between the last two rows in Table 1? Is it a type error? \n3.\tComparing rows 3-8 in Table 1 with the 9th row, it can be observed that the model is very good at distinguishing between High and Low Ratings on a scale of 0-9. However, when the score scale changes to 1-10, there is a significant decrease in performance. Does this indicate that the model can only identify very poor items? (0 score)\n4.\tI suggest the author showcases the performance of several traditional models and conventional RL methods, under your reward metric (Figure .4a)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2439/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699108927660,
            "cdate": 1699108927660,
            "tmdate": 1699636179401,
            "mdate": 1699636179401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PFBBkUpKa2",
                "forum": "w327zcRpYn",
                "replyto": "KskYvuK3qX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their effort on reviewing our work. We address their concerns in the following.\n\n> The manuscript could benefit from more robust experimental support. The absence of comparisons with other simulation algorithms may impact the persuasiveness of the paper.\n\nThe lack of comparisons with other environments is mainly due to the synthetic nature of our environment. As a result, comparing our environment with others is not feasible. From this perspective, we are more similar to Recogym [1] and Recsim [2]. We have provided more details in the general response.\n\n> The paper appears to lack some key experiments that validate the effectiveness and advantages of RL training in the proposed environment. While the paper presents ablation studies on different components of the environment, it does not compare the RL-based recommender system with other baselines or state-of-the-art methods on real user data or benchmark datasets. It would be advantageous if the authors could include such experimental validations in future research.\n\nWe have added additional experiments with different RL models in our environment, in particular A2C, PPO, DQN, and TRPO. In Section 4.3, we demonstrated that A2C can effectively learn in our environment by achieving a high average reward, MAP@10, and MRR@10 score. Additionally, we also demonstrate random qualitative examples. Appendix E further illustrates how A2C is able to recommend movies according to the user's genre preferences.\n\n> The use of LLMs to generate synthetic users is an innovative approach that leverages the powerful capabilities of LLMs to simulate human behavior and preferences. However, the paper does not evaluate the quality and diversity of the user generation, nor does it compare it with real user data. This could potentially lead to biases and inaccuracies in the simulation. It would be beneficial if the authors could delve deeper into this aspect in future research.\n\nWe agree with the reviewer that the use of LLMs could potentially lead to bias. To mitigate this, we reduce the bias by explicitly mentioning in the system prompt that the model should not be biased and should answer the prompts realistically (Appendix A.3.1). Furthermore, we sampled characteristics of users like professions, hobbies, age, and interests. This process ensures the LLM creates unbiased synthetic users (e.g., not biased on age or interests) (Appendix A.1). Hobbies and professions are randomly sampled from a list, while age and interests are sampled from real human distributions (Appendix A.1). These choices are included in the prompt, so when the LLM generates the user description, it is conditioned on the previously sampled characteristics. This allows for a more diverse distribution of users, rather than being biased towards a particular group of people. We show the effectiveness of our approach in Figure 4 (Appendix A.1).\n\n> The related work section of the paper seems too general and lacks precision. It does not adequately highlight the differences and connections between their work and existing research. It would be advantageous if the authors could elaborate more on the relationship and uniqueness of their work in relation to existing research, to enhance the depth and breadth of the paper.\n\nWe have added a table and updated the related work section to provide a more in-depth comparison with existing research."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427479814,
                "cdate": 1700427479814,
                "tmdate": 1700427479814,
                "mdate": 1700427479814,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B8cH6EtkNB",
                "forum": "w327zcRpYn",
                "replyto": "KskYvuK3qX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Can the author provide a distribution chart for scores predicted by LLMs and actual scores? The numbers reported in the table do not provide an intuitive image. On the other hand, it is more important to focus not on the overall score distribution, but on the differences in scoring for each item (it is possible for the overall score distribution to be the same, but with significant differences in individual item scores).\n\nWe have added a graph of the score probability distribution in Figure 5 (Appendix D.1) of the revised manuscript.\n\n> What is the difference between the last two rows in Table 1? Is it a type error?\n\nNot an error, these are two different strategies of prompting, in one case \u201c1-10\u201d refers to the digits between \u201c1\u201d and \u201c10\u201d in the prompt, while \u201cone-ten\u201d refers to ratings in the prompt which were written in words (\u201cone\u201d, \u201ctwo\u201d, \u201cthree\u201d, ..., \u201cten\u201d) (cf. 4th paragraph section 4.1). We have updated the table captions to better reflect this in the revised manuscript.\n\n> Comparing rows 3-8 in Table 1 with the 9th row, it can be observed that the model is very good at distinguishing between High and Low Ratings on a scale of 0-9. However, when the score scale changes to 1-10, there is a significant decrease in performance. Does this indicate that the model can only identify very poor items? (0 score)\n\nWe found this to be an interesting problem. We believe this is related to the ambiguity of the tokenization, however, we were unable to find any research discussing this issue. In general, the 0-9 scale performs much better than the 1-10 scale in almost all of our experiments, mainly because the token \u201c10\u201d, which represents the best score, can be tokenized in 2 ways, either the token \u201c10\u201d directly or the token \u201c1\u201d and then the token \u201c0\u201d (cf. 4th paragraph section 4.1).\nIn these experiments, the 0-9 scale outperforms the 1-10 scale significantly. This is because, in the 0-9 scale, when the language model assigns a high rating, it directly corresponds to the token \u201c9\u201d. In contrast, in the 1-10 scale, the model occasionally generates the rating \u201c10\u201d by producing the digits \u201c1\u201d and \u201c0\u201d separately, whereas, in some cases, the model generates the EOS-token after computing the token \u201c1\u201d, leading the model to obtain a high loss since it predicted score (1) compared to the true score (10).\n\n> I suggest the author showcases the performance of several traditional models and conventional RL methods, under your reward metric (Figure .4a)\n\nIn the revised manuscript, we added more experiments on training RL methods like PPO, A2C, DQN and TRPO. The experiments can be found in Section 4.3.\n\nReferences:\n\n[1] Rohde, et al. (2018). Recogym: A reinforcement learning environment for the problem of product recommendation in online advertising.\n\n[2] Ie et al. (2019). Recsim: A configurable simulation platform for recommender systems."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427534378,
                "cdate": 1700427534378,
                "tmdate": 1700427534378,
                "mdate": 1700427534378,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nCUH7K1COq",
            "forum": "w327zcRpYn",
            "replyto": "w327zcRpYn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2439/Reviewer_g6on"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2439/Reviewer_g6on"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a promising solution to the challenge of training recommender systems when real user interactions are not available. They propose SUBER, a novel Reinforcement Learning (RL) simulated environment tailored for recommender system training, which leverages recent advancements in Large Language Models (LLMs) to simulate human behavior within the training setting. A series of ablation studies and experiments demonstrate the effectiveness of their approach. This research represents a significant step towards creating more realistic and practical training environments for recommender systems, even in the absence of direct user interactions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe concept of employing Large Language Models (LLMs) to mimic synthetic users is intriguing. SUBER offers a multifaceted approach, generating synthetic data while harnessing the potential of LLMs to accurately emulate the behavior of users with undisclosed patterns.\n2.\tThe authors meticulously conduct comprehensive ablation studies to dissect the various components of LLMs, demonstrating the scalability and versatility of SUBER in the process.\n3.\tThe article is impeccably articulated, presenting its ideas with clarity and maintaining a logical flow throughout."
                },
                "weaknesses": {
                    "value": "1.\tHow do the authors assess the accuracy of their simulated environment? The paper primarily showcases the training curve of the RL model within this environment but lacks a comparative analysis against other environment simulation methods. The absence of online experiments further challenges the validity of the simulated environment.\n\n2.\tThe authors should expound on their rationale for using LLMs to simulate users, clarify why this approach is effective, and outline the advantages it offers over alternative environment simulation methods.\n\n3.\tThe authors claim that this dynamic environment can serve as a model evaluation tool for recommender systems, but it lacks empirical evidence to support this claim. A clear methodology for measuring the accuracy of the proposed evaluation method is needed.\n\n4.\tThe absence of t-tests or error bars in the results section raises concerns about the reliability and reproducibility of the experimental findings.\n\n5.\tThe paper mentions the limitation of context length for providing a list of all possible items to an LLM. Further details are required regarding how the author addressed this particular issue."
                },
                "questions": {
                    "value": "1.\tI suggest the authors evaluate the simulated environment from more aspects. To establish the accuracy of the simulated environment, consider conducting comparative experiments. Compare the performance of your proposed environment with existing methods for simulating user interactions. Additionally, performing online experiments where applicable, could help validate the authenticity of your simulated environment.\n\n2.\tI suggest the authors provide a more in-depth explanation of why LLMs are chosen to simulate users. Elaborate on the effectiveness of this approach by highlighting its advantages over alternative simulation methods. This could include discussing how LLMs can capture complex user behavior or adapt to changing patterns more effectively.\n\n3.\tTo substantiate the claim that your dynamic environment serves as a model evaluation tool, conduct experiments that demonstrate its utility in evaluating recommender systems. Present a clear experimental setup and results that support this assertion.\n\n4.\tEnhance the reliability and reproducibility of your experimental results by including t-tests or error bars. \n\n5.\tExplain in detail how you addressed the limitation of limited context length. What techniques or strategies did you employ to mitigate this constraint when using LLMs in your environment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2439/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2439/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2439/Reviewer_g6on"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2439/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699677528249,
            "cdate": 1699677528249,
            "tmdate": 1699677528249,
            "mdate": 1699677528249,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EsUx19GYx3",
                "forum": "w327zcRpYn",
                "replyto": "nCUH7K1COq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their effort on reviewing our work. We address their concerns in the following.\n\n> How do the authors assess the accuracy of their simulated environment? The paper primarily showcases the training curve of the RL model within this environment but lacks a comparative analysis against other environment simulation methods. The absence of online experiments further challenges the validity of the simulated environment.\n\nThe absence of any comparisons with other environments is primarily due to the synthetic nature of our environment. Unfortunately, comparing our environment with others is unfeasible. In this regard, our environment bears more similarity to Recogym [1] and Recsim [2]. We have provided more details on this matter in the general response.\n\n\n> The authors should expound on their rationale for using LLMs to simulate users, clarify why this approach is effective, and outline the advantages it offers over alternative environment simulation methods.\n\nThe main motivation for using LLMs is their ability to simulate human behavior well, as demonstrated by previous work [3, 4]. Therefore, such an environment is able to capture human dynamics and interests. We have updated the introduction of the revised manuscript to emphasize this aspect. Furthermore, LLMs undergo training on extensive datasets that include information related to various subjects. This enhances their capability to grasp context, and therefore, respond in a realistic manner.\n\n\n> The authors claim that this dynamic environment can serve as a model evaluation tool for recommender systems, but it lacks empirical evidence to support this claim. A clear methodology for measuring the accuracy of the proposed evaluation method is needed.\n\nWe conduct additional experiments using different RL methods within our proposed environment (Section 4.3). This environment is based on user ratings - in other words, when a movie is recommended to a user, the RL model receives a reward corresponding to the user's rating for that movie. Consequently, the average reward accumulated by the RL model serves as a key metric for evaluating its performance in the environment.\n\nAs shown in the experimental results described in Section 4.3, we can measure the effectiveness of the RL model by monitoring its average reward. A higher average reward indicates that the model has successfully selected movies that match interests of users. In addition to average reward, we have added additional metrics such as MAP@10, MMR@10, and personalization to provide a comprehensive evaluation of different RL models.\n\n> The absence of t-tests or error bars in the results section raises concerns about the reliability and reproducibility of the experimental findings.\n\nWe have added error bars to the figures and tables in the revised manuscript.\n\n> The paper mentions the limitation of context length for providing a list of all possible items to an LLM. Further details are required regarding how the author addressed this particular issue.\n\nTo address the limitation of a fixed context length, we have implemented the Item Retrieval component. For a given user, when constructing a prompt, the Item Retrieval component selects only a subset of items previously seen by the user. We mention this aspect and the role of the Item Retrieval component in Section 3.2."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427588565,
                "cdate": 1700427588565,
                "tmdate": 1700427588565,
                "mdate": 1700427588565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NDMe6L1oMt",
                "forum": "w327zcRpYn",
                "replyto": "nCUH7K1COq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2439/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> I suggest the authors evaluate the simulated environment from more aspects. To establish the accuracy of the simulated environment, consider conducting comparative experiments. Compare the performance of your proposed environment with existing methods for simulating user interactions. Additionally, performing online experiments where applicable, could help validate the authenticity of your simulated environment.\n\nTo the best of our knowledge, there is no practical and accurate method for comparing synthetic environments. We address this topic in more detail in the general response. If the reviewer holds a different perspective on this matter, we welcome the opportunity to engage in further discussion.\n\n> I suggest the authors provide a more in-depth explanation of why LLMs are chosen to simulate users. Elaborate on the effectiveness of this approach by highlighting its advantages over alternative simulation methods. This could include discussing how LLMs can capture complex user behaviour or adapt to changing patterns more effectively.\n\nWe have added more qualitative explanations why LLMs are good for simulating users in the revised manuscript. We base ourselves on the work by Park et al. [3] and Argyle et al. [4] which show that LLMs are good simulators of human behavior.\n\n> To substantiate the claim that your dynamic environment serves as a model evaluation tool, conduct experiments that demonstrate its utility in evaluating recommender systems. Present a clear experimental setup and results that support this assertion.\n\nWe added more experiments in Section 4.3 to compare different RL methods trained on SUBER.\n\nAs shown in the experimental results described in Section 4.3, we can measure the effectiveness of the RL model by monitoring its average reward. A higher average reward indicates that the model has successfully selected movies that match interests of users. In addition to average reward, we have added additional RecSys specific metrics such as MAP@10, MMR@10, and personalization to provide a comprehensive evaluation of different RL models.\n\n> Enhance the reliability and reproducibility of your experimental results by including t-tests or error bars.\n\nWe have conducted more experiments and added error bars and confidence intervals in the experiment section of the revised manuscript.\n\n> Explain in detail how you addressed the limitation of limited context length. What techniques or strategies did you employ to mitigate this constraint when using LLMs in your environment?\n\nWe implemented three types of item retrieval: Recency Retrieval, in other words we retrieve the most recently seen items, Feature Similarity Retrieval based on the movie features, and T5 Similarity Retrieval based on the movie description. See Section 3.2 and Section 4.1 for more details. Furthermore, we compare these retrieval approaches against each other in Section 4.2.\n\nReferences:\n\n[1] Rohde, et al. (2018). Recogym: A reinforcement learning environment for the problem of product recommendation in online advertising.\n\n[2] Ie et al. (2019). Recsim: A configurable simulation platform for recommender systems. \n\n[3] Park et al. (2023). Generative agents: Interactive simulacra of human behavior.\n\n[4] Argyle et al. (2023). Out of one, many: Using language models to simulate human samples"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2439/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427605509,
                "cdate": 1700427605509,
                "tmdate": 1700427605509,
                "mdate": 1700427605509,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]