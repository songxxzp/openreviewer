[
    {
        "title": "Fast Neural Architecture Search with Random Neural Tangent Kernel"
    },
    {
        "review": {
            "id": "lzcfpM4cIp",
            "forum": "PYdk0V880P",
            "replyto": "PYdk0V880P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2379/Reviewer_6Yvs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2379/Reviewer_6Yvs"
            ],
            "content": {
                "summary": {
                    "value": "Neural Architecture Search (NAS) automates DNN architecture design. Existing training-free NAS methods aim to reduce search costs but often rely on training error, not generalization error. We propose NAS-NGE, using bias-variance decomposition of the normalized generalization error, outperforming SOTA training-free NAS in short search times with NAS Benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is readable and well-written. It demonstrates a clear sense of purpose, emphasizing the importance of focusing on generalization performance rather than training performance when conducting NTK-based NAS. The paper then provides a mathematical framework for this approach and demonstrates it effectively. The overall structure, from the statement of purpose to the specific approach, is well organized. The experiments also report that the proposed approach often outperforms existing methods."
                },
                "weaknesses": {
                    "value": "I'm not sure about the significance of the contribution. Although the experimental results appear promising, the derivation of the bias-variance decomposition and its associated equations (Eq. 3 and Eq. 4) seems relatively straightforward, which could imply that the contribution is somewhat incremental to previous works. If the novelty of what you're doing can be highlighted even more, I believe it has the potential. \n\nI believe it's important to carefully explain how much the behavior of models trained outside the NTK-regime can be accounted for by theoretical analysis using NTK, as unrealistic settings like lazy training are implicitly imposed. Currently, only the performance of NAS is reported, but a more direct analysis would increase the level of confidence. Specific points are listed in the \"Questions\" section."
                },
                "questions": {
                    "value": "1: I believe that NAS-Bench stores performance at multiple checkpoint epochs (e.g., 4, 12, 36, 108 epochs). In this paper, empirical evaluation is conducted with the shortest training time, both for NAS-Bench-101 and NAS-Bench-201. Do you think that extending these checkpoint epochs significantly impacts the results? Do you have results under such settings? Since NTK theory assumes that NTK does not change during training, I imagine that as training epochs increase, it may deviate somewhat from the theory of infinite width. I'm interested in understanding this deviation. If possible, demonstrating its superiority based on differences in trends on epochs compared to other methods using NTKs could potentially strengthen the paper.\n\n2: Like the TE-NAS paper (Chen et al. (2021)), can you visualize a scatter plot of the current metrics against the actual generalization performance? This might be a more direct way to validate the effectiveness compared to conducting NAS."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2379/Reviewer_6Yvs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697689355802,
            "cdate": 1697689355802,
            "tmdate": 1699636172646,
            "mdate": 1699636172646,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Yl8KLYsD9",
                "forum": "PYdk0V880P",
                "replyto": "lzcfpM4cIp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewers for their feedback and we appreciate the questions in the reviews.\n\n### **The novelty of the paper** \n\nThe main purpose of our paper is to propose a training-free NAS method that searches appropriate DNN architectures in a very short time compared to existing methods. For that purpose, we use the normalized generalization error, which is efficiently approximated using the normalized 2nd-order moment of NTK. The performance measure based on Eq.3 and Eq.4 is derived by combining some existing theoretical techniques of NTK. ***As shown in Figure 1 in the revised paper, however, our method, NAS-NGE, provides a highly accurate prediction of DNN's long-term training dynamics. Due to that property, we can construct a computationally efficient method for training-free NAS.***\n\n\n### **Q. Comparison of trends on epochs to other methods using NTKs.** \n\nAs the reviewer pointed out, there is a gap between NTK theory and practical performance. In the revised paper, we compare the performance of NTK-based training-free NAS methods, NASI, and our method, NAS-NGE. ***Figure 1 in the revised manuscript shows the test errors at several checkpoint epochs.*** Our numerical experiments indicate that our method, NAS-NGE, well predicts the generalization performance than NASI."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483016904,
                "cdate": 1700483016904,
                "tmdate": 1700724706065,
                "mdate": 1700724706065,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nHsyAQGVeJ",
            "forum": "PYdk0V880P",
            "replyto": "PYdk0V880P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2379/Reviewer_vdk2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2379/Reviewer_vdk2"
            ],
            "content": {
                "summary": {
                    "value": "The paper concerns train-free Neural Architecture Search (NAS). This concerns the task of automatically determining the architecture for a prescribed task, i.e. image classification in this case. In this paper, a core stated motivation concerns devising a NAS method with theoretical guarantees based on the generalization error. The bias-variance decomposition analysis of the normalized generalization error is used for NAS. Concretely, they use the (normalized) 2nd order moment of the NTK along with the normalized bias. The standard benchmark of Nasbench-201 is used for the comparison."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Automatically designing architectures is an important task and currently it is very costly. The train-free methods that can indeed reduce the cost of search have yet to demonstrate the capacity of DARTS or more recent models. At the same time, theoretical understanding of architectures and their inductive bias is important, therefore I do consider that the broader area is important and relevant for the ICLR audience. The analysis of the moments of NTK conducted in this work is something I have not seen before. Having said that, I am not sure of the precise conditions that this analysis holds."
                },
                "weaknesses": {
                    "value": "- A strong motivation is built upon the premise that generalization guarantees are missing in train-free NAS. However, the papers Generalization guarantees for neural architecture search with train-validation split (ICML'21), Generalization Properties of NAS under Activation and Skip Connection Search (NeurIPS\u201922) are precisely providing generalization guarantees for different cases. Given those papers, I believe one of the core motivations is less clear to me. Could the authors elaborate on the differences from those? \n\n- The experimental validation seems rather weak, using the dated nas-bench-201 as the main comparison platform. If the method scales so well in terms of time, why not use benchmarks with larger search space?\n\n- The paper skips many important details and intuitions that make it quite hard to understand. For instance, why is the second-order moment used in the analysis and why is the NTK assumed here?"
                },
                "questions": {
                    "value": "- It seems the reported accuracies in this work are lower than existing standard networks, e.g. ResNets. Even if we do not account for those cases, the aforementioned Neurips\u201922 paper on the same benchmark reports results closer to standard accuracies on cifar10/100. Could the authors elaborate on those discrepancies? \n\n- Given that one of the main claims is built around the generalization error, I would expect to see how the proposed model extends beyond the test data of the dataset, which is one of the critical points in NAS overall. \n\n- One of the messages repeatedly relayed throughout the manuscript is the speed of the proposed method. However, I find that the reporting of \u201cabout 10 sec.\u201d to be quite rough; could the authors conduct a refined analysis on this? \n\n- Continuing on the aforementioned point, does the reported time include the time to compute the NTK of each architecture? \n\n- In condition 3.2 it mentions the NTK, however the papers cited mostly focus on Relu-nets, so are there any additional conditions that should hold for the neural network in order for this analysis to hold? \n\n- One of the sentences in the first paragraph of the introduction claims that \u201cArchitectures found in NAS are even surpassing the performance of manually designed architectures\u201d. Is there a reference for this? How does the proposed method perform in imagenet that is the standard benchmark in image classification?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697735067754,
            "cdate": 1697735067754,
            "tmdate": 1699636171989,
            "mdate": 1699636171989,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h1EYWhrPPB",
                "forum": "PYdk0V880P",
                "replyto": "nHsyAQGVeJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewers for their feedback and we appreciate the questions in the reviews.\n\n### **Our motivation and differences from [1] and [2]**\n\nIn the paper [1], the authors derived an end-to-end generalization bound for DNN learning using training-validation split, and used the bound to training-based method. The proposed method is not training-free NAS. That is the main difference from our method.\n\nIn the paper [2], mainly the authors consider theoretical analysis of training-free NAS methods for binary classification problems. On the other hand, we proposed a NTK-based approximation of the normalized generalization error for regression and classification problems. We find that our method provides an accurate prediction of DNN\u2019s long-term training dynamics. Due to that property, we can construct a computationally efficient method for training-free NAS. ***We added numerical experiments in Figure 1 of the revised paper.***\n\n\n\n### **Why is the second-order moment used in the analysis and why is the NTK assumed here?**\n\nThe second-order moments naturally appear since we consider the squared error as the loss. NTK is used to approximate the learning dynamics of DNN. By approximating learning dynamics using NTK, we can estimate the generalization performance of DNNs after several learning steps without learning. We added the details in the first paragraph of Section 3.\n\n\n### **Q1. Discrepancies from benchmark reports in [2]**\n\nOur numerical studies indicate that NAS-NGE can select an appropriate DNN architecture in a very short time. ***Table 4 in the revised paper shows numerical results using NAS-Bench-1shot1, which has a larger search space than NAS-bench-201.*** We see that NAS-NGE is about three times more computationally efficient than NASI and attains a comparable accuracy.\n\n\n### **Q2. how does the proposed model extend beyond the test data of the dataset.**\n\nIf the reviewer asks about the relation of NAS to out-of-distribution (OOD) generalization, that\u2019s a very interesting problem. Thank you for the suggestion. ***The theoretical evaluation of NAS under the OOD setting was added to the Conclusion as an interesting future work.***\n\n### **Q3. a refined analysis on reporting computation cost**\n\nIn numerical experiments, the accurate computation time was supplemented in the tables.\n\n### **Q4. the reported time include the time to compute the NTK of each architecture?**\n\nYes, the computation time of NTK is included. This is supplemented in the caption of Table 2.\n\n### **Q5. the papers cited mostly focus on ReLU-nets, so are there any additional conditions?**\n\nAs shown in [3], the NTK approximation applies to a wide range of DNNs. In the binary classification problem, Zhu et al.[2] studied generalization properties of NAS for DNNs with skip connections and pseudo-Lipschitz activation functions. ***Description on these works were added to Remark 1 in the end of Section 3.2 of the revised paper.*** \n\n### **Q6. \"Architectures found in NAS are even surpassing the performance of manually designed architectures\". Is there a reference for this?**\n\nThe paper [4] commented as follows: \"In the literature, various search algorithms (Luo et al., 2018; Zoph et al., 2018; Real et al., 2019) have been proposed to search for architectures with comparable or even better performance than the handcrafted ones.\" We referred to [4] in the first paragraph of Section 2. \n\n\n\n### **References**\n\n[1] S. Oymak, M. Li, M. Soltanolkotabi, \"Generalization Guarantees for Neural Architecture Search with Train-Validation Split\", ICML 2021\n\n[2] Z. Zhu, F. Liu, G. Chrysos, V. Cevher, \"Generalization Properties of NAS under Activation and Skip Connection Search\", NeurIPS 2022.\n\n[3] G. Yang and E. Littwin, \"Tensor programs iib: Architectural universality of neural tangent kernel training dynamics\", ICML 2021.\n\n[4] Y. Shu, et al., \"NASI: Label- and data-agnostic neural architecture search at initialization\", ICLR 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483285072,
                "cdate": 1700483285072,
                "tmdate": 1700724584792,
                "mdate": 1700724584792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "z13E6ZEhZn",
                "forum": "PYdk0V880P",
                "replyto": "nHsyAQGVeJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (2/2)"
                    },
                    "comment": {
                        "value": "We supplemented numerical experiments with a larger search space.\n\n> If the method scales so well in terms of time, why not use benchmarks with larger search space?\n\n***In the revision, we added numerical experiments with the DARTS search space in Table 11 of Appendix F.*** The result is shown in the following table. In the experiment, NASI and the proposed method with about 10 minutes of computation were applied for NAS. Our method attains a comparable prediction performance with about 1.5 times higher computation efficiency for NAS. We hope the above result addresses the reviewer's concern.\n\n||||\n|:---:|:---:|:---:|                                \n|       |NASI|PROPOSED|\n| step | 20                 |    10               |\n| time  | 599.0 $\\pm$ 0.00[s]| 392.7 $\\pm$ 0.47[s]|\n| test acc.      | 96.85 $\\pm$ 0.23   | 97.02 $\\pm$ 0.07    |\n||||"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724677473,
                "cdate": 1700724677473,
                "tmdate": 1700737776906,
                "mdate": 1700737776906,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OxaAydHnOw",
            "forum": "PYdk0V880P",
            "replyto": "PYdk0V880P",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2379/Reviewer_C4Cd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2379/Reviewer_C4Cd"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel surrogate of the generalization error of neural networks for training-free Neural Architecture Search(NAS). By normalize the estimation of generalization error on different data samples and random initialization, proposed NAS-NGE outperform the other training-free pruning based NAS methods on NAS-Bench-1Shot1, and NAS-Bench-201."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors analyzed the output of the model by decomposing it into bias and variance. As far as I know, this approach is new and novel.\nIn particular, the authors' method of normalizing the variation in output due to random initialization to provide a more reliable measure of generalization error is reasonable and well explained in the paper."
                },
                "weaknesses": {
                    "value": "The purpose of NAS is to find the optimal architecture within a reasonable cost. The architecture found by the proposed method in the experiments seems to be far from optimal (even considering that it was explored in a very short time). For example, the optimal architectures in NAS-Bench-201 had accuracies of 94.37, 73.51, and 47.31, respectively, and many training-free methods discover architectures with only 1-2% accuracy difference from the optimal architectures in less than an hour [1]. The proposed method explored architectures that lagged behind the optimal architecture by 2-8% in less than a minute of search time. I am not sure how this result can be utilized. This concern would be alleviated if the authors reported the variation in the accuracy of the found architectures for different search times, or if they indicated how good the found architectures were within the overall architecture pool. It would also be helpful to compare the proposed method with other methods for training-free NAS (NASWOT[2], KNAS[3]), or to compare it with other (more expensive) NAS algorithms.\n\n[1] Shu, Y., Cai, S., Dai, Z., Ooi, B. C., & Low, B. K. H. (2021). Nasi: Label-and data-agnostic neural architecture search at initialization. arXiv preprint arXiv:2109.00817.\n\n[2] Mellor, J., Turner, J., Storkey, A., & Crowley, E. J. (2021, July). Neural architecture search without training. In International Conference on Machine Learning (pp. 7588-7598). PMLR.\n\n[3] Xu, J., Zhao, L., Lin, J., Gao, R., Sun, X., & Yang, H. (2021, July). KNAS: green neural architecture search. In International Conference on Machine Learning (pp. 11613-11625). PMLR."
                },
                "questions": {
                    "value": "It seems that the training set is used to calculate the NTK, why is it acceptable to calculate it with the training set alone without using the validation set?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2379/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2379/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2379/Reviewer_C4Cd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2379/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763013388,
            "cdate": 1698763013388,
            "tmdate": 1699636171506,
            "mdate": 1699636171506,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I2Z5Gcv5cs",
                "forum": "PYdk0V880P",
                "replyto": "OxaAydHnOw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2379/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2379/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewers for their feedback and appreciate the questions in the reviews.\n\n### **The variation in the accuracy of the found architectures for different search times.**\n\nWe conducted additional numerical experiments with NAS-Bench-1shot1 to investigate the performance of NAS in different search times. The computation time and test errors are shown in the following table. The proposed method, NAS-NGE, shows a similar search performance to NASI, with about three times higher calculation efficiency. ***The following result was added to Table 4 in the revised paper.*** \n\n||PROPOSED|PROPOSED|NASI [1]|\n|:---:|:---:|:---:|:---:|\n|STEP           |           20       |          60           |         360         |\n|TIME           |  35.2 $\\pm$ 3.66[s]|  104.7 $\\pm$ 10.93[s] | 291.3 $\\pm$ 30.48[s]|\n|SEARCH SPACE 1 | 0.086 $\\pm$ 0.024  |  0.072 $\\pm$ 0.013    | 0.068 $\\pm$ 0.004   |\n|SEARCH SPACE 2 | 0.074 $\\pm$ 0.006  |  0.074 $\\pm$ 0.008    | 0.071 $\\pm$ 0.003   |\n|SEARCH SPACE 3 | 0.085 $\\pm$ 0.014  |  0.068 $\\pm$ 0.006    | 0.074 $\\pm$ 0.013   |\n\n\n### **Q1. Calculation of NTK with the training set alone without using the validation set.**\n\nIn the paper [1], also the validation dataset is not employed. The following is the comments in [1]; \"Since both theoretical (Mohri et al., 2018) and empirical (Hardt et al., 2016) justifications in the literature have shown that training and validation loss are generally highly related, we simply use $L_{\\mathrm{t}}$ to approximate $L_{\\mathrm{val}}$\", where $L_{\\mathrm{t}}$ (resp. $L_{\\mathrm{val}}$) is the loss over the training (resp. validation) dataset. For the same reason, we used the training set to calculate the NTK. \n\n\n\n### **References**\n\n[1] Y. Shu, et al., \"NASI: Label- and data-agnostic neural architecture search at initialization\", ICLR 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2379/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482935379,
                "cdate": 1700482935379,
                "tmdate": 1700482935379,
                "mdate": 1700482935379,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]