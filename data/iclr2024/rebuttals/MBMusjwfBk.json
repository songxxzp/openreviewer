[
    {
        "title": "Prediction-Consistent Koopman Autoencoders"
    },
    {
        "review": {
            "id": "E1nOX0qDEa",
            "forum": "MBMusjwfBk",
            "replyto": "MBMusjwfBk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4553/Reviewer_41Am"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4553/Reviewer_41Am"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method for predicting nonlinear dynamic using Koopman autoencoder neural networks. The authors suggest to augment existing techniques with a prediction constraint that promotes latent states to be linearly related. The method is evaluated on several datasets in comparison to several baseline approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is easy to follow; The method is described concisely and effectively; the evaluation section seems to be detailed."
                },
                "weaknesses": {
                    "value": "The main weakness of this paper is the proposal of a loss term that was introduced and used before. Specifically, the paper by Lusch et al. \"Deep learning for universal linear embeddings of nonlinear dynamics\" discusses the same loss term (see page 4, 'linear dynamics'). Since the paper by Lusch et al., several other works have used a similar term, and it is generally known as one of the loss terms to be used in Koopman-based autoencoder frameworks. Thus, unfortunately, this work is not new from an algorithmic viewpoint.\n\nFurther, while the evaluation section is decent, the results are not promising. Specifically, the results in Tables 3, 4, 5 show that there is no statistical significance between cKAE and pcKAE (as measured by the standard deviation). This may explain why cKAE did not consider the additional loss term in their work."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4553/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698326129306,
            "cdate": 1698326129306,
            "tmdate": 1699636432993,
            "mdate": 1699636432993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jeBmb9Xmef",
                "forum": "MBMusjwfBk",
                "replyto": "E1nOX0qDEa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "There seems to be a misunderstanding regarding how our $L_{pc}$ is different from $L_{lin}$ as proposed in [1]. Note that $L_{lin}$ as proposed in [1] is limited by the number of samples in training dataset. Let us consider a training dataset $\\\\{\\mathbf{x}_1, \\mathbf{x}_2 \\\\}$ consisting only two time samples of state. Let the corresponding latent space representation be $\\\\{\\mathbf{z}_1, \\mathbf{z}_2 \\\\}$ where $\\mathbf{z}_i\\approx \\Psi_e(\\mathbf{x}_i)$. Now based on this,\n\nLinear loss in [1]: $L_{lin} = || \\mathbf{z}\\_2 - K\\mathbf{z}\\_1 ||$\n\nOur proposed loss: $L_{pc} = \\sum_{\\kappa=1}^{\\kappa_m} || K^\\kappa \\mathbf{z}_2 - K^{\\kappa + 1} \\mathbf{z}_1 ||$, where $\\kappa_m$ is a hyperparameter.\n\nThe main difference being our proposed $L_{pc}$ is not limited by the number of samples in training dataset.\n\n[1] B. Lusch, J. N. Kutz, S. L. Brunton, Deep learning for universal linear embeddings of nonlinear dynamics, Nature communications 9 (1) (2018) 4950."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649486975,
                "cdate": 1700649486975,
                "tmdate": 1700649835998,
                "mdate": 1700649835998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QIawNUZdq6",
                "forum": "MBMusjwfBk",
                "replyto": "jeBmb9Xmef",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Reviewer_41Am"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Reviewer_41Am"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their response. However, the linear loss in [1] is controlled by a hyperparameter for the number of steps, and while the authors of [1] may not consider the case you mention, it is covered by their approach. Moreover, the authors of the current submission should have justified more thoroughly the importance of this hyperparameter, its effect on forecasting, and corresponding analysis. All of these aspect are unfortunately missing in this submission."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651278019,
                "cdate": 1700651278019,
                "tmdate": 1700651278019,
                "mdate": 1700651278019,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uXlTr4Bpyd",
                "forum": "MBMusjwfBk",
                "replyto": "E1nOX0qDEa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment on Statistical Significance of the Results"
                    },
                    "comment": {
                        "value": "Regarding author's comment on statistical significance of the results, we would like to point out that for limited data set, pcKAE outperforms cKAE significantly for Table 2, Table 3 and Table 4. Note that since advantage of pcKAE is obvious for limited data, we have tested noisy data only for large training set for Table 3 and Table 4. We have intentionally included Table 5 to show possible drawbacks of pcKAE. Since the proposed loss term in pcKAE exploits time-invariance nature of the dynamical system, for time-varying system it might not be that advantageous. As we have mentioned in the manuscript, \"The inherent dynamics of SSTs are influenced by both periodic seasonal changes and non-periodic variations such as El Ni\u00f1o and La Ni\u00f1a events (Ohba & Ueda, 2007), which can inject anomalies in the SST variation.\""
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653712053,
                "cdate": 1700653712053,
                "tmdate": 1700653712053,
                "mdate": 1700653712053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KKyWGnCquO",
            "forum": "MBMusjwfBk",
            "replyto": "MBMusjwfBk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4553/Reviewer_RvL6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4553/Reviewer_RvL6"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a physics constrained autoencoder for time-series data forecasting. It builds on existing work done on Koopman Autoencoder and it's subsequent improvement consistent Koopman Autoencoder to propose prediction consistent KAE. The main idea of the paper is to use the time invariance property of the Koopman operator to enforce prediction consistency in the latent space. The final result is an added regularization term on top of the loss used in its predecessor cKAE."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Even though the ultimate contribution of the paper boils down to proposing a regularization term, the regularization itself is well motivated from a theoretical standpoint and is very clearly backed up by the improvements shown in the experiments. So, while the model may not be completely original it is definitely a significant improvement over its predecessors. The paper is also clearly presented and the experiments section is very thoroughly and fairly done. It is nice to see the confidence intervals and not just mean results being presented, and also full details of hyperparameter optimization being presented and fairly held the same across competing methods. The improvements in the results seem drastic and very significant!"
                },
                "weaknesses": {
                    "value": "I am not an expert in this field and had a hard time finding any weaknesses in the paper. But when I see a loss function of the form presented in Eq 12 it does make me wonder about both the data and time cost of grid search on those hyperparameters."
                },
                "questions": {
                    "value": "I'd appreciate author's comment on the question raised in weakness section. \n\nFrom a visual perspective, it might also be worth adding a figure showing the predictive performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4553/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614750398,
            "cdate": 1698614750398,
            "tmdate": 1699636432919,
            "mdate": 1699636432919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jDlew3fsWx",
                "forum": "MBMusjwfBk",
                "replyto": "KKyWGnCquO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for valuable comments. The issue the reviewer raised regarding multiple loss functions is a relevant one, but not unique to pcKAE, rather present in all KAEs [1,2]. This issue is common in multi-objective learning where one has to deal with multiple loss functions. The weights are essentially the hyperparameters, and needs tuning through grid search or random search. In this case, our method is not any different from [1,2]. Although there are no strict rules for choosing the weights, typically the weights corresponding to identity and forward loss i.e. $\\gamma_{id}$ and $\\gamma_{fwd}$ will have the higher values compared to the rest. This is because $\\gamma_{id}$ is used to train the encoding and decoding of the state using labelled data, ensuring the basic autoencoder operation. Since we are mostly interested in forward prediction, $\\gamma_{fwd}$ also plays an important role.\n\nWe agree that it is worth adding figures for visual perspective. However, for the initial draft we couldn't add the figures due to the page limit. We thank the reviewer for the suggestion.\n\n[1] B. Lusch, J. N. Kutz, S. L. Brunton, Deep learning for universal linear embeddings of nonlinear dynamics, Nature communications 9 (1) (2018) 4950.\n\n[2] O. Azencot, N. B. Erichson, V. Lin, M. Mahoney, Forecasting sequential data using consistent koopman autoencoders, in: International Conference on Machine Learning, PMLR, 2020, pp. 475\u2013485."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647695667,
                "cdate": 1700647695667,
                "tmdate": 1700647860186,
                "mdate": 1700647860186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VshBAz2M1m",
            "forum": "MBMusjwfBk",
            "replyto": "MBMusjwfBk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4553/Reviewer_NiJE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4553/Reviewer_NiJE"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript introduced a prediction-consistent Koopman autoencoder (pcKAE) for predicting the behavior of dynamical systems. The authors state that by introducing the prediction consistency loss which satisfies the mathematical constraint, their pcKAE model leads to higher expressivity and generalizability. The authors provided some interesting findings on the dynamical system learning with the help of Koopman theory. The results seem to support the authors\u2019 conclusion."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "++ The incorporation of the prediction consistency loss improves the long-term predictability of cKAE. \n\n++ The paper is easy to read and understand."
                },
                "weaknesses": {
                    "value": "-- The novelty of the paper is very limited. Adding the prediction consistency loss to the training process as a regularizer is not new, which has been used in many other similar models.\n\n-- The experiments considered to demonstrate the capability of the model are rather simple. The authors should test the method on other complex systems, e.g., 2D/3D GS reaction-diffusion equations, 2D homogeneous isotropic turbulence at Re > 1000, etc."
                },
                "questions": {
                    "value": "1. The authors should provide more creative thinking on the network structure or put forward some further theoretical analyses of the method instead of just doing some obvious mathematical derivation. For example, the authors may find it helpful to improve the latent space learning by adding Fourier transformation, e.g., introduced in [1], making the operator learning to focus on the frequency quantities of the dynamics. Such an improvement may make their work distinguish from the simple incremental one.\n\n2. The author should design the experiments more carefully to prove the advantages of the mode. In particular, the experiments considered to demonstrate the capability of the model are rather simple. The authors should test the method on other complex systems, e.g., 2D/3D GS reaction-diffusion equations, 2D homogeneous isotropic turbulence at Re > 1000, etc.\n\n3. The author should pay more attention on the writing. On page 4, the formulation of the forward loss, I guess, represents the $k$-step prediction of $\\hat{\\mathbf{x}}_{n+k}$. For different $n$, the symbol should have different meaning. Such a notation may lead to ambiguity. A similar issue exists on the formulation the backward loss of the same page. On page 7, the beginning of subsection 3.5, the authors used the same symbol to represent the train and test data by mistake. It's necessary to use more accurate notations in the manuscript for the sake of preciseness of scientific writing.\n\n[1] Xiong, Wei et al. \u201cKoopman neural operator as a mesh-free solver of non-linear partial differential equations.\u201d ArXiv abs/2301.10022 (2023)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4553/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699178421077,
            "cdate": 1699178421077,
            "tmdate": 1699636432837,
            "mdate": 1699636432837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1A6QGqe659",
                "forum": "MBMusjwfBk",
                "replyto": "VshBAz2M1m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Weakness 1,2 and Qustions 1,2 and 3"
                    },
                    "comment": {
                        "value": "Weakness:\n\nW1. To the best of our knowledge we have not come across any work implementing prediction consistency for time-series prediction. If the reviewer can point out the reference, we would be grateful.\n\nW2. While we agree that turbulence at Re$> 1000$ is more challenging to model, the vortex shedding with Re $=100$, have been used in [2] as a benchmark problem.  Fluid flow past cylinder with vortex shredding have been used as a benchmark problem in data-driven modeling of dynamical system [1,3,4]. In fact, [2] mentions \"This model has been a benchmark in fluid dynamics for decades...\". The pendulum example is also used extensively for benchmarking data-driven methods [1,5,6,7,8]. Authors in [1] mentions \"The nonlinear (undamped) pendulum (Hirsch et al., 1974) is a classic textbook example for dynamical systems, which is also used for benchmarking deep models...\". So we respectfully disagree with the reviewer's remark regarding benchmark dataset. Ideally, no number of experiments is enough. But we have tried to cover wide range of physical systems from benchmark test-cases like pendulum, flow-past cylinder to plasma and sea-surface temperature data.\n\nQuestions:\n\nQ1. While the idea presented in this work is rather intuitive and \"simple\", we do not think that as a disadvantage. Rather we think its simplicity yet effectiveness, makes it more attractive. With this simple prediction consistency term we are able to outperform state-of-the art KAE models for limited and noisy training data. The mathematical derivations might not be very complicated, but required to establish a theoretical backing of our proposed work. The reviewers suggestion regarding learning in Fourier space is interesting, and we will look into it. We thank the reviewer for the suggestion.\n\nQ2. This is addressed in W2.\n\nQ3. With due respect, we could not quite comprehend the concern with the notation. Different $n$ denotes different time-samples in the training dataset for which we want to calculate the forward/backward loss. For example, if our training set is $\\{\\mathbf{x}_1,\\mathbf{x}_2,\\mathbf{x}_3,\\mathbf{x}_4\\}$, and we choose $k=2$, then we for forward loss we will take $n=1,2~(M=2)$. We will apply the learned koopman operator twice on $\\mathbf{x}_1$ and $\\mathbf{x}_2$ to get $\\hat{\\mathbf{x}}_3$ and $\\hat{\\mathbf{x}}_4$ respectively. We will then compare $\\hat{\\mathbf{x}}_3$ with ${\\mathbf{x}}_3$ and $\\hat{\\mathbf{x}}_4$ with ${\\mathbf{x}}_4$.\n\nReferences:\n\n[1] B. Lusch, J. N. Kutz, S. L. Brunton, Deep learning for universal linear embeddings of nonlinear dynamics, Nature communications 9 (1) (2018) 4950.\n\n[2] O. Azencot, N. B. Erichson, V. Lin, M. Mahoney, Forecasting sequential data using consistent koopman autoencoders, in: International Conference on Machine Learning, PMLR, 2020, pp. 475\u2013485.\n\n[3] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equations from data by sparse identification of nonlinear dynamical systems, Proceedings of the national academy of sciences 113 (15) (2016) 3932\u20133937.\n\n[4] S. Bagheri, Koopman-mode decomposition of the cylinder wake, Journal of Fluid Mechanics 726 (2013) 596\u2013623.\n\n[5] S. Greydanus, M. Dzamba, J. Yosinski, Hamiltonian neural networks, Advances in neural information processing systems 32 (2019).\n\n[6] Z. Chen, J. Zhang, M. Arjovsky, L. Bottou, Symplectic recurrent neural networks, in: International Conference on Learning Representations, 2020. \n\n[7] O. Bounou, J. Ponce, J. Carpentier, Online learning and control of dynamical systems from sensory input, in: NeurIPS 2021-Thirty-fifth Conference on Neural Information Processing Systems Year, 2021.\n\n[8] B. Azari, D. Erdogmus, Equivariant deep dynamical model for motion prediction, in: International Conference on Artificial Intelligence and Statistics, PMLR, 2022, pp. 11655\u201311668."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647335841,
                "cdate": 1700647335841,
                "tmdate": 1700647335841,
                "mdate": 1700647335841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vlmvyip0Vp",
                "forum": "MBMusjwfBk",
                "replyto": "1A6QGqe659",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Reviewer_NiJE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Reviewer_NiJE"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' rebuttal"
                    },
                    "comment": {
                        "value": "I don't think my questions and comments are well addressed. In particular, I don't buy the authors' argument \"no number of experiments is enough\". The examples considered in the paper are too simple, which cannot support the conclusion that the proposed method is powerful in modeling complex spatiotemporal dynamics. A proper selection of test cases covering different levels of difficulty is essential. Hence, I maintain my original rating of rejecting the paper for solid reasons."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651439491,
                "cdate": 1700651439491,
                "tmdate": 1700651439491,
                "mdate": 1700651439491,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FZMslP8zwk",
            "forum": "MBMusjwfBk",
            "replyto": "MBMusjwfBk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4553/Reviewer_dPS3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4553/Reviewer_dPS3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the predication-consistent Koopman autoencoder (pcKAE), which introduces a consistency regularization term that enforces consistency among predictions at different time-steps. It is capable of accurate long-term predictions with limited and noisy training data. The paper also presents an analytical justrification for consistency regularization using the Koopman spectral theory. The paper performs comparative experiments on 4 classic nonlinear systems or datasets to show its performance."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper proposes a new type of regularisation term, which contributes to better Koopman autoencoder for long-term prediction."
                },
                "weaknesses": {
                    "value": "1. The statements or descriptions of the conclusions or technical points sometimes are fairly casual or inaccurate. We strongly recommend the authors to dive into the Koopman theory and address them carefully and rigorously. Like, \"learn a reduced-order feature space exhibiting simpler linear dynamics\", \"Koopman operator maps *between* infinite-dimensional function spaces\", \"the dynamics can be linearly approximated\", \"by the finite-dimensional Koopman operator\", etc. It seems the authors use the LTI system perspective to understand the linear property of Koopman operator, the dynamics of functionals (ie. observables) is linear, etc. \n2. Theorem 1, as the main result in theory, is not satisfactorily rigorous in math, which even involves mistakes (e.g., what is exactly the G? It is not matched with what claimed in its proof). If the latent space is just G, according to your math description, it is just a set of N_l functionals/elements?? And, the theorem does not carefully addresses the conditions of the flow or trajectory, x_t. The proof seems using the coordinates of the operator to show something, please address rigorous in math.  \n3. The contribution of pcKAE may not be promising. As far as we can see, the paper contributes by introducing the so-called \"prediction-consistency\" regularisation term, which however is straightforward for enhancing the k-step predictiability.\n4. The authors claimed the performance of pcKAE for long-term and high-dimensional prediction from noisy data. For long-term prediction, we assume pcKAE achieves it by increase the time-span in your proposed regularisation term, neglecting its practicability. For high-dimensional point, the authors seem misunderstanding this concept. Our nonlinear system eq.(1) is the state-space model without output equation, where the state is usually multivariate. High-dimensional (statistical) modeling or learning usually refers to such a task that the dimension of problem is so high that the data is deficit. Moreover, as indicated by the autoencoder (AE) word in pcKAE, is the dimension of latent layer N_l even smaller than the state dimension N_d? If so, yours actually deal with a very special case, where the dimension of the Koopman invariant space (that is large enough to model the given flow) is finite and rather small (smaller than the state dim). Actually the Koopman approach implicitly acqures the finite-dim lifted space is high enough, since it is used to approximate an infinite-dim space of functionals. Well, this is not argument for pcKAE only, it is for all KAE structures. The last point for noisy data: as we know for the Koopman setup, we are building the Koopman-based identification framework for state-space equation without process noise, as eq.(1); you have to be careful when addressing any properties or performance for noisy-data performance.\n\n5. The comparative study is not enough. As the literature review has addressed, the paper has to show that pcKAE can really help to improve the performance of nonlinear system identification in the Koopman perspective.  There are many Koopman-based neural network models for time-series prediction. The only improvement over KAE showed in experiments may not convince readers of the values of pcKAE  for nonlinear system modeling."
                },
                "questions": {
                    "value": "1. There seems mistakes in eq.(7), where there are matrix dimension-matching issues.\n2. The propose loss function eq.(12) for pcKAE consists of so many regularisation terms, where how these regularisation parameters can be tuned in practice. Are the performance sensitive to the choice of these parameters?\n3. What do you mean by \"consistency\" in your proposed regularisation term? It seems it is nothing related to the \"consistency\" in statistics or any well-known concept."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4553/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4553/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4553/Reviewer_dPS3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4553/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699330286427,
            "cdate": 1699330286427,
            "tmdate": 1699636432741,
            "mdate": 1699636432741,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RMw44EXEgv",
                "forum": "MBMusjwfBk",
                "replyto": "FZMslP8zwk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Due to the character limit, we will respond via multiple comments. This comment  addresses point 1, 2 and 3.\n\n1. We respectfully point out that we could not quite comprehend the issues with the mentioned statements. Let us address them one by one:\n\na) \"learn a reduced-order feature space exhibiting simpler linear dynamics\" : The idea of autoencoders is to compress the high-dimensional data to a low dimensional (\"reduced-order) feature space, where the dynamics is linear and ``simpler\" because of ease of analysis, prediction and control compared to non-linear counterpart.\n\n b) \"Koopman operator maps between infinite-dimensional function spaces\": By nature, Koopman operator operates on functionals which are infinite dimensional.\n\n c) \"the dynamics can be linearly approximated\": The idea of using neural network is to approximate the finite-dimensional Koopman invariant subspace which exhibits linear dynamics.\n\n d) \"by the finite-dimensional Koopman operator\" : We agree that the sentence could be restructured as ``by the finite-dimensional approximation of Koopman operator\", for better understanding. But it should be clear from the previous sentence which states \"Machine learning techniques utilize a finite-dimensional approximation of the Koopman operator by assuming the existence of a finite-dimensional Koopman invariant function space.\".\n\n e)  \"It seems the authors use the LTI system perspective to understand the linear property of Koopman operator, the dynamics of functionals (ie. observables) is linear, etc.\": Under the Koopman invariance assumption, the latent state dynamics is LTI by the virtue of time-homogeneity of the original nonlinear\u00a0system. If we can find a finite-dimensional Koopman invariant subspace, the restriction of the infinite-dimensional Koopman operator is indeed a finite-dimensional linear operator. All the major works [1,2] in Koopman based forecasting inherently assumes the time-invariance of the dynamics, since the prediction loss for training is evaluated at multiple time-steps for the same operator. There is also plethora of literature on DMD and EDMD [4,5] which essentially uses the LTI system perspective to understand the linear property of finite-dimensional approximation of the Koopman operator. \n\n\n\n2. We agree that there is a slight abuse of notation while writing $\\mathcal{G}$. In the theorem $\\mathcal{G}$ is mistakenly written as just the enumeration of the dictionary, i.e., the set of $N_l$ functionals. It should be the latent functional space (the one that should be Koopman invariant) is $\\operatorname{span} \\\\{ \\psi_1(\\cdot)\\ldots,\\psi_{N_l}(\\cdot)\\\\}$. The theorem should be read: Let $\\mathbf{\\Psi}(\\cdot)=[\\psi_1(\\cdot),\\ldots,\\psi_{N_l}(\\cdot)]^T \\in \\mathcal{F}^{N_l}$ denotes a vector valued function comprised with scalar functions $\\psi_i(\\cdot)\\in \\mathcal{F}$. The latent function space $\\mathcal{G} = \\operatorname{span}\\\\{\\psi_1(\\cdot)\\ldots,\\psi_{N_l}(\\cdot)\\\\}$ forms a Koopman invariant subspace with respect to the system dynamics (1) if and only if there exists $K\\in \\mathbb{R}^{N_l\\times N_l}$ such that $\\mathbf{\\Psi}_e (\\mathbf{x}\\_{n+\\kappa})=K^{\\kappa} \\mathbf{\\Psi}_e(\\mathbf{x}\\_{n})$ for all $n \\geq 0$ and $ \\kappa\\geq 1$.\n\nIn the proof, $\\operatorname{span}\\mathcal{G}$ should be replaced by $\\mathcal{G}$ which itself is a span of observables. The authors apologize for this inadvertent mistake and corrects it in the revised version.\n\nThe authors are unable to understand what the reviewer meant by ``coordinates of the operator\", a clarification will be greatly appreciated.\n\n3. We respectfully disagree with the reviewer regarding promise of this work. We believe that promise of the work should be judged by the results. We have tested our method for benchmark problems (pendulum, flow-past cylinder) problems, and have shown improvement (in some cases drastic) in long-term prediction accuracy in presence of noise of limited data. The fact that the a simple and intuitive idea can make such significant improvement, makes this work promising.\n\nReferences:\n\n[1] B. Lusch, J. N. Kutz, S. L. Brunton, Deep learning for universal linear embeddings of nonlinear dynamics, Nature communications 9 (1) (2018) 4950.\n\n[2] O. Azencot, N. B. Erichson, V. Lin, M. Mahoney, Forecasting sequential data using consistent koopman autoencoders, in: International Conference on Machine Learning, PMLR, 2020, pp. 475\u2013485.\n\n[3] J. N. Kutz, S. L. Brunton, B. W. Brunton, J. L. Proctor, Dynamic mode decomposition: data-driven modeling of complex systems, SIAM, 2016.\n\n[4] M. O. Williams, I. G. Kevrekidis, and C. W. Rowley, \"A data-driven approximation of the Koopman operator: Extending dynamic mode decomposition,\" Journal of Nonlinear Science, vol. 25, no. 6, pp. 1307-1346, 2015."
                    },
                    "title": {
                        "value": "Point 1,2 and 3"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644623873,
                "cdate": 1700644623873,
                "tmdate": 1700646566560,
                "mdate": 1700646566560,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CYeDpyzuWv",
                "forum": "MBMusjwfBk",
                "replyto": "FZMslP8zwk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Point 4, 5"
                    },
                    "comment": {
                        "value": "4. We thank the reviewer for pointing out few interesting points. We have addressed them pointwise and clarified few :\n\n    a) \"For long-term prediction, we assume pcKAE achieves it by increase the time-span in your proposed regularisation term, neglecting its practicability.\" : The look-ahead steps or as the reviewer calls it ``time-span\" is a crucial hyperparameter for training Koopman autoencoders (KAE) for accurate long-term prediction. However, this accuracy comes at the cost of training time. This is not only drawback of KAE, but any long-term forecasting model which uses multiple look-ahead steps. Now with limited data we do not have the luxury of a trade-off between training time and accuracy because we don't have the option available for large look-ahead steps. So, with limited data with state-of-the-art KAE fails to produce accurate long-term predictions. But our proposed prediction consistency term enables KAE to look ahead multiple steps. Obviously it comes at the increased training-time, but without this loss the predictions would have been anyway useless.\n\n     b) \"For high-dimensional point, the authors seem misunderstanding this concept.\": The reviewer most probably misunderstood the nature of ``high-dimensionality\" we are discussing in this paper. Note that here we are concerned about the high-dimensional physical systems such as fluid, plasma, weather systems etc. Since we are mostly dealing with high-fidelity spatio-temporal simulations (the test cases involving fluid and plasma), the high-dimensionality arises from the discretization of the partial differential equation (PDE), to be very specific, the discretization of the spatial domain via large number of mesh elements (nodes, edges etc.). For experimental data (SST), it corresponds to different spatial(sensor) locations in sea surface. It is different from the high-dimensional statistical modeling since in spatio-temporal physical systems, there is often some spatial dependencies, typically resulting in an underlying low-dimensional structure. \n\n     Since we are searching for a `reduced\"-order model, we intentionally keep the number of nodes in bottleneck layer less than the number of input nodes. We mention this in the abstract as well. In this respect the goal of the paper aligns more with [2] compared to [1]. We understand that reviewer is concerned in general about the nature of KAE itself. The underlying assumption for applying KAE is the existence of a low-dimensional dynamics. Some other works where autoencoders are used for reduced-orde modeling of high-dimensional PDEs are [3,4,5]. A typical example of such Koopman-operator based reduced-order modeling is DMD [7] and EDMD [8]. Note that KAEs are just the neural network version of\u00a0EDMD.\n     \n     c) \"The last point for noisy data ... for noisy-data performance.\": We thank the reviewer for mentioning this important point. We agree that we have to be careful and specific while addressing our model's noise handling property. One key factor which most probably helps mitigate the zero-mean Gaussian noise is some kind of ``averaging\" effect when we enforce prediction consistency among different predictions. For studying the effect of noise, we have followed similar approach as in [2].\n  \n5. With due respect, we did not understand the concern here since we have shown improvement over the state-of-the-art consistent KAE [2] (benchmark) for limited and noisy data for benchmark test-cases like pendulum and flow-past cylinder. Furthermore, [2] assumes existence of a backward dynamics, whereas our pcKAE does not make such assumption. We agree that use of \"prediction-consistency\" is not specific or limited to Koopman perspective, but we see it as a strength since it can be extended to any time-invariant system. \n\n[1] B. Lusch, J. N. Kutz, S. L. Brunton, Deep learning for universal linear embeddings of nonlinear dynamics, Nature communications 9 (1) (2018) 4950.\n\n[2] O. Azencot, N. B. Erichson, V. Lin, M. Mahoney, Forecasting sequential data using consistent koopman autoencoders, in: International Conference on Machine Learning, PMLR, 2020, pp. 475\u2013485.\n\n[3] L. Agostini, Exploration and prediction of fluid dynamical systems using auto-encoder technology, Physics of Fluids 32 (6) (2020).\n\n[4] N. B. Erichson, M. Muehlebach, M. W. Mahoney, Physics-informed autoencoders for lyapunov-stable fluid flow prediction, arXiv preprint arXiv:1905.10866 (2019).\n\n[5] I. Nayak, M. Kumar, F. L. Teixeira, Koopman autoencoders for reduced-order modeling of kinetic plasmas, Advances in Electromagnetics Empowered by Artificial Intelligence and Deep Learning (2023) 515\u2013542.\n\n[7] P. J. Schmid, Dynamic mode decomposition and its variants, Annual Review of Fluid Mechanics 54 (2022) 225\u2013254.\n\n[8] M. O. Williams, I. G. Kevrekidis, and C. W. Rowley, \"A data-driven approximation of the Koopman operator: Extending dynamic mode decomposition,\" Journal of Nonlinear Science, vol. 25, no. 6, pp. 1307-1346, 2015."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645735416,
                "cdate": 1700645735416,
                "tmdate": 1700645735416,
                "mdate": 1700645735416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GRLGoDxIIW",
                "forum": "MBMusjwfBk",
                "replyto": "FZMslP8zwk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions 1,2 and 3"
                    },
                    "comment": {
                        "value": "Q1. We thank the reviewer for pointing it out. There is a typo where $*$ and $i$ should be interchanged in the subscript of $K$ in the second term.  We will correct it in the revised manuscript.\n\nQ2. The issue the reviewer raised is a relevant one, but not unique to pcKAE, rather present in all KAEs [1,2]. This issue is common in multi-objective learning where one has to deal with multiple loss functions. The weights are essentially the hyperparameters, and needs tuning through grid search or random search. In this case, our method is not any different from [1,2]. Although there are no strict rules for choosing the weights, typically the weights corresponding to identity and forward loss i.e. $\\gamma_{id}$ and $\\gamma_{fwd}$ will have the higher values compared to the rest. This is because $\\gamma_{id}$ is used to train the encoding and decoding of the state using labelled data, ensuring the basic autoencoder operation. Since we are mostly interested in forward prediction, $\\gamma_{fwd}$ also plays an important role.\n\nQ3. In statistics, consistency usually refers to a property of an estimator where the estimates it produces converge to the true parameter value as the sample size increases. But in our case consistency means similarity between two predictions. So, in our case, higher consistency refers to the reduction of variance among different\u00a0estimates.\n\n[1] B. Lusch, J. N. Kutz, S. L. Brunton, Deep learning for universal linear embeddings of nonlinear dynamics, Nature communications 9 (1) (2018) 4950.\n\n[2] O. Azencot, N. B. Erichson, V. Lin, M. Mahoney, Forecasting sequential data using consistent koopman autoencoders, in: International Conference on Machine Learning, PMLR, 2020, pp. 475\u2013485."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646042381,
                "cdate": 1700646042381,
                "tmdate": 1700646042381,
                "mdate": 1700646042381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Anea4YyrbW",
                "forum": "MBMusjwfBk",
                "replyto": "FZMslP8zwk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4553/Reviewer_dPS3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4553/Reviewer_dPS3"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the authors for their detailed responses and further clarifications!\n\nLet me summarize our comments. Before that we would like to point out that we are familiar with both Koopman theory, including Koopman operator (math), Koopman-based system identification (control engineering), Koopman-based nonlinear system modeling (complex systems in physics), and deep learning. Almost all papers you listed had been read by reviewers. So there is no knowledge gap or misunderstanding between our backgrounds.\n\nOur arguments on weakness are mainly from the following two sides:\n\n- (Koopman theory/methods) Your phrases/descriptions/clarifications on technical points and math are fairly careless. Your article hasn't been well-cooked. We really cannot accept your math \"typos\" appearing even in major problem formulation, theorems ,etc. When you address something on Koopman, (what is \"finite-dim\", \"infinite-dim\", can or cannot be approximated, when approximated do we assume finite point-spectra or existence of invariant subspace, etc.,) all these details have to be carefully handled.  Nonlinear system modeling is complicated, and you have to carefully draw a boundary on what your method can solve. \n\n- (deep learning) We are sorry to say that your contribution is minor. Proposing a regularization term additionally, without throughout analysis/argument in theory or convincing experiments, is not qualified for publishing ICLR. From your article, we might predicate you may either an early researcher or  a researcher that is new to either of deep learning or Koopman nonlinear modeling. We strongly suggest the author to deepen your understanding of Koopman theory and deep learning, think about your method strictly in math, and, if theoretical analysis is difficult (mostly in NN), test it throughout in experiments. We understand that there are early papers on Koopman or Koopman-based NN getting published with a few toy experiments, while they are valued by their pioneered work. \n\nThus, it is unfortunate to say that your current article is not qualified for ICLR. Note that this does not imply your research topic is not qualified. The evaluation is drawn purely for your paper quality and your current outputs presented."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4553/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709946045,
                "cdate": 1700709946045,
                "tmdate": 1700709946045,
                "mdate": 1700709946045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]