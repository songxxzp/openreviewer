[
    {
        "title": "Cleaning label noise with vision-language models"
    },
    {
        "review": {
            "id": "aSgfqRoqQV",
            "forum": "1rgMkDWfYV",
            "replyto": "1rgMkDWfYV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the integration of pretrained vision-language models, like CLIP, into the process of learning from noisy labels. To this end, the authors introduce a method called CLIPSelector, which leverages CLIP's powerful zero-shot classifier and an easily-inducible classifier based on CLIP's vision encoder to select clean samples. Additionally, they introduce a semi-supervised learning approach called MixFix to gradually incorporate missing clean samples and re-label noisy samples based on varying thresholds to enhance performance. The authors validate their approach through a series of experiments on different benchmarks, including datasets with synthetic and real-world noise."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper breaks new ground by exploring the use of pretrained vision-language models, such as CLIP, to address the challenge of noisy labels. This approach is promising as it goes beyond relying solely on information from the noisy dataset.\n2. The fixed hyperparameters across all experiments showcase the robustness and practicality of the proposed method."
                },
                "weaknesses": {
                    "value": "1. My major concern is the potential unfair comparisons. The notable performance improvements shown in Tables 2-4 could be attributed to CLIP's superior representation learning capabilities. A fairer comparison could involve replacing the baselines' backbone with CLIP's visual encoder. Furthermore, Table 3 lacks comparison results with recent works focused on instance-dependent noise from 2022-2023.\n2. Discrepancies between the CLIP's zero-shot results on CIFAR in Table 1 and the original paper need clarification.\n3. The claims regarding inferior performance on Red Mini-ImageNet require more explanation and context.\n4. What does SOTA in Table 1 means? Please supplement the necessary details.\n5. Ambiguous statements like \"on the clean test set of in-question noisy datasets\" should be elucidated to enhance clarity.\n6. The derivation of Eq. 4 from Eq. 3 is not explained, and the effect of the added class feature in the prompt remains unclear. Additional ablation studies are necessary to substantiate these claims."
                },
                "questions": {
                    "value": "Please see the weaknesses, thx"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3300/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751659621,
            "cdate": 1698751659621,
            "tmdate": 1699636279061,
            "mdate": 1699636279061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OoB9r4aRJl",
                "forum": "1rgMkDWfYV",
                "replyto": "aSgfqRoqQV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tEQg (part 1)"
                    },
                    "comment": {
                        "value": "Thanks for the insightful reviews and acknowledgement about the method and presentation. We are happy to answer your raised questions below:\n> *Q1.1.My major concern is the potential unfair comparisons. The notable performance improvements shown in Tables 2-4 could be attributed to CLIP's superior representation learning capabilities. A fairer comparison could involve replacing the baselines' backbone with CLIP's visual encoder.*\n\nWe understand the reviewers' concerns, but we chose the specific settings precisely to allow for fair comparisons: as it stands, all our comparisons use the same backboneas reported in the original paper. Our main motivation is to bring the benefits of using visual language models to the \"Learning with Noisy Labels\" community -- we believe that this is a significant contribution. We show that with a very simple solution we achieve SOTA. There are indeed many other ways that VL models could be used in this domain, we simply use CLIP for sample selection to avoid its direct impact on the in-training model to further ensure a fair comparison. And we hope that other researchers will explore them in their work. See our response to reviewer s2nG Q2 for more discussion and results.\n\n> *Q1.2.A fairer comparison could involve replacing the baselines' backbone with CLIP's visual encoder.*\n\nPlease refer to our response to reviewer Vnif Q1.2.\n\n>*Q1.3. Furthermore, Table 3 lacks comparison results with recent works focused on instance-dependent noise from 2022-2023.*\n\nWe will incorporate more recent results in Table 3 in the updated verision. Due to time limitation, we present below an initial version in Table R2 with one more recent work for your perusal:\n| Method | 10\\% | 20\\% | 30\\% | 40\\% |\n|---|---|---|---|---|\n| Cross-Entropy | 91.25 | 86.34 | 80.87 | 75.68 |\n| F-correction | 91.06 | 86.35 | 78.87 | 71.12 |\n| Co-teaching | 91.22 | 87.28 | 84.33 | 78.72 |\n| GCE | 90.97 | 86.44 | 81.54 | 76.71 |\n| DAC | 90.94 | 86.16 | 80.88 | 74.80 |\n| DMI | 91.26 | 86.57 | 81.98 | 77.81 |\n| SEAL | 91.32 | 87.79 | 85.30 | 82.98 |\n| SELC [1] | 91.63 | 88.33 | 86.28 | 84.23 |\n| Cross-Entropy (reproduced) | 90.76 | 86.08 | 80.64 | 75.27 |\n| CLIPSelector + Cross-Entropy | **92.33** | **91.06** | **89.71** | **88.26** |\n\nTable R2. More results on instance-dependent noise from [2]\n\nWe are happy to include more discussions if the reviewer is aware of any other related works reported on the same bencmarks. There are also other works proposing different models for instance-dependent noise such as [2]. We here report extra results in Table R3 with noisy labels provided by [3] (https://github.com/pxiangwu/PLC/tree/master/cifar/noisy_labels). According to Tables R2 and R3, we can verify that at various noise ratios and noise types, significant improvements are obtained by simply introducing CLIP-based sample selection.\n\n| Method | Type-I (35%) | Type-I (70%) | Type-II (35%) | Type-II (70%) | Type-III (35%) | Type-III (70%) |\n|---|---|---|---|---|---|---|\n| Co-teaching+ | 79.97\u00b10.15 | 40.69\u00b11.99 | 77.35\u00b10.44 | 45.44\u00b10.64 | 78.38\u00b10.67 | 41.90\u00b10.86 |\n| GCE | 80.65\u00b10.39 | 36.52\u00b11.62 | 77.60\u00b10.88 | 40.30\u00b11.46 | 79.18\u00b10.61 | 37.10\u00b10.59 |\n| SL | 79.76\u00b10.72 | 36.29\u00b10.66 | 77.92\u00b10.89 | 41.11\u00b11.92 | 78.81\u00b10.29 | 38.49\u00b11.46 |\n| LRT | 80.98\u00b10.80 | 41.52\u00b14.53 | 80.74\u00b10.25 | 44.67\u00b13.89 | 81.08\u00b10.35 | 44.47\u00b11.23 |\n| PLC [3] | 82.80 \u00b1 0.27 | 42.74 \u00b1 2.14 | 81.54 \u00b1 0.47 | 46.04 \u00b1 2.20 | 81.50 \u00b1 0.50 | 45.05 \u00b1 1.13 |\n| MDDC     [4] | 83.12\u00b10.44 | 43.70\u00b10.91 | 80.59\u00b10.19 | 47.06\u00b10.41 | 82.02\u00b10.23 | 46.31\u00b10.92 |\n| CDDC  [4] | 83.17\u00b10.43 | 45.14\u00b11.11 | 80.76\u00b10.25 | 48.10\u00b11.74 | 81.58\u00b10.07 | 47.92\u00b11.28 |\n| CLIPSelector + Cross-Entropy | **89.35\u00b10.24** | **67.47\u00b10.56** | **88.59\u00b10.27** | **67.74\u00b10.46** | **89.60\u00b10.29** | **66.84\u00b10.87** |\n\n\nTable R3. Results on instance-dependent noise from [3]\n\n> *Q2.Discrepancies between the CLIP's zero-shot results on CIFAR in Table 1 and the original paper need clarification.*\n\nAdmittedly, reproducing the reported results from the original CLIP paper is actually challenging, as acknowledged in the discussion here: https://github.com/openai/CLIP/issues/164.  The reported numbers in our paper were obtained under consistent experimental settings to ensure a fair and reliable comparison."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900222599,
                "cdate": 1699900222599,
                "tmdate": 1699900222599,
                "mdate": 1699900222599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W5z4i5riMF",
                "forum": "1rgMkDWfYV",
                "replyto": "hz4XjG0MyE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
                ],
                "content": {
                    "title": {
                        "value": "Reply to ``Reply to Reviewer tEQg (part 1&2)\""
                    },
                    "comment": {
                        "value": "I commend the authors for their comprehensive response, including detailed statements and additional experimental results. However, a significant concern, echoing the sentiment of Reviewer Vnif, remains unaddressed - the replacement of the baselines' backbone with CLIP's visual encoder. This issue is crucial for the thorough understanding and validation of the proposed approach.\n\nFurthermore, I share the sentiment expressed by Reviewer GWvS regarding the paper's clarity. The confusion noted in the writing style, along with the perceived unnecessary complexity in the presentation of deviation details, hinders the paper's accessibility for readers. Clarifying and simplifying these aspects would greatly enhance the overall quality of the manuscript.\n\nRegrettably, in light of these concerns, I maintain my negative rating for the submission."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699928084368,
                "cdate": 1699928084368,
                "tmdate": 1699928084368,
                "mdate": 1699928084368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4tnfnAOOno",
                "forum": "1rgMkDWfYV",
                "replyto": "aSgfqRoqQV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further reply to Reviewer tEQg"
                    },
                    "comment": {
                        "value": "We appreciate your prompt reply. We would like to further clarify your concerns below:\n>*Q7: Backbone Replacement with CLIP's Visual Encoder.*\n\nTo finetune the CLIP visual encoder is another available style utilizing a vision-language model for LNL. Instead, we use offline sample selection, specifically because: \n\n- **Acknowledgement**: sample selection is the dominant paradigm for LNL, by utilizing CLIP for sample selection our method can be seamlessly utilized with existing methods. For example, simply adding an extra warmup stage before the original training frmework with the selected samples from CLIP. \n\n- **Effectiveness**: Viewing CLIP as a 'black-box foundation model', considering the possible computational cost/deployment issues, etc, the most direct and convenient way to utilize it would be the offline one. \n\nWe expect our method to **motivate the usage of CLIP in the LNL community since CLIP with only naive/offline sample selection shows competitive results**. We hope that other researchers in the field of LNL will explore other styles using vision-language methods in their work. \n\n\n**Fine-tuning experiment:** Regarding the suggestion to replace the model with CLIP's pretrained model, we here report fine-tuning results on SSR[1], as mentioned by Reviewer Vnif. Specifically, we fine-tune the CLIP pre-trained model with VIT-B/32 backbone, using its visual encoder with an additional linear layer classifier. We initially froze the encoder, train the classifier for one epoch with LR=0.02, and subsequently train the whole model with LR=0.001 and LR_scheduler=CosineAnnealing for 100 epochs, with a weight decay of 0.01. As a comparison, we use the same structure but randomized the weights of the encoder, train for 100 epochs with LR=0.001 and LR_scheduler=CosineAnnealing, with a weight decay of 0.01.  Initial results are presented below in Table R4:\n\n| Method | CIFAR10 90% symmetric noise |\n|---|---|\n| SSR | 93.45 |\n| CLIP+SSR | 95.98 |\n\nTable R4. SSR with CLIP\n\nAs expected, the pre-trained CLIP encoder provides effective improvement, further demonstrating the potential of the CLIP model for LNL. Also, please note that fine-tuning CLIP is not trivial, as described in the following GitHub issues. Through better hyperparameter settings, we believe there is still room for further progress. \n- https://github.com/openai/CLIP/issues/83\n- https://github.com/openai/CLIP/issues/150\n- https://github.com/openai/CLIP/issues/362\n\n---\n\n>*Q8: Paper's Clarity.*\n\nRegarding the presentation of the paper, we are currently making revisions to the manuscript based on the feedback from all reviewers. We would like to convey that the majority of the changes are straightforward and actionable.\n\nWe hope that this further clarification addresses your concerns. If you have any additional questions, we are open to further discussion.\n\n*[1] SSR: An Efficient and Robust Framework for Learning with Unknown Label Noise, BMVC2022.*"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699955190091,
                "cdate": 1699955190091,
                "tmdate": 1699955197708,
                "mdate": 1699955197708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zntmf46rtk",
                "forum": "1rgMkDWfYV",
                "replyto": "4tnfnAOOno",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
                ],
                "content": {
                    "title": {
                        "value": "Further concerns"
                    },
                    "comment": {
                        "value": "Thank you for the additional clarification and conducting further experiments. Upon reviewing Table R4, I observed that the SSR results (93.45) are lower than those reported in [1] (95.2). I'd also like to confirm: does 'SSR' in Table R4 refer to SSR utilizing CLIP as the backbone, while 'CLIP+SSR' denotes SSR employing the suggested CLIP selector?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625341254,
                "cdate": 1700625341254,
                "tmdate": 1700625341254,
                "mdate": 1700625341254,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v1vr0kJMxO",
                "forum": "1rgMkDWfYV",
                "replyto": "twmVdBzVA4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your prompt response. However, my concern remains unresolved regarding Table R4. Specifically, I believe that comparing the proposed method to CLIP+SSR is a fair assessment. Yet, the performance of CLIP+SSR (95.98) surpasses that of your approach (94.23). It would be beneficial to include the results of 'SSR+your approach' to thoroughly explore the effectiveness of the proposed method."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653962492,
                "cdate": 1700653962492,
                "tmdate": 1700653962492,
                "mdate": 1700653962492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qmc51V2k1g",
                "forum": "1rgMkDWfYV",
                "replyto": "aSgfqRoqQV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further reply"
                    },
                    "comment": {
                        "value": "We appreciate your prompt reply and willingness to engage in further discussion. However, we would like to respectfully reiterate that comparing the online usage of large-scale pretrained CLIP models to offline sample selection of CLIP falls outside the scope of this work -  as discussed in the paper, the best style for CLIP in LNL is still underexplored. Besides, It's a well-established fact that directly finetuning large-scale pretrained models is highly possible to yield superior performance compared to offline usage. \n**Our primary goal is to introduce CLIP to the LNL community, and we believe this comparison is unnecessary and distracts from our focus.** We kindly refer the reviewer's attention to the **<Additional comments about the fairness and contribution of the proposed method>** response section for a more in-depth explanation of our approach's significance. Thanks for your time and consideration.\n\n**Specifically, regarding the numbers in Table R4, we can actually never make a comparison. For example, on CIFAR dataset, CLIP model applies VIT-B/32 with 224x224 px image input, while all current works utilize PreResNet18 with 32x32 px input. This is why we insist a offline usage of CLIP model to avoid effects of these extra factors in the original paper.**"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654799565,
                "cdate": 1700654799565,
                "tmdate": 1700656328740,
                "mdate": 1700656328740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1Z7Slo7QJv",
                "forum": "1rgMkDWfYV",
                "replyto": "Qmc51V2k1g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_tEQg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your quick response. While I understand your points, I remain concerned that comparing the online usage of CLIP models versus the proposed offline sample selection of CLIP would offer a more compelling comparison."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657596267,
                "cdate": 1700657596267,
                "tmdate": 1700657596267,
                "mdate": 1700657596267,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LlbIF7IF7G",
            "forum": "1rgMkDWfYV",
            "replyto": "1rgMkDWfYV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3300/Reviewer_Vnif"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3300/Reviewer_Vnif"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes using the pre-trained vision-language model CLIP for sample selection to mitigate selfconfirmation\nbias. Specifically, they introduce the CLIPSelector, which utilizes both the CLIP\u2019s zero-shot\nclassifier and an easily-inducible classifier based on its vision encoder and noisy labels for sample selection.\nAnd they further introduce a semi-supervised learning method called MixFix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well presented and explains the algorithm and experiments clearly.\n2. The experiments are conducted on various datasets."
                },
                "weaknesses": {
                    "value": "1. The performance lacks some competitiveness. Some methods are not compared, for example, SSR: An\nEfficient and Robust Framework for Learning with Unknown Label Noise.\n2. The main idea of the paper is to use the CLIP zero-shot classifier for sample selection and lacks novelty. And\nthe semi-supervised learning methods has also been applied in previous works."
                },
                "questions": {
                    "value": "1. This paper uses the CLIP pre-trained model, I think this is unfair for previous works without pre-trained model. Combining\nprevious methods with training from CLIP pre-trained model other than training from scratch should also be compared.\n2. Equations (2) misses )."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3300/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758088315,
            "cdate": 1698758088315,
            "tmdate": 1699636278976,
            "mdate": 1699636278976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zzrrOLVEpJ",
                "forum": "1rgMkDWfYV",
                "replyto": "LlbIF7IF7G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Vnif"
                    },
                    "comment": {
                        "value": "Thanks for the insightful reviews and acknowledgement about the experiments and presentation. We are happy to answer your raised questions below:\n\n> *Q1.1.This paper uses the CLIP pre-trained model, I think this is unfair for previous works without pre-trained model. *\n\nWe understand the concerns of the reviewer but we chose the specific setting precisely so as to perform fair comparisons: as it stands, all our comparisons are using the same encoders (e.g. PreResNet18) as reported in the original papers. We note that , in several cases,  CLIP-based encoders may be heavy and not desired to be used. In this condition, sample selection enables offline and efficient usage of CLIP model and ensures a fair comparison for all methods. \n\nOur main motivation is to introduce the benefits of using vision-language models into the \u201cLearning with Noisy Labels\u201d community -- we believe that this is a significant contribution. We show that with a very simple scheme we achieve SOTA. There are indeed many other ways that VL models could be used in this domain, we simply use CLIP for sample selection to avoid its direct impact on the in-training model to further ensure a fair comparison.  We hope that other researchers in the field of LNL will explore using VL methods in their work. Please refer to our reply to Q2 of Reviewer s2nG for more discussions and results.\n\n> *Q1.2.Combining previous methods with training from CLIP pre-trained model other than training from scratch should also be compared.*\n\nIntegrating CLIP with existing techniques for learning with noisy labels is an interesting research direction that we intend to explore in the future. Preliminary experiments demonstrate that a pretrained clip backbone does helps. In this paper, however, we focus on utilising CLIP as a sample selection method that does not require any adaptation to the training of the classifier,  and achieve SOTA/competitive results. We believe that this is a sufficient contribution.\n\n> *Q2.Equations (2) misses ).*\n\nWe apologize for the typo, we are fixing this along with other changes. We will upload the updated verison later.\n\nIf the reviewer has any additional questions or anything else to discuss, we are more than happy to engage further in the conversation.\n\n*[1] SSR: An Efficient and Robust Framework for Learning with Unknown Label Noise.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900144999,
                "cdate": 1699900144999,
                "tmdate": 1699900144999,
                "mdate": 1699900144999,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i3rIt3w2hQ",
            "forum": "1rgMkDWfYV",
            "replyto": "1rgMkDWfYV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3300/Reviewer_GWvS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3300/Reviewer_GWvS"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the issue of learning with noisy labels. They present an approach based on learning to select samples from a downstream dataset optimally to improve performance for a downstream task. Their approach is based on the CLIP model and is named CLIPSelector. Their central idea is to use a thresholding mechanism based on the zero-shot ability of the CLIP model to enable selection of cleaner samples and to detect which samples need to be relabeled. They utilize this approach to data augment the training set gradually thereby increasing sample difficulty by using the predictions of the trained model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall the paper addresses an important problem of learning under noise labels, which is critical for ML deployment. Moreover, the authors use an auxiliary foundation model that enables sample selection and since their approach is modular, this model can be substituted for a stronger model in the future. The hyper-parameter experiments for different thresholds will be useful for the readers. Additional discussions about the applicability of the method and how it performs on granularly labeled dataset (including identifying its shortcomings) is very welcome."
                },
                "weaknesses": {
                    "value": "W1: The biggest weakness of the paper is the writing. the authors have made the paper extremely complicated with inconsistent and complex notation. For eg: the addition of theorems 1 and 2 is not necessary for the paper, they can be relegated to the appendix. In addition, the strength of the inequality relies on the tightness of the bound. So it isn't a surprise that the conclusions drawn from the theorems hold, but the key point is how tightly they hold, which is impossible to know. Several important details that are required to be in the paper are relegated to the appendix, such as the hyper-parameter ablations on theta_r. Overall, the approach can be explained more simply and clearly instead of the complex framework that the authors have presented here, which seems unnecessary. \n\nW2: Incomplete description of experimental setups. The experiments section does not appear well constructed, although the experiments themselves are useful. For instance, it is unclear why Sec 4.1 exists before the results about model performance. The explanation of the first paragraph of section 1 is incredibly hard to parse through. \n\nW3: No qualitative results are presented. The authors present results on traditional benchmarks and claim their method performs better than SOTA (it isn't clear what SOTA is here from the tables), but fail to ask the question why do their approach perform better? What is the difference in behavior between \"easy noise\" and \"hard noise\"? Absence of qualitative analysis make it a subpar presentation for the reader."
                },
                "questions": {
                    "value": "1. Eq1: Sample selection mechanism takes as input the predicted probability and the label? Please clarify.\n2. Typo: Sec 3.2: \u201cwe consistent the notations for CLIP\u2019s training\u201d\n3. Clarify Sec 4.1 \u201cclean test set of in-question noisy datasets\u201d\n4. Appendix F: \"label noise normally does not affect the sample itself\": Label noise can be highly sample dependent, so I am unsure what the authors mean by this statement. \n5. Estimate P\u02dc(yi|xi) with CLIP zero-shot classifier: Re-formulating the CLIP loss, including the fact that sampling at a prompt level might yield a better ZS estimate. None of this is new, but it reads as though the authors are claiming this formulation as new. It will help to state that this is a reformulation of the standard CLIP ZS process, the only addition being a different prompt template, basically just Eq. 4. In addition, multiple prompt generation uses another language model that has its own biases which are conveniently not accounted for in the main text and case into the appendix. \n6. Using CLIP is suboptimal in one key manner since we dont have access to the training set, we are unsure of the biases existing in the CLIP model.\n7. Section 4.2:  \u201csynthetic symmetric/asymmetric noise.\u201d What is this noise model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3300/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699482659499,
            "cdate": 1699482659499,
            "tmdate": 1699636278908,
            "mdate": 1699636278908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z1386YAQIP",
                "forum": "1rgMkDWfYV",
                "replyto": "i3rIt3w2hQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer GWvS"
                    },
                    "comment": {
                        "value": "Thanks for the insightful reviews and suggestions. We are sorry about the confusion caused by the presentation and we appreciate that the reviewer  checked the appendix to find some  important points-- we will be happy to move material between the main paper and the appendix as the reviewer suggests. We are happy to provide further clarifications below:\n\n>*Q1: Eq1: Sample selection mechanism takes as input the predicted probability and the label? Please clarify.*\n\nYes, as formalized in sec 3.1, current sample selection methods usually rely on an estimated probability vector and noisy labels. Intuitively, a \u2018distance\u2019 or \u2018consistency\u2019 is calculated between the estimated probability and the annotated label, to measure the \u2018cleaness\u2019 of the label, such as the per-sample losses. \n\n>*Q2: Typo: Sec 3.2: \u201cwe consistent the notations for CLIP\u2019s training\u201d*\n\nWe apologize for the typos here, we will fix it as \u2018We make the notations consistent for CLIP's training\u2019.\n\n>*Q3: Clarify Sec 4.1 \u201cclean test set of in-question noisy datasets\u201d*\n\nWe apologize for any confusion. To clarify, when we mention \u2018the clean test set of in-question noisy datasets\u2019, we mean the clean test set corresponding to the noisy train set. We will address and rectify this point in the updated version of the paper.\n\n>*Q4: Appendix F: \"label noise normally does not affect the sample itself\": Label noise can be highly sample dependent, so I am unsure what the authors mean by this statement.*\n\nFollowing previous works in LNL community, we assume the causal mechanism between sample $x$ and label $y$ as $x \\rightarrow y$. Thus we assume the conditional probability ($P(y|x)$ - noisy labelling) will not affect the sample coefficient prior ($P(x)$).\n\n>*Q5: Estimate P\u02dc(yi|xi) with CLIP zero-shot classifier: Re-formulating the CLIP loss, including the fact that sampling at a prompt level might yield a better ZS estimate. None of this is new, but it reads as though the authors are claiming this formulation as new. It will help to state that this is a reformulation of the standard CLIP ZS process, the only addition being a different prompt template, basically just Eq. 4. In addition, multiple prompt generation uses another language model that has its own biases which are conveniently not accounted for in the main text and case into the appendix.*\n\nSorry for the confusion here. We do not claim the CLIP zero-shot classification as our contribution, rather we simply want to present the multi-feature prompting in our framework. We will improve the presentation in the updated version.\n\n>*Q6: Using CLIP is suboptimal in one key manner since we dont have access to the train set, we are unsure of the biases existing in the CLIP model.*\n\nIndeed, biases, errors and domain shifts are risks inherent when using any pre-trained model. With a very large train set, we expect the CLIP model can cover a wide range of domains. In comparison to ordinary vision-only pre-trained models, CLIP still possesses its unique advantages due to its language modality and zero-shot classification capability, offering sample selection that is not influenced by noisy labels.\n\n>*Q7: Section 4.2: \u201csynthetic symmetric/asymmetric noise.\u201d What is this noise model?*\n\nWe follow the widely-acknowledged definitions of symmetric and asymmetric noise in the LNL community. In particular, symmetric noise is generated through random label flipping, while asymmetric noise is induced by specific transitions between semantically similar classes, such as 'horse' \u2194 'deer' for the CIFAR-10 dataset.\n\nIf the reviewer has any additional questions or anything else to discuss, we are more than happy to engage further in the conversation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900171725,
                "cdate": 1699900171725,
                "tmdate": 1699900171725,
                "mdate": 1699900171725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "itMfBjO53x",
            "forum": "1rgMkDWfYV",
            "replyto": "1rgMkDWfYV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3300/Reviewer_s2nG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3300/Reviewer_s2nG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for learning with noisy labels, which focuses on selecting examples with vision-language models to alleviate the self-confirmation bias in vision-only models. Experiments on synthetic and real-world datasets are conducted to support the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea to exploit V-L models to address the self-confirmation problem is reasonable and interesting.\nThe presentation is clear."
                },
                "weaknesses": {
                    "value": "The second method to estimate the \\tilde{p}_{y|x} seems similar to the estimation of p_{y|x} with noisy labels. Since the classifier is learned with noisy data, how can it be used to estimate the clean probability? Authors should provide more explanation for this problem.\n\nThe results are inferior to many state-of-the-art methods, such as Unicon, PES, etc."
                },
                "questions": {
                    "value": "Please clarify the concerns stated in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3300/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699500572803,
            "cdate": 1699500572803,
            "tmdate": 1699636278839,
            "mdate": 1699636278839,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gg7S0aDKYn",
                "forum": "1rgMkDWfYV",
                "replyto": "itMfBjO53x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer s2nG"
                    },
                    "comment": {
                        "value": "Thanks for the insightful reviews and acknowledgement of our exploration. We are happy to answer your raised questions below:\n\n> *Q1.The second method to estimate the \\tilde{p}{y|x} seems similar to the estimation of p{y|x} with noisy labels. Since the classifier is learned with noisy data, how can it be used to estimate the clean probability? Authors should provide more explanation for this problem.*\n\nWe appreciate your concerns. As elucidated in Theorem 2, the second method is affected by label noise since we train a new classifier based on it. This is the motivation behind presenting two distinct methods for estimating $P(y|x)$ using CLIP. We believe that CLIP, with its unique language modality compared to ordinary pre-trained visual models, holds a significant advantage, particularly because the first method is entirely free from noise.\n\n> *Q2.The results are inferior to many state-of-the-art methods, such as Unicon, PES, etc.*\n\nWe appreciate your concerns and would like to emphasize that our method is not mutually exclusive but can be used in conjunction with existing techniques. For example, we can utilize CLIPSelector seamlessly with any existing methods including the mentioned UNICON and PES, by adding an extra warmup stage with only the selected samples from CLIPSelector. Due to time limitation, as an illustration, we present results after incorporating model co-training (one of the validated effective and simple technique) with our method (by exchanging the selected samples between two models) on CIFAR100 and Animal-10N datasets (Table R1), further demonstrating that our method has great potential along with existing techniques and it is comparable with existing works.\n\n| Dataset           | CIFAR100 0.5sym | CIFAR100 0.9sym | Animal-10N |\n|-------------------|-----------------|-----------------|------------|\n| Ours              | 75.23           | 63.11           | 88.14      |\n| Ours + Co-training | 77.51           | 66.72           | 88.79      |\n| UNICON [1] | 77.6           | 44.8           | \\      |\n| PES [2] | 74.3           | \\           | \\      |\nTable R1. Effect of co-training\n\nWe will include above discussions in the updated version.If the reviewer has any additional questions or anything else to discuss, we are more than happy to engage further in the conversation.\n\n*[1] UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning, CVPR2022.*\n\n*[2] Understanding and Improving Early Stopping for Learning with Noisy Labels, NeurIPS2021.*"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900115889,
                "cdate": 1699900115889,
                "tmdate": 1699900115889,
                "mdate": 1699900115889,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NHpyJBUlI3",
                "forum": "1rgMkDWfYV",
                "replyto": "gg7S0aDKYn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_s2nG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3300/Reviewer_s2nG"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the results in Table R1. However, the answer for Q1 is not totally convincing for me. Two distinct methods should be both reasonable by themselves."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3300/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621341972,
                "cdate": 1700621341972,
                "tmdate": 1700621341972,
                "mdate": 1700621341972,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]