[
    {
        "title": "TokenFlow: Consistent Diffusion Features for Consistent Video Editing"
    },
    {
        "review": {
            "id": "5WkPpRLusH",
            "forum": "lKK50q2MtV",
            "replyto": "lKK50q2MtV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3344/Reviewer_Mqtb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3344/Reviewer_Mqtb"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a framework, TokenFlow, for video editing task.\nTokenFlow runs in a correspondence-propagation manner, i.e., first seeks for the correspondences across different frames, jointly edits the keyframes, and then propagates features to ensure the temporal consistency. \nCompared to prior arts, TokenFlow shows a better temporal consistency and competitive editing fidelity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Approach:\n- Global consistency. TokenFlow utilizes a joint editing and feature propagation via NN feature correspondences. Compared to attention-based methods, this way is more explicit and tends to keep a global consistency across different frames in a video. \n- Compatible with other image-based editors. TokenFlow seems to be able to work with other diffusion-based image editors. \n\nExperiments & validation:\n- Proposed method is intuitive. First doing joint editing and then propagating the features makes sense. \n- From qualitative results, TokenFlow improves the temporal consistency and preserves fair fidelity. \n- Instead of using other pixelwise correspondences (e.g., dense flow or pixelwise trajectory), the authors propose to use nearest neighbor (NN) to find the correspondence. This seems new to me. \n\nWriting & presentation:\n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "Experiments:\n- Compared to other baselines, like Text2LIVE and Gen1, TokenFlow still shows some \"flickering\" when there are high-frequency patterns. For example, in \"Comparisons to Baselines\" SM, in the first example \"running dog\", the ground has severe flickering compared to Gen 1. Also in the third example \"cutting bread\", there is more flickering in the bread and background compared to Gen 1. Same thing also happened in the comparison to Text2LIVE in \"Additional Qualitative Comparisons\". Why is TokenFlow not able to maintain the consistency for the high-frequency patterns?\n- Based on the previous point, I think the authors could consider analyzing the reason behind the flickering and include it in **Limitations**.\n- How do different image editors affect the results? Specifically,  the comparison w/ PnP + propagation. The authors mention that they have an additional comparison with PnP-Diffusion + propagation in the **last sentence, Section 5 Baselines**, but this part seems missing either in the main paper or in the SM. \n- It would be great if the authors could also include runtime comparisons."
                },
                "questions": {
                    "value": "- Features & RGB images: In Figure 3, authors show that TokenFlow improves the consistency in feature level. However, can the feature level consistency ensure the RGB output consistency? \n- Correspondence ablation: Why using NN for the correspondences instead of using optical flow? Can pixel-level correspondences like dense optical flow be used for this token-based framework? Can we apply downsampled optical flow maps to the feature maps? \n- Occlusion: Is the current framework be able to handle some extreme cases, like occlusion? For example, a dog running through some poles but sometimes the dog is occluded by pole. \n- Video length: What is the maximal length TokenFlow can handle? \n- What is the post-procssing deflickering that is used in the SM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The proposed method might be used for human subject editing and could spread misinformation."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3344/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3344/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3344/Reviewer_Mqtb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3344/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698160064128,
            "cdate": 1698160064128,
            "tmdate": 1699636283992,
            "mdate": 1699636283992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PTtZFOIgwh",
                "forum": "lKK50q2MtV",
                "replyto": "5WkPpRLusH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3344/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the supportive feedback.  \n**High frequency patterns**  \nAs discussed in Sec 6 of our paper, the high frequency patterns in our results are attributed to the inherent nature of the latent diffusion model. Given that the latent space of the diffusion model is that of a VAE, which involves lossy compression, some high frequencies in the final RGB frames are hallucinated by its decoder. Such patterns are also evident when encoding-decoding a natural video without any further manipulations. We further discuss this limitation in Sec 6 of the paper.  \nDifferent image editing techniques  \nAs discussed in the paper (Sec 5), our method can be combined with different structure-preserving image editing methods. All the results in the paper are using PnP as the image editing method; See our SM for results with ControlNet and SDEdit.   \n**RGB propagation result**  \nFor the result of PnP editing + RGB propagation, please see our SM section \u201cAdditional Qualitative Comparisons\u201d under the column \u201cebsynth\u201d (the RGB propagation method); we are sorry if this title is misleading.  \n**Consistency in features and in RGB**  \nOur main premise in the paper is the correlation between feature and RGB consistency; our method shows that indeed, up to high frequencies that are hallucinated by the LDM decoder, feature consistency results in RGB consistency.   \n**Video length**  \nOn an A100 it is possible to run ~300 frames. We believe some optimisations can be made to increase the amount of frames (for example, computing the extended attention on random subsets of the keyframes rather than on all of them).   \n**Runtime**  \nPlease see our global response for runtime analysis.  \n**Optical flow**  \nSince our method performs propagation in the feature space, the swapped tokens should be ultimately interchangeable for the diffusion model. The most direct way to achieve this is through matching the features explicitly, rather than using subsampled pixel-level optical flow.  \n**Post-process in SM**  \nWe used [this script](https://github.com/OndrejTexler/Few-Shot-Patch-Based-Training/blob/master/train.py).  \n**Occlusions**  \nPlease see our global response."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969770940,
                "cdate": 1699969770940,
                "tmdate": 1699969770940,
                "mdate": 1699969770940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QXYqDQTlPd",
            "forum": "lKK50q2MtV",
            "replyto": "lKK50q2MtV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3344/Reviewer_6aWr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3344/Reviewer_6aWr"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to address the problem of consistent video editing. The proposed method is based on text-to-mage diffusion models, and the task is to convert a source video with a target text prompt into a new video that associates with the target text while preserving the motion of the source video. The emphasis is on producing consistent frames as naively applying an image-based text-to-image diffusion model would generate individually good-quality frames, but when they are put together, it would jointly result in an inconsistent video. The key idea proposed in this paper to solve the problem is called TokenFlow, which enforces the edited internal representation of the diffusion process to preserve the inter-frame correspondences of the original video. The approach is simple and the results shown in the paper and the supplementary material look quite good."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. \nThe proposed method is simple and lightweight. It is built on an existing diffusion-based image editing method and does not need to fine-tune the model. The \"TokenFLow Editing\" algorithm is easy to implement (code available in the supplementary material). It directly utilizes Stable Diffusion, DDIM inversion, and PnP-Diffusion, and the TokenFlow procedure just requires computing the nearest-neighbor fields for token feature maps. \nFurther, as mentioned in the summary in Sec. 1 of the paper, state-of-the-art editing results are one of the main contributions of this work. Indeed, the edited videos presented in the supplementary results exhibit better consistency than other methods' outputs.  \n\n2. \nA helpful finding from this work is that the internal features offer a shared and consistent representation across frames, and the corresponding features are interchangeable for the diffusion model. The spatial and semantic properties of diffusion features are also mentioned in the concurrent work \"Emergent Correspondence from Image Diffusion\" as DIFT, proposed by Tang et al.; however, they focus more on matching different images and show some results for edit propagation in image editing and for video label propagation on DAVIS and JHMDB instead of enforcing consistency in video editing. While it is not necessary to empirically compare TokenFlow with DIFT, it would be helpful to highlight the differences and the shared ideas.\nNevertheless, these findings of diffusion features provide a promising direction to revisit prior ideas like *Image Analogies* and *PatchMatch*."
                },
                "weaknesses": {
                    "value": "1. \nThe results in the paper and the supplementary material mainly demonstrate the visual effect of video style transfer. For more general video editing tasks, one might expect to see some results of motion-based or composition-based video editing. Since the proposed method relies on the feature correspondences in the original video, it seems not trivial if one would like to modify the TokenFlow for motion-based editing. \n\n2. \nRegarding the quantitative evaluation:\n- The *edit fidelity* measured by CLIP score does not provide useful/discriminative information. It might also need to include a user study on the visual quality and fidelity.\n- The *temporal consistency* measured by optical flow and warping might over-penalize edits that change in shape and tend to favor edits that involve only color/texture changes.\n\n\n3. \nMinor typos:  \n-  \"to operate on more **then** a single frame\"\n-  **keyframess'**\n- The first words after Eqs. (4) \\& (5) are in upper case: **Where**"
                },
                "questions": {
                    "value": "* What would happen if the target of editing is partially occluded for a few frames in the video?\n* The supplementary material shows some results of per-frame editing using ControlNet. Is it possible for ControlNet to be used not only for editing but also for providing optical flow guidance? If so, how would it differ from TokenFLow?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3344/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698508723423,
            "cdate": 1698508723423,
            "tmdate": 1699636283922,
            "mdate": 1699636283922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xhgaV6mHvh",
                "forum": "lKK50q2MtV",
                "replyto": "QXYqDQTlPd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3344/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the supportive feedback.  \n\n**Diffusion Features - ongoing and future research**  \nThank you for the insightful discussion and ideas.  \nThe concurrent work DIFT is related to ours by studying the intermediate representation learned by a pre-trained text-to-image model. DIFT focuses on identifying features that can serve as localized visual descriptors for general visual tasks such as segmentation and image correspondences.  We focus our analysis on the properties of the features across video frames (e.g., interchangeability w.r.t. the diffusion model), and harness our findings, during the generation process, to expand the capabilities of a text-to-image model to consistent video editing. Leveraging our findings to revisit classical patch-based methods is an intriguing future research direction. \n\n**Structure and motion deviation**  \nPlease see our common comment to all reviewers for a discussion on structure and motion deviations.\n\n**Metrics**  \nWe wish to note that (a) measuring edit fidelity using CLIP score shows that our method is on par in terms of editing capabilities w.r.t. the baselines and other methods; we do not claim to improve visual quality w.r.t. per-frame editing (e.g., PnP); we will make an honest effort to further evaluate visual quality with a user study in the revised version. (b) Ours and the competing methods (besides TAV) are designed to preserve the structure of objects, hence we find this metric valid.  Note that in addition to this metric,  the paper includes a thorough user study to evaluate temporal consistency. \n\n**Occlusions**  \nPlease see our global response. \n\n**ControlNet and optical flow**  \nWe would appreciate further clarification of this question by the reviewer."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969535911,
                "cdate": 1699969535911,
                "tmdate": 1699969535911,
                "mdate": 1699969535911,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IHOUetCg1P",
            "forum": "lKK50q2MtV",
            "replyto": "lKK50q2MtV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3344/Reviewer_vSnb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3344/Reviewer_vSnb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes TokenFlow for text-driven video editing, aiming to generate a temporally consistent video that adheres to the text prompt while preserving the spatial structure/motion of the source video. Specifically, TokenFlow leverages a pre-trained text-to-image diffusion model to extract features/tokens of each video frame, compute latent patch correspondence between neighboring frames, and temporally propagate the key-frame tokens to other frames during the diffusion process. Qualitative and quantitative results show that the TokenFlow performs similarly to prior methods in terms of edit fidelity (CLIP similarity) while achieving higher temporal consistency (warping error and user study)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1: Sensible model design\nAlthough the ideas of 1) using text-to-image diffusion model for video generation and 2) using latent feature flow for temporal consistency are not new, the proposed framework combines these components sensibly. The simplicity of this method also makes it compatible with existing video editing methods and more efficient than most prior arts.\n\nS2: Temporally consistent results\nThe visual results show a significant improvement from prior methods in terms of temporal consistency of both texture and structural details.\n\nS3: Good writing\nThe paper is well-written and easy to follow. I find the illustrations and algorithm pseudo-code quite helpful to understand the framework."
                },
                "weaknesses": {
                    "value": "W1: Novelty \nThe novelty of the proposed framework is slightly limited, considering that the key components (keyframe sampling, feature aggregation and propagation across frames) are introduced in prior works. Also, it is unclear which part of Section 4.1 is newly proposed in the paper and which is borrowed from other works. It would be great if the authors can elaborate on the main differences from prior methods and specify the novel components/modifications. \n\nW2: Limited structural deviation \nAs shown in Figure 7, TokenFlow outputs strictly follow the structural layout of the source video, which might limit its generative capability/application. I\u2019m wondering if there is a way to relax the temporal consistency constraint around object boundaries, so that one can find the desired tradeoff between temporal consistency and structural editing (maybe by tuning some hyper-parameters)."
                },
                "questions": {
                    "value": "Q1: The paper mentions that TokenFlow is more computationally efficient. What is the overall runtime to generate a new video and how is it compared to the methods listed in Table 1?\n\nQ2: The ablation study on keyframe sampling only covers fixed and random sampling. It would be good to also ablate on sampling interval (tradeoff between computation overhead and temporal smoothness). I\u2019m also curious if a dynamic keyframe sampling scheme would further improve the results, especially for occlusion cases."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3344/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3344/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3344/Reviewer_vSnb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3344/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711594638,
            "cdate": 1698711594638,
            "tmdate": 1699636283732,
            "mdate": 1699636283732,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5DgaGNIw1L",
                "forum": "lKK50q2MtV",
                "replyto": "IHOUetCg1P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3344/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the supportive feedback.  \n**Differences w.r.t prior work**  \n**To the best of our knowledge, we are the first to introduce a method that performs keyframe editing and propagation directly in the feature space of the model, during the generation process** (as pointed out by reviewers FeQk and Mqtb). Furthermore, our work provides new findings about diffusion features across video frames, supported by empirical analysis (as acknowledged by reviewers FeQk and 6aWr). On the technical side, our framework involves several key and novel components including: TokenFlow propagation \u2013 directly extracting and enforcing inter-frame correspondence in the feature space, and a dedicated generation process that alternates between keyframe editing and feature propagation.  \n\nThis is in contrast to prior work that either: (i) solely rely on extended attention (Sec. 4.1) , which is insufficient for temporal consistency, as we thoroughly demonstrated in Sec. 5, (e.g. [1], [2]) or (ii) edits keyframes relying on optical flow estimation between them, and propagates edits in RGB using an off-the-shelf propagation method (e.g., [3]); an approach which results in long range inconsistencies and artifacts (see our SM video comparisons), since optical flow estimation between distant frames (i.e. keyframes) is prone to errors.  \n\n**The effect of interval size**  \nWe thank the reviewer for the interesting suggestion. The interval size is equivalent to changing the number of keyframes. We note that as less keyframes are used in our framework: (i) runtime decreases (editing less keyframe is faster and requires less memory), and (ii) temporal consistency is improved (since the same tokens are shared across more frames). Nevertheless, too few keyframes will result in inaccurate correspondences which may result in artefacts. Following this comment, we evaluated the effect of the interval size numerically on a subset of videos of 96 frames:  \n\n| interval size | warp error (x 10^-3) | runtime |\n|---|---|---|\n| 4 | 2.3 | 1656 |\n| 8 | 2.2 | 656 |\n| 12 | 2.1 | 435 |\n| 16 | 2.0 | 359 |\n\n\n\nEmpirically, we found in all our experiments that an interval size of 8 is a robust choice, balancing runtime and quality of the result.\n\n**Structure deviation**  \nPlease see our global response. \n\n**Occlusions**  \nPlease see our global response.  \n\n[1]Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv:2303.09535, 2023.  \n\n[2] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. ArXiv, abs/2303.13439, 2023a.  \n\n[3]  Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation, 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969388675,
                "cdate": 1699969388675,
                "tmdate": 1699969388675,
                "mdate": 1699969388675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yPYl5Y2a0Y",
            "forum": "lKK50q2MtV",
            "replyto": "lKK50q2MtV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3344/Reviewer_FeQk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3344/Reviewer_FeQk"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a method for text-based video editing, called TokenFlow. TokenFlow utilizes a pre-trained text-to-image diffusion model without the need for finetuning or video training data. Independently using text-based image editing techniques on frames will produce temporal artifacts. The paper proposed a method to improve the temporal consistency. More specifically, the method uses extended attention to edit several keyframes and then propagates the keyframe features to all the frames based on a Nearest Neighbour field. The Nearest Neighbour field is computed based on features of DDIM inversion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The video editing results are impressive, the temporal consistency is pretty good.\n- The analysis and visualization of UNet features on video tasks are helpful for future research on video generation.\n- The idea of TokenFlow is novel. Based on the ablation study and qualitative results in the supplemental material, TokenFlow is also very critical to good temporal consistency. \n- The paper reads well and is easy to follow."
                },
                "weaknesses": {
                    "value": "Although it's not necessary, it will be helpful to compare TokenFlow with Pix2Video."
                },
                "questions": {
                    "value": "Are self-attention features the only features that are replaced by features of neighboring frames? Have you tried to replace some other features such as ResBlock features or attention masks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3344/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809160932,
            "cdate": 1698809160932,
            "tmdate": 1699636283653,
            "mdate": 1699636283653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E67MQ1qHYS",
                "forum": "lKK50q2MtV",
                "replyto": "yPYl5Y2a0Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3344/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3344/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the supportive feedback.  \n**Feature selection**  \nIt is indeed only the self attention tokens that are being replaced. We selected these features as they have been shown to capture fine-grained spatial information, and have been used for controlled image generation [1]. This is in contrast to the cross attention maps that capture only rough layout and hence would not be a good fit for our goal of fine-grained temporal correspondence [2].\nNote that the residual block is followed by a self attention block in every layer, thus the residual features are also overridden (up to skip connections) by our token replacement. \n\n\n[1] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023.  \n[2] Hertz, Amir, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. ICLR (2022).$"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3344/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969302922,
                "cdate": 1699969302922,
                "tmdate": 1699969302922,
                "mdate": 1699969302922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]