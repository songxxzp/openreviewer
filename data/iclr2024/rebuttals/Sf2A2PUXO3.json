[
    {
        "title": "Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation"
    },
    {
        "review": {
            "id": "BGVg9HH2dT",
            "forum": "Sf2A2PUXO3",
            "replyto": "Sf2A2PUXO3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper\u2019s main goal is to explore Rashomon sets of feed-forward neural networks with the help of dropout. Both Gaussian and Bernoulli dropouts are considered, the former involving the addition of noise to the weights of the networks. The approach is used to estimate the Rashomon set, and proof of the consistency of the approach is provided. Empirical results show the effectiveness of the approach on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea is intuitive. It leads to impressive computation time saving compared to other approaches from the literature. The article is well-written and clear. The experiments are directly in line with the motivations of the work (ethical concerns)."
                },
                "weaknesses": {
                    "value": "**Major**\n\n1.1 \u2013 The biggest weakness of the approach concerns its limitations. As honestly discussed by the authors in Section 6 \u2013 Limitations, the fact that when the hypothesis space to explore is huge (that is when the predictor has many parameters to tune), the exploration that is done by the dropout approach is fairly limited. This is clearly related to Proposition 4, where an important value of $M$ is necessary for layers having hundreds of neurons and an important value of $k$ is necessary with complex neural networks.\n\n1.2 \u2013 Two scenarios could occur: the first one is that the hypothesis space to explore is relatively small. That is, it is fairly explored by the dropout method. But, even though a 30x to 5000x speedup over other approaches is seen, it is never defended that those other approaches are not scalable with small hypothesis space. Even though there is a huge speedup gain, if the other approaches are relatively fast (do not take hours to compute), then why favourising dropout? The second scenario is that the hypothesis space to explore is large. The time gain is then undermined by the limitation in the exploration. Plus, when it comes to large models, it is common to retrain only the classification head of the predictor, or to fix many layers; doing so could really fasten the retraining scheme, thus undermining the potential advantage of the dropout approach.\n\n2 \u2013 It seems to me that both the depiction in Figure 2 and the speedup reported in Table 1 are lacking important details. For example: How many different models are sought? How many reruns were done for retraining VS how many dropouts were computed? What was the total time for each individual method? To my understanding, many reruns are already needed in the first place, no matter the approach, in order to ensure that the reference model is an \u00ab\u00a0empirical minimizer\u00a0\u00bb; was that taken into account when comparing the time for building the empirical Rashomon sets in Table 1? What was the size of the predictor used on these different UCI tasks (this kind of information is necessary in the main article, not the supplementary material)?\n\n3 \u2013 I feel like something is conceptually wrong with the comparison between retraining and the current dropout scheme. Retraining makes it such that the validation loss is the highest possible. Therefore, it makes sense that many runs are needed in order to find models close to the \u00ab\u00a0empirical minimizer\u00bb. With the dropout scheme, an empirical minimizer is found, and then dropout is applied while making sure the training loss does not diminish too much. The two approaches do not have the same objectives.\n\n4.1 - The dropout leads to a scheme where each new model depends on the initial model. All of the models are thus dependent. Therefore, the estimation of the Rashomon metrics is biased. And while \u00ab\u00a0not all estimators of predictive multiplicity metrics carry a theoretical analysis of its statistical properties such as consistency and sample complex\u00a0\u00bb, I feel like it is a property of interest. Indeed, one of the motivations of the work is the need for ethics and, more specifically, fairness. I see the goal in exploring the Rashomon sets to find many predictors giving different predictions to people for different reasons. But having all of the models interconnected makes it such that the reasons for the predictions are all linked and just a few are explored with the dropout scheme.\n\n4.2 \u2013 Proposition 5 aims at proving that the approach is not biased, but relies on the assumption that \u00ab\u00a0 the models around W\u2217 are uniformly distributed in a d-dimensional ball with center $\\mathbf{W}^*$ and radius $\\delta$, i.e., $B(\\mathbf{W}^*, \\delta)$. Accordingly, we may assume that the population means $\\mu$ for a sample can be expressed as [...]\u00a0\u00bb. The method explicitly does that (especially the Gaussian dropout), exploring around the \u00ab\u00a0population mean\u00a0\u00bb, that is, the empirical minimizer. Therefore, assuming the uniform distribution of the Rashomon set around a center trivially leads to the unbiasedness of the dropout scheme, but is unreasonable.\n\n**Minor**\n\n1 \u2013 Typo: \u00ab\u00a0Moreover, as lone as\u00a0\u00bb"
                },
                "questions": {
                    "value": "1 \u2013 It is said that \u00ab\u00a0not all estimators of predictive multiplicity metrics carry a theoretical analysis of its statistical properties such as consistency and sample complex\u00a0\u00bb Could you provide some citation supporting this claim?\n\n2 \u2013 What justifies fixing a single Bernoulli or a Gaussian dropout parameter for all layers simultaneously? Shouldn\u2019t the layers be treated independently?\n\n3 \u2013 Concerning the quantification of predictive multiplicity, it is said that \u00ab\u00a0[f]or example, Long et al. [2023], Cooper et al. [2023] and Watson-Daniels et al. [2023] quantify predictive multiplicity by the standard deviation, variance and the largest possible difference of the scores (termed viable prediction range (VPR) therein) respectively\u00a0\u00bb So, what definition between those three is retained in the article?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Other reasons (please specify below)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This might not be of an \"ethical\" concern, but the 9-page limit is exceeded by  1/4th ~ 1/3rd of a page."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698328653953,
            "cdate": 1698328653953,
            "tmdate": 1699636232099,
            "mdate": 1699636232099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TNxmXj1ZSD",
                "forum": "Sf2A2PUXO3",
                "replyto": "BGVg9HH2dT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2885/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2885/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BakJ"
                    },
                    "comment": {
                        "value": "1. **The biggest weakness of the approach concerns its limitations. As honestly discussed by the authors in Section 6 \u2013 Limitations, the fact that when the hypothesis space to explore is huge (that is when the predictor has many parameters to tune), the exploration that is done by the dropout approach is fairly limited. This is clearly related to Proposition 4, where an important value of M is necessary for layers having hundreds of neurons and an important value of k is necessary with complex neural networks.**\n    \n    Indeed, when the hypothesis space is huge, exploring models in the Rashomon set, i.e., obtaining almost-equally-optimal models from the hypothesis space, is in general computationally infeasible. Despite dropout-based exploration is limited, it makes the exploration possible compared to other strategies such as re-training and AWP, which are much harder computational tasks as they requires repeated re-training and perturbation. We thank the reviewer for bringing this up, and will add a more thorough comparisons of the pros and cons among the three strategies, re-training, AWP, and the proposed one in Section 3 and 4. \n    \n2. **Two scenarios could occur: the first one is that the hypothesis space to explore is relatively small. That is, it is fairly explored by the dropout method. But, even though a 30x to 5000x speedup over other approaches is seen, it is never defended that those other approaches are not scalable with small hypothesis space. Even though there is a huge speedup gain, if the other approaches are relatively fast (do not take hours to compute), then why favourising dropout? The second scenario is that the hypothesis space to explore is large. The time gain is then undermined by the limitation in the exploration. Plus, when it comes to large models, it is common to retrain only the classification head of the predictor, or to fix many layers; doing so could really fasten the retraining scheme, thus undermining the potential advantage of the dropout approach.**\n    \n    We thank the reviewer for bringing up this practical concern. When the hypothesis space is relatively small, the re-training and AWP strategies could be fast; however, the dropout method could still be faster (despite that the speedup could be smaller) than re-training and AWP for the same reason\u2014it does not require gradient computation and weight updating. Moreover, whether the proposed dropout method could explore the Rashomon set as effective as re-training or AWP when the hypothesis space is mall, is an interesting theoretical question and also requires numerical evidence.  \n    \n    The second scenario is quite a common practice when fine-tuning a pre-trained model. Indeed, re-training and AWP could be significantly faster when only applied on part of the weights (e.g., the classification head). In this sense, the dropout method could also be applied on part of the weights in order to accelerate. The comparison of runtime performance among re-training, AWP, and the dropout method when applied on partial weights (also briefly discussed in Section 6 Future Directions.) is an interesting and practical research direction. Due to needlessness of updating weights, the proposed dropout strategy could still be more computationally efficient with the speedup varying depending on different complexity of the hypothesis space.\n    Finally, as we have discussed in Section 6 Future Direction, the proposed dropout method could also be applied as a compliment to re-training, in order to leverage between efficiency and effectiveness of Rashomon set exploration. We will address this concern by adding a paragraph in Section 4 or 5 to further discuss the impact of the cardinality of the hypothesis space with some numerical examples.\n\n**Continued...**"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109202396,
                "cdate": 1700109202396,
                "tmdate": 1700109202396,
                "mdate": 1700109202396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9IFEzkzU1P",
                "forum": "Sf2A2PUXO3",
                "replyto": "BGVg9HH2dT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their exhaustive response.\n\nI don't feel like the responses (#1, #2, #4, #5, #6) actually answer my concern. For example, having literature comparing their approach to retraining (#4) might not be relevant here, for it depends on how these approaches resemble or not retraining on the point I raised in Weakness - Major - 3. Another example would be, response #5: sure, \"[the goal of this paper is to] provides an alternative method to estimate predictive multiplicity metrics more efficiently\", but in the article, what motivates its estimation is that \"when predictive multiplicity is left out of account, an arbitrary choice of a single model in the Rashomon set may lead to systemic exclusion from critical opportunities, unexplainable discrimination, and unfairness, to individuals\". Finally, response 6: even though the mass is concentrated around $\\sqrt{d}$, it is uniformely distributed on each point at a given distance of the mean.\n\nResponse #3 did help me understand how a single empirical minimizer was shared by the approaches. I do feel like some of the details in Apprendix (some of the points I raised) deserve to be in the main part of the paper and not the appendices and am glad to know that some of that information will be transfered to the main part.\n\nI thank again the authors for their honest responses; I will keep my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700439072373,
                "cdate": 1700439072373,
                "tmdate": 1700439100304,
                "mdate": 1700439100304,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pvWddwzEwP",
                "forum": "Sf2A2PUXO3",
                "replyto": "TjlpF2vGFA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_BakJ"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewers for their response. I stand by my score and will be defending acceptance."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668180338,
                "cdate": 1700668180338,
                "tmdate": 1700668180338,
                "mdate": 1700668180338,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jG7KzS4g5I",
            "forum": "Sf2A2PUXO3",
            "replyto": "Sf2A2PUXO3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2885/Reviewer_kvbj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2885/Reviewer_kvbj"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the possibility of using Dropout to explore the Rashomon set. It proves that for a FFNN, we could bound the probability that a Dropout realization is in a certain Rashomon set. In experiments it shows that the proposed the method does not explore the Rashomon set as effectively as AWP (as measured by several predictive multiplicity metrics), but is much faster as it does not retrain any model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper establishes some theoretical bounds (although seemingly loose) on the probability that FFNNs with Dropout are in the corresponding Rashomon Set.\n2. The proposes method is easy to implement."
                },
                "weaknesses": {
                    "value": "1. It is unclear what's the practical use of the propose method. It is fast, but it does not explore the Rashomon set well. For this reason, we can only mitigate predictive multiplicity *as estimated by Dropout* but not in general. \n2. Following 1, it seems like additional experiments on whether the mitigation via Dropout also transfers to, say, AWP, is interesting. \n3. The bounds in Proposition 2 and 3 only converge to 1 when $d\\to\\infty$, which does not seem like useful. See Q3 as well.\n4. It is not clear why a concentration bound helps. Notably, in applications, we want the models that are in the Rashomon set but closer to the boundary. In fact, it seems like in practice we need to sample a few weights and empirically verify that they have low loss (?). If so, a concentrated distribution, especially one that's more concentrated when the dim of the model increases, seems like a bad feature. A method that samples very diverse model that potentially has a higher probability of falling outside the Rashomon set seems more desirable."
                },
                "questions": {
                    "value": "1. AWP is slower due to re-training, but the models are trained only once. Therefore, doesn't it run *faster* than Dropout (because it uses fewer samples/models to explore the Rashomon set) with a reasonably large test data?\n2. What does \"5 models\" mean in Figure 4b? 5 different base weights, or 5 different architectures?\n3. Is $\\epsilon$ and the $L$ in Eq.(5), (7) and (10) related to the \"sum\" of loss or the \"mean\" of loss? It seems like it's the sum? If so, by changing $\\epsilon$ to some offset on the mean loss, we can probably get a convergence basing on the sample size, which is much more meaningful than dimension of the model's hidden layers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Reviewer_kvbj"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698354370056,
            "cdate": 1698354370056,
            "tmdate": 1699636232011,
            "mdate": 1699636232011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dfzddDw4vR",
                "forum": "Sf2A2PUXO3",
                "replyto": "jG7KzS4g5I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2885/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2885/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kvbj"
                    },
                    "comment": {
                        "value": "1. **It is unclear what's the practical use of the propose method. It is fast, but it does not explore the Rashomon set well. For this reason, we can only mitigate predictive multiplicity *as estimated by Dropout* but not in general.**\n    \n2. **Following 1, it seems like additional experiments on whether the mitigation via Dropout also transfers to, say, AWP, is interesting.**\n    \n    We address the reviewer's concerns in Weaknesses 1 and Weaknesses 2 together here. The *good* exploration of the Rashomon set, i.e., obtaining very *diverse* models with almost-equally-optimal performance, comes with the cost of efficiency (cf. e.g. [Xin et al., 2023], [Watson-Daniels et al., 2023]). \n    With this in mind, the practical use of the proposed dropout method is to provide an alternative to the current re-training strategy that is not only more efficient but also bears with theoretical analysis (Proposition 1 to Proposition 4).\n    Numerical results, e.g., in Figure 2, also show that the proposed method is comparable or outperforms the re-training strategy in terms of exploring the Rashomon set. \n    Note that both re-training and the dropout methods give a subset (defined in (2)) of the true Rashomon set, and will all lead to an *under-estimate* of predictive multiplicity metrics. Therefore, higher estimates of the multiplicity metrics indicate a better exploration of the Rashomon set. \n    To the best of our knowledge, it is the first time when the dropout method is connected and analyzed with notions of the Rashomon set and predictive multiplicity, making reporting predictive multiplicity for large-scale models possible. \n    \n    Moreover, the methods to mitigate predictive multiplicity discussed in Section 5 are not limited to the dropout methods. In fact, the ensemble method shown in Figure 4(a) work for any strategies that could obtain models in the Rashomon set. Since more models in the the ensemble leads to smaller multiplicity metrics, the proposed dropout method serves as an efficient way to obtain a huge amount of models. \n    \n    Mitigating predictive multiplicity, especially when selecting a model with the least predictive multiplicity from several given models, as shown in Figure 4(b), could also be achieved by the AWP. We will include an additional experiment in the Appendix of future revision. \n    Lastly, please note that the re-training strategy can be combined with the dropout method to order to leverage the efficiency and efficacy of Rashomon set exploration, as already discussed in Section 6 Future Directions. We will summarize the discussion above and add a paragraph accordingly in Section 2. \n    \n3. **The bounds in Proposition 2 and 3 only converge to 1 when $d\\to\\infty$, which does not seem like useful. See Q3 as well.**\n\n    We thank the reviewer for bring this up, and will add explanations and citations to support this assumption. \n    The condition $d\\to\\infty$ is a regime commonly discussed in the asymptotic analysis of learning models such as over-parameterization [1], and the universal approximator theorem [2]. We would like to emphasize that the asymptotic behavior when $d\\to\\infty$ is based on the bound we derived in Eq. (5).\n\n    [1] Cao, Y. and Gu, Q., 2020, April. Generalization error bounds of gradient descent for learning over-parameterized deep relu networks. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 04, pp. 3349-3356).\n    \n    [2] Hornik, K., Stinchcombe, M. and White, H., 1989. Multilayer feedforward networks are universal approximators. Neural networks, 2(5), pp.359-366.\n\n**Continued...**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108958360,
                "cdate": 1700108958360,
                "tmdate": 1700108958360,
                "mdate": 1700108958360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tGsXt2gnyM",
            "forum": "Sf2A2PUXO3",
            "replyto": "Sf2A2PUXO3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2885/Reviewer_pYox"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2885/Reviewer_pYox"
            ],
            "content": {
                "summary": {
                    "value": "The paper described a Rashomon set exploration method through Drop-out with probabilistic bound. The paper starts with fairly well-covered literature of Rashomon set research and motivates its proposal by pointing out the computation cost of existing empirical solution (re-training, AWP). The solution is fairly simple by adopting Drop-out where Rashomon set likely rests. Probably the most significant part of this paper (theoretically) would be pointing out the probabilistic bound of Rashomon set under Drop-out. Empirical results show the proposed method is computationally efficient than previous solutions and even showing better divergence metric than retraining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Predictive multiplicity itself is an interesting topic that worths more investigation. The method proposed in this work is a great complement of existing literature in this field.\n2. The paper is well written and motivated. It comes with sufficient background knowledge to understand the gap in the literature.\n3. Potential application of this approach is covered in Section 5, which is good since I was concerning where people can use this innovation in their work."
                },
                "weaknesses": {
                    "value": "1. Model augmented by Dropout could result in a fairly small search space of Rashomon set. I am not very convinced that this is a good idea in practice if our goal is to look for a better model that can address various reliability problem of predictive model. e.g. fairness etc. It maybe inspirational to see the movement of predictive multiplicity measurement, but I am wondering what is the practical meaning of it.\n2. The paper demonstrates the effectiveness of the proposed method on toy datasets that were used for decades. As the paper concerns the efficiency of existing methods, I am wondering if the authors can introduce more realistic tasks to show the effectiveness of the proposed method quantitatively.  While COCO is good example, it is very qualitative without much statistic support. \n3. There is a descriptive gap in section 3.2 where transforming deviation between $L_{SSE}(\\mathbf{w}_D^*)$ and $L_{SSE}(\\mathbf{w})$ suddenly become  deviation between  $L_{SSE}(\\mathbf{w}_D^*)$ and $L_{SSE}(\\mathbf{w}')$. I don't quite see why they are aligned or if the model works correctly under $L_{SSE}(\\mathbf{w}')$ if it is not trained with such dropout rate."
                },
                "questions": {
                    "value": "The proposition 1 uses deviation between  $L_{SSE}(\\mathbf{w}_D^*)$ and $L_{SSE}(\\mathbf{w}')$ but not original model parameter $L_{SSE}(\\mathbf{w}')$. How to make the connection ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769430926,
            "cdate": 1698769430926,
            "tmdate": 1699636231930,
            "mdate": 1699636231930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BXWYidUKKz",
                "forum": "Sf2A2PUXO3",
                "replyto": "tGsXt2gnyM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2885/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2885/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pYox"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback and encouragement! We address the questions point-by-point below.\n\n1. **Model augmented by Dropout could result in a fairly small search space of Rashomon set. I am not very convinced that this is a good idea in practice if our goal is to look for a better model that can address various reliability problem of predictive model. e.g. fairness etc. It maybe inspirational to see the movement of predictive multiplicity measurement, but I am wondering what is the practical meaning of it.**\n    \n    We thank the reviewer to point out this confusion. Our goal is not finding models in the Rashomon set that comply with additional reliability problems such as fairness, but to efficiently estimate predictive multiplicity, despite that both involve exploring the Rashomon set, i.e., obtaining almost-equally-optimal models from the hypothesis space. As stated in the second paragraph in Section 1, our focus is estimating predictive multiplicity, a problem induced from the Rashomon effect, not the Rashomon effect stated in the first paragraph of Section 1. We agree with the reviewer that studying if the dropout models in the Rashomon set could lead to other reliability properties such as fairness is an interesting future direction. \n\n    There is a trade-off between the efficiency and efficacy of exploring the Rashomon set and prediction multiplicity estimation. Note that re-training and our proposed dropout method all aim to approximate a subset of the entire (true) Rashomon set, and thus all estimates based on re-training or our method are under-estimation of the true multiplicity metrics. Despite that our method may only explore a small subset in the Rashomon set, it give an lower bound of the multiplicity metrics in an efficient manner, as clearly shown in Table 1. Moreover, the estimation using the dropout method also gives results comparable to re-training and AWP, as shown in the empirical evidence in Figure 2. Therefore, the proposed dropout strategy serves as yet another option (besides AWP and re-training) to explore the Rashomon set depending on the need of practitioners. \n    \n    We will summarize the discussion above and add it to the final revision. \n    \n2. **The paper demonstrates the effectiveness of the proposed method on toy datasets that were used for decades. As the paper concerns the efficiency of existing methods, I am wondering if the authors can introduce more realistic tasks to show the effectiveness of the proposed method quantitatively. While COCO is good example, it is very qualitative without much statistic support.**\n    \n    The datasets we used for evaluation throughout this paper includes 6 UCI datasets, CIFAR-10/-100, and the MS COCO dataset, ranging from small tabular data to large-scale image datasets. Indeed, the UCI datasets are small and have been used for decades; however, they are still widely used in predictive multiplicity literature, especially in the early stage of this new field. For example, the UCI Adult dataset is used in [Long et al., 2023] and [Kulynch et al., 2023]; the mammography dataset is used in [Watson-Daniels et al., 2023]; and other UCI datasets are also adopted in [Xin et al., 2022]. Note that all these literatures are very recent.\n    \n    Aside from the UCI datasets, we also evaluate the proposed method on image datasets such as CIFAR-10/-100 and the MS COCO, where the corresponding quantitative metrics including loss/accuracy changes and predictive multiplicity metrics are all reported in Sections E.2 and E.3, and also referred in the main text. We believe the datasets we used here are on par or more abundant than existing literature. We will re-emphasize the reasons (as above) why we select these datasets in the final version of the paper. \n    \n\n**Continued...**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108833343,
                "cdate": 1700108833343,
                "tmdate": 1700108833343,
                "mdate": 1700108833343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dtx9QmQkwL",
                "forum": "Sf2A2PUXO3",
                "replyto": "LUfP61oT2N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_pYox"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_pYox"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I will keep my current score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659346413,
                "cdate": 1700659346413,
                "tmdate": 1700659346413,
                "mdate": 1700659346413,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Lri9gAkvBT",
            "forum": "Sf2A2PUXO3",
            "replyto": "Sf2A2PUXO3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2885/Reviewer_7265"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2885/Reviewer_7265"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of how to measure and mitigate predictive multiplicity.\nTo achieve them, the authors utilize the dropout technique to explore the models in the Rashomon set.\nRigorous theoretical analysis is provided to connect dropout and Rashomon set.\nNumerical results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method is simple, straightforward, and well-motivated. Utilizing the dropout technique to explore the models in the Rashomon set is interesting.\n2. Rigorous theoretical analysis is provided to connect dropout and Rashomon set.\n3. The paper is well-written and well-organized. The authors first show the implementations on linear models and extend them to feedforward neural networks. \n4. The limitations and potential solutions are also discussed in the paper."
                },
                "weaknesses": {
                    "value": "1. In the experiments, the authors mentioned that \"On the other hand, AWP outperforms both dropouts and re-training, since it adversarially searches the models that mostly flip the decisions toward all possible classes for each sample.\" I may miss some details of the method part, how can the proposed method to adversarially search the models since the dropout is random?\n2. As mentioned by the authors, good performance comes at the cost of efficiency."
                },
                "questions": {
                    "value": "1. It seems the proposed method is only evaluated on in-distribution scenarios. Can it be applied to out-of-distribution data?\n2. Are the uncertainty scores calibrated? In other words, are the confidence scores reliable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2885/Reviewer_7265"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698931973849,
            "cdate": 1698931973849,
            "tmdate": 1699636231810,
            "mdate": 1699636231810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EfkSSPrJGO",
                "forum": "Sf2A2PUXO3",
                "replyto": "Lri9gAkvBT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2885/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2885/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7265"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback and encouragement! We address the questions point-by-point below.\n\n1. **In the experiments, the authors mentioned that \"On the other hand, AWP outperforms both dropouts and re-training, since it adversarially searches the models that mostly flip the decisions toward all possible classes for each sample.\" I may miss some details of the method part, how can the proposed method to adversarially search the models since the dropout is random?**\n    \n    We thank the reviewer for pointing out this confusion. We would like to further clarify that AWP and re-training  are two existing baselines we compared against with our proposed dropout method. The AWP, re-training and ours are three different strategies to explore the Rashomon set, i.e., obtaining almost-equally-optimal models from the hypothesis space, and ours and AWP will not be applied at the same time. The AWP algorithm, as proposed in [Eq. 9, Hsu \\& Calmon, 2022], will adversarially perturb the output scores of a given sample to each class. For example, if there are $c$ classes, AWP maximizes each entries of $h\\_w(x)$, i.e., $[h\\_w(x)]\\_i$ for each $i \\in [c]$, under the constraint that the model after perturbation is still inside the Rashomon set. Therefore, for one sample, AWP requires to perform one perturbations (adversarial training) via SGD for each class (i.e., $c$ adversarial training in total for one sample), and if there is a dataset with $n$ samples, we need $n\\times c$ adversarial training to estimate multiplicity metrics for the entire dataset. We can imagine that each of these $n\\times c$ perturbations leads to a model in the Rashomon set that outputs the most inconsistent score for a given class per sample. On the other hand, the dropout method does not require to iterate over all samples and does not require gradient computations and updating model weights. In this sense, AWP is very different from the proposed dropout strategy.\n    \n    Due to space limit, we refer the readers to [Section 4, Hsu \\& Calmon, 2022] for more details of the AWP algorithm in the initial draft. We will add pseudo codes for the AWP in Appendix B, and a brief discussion in Section 2 to clarify the difference of the three strategies in the final version of the paper. \n\n2. **It seems the proposed method is only evaluated on in-distribution scenarios. Can it be applied to out-of-distribution data?**\n    \n    We thank the reviewer for bringing this up. Indeed, the loss, accuracy and predictive multiplicity metrics reported in this paper are all evaluated on test data, i.e., unseen but in-distribution samples. The proposed method can also be applied for out-of-distribution (OOD) data. However, without special treatment (e.g., OOD generalization), the pre-trained model would have much higher loss $L(h_{w*}, \\mathcal{D}\\_{OOD})$ when fed with OOD data, leading to a larger Rashomon set (with the same $\\epsilon$) and potentially much higher values of predictive multiplicity metrics (since multiplicity metrics are optimized over the Rashomon set; please also refer to [Defn 1, Hsu et al., 2022]). We will add a brief paragraph to include the discussion above in Section 3.\n\n3. **Are the uncertainty scores calibrated? In other words, are the confidence scores reliable?**\n    \n    Thanks for the great question! We would like to further clarify that if a perfectly calibrated classifier assigns a 50\\% score to a sample (e.g., in binary classification), it does not necessarily mean that this sample has high multiplicity. A perfectly calibrated classifier is one whose predicted classes matches the true classes **on average** across samples (e.g., samples predicted to be 50\\% of one class have true outcomes matching that class 50\\% of the time). However, this does not necessarily translate to a (in)consistent set of predictions for a **single target sample** across equally calibrated classifiers. It may be the case that **all** calibrated models drawn from the Rashomon set assign the same 50\\% probability for that sample (no multiplicity). Conversely, some models may assign higher and lower confidence for that sample (high multiplicity) yet, on average, still be well-calibrated. Again, this happens because calibration (like accuracy and loss) is an average metric across all samples. We will add a brief discussion to include the response above in Section 4 in the final revision.\n\nThanks again for the review! We would be happy to provide more clarifications and answer any follow-up questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108672565,
                "cdate": 1700108672565,
                "tmdate": 1700108672565,
                "mdate": 1700108672565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zO1RpVEEt6",
                "forum": "Sf2A2PUXO3",
                "replyto": "EfkSSPrJGO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_7265"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2885/Reviewer_7265"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thank the authors for the detailed response. The answer to Q1 helps me correct my understanding of AWP. I admit that the question about OOD generalization is a little bit beyond the scope of this paper. I prefer to keep my score and vote for acceptance."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720149563,
                "cdate": 1700720149563,
                "tmdate": 1700720149563,
                "mdate": 1700720149563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]