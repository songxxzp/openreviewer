[
    {
        "title": "Rethinking the Smoothness of Node Features Learned by Graph Convolutional Networks"
    },
    {
        "review": {
            "id": "Q0VghigzMW",
            "forum": "tUoBaW8KH1",
            "replyto": "tUoBaW8KH1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4062/Reviewer_pkn8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4062/Reviewer_pkn8"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the impact of ReLU and LeakyReLU nonlinearities on the smoothness of node features and proposes a method for regulating feature smoothness under a normalized smoothness metric. While earlier theories suggested that these nonlinearities always result in smoother features, this study reveals that, when considering feature magnitude and applying a normalized smoothness metric, ReLU and LeakyReLU can actually increase, decrease, or maintain the smoothness metric. Notably, by adjusting the input's projection in certain eigenspaces, one can manipulate the output's smoothness to achieve a desired level. The paper introduces a technique known as the \"smoothness control term\" (SCT), which is designed to regulate node feature smoothness, and it is experimentally tested to validate its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Understanding the effect of nonlinearities is an important question.\n2. The empirical performance of SCT looks promising in Table 2."
                },
                "weaknesses": {
                    "value": "1. Although seemingly making sense, \u201cnormalized smoothness\u201d, as measured by s defined in eq.(4) is not valid for interpretation. It disconnects node smoothness from model performance, making it carry no insights into practice and thus it is meaningless to study. \n\n    For instance, consider a graph with two classes of nodes: one class having feature values of 1, and the other class with feature values of -1. In this case, a linear classifier would have perfect classification performance.  If we consistently add one to each node's feature, the differences among node features would not change, and if we apply a classifier again to classify based on the new features, the performance would not change either---We are basically shifting all the node features by the same value and the bias term of a classifier can easily accommodate that. Such a phenomenon is well justified by the unnormalized metrics such as conventional Dirichlet energy because it would remain the same before and after we add the same value to each node. However, the normalized smoothness metric s proposed in this paper would get larger and larger, indicating that the node features are getting \"smoother\" and \"smoother\".\n\n    Given the above concern and the established research on the effects of ReLU and LeakyReLU under unnormalized smoothness [5, 27] (citations provided by the paper), this paper provides very little new theoretical insight. \n\n\n\n\n2. I also checked [5] (citation provided by the paper), and I didn\u2019t see any serious evidence, either theoretical or empirical, supporting the following highlighted claim in this paper:\n\n> [5] points out that over-smoothing \u2013 measured by the distance of node features to the eigenspace M or the Dirichlet energy \u2013 is a misnomer, and the real smoothness of a graph signal should be characterized by a normalized smoothness, e.g., normalizing the Dirichlet energy by the magnitude of the features. \n\nThe only related sentence I saw was\n\n> Finally, analyzing the real over-smoothing effect, i.e., the Rayleigh quotient $\\frac{tr(X^T \\tilde{\\Delta} X)}{||X||^2_2}$ for\ndeep GNNs is still an open and important question.\n\n But this itself doesn't justify the validity of the normalized smoothness. \n\n3.  The improvement over stronger baselines (GCNII and EGNN) in Table 1 is limited in most cases, which raises doubts about the overall effectiveness of SCT."
                },
                "questions": {
                    "value": "Could the authors provide standard deviations for the experimental results in Table 1 (particularly for the baseline methods) and Table 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4062/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4062/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4062/Reviewer_pkn8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4062/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634287359,
            "cdate": 1698634287359,
            "tmdate": 1699636370530,
            "mdate": 1699636370530,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QJN32GDjjt",
                "forum": "tUoBaW8KH1",
                "replyto": "Q0VghigzMW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pkn8 (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the review. However, **we strongly disagree with your highly unethical and insulting comments, which falsify the truth**. We want to stress that respecting truth is the ethics of being a reviewer. Criticizing another's paper by falsifying the truth is scientific misconduct. It is a severe offense that can damage the reputation of other scientists and undermine public trust in science.\n\n Before addressing your comments, we would like to point out that: \n\n- Your criticism that \u201cstudying the normalized smoothness of GCN features is meaningless\u201d is rootless, unethical, and insulting.\nIn particular, studying an existing smoothness notion and an existing important open problem is not a weakness.\n\n- ***Evidence for studying normalized smoothness:*** The correlation between the relative magnitude $||{\\bf z}\\_{\\mathcal{M}^\\perp}||/||{\\bf z}\\_\\mathcal{M}||$ -- a quantity that is closely related to the normalized smoothness -- and classification accuracy was first empirically studied in Figure 4 by Oono & Suzuki (ICLR 2020). Moreover, Cai & Wang (arXiv:2006.13318) pointed out that studying normalized smoothness is an open and important problem. Again, ***we believe that studying an open and important problem, pointed out in the pioneering paper, as part of our paper is not a weakness.***\n\n---\n\nBelow, we address your comments in detail.\n\n---\n\n**Studying the normalized smoothness is a weakness**\n\n**Reply:** ***We respectfully but strongly disagree that studying an existing smoothness notion and an existing open problem as part of our work is a weakness.*** \n\nThe importance of studying normalized smoothness is supported by evidence. ***Empirical evidence:*** The correlation between the relative magnitude $||{\\bf z}\\_{\\mathcal{M}^\\perp}||/||{\\bf z}\\_\\mathcal{M}||$ -- a quantity that is closely related to the normalized smoothness -- and classification accuracy was first empirically studied in Figure 4 by Oono & Suzuki (ICLR 2020). While they demonstrate a strong correlation between classification accuracy and $||{\\bf z}\\_{\\mathcal{M}^\\perp}||/||{\\bf z}\\_{\\mathcal{M}}||$, their results neither refute nor conclusively establish that performance improves with an increasing $||{\\bf z}\\_{\\mathcal{M}^\\perp}||/||{\\bf z}\\_\\mathcal{M}||$. Despite this, we share the common belief that the ratio $||{\\bf z}\\_{\\mathcal{M}^\\perp}||/||{\\bf z}\\_\\mathcal{M}||$ or the normalized smoothness $||{\\bf z}\\_\\mathcal{M}||/||{\\bf z}||$ correlates with performance. In contrast to intentionally making features unsmooth, our proposed smoothness control term aims to enable the model to control the smoothness of node features automatically. ***Evidence:*** As the reviewer mentioned, the paper [5] states that **\u201cFinally, analyzing the real over-smoothing effect, i.e., the Rayleigh quotient $\\frac{tr(X^T\\tilde{\\Delta}X)}{|\\|X||_2^2}$ for deep GNNs is still an open and important question.\u201d** The Rayleigh quotient is the normalized smoothness. \n\nIn the following three paragraphs, we would like to clarify three substantial mistakes in your review:\n\nFirst, ***In the footnote of the first page of the paper [5]***, Cai & Wang state that \u201cStrictly speaking, over-smoothing is a misnomer. As we will show, what is decreasing is $tr(X^T\\tilde{\\Delta}X)$, not the real smoothness $\\frac{tr(X^T\\tilde{\\Delta}X)}{|\\|X||_2^2}$ of graph signal $X$.\u201d Since the distance of node features to the eigenspace $\\mathcal{M}$ and the Dirichlet energy are two equivalent seminorms, we rephrase that \u201c[5] points out that over-smoothing -- measured by the distance of node features to the eigenspace $\\mathcal{M}$ or the Dirichlet energy -- is a misnomer, and the real smoothness of a graph signal should be characterized by a normalized smoothness, e.g., normalizing the Dirichlet energy by the magnitude of the features.\u201d"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068019084,
                "cdate": 1700068019084,
                "tmdate": 1700090624881,
                "mdate": 1700090624881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0kOmr8JCFg",
                "forum": "tUoBaW8KH1",
                "replyto": "Q0VghigzMW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pkn8 (2/3)"
                    },
                    "comment": {
                        "value": "Second, ***the normalized smoothness is not a notion we proposed, which is one of the two existing smoothness notions for characterizing the smoothness of GCN features*** We disagree with the statement that \u201cnormalized smoothness disconnects node smoothness from model performance, making it carry no insights into practice and thus it is meaningless to study.\u201d On the one hand, ***your statement that normalized smoothness is meaningless to study is a falsification*** and counters to the empirical study in Oono & Suzuki (ICLR 2020) and the paper [5]\u2019s statement that it is an open and important problem. On the other hand, ***the example you provided is about linear classifier (we will comment on it later), which is irrelevant to GCN.*** The GCN layer is given by $H^l=\\sigma(W^lH^{l-1}G)$ and the last layer\u2019s feature $H^L$ is directly passed to log\\_softmax to get the prediction softmax score and there is no linear layer; see https://github.com/tkipf/pygcn/blob/master/pygcn/models.py. Notice that there is no bias term in the GCN layer. Even after adding a bias term to make a new layer $H^l=\\sigma(W^lH^{l-1}G+B)$, how can you learn a bias term, start from random initialization, to easily accommodate the shift in the features -- say $H^{l-1}$ -- while taking into account the multiplication by $G$?\n\nThird, we disagree with your comment that the paper provides very little new theoretical insight. ***Our study both unnormalized and normalized smoothness.*** The paper [5,27] shows that over-smoothing, measured by unnormalized smoothness, is inevitable for GCNs. The results in [5,27] do not imply an approach to control the smoothness of node features. Compared to [5,27], our paper provides important new insights ***both theoretically and practically***: 1) In Section 3, we establish ***a geometric characterization*** of how ReLU and Leaky ReLU affect the smoothness of node features, showing that ***the smoothness of node features are controllable*** by adjusting the projection of node features onto the eigenspace $\\mathcal{M}$, which informs the practical smoothness control term. 2) In Section 4, we show that adjusting the projection of node features onto the eigenspace $\\mathcal{M}$ results in ***disparate effects on the normalized and unnormalized smoothness***. 3) We study an open problem proposed in [5], which was considered to be important by the authors of [5].\n\n\n---\n\n**Normalized smoothness lacks interpretability of the classification results of the linear classifier.**\n\n**Reply:** Though irrelevant to our work, we would like to comment on your comment that normalized smoothness lacks interpretability of the classification results of the linear classifier, say $softmax(Wx+b)$. If the features of the training set are shifted, the bias $b$ can accommodate it during training. However, notice that the linear classifier is first trained and then applied to new data. Once the model is trained, we will have a fixed set of weights $W$ and $b$. For fixed $W$ and $b$, $softmax(Wx+b)$ is changed when $x$ is shifted. ***We are not changing $b$ for every new $x$.***\n\n---\n\n**The improvement over stronger baselines (GCNII and EGNN) in Table 1 is limited in most cases. Could the authors provide standard deviations for the experimental results in Table 1 (particularly for the baseline methods) and Table 2?**\n\n**Reply:**\n***Standard deviation:*** In our submission, the standard deviations of the models with SCT have been reported when they exist under the benchmark experimental setting. The standard deviation only exists for the experiments of Coauthor-Physics and Ogbn-arxiv in Table 1 and exists for all experiments in Table 2. We stress the benchmark experimental setting here again that we use the fixed public split for the Citation datasets and the particular splits from reference [37] in the revision in the Coauthor-Physics and Ogbn-arxiv datasets. For the baseline models without SCT, the standard deviation results are not reported in the benchmark papers [7,42] in the revision. The code released by the authors of the benchmark papers does not include the hyperparameters used, and we cannot get the exact accuracies by running their released code. For appropriate comparisons, we use the accuracy reported in the benchmark papers."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068150626,
                "cdate": 1700068150626,
                "tmdate": 1700082059497,
                "mdate": 1700082059497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mHGobwLMKp",
                "forum": "tUoBaW8KH1",
                "replyto": "Q0VghigzMW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pkn8 (3/3)"
                    },
                    "comment": {
                        "value": "***Improvement over GCNII and EGNN:*** GCNII is a state-of-the-art GCN-style model and EGNN can be considered as GCNII with constraints on the Dirichlet energy of the node features. The proposed smoothness control term (SCT) can consistently improve the state-of-the-art baseline models, especially on heterophilic graphs. For homophilic graphs, the improvement is more significant on larger-scale datasets -- Coauthor-Physics and Ogbn-arxiv -- than on the smaller-scale datasets -- Cora, Citeseer, and PubMed. In particular, Table 1 in our paper shows that SCT can often improve the accuracy of GCNII and EGNN by more than 1% on Coauthor-Physics and Ogbn-arxiv. As far as we are aware, the performance of GCNII-SCT and EGNN-SCT is the state-of-the-art accuracy for GCN-style models; see Section 5.1 and Appendix A.2 in reference [1] listed below for details.\n\nIn Table 1, though the accuracy gain is not huge in absolute values for GCNII and EGNN in some tasks, they are significant in almost all cases. To show the statistical significance of the improvement using SCT for smaller-scale datasets, we have included statistical significance tests for accuracy improvement for Cora, PubMed, and Citeseer. In particular, we train each model with 100 different random initializations using the optimal hyperparameters; the mean and standard deviation of the test accuracy are provided in Table 8 of the revision. Moreover, the t-test results on the statistical significance of the accuracy improvement are provided in Table 9 of the revision.\n\n[1] Zhou et al. Dirichlet Energy Constrained Learning for Deep Graph Neural Network, NeurIPS 2021.\n\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700068180404,
                "cdate": 1700068180404,
                "tmdate": 1700068220594,
                "mdate": 1700068220594,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K2TnJ6SVj5",
            "forum": "tUoBaW8KH1",
            "replyto": "tUoBaW8KH1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4062/Reviewer_h1DY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4062/Reviewer_h1DY"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the properties of node features learned by Graph Convolutional Networks (GCNs) with a focus on the smoothness of the features. It challenges the conventional understanding of the role of activation functions like ReLU or leaky ReLU in the smoothing process of node features in GCNs. Traditionally, it was believed that these activation functions contribute to smoothing the node features, which is beneficial for tasks like node classification when using a limited number of Graph Convolutional Layers (GCLs). However, the authors argue that this might not always be the case, especially in deeper GCNs. Through empirical studies and theoretical analysis, the paper presents evidence that in deeper networks, the node features might actually become less smooth, contrary to the established belief. This finding is significant as it opens up new avenues for understanding and improving the learning process in deep GCNs, particularly concerning the choice and role of activation functions in shaping the learned node features."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The argument that challenges the traditional understanding of activation function roles in GCNs seems to be presented clearly and logically, enhancing the paper\u2019s accessibility and impact.\n\n* The paper appears to delve deeply into the nuances of node feature smoothness in GCNs, providing a comprehensive analysis that bolsters the quality of the work."
                },
                "weaknesses": {
                    "value": "* The paper seems to heavily rely on previous works [1,2] for its theoretical results. A more independent theoretical contribution or a clearer delineation of the novel aspects beyond the referenced works would strengthen the paper's originality.  \n\n* The empirical validation could be broadened to enhance the robustness of the findings. Incorporating a more diverse array of datasets and experimenting with various network architectures would be beneficial. Notably, the largest non-homophily graph used is the Squirrel dataset, which consists of 5201 nodes. Exploring larger and more varied graphs could provide more comprehensive insights.\n\n* The proposed method, as represented by Equation 6, appears to be a somewhat incremental modification, seemingly adding only a bias term to the graph layer. A deeper discussion on the novelty and impact of this modification would be beneficial to understand its significance and contribution better.\n\n* The presentation of results in Table 1 could be improved for clarity and comprehensiveness. Enhancing the table's presentation could make the findings more accessible and effectively communicate the research outcomes.\n\n* The benifit of proposed method is somewhat weak when nerual network is deep.\n\n\n[1] Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. arXiv preprint arXiv:2006.13318, 2020.\n\n[2] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In International Conference on Learning Representations, 2020."
                },
                "questions": {
                    "value": "* Could you clarify the specific novel contributions of your theoretical analysis beyond the foundations laid by references [1,2]?\n\n* Have you considered testing your approach on a broader variety of datasets, especially larger and more complex non-homophily graphs beyond the Squirrel dataset?\n\n* Could you elaborate on the novelty and significance of the modification introduced in Equation 6? How does the addition of a bias term fundamentally impact the model's behavior or performance?\n\n* It seems that the benefits of the proposed method diminish in deeper neural networks. Could you provide more insights into why this might be the case and whether there are ways to mitigate this limitation?\n\n\n-----------------------\nThank you for addressing the feedback provided. After reviewing your rebuttal and considering other reviewers' comments, I have decided to maintain my original score for your paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4062/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4062/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4062/Reviewer_h1DY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4062/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733550778,
            "cdate": 1698733550778,
            "tmdate": 1701050626034,
            "mdate": 1701050626034,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l6x7muIS8g",
                "forum": "tUoBaW8KH1",
                "replyto": "K2TnJ6SVj5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h1Dy (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. In what follows, we provide point-by-point responses to your comments on the weakness of our paper and answer your questions on our paper.\n\n---\n\n**Q1. The paper seems to heavily rely on previous works [1,2] for its theoretical results. Could you clarify the specific novel contributions of your theoretical analysis beyond the foundations laid by references [1,2]?**\n\n**Reply:** Our theoretical results significantly differ from those in [1,2]. Recall that Oono & Suzuki [1] and Cai & Wang [2] focus on showing that over-smoothing is inevitable for GCNs with ReLU or Leaky ReLU activation function. At the same time, Cai & Wang also point out that studying the normalized smoothness is an open and important problem. In contrast, our theory is novel in the following sense: 1) In Section 3, we establish ***a geometric characterization*** of how ReLU and Leaky ReLU affect the smoothness of node features, showing that ***the smoothness of node features are controllable*** by adjusting the projection of node features onto the eigenspace $\\mathcal{M}$ for the first time. Our theory also informs a practical smoothness control term. 2) In Section 4, we show that adjusting the projection of node features onto the eigenspace $\\mathcal{M}$ results in ***disparate effects*** on the normalized and unnormalized smoothness. 3) We study an open problem proposed by Cai & Wang. These novelties have been listed in Section 1.1 of our paper.\n\n---\n\n**Q2. The largest non-homophily graph used is the Squirrel dataset, which consists of 5201 nodes. Exploring larger and more varied graphs could provide more comprehensive insights. Have you considered testing your approach on a broader variety of datasets, especially larger and more complex non-homophily graphs beyond the Squirrel dataset?**\n\n**Reply:** The existing experiments in our paper provide a direct comparison against the baseline models by providing a comprehensive study for each task proposed by the baseline papers. The 10 datasets we studied contain a diverse number of nodes, edges, features, and classes as shown in Table 4 in the revision. These tasks sufficiently cover a broad range of applications for GNNs.\n\nTo provide broader empirical results of our techniques on large non-homophily graphs, we have included results for the Roman-empire heterophilic graph in Appendix D.5 Table 13 of the revision. The Roman-empire graph has a large diameter which supports using deeper models. We utilize the architectures from Section 6.2 which are equipped with ReLU activation functions and perform hyperparameter tuning following the same procedure. For the reviewer's ease, we have also presented the results below.\n\n| Layers   \t| 2             \t| 4             \t| 16            \t| 32            \t|\n|--------------|-------------------|-------------------|-------------------|-------------------|\n| GCN \t \t| 84.48 \u00b1 0.53  \t| 84.00 \u00b1 0.71  \t| 74.56 \u00b1 0.75  \t| 14.32 \u00b1 1.02  \t|\n| GCN-SCT  \t| 85.37 \u00b1 0.56  \t| 84.08 \u00b1 0.71  \t| 82.58 \u00b1 0.57  \t| 79.6 \u00b1 0.49   \t|\n| GCNII \t| 83.49 \u00b1 0.36  \t| 83.43 \u00b1 0.40  \t| 80.01 \u00b1 0.50  \t| 76.52 \u00b1 0.70  \t|\n| GCNII-SCT\t| 85.44 \u00b1 0.56  \t| 85.08 \u00b1 0.24  \t| 81.44 \u00b1 0.35  \t| 77.28 \u00b1 0.55  \t|\n\n---\n\n**Q3. The proposed method, as represented by Equation 6, appears to be a somewhat incremental modification, seemingly adding only a bias term to the graph layer. Could you elaborate on the novelty and significance of the modification introduced in Equation 6? How does the addition of a bias term fundamentally impact the model's behavior or performance?**\n\n**Reply:** The proposed smoothness control term in Equation 6 is simple but novel and different from existing works. Our innovation stems from a theoretical analysis of how ReLU and Leaky ReLU affect the smoothness of node features learned by GCN. Below we elaborate on the novelty and significance of the modification introduced in Equation 6. \n\nIn Section 3, we establish a geometric characterization of how ReLU and Leaky ReLU affect the smoothness of node features learned by GCN. Our analysis demonstrates that the smoothness of node features can be modulated by adjusting the projection of node features in the eigenspace $\\mathcal{M}$ -- corresponding to the largest eigenvalue of the message passing matrix $G$ in GCL. In Section 4, we provide a detailed study on how adjusting the projection of node features onto eigenspace $\\mathcal{M}$ affects the smoothness of node features. Based on our theoretical analysis, we propose the smoothness control term in Equation 6. As far as we are aware, this is ***the first biased term, with guarantees to control the smoothness of the learned node features in GCL with non-linear activations***. \n\nThe proposed smoothness control term effectively controls the smoothness of node features learned by GCN, resulting in feature vectors with a desired smoothness that empirically enhances node classification accuracy in GCNs.\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067308724,
                "cdate": 1700067308724,
                "tmdate": 1700079815736,
                "mdate": 1700079815736,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sOFUw80m9l",
                "forum": "tUoBaW8KH1",
                "replyto": "K2TnJ6SVj5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h1Dy (2/2)"
                    },
                    "comment": {
                        "value": "**Q4. The presentation of results in Table 1 could be improved for clarity and comprehensiveness.**\n\n**Reply:** Thank you for your suggestion, and we have modified the Table 1 in the revision to make it more clear and comprehensive.\n\n---\n\n**Q5. It seems that the benefits of the proposed method diminish in deeper neural networks. Could you provide more insights into why this might be the case and whether there are ways to mitigate this limitation?**\n\n**Reply:** GCNII is a state-of-the-art GCN-style model and EGNN can be considered as GCNII with constraints on the Dirichlet energy of the node features. The proposed smoothness control term (SCT) can consistently improve the state-of-the-art baseline models, especially on heterophilic graphs. For homophilic graphs, the improvement is more significant on larger-scale datasets -- Coauthor-Physics and Ogbn-arxiv -- than on the smaller-scale datasets -- Cora, Citeseer, and PubMed. In particular, Table 1 in our paper shows that SCT can often improve the accuracy of GCNII and EGNN by more than 1% on Coauthor-Physics and Ogbn-arxiv. As far as we are aware, the performance of GCNII-SCT and EGNN-SCT is the state-of-the-art accuracy for GCN-style models; see Section 5.1 and Appendix A.2 in reference [1] listed below for details.\n\nIn Table 1, though the accuracy gain is not huge in absolute values for GCNII and EGNN in some tasks, they are significant in almost all cases. To show the statistical significance of the improvement using SCT for smaller-scale datasets, we have included statistical significance tests for accuracy improvement for Cora, PubMed, and Citeseer. In particular, we train each model with 100 different random initializations using the optimal hyperparameters; the mean and standard deviation of the test accuracy are provided in Table 8 of the revision. Moreover, the t-test results on the statistical significance of the accuracy improvement are provided in Table 9 of the revision.\n\n[1] Zhou et al. Dirichlet Energy Constrained Learning for Deep Graph Neural Network, NeurIPS 2021.\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067351167,
                "cdate": 1700067351167,
                "tmdate": 1700068245840,
                "mdate": 1700068245840,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oz4mQlsKoH",
            "forum": "tUoBaW8KH1",
            "replyto": "tUoBaW8KH1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4062/Reviewer_MfPc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4062/Reviewer_MfPc"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores how ReLU and leaky ReLU activation functions affect the smoothness of node features in Graph Convolutional Networks (GCNs). It introduces a theoretical framework and a practical algorithm to control feature smoothness. The paper's main contributions include demonstrating that these activations smooth input features without considering magnitude and proposing a learnable smoothness control term (SCT) to enhance node classification in GCNs. This work is the first to comprehensively investigate these aspects, offering insights and practical improvements for graph node classification with GCNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This paper comprehensively investigates the impact of ReLU and leaky ReLU for the first time in graph convolution, which is very meaningful.\n2) The theoretical and empirical evidence presented in this paper appears to be quite sound.\n3) The proposed learnable smoothness control term (SCT) can enhance the performance of existing GNN models in node classification."
                },
                "weaknesses": {
                    "value": "1) The writing of this paper needs further improvement, as the theoretical part is not very easy to understand. It is recommended to add a summary of notations and optimize the formatting.\n2) Experiments show that the performance improvement of SCT on deep models like GCNII and EGNN is relatively marginal."
                },
                "questions": {
                    "value": "1) Please refer to the aforementioned weaknesses.\n2) I don't have major concerns about this paper. My concern lies in the further improvement in writing is needed. Additionally, I haven't thoroughly reviewed the paper's proofs, and I will consider the opinions of other reviewers and relevant discussions before making a final decision."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4062/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736460311,
            "cdate": 1698736460311,
            "tmdate": 1699636370276,
            "mdate": 1699636370276,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bdjhU9ztNK",
                "forum": "tUoBaW8KH1",
                "replyto": "oz4mQlsKoH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer  MfPc"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review, valuable feedback, and endorsement. In what follows, we provide point-by-point responses to your comments on the weakness of our paper and answer your questions on our paper.\n\n\n---\n\n**Q1. The writing of this paper needs further improvement, as the theoretical part is not very easy to understand. It is recommended to add a summary of notations and optimize the formatting.**\n\n**Reply:** We appreciate the reviewer\u2019s suggestion. In Appendix A of the revision, we have added a table to summarize all notations -- we did not put the table in the main text due to page limit while Section 1.3 briefly overviews the notations used in our paper.\n\nTo make the theoretical part more straightforward to understand, we have further polished Section 1.1 to make it a better navigation of our theoretical results. In particular, our theoretical results are twofold: 1) In Section 3, we establish a geometric characterization of how ReLU or Leaky ReLU affects the smoothness of the node features learned by GCN, showing that the smoothness of node features are controllable by adjusting the projection of input onto the eigenspace $\\mathcal{M}$. 2) In Section 4, we comprehensively examine how adjusting the projection of input to $\\mathcal{M}$ affects the smoothness of node features measured by two existing smoothness notions.\n\nWe have also gone through the paper and done our best to improve the formatting. We are happy to include any further suggestions from the reviewer to improve the paper.\n\n\n---\n\n**Q2. Experiments show that the performance improvement of SCT on deep models like GCNII and EGNN is relatively marginal.**\n\n**Reply:** GCNII is a state-of-the-art GCN-style model and EGNN can be considered as GCNII with constraints on the Dirichlet energy of the node features. The proposed smoothness control term (SCT) can consistently improve the state-of-the-art baseline models, especially on heterophilic graphs. For homophilic graphs, the improvement is more significant on larger-scale datasets -- Coauthor-Physics and Ogbn-arxiv -- than on the smaller-scale datasets -- Cora, Citeseer, and PubMed. In particular, Table 1 in our paper shows that SCT can often improve the accuracy of GCNII and EGNN by more than 1% on Coauthor-Physics and Ogbn-arxiv. As far as we are aware, the performance of GCNII-SCT and EGNN-SCT is the state-of-the-art accuracy for GCN-style models; see Section 5.1 and Appendix A.2 in reference [1] listed below for details.\n\nIn Table 1, though the accuracy gain is not huge in absolute values for GCNII and EGNN in some tasks, they are significant in almost all cases. To show the statistical significance of the improvement using SCT for smaller-scale datasets, we have included statistical significance tests for accuracy improvement for Cora, PubMed, and Citeseer. In particular, we train each model with 100 different random initializations using the optimal hyperparameters; the mean and standard deviation of the test accuracy are provided in Table 8 of the revision. Moreover, the t-test results on the statistical significance of the accuracy improvement are provided in Table 9 of the revision.\n\n[1] Zhou et al. Dirichlet Energy Constrained Learning for Deep Graph Neural Network, NeurIPS 2021.\n\n\n---\n\n**Q3. I don't have major concerns about this paper. My concern lies in the further improvement in writing is needed.**\n\n\n**Reply:** Thank you for your comment. We have further improved the writing in the revision to make the paper clearer, and we are happy to include any further suggestions from the reviewer.\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067176560,
                "cdate": 1700067176560,
                "tmdate": 1700079021070,
                "mdate": 1700079021070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "usJJAfMytl",
                "forum": "tUoBaW8KH1",
                "replyto": "bdjhU9ztNK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Reviewer_MfPc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Reviewer_MfPc"
                ],
                "content": {
                    "title": {
                        "value": "Re"
                    },
                    "comment": {
                        "value": "Thank you for the author's response. I have already read it. The author's answer has addressed my concerns, and I will keep my current score and discuss it with other reviewers."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632090323,
                "cdate": 1700632090323,
                "tmdate": 1700632090323,
                "mdate": 1700632090323,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bxOiRN6nEg",
            "forum": "tUoBaW8KH1",
            "replyto": "tUoBaW8KH1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4062/Reviewer_Pp4G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4062/Reviewer_Pp4G"
            ],
            "content": {
                "summary": {
                    "value": "This paper first shows that the image of ReLU and Leaky ReLU is contained in a particular sphere. As a corollary, alternative proofs are given to the contraction property of ReLU and Leaky ReLU (with respect to the distance to the eigenspace $\\mathcal{M}$). Then, this paper defines the normalized smooth index $s(\\cdot)$ for each feature dimension and elucidate how the parallel component of a feature vector to $\\mathcal{M}$ affects the change of $s(\\cdot)$ by applying ReLU and Leaky ReLU. Based on this, this paper proposes Smoothness Control Term (SCT) to adjust the feature component parallel to $\\mathcal{M}$ as bias terms of GNN layers. SCT is applied to GCNII and EGCN models and evaluates its performance on five Citation Network datasets and node classification problems on five heterophilic datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method improves prediction accuracy, especially for datasets with high heterophily (Table 2). This result is consistent with the claim that the proposed method is effective for over-smoothing.\n- The proposed method applies to most GNNs of MPNN type, although numerical verifications are limited to GCNII and ECGN.\n- The proof is carefully written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The proof about the contraction property applies only to ReLU and Leaky ReLU. Therefore, this theoretical analysis does not broaden the applicable GNN types."
                },
                "questions": {
                    "value": "* P.2: *We prove that there is a high-dimensional sphere ... ReLU or leaky ReLU*: It is difficult to grasp what is intended by this sentence alone. It would be better to be more specific. For example, *We prove the output of ReLU or Leaky ReLU lies in a high-dimensional space characterized by the input.*\n* P.5, Definition 4.1: $\\|\\boldsymbol{z}_{\\mathcal{M}}^{(i)}\\|$ is undefined.\n* P.7: If I understand correctly, the $\\beta_l$ parametrization comes from the work of GCNII. If this paper references it, the paper should be cited explicitly.\n* P.7: $\\boldsymbol{W}^1$ -> $\\boldsymbol{W}^l$\n* P.8, Table 1: The column \"16 Layers\" is not aligned."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4062/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746154287,
            "cdate": 1698746154287,
            "tmdate": 1699636370186,
            "mdate": 1699636370186,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Sa6ggc3jK3",
                "forum": "tUoBaW8KH1",
                "replyto": "bxOiRN6nEg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Pp4G"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. In what follows, we provide point-by-point responses to your comments on the weakness of our paper and answer your questions on our paper.\n\n---\n\n**Q1. P.2: We prove that there is a high-dimensional sphere ... ReLU or Leaky ReLU: It is difficult to grasp what is intended by this sentence alone. It would be better to be more specific. For example, We prove the output of ReLU or Leaky ReLU lies in a high-dimensional space characterized by the input.**\n\n\n**Reply:** Thank you for your suggestion. To be more specific, we have updated the statement as follows: We prove that the projection of the output of ReLU/Leaky ReLU onto the eigenspace $\\mathcal{M}^\\perp$ -- corresponding to eigenvalue 1 of matrix ${\\bf G}$ in equation (1) -- lies in a high-dimensional sphere, whose center only depends on the input but the radius depends on both input and output of ReLU/Leaky ReLU.\n\n---\n\n**Q2. P.5, Definition 4.1: $||\\mathbf{z}_{\\mathcal{M}}^{(i)}||$ is undefined.**\n\n\n**Reply:** We have defined $||\\mathbf{z}_{\\mathcal{M}}^{(i)}||$ in Definition 4.1 in the revised paper.\n\n---\n\n**Q3. P.7: If I understand correctly, the $\\beta_l$ parametrization comes from the work of GCNII. If this paper references it, the paper should be cited explicitly.**\n\n\n**Reply:** We have cited the GCNII paper right after the $\\beta_l$ parameterization in the revision.\n\n---\n\n**Q4. P.7: $\\mathbf{W}^1 \\rightarrow \\mathbf{W}^l$**\n\n**Reply:** Thank you for your very careful review. This is not a typo, and this is precisely what EGNN uses. In our revision, we refer to the orthogonal initialization and orthogonal regularization in the original EGNN paper, i.e., reference [42]. In summary, EGNN initializes the first weight matrix $\\mathbf{W}^1$ as a diagonal matrix $\\sqrt{c_{\\max}}\\cdot \\mathbf{I}$, and the subsequent weight matrices $\\mathbf{W}^l$ for $l>1$ as diagonal matrices $\\mathbf{I}$. Subsequently, orthogonal regularization is applied to penalize the distances between the trainable weights $\\mathbf{W}^1$ and $\\mathbf{W}^l$ and the initial weights.\n\n---\n\n**Q5. P.8, Table 1: The column \"16 Layers\" is not aligned.**\n\n**Reply:** This has been fixed in the revised version.\n\n**Q6. The proof about the contraction property applies only to ReLU and Leaky ReLU. Therefore, this theoretical analysis does not broaden the applicable GNN types.**\n\n\n**Reply:** Understanding whether over-smoothing happens or not and how to control the smoothness when other activation functions are used is an interesting problem. However, the over-smoothing has only been theoretically justified for GCN using ReLU and Leaky ReLU activation functions. \n\nTo be more specific, the established proofs of over-smoothing by Oono & Suzuki (ICLR 2020) and Cai & Wang (arXiv:2006.13318) rely on the contraction property of ReLU and Leaky ReLU. In particular, Oono & Suzuki (ICLR 2020) show over-smoothing for GCN with ReLU by using the contraction property of ReLU and point out that it is hard even to extend this result to Leaky ReLU. Cai & Wang prove the contraction property of ReLU or Leaky ReLU by using Dirichlet energy to characterize the smoothness of node features. Importantly, they highlight that other activation functions, such as Sigmoid and Tanh, may not exhibit the contraction property. \n\nNevertheless, our work builds a geometric understanding of the contract property of ReLU and Leaky ReLU, which further informs a practical approach to control the smoothness of the learned node features by GCN with ReLU or Leaky ReLU activation function.\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700067075844,
                "cdate": 1700067075844,
                "tmdate": 1700090330072,
                "mdate": 1700090330072,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uCQXcMrZyT",
                "forum": "tUoBaW8KH1",
                "replyto": "Sa6ggc3jK3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Reviewer_Pp4G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Reviewer_Pp4G"
                ],
                "content": {
                    "title": {
                        "value": "Reponses to authors' comments"
                    },
                    "comment": {
                        "value": "I thank the authors for answering my review comments. Here, I respond to the authors' comments one by one.\n\nQ.1: OK\n\nQ.2: OK\n\nQ.3: OK\n\nQ.4: OK. Thank you for your detailed explanation and I am sorry for my misunderstanding.\n\nQ.5: OK\n\nQ.6: \n\n> However, the over-smoothing has only been theoretically justified for GCN using ReLU and Leaky ReLU activation functions.\n\nThank you for the explanation. My point was that it would be a plus if either:\n1. the theory can explain the over-smoothing for activation functions that are practically observed but have not been theoretically justified or\n2. the theory can predict the over-smoothing for (possibly new) activation functions other than ReLU and Leaky ReLU and confirm the phenomena practically."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650330259,
                "cdate": 1700650330259,
                "tmdate": 1700650330259,
                "mdate": 1700650330259,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bd8XqLZZI9",
                "forum": "tUoBaW8KH1",
                "replyto": "bxOiRN6nEg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4062/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer Pp4G"
                    },
                    "comment": {
                        "value": "Dear Reviewer Pp4G,\n\nThank you for considering our rebuttal and for your invaluable feedback. The two problems you pointed out are both very interesting and thank you again.\n\nOur study was motivated by an empirical finding by Oono & Suzuki and the open problem pointed out by Cai & Wang rather than justifying over-smoothing for GCN with ReLU or leaky ReLU again. In particular, Oono & Suzuki empirically find that the accuracy of GCN is strongly correlated with a normalized smoothness-related quantity, which motivates us to design practical algorithms to let GCN automatically learn the desired smoothness of node features to improve classification accuracy. Moreover, Cai & Wang pointed out that studying the over-smoothing measured by normalized smoothness is an important open problem.\n\nTo theoretically study the empirical findings by Oono & Suzuki and the open problem pointed out by Cai & Wang, we first establish a geometric relation between the input and output of ReLU and leaky ReLU. Our established geometric relation not only confirms over-smoothing but also informs a new smoothness control term that can let GCN learn a desired smoothness to improve the classification accuracy. Our established geometric relation leverages the special structure of ReLU and leaky ReLU. It is possible but highly nontrivial to establish a similar geometric relation for other activation functions.\n\nWe further study the disparate effects of our proposed smoothness control term on the normalized and unnormalized smoothness of node features. Our theoretical study shows that each GCN layer -- even with our proposed smoothness control term -- smooths node features when measured by the unnormalized smoothness. In contrast, the GCN layer with our proposed smoothness control term can increase, decrease, and preserve the smoothness of node features when measured by the normalized smoothness. \n\nMoreover, our empirical results confirm that the proposed smoothness control term can effectively improve the classification accuracy of GCN and related models.\n\nThank you for considering our rebuttal. We are open to addressing any further questions or concerns on our paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4062/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678785455,
                "cdate": 1700678785455,
                "tmdate": 1700682762910,
                "mdate": 1700682762910,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]