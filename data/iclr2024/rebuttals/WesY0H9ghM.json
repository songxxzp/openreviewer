[
    {
        "title": "Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback"
    },
    {
        "review": {
            "id": "erePGJBnR5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3363/Reviewer_BKHb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3363/Reviewer_BKHb"
            ],
            "forum": "WesY0H9ghM",
            "replyto": "WesY0H9ghM",
            "content": {
                "summary": {
                    "value": "This work introduces Uni-RLHF, an eco-system for Reinforcement Learning with Human Feedback to facilitate the data collection with human annotators, the sharing of datasets and the RL alignment.\nIn particular, the annotation platform named Enhanced-RLHF supports various feedback types.\nThen, the paper investigates different offline RL strategies, and show that the reward models trained on their crowdsourcing labels lead to better performances than when using synthetic labels, and can approximate the Oracle rewards. This is done on motion control/manipulation tasks such as D4RL or Atari or Smarts. They aim for fair evaluation of RLHF strategies."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The work effectively succesfully presents a comprehensive system to deal with the data collection process with diverse human feedback types.\n- The motivation is clear and interesting: indeed, RLHF appears nowadays as a go-to strategy to ensure the reliability of AI systems. Therefore the proposed eco-system can be of interest to some researchers.\n- Crowdsourced labels are sufficient to approximate Oracle-based reward, showing the flexibility of RLHF even in those D4RL datasets."
                },
                "weaknesses": {
                    "value": "- The main weakness is the limitation to control/locomotion tasks. More real world tasks/other modalities (such as text), and also larger architectures are required to make this eco-system more attractive.\n- The benchmark only compares offline RL, thus for example the de facto online strategy for RLHF (PPO) is not ablated.\n- The different query samplers are not ablated.\n- Only the comparative feedback is ablated. It would have been interested to compare the quality of the reward models.\n- While the authors do acknowledge this, providing a data cleaning procedure (or data filters) in the eco-system would be very positive to foster its applicability."
                },
                "questions": {
                    "value": "- could you please clarify the differences with RLHF-Blender ? in which case we should use one or the other ?\n- How did you select the hyperparameters?\n- Have you applied any strategy to refine the dataset quality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3363/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3363/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3363/Reviewer_BKHb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697200860695,
            "cdate": 1697200860695,
            "tmdate": 1699636286427,
            "mdate": 1699636286427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SUqsMttfG9",
                "forum": "WesY0H9ghM",
                "replyto": "erePGJBnR5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BKHb (Part 1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and useful feedback, please see the following for our response.\n\n**[Q1: The main weakness is the limitation to control/locomotion tasks. ]**\n\nWe would like to clarify that Uni-RLHF primarily focus on RLHF research for **decision making (locomotion/manipulation/navigation\u2026)**, building upon the settings established by a series of mainstream research works such as Preference-PPO[1], Pebble[2]. These works concentrate on decision-making tasks too, and we have developed a labeling platform and benchmark that can accommodate a broader range of decision making tasks. \n\nSame as many widely used annotation tools, e.g. PixelAnnotationTool[3] for vision domain and Curve[4] for time series domain, Uni-RLHF also focuses on specific areas and hopes to build a robust eco-system for decision-making.  Therefore, we are able to focus on solving specific problems in decision tasks annotation, such as a robust environment access methods, feedback format encoding, and so on. Also, we modify relevant description of introduction to avoid confusion and add the statement that we specialise in decision making domain. (**See red text in the updated manuscript, page 1 and 7**)\n\nBased on the above core design concepts, we will also continue to maintain and update the platform to support more real-world problem environments/datasets and more modalities. \n\n**[Q2: The benchmark only compares offline RL, online RLHF is not ablated. ]**\n\nThe reason we mainly use Offline RL is that we want to provide researchers with reusable labelled datasets to easily evaluate algorithms without additional labelling cost on a uniform benchmark. While decision making tasks in Online RL needs to learn from scratch, the experimental samples and annotations will be completely different from the algorithms, sampling methods, etc., which makes it difficult to reuse.\n\nAs **section 3.1** mentioned, Uni-RLHF is able to support the online training mode. We add a more detailed implementation of the online asynchronous annotation process in the revision. (**see Appendix E and Fig.8, page 21**)\n\nTo better show the effectiveness of the online mode of Uni-RLHF, we add an extra online RLHF experiment in the revision (**See Appendix E and Fig.9, page 23**). **After totally 200 queries of human annotation, walker can master the continuous multiple front flip fluently**. Through our experiments we demonstrated that Uni-RLHF can access online training to allow agent to learn novel behaviours that are difficult to design reward.\n\n**[Q3: The different query samplers are not ablated. ]**\n\nOffline RLHF experimental results with crowdsourced annotation are based on the random sampler implementation because of limited resource. However, we provide several ways of sampler definition in our platform to facilitate flexible selection by users and researchers, and **our focus is to provide an open and easily extensible platform to customise the appropriate query according to specific tasks.** Therefore we hope to open up better tools and remain some open research problems to RLHF community like how to choose the sampler. \n\nTo better validate the role of different sampler, we collected new crowdsourcing labels and conducted the additional experiments to demonstrate the performance of different sampling methods. In each experiment, we collect **100 annotation labels** (comparative feedback) using a different sampler separately and use IQL-MLP-CrowdSource method for simplicity. **See Appendix D.3 (page 19)**\u00a0of our revision for detailed experimental setup and results. \n\n| Env Name | Random Sampler | Disagreement Sampler | Entropy Sampler |\n| --- | --- | --- | --- |\n| antmaze-medium-diverse-v2 | $46.95 \u00b1 11.91$ | **64.43 \u00b1 6.57** | **62.69 \u00b1 7.51** |\n| hopper-medium-replay-v2 | $42.75\u00b15.27$ | **63.28 \u00b1 8.58** | 44.53 \u00b1 6.83 |\n\n\nWe observe that suitable label sampler could improve the task performance,  this is also consistent with a similar conclusion by OPRL[5]. More ablation experiments are in process and we will continue to update the results in rebuttal phases."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666082749,
                "cdate": 1700666082749,
                "tmdate": 1700666082749,
                "mdate": 1700666082749,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g3ZuKK2HOO",
            "forum": "WesY0H9ghM",
            "replyto": "WesY0H9ghM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3363/Reviewer_FdzJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3363/Reviewer_FdzJ"
            ],
            "content": {
                "summary": {
                    "value": "Uni-RLHF is a comprehensive system for reinforcement learning with diverse human feedback. It includes an Enhanced-RLHF platform that supports multiple feedback types, environments, and parallel user annotations, along with a feedback standard encoding format. The system also provides large-scale datasets, offline RLHF baselines, and human feedback datasets for relabeling and human-aligned reward models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Uni-RLHF provides a universal platform for RLHF that supports multiple feedback types, environments, and parallel user annotations. The system includes a feedback standard encoding format that facilitates the integration of diverse feedback types.\n2. Uni-RLHF provides large-scale datasets and offline RLHF baselines for evaluating the performance of RL algorithms with human feedback. The system also includes human feedback datasets for relabeling and human-aligned reward models, which can improve the efficiency and effectiveness of RLHF.\n3. Uni-RLHF can foster progress in the development of practical problems in RLHF by providing a complete workflow from real human feedback."
                },
                "weaknesses": {
                    "value": "1. Uni-RLHF's large-scale crowdsourced feedback datasets may contain noise and bias, which can affect the performance of RL algorithms trained on these datasets.\n2.  The system's offline RLHF baselines may not be optimized for specific applications, which can limit their usefulness in practical settings.\n3. The system's reliance on human feedback may introduce additional costs and delays in the RL development process, compared to purely synthetic feedback.\nUni-RLHF is an engineering-focused system that emphasizes incremental improvements rather than groundbreaking innovations."
                },
                "questions": {
                    "value": "Please refer to weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3363/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3363/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3363/Reviewer_FdzJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658313667,
            "cdate": 1698658313667,
            "tmdate": 1699636286349,
            "mdate": 1699636286349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8FHXPfG2lB",
                "forum": "WesY0H9ghM",
                "replyto": "g3ZuKK2HOO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FdzJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and useful feedback, please see the following for our response.\n\n**[Q1: Uni-RLHF's large-scale crowdsourced feedback datasets may contain noise and bias, which can affect the performance of RL algorithms trained on these datasets.** **]**\n\nWe would like to clarify that noise and bias can't be avoided in pratical problems and annotations,  that's what we're focusing on. We hope to estimate human irrationality and bias in realistic annotators to futher study the RL algorithms in more realistic scenario. \n\nSeveral RLHF benchmarks[1][2] utilize predefined hand-engineered reward functions from the environment as scripted teachers, which can be considered a noise-free environment. Experimental results in **Table.2** show that **our crowdsourced human feedback can achieve competitive performance or outperform compared to noise-free scripted teachers**. \n\nIn the data collection process, we try to minimise extreme noise and refine the dataset quality including continuous optimisation of task instructions, annotation example and sampling inspection.\n\nWe have added further analysis and visualisation of the trained reward model (**See Appendix D.2, page 19, revised version paper**) and experimentally demonstrated that the reward model trained by human feedback can be well fitted to trends of the hand-designed reward function (**high relevance**) and aligned with human intent (**high prediction accuracy**).\n\n**[Q2: The system's offline RLHF baselines may not be optimized for specific applications, which can limit their usefulness in practical settings.** **]**  \n\nWe agree with the reviewer that the addition of more specific applications can increase the usefulness of the Uni-RLHF, which is also in line with our core motivation. Due to the high cost of labelling, previous research has mostly been in more ideal scenarios such as synthetic labels,  however, we provide annotation tools for multi-user annotation and a large number of  crowdsourced labels and this will facilitate the RLHF community to research in more realistic settings. Our main goal is to demonstrate the feasibility of a complete annotation pipeline and an open source reusable dataset. As for offline RLHF baseline, we only utilize the collected crowdsourced feedback datasets to verify the reliability of the Uni-RLHF system, which is not the core contribution of this work. We hope that more researchers will access their real-world problems in the platform to annotate and further optimise the framework based on this codebase. \n\n**[Q3: The system's reliance on human feedback may introduce additional costs and delays in the RL development process, compared to purely synthetic feedback.** **]**\n\nIt is worth noting that in the most of RLHF application scenarios, **we do not have access to synthetic feedback. Purely synthetic labels exist only in ideal research simulation environments**, as a wide range of researchers would struggle to afford the cost of crowdsourcing labels in all experiments. The process of generating synthetic labels is as follows: 1) utilize predefined hand-engineered reward functions as scripted teachers, 2) score of trajectories by scripted teachers to obtain synthetic labels. In application scenarios, RLHF is mostly suitable for environments where manual design rewards are difficult, when additional costs and delays are unavoidable.\n\nTake SMARTS domain as an example, designing reward functions is extremely difficult, requiring extensive expertise and multiple rounds of training attempts for iteration, the final training reward (marked as oracle) consists of five reward items of widely varying magnitude: Distance, Near Goal, Mistake Event, Lane-Selection and Time Penalty. Adjusting the reward function also requires a significant amount of cost, whereas crowdsourcing feedback annotations enable simply to achieve competitive performance to oracle and more consistent with human intentions. **(See Section 4.1.3, page 8 and Appendix D.1, page 18 for details)** \n\n---\n\n[1] Lee K, et al. B-pref: Benchmarking preference-based reinforcement learning. arXiv preprint arXiv:2111.03026, 2021.\n\n[2] Shin D, Dragan A, Brown D S. Benchmarks and Algorithms for Offline Preference-Based Reward Learning. Transactions on Machine Learning Research, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666283324,
                "cdate": 1700666283324,
                "tmdate": 1700666439195,
                "mdate": 1700666439195,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AGzVOBphhB",
            "forum": "WesY0H9ghM",
            "replyto": "WesY0H9ghM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3363/Reviewer_3bfZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3363/Reviewer_3bfZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new framework to collect human feedback for RLHF. The framework allows for five different types of feedback to be collected with a convenient user interface, depending on the task at hand. A dataset of feedback of three of these types is collected using crowdsourced human workers. This dataset is then used to evaluate multiple existing offline RLHF approaches, where a trained reward model is used to label trajectories from a dataset, and then an offline RL algorithm is used to produce a policy. The evaluation shows that the collected data for comparative and attribute feedback is of good quality, and allows for the learned policy to perform at a quality comparable to a policy trained from hand-crafted reward models. In some cases the policies trained from human feedback outperform those trained from hand-crafted reward models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In my view, the main strength of the paper is the open-sourced dataset and the tool for feedback collection. These will bring great value to the community and allow for easy benchmarking of offline RLHF methods. The extensive evaluation on D4RL is another strong point of the paper. It will provide a reasonable baseline for future offline RLHF approaches. Finally, the dataset with collected attribute feedback can be used for multi-objective learning, an area where fewer datasets in general are available."
                },
                "weaknesses": {
                    "value": "- The claim for a \"standardized feedback encoding format along with corresponding training methodologies\" in Section 3.2 might be overly ambitious. Most importantly, the training methodologies are only provided in the appendix for two out of the five feedback types. Comparative feedback interface also does not allow the user to indicate the strength of preference, as done e.g. in the Anthropic's dataset for language model alignment [1]. The extension of the methodology in Appendix E.1 to this type seems straightforward (introduce y=(0.75,0.25), for example), and it could be beneficial to mention this.\n- Related to the previous point, the paper provides too few details on the way Atari experiments were performed. Appendix G and Section 4.1.1 imply that comparative feedback was collected, but Figure 5 (d) in the appendix -- that visual feedback was used instead. The highlighting in Table 8 should be explained. My guess is that the best of the ST and CS labels is highlighted.\n- There is no dataset or benchmarking for evaluative feedback, hence it is hard to assess the usefulness of this part of the system. It is unclear whether data for keypoint and visual feedback on the Atari environments was collected. I believe that the datasets for comparative and attributive feedback are already a good enough contribution, and an interface for other feedback types is a nice-to-have extra, so this point does not make me give this paper a negative rating.\n- Star annotations in Table 2 are incomplete. As I understand it, higher is better in the entire table. If that is the case, stars that should mark that the method performs better than the oracle are missing in several places: (hopper-m, CQL, CS-TFM), or (antmaze-u-d, IQL, CS-MLP). There are more missing stars in the table.\n- Blue annotations in Table 2 are also significantly wrong, if I understand them correctly. Blue should mark the methods where crowdsourced labels (CS) show better results than the synthetic labels (ST) for the corresponding method. Then, for example, (walker2d-m-r, IQL, CS-MLP) should not be colored in blue, since (walker2d-m-l, IQL, ST-MLP) shows a better score (109.8 > 109.4). I counted 10 instances of such mislabeling in the IQL group alone. This is especially important since the paper claims that \"the performance of models trained using crowd-sourced labels (CS) tends to slightly outperform those trained with synthetic labels (ST) in most environments, with significant improvements observed in some cases.\" Once the blue labels are revised, it is questionable whether this claim still holds. Other offline RL approaches (CQL and TD3BC) show that CS labels are better more consistently, but these approaches perform worse than IQL, so the results for them are correspondingly less important.\n- Some CS-TFM results are missing from Table 2. I did not find the explanation for this, maybe I missed it? It would be helpful to see these results in the final version, to better assess the claim of the paper that \"CS-TFM exhibits superior stability and average performance compared to CS-MLP\".\n- For the SMARTS experiment in Section 4.1.2, the paper claims that \"the best experimental performance can be obtained simply by crowdsourcing annotations, compared to carefully designed reward functions or scripted teachers.\" I would say that from the three tasks used one cannot conclude that carefully designed rewards perform worse than crowdsourced annotations. In table 3, the former outperforms the latter on two out of three methods. The claim seems to be made based on average success rate, which only differs by 0.01, less than one standard deviation.\n- In Figure 4, speed and torso height are plotted against time. Every 200 steps, the speed attribute value changes (1 to 0.1 and back), so we see the respective changes in speed on the first plot. The relative strength of the \"torso height\" attribute, however, does not change (stays at 0.1), and the torso height parameters do not change much between the changes. It would be more interesting to see the results where the strength of torso height also changes, so that we can see that it influences the plot.\n- Generally speaking, RLHF is most useful in domains where only the humans understand the true reward. This is the case, for example, with \"helpfulness\" of an LLM, or with \"humanness\" of the gait of a walking robot. An important evaluation, then, is to see whether the human evaluators prefer the policies trained with an RLHF approach in terms of these hard-to-define measures. This paper, however, does not present such an evaluation. \"Humanness\" comparisons are collected as shown in Section 4.2, but it is never compared to a policy that is trained without taking humanness into account.\n\nOverall, these weaknesses do not seem to me to be strong enough to motivate a rejection. The dataset and tool contributions are strong, and the problems with baseline evaluations can be clarified in the final version of the paper."
                },
                "questions": {
                    "value": "- In table 2, the results are normalized against an \"expert\", as the formula in the beginning of Section 4.1.1 shows. How is the expert trained? It is interesting that some of the methods in the table outperform the expert.\n- I found the attribute training model in Appendix E.2 confusing. The learned log-preferences $\\hat{\\zeta}^\\alpha$ in eq. (7) probably also need the subscript $\\theta$.  The relative strengths of attributes $v^\\alpha_{opt}$ are provided as hyperparameters. These strengths are in $[0,1]$. Why, then, in eq. (7) the attributes are checked for being close to $\\hat{\\zeta}^\\alpha$, which is supposed to become the probability of preference according to the respective attribute only after the softmax operation (5)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3363/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3363/Reviewer_3bfZ",
                        "ICLR.cc/2024/Conference/Submission3363/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699297065957,
            "cdate": 1699297065957,
            "tmdate": 1700736671217,
            "mdate": 1700736671217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JboOkJlHz3",
                "forum": "WesY0H9ghM",
                "replyto": "AGzVOBphhB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3bfZ (Part 1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and useful feedback, please see the following for our response.\n\n**[Q1: The claim for a \"standardized feedback encoding format along with corresponding training methodologies\" might be overly ambitious.]**\n\nThank you for your valuable suggestion! After careful consideration, we focus on proposing a set of feedback encoding format which are easy to use. We then train them with possible training methodologies based on adapting and improving existing literature, like comparative and attribute feedback in the previous paper version. So we change the statement to **standardized feedback encoding format along with possible training methodologies** in the revision.  (**See Section 3.2, page 5**)\n\nBased on this core idea, we complement the revision with **new crowdsourced labels of keypoint feedback and corresponding experiments** (**See Appendix D.5, page 20**)**,** so the dataset and benchmark contains comparative feedback, attribute feedback and keypoint feedback now. And we provide **more possible training methodologies** **for the all 5/5 feedback types**. (**See Appendix G, page 5**) We will continue to research and expand the platform and datasets for diverse human feedback.\n\n**[Q2: Comparative feedback interface also does not allow the user to indicate the strength of preference]**\n\nWe agree with the reviewer that it might be very meaningful to further allow the user to indicate the strength of preference. We have added this option to the platform design through a slider like Anthropic's dataset[1]. We've also updated the description and cited the paper in the **Appendix G.1, page 23:**\n\n> It is worth noting that $y_\\text{comp}$ can simply be extended to a finer-grained division of labels including the degree to which A is better than B. For example,  $y_\\text{comp} = (0.75, 0.25)$ represents that A is better than B with a smaller degree than (1, 0). Also, the expansion method can simply be trained along the Eq. (4) without any changes.\n> \n\n**[Q3: the paper provides too few details on the way Atari experiments were performed]**\n\n> Related to the previous point, the paper provides too few details on the way Atari experiments were performed. Appendix G and Section 4.1.1 imply that comparative feedback was collected, but Figure 5 (d) in the appendix -- that visual feedback was used instead.\n> \n\nSorry for the confusion. We begin by clarifying that our atari experiments use comparative feedback for all, which is intended as an implementation of the CNN reward structure. In Figure 5 (d) in the appendix, we demonstrate that we can use the atari environment offline dataset for visual feedback annotation in the Uni-RLHF platform, but we do not provide the annotated dataset for visual feedback and the corresponding experiments due to limited resource in the current version, and we will continue to explore the performance gain of visual feedback in our future work.\n\n> The highlighting in Table 8 should be explained. My guess is that the best of the ST and CS labels is highlighted.\n> \n\nYes, you are absolutely correct in your understanding! We use highlighting to mark the better one of the ST and CS labels. And, we increase the importance of atari experiments and provide clearer experimental details In the revised main text. (**See Section 4.1.2 Atari Experiments, page 8**)\n\n**[Q4: About some confusion in full results of Table 2]**\n\nWe sincerely thank the reviewer for efforts to improve the quality of our papers, In revision, we have carefully examined the data and marks throughout the table, redesigned some of the annotation (blue highlight and *) and re-analysed experimental findings. (**Table 2, page 8**)\n\n> Star annotations in Table 2 are incomplete.\n> \n\nSorry for the confusion. In the updated revision, we use * to mark the method that works best of all (usually Oracle). Since multiple algorithms are close in effect in many datasets, we take the best performing as the baseline, and if standard deviations in the rest of the algorithms intersect, they are similarly labelled as *. \n\n> Blue annotations in Table 2 are also significantly wrong.\n> \n\nWe apologise for the unclear description. In our definition, blue is used to highlighted which of the CS-MLP and CS-TFM methods is better and we've carefully checked in the revised version.\n\n> Some CS-TFM results are missing from Table 2.\n> \n\nWe filled in all the missing results in Table 2 and reanalysed them based on complete results.\n\n> About performance comparison using ST and CS.\n> \n\nThanks for the correction. We modify the description in the revision to argue that CS can achieve competitive performance with ST in IQL backbone, even script teacher (ST) can be regarded as an expert with some degree. And other offline RL approaches (CQL and TD3BC) show that CS labels are better more consistently. We obtain a similar conclusion in atari domain (image observation and discrete action space)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666547892,
                "cdate": 1700666547892,
                "tmdate": 1700666593994,
                "mdate": 1700666593994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MiOCoAMX1C",
                "forum": "WesY0H9ghM",
                "replyto": "AGzVOBphhB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3363/Reviewer_3bfZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3363/Reviewer_3bfZ"
                ],
                "content": {
                    "title": {
                        "value": "The revised version of the paper, other reviewers' comments"
                    },
                    "comment": {
                        "value": "First, I would like to thank the authors for an extensive revision that carefully takes into account the reviewers' suggestions.\n- [regarding the inconsistencies in Table 2 and its analysis in the main text] The new version looks consistent and complete. I agree with the updated analysis in the paper as well. \n- [regarding Atari experiments] I thank the authors for the clarifications in the reply and in the paper.\n- [regarding the more complete discussion of RLHF training methods and the extra keypoint dataset and evaluation] I think this is a valuable addition to the system.\n\nRegarding other reviewer's comments, I believe that the authors' response addresses the concerns well. \n- Reviewer FdzJ points out some weaknesses of the system that are inherent in any RLHF approach where real human feedback is used. I agree with Reviewer FdzJ that the nature of work is incremental, but we both seem to think that the contribution is enough for publication.\n- Reviewer BKHb points out missing experiments for a better understanding of the results, which the authors do provide, with the exception of labels for conversational models. I believe that a system with a focus on non-textual agents is still a sufficient contribution, even if it does not contain evaluation for online RLHF. The revised version goes even further and does include experiments on online RLHF as well.\n\n**Based on these considerations, I am happy to update my rating for the paper from 6 to 8.**"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736636236,
                "cdate": 1700736636236,
                "tmdate": 1700736699078,
                "mdate": 1700736699078,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]