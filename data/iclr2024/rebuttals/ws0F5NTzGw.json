[
    {
        "title": "AdapTable: Test-Time Adaptation for Tabular Data via Shift-Aware Uncertainty Calibrator and Label Distribution Handler"
    },
    {
        "review": {
            "id": "CFX4wIjFoz",
            "forum": "ws0F5NTzGw",
            "replyto": "ws0F5NTzGw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7383/Reviewer_DhMn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7383/Reviewer_DhMn"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes AdapTable to address problems about tabular-specific test-time adaption. AdapTable uses a shift-aware uncertainty calibration module to correct the poor confidence calibration, and uses a label distribution handler to adjust the output distribution. Experimental results show the effectiveness of AdapTable."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper comprehensively investigate the problems about tabular-specific test-time adaption, and propose a new method called AdapTable to address these problems."
                },
                "weaknesses": {
                    "value": "This paper does not investigate and justify (semantically and experimentally) the advantages of the proposed post-hoc output calibration method compared with traditional model calibration methods such as isotonic regression calibration and Platt calibration. Besides, it is not clear how the target label distribution output by the calibration model is guaranteed to be correct."
                },
                "questions": {
                    "value": "1. What are the advantages of the calibration method proposed in this paper compared to the traditional calibration method such as isotonic regression calibration and Platt calibration?\n2. How is the target label distribution ensured (or ensured with a certain probability) to be correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698030864943,
            "cdate": 1698030864943,
            "tmdate": 1699636883841,
            "mdate": 1699636883841,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cZZPikbYnN",
                "forum": "ws0F5NTzGw",
                "replyto": "CFX4wIjFoz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DhMn [1]"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the helpful and insightful comments. We address the reviewer's concerns below.\n\n### [W1, Q1. This paper does not investigate and justify (semantically and experimentally) the advantages of the proposed post-hoc output calibration method compared with traditional model calibration methods such as isotonic regression calibration and Platt calibration. Besides, it is not clear how the target label distribution output by the calibration model is guaranteed to be correct. / What are the advantages of the calibration method proposed in this paper compared to the traditional calibration method such as isotonic regression calibration and Platt calibration?]\n\nThe calibration method proposed in the 'AdapTable' paper, referred to as the shift-aware post-hoc uncertainty calibrator, offers several advantages over traditional calibration methods like isotonic regression calibration and Platt calibration, especially in the context of handling distribution shifts in tabular data. Here are the key advantages:\n\n**Shift-Awareness:** One of the main innovations of the calibration method in 'AdapTable' is its awareness of distribution shifts. Traditional methods like isotonic regression and Platt calibration are typically applied in a setting where the training and test data are assumed to be drawn from the same distribution. In contrast, the shift-aware calibrator in **'AdapTable' is specifically designed to handle distribution shifts,** making it more suitable for real-world scenarios where the test data might differ significantly from the training data.\n\n**Graph Neural Network Integration:** The calibrator in 'AdapTable' utilizes a Graph Neural Network (GNN) to **understand the relationships between different features (columns) of the data.** This approach is particularly beneficial for tabular data, which often comprises a mix of numerical and categorical features, and each feature encompasses a distinctive meaning, unlike pixels within vision domains. Traditional methods do not typically leverage such relational information between features, which can be crucial for accurate calibration in complex datasets.\n\n**Handling Underconfidence and Overconfidence:** The 'AdapTable' method **addresses both underconfidence and overconfidence in predictions,** a significant improvement over traditional methods. Platt calibration, for instance, is often more focused on correcting overconfident predictions and may not be as effective in dealing with underconfidence.\n\nTo address the concerns of the reviewer, we\u2019ve additionally conducted an ablative study by exchanging the calibrator to a. Platt calibrator *(PC)* and b. Isotonic regression *(IR)* in the table below. It is empirically shown that such calibration methods such as Platt Calibration, and Isotonic Regression, denoted as their abbreviations, show suboptimal performance when jointly used with our label shift handler. (DIABETES indicates DIABETES READMISSION dataset.)\n\n| (Acc. %)  | HELOC      | ANES       | DIABETES   | CMC        | MFEAT-PIXEL | DNA        |\n| ---- | ---------- | ---------- | ---------- | ---------- | ----------- | ---------- |\n| Unadapted | 47.0 \u00b11.6 | 79.3 \u00b10.2 | 61.3 \u00b10.1| 53.5 \u00b11.4 | 96.4 \u00b10.2 | 91.4 \u00b11.2 |\n| PC   | 61.6 \u00b1 1.3 | 79.2 \u00b1 0.2 | 61.4 \u00b1 0.3 | 52.1 \u00b1 1.5 | 96.4 \u00b1 0.3  | 91.4 \u00b1 0.8 |\n| IR   | 61.3 \u00b1 1.7 | 79.2 \u00b1 0.2 | 61.0 \u00b1 0.4 | 51.9 \u00b1 1.6 | 96.0 \u00b1 0.3  | 91.3 \u00b1 0.8 |\n| **Ours** | **64.5 \u00b1 0.3** | **79.6 \u00b1 0.1** | **61.7 \u00b1 0.0** | **55.7 \u00b1 2.0** | **97.8 \u00b1 0.2**  | **95.0 \u00b1 0.5** |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234543807,
                "cdate": 1700234543807,
                "tmdate": 1700234887698,
                "mdate": 1700234887698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cgxn1ARtqM",
                "forum": "ws0F5NTzGw",
                "replyto": "CFX4wIjFoz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DhMn [2]"
                    },
                    "comment": {
                        "value": "### [Q2. How is the target label distribution ensured (or ensured with a certain probability) to be correct?]\n\nEnsuring the accuracy of the target label distribution in our 'AdapTable' framework involves two key strategies:\n\n**De-biasing Source Label Distribution:** Grounded in Bayes' theorem, we address the inherent bias in the prediction logits of the source model towards the source label distribution. This is achieved by dividing the prediction logits by the source label distribution, which is obtained prior to the model's deployment. This step effectively debiases the predictions, making them more representative of the actual target distribution rather than being overly influenced by the source data characteristics.\n\n**Iterative Refinement Strategy:** The estimation and adjustment of the target label distribution in 'AdapTable' is an iterative process. As the model encounters more test data, it continually refines its understanding and estimation of the target label distribution. This iterative refinement is key to gradually improving the accuracy of the label distribution estimation, ensuring that the model adapts more effectively to the target domain over time.\n\nWith these strategies, we are able to estimate the target label distribution with a considerable degree of accuracy. The empirical performance of this approach is demonstrated in our paper, particularly in **Figure 5** and **Figure 9** in our revised paper, where we plot the Jensen-Shannon divergence of distributions before and after applying our label distribution handler. This plot illustrates the effectiveness of our method in aligning the model\u2019s predictions with the actual label distribution in the target domain, showcasing the practical utility of our approach in handling label distribution shifts in real-world scenarios.\n\nWe sincerely appreciate the reviewer again for the thoughtful comments."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700234701851,
                "cdate": 1700234701851,
                "tmdate": 1700234701851,
                "mdate": 1700234701851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VRCet9QJDX",
                "forum": "ws0F5NTzGw",
                "replyto": "CFX4wIjFoz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback! Have we adequately resolved the issues raised?"
                    },
                    "comment": {
                        "value": "We express our gratitude for dedicating time to review our paper. In our rebuttal, we have expounded on the contribution of our work, offering a more comprehensive explanation and additional experiments for the proposed framework.\n\nConsidering the limited duration of the author-reviewer discussion phase, we seek your input on whether our primary concerns have been sufficiently tackled. We are prepared to furnish further explanations and clarifications if needed. Thank you sincerely!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570875069,
                "cdate": 1700570875069,
                "tmdate": 1700570875069,
                "mdate": 1700570875069,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qUSLb5o7Bc",
            "forum": "ws0F5NTzGw",
            "replyto": "ws0F5NTzGw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7383/Reviewer_P5dW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7383/Reviewer_P5dW"
            ],
            "content": {
                "summary": {
                    "value": "Tabular data in real-world applications often face distribution shifts, impacting machine learning model performance during testing. Addressing these shifts in tabular data is challenging due to varying attributes and dataset sizes, and limitations of deep learning models for tabular data. To tackle these challenges, the AdapTable method is introduced, which estimates target label distributions and adjusts initial probabilities based on calibrated uncertainty, demonstrating its effectiveness in experiments with real-world and synthetic data shifts using unlabeled test data alone."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. TTT on tabular data has its unique challenge, and authors well clarify this point in Sec.2. I really appreciate such clarification.\n2. Experimental results are extensive and convincing."
                },
                "weaknesses": {
                    "value": "1. Why SHIFT-AWARE UNCERTAINTY CALIBRATOR and  LABEL DISTRIBUTION HANDLER are combined into each other? I mainly  concern on whether your solution looks like a A+B combination. If you can clarify this point, I will be pleased to raise my score."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698583716354,
            "cdate": 1698583716354,
            "tmdate": 1699636883735,
            "mdate": 1699636883735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I3rrziHD7z",
                "forum": "ws0F5NTzGw",
                "replyto": "qUSLb5o7Bc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer P5dW"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the helpful and insightful comments. We address the reviewer's concerns below.\n\n### [W1. Why SHIFT-AWARE UNCERTAINTY CALIBRATOR and LABEL DISTRIBUTION HANDLER are combined into each other? I mainly concern on whether your solution looks like a A+B combination. If you can clarify this point, I will be pleased to raise my score.]\n\nThank you for your constructive feedback regarding the integration of the Shift-Aware Uncertainty Calibrator and the Label Distribution Handler in our **'AdapTable'** framework. We realize we may not have clearly articulated the rationale behind this integration, and we apologize for any confusion caused. To address this issue more comprehensively, we have added additional insights in **section 2.2** of our revised paper.\n\nThe key points summarized in this section are as follows:\n\n**Pivotal Observation:** A critical observation we made is that when the source model is appropriately calibrated \u2013 increasing confidence for correct samples while decreasing it for incorrect ones \u2013 our Label Distribution Handler significantly enhances performance. This improvement is especially notable because it directly tackles the inherent challenge in tabular data, where label distribution shifts are common. This content can be found in Table 1 of our revised paper.\n\n**Challenges in Obtaining Good Calibration:** Achieving effective calibration in scenarios with distribution shifts is a complex task. To address this, we utilize Graph Neural Networks (GNNs) in our Shift-Aware Uncertainty Calibrator. The GNNs are adept at learning the impact of shifts in each column on the label, thus ensuring robust calibration even when distribution changes occur during test-time. This is crucial for our label-shift handler to show effectiveness even under distributional shifts such as: (1) distribution change of certain columns, (2) observation noises on certain features, etc. \n\nIn summary, the combination of the Shift-Aware Uncertainty Calibrator and the Label Distribution Handler in 'AdapTable' is a synergistic approach where **each component complements and enhances the effectiveness of the other.** The calibration provided by the Shift-Aware Uncertainty Calibrator lays the groundwork for the Label Distribution Handler to make more precise adjustments, reflecting the actual label distribution shifts in the target domain. This integrated approach is designed to address the unique challenges posed by both covariate and label shifts in tabular data, providing a comprehensive solution for test-time adaptation.\n\nWe sincerely appreciate the reviewer again for the thoughtful comments."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700230703013,
                "cdate": 1700230703013,
                "tmdate": 1700230703013,
                "mdate": 1700230703013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fuT6xC7SZt",
                "forum": "ws0F5NTzGw",
                "replyto": "qUSLb5o7Bc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback! Have we adequately resolved the issues raised?"
                    },
                    "comment": {
                        "value": "We express our gratitude for dedicating time to review our paper. In our rebuttal, we have expounded on the contribution of our work, offering a more comprehensive explanation and additional experiments for the proposed framework.\n\nConsidering the limited duration of the author-reviewer discussion phase, we seek your input on whether our primary concerns have been sufficiently tackled. We are prepared to furnish further explanations and clarifications if needed. Thank you sincerely!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570859406,
                "cdate": 1700570859406,
                "tmdate": 1700570859406,
                "mdate": 1700570859406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "L0x5XHGHn3",
            "forum": "ws0F5NTzGw",
            "replyto": "ws0F5NTzGw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7383/Reviewer_xrMn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7383/Reviewer_xrMn"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an interesting problem of test-time adaptation for tabular data. This problem is meaningful since tabular data suffers from distribution shift problems while lacking effectiveness in solving them. The authors propose a model-independent test-time adaptation method, which estimates the temperature for each sample during testing and modifies the label distribution of outputs. The experiments show a significant performance improvement on three datasets in the TableShift benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper studies an interesting problem, namely test-time adaptation for tabular data.\n2. The proposed method is suitable for addressing label distribution shifts in tabular datasets."
                },
                "weaknesses": {
                    "value": "1. The experiments in this paper are insufficient. There are 15 datasets in the TableShift benchmark; however, only three of them are considered in the experiments. This makes the results of this paper unconvincing.\n2. The performance improvement on ANES and Diabetes ReadMission is relatively weak in the Supervised setting (which is the setting with the overall best performance). This makes the proposed method weak.\n3. The hyper-parameter of the proposed method lacks thoughtful discussion. For example, the alpha in the label distribution handler determines how quickly the model can adapt to the latest label distribution, which may seriously affect the performance.\n4. This method calibrates the logits with sample-wise temperature and estimated label distribution, which can only handle the label distribution shift problems rather than covariate shift problems.\n5. Minor issue: The left sub-figures in Figures 2 and 5 contain black borders."
                },
                "questions": {
                    "value": "1. The authors should explain why only three datasets are adopted in the experiments and why these three datasets are selected.\n2. The running time of the proposed method should be reported for both training and evaluation.\n3. Please refer to the questions in the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7383/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7383/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7383/Reviewer_xrMn"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733147553,
            "cdate": 1698733147553,
            "tmdate": 1699636883630,
            "mdate": 1699636883630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sC3u8gKMOR",
                "forum": "ws0F5NTzGw",
                "replyto": "L0x5XHGHn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xrMn [1]"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the helpful and insightful comments. We address the reviewer's concerns below.\n\n### [W1, Q1. The experiments in this paper are insufficient. There are 15 datasets in the TableShift benchmark; however, only three of them are considered in the experiments. This makes the results of this paper unconvincing. / The authors should explain why only three datasets are adopted in the experiments and why these three datasets are selected.]\n\nYour concern regarding the selection of only three datasets from the TableShift benchmark for our experiments is indeed valid and warrants a detailed explanation. In 'AdapTable', our focus was on demonstrating the method's effectiveness in real-world scenarios, and the choice of datasets was guided by these objectives.\n\n**Representativeness:**\n\nThe datasets HELOC, Diabetes Readmission, and ANES are foundational and widely recognized in their respective data science domains. Their significant citation counts \u2013 over 2000 for HELOC and DIABETES, and more than 8000 for ANES \u2013 are indicative of their broad acceptance and extensive utilization in research. \n\n**Realistic Scenario and Practical Relevance:**\n\nTest-time adaptation (TTA) is particularly pertinent when domain shifts are prevalent during the deployment stage of models, thus we selected datasets that target a realistic scenario in which (1) deployment of pretrained models, and (2) domain shift during test-time, is likely. Each of the selected datasets \u2013 HELOC, DIABETES, and ANES \u2013 represents a distinct and realistic application of machine learning in the fields of healthcare, finance, and political science, where domain shifts such as regions may directly affect the underlying distribution of data.\n\n**Additional Experiments for Broader Validation:**\n\nAcknowledging the potential limitation of using a relatively small number of datasets for validation, we have conducted additional experiments within the TableShift benchmark. These experiments include datasets varying in size, such as ASSISTments with 2,667,776 observations and Childhood Lead with 27,499 observations. For these experiments, we used the same backbone model (MLP) as in our initial studies. This extension of our experimental evaluation further demonstrates the adaptability and effectiveness of 'AdapTable' across a broader range of scenarios and dataset sizes.\n\nTo verify the validity of ours in \bvarious datasets in TableShift, we conduct extra experiments in TableShift datasets (BRFSS BLOOD PRESSURE, NHANES LEAD, and ASSISMENTS) with MLP backbone, and the results are in the table in the following comment. We confirm that our method is not tailored for some specific TableShift datasets."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215979015,
                "cdate": 1700215979015,
                "tmdate": 1700215979015,
                "mdate": 1700215979015,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KfGECm5vHR",
                "forum": "ws0F5NTzGw",
                "replyto": "L0x5XHGHn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xrMn [2] (Table)"
                    },
                    "comment": {
                        "value": "(1), (2), and (3) indicate dataset BRFSS BLOOD PRESSURE, NHANES LEAD, and ASSISMENTS each.\n\n|  | (1) |  |  | (2) |  |  | (3)|  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | acc | bacc | f1 | acc | bacc | f1 | acc | bacc | f1 |\n| baseline | 53.9  | 49.5  | 47.2  | 92.2  | 50.0  | 48.0  | 58.0  | 62.8  | 54.1  |\n| PL | 54.0  | 49.4  | 46.8  | 92.2  | 50.0  | 48.0  | 58.0  | 62.8  | 54.1  |\n| EM | 49.0  | 47.8  | 47.8  | 92.2  | 50.0  | 48.0  | 58.3  | 63.0  | 54.6  |\n| SAM | 57.2  | 49.6  | 38.5  | 92.2  | 50.0  | 48.0  | 55.9 | 60.7 | 51.4 |\n| SAR | 45.7 | 43.2 | 41.1 | 92.2  | 50.0  | 48.0  | 58.3  | 63.0  | 54.5  |\n| TTT++ | 51.1 | 50.5 | 50.4 | 92.2  | 50.0  | 48.0  | 58.3  | 63.0  | 54.5  |\n| EATA | 49.4  | 51.1  | 49.4  | 92.2  | 50.0  | 48.0  | 58.4  | 63.1  | 54.7 |\n| LAME | 44.6 | 50.3 | 38.0 | 92.2  | 50.0  | 48.0  | 58.3  | 63.0  | 54.5  |\n| Ours | **59.8** | **62.3** | **59.6** | **93.3** | **58.6** | **56.4** | **58.6** | **63.8** | **54.9** |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219006557,
                "cdate": 1700219006557,
                "tmdate": 1700219006557,
                "mdate": 1700219006557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aL2CugA3O5",
                "forum": "ws0F5NTzGw",
                "replyto": "L0x5XHGHn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xrMn [3]"
                    },
                    "comment": {
                        "value": "### [W2. The performance improvement on ANES and Diabetes ReadMission is relatively weak in the Supervised setting (which is the setting with the overall best performance). This makes the proposed method weak.]\n\nAs discernible from the main paper text or algorithm names, architectures labeled 'supervised' in the Table 1 and Table 2 are currently not adopted in other modalities but are still embraced in the tabular domain due to their performance, representing relatively classical algorithms that diverge from modern deep learning practices. Furthermore, a comparison focusing solely on the highest performances across the entire table indicates that these methods, as commonly known, continue to be adopted due to their ability to surpass deep learning performance (regardless of the architecture of deep learning models). When dealing with tabular data in a data-wise manner, many issues arise from this, the difficulty in utilizing recent deep learning-based research analyses and their derivatives, including test time adaptation. However, our approach differs from other TTA methods by not requiring updates or gradients from the main model. Our method is capable of orthogonal utilization even with algorithms that are SoTA but do not seamlessly align with deep learning methodologies. Despite its amount, this has resulted in performance improvements in numerous cases, demonstrating that using our method alongside classical algorithms is not only feasible but also a rational choice.\n\n### [W3. The hyper-parameter of the proposed method lacks thoughtful discussion. For example, the alpha in the label distribution handler determines how quickly the model can adapt to the latest label distribution, which may seriously affect the performance.]\n\nTo assess the robustness of AdapTable against hyperparameter configurations, we conduct an exhaustive hyperparameter sensitivity analysis covering all test-time parameters, including the smoothing factor $\\alpha$, low uncertainty quantile $q_{\\text{low}}$, and high uncertainty quantile $q_{\\text{high}}$. Specifically, in our experiments utilizing MLP on HELOC dataset, we perform hyperparameter optimization by varying one parameter while keeping the others fixed at the identified optimal setting, *i.e.*, $(\\alpha, q_{\\text{low}}, q_{\\text{high}}) = (0.1, 0.25, 0.75)$. Notably, as Figure 7 exhibits, our findings reveal that the adaptation performance remains insensitive to alterations in all three types of hyperparameters, particularly when varying $q_{\\text{low}}$, demonstrating minimal performance fluctuations. Furthermore, for the smoothing factor $\\alpha$ and high quantile $q_{\\text{high}}$, we pinpoint sweet spots at $[0, 0.2]$ and $[0.5, 0.6]$, respectively. This observation underscores the adaptability of our approach, allowing flexible hyperparameter selection and demonstrating generalizability across diverse test conditions. This stands in stark contrast to the hyperparameter sensitivity exhibited by the tent, as depicted in Figure 8 in our revised appendix of paper. Notably, regardless of an extensive hyperparameter search in the tabular domain for the tent, the performance post-adaptation fails to surpass the unadapted performance, as evidenced by our main table experiment results in Table 2 and Table 3.\n\n### [W4. This method calibrates the logits with sample-wise temperature and estimated label distribution, which can only handle the label distribution shift problems rather than covariate shift problems.]\n\nWhile our approach does not explicitly target covariate shifts, **the nature of the shift in tabular data differs from that in visual data, where a specific corruption can easily be applied to samples while preserving class information.**\n\nIn most cases in tabular data, a shift in input space is highly correlated with the shift in the target label  - as shown in the table in the following comment. For instance, in a tabular dataset concerning patient and health information, a shift in the age range of patients directly affects other features and output labels. Our method also deals with covariate shifts by modeling unobserved shifts through the interdependence of shift values during shift-aware calibration. Additionally, by leveraging GNNs to learn the dependence of each column\u2019s shifts, our approach demonstrates superior performance even in synthetic shift scenarios such as random noise or column missing, which are label-shift-free, highlighting its effectiveness in handling covariate shifts.\n\n### [W5. Minor issue: The left sub-figures in Figures 2 and 5 contain black borders.]\n\nWe sincerely thank you for letting us resolve this issue. We update Figure 2 and Figure 5 without black borders in our new draft."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700221387732,
                "cdate": 1700221387732,
                "tmdate": 1700221387732,
                "mdate": 1700221387732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "csssl7H5Yu",
                "forum": "ws0F5NTzGw",
                "replyto": "L0x5XHGHn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xrMn [4] (Table)"
                    },
                    "comment": {
                        "value": "| (%)                  |       | Class 0 | Class 1 |\n| -------------------- | ----- | ------- | ------- |\n| **HELOC**                | Train | 75.9    | 24.1    |\n|| Valid                | 80.2  | 19.8    |\n|| Test                 | 43.1  | 56.9    |\n| **ANES**                 | Train | 31.8    | 68.2    |\n|| Valid                | 32.3  | 67.7    |\n|| Test                 | 40.1  | 59.9    |\n| **DIABETES READMISSION** | Train | 57.6    | 42.4    |\n|| Valid                | 57.8  | 42.2    |\n|| Test                 | 50.6  | 49.4    |\n| **BRFSS BLOOD PRESSURE** | Train | 59.7    | 40.3    |\n|| Valid                | 60.1  | 39.9    |\n|| Test                 | 41.9  | 58.1    |\n| **NHANES LEAD**          | Train | 97.3    | 2.7     |\n|| Valid                | 96.6  | 3.4     |\n|| Test                 | 92.2  | 7.8     |\n| **ASSISMENTS**           | Train | 30.6    | 69.4    |\n|| Valid                | 30.7  | 69.3    |\n|| Test                 | 56.3  | 43.7    |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222418132,
                "cdate": 1700222418132,
                "tmdate": 1700222418132,
                "mdate": 1700222418132,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O5vRgxDDyq",
                "forum": "ws0F5NTzGw",
                "replyto": "L0x5XHGHn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xrMn [5]"
                    },
                    "comment": {
                        "value": "### [Q2. The running time of the proposed method should be reported for both training and evaluation.]\n\nIn order to show the efficiency of the proposed AdapTable, we perform a computational efficiency comparison between test-time adaptation baselines and AdapTable in **Figure 6**. The averaged adaptation time is calculated by averaging adaptation time over all test instances in CMC dataset corrupted by Gaussian noise. We find that the adaptation time of AdapTable ranks third among eight TTA methods, by showcasing its computational tractability. Furthermore, we observe that our approach requires significantly less adaptation time compared to TTA baselines such as TTT++[1], SAM[2], EATA[3], and SAR[4], despite constructing shift-aware graph and incorporating a single forward pass for GNN with negligible extra cost of adjusting output label estimation are required. This can be attributed to the fact that the graph we generate places each column as a node, resulting in a graph of a very small scale -- typically ranging from tens to hundreds of nodes. This minimizes the cost of message passing in GNN forward process, while other baselines iterate through multiple adaptation steps with forward and backward processes, leading to increased computational expenses. \n\nFurthermore, we also provide GNN post-training time of AdapTable under different scales in Table 5, encompassing small-scale (CMC, MLP), medium-scale (HELOC, FT-Transformer[5]), and large-scale (Diabetes Readmission, TabNet[6]). GNN post-training requires only a few seconds for small- and medium-scale settings, and notably, it remains negligible, even in our largest experimental setting.\n\nWe sincerely appreciate the reviewer again for the thoughtful comments.\n\n\n[1] Yuejiang Liu, et al. \"TTT++: When does self-supervised test-time training fail or thrive?\" NeurIPS 2021.\n\n[2] Pierre Foret, et al. \"Sharpness-aware minimization for efficiently improving generalization\" ICLR 2021.\n\n[3] Niu, Wu, Zhang, et al. \"Efficient test-time model adaptation without forgetting\" ICML 2022.\n\n[4] Niu, Wu, Zhang, et al. \"Towards stable test-time adaptation in dynamic wild world\" ICLR 2023.\n\n[5] Yury Gorishniy, et al. \"Revisiting deep learning models for tabular data\" NeurIPS 2021.\n\n[6] Sercan \u00d6 Arik, et al. \"TabNet: Attentive interpretable tabular learning\" AAAI 2021."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222566568,
                "cdate": 1700222566568,
                "tmdate": 1700222566568,
                "mdate": 1700222566568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7VSx15W1mV",
                "forum": "ws0F5NTzGw",
                "replyto": "L0x5XHGHn3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback! Have we adequately resolved the issues raised?"
                    },
                    "comment": {
                        "value": "We express our gratitude for dedicating time to review our paper. In our rebuttal, we have expounded on the contribution of our work, offering a more comprehensive explanation and additional experiments for the proposed framework.\n\nConsidering the limited duration of the author-reviewer discussion phase, we seek your input on whether our primary concerns have been sufficiently tackled. We are prepared to furnish further explanations and clarifications if needed. Thank you sincerely!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570839653,
                "cdate": 1700570839653,
                "tmdate": 1700570839653,
                "mdate": 1700570839653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lWh5vcFnCE",
            "forum": "ws0F5NTzGw",
            "replyto": "ws0F5NTzGw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7383/Reviewer_A2qG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7383/Reviewer_A2qG"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the test-time adaptation problem for the tabular data. Specifically, the authors discuss the challenges related to test-time adaptation for tabular data and propose a new method including two modules: a shift-aware uncertainty calibration module and a label distribution handler. Experimental results show the proposal can improve learning performance on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This paper studies the problem of adaptation for tabular data. This problem is important yet under-studied.\n\n2) The authors conduct a large number of experiments and the results show that the proposal can improve performance."
                },
                "weaknesses": {
                    "value": "1) Although the proposal can improve the performance, it is mainly a combination of some existing techniques. There is neither good theoretical analysis nor much inspiration, thus, the novelty and contribution are limited. From my personal point of view, I don\u2019t really appreciate papers that improve performance by integrating multiple tricks and existing methods.\n2) For tabular data, the shift may exist in various perspectives, such as the distribution shift, the feature dimension (new features occur or old features are lost), and class space varies. These problems should discussed separately.\n3) Will the introduction of GCN in the method lead to a higher computational complexity of the algorithm? This should be discussed theoretically or empirically."
                },
                "questions": {
                    "value": "As discussed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698850016357,
            "cdate": 1698850016357,
            "tmdate": 1699636883527,
            "mdate": 1699636883527,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Anxuh6We59",
                "forum": "ws0F5NTzGw",
                "replyto": "lWh5vcFnCE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer A2qG [1]"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the helpful and insightful comments. We address the reviewer's concerns below.\n\n### [W1. Although the proposal can improve the performance, it is mainly a combination of some existing techniques. There is neither good theoretical analysis nor much inspiration, thus, the novelty and contribution are limited. From my personal point of view, I don\u2019t really appreciate papers that improve performance by integrating multiple tricks and existing methods.]\n\nWe conducted extensive experiments to understand the characteristics of deep tabular learned models and the features of domain shifts in the tabular data. Our findings can be summarized as follows: \n1) Deep-learned models for tabular data are often observed to be uncalibrated in terms of uncertainty, exhibiting an overall underconfident nature, in contrast to the widely recognized overconfident behavior of vision domains. (This discovery, especially considering that prior techniques in TTA and domain adaptation predominantly stemmed from vision-based research with many early methods relying on smoothness assumptions like EM and pseudo-labeling, underscores the value of this work.) \n2) Tabular domain shifts, unlike in the vision domain as well, affect the actual output label in most cases, resulting in discernible differences in label distributions between training and testing. While methods addressing label distribution shifts in vision domains have emerged recently, the task itself simulates situations where the output label distribution is adjusted without the cases that the shifts affect the output label, as seen in scenarios like Cifar-corrupted or ImageNet-corrupted datasets. It is crucial to emphasize the distinct nature of tabular data, where shifts in input (covariate) directly influence label and label distribution shifts, prompting the proposal of observation-based, tabular-specific modules capable of handling these shifts. \n3) When utilizing label distribution shift handlers in deep learning models, the effectiveness of the label distribution handler amplifies when the output of the deep learning model is well-calibrated in terms of uncertainty, which is a relatively well-known fact than previous ones. This statement holds in tabular settings, where this effect appears to be substantial. In our revised paper, **Table 1** indicates that if the source model is perfectly calibrated by increasing the confidence for correct samples while decreasing the confidence for incorrect samples, our label distribution handler leads to a remarkable improvement in performance. These interconnected observations emphasize the need for a calibrator tailored for tabular data and a unique handler to address shifts specific to the tabular domain. This elucidates the essential interdependence between both in addressing the nuanced shifts observed in tabular datasets.\n\n### [W2. For tabular data, the shift may exist in various perspectives, such as the distribution shift, the feature dimension (new features occur or old features are lost), and class space varies. These problems should be discussed separately.]\n\nLike many previous test-time adaptation (TTA) works, our primary focus in the 'AdapTable' paper is on handling distribution shifts. These shifts pertain to changes in the data distribution between the training (source) and testing (target) phases, without affecting the input dimension or the number of features. This is a critical point of distinction, as our method is designed under the assumption that the input features (feature space) and the output classes (class space) remain fixed throughout.\n\nSpecifically, our method is designed to handle column-wise distributional shifts(corresponding to splitting training and test data with respect to the distribution of important columns), but has shown its effectiveness in handling other shifts as well such as noise(gaussian and random noise), and missing data (random drop, column drop), as demonstrated in our experimental section. In which all cases do not incur dimensional changes nor additional classes during testing. It should be noted that for missing features, imputation was done to match the feature dimensions of the input.\n\nIn summary, **AdapTable** is designed to address distributional shifts within tabular data where the dimensionality of input features and number of output classes remain constant - aligning with previous works in TTA."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700213951114,
                "cdate": 1700213951114,
                "tmdate": 1700215567610,
                "mdate": 1700215567610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gddTAEdEZX",
                "forum": "ws0F5NTzGw",
                "replyto": "lWh5vcFnCE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer A2qG [2]"
                    },
                    "comment": {
                        "value": "### [W3. Will the introduction of GCN in the method lead to a higher computational complexity of the algorithm? This should be discussed theoretically or empirically.]\n\nIn order to show the efficiency of the proposed AdapTable, we perform a computational efficiency comparison between test-time adaptation baselines and AdapTable in **Figure 6**. The averaged adaptation time is calculated by averaging adaptation time over all test instances in CMC dataset corrupted by Gaussian noise. We find that the adaptation time of AdapTable ranks third among eight TTA methods, by showcasing its computational tractability. Furthermore, we observe that our approach requires significantly less adaptation time compared to TTA baselines such as TTT++ [1], SAM [2], EATA [3], and SAR [4], despite constructing shift-aware graph and incorporating a single forward pass for GNN with negligible extra cost of adjusting output label estimation are required. This can be attributed to the fact that the graph we generate places each column as a node, resulting in a graph of a very small scale -- typically ranging from tens to hundreds of nodes. This minimizes the cost of message passing in GNN forward process, while other baselines iterate through multiple adaptation steps with forward and backward processes, leading to increased computational expenses. \n\nFurthermore, we also provide GNN post-training time of AdapTable under different scales in Table 5, encompassing small-scale (CMC, MLP), medium-scale (HELOC, FT-Transformer [5]), and large-scale (Diabetes Readmission, TabNet [6]). GNN post-training requires only a few seconds for small- and medium-scale settings, and notably, it remains negligible, even in our largest experimental setting.\n\nWe sincerely appreciate the reviewer again for the thoughtful comments.\n\n\n[1] Yuejiang Liu, et al. \"TTT++: When does self-supervised test-time training fail or thrive?\" NeurIPS 2021.\n\n[2] Pierre Foret, et al. \"Sharpness-aware minimization for efficiently improving generalization\" ICLR 2021.\n\n[3] Niu, Wu, Zhang, et al. \"Efficient test-time model adaptation without forgetting\" ICML 2022.\n\n[4] Niu, Wu, Zhang, et al. \"Towards stable test-time adaptation in dynamic wild world\" ICLR 2023.\n\n[5] Yury Gorishniy, et al. \"Revisiting deep learning models for tabular data\" NeurIPS 2021.\n\n[6] Sercan \u00d6 Arik, et al. \"TabNet: Attentive interpretable tabular learning\" AAAI 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214135503,
                "cdate": 1700214135503,
                "tmdate": 1700739832835,
                "mdate": 1700739832835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Dh3g0iP5Ol",
                "forum": "ws0F5NTzGw",
                "replyto": "lWh5vcFnCE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the feedback! Have we adequately resolved the issues raised?"
                    },
                    "comment": {
                        "value": "We express our gratitude for dedicating time to review our paper. In our rebuttal, we have expounded on the contribution of our work, offering a more comprehensive explanation and additional experiments for the proposed framework.\n\nConsidering the limited duration of the author-reviewer discussion phase, we seek your input on whether our primary concerns have been sufficiently tackled. We are prepared to furnish further explanations and clarifications if needed. Thank you sincerely!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570800313,
                "cdate": 1700570800313,
                "tmdate": 1700570800313,
                "mdate": 1700570800313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]