[
    {
        "title": "Understanding Certified Training with Interval Bound Propagation"
    },
    {
        "review": {
            "id": "SbliAA4cd3",
            "forum": "h05eQniJsQ",
            "replyto": "h05eQniJsQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8059/Reviewer_KaHF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8059/Reviewer_KaHF"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a notion of IBP tightness, defined as the ratio between the optimal output interval bounds and those obtained via IBP.\nA series of technical results, mostly on Deep Linear Networks (DLN) is presented, describing the requirements for perfect tightness (\"propagation invariance\") and the influence of width, depth and IBP training on tightness.\nExperimental results supporting the technical results are shown."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The research goal, providing further understanding to certified training and on the role of IBP in state-of-the-art methods, is of great interest to the community.\nThe paper is well-written and technical results are relatively coherent and well-structured.\nThe results detailing conditions for propagation invariance are novel and of potential interest to the community."
                },
                "weaknesses": {
                    "value": "While the motivation is great, I am not sure this work is an actual step forward in understanding certified training.\n\nMost of the results are either fairly obvious within the certified training community (at initialisation tightness will increase with width and depth) or follow from relevant and non-cited previous work on adversarial robustness [1] (for trained networks, width will help but depth won't).\nThe authors repeatedly overclaim: I do not see why this work would \"pave the way for further advances in certified training\", or \"explain the success of recent certified training methods\". For instance, concerning the latter, the SABR paper already shows quite clearly (in its Figure 7) that strong IBP regularization is not required to obtains state-of-the-art results.\nI am also not fully convinced by the repeated arguments being made about the relevance of DLNs in this context. As it is also clear in the paper, if a network was locally linear in a perturbation region, any linear relaxation would imply exact certification. This is clearly extremely far from the networks that yield state-of-the-art certified training results, which typically require the use of branch-and-bound post-training to reach high certified accuracy (e.g., SABR).\nFinally, the presented improvements over literature results (table 1) are somewhat overstated: for SABR, the best-performing methods amongst the ones considered, they are not even necessarily a strict improvement on both standard and certified accuracy (MNIST 0.3) and they come at the cost of memory and runtime overhead. Would these result hold on different perturbation magnitudes, for instance MNIST 0.1 and CIFAR-10 8/255?\n\n[1] Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization), Zhu et al., NeurIPS 2022"
                },
                "questions": {
                    "value": "The main question I had throughout the paper is: why would this notion of tightness be any more useful than measuring the standard IBP loss of a network? Tightness does not necessarily imply robustness, as IBP bounds could be very close to optimality but the network may still display adversarial examples. On the other hand, a lower IBP loss will imply robustness. For instance, the custom initialization by (Shi et al., 2021) clearly improves certified training performance (as it brings down the IBP loss at initialisation) but this is not captured by the technical results on tigthness (as stated after corollary 3.8).\nFurthermore, I am under the impression that most of the plots of the paper about tightness would apply to the (inverse of) the IBP loss too: decrease with width, depth, increase with IBP training and so on.\nI believe this should be addressed and made an integral part of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698581855212,
            "cdate": 1698581855212,
            "tmdate": 1699636996533,
            "mdate": 1699636996533,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KrhJu8tEyZ",
                "forum": "h05eQniJsQ",
                "replyto": "SbliAA4cd3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8059/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KaHF Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their interesting questions, which we address below, and are happy to hear that they believe both the problem we work on as well as our novel results to be of interest to the community.\n\n\n**Q1. Given that tightness does not directly imply robustness, why is the notion of tightness, introduced in this work, useful beyond the standard IBP loss? Further, does it yield any different trends than (inverse) IBP loss?**  \n\nAs this question is the Reviewer\u2019s main concern, we break it down into multiple sub-questions and address them individually:\n\n*Q1.1: Does a low IBP loss imply robustness?*  \nNot necessarily. For example, if the minimal logit difference to the runner-up class is 0, we can obtain a robust cross-entropy loss of $log(2)=0.7$, which is lower than the losses typically achieved with IBP training [1]. Similarly, as we scale the final layer weights towards 0, the IBP loss converges to $log(K)$, which is less than the IBP loss at initialization (see the new Figure 15) but will hurt certified training. Finally, networks trained with SABR often have a 10x larger IBP loss than IBP-trained networks, while actually being more robust (see Figure 10 in M\u00fcller et al. [1]). We thus conclude that a small IBP loss is neither necessary nor sufficient for robustness, and state-of-the-art SABR-trained networks in fact exhibit very high IBP losses.\n\n*Q1.2: Does the initialization of Shi et al., 2021 [2] improve certified training performance by reducing IBP loss (at initialization)?*  \nWe want to highlight that while Shi et al.\u2019s initialization reduces IBP loss at initialization (although the first epoch (for CIFAR-10) does not use an IBP loss), they mostly prevent gradient explosions during the epsilon annealing phase of the training schedule thus allowing for much faster annealing and training schedules (see their Section 3.2.1). In fact, just using their initialization without additional regularizers can even hurt final performance (see their Table 4).\n\n*Q1.3: Do tightness and inverse IBP loss exhibit similar trends with width, depth, and perturbation magnitude?*  \nNo. In the new Figure 15 in Appendix D.4, we compare tightness and inverse BP loss over different depths and widths at initialization (using the de facto standard initialization by Shi et al. [2]) and observe opposite trends. The inverse IBP loss increases slightly with depth and stays roughly constant with width, while tightness decreases by multiple orders of magnitude in both cases. This is generally expected, as Shi et al.\u2019s [2] initialization aims to keep the box size constant throughout the network. We have further computed the IBP loss for the settings and networks considered in Figure 7 using either a constant or the same epsilon as for training and evaluating the certified accuracy (Figure 16). We find the inverse IBP loss depends strongly on the epsilon used for evaluation. When using the same epsilon as for training (which we believe to be the most natural choice), we observe the largest inverse IBP losses for the smallest perturbation magnitudes, decreasing as perturbation magnitudes increase. This is the exact opposite of what we observe for tightness.\nIn summary, inverse IBP loss shows completely different trends over architecture choices and (training) perturbation magnitudes. We have thus established the complementarity of tightness and the (inverse) IBP loss.\n\n*Q1.4: What is the value of the tightness metric as an analysis tool?*  \nTightness is the (to the best of our knowledge) first metric to consider the ratio of optimal and relaxed bound sizes in neural network verification. This allows us to directly quantify how strongly regularized a network is (or would need to be) for (IBP) certifiability and what methodological choices dominate this regularization strength. In contrast to the IBP loss, tightness disentangles robustness from accuracy. This is of particular importance as regularization for robustness, in particular higher tightness, often reduces accuracy, making their interaction hard to analyze. \nThis perspective is, e.g., key to the interpretation of the results in Figure 5, suggested by Theorem 3.10, and leads to our architectural modifications that yield state-of-the-art results (Table 1). See also our answer to Q2 for concrete insights into, e.g., the mechanisms behind SABR training.\n\n**References:**  \n[1] M\u00fcller et al. \"Certified training: Small boxes are all you need.\" ICLR 2023  \n[2] Shi et al. \"Fast certified robust training with short warmup.\" NeurIPS 2021"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700239557802,
                "cdate": 1700239557802,
                "tmdate": 1700239557802,
                "mdate": 1700239557802,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zdpzuBcupz",
            "forum": "h05eQniJsQ",
            "replyto": "h05eQniJsQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8059/Reviewer_C4Wz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8059/Reviewer_C4Wz"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the interval bound propagation-based (IBP-based) training method, one of the most popular approaches to obtaining neural networks with certifiable guarantees. Although existing work demonstrated the effectiveness of this approach, the theoretical understanding of IBP is limited. Under the assumption of linear neural networks and Gaussian weights, this paper proposed a new measure of bound tightness and derived a few theorems to show how the bound tightness changes when propagating among linear layers, and how the width and depths of a randomly initialized network impact bound tightness.\n\nSome empirical results on a few MNIST and CIFAR-10 models demonstrate that certified training indeed improves the tightness measure proposed in this paper. Some interesting empirical results were demonstrated with models varying depths and widths, demonstrating their correlations with bound tightness and accuracy.\n\nThe paper studies an important topic with some novel results, however, its current version has some weaknesses and unresolved questions, see below, so I feel the current version of the paper is below the acceptance threshold."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The topic of the paper is relevant, and it is an open challenge. We still don\u2019t understand certified training very well, and this paper is a great attempt to bring in some new understanding.\n\n2. Some novel theoretical insights are given, such as on the tightness of bound propagation and propagation invariance. Also, the growth of the bounds under initialization and its relation with model width may be a useful result to guide practical training.\n\n3. The bound propagation invariance condition may lead to new insights into the design of neural network architecture to make the bounds tighter.\n\n4. Some results are extended to a 2-layer ReLU network, although this part was not emphasized in the paper."
                },
                "weaknesses": {
                    "value": "1. The theoretical results have strong assumptions such as linear neural networks, and neural network weights under Gaussian distribution. This is generally not a big concern if the authors can demonstrate that these theoretical insights can lead to great practical improvements, \n\n2. but here the theoretical results developed do not lead to a better model that can outperform existing approaches, and some evaluations are quite weak (e.g., on a single MNIST model only). Since only a few models and networks are shown, it is unsure how general the results are.\n\n3. Although some empirical results are shown to support the theory for ReLU networks, it is hard to argue these observations are indeed the consequence of the theory. For example, \u201ccertified training increases tightness\u201d and \u201clarger networks lead to better accuracy\u201d are very generic conclusions, and it is hard to convince the readers that they result from the theory developed in this work."
                },
                "questions": {
                    "value": "1. Figure 6 shows that increasing model width is beneficial, however, it is on a simple MNIST network. Can you demonstrate this result on larger networks and datasets? In particular, if we use a state-of-the-art method and model, and enlarge the model size by 4x, how much gain can we obtain over the state-of-the-art? Is the gain consistent over multiple models (e.g., CNN, resnet), training methods (IBP, CROWN-IBP, SABR, MTL-IBP), and datasets (CIFAR10, CIFAR100, TinyImageNet)?\n\n2. Based on Theorem 3.4, can we reparameterize the network such that the bounds are always tight, and the training process just needs to search from a subspace of weights that lead to tight bounds, rather than using gradient descent to enforce tight bounds? For L2 norm certified defense, the state-of-the-art methods use this approach (such as orthogonal convolution layers and their variants)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654014395,
            "cdate": 1698654014395,
            "tmdate": 1699636996410,
            "mdate": 1699636996410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VRtAiK46gn",
                "forum": "h05eQniJsQ",
                "replyto": "zdpzuBcupz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8059/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C4Wz Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their interesting questions and thoughtful suggestions. We are encouraged to hear that they consider our work to be both relevant and novel. Below, we address their remaining questions.\n\n**Q1. Can you discuss the implications of the (strong) assumptions (DLNs and Gaussian weight distributions) made during the theoretical analysis?**  \n\nFirst, we want to highlight that the definitions of propagation invariance and bound tightness (Lemma 3.3 and Definition 3.6) do not assume a Gaussian weight distribution. Only our results on tightness at initialization make this assumption. There, however, it agrees well with commonly used initialization schemes, which draw weights from a Gaussian distribution.\nSecond, while we do assume (local) linearity for many of our theoretical results, we want to highlight that ReLU networks are locally linear and thus behave linearly for infinitesimally small perturbation magnitudes. Even at the typically considered perturbation magnitudes, as few as 1% of ReLUs are unstable, leading to empirically small approximation errors (see Figure 2 and the new Figure 13). As we empirically validate all our results on ReLU networks and even generalize some of them theoretically to this setting (see Corollary A.2), we believe the insights obtained under the assumption of local linearity to be valuable and likely to generalize.\n\n**Q2. Can you discuss the generality of the empirical support of your theoretical findings given that some experiments are conducted for only one dataset and network architecture?**  \n\nAs most of our experiments involve sweeps over either architectures or perturbation magnitudes and certification with a precise neural network verifier, they can be quite compute-intensive. We have thus focused on what we believe to be the most informative and interesting settings, using the popular CNN7 (which yields state-of-the-art results in most settings) when trying to demonstrate performance improvements and a smaller CNN3 when computing exact bounds using MILP or aiming to certify PGD trained networks, which would be intractable for larger architectures. \nIn addition, we confirm key results across multiple training methods and datasets (see below), e.g., while the results in Figure 6 already constitute a new state-of-the-art, we confirm it on CIFAR-10 in Table 1, where we also observe strict improvements on multiple state-of-the-art training methods. We have added new results (Figures 13 and 14) to the appendix confirming the results previously established in only one setting (Figures 2 and 9).\nBelow, we provide an overview of results and settings we considered. \n\nIf the reviewer believes any results to still be insufficiently established, we are more than happy to conduct additional experiments to address this.\n\n*Approximation error between local (Definition 3.6) and true tightness:* CNN3 on MNIST for 5 perturbation magnitudes (Figure 2). Now additionally for CIFAR-10 (Figure 13)\n\n*Tightness at initialization (Lemma 3.7 and Corollary 3.8, rigorous theoretical results):* 11 architectures based on CNN7 on CIFAR-10, although this is mostly dataset and completely perturbation magnitude independent, as we consider the network at initializtion (Figure 3).\n\n*Low dimensional embedding and reconstruction (Theorem 3.10, rigorous theoretical results):* 19 toy datasets constructed from projections of multivariate standard Gaussians (Figure 4).\n\n*Effect of depth on trained networks:* 6 architectures derived from CNN7 for CIFAR-10 at $\\epsilon=2/255$ (Figure 5).\n\n*Effect of width on trained networks:* 6 architectures derived from CNN7 for CIFAR-10 at $\\epsilon=2/255$ (Figure 5) and 3 training methods, two width factors, and 2 datasets (all using state-of-the-art networks as baseline) (Table 1 and Figure 6), now extended to 3 datasets and 5 perturbation magnitudes.\n\n*Effect of perturbation magnitude on trained networks (Theorem 3.9):* 8 perturbation magnitudes and 3 training methods for CNN3 and CIFAR-10 (Figure 7). 2 Perturbation magnitudes and 5 training methods for a state-of-the-art architecture from prior work (Table 2).\n\n*Regularization strength at same perturbation magnitude:* 2 training methods with 4 regularization strengths each for CNN7 on CIFAR-10 (Figures 8 and 12).\n\n*Perturbation magnitude vs propagation region size:* SABR training with 3 different lambda and 6 perturbation region sizes each for a CNN3 on CIFAR-10 (Figure 9), now also for MNIST (Figure 14).\n\n**References:**  \n[1] M\u00fcller et al. \"Certified training: Small boxes are all you need.\" ICLR 2023    \n[2] Shi et al. \"Fast certified robust training with short warmup.\" NeurIPS 2021    \n[3] Mao et al. \"Connecting Certified and Adversarial Training.\" NeurIPS 2023    \n[4] Singh et al. \"An abstract domain for certifying neural networks.\" POPL 2019"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238463302,
                "cdate": 1700238463302,
                "tmdate": 1700238463302,
                "mdate": 1700238463302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "L8InrTirVU",
                "forum": "h05eQniJsQ",
                "replyto": "0KmCEyDYro",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8059/Reviewer_C4Wz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8059/Reviewer_C4Wz"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed response. Still having concerns about the paper and cannot support it."
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. I greatly appreciate the newly added results on different epsilon and models.\n\nAfter reading the author's response and comments from other reviewers, my main concern is the same as reviewer KaHF:\n\n> Most of the results are either fairly obvious within the certified training community (at initialisation tightness will increase with width and depth) or follow from relevant and non-cited previous work on adversarial robustness [1] (for trained networks, width will help but depth won't). The authors repeatedly overclaim\n\nI've read the response carefully, but to be honest, I am still not fully convinced by the claims made in this paper. The theoretical results and the experiments are somewhat disconnected, and it is hard to argue whether the observations are due to theoretical analysis.\n\nI appreciate the great effort the authors have made during the response period, so I will not decrease my score. Unfortunately, this key issue is still unsolved, so I cannot support the acceptance of this paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715589971,
                "cdate": 1700715589971,
                "tmdate": 1700715589971,
                "mdate": 1700715589971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ckVwmKEQO3",
            "forum": "h05eQniJsQ",
            "replyto": "h05eQniJsQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8059/Reviewer_HbFM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8059/Reviewer_HbFM"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the certified training from the theoretical perspective and applies it to explain and understand the mechanism of certified training with IBP in robustness certification. The idea of propogation tightness is formulated to analyze how IBP works and extensive experiments validate the theories from different aspects, including network width and depth, tightness and accuracy, etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The motivation of the work makes sense to me and the theory is sound, especially I like the formulation of tightness in terms of optimal box and layerwise box.\n- The paper is generally well-written and easy to read, and there are some easy examples to help the audience follow.\n- The experiments are comprehensive, which mostly validates the theory part and gives many interesting insights for certified training."
                },
                "weaknesses": {
                    "value": "-  Although there are some examples in the introduction and formulation, the theory details lack some intuitive insights or explanations, e.g. Theorem 3.9 needs more insights to make it intuitive as it is one of the core theorems in this work.\n- The details of the experiments are not given in the main text; specifically, the datasets and models used in Fig. 3 are not clear. It is better to re-organize experiments part by adding a setup subsection for these necessary details.\n- Why is the certified accuracy decreasing when $\\epsilon$ increases in the middle figure in Fig. 7? More explanations and justifications are needed. Besides, it seems that there is no explanation of the trade-off between accuarcy and robustness as claimed in the abstract and introduction, excpet for tightness defined in the work, how about the certified robustness (e.g. certified accuracy or certifed radii) for the trade-off?"
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8059/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8059/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8059/Reviewer_HbFM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811936382,
            "cdate": 1698811936382,
            "tmdate": 1699636996265,
            "mdate": 1699636996265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WXyJbqdjBw",
                "forum": "h05eQniJsQ",
                "replyto": "ckVwmKEQO3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8059/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HbFM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their interesting questions and thoughtful suggestions. We are encouraged to hear that they consider our work to be well-motivated and our experiments comprehensive. Below, we address their remaining questions.\n\n**Q1. Can you add more intuitions for the theoretical results you prove, e.g., Theorem 3.9?**  \nWhile many of the proofs are quite technical, the results are intuitive, we have thus focused on providing intuitions on the results rather than the proof techniques.   \nFor example, the proof of Theorem 3.9 uses a first-order Taylor approximation (in perturbation magnitude) of the robust risk to show that the gradient change $R(\\epsilon + \\Delta \\epsilon) - R(\\epsilon)$ with increasing $\\epsilon$ is aligned with the tightness gradient. We then sum over many perturbation magnitude increments $\\Delta \\epsilon$, and use the linearity of the inner product to obtain that $R(\\epsilon) - R(0)$ is also aligned with the tightness gradient. As we let $\\Delta$ go against 0, we obtain our result.   \nThe best intuition we can give is that every increase of the perturbation magnitude makes the gradient more aligned with tightness.\n\n**Q2. Can you provide more details on the experimental setup? For example, the setting of Fig. 3 remains unclear.**  \nOf course! We have added the key details for every experiment (in particular for Figures 2, 3, and 9) to the main text. However, as we consider a large number of training methods and multiple datasets, each with different (default for these methods) hyperparameters, we defer additional details to Appendix C (almost one page) due to space constraints. \n\n**Q3. Why is the certified accuracy decreasing as $\\epsilon$ increases in the middle part of Figure 7?**  \nIn Figure 7, we vary the $\\epsilon$ used for both training and testing. Thus, the certified accuracy in the middle of Figure 7 is w.r.t. larger perturbation magnitudes as $\\epsilon$ increases, which makes it a strictly harder task. Thus two effects come together. First, standard accuracy decreases (Figure 7 left) as networks are more heavily regularized for these larger perturbation magnitudes (see increases in tightness in Figure 7 right). Second, for the same network, every sample that is robust at $\\epsilon$ is also robust at $\\epsilon\u2019 < \\epsilon$ but not vice versa, thus certified accuracy is always monotonically decreasing with perturbation magnitude (for a fixed network).\n\n**Q4: Can you provide a more detailed explanation of your results on the robustness accuracy trade-off and in particular how your results on tightness relate to certified accuracy?**  \nYes. We argue along the following lines: To achieve high certified accuracies, certified training is required*. Empirically, IBP-based training methods have proven the most successful, yielding state-of-the-art results in every setting [1,2,3]. However, training with IBP increases tightness (Theorem 3.9), which induces strong regularization (Theorem 3.4), which in turn leads to reduced accuracy. We thus provide an explanation for the robustness accuracy trade-off for IBP-based training methods, which are practically the most relevant group of certified training methods.\n\n*We note that while the combination of a relatively small network (CNN3) and a complete verifier (MN-BaB) yields comparatively high certified accuracies for the PGD-trained network in Figure 7, especially at small perturbation magnitudes, certified training methods are required for state-of-the-art performance (which require larger networks).\n\nWe hope to have been able to address the reviewer's concerns, are happy to answer any follow-up questions, and look forward to their reply.\n\n**References:**  \n[1] M\u00fcller et al. \"Certified training: Small boxes are all you need.\" ICLR 2023  \n[2] Mao et al. \"Connecting Certified and Adversarial Training.\" NeurIPS 2023   \n[3] De Palma et al. \"Expressive Losses for Verified Robustness via Convex Combinations.\" arXiv 2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238057327,
                "cdate": 1700238057327,
                "tmdate": 1700238057327,
                "mdate": 1700238057327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4U90XUGY5y",
            "forum": "h05eQniJsQ",
            "replyto": "h05eQniJsQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides theoretical analysis on IBP training and helps explain the success of IBP training over other non-IBP methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper gives a definition of the global and local propagation tightness which is new in the literature.\n2. Theorem 3.9 gives a pretty interesting result that IBP improves tightness by proving the alignment between gradients."
                },
                "weaknesses": {
                    "value": "1. Some analysis in Section 4.1 is not clear, please see the questions below.\n2. What can be the potential improvement for certified training methods from your analysis?\n3. Some missing related works:\n     - [1] has a relevant conclusion on the diminishing improvement with increasing width in IBP training.\n\n[1] On the Convergence of Certified Robust Training with Interval Bound Propagation"
                },
                "questions": {
                    "value": "1. In figure 5, why does a decreasing tightness lead to higher accuracy? If a looser bound is better due to weaker regularization, why is increasing the depth worse than increasing the width?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8059/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8059/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8059/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825524513,
            "cdate": 1698825524513,
            "tmdate": 1700678064677,
            "mdate": 1700678064677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LjdysLUYdx",
                "forum": "h05eQniJsQ",
                "replyto": "4U90XUGY5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8059/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8059/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FpvR"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful questions, which we address below and are encouraged to hear that they acknowledge our results to be interesting and novel.\n\n**Q1. Why does a lower tightness lead to higher accuracy in Figure 5? And why is increasing depth worse than increasing the width if less tightness is desirable?**  \nAt initialization, both higher width and depth decrease tightness significantly (see Figure 3). Then, IBP-based training increases tightness (Theorem 3.9), inducing (strong) regularization in the process (Theorem 3.4). The larger this increase in tightness, the stronger the induced regularization. As (IBP-)trained networks exhibit roughly similar tightness across depths and widths (compared to those at initialization), smaller tightness at initialization requires stronger regularization to reach this level (see the updated Figure 5). If this increase in regularization dominates the increase in capacity, we achieve worse goodness of fit (training set robust accuracy) and thus performance. This is what we observe in Figure 5, tightness increases much more over training for large depth than for large width, indicating stronger regularization, explaining the worse certified accuracy. So less tightness is only a positive sign (of less regularization) after IBP-training.\n\n**Q2. Can you relate your results to those of Wang et al. [1] which conclude that there are diminishing improvements with increasing width in IBP training?**  \nFirst, we want to highlight that we already cite Wang et al. [1] in our related work section. Their work considers the convergence of IBP training in the over-parameterized setting for small perturbation magnitudes with high probability and finds that sufficient width is required to achieve zero training loss with high probability. In this setting, the required width increases inversely proportional to the square of the non-convergence probability (for very small perturbation magnitudes). However, while this can be seen as diminishing \u201cimprovements\u201d of convergence probability with increasing width, we observe diminishing improvements in test set certified accuracy, a fundamentally different metric. Further, the work of Wang et al. [1] is limited to 10x smaller perturbations than we consider (see e.g. their Figure 1). \n\n**Q3. Can you highlight the potential improvements to certified training enabled by your analysis?**  \nWe believe that tightness can become a valuable practical and theoretical analysis tool for designing and understanding new certified training methods, as it allows measuring the regularization strength induced by IBP-based training. For example, we demonstrate in Figure 9 that tightness and thus regularization strength are dominated by the size of the propagation region instead of the permissible perturbation magnitude, providing deep insights into the success of SABR [2].\nFurther, it provides a promising new angle to theoretically investigate IBP-based training. Finally, our theoretical results directly predicted architectures yielding improvements over the previous state-of-the-art (see Table 1 and Figure 6).\n\nWe hope to have been able to address all the reviewer's concerns, are happy to answer any follow-up questions they might have, and are looking forward to their reply.\n\n**References:**  \n[1] Wang et al. \"On the convergence of certified robust training with interval bound propagation.\" ICLR 2022  \n[2] M\u00fcller et al. \"Certified training: Small boxes are all you need.\" ICLR 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237663614,
                "cdate": 1700237663614,
                "tmdate": 1700237663614,
                "mdate": 1700237663614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YlneuPgH7T",
                "forum": "h05eQniJsQ",
                "replyto": "LjdysLUYdx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' feedback.\nFor the first question, if less tightness is better due to weaker regularization, can we just make the IBP box larger for better performance?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641045900,
                "cdate": 1700641045900,
                "tmdate": 1700641045900,
                "mdate": 1700641045900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zM4dF9hLO8",
                "forum": "h05eQniJsQ",
                "replyto": "cdY3GtDCuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8059/Reviewer_FpVR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the answer. I have increased the rating to 6 for now.\nBut the result still seems counter-intuitive for me. Following Def 3.6, if we control the same optimal box size, larger tightness means smaller actual box radius. Then this should actually leads to a weaker regularization as the difference between standard training and certified training with IBP is smaller.\nCould the authors explain more on this?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8059/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678025490,
                "cdate": 1700678025490,
                "tmdate": 1700678025490,
                "mdate": 1700678025490,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]