[
    {
        "title": "Bootstrapping Variational Information Pursuit with Foundation Models for Interpretable Image Classification"
    },
    {
        "review": {
            "id": "4zbVe48Iji",
            "forum": "9bmTbVaA2A",
            "replyto": "9bmTbVaA2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7046/Reviewer_3dNV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7046/Reviewer_3dNV"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of variational information pursuit (V-IP). The focus of V-IP is to generate the final answer by answering a series of interpretable queries/questions, facilitating the interpretability of model prediction. In this paper, the authors focus on a challenging problem of how to train a model to answer the interpretable queries as those queries are typically auto-generated and have no associated ground-truth labels. The authors thus proposed to generate pseudo-labels with the aid of GPT and CLIP.\n\nThe authors empirically show that the proposed Concept-QA model leads to shorter and more interpretable query-chains than the CLIP baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is easy to follow. The authors explain their proposed method and give background knowledge about V-IP in an easy-to-follow way. More technical details are also explained and provided in the appendix.\n\n- The designed experiments indeed show that the proposed Concept-QA helps the whole V-IP improve interpretability and results in a shorter query chain compared to the CLIP baseline.\n\n- Fig 4 illustrates how using the proposed Concept-QA improves the interpretability of query chain."
                },
                "weaknesses": {
                    "value": "- The major (if not the only) competitor of the proposed Concept-QA is CLIP baseline. It would be good to include other baseline methods for comparison.\n\n- What the quality of the pseudo-label when evaluated on the Table 1 experiment?\n\n- Since there exist many vision language models (commercial or open source), there might be other easier and more effective ways to generate pseudo-labels for training the Concept-QA model. For example, prompt a LLaVA or InstructBLIP model. The authors should compare to other pseudo-label generation methods as this is part of the major contribution of this paper.\n\n- What's the correlation between the performance of the QA model and the final V-IP system? In the paper, the authors $\\textbf{imply}$ that a better QA model leads to better overall V-IP performance, without supporting experiments and evidence. This weakens the connection between the results in Table 1 and the overall V-IP problem setting. For example, the author may show that using BLIP2 on Places 365 or CIFAR-100 brings better V-IP performance (improved interpretability of query chain, shorter query chain, etc). The reviewer is aware that the computation of BLIP2 is much more expensive than that of Concept-QA. The point here is to build the connection between QA model performance and V-IP performance.\n\n- It would be good to come up with a way to quantify the interpretability and the length of query chain so that the reader can be more confident that the proposed Concept-QA indeed improves over CLIP baseline in these two aspects. For example, a few qualitative results in Fig 4 and the appendix might not be representative enough and might be cherry-picked."
                },
                "questions": {
                    "value": "Please see the questions in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698912320680,
            "cdate": 1698912320680,
            "tmdate": 1699636828128,
            "mdate": 1699636828128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "dmvezA5oxj",
            "forum": "9bmTbVaA2A",
            "replyto": "9bmTbVaA2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7046/Reviewer_ZeKa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7046/Reviewer_ZeKa"
            ],
            "content": {
                "summary": {
                    "value": "Background: Variational Information Pursuit is an approach to generate interpretable explanations that leverages densely annotated data. \n\nKey idea: Overcome the bottleneck of dense-human-labeling by automatic labeling using an LLM. \n\nApproach: Start from an existing list of \u201csemantic concepts\u201d (queries) generated by GPT. Get pseudo-labels generated from CLIP + GPT. QA model provides the answers. The QA model is trained on the GPT queries and  Together the query-answer pairs serve as explanations.   \n\nContributions:\n\n- Lightweight \u201cConcept-QA\u201d model\n- Generated explanations (query-chains) are shorter, more accurate than with baseline QA model (CLIP-similarity score)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Performance: Concept-QA appears to perform well when evaluated along multiple axes. The results are highly correlated with concepts that are actually present in the images; i.e., there seems to be very little confabulation (often called \u201challucination\u201d). In addition, the explanations (query-answer chains) are shorter and more human-interpretable.\n\n2. The key contribution of the paper appears to be the formulation of the Concept-QA model based on query information and answers from GPT + CLIP. The paper demonstrates that naively using CLIP-score between (query-concept, image) does not work well out-of-the-box, and proposes learning a new light-weight network based on pseudo-labels.\n\n3. Impact: The labeling requirement was a huge bottleneck. With this approach, that requirement doesn\u2019t exist anymore. It paves the way for more widespread application of VIP in scenarios where interpretable-by-design approaches are critical. However, we should credit the core idea and (part of the implementation) to the earlier work on label-free CBMs."
                },
                "weaknesses": {
                    "value": "1. The framework of Variational Information Pursuit is quite similar to the Concept Bottleneck Models. The main difference is the sequential selection of informative queries (concepts) in VIP. The key contribution of the paper is the approach to overcome the limitation of annotating  query sets and labels. However, this was already proposed and implemented by an earlier paper (Oikarinen et al.).\n\n2. Sec. 3.2.2 presents the observation that choosing a set of queries from the dataset a-priori (agnostic to the image), does not result in either an optimal or interpretable query set. This is understandable, since two main information sources are ignored \u2014 the image content, and taking advantage of the answers to the queries. Considering the actual experimental setting is significantly different from the VIP setting, the title \u201cAnswering Queries with CLIP Adversely affects VIPs explanations\u201d and the conclusions seem a bit misleading. While floating-point-dot-product-scores from CLIP might also be a problem (as discussed in Sec. 4.1), the more obvious problem in this setting appears to be the query selection strategy. Please let me know if I have misunderstood something.\n\n3. The proposed ConceptQA architecture is intuitive and quite straightforward. While this is not a downside by itself, it is probably something that one implement as a baseline. It is a bit hard to identify the interestingness or novelty in the approach. The closest baseline is simply a comparison against a CLIP(image, concept) similarity score.\n\nReferences:\n- Oikarinen et al.: Label-Free Concept Bottleneck Models; ICLR 2023."
                },
                "questions": {
                    "value": "1. The overall approach is inspired by work on label-free CBMs and the important modules (query generation, answer-generation and concept-qa) are reasonable. However, it is a bit difficult for me to articulate precisely what are the novel scientific ideas or findings from this paper. It might be nice if the authors could reiterate this.\n\n2. The main baseline for the proposed Concept QA model is binarized CLIP after normalization. There\u2019s an assumption here that the value of \u201c0\u201d is a good threshold. Were there experiments performed to identify the optimal similarity threshold for accurate classification? For instance, the min-max normalization described in 3.2.3 might be a good starting point."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7046/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699109333907,
            "cdate": 1699109333907,
            "tmdate": 1699636828027,
            "mdate": 1699636828027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ZEGegJRgpA",
            "forum": "9bmTbVaA2A",
            "replyto": "9bmTbVaA2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7046/Reviewer_8mpd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7046/Reviewer_8mpd"
            ],
            "content": {
                "summary": {
                    "value": "The focus of this paper is to revisit \"Variational Information Pursuit (VIP) for image classification. VIP is an interpretable framework that makes predictions by sequentially selecting Question-Answer chains to reach the prediction. The paper states that VIP has limitations as it requires specification of a query set and densely annotated sequential QA data.  To relieve this limitation, the paper proposes that language models could be leveraged to generate queries for image classification, and a QA network is proposed (ConceptQA) to answer binary questions about the image. \n\nFindings: Experiments are conducted on 5 image classification datasets and compared to 3 vision-language models (VLMs). The model outperforms the baselines on 2/5 datasets and is second best on the others."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This is an interesting take to use VIP / sequential QA for image classification and the experiments demonstrate the effectiveness of this method for interpretability/explanations.\n2. In general the idea could be useful beyond image classification for other tasks with data-scarce settings, especially unlabeled datasets, where language models can help to create pseudo labels.\n3. The paper put together (main + appendix) is well written and explains the preliminaries, proposed method, implementation, and experiments in good detail."
                },
                "weaknesses": {
                    "value": "1. Comparisons to vision-only models are missing: it would be useful to know how much of a gap there is between supervised as well as self-supervised vision models trained on these datasets. This comparison would tell us how much performance is being sacrificed for interpretability. Comparisons in terms of #parameters are also important in this regard.\n2. The method only uses OpenAI's GPT models (which are not open-source but only accessible via API calls -- i.e. need an internet connection and OpenAI account for inference) -- it would have been better to also implement the method with a local language model -- it is understandable that the performance could potentially be lower than GPT.\n3. A human study could have helped to supplement the study on explanation length. See Q2 and Q3 below.\n4. Useful details and useful visualizations are relegated to the appendix -- for instance Fig 9 could be moved to the main paper, some of the details on query set generation (App C) and training process (App D) could be briefly added in the main paper to improve the flow/readability of the paper."
                },
                "questions": {
                    "value": "1. Given that the method works by using QA -- could it also be useful for improving VQA performance (eg. for VQA-v2, VizWiz, GQA, CLEVR etc datasets).\n2. Do humans prefer shorter or longer explanations when it comes to trusting a model? While the tradeoff in figure 5 is a good finding, I'm not sure how it translates to helping humans understand decisions made by the model.\n3. Similarly, when humans know that the prediction has failed -- do they prefer shorter or longer explanations to debug why it failed? This study could be added.\n\nComment:\n1. I'm going to be \"that\" person and say that we should really avoid the marketing lingo of \"Foundation models\" in scientific papers -- who gets to decide which model is a foundation model and which isn't? Why is CLIP a foundation model but ResNet / RNN isn't?  The term \"Large\" language model (like \"deep\" learning) is also dicey, but at least we could define \"large\"/\"deep\" in terms of some threshold on number of trainable parameters/layers in the model. Is there really a definition for foundation models -- or is it just a marketing trick that everyone is accepting? I would recommend replacing \"Foundation Models\" in the title with \"Language Models\" -- it is more informative (because the paper relies on prompting GPT with natural language)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7046/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7046/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7046/Reviewer_8mpd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7046/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700583623365,
            "cdate": 1700583623365,
            "tmdate": 1700583623365,
            "mdate": 1700583623365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mZsQGulbci",
            "forum": "9bmTbVaA2A",
            "replyto": "9bmTbVaA2A",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7046/Reviewer_8xTQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7046/Reviewer_8xTQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel methodology for training a Concept Question-Answering system (Concept-QA) that determines the presence of a concept in an image using binary answers. This approach eliminates the need for manually annotated training data, relying instead on pseudo-labels generated by GPT and CLIP. The authors empirically validate Concept-QA on various datasets, demonstrating its accuracy in representing true concepts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper introduces a methodology for training Concept-QA that doesn't require manually annotated training data, reducing the burden of data annotation.\n- Experiments on multiple datasets demonstrate the proposed method's effectiveness.\n- This paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "- It would be better to try more LLMs to show the generality of this idea.\n- Limitations could be discussed. Is there anything about **the proposed method itself** that fails in certain scenarios or falls short compared to prior work? \n- While the paper mentions the use of pseudo-labels from GPT and CLIP, it could discuss the interpretability and potential biases associated with these labels. Are there instances where the pseudo-labels might lead to incorrect or biased answers?"
                },
                "questions": {
                    "value": "While the paper highlights the advantage of not requiring manually annotated training data, it could delve deeper into the data efficiency aspect. Does Concept-QA require a large amount of unlabeled data to perform well, and how does its data efficiency compare to alternative approaches?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7046/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7046/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7046/Reviewer_8xTQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7046/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700659240525,
            "cdate": 1700659240525,
            "tmdate": 1700659240525,
            "mdate": 1700659240525,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]