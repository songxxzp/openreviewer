[
    {
        "title": "Frequency-Aware Transformer for Learned  Image Compression"
    },
    {
        "review": {
            "id": "OJFfVCq8OS",
            "forum": "HKGQDDTuvZ",
            "replyto": "HKGQDDTuvZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7366/Reviewer_ZpGq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7366/Reviewer_ZpGq"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the frequency problem of learned image compression and develops a frequency-aware method for this problem based on multiscale and directional analysis, called FAT. The method introduces two modules to capture frequency component. Based on FAT, the learned image compression achieves better rate-distortion performance. The method shows improvement over learned codec and conventional codec baselines by a healthy margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed FAT is a novel idea that captures multiscale and directional frequency components and outperforms SOTA.\n\n2. Nice visualization on multiscale and directional decomposition of frequency component. It is an interesting finding that structural information within different frequency ranges also plays a crucial role in learned image compression."
                },
                "weaknesses": {
                    "value": "1. It is interesting to find that FDWA achieves a significant improvement compared to 4x4 blocks and 16x16 blocks. However, it is not clear that how does the multiscale decomposition and the directional decomposition affect. The authors are suggested to provide an ablation study on FDWA.\n \n2. The frequency-aware transformer is realized by two modules, FDWA and FMFFN. Have you tried these two mechanisms in the entropy model? Entropy model plays a crucial role in learned image compression. In my opinion, it is more important to capture frequency component in entropy model. It would be a great improvement if the frequency-aware mechanism works."
                },
                "questions": {
                    "value": "1. Have you ever tried other decomposition ways such as (1) smaller size and bigger size for square window (I am wondering how much improvement can be obtained in this mechanism), or (2) more diverse shapes.\n \n2. What impact does the block size in FMFFN have? When you set the block size responding to the maximum window size in FDWA, does it mean that the FMFFN is used to refine the high-frequence of features?\n \n3. If the decomposition of window size works, how about directly using different sizes of convolution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698397685577,
            "cdate": 1698397685577,
            "tmdate": 1699636880746,
            "mdate": 1699636880746,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nRETVFKW6q",
                "forum": "HKGQDDTuvZ",
                "replyto": "OJFfVCq8OS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2"
                    },
                    "comment": {
                        "value": "We appreciate your valuable comments and insightful  suggestions. We have addressed your concerns as below.\n\n**W1: Ablation Study on FDWA:**\n\n* Thanks for your valuable suggestion. We have performed an ablation study on our FDWA to demonstrate the effect of both multiscale and directional decomposition. Due to the limitations on time and computational resources, we only provide models trained for 500,000 training steps in this response. The Lagrangian multiplier $\\lambda$ is set to 0.0483 for all the models.\n\n  In the following table, we use **Multiscale** to denote anisotropic $4\\times 4$ and $16\\times 16$ windows, and we employ **Directional** to represent isotropic $4\\times 16$ and $16\\times 4$ windows. Besides, we provide a baseline result with only $4\\times 4$ windows for reference. The PSNR, BPP, and R-D loss are averaged across 24 images of the Kodak dataset. Results show that both multiscale and directional decomposition enhance the R-D performance.\n\n\n* | Models            | Multiscale | Directional | PSNR   | BPP    | R-D Loss |\n|-------------------|:------------:|:-------------:|--------|--------|----------|\n| Baseline          | x    | x    | 37.031 | 0.8949 | 1.552    |\n| Multiscale-only   | \u2714\ufe0f         | x  | 37.025 | 0.8722 | 1.537    |\n| Directional-only  | x   | \u2714\ufe0f         | 37.080 | 0.8889 | 1.540    |\n| Proposed          | \u2714\ufe0f         | \u2714\ufe0f          | 37.248 | 0.8910 | 1.517    |\n\n\n**W2: Capturing Frequency Components in Entropy Model:**\n\n* Thanks for your suggestion. We agree that capturing frequency components can further benefit entropy models in LIC, and **we specifically propose a TC-A entropy model to exploit the correlations between the frequency components and reduce the coding budget**, as elaborated in Section~3.3.\n\n  Since the latent representation extracted by the proposed FAT block consists of diverse frequency components across different channels, we employ channel attention in our TC-A entropy model to model the dependencies between these frequency components. Moreover, the channel-wise correlations (*i.e.*, frequencies correlations) captured by our TC-A can adaptively vary with input image, which cannot be achieved by the commonly used CNN-based channel-wise auto-regressive entropy model (*i.e.*, CHARM [R1]).\n\n  **Apply FDWA and FMFFN in Entropy Model:**\n\n* Thanks for your valuable comments. Theoretically, FDWA and FMFFN can be used in the entropy model. Here, we provide additional experimental results based on a recent work [R2]. In this experiment, we adopt the entropy model from [R2] as our baseline and conduct an ablation study to demonstrate the additional benefits of FDWA and FMFFN. The experimental setup follows the ablation study on the TC-A entropy model in Section 4.3 of our paper.\n\n  Despite the Swin-Transformer being only a modest sub-module in the baseline entropy model [R2], our results demonstrate that FDWA and FMFFN can also improve its performance, showing the potential of FDWA and FMFFN in helping achieve more accurate probability distribution estimation.\n  | FDWA         | FMFFN        | BD-rate (Anchor: BPG) |\n  |:--------------:|:--------------:|:------------------------:|\n  |      $\\times$ |      $\\times$ |         -17.4\\%         |\n  |\u2714\ufe0f          |      $\\times$ |         -18.1\\%         |\n  |      $\\times$ | \u2714\ufe0f          |         -17.6\\%         |\n  |\u2714\ufe0f         | \u2714\ufe0f         |         -18.2\\%         |\n\n\n  We do not directly apply FDWA and FMFFN in the proposed entropy model (*i.e.*, TC-A) since both these two modules are designed to enhance the Swin-Transformer based on **spatial self-attention**, while our **channel self-attention**-based TC-A is not constructed with the Swin-Transformer.\n\n[R1] Minnen, David, and Saurabh Singh. \"Channel-wise autoregressive entropy models for learned image compression.\" 2020 IEEE International Conference on Image Processing (ICIP). IEEE, 2020.\n\n[R2] Liu, Jinming, Heming Sun, and Jiro Katto. \"Learned image compression with mixed transformer-cnn architectures.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738585454,
                "cdate": 1700738585454,
                "tmdate": 1700741023566,
                "mdate": 1700741023566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s2ZoPWhf4F",
                "forum": "HKGQDDTuvZ",
                "replyto": "OJFfVCq8OS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: Other decomposition ways**\n\nWe have tried the decomposition ways mentioned by the reviewer in our earlier experiments, but we did not adopt these configurations in our manuscript due to the out-of-memory (OOM) problem raised by using the bigger size for a square window ($32 \\times 32$). Here, we also conduct experiments to comprehensively understand how different decomposition ways affect the performance.\n\n**(1) OOM problem of 32 $\\times$ 32 window**: We emphasize that employing a bigger square window ($32 \\times 32$) results in significantly increased memory consumption and running time. For example, when we set the window size of **half of the attention heads** as $32\\times 32$ and the remaining ones as $2 \\times 2$, we meet the OOM problem on our Nvidia RTX 4090Ti GPU (with 24 GB memory) during the training process with each batch containing 8 images with a patch size of 256$\\times$ 256.\n\n**(2) Effect of different decomposition ways**: We explored various decomposition ways in this ablation study, including:\n\n- **(a) With Only One Small Window** (*i.e.*, $4\\times 4$).\n- **(b) With Multiscale Decomposition**, including two types of windows \uff08*i.e.*,$4\\times4$ and $16\\times16)$.\n- **(c) With Smaller Size and Bigger Size for Square Window (Reviewer's Suggestion)**, including four types of windows (*i.e.*,$2\\times2, 4\\times4, 16\\times16,$ and $ 32\\times32)$.\n- **(d) With Our FDWA**, including four types of window (*i.e.*,$4\\times4, 16\\times16, 4\\times16, 16\\times4$).\n- **(e) With More Diverse Shapes (Reviewer's Suggestion)**, including 8 types of window (*i.e.*,$2\\times2, 4\\times4, 16\\times16,  32\\times32, 2\\times32, 32\\times2, 4\\times16$, and $16\\times4$).\n\nIn each of the above methods, the number of attention heads for each type of window attention is identical (*i.e.*, $K/N$ heads, where $K$ denotes the total number of attention heads, and $N$ denotes the number of window types). All the models trained for 500,000 training steps, and the Lagrangian multiplier $\\lambda$ for all the models is 0.0483. We observe that the decomposition method employing more diverse shapes achieves slightly lower R-D loss, with a sacrifice in memory consumption and training speed.\n\n\n| **Methods** | **PSNR** | **BPP** | **R-D Loss** | **GPU Memory for Training** | **Training Speed (steps/s)** |\n|:------------:|:--------:|:-------:|:------------:|:---------------------------:|:-----------------------------:|\n|     (a)      |  37.031  |  0.8949 |    1.552     |           10.36GB           |             3.33              |\n|     (b)      |  37.025  |  0.8722 |    1.537     |           13.96GB           |             3.08              |\n|     (c)      |  37.135  |  0.8802 |    1.525     |           20.22GB           |             2.16              |\n|     (d)      |  37.248  |  0.8910 |    1.517     |           12.68GB           |             3.17              |\n|     (e)      |  37.213  |  0.8817 |    1.513     |           15.02GB           |             2.57              |\n\n**Q2: Effect of the Block Size in FMFFN**\n\nThe block size in FMFFN influences the balance between modulating local and global features in FMFFN. A block size that is too small in FFT may result in the loss of some global features ( *i.e.,* low-frequency information), while a block size that is too large may impede high-frequency local structure and increase the parameters of the learnable filter matrix.\n\nWe want to clarify that FMFFN is not used to refine the high-frequency features only. Setting the block size corresponding to the maximum window size ( *i.e.,* 16$\\times$ 16) in FDWA indicates that FMFFN operates within a similar frequency range as FDWA, so that we can comprehensively modulate both low and high-frequency components produced by FDWA. In addition, FFT with 16$\\times$ 16 block size also resembles the 16$\\times$ 16 block-wise DCT transforms in H.264. Considering these factors, we chose the 16$\\times$ 16 block size for FMFFN in our manuscript.\n\nWe also conducted ablation studies on the block size of FMFFN, and the results are in the table below. All the models are trained for 500,000 training steps, and the Lagrangian multiplier $\\lambda$ for all the models is 0.0483. It indicates that FMFFN with a block size of 8 achieves the best R-D performance in this training set up.\n\n|   Block Size   |   PSNR   |   BPP   |   R-D Loss   |   # Params   |\n|:--------------:|:--------:|:-------:|:-----------------------------:|:------------:|\n|       4        |  37.117  | 0.8812  |            1.524              |   70.44M     |\n|       8        |  37.211  | 0.8789  |            1.511              |   70.55M     |\n|      16        |  37.248  | 0.8910  |            1.517              |   70.97M     |\n|      32        |  37.092  | 0.8793  |            1.529              |   72.55M     |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738987572,
                "cdate": 1700738987572,
                "tmdate": 1700740080171,
                "mdate": 1700740080171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iUrkKWRL9M",
                "forum": "HKGQDDTuvZ",
                "replyto": "OJFfVCq8OS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q3: Convolutions with Diverse Kernel Size.**\nSeveral works have discussed the impact of convolution kernel sizes in learned image compression. Cheng *et al.* [R3] compare the learned image compression model with diverse convolutional kernel sizes (*i.e.*, $5\\times 5$, $7\\times 7$, and $9\\times 9$) and observe that a larger convolutional kernel size would lead to better R-D performance. Moreover, a larger kernel size could be obtained by stacking small convolutional kernels. Cui *et al.* [R4] employ mask convolutions with diverse kernel sizes { *i.e.*, $3\\times 3$, $5\\times 5$, and $7\\times 7$) to achieve a more precise autoregression model.\n\nHowever, we have not found discussions on anisotropic kernel size or analyses on the effect of kernel size from the perspective of frequency decomposition for convolution-based LIC models. Some works [R5, R6, R7] show that convolutions with a local receptive size tend to present the characteristics of a high-frequency filter. While there is potential to achieve frequency decomposition with diverse kernel sizes of convolutions, achieving much larger kernel sizes may pose challenges in computation complexity or require stacking too many small kernel convolution layers.\n\nWe highlight that our FDWA can achieve multiscale and directional frequency decomposition **in each attention layer in a more effective and simple way.**\n\n- [R3] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, \"Deep Residual Learning for Image Compression,\" CVPR Workshops. 2019. \n \n- [R4] Z. Cui, J. Wang, S. Gao, T. Guo, Y. Feng, and B. Bai, \"Asymmetric Gained Deep Image Compression With Continuous Rate Adaptation,\" 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA, 2021, pp. 10527-10536.\n\n- [R5] Park, Namuk, and Songkuk Kim. \"How Do Vision Transformers Work?.\" International Conference on Learning Representations. 2022.\n\n- [R6] Yin, Dong, et al. \"A fourier perspective on model robustness in computer vision.\" Advances in Neural Information Processing Systems 32 (2019).\n\n- [R7] Wang, Haohan, et al. \"High-frequency component helps explain the generalization of convolutional neural networks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n\nWe sincerely appreciate your dedicated review of our paper. The author response period for this submission has been extended until December 1st, and we are looking forward to your further response."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739257712,
                "cdate": 1700739257712,
                "tmdate": 1700741255423,
                "mdate": 1700741255423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zyFAg6Y7e1",
            "forum": "HKGQDDTuvZ",
            "replyto": "HKGQDDTuvZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7366/Reviewer_fcgY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7366/Reviewer_fcgY"
            ],
            "content": {
                "summary": {
                    "value": "To overcome the problem of existing LIC (Learned Image Compression) methods are redundant in latent representation, this paper suggests a nonlinear transformation and makes the following three improvements:\n\n1)\tThis paper proposes a frequency-decomposition window attention (FDWA), which leverages diverse window shapes to capture frequency components of natural images.\n\n2)\tThis paper develops a frequency-modulation feed-forward network (FMFFN) that adaptively ensembles frequency components for improved R-D performance.\n\n3)\tThis paper presents a transformer-based channel-wise autoregressive model (T-CA) for effectively modeling dependencies across frequency components.\n\nExperimental results show that this paper achieves state-of-the-art R-D performance on several datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper is overall easy to understand and clearly written. One of the primary strengths of this paper is the claimed SOTA rate-distortion performance. \n2) The authors found that existing learned image compression methods lead to potential representation redundancy due to limitations in capturing the anisotropic frequency components of anisotropy and preserving directional details. Some attempts including FMFFN and FDWA are proposed to address this issue."
                },
                "weaknesses": {
                    "value": "1) The idea of applying frequency processing to the learned compression framework is not new. For example, conv in the frequency domain [1] has been used in Balle\u2019s early work [2]. Wavelet-based compression framework has also been proposed [3]. The authors should cite, compare, and identify their differences.\n\n[1] Rippel, Oren, Jasper Snoek, and Ryan P. Adams. \"Spectral representations for convolutional neural networks.\" Advances in neural information processing systems 28 (2015).\n\n[2] Ball\u00e9, Johannes, Valero Laparra, and Eero P. Simoncelli. \"End-to-end optimized image compression.\" arXiv preprint arXiv:1611.01704 (2016).\n\n[3] Ma, Haichuan, et al. \"iWave: CNN-based wavelet-like transform for image compression.\" IEEE Transactions on Multimedia 22.7 (2019): 1667-1679.\n\n2) The authors should compare with more existing works (e.g. [3,4]) to demonstrate the SOTA performance of the paper.\n\n[4] Liu J, Sun H, Katto J. Learned image compression with mixed transformer-cnn architectures[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 14388-14397.\n\n[5] Fu H, Liang F, Liang J, et al. Asymmetric Learned Image Compression with Multi-Scale Residual Block, Importance Scaling, and Post-Quantization Filtering[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2023."
                },
                "questions": {
                    "value": "1. Table 1 and Table 5 in the article involve flop calculations, may I ask the authors what methods or tools they used to calculate the complexity? As far as I know, many existing tools that count complexity do not calculate correctly the complexity of the internal operators of the transformer. Besides, Table 5 is kind of confusing. Can the authors explain what the (1), (2), (3) in the table mean here.\n\n2. Adding a transformer on the codec side would result in a longer training time and a larger memory requirement that is not hardware-friendly for the actual deployment of the compression model. Can the authors provide the training time as well as the corresponding average and peak memory consumption for both training and test? Besides, the three methods compared in Table 5 are all transformer-based methods, and there is no comparison of decoding time with, for example, Minnen[1] and GMM[2], where both the encoder and decoder use a CNN structure.\n\n[1] Minnen D, Singh S. Channel-wise autoregressive entropy models for learned image compression[C]//2020 IEEE International Conference on Image Processing (ICIP). IEEE, 2020: 3339-3343.\n\n[2] Cheng Z, Sun H, Takeuchi M, et al. Learned image compression with discretized gaussian mixture likelihoods and attention modules[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 7939-7948."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Reviewer_fcgY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7366/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724257385,
            "cdate": 1698724257385,
            "tmdate": 1700631960061,
            "mdate": 1700631960061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2aFPCmLW6g",
                "forum": "HKGQDDTuvZ",
                "replyto": "zyFAg6Y7e1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fcgY"
                    },
                    "comment": {
                        "value": "We appreciate your insightful comments and valuable suggestions. We address your concerns as below.\n\n**W1: Novelty of this paper.**\n\n- First of all, we would like to clarify that **the novelty of this paper is the novel method to exploit multi-scale and directional frequency information in nonlinear transforms rather than employing frequency-based operations.** We HAVE NOT claimed that our work is the first to apply frequency-based operations in LIC. On the contrary, we break the limitation of existing transformer-based LIC models that cannot achieve multi-scale and directional analysis due to the isotropic window attention.\n\n- Subsequently, we have discussed the three papers [R1, R2, R3] mentioned by the reviewer and clarified the difference between this paper and [R1, R2, R3]. In summary, [R1] and [R2] **aim to accelerate network optimization** and directly learn the parameters of CNN in the spectral domain. [R3] substitutes the filters in the traditional separable wavelet transforms with trained CNNs to improve the R-D performance of  JPEG2000. **These methods cannot achieve multi-scale directional analysis by leveraging isotropic DFT [R1] and DCT [R2] bases or introducing CNNs into 1-D Wavelet transforms [R3].** By contrast, in this paper, we propose to capture diverse frequency components with the frequency-aware transformer and achieve an end-to-end optimized multi-scale and directional analysis by anisotropic window attention. We elaborate our differences from [R1, R2, R3] as below.\n\n  * **Difference from [R1] and [R2].** [R1] and [R2] parameterize CNNs in the frequency domain for fast convergence since spatial domain convolutions are equal to spectral domain multiplications. DCT [R1] and DFT [R2] are adopted as the transforms for the linear filters in CNNs. However, such parameterization only affects convergence speed in optimizing the parameters but does not change the network architectures.  It should be noted that this idea is also considered for speeding up various tasks such as  image classification [R1], inverting deep networks [R6], and mobile sensing [R7], rather than specifically designed for extracting frequency components in LIC.\n\n    In contrast, our approach captures and processes diverse frequency components to achieve more  compact image representations for improved R-D performance.  **Different from [R1] and [R2], we design FMFFN specifically for LIC to to adaptively decide the needed frequency components** and further eliminate redundancy across different components, though FFT is used to transform the features into frequency domain.**\n\n  * **Difference from [R3].** iWave [R3] is developed based on the JPEG2000 schemes by substituting the discrete wavelet transforms (*e.g.*, Daubechies 5/3 and 9/7 wavelets) with CNNs. iWave [R3] cannot be end-to-end optimized by fixing the entropy model (*i.e.*, the probability distribution of discretized symbols) for entropy coding and yields limited R-D performance gain over traditional JPEG2000.  iWave++ [R8] is later developed to enable end-to-end learning for LIC. However,  iWave [R3] and iWave++ [R8] realize  nonlinear transform by stacking1-D transforms on horizontal and vertical directions, inspired by JPEG2000 that leverages 2-D separable wavelet transforms realized independently on the horizontal and vertical directions. As a result, they cannot realize multi-dimensional directional transform and is restricted in achieving compact representation for image compression \n  * **Different from [R3] and [R8] that independently perform 1-D transforms, our method achieves directional transforms that process the horizontal and vertical directions simultaneously.**  We are the first to leverage the transformer to exploit the different frequency components, which has not been achieved in existing LIC model. Furthermore, we  propose the T-CA entropy model to model the correlations across different frequency components for end-to-end optimization.\n\nWe have cited these works as. Ball'e et al. (2016), Rippel et al. (2015), and Ma et al. (2019; 2020) and discussed them in the related work section in our revision.\n\n**W2: SOTA performance.**\n* We exactly achieve the SOTA performance and outperform [R4] and [R5] mentioned by the reviewer. [R4] is the current SOTA method for LIC and has been compared in the previously submitted manuscript. We show that, compared with [R4], we yield 2.6% BD-rate saving on the Kodak dataset. Moreover, [R5] is inferior to [R4] in R-D performance. Compared with VVC, [R5] yields 2.1% BD-rate reduction on the Kodak dataset, while **we can achieve a significantly higher 14.5% BD-rate reduction**. We have included the results for [R5] in Fig.~4 in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047314589,
                "cdate": 1700047314589,
                "tmdate": 1700155270368,
                "mdate": 1700155270368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GGhh5F6d7D",
                "forum": "HKGQDDTuvZ",
                "replyto": "zyFAg6Y7e1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: We address the concerns on Tables 1 and 5 as below.**\n\n**Table 5 for calculating the complexity:**\nThanks for your kind remainder. We noticed that *deepspeed* used in the previous transformer-based LIC work [R9] ignores the matrix multiplication and softmax operation of self-attention. Thus, in our evaluation, we used  the *FlopCountAnalys* function from *fvcore* library, which can appropriately process these internal operations of the transformer.\n\n**Presentation of Table 1:**\nWe have revised Table 1 to improve its clarity. Here, (1) (2) (3) mean three different settings of architectures of FAT Block, and the number only represents the indices of different architectures.\n\n**Q2: Comparison with CNN-based methods**\n\n**The integration of transformers into LIC is a notable trend for enhancing compression performance.** However, existing transformer-based LIC methods cannot achieve multiscale and directional analysis, which is an important and indispensable topic spanning from conventional codecs to recent LIC methods. Therefore, in this paper, we propose a frequency-aware transformer for LIC to further enable multiscale and directional analysis. To this end, we focus on **achieving better R-D performance with equivalent complexity over existing transformer-based LIC.**\n\nAccording to the reviewer's comments, we compare our method with existing CNN-based [R10, R11] and transformer-based methods [R4, R9, R12] in terms of training speed, memory requirement, and FLOPs in Tables 5 and 6 in the revision (also seen as below). Note that all the existing methods claim that they require 2M$\\sim$3.5M training steps in total for convergence. Compared with CNN-based methods, transformer-based methods exhibit evidently high R-D performance at the cost of memory consumption, training speed, and coding time. Notably, compared with existing transformer-based methods, the proposed method achieves apparent R-D performance improvement with only a slightly higher computational complexity.  In addition, many existing works [R13,R14,R15] focus on achieving efficient transformer, these facts imply the potential of our method in further exploring the efficient transformer-based LIC. \n\n\n\n| **Model**      | **GFLOPs (Enc.)** | **GFLOPs (Dec.)** | **Inference Latency (Enc.) (ms)** | **Inference Latency (Dec.) (ms)** | **#Params** | **BD-rate** |\n| ----------------------------- | :------: |:-:| :-: |:-:|:-:|-:|\n| Cheng et al. (2020) [R10]    | 154               | 229               | $>$1000                          | $>$1000                          | 26.60M      | 3.6%        |\n| Minnen & Singh (2020)  [R11]  | 101               | 100          | 56                               | 43                               | 55.13M      | 1.1%        |\n| Zhu et al. (2022) [R9] | 116              | 116               | 110                              | 99                               | 32.71M      | -3.3%       |\n|   Zou et al. (2022) [R12]       | 285               | 276               | 97                               | 101                              | 99.58M      | -4.3%       |\n| Liu et al. (2023)  [R4]     | 317               | 453               | 255                              | 322                              | 76.57M      | -11.9%      |\n| Ours                          | 141               | 349               | 125                              | 242                              | 70.97M      | -14.5%      |\n| VTM-12.1                       | -                 | -                 | -                                | -                                | -           | 0%          |\n\n| **Model**                       | **Peak GPU Memory (GB) for Training** | **Peak GPU Memory (GB) for Test** | **Training Speed (steps/s)** |\n| :------------------------------- | :-----------------------------------:| :--------------------------------: | :-----------------------------: |\n| Cheng et al. (2020) [R10]       | 2.32                                | 0.59                             | 14.23                         |\n|Minnen & Singh (2020)  [R11]     | 3.48                                | 0.50                             | 8.26                          |\n| Zhu et al. (2022) [R9] | 16.84                               | 2.10                             | 2.66                          |\n|   Zou et al. (2022) [R12]           | 10.87                               | 0.68                             | 4.29                          |\n| Liu et al. (2023)  [R4]          | 14.14                               | 1.71                             | 2.13                          |\n| Ours                            | 12.68                               | 1.09                             | 3.17                          |\n\n\nWe sincerely thank you for your efforts in reviewing this paper, and we are looking forward to your response."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700047550418,
                "cdate": 1700047550418,
                "tmdate": 1700368705754,
                "mdate": 1700368705754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2v68C1c6kZ",
                "forum": "HKGQDDTuvZ",
                "replyto": "zyFAg6Y7e1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7366/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reference"
                    },
                    "comment": {
                        "value": "**Reference**\n- [R1] Rippel, Oren, Jasper Snoek, and Ryan P. Adams. \"Spectral representations for convolutional neural networks.\" Advances in neural information processing systems 28 (2015).\n- [R2] Ball\u00e9, Johannes, Valero Laparra, and Eero P. Simoncelli. \"End-to-end optimized image compression.\" arXiv preprint arXiv:1611.01704 (2016).\n- [R3] Ma, Haichuan, et al. \"iWave: CNN-based wavelet-like transform for image compression.\" IEEE Transactions on Multimedia 22.7 (2019): 1667-1679.\n- [R4] Liu, Jinming, Heming Sun, and Jiro Katto. \"Learned image compression with mixed transformer-cnn architectures.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n- [R5] Fu, Haisheng, et al. \"Asymmetric Learned Image Compression with Multi-Scale Residual Block, Importance Scaling, and Post-Quantization Filtering.\" IEEE Transactions on Circuits and Systems for Video Technology (2023).\n- [R6] Wong, Eric, and J. Zico Kolter. \"Neural network inversion beyond gradient descent.\" Advances in Neural Information Processing Systems, Workshop on Optimization for Machine Learning. 2017.\n- [R7] Yao, Shuochao, et al. \"Deepsense: A unified deep learning framework for time-series mobile sensing data processing.\" Proceedings of the 26th international conference on world wide web (WWW). 2017.\n- [R8] Ma, Haichuan, et al. \"End-to-end optimized versatile image compression with wavelet-like transform.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.3 (2020): 1247-1263.\n- [R9] Zhu, Yinhao, Yang Yang, and Taco Cohen. \"Transformer-based transform coding.\" International Conference on Learning Representations. 2022.\n- [R10] Cheng, Zhengxue, et al. \"Learned image compression with discretized gaussian mixture likelihoods and attention modules.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\n- [R11] Minnen, David, and Saurabh Singh. \"Channel-wise autoregressive entropy models for learned image compression.\" 2020 IEEE International Conference on Image Processing (ICIP). IEEE, 2020.\n- [R12]Zou, Renjie, Chunfeng Song, and Zhaoxiang Zhang. \"The devil is in the details: Window-based attention for image compression.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n- [R13] Zhang, Xindong, et al. \"Efficient long-range attention network for image super-resolution.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n- [R14] Pan, Zizheng, Jianfei Cai, and Bohan Zhuang. \"Fast vision transformers with hilo attention.\" Advances in Neural Information Processing Systems 35 (2022): 14541-14554.\n- [R15] Li, Yanyu, et al. \"Rethinking vision transformers for mobilenet size and speed.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220189606,
                "cdate": 1700220189606,
                "tmdate": 1700289289786,
                "mdate": 1700289289786,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t9YHRxKM9s",
                "forum": "HKGQDDTuvZ",
                "replyto": "2v68C1c6kZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7366/Reviewer_fcgY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7366/Reviewer_fcgY"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' comments"
                    },
                    "comment": {
                        "value": "Thank you for your response. The overall evaluation, with these supplemented results and explanations, is comprehensive. Despite my reservations regarding the novelty and contribution of this paper, I have decided to raise my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7366/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631910686,
                "cdate": 1700631910686,
                "tmdate": 1700631910686,
                "mdate": 1700631910686,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yJhqk0MgCP",
            "forum": "HKGQDDTuvZ",
            "replyto": "HKGQDDTuvZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7366/Reviewer_kfZs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7366/Reviewer_kfZs"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to reduce redundancies in the latent representation of Learned Image Compression methods.\nFor this purpose it proposes a new frequency-aware transformer block which utilizes two new components.\n1. FDWA: a window attention block to capture various frequency components\n2. FMFFN: a block which modulates the frequency components \n\nAdditionally this paper proposes a transformer-based channel-wise autoregressive entropy model (T-CA)\nIn combination these methods achieve SOTA performance on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The method achieves SOTA performance without unreasonable performance cost.\n2. The paper is well written and provides nice visualizations of core ideas."
                },
                "weaknesses": {
                    "value": "There are no explicit ablations of some design decisions of the FAT block. What effect do the relative window sizes of the FDWA module have? What's the impact of omitting some of the windows (eg. omitting vertical/horizontal windows)?\n\nThere are typos in the caption of Figure 3 (p.6) and Table 2 (p.8)"
                },
                "questions": {
                    "value": "How does the use of the T-CA entropy model affect inference times (compared to existing models such as CHARM)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Reviewer_kfZs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7366/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700666970677,
            "cdate": 1700666970677,
            "tmdate": 1700666970677,
            "mdate": 1700666970677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "AHc9BqO2dv",
            "forum": "HKGQDDTuvZ",
            "replyto": "HKGQDDTuvZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7366/Reviewer_Xwct"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7366/Reviewer_Xwct"
            ],
            "content": {
                "summary": {
                    "value": "A learned image compression approach based on transformer architecture is presented. The main innovation is the introduction of a frequency-aware module that performs multi-scale, directional analysis of the transformer features, and then uses an FFT to weight the important frequency components. Results show mild to moderate improvements in the RD-curve and BD-metric on various datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The intuition behind the method is sound. Writing and explanation is clear. Results show improvements over various transformer-based baselines."
                },
                "weaknesses": {
                    "value": "This is a general comment rather than a specific weakness. It is not entirely clear how much improvement is brought about by the specific structure of the FDWA and FMFFN blocks, rather than the fact that some learnable layers have been included which would increase the overall capacity of the network. The authors have provided a few ablations in Figures 5. and 6. which do somewhat support their claim, but I feel this could be made stronger in future versions. For instance,\n\n1. What if additional windows with different aspect ratios were used? Would these provide further improvement? This could be achieved by either increasing $K$ (the number of heads), or by using the same number of heads but partitioning into more groups to accommodate the different aspect ratios?\n\n2. The impact of the FMFFN block seems to be rather small. What if instead of a block FFT, a learnable conv layer (and deconv for the IFFT) was used?"
                },
                "questions": {
                    "value": "Included in comment above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7366/Reviewer_Xwct"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7366/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1701023921291,
            "cdate": 1701023921291,
            "tmdate": 1701023921291,
            "mdate": 1701023921291,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]