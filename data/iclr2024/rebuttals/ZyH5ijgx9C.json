[
    {
        "title": "Efficient Stagewise Pretraining via Progressive Subnetworks"
    },
    {
        "review": {
            "id": "gYU8PobFKD",
            "forum": "ZyH5ijgx9C",
            "replyto": "ZyH5ijgx9C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6775/Reviewer_BbtU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6775/Reviewer_BbtU"
            ],
            "content": {
                "summary": {
                    "value": "This article proposes a new form of stagewise training, called Random Path Training (RAPTR). Rather than increasing the number of layers trained by stacking them like in Progressive or Gradual Stacking, the authors propose to sample a subnetwork of the full network during training randomly and progressively increase the subnetwork size (while scaling outputs appropriately), which allows evaluation during training.\nThey show empirical improvement for the same FLOPS over other stagewise training techniques, as well as a theoretical study of the change of the losses when changing stages.\n\n[Edit: following my comment, I am increasing my note from a 3 to a 5 for the various improvements and clarifications provided by the authors, and may increase further to a 6 following discussions with the other reviewers.]"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The article presents a simple and effective stagewise training paradigm, by using layer dropping in the start of training. \nThe experiments are clear and show the effectiveness of the method for a given FLOPs budget. \nThe new proposed rescaling of the outputs seems sound and improves the results.\nThe theoretical analysis of the stability of the network seems interesting."
                },
                "weaknesses": {
                    "value": "**Improvement** The experimental results do not show a consistent improvement of the method over the other stacking methods. Table 1 and 2 show very limited improvement in some cases, for the same value of FLOPs; for schedules that have been chosen by unclear means.\n\nThe results in Table 3 are also surprising. RaPTr does show an improvement on Tydi QA and SQuADv2 other Stacking, but not for the other metrics. The alternative schedule is not detailed and merits more introduction. It is impossible to compare Stacking with RaPTras as long as Stacking has not been trained with this alternative schedule, or a similar one. (This could be done by training the entire model with 30K steps, then training only the first layers, and restacking the previously trained layers to extend the model, rather than stacking the same layer). Otherwise, the improvement due to RaPTr seems relatively small without this schedule. The small variance claimed by the authors is only proven with the alternative schedule and not RaPTr in general. \n\n\n**Notations** All along the article, the notations of the schedule of RaPTr are very inconsistent and unclear.\nThe schedule $T^{1:k} \u00d7 (p^{1:k}, I^{1:k})$ is never used after its definition if I'm not mistaken. This is logical as it is often useless: $I$ is denoted to be the entire set during all of the experiments, and the stage times are always equal. Only $p$ ever changes during the experiments. Similarly,  The sense of $I$ reverses during the paper. At first, it defines the set of fixed layers before seemingly being reversed during the experiments as $I$ is the entire set $(1,24)$ (which is also not consistent with the claim that the first and last layers are always fixed). $I_s$ in Algorithm 1. is not defined and just seems to be $I$. \n\n**Schedule** The way schedules are chosen is never clear. In the experiments, the whole set of layers is used for $I$ with equal the same number of steps in every stage. Why is only $p$ varying? In Table 2, the stages are defined as 6-8-10-12 for RaPTr and 6-9-12 for stacking for BERT-base, and then once again equal for BERT-Large. Why were they different at first?\n\n**Section 4.2** is extremely unclear. The normalization is very dependent on the initialization, however, the choice of initialization is never discussed. Similarly, the experimental values during training compared to initialization are not discussed. What we are supposed to conclude from Lemma 4.3. is not clear at all.\nIn particular, the link between Section 4.2. and the experimental results in Table 2. are unclear. The goodness of fit of Fig 2.b seems pretty low. The sentence \"$\\Psi^l$ increases slowly with $l$\" is surprising considering that the value decreases until $l=21$. Many claims do not seem to make sense or are never explained: \"suggesting a worse-case bound of $O(L^{\u22120.88})$\" ? What does \"A simple calculation shows that the gap between a $L\u22121$ random subnetwork and the full model scales as $O(L^\u2212c)$ for some c \u2265 0.5\" mean?\n\n**Novelty** The technique proposed is very close to other layer-dropping techniques, in particular of Zhang et al. 2019, which also aims at reducing the FLOPs needed for the training. The main difference is that the dropping is done at the start of training rather than the end, and the different rescaling.\n\n**Figures** The Figures either bring very limited information or are very hard to read. Figure 1 could have been substituted by a simple Table. Figure 2 is never referred to anywhere. Figure 3 is extremely hard to read and never really explained well.\n\n\nThis article thus brings limited novelty or improvement to other methods and is often very unclear. As I have trouble understanding Section 4.2, I am for now keeping my confidence at 3 in case I am just misreading this Section."
                },
                "questions": {
                    "value": "\"The intuition is that since the first and last layers play different roles\": This seems logical. This does ask the question of whether not only the first and last but $k$ first or/and $k$ lasts layers may need to be fixed. Are the early or later layers more important to be fixed?\n\nI don't really understand why the experiments on BERT are qualified as \"short horizon settings\".\n\n*Errors or remarks:*\n\n* Introduction: \"Gradual stacking (Gong et al., 2019) and Progressive stacking (Reddi et al., 2023)\", the order is reversed, it should be Progressive for Gong et al. and Gradual for Reddi et al.\n\n* Sec 2: \"the computed based\"\n\n* Figure 1.b. \"for fixed set\" -> \"for the fixed set\" \n\n* Table 2 is quite unclear. The values displayed are not defined clearly (losses?). \"lower better loss\" means nothing. \"Layerdrop\" is not defined anywhere in the article. If it is meant to represent the paper method, it is concerning that it uses a name that is already of another method.\n\n* Table 3: Equiflop is not defined.\n\n* Sec 4. \"L-RaPTr\" was not defined until now.\n\n* \"table\" and \"appendix\" should be in upper case. Replace \"fig\" by \"Figure\".\n\n* Sec 4.2 The norm is not defined. Vector/Matrix norm?\n\n* Why is Section 4.2 referred several times inside Section 4.2??\n\n* \"O(L\u22650.88)\" means nothing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Reviewer_BbtU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6775/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675199031,
            "cdate": 1698675199031,
            "tmdate": 1700758342333,
            "mdate": 1700758342333,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dkKzWlhnn0",
                "forum": "ZyH5ijgx9C",
                "replyto": "gYU8PobFKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their remarks and suggestions. Please find our responses to your questions below.\n\n***Weaknesses***\n\n**Improvement**\n\n- **\u201cResults do not show a consistent improvement of the method over the other stacking methods\u201d. \"Table 1 and Table 2 show very limited improvement\u201d.**\n\nTable 1 aims to compare RAPTR and the baseline models at equal training steps. The aim is to show that RAPTR can achieve similar perplexity and fine-tuning performance as the baseline. However, motivated by previous works like [2], we conducted extensive experiments (with learning rate tuning) to compare baseline and RAPTR at equivalent flops. Even with extensive hyperparameter tuning, RAPTR still performs better than the baseline at shorter horizon training, when the models observe only a few epochs of training over the training data. This observation can be crucial for large language models, which are mostly trained with a single epoch over data. \n\n- **\u201cSchedules \u2026 chosen by unclear means.\u201d**\n\nWe followed the schedules from [1] and reported the best that we observed during our experiments. We provide a description of the schedules and an ablation study on the effect of schedules on bert-base in section C in the appendix in the newer version.  \n\n- **The set of evaluation tasks in Table 3 isn\u2019t comprehensive enough.**\n\nWe further perform evaluations on other downstream tasks in Table 11 in the appendix. In summary, RAPTR performs better than baseline models in completion tasks like Lambada, Hellaswag, and Storycloze, in addition to QA tasks like web questions and natural questions. We also provide downstream performance at higher shot in-context settings. On average, RAPTR performs 1-2% better than baseline training while being 20% faster.\n\n\n- **\u201cRAPTR does show an improvement on Tydi QA and SQuADv2 other Stacking, but not for the other metrics.\u201d**\n\nThis is not entirely correct. RAPTR without a 30k warmup is also better on TriviaQA and is almost as good on SuperGLUE. We have included more detailed evaluations and find RAPTR to be better than stacking on multiple metrics.\n\n- **\u201cThe improvement due to RAPTR seems relatively small without this (initial warmup based) schedule\u201d,\n\u201cIt is impossible to compare Stacking with RAPTR as long as Stacking has not been trained with this alternative schedule\u201d**\n\nThis is not true; RAPTR without this schedule is already ~4% better on evals compared to Stacking, which is a significant gap. With the alternative schedule, the gap is even larger (5.5%). Thus the schedule is not as important when compared to Stacking.\n\nBesides, the schedule proposed by the reviewer for stacking with warmup is RAPTR in disguise, with a different subnetwork schedule (the first stage trains all layers, the second stage only trains the first few layers, and the depth is increased incrementally over stages). At this point, the method is no longer performing \u201cstacking\u201d of layers.\n\n\n\n\n\n**Notation issues**\n- **\u201cThe schedule $T^{1:k} \\times (p^{1:k}, I^{1:k})$ is never used\u201d.**\n\nThis notation is used for a more general definition of RapTr in Algorithm 1.\n\n- **\u201c$I$ is denoted to be the entire set during all of the experiments, and the stage times are always equal.\u201d, \u201cThe definition of $I$ reverses\u201d**\n\nThis is not true. $I$ is defined as the set {1, 24} and not the range $(1, 24)$ in our experiments (see \u201cNotations for RAPTR and stacking schedules\u201d section).\n\nAs for stage times, they need not always be equal. For BERT we use equal schedules, but for UL2 experiments, as presented in section B.2, we use a different \u201cproportional\u201d schedule, where we spend time proportional to $i$ in stage $i$. We have added more details in section C in the new version. The only quantity that stays fixed during training is the fixed set $I$ that comprises the first and last set. However, this set in itself can be changed over time to give other RAPTR schedules. \n\n\n\n- **\"How were schedules selected\", \"Changes in schedule from table 2 to table 3?\"**\n\nThe decision of schedules is motivated by the schedules in [1]. The schedule lengths were decided to be either Equal or Proportional, depending on the flop counts we need in the end. Please see the response to the previous question for more details. Please find more details in Appendix C in the new version. As for the difference between RAPTR and Stacking schedules in Table 2, we picked the best schedule for each based on Table 1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6775/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700459941626,
                "cdate": 1700459941626,
                "tmdate": 1700625060787,
                "mdate": 1700625060787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rhVBzJjqRO",
                "forum": "ZyH5ijgx9C",
                "replyto": "gYU8PobFKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continued response"
                    },
                    "comment": {
                        "value": "**Clarity: Section 4.2 is extremely unclear.** \n\n- **\u201cNormalization is dependent on initialization, however, the choice of initialization is never discussed.\u201d**\n\nThe initialization has been clearly stated in the statement of Lemma 4.3. We show that under Xavier initialization normally used in deep learning, normalization layers, and residual connections help stabilize the training of a linear model under RAPTR.\n\n- **\u201cWhat we are supposed to conclude from Lemma 4.3\u201d. \u201c... the link between Section 4.2. and the experimental results in Table 2 (is unclear)\u201d**\n\nLemma 4.3 aims to understand how stable the loss is at the transition between different stages, which is key to why RAPTR can provide speedups. In general, one could expect the loss to spike and training could spend additional training steps (FLOPs) to first recover after the stage transition. However, Lemma 4.3 shows that because of the presence of normalization layers and residual connections, the blowup in training loss at the transitions is bounded by $\\mathcal{O}(1/\\sqrt{L})$ for linear networks. We believe a theoretical understanding of the benefit of RAPTR for training is an important future direction to consider as a follow-up to this work.\n\n\n- **Many claims do not seem to make sense. \u201cSuggesting a worst-case lower bound of $\\mathcal{O}(L^{-0.88})$\u201d?**\n\nThe lower bound of $\\mathcal{O}(L^{-0.88})$ follows from the observation that $\\Psi_{\\ell}$ varies with $\\ell$ as $(L/\\ell)^{0.12}$ (please see figure 2b), while the norm of the output changes linearly with $L$ (please see figure 2a). Hence, the net difference between the two losses under study by theorem 4.2 should be bounded by $\\mathcal{O}$ $(1/L$  $\\sum_{\\ell} \\Psi_{\\ell} / ||F||)$ $=$ $1/L$ $\\mathcal{O}(1/L^{1.88} \\cdot \\sum_{\\ell} 1/\\ell^{0.12} )$  $=$ $\\mathcal{O}(L^{-0.88})$.\n\n- **\u201cA simple calculation shows the gap between L-1 random subnetwork and full model scales as $\\mathcal{O}(L^{-c})$\u201d even mean?**\n\nWe observe that the norm of the output increases as $\\Omega(L^{0.88})$ in all settings, while $\\psi_{\\ell}$ varies as $(L/\\ell)^{0.5}$ with $\\ell$. Hence, the net difference between the two losses under study by theorem 4.2 should be bounded by $\\mathcal{O}(1/L \\sum_{\\ell} \\Psi_{\\ell} / ||F||)$ $= \\mathcal{O}(1/L^{1.38} \\cdot \\sum_{\\ell}  (1/\\ell)^{0.5} ) =   \\mathcal{O}(1/L^{0.88})$.\n\nWe will clarify more on this in the main paper in the revision.\n\n**Novelty: \u201cThe technique proposed is very close to other layer-dropping techniques\u201d**\n\nWe would argue that RAPTR (and progressive subnetworks) is different from layer-dropping in motivation & concept, implementation, and effectiveness. Please see the common response for details on differences.\n \n\n**Figures carry minimal information as well.**\n\nThank you for the suggestion. We will add additional details to the plots in the new version.\n\n***Questions***\n\n**Q1: Further ablation studies are necessary on the layers that are necessary to be fixed during training.**\n\n Thank you for the suggestion. We conducted more experiments for the BERT-base, which shows that fixing more layers during training can hurt performance. Fixing the first and last layers seems to be the best. Please refer to Table 6 in the revision.\n\n**Q2: Why are experiments on BERT qualified as \u201cshort horizon settings\u201d?**\n\n We conducted experiments on BERT-base in Table 2 under 3 different settings, short horizon (75k training steps on baseline and 100k training steps on RAPTR and stacking), medium horizon (170k training steps on baseline and 225k training steps on RAPTR and stacking), and longer horizon setting (510k training steps on baseline and 675k training steps on RAPTR and stacking). By short horizon, we mean training for fewer steps (or epochs), motivated by experiments in Kaddour et al.\n\n\n\n***Errors and typos***\n\nThank you for the suggestions. We have fixed the typos and the errors pointed out by the reviewer in the revision.\n\n***References***\n\n1: Efficient training of language models using few-shot learning. Reddi et al'23\n\n2: No train no gain: Revisiting efficient training algorithms for transformer-based language models. Kaddour et al.'23"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6775/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700460409051,
                "cdate": 1700460409051,
                "tmdate": 1700460501665,
                "mdate": 1700460501665,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fZX8ruyAsh",
            "forum": "ZyH5ijgx9C",
            "replyto": "ZyH5ijgx9C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6775/Reviewer_NDaJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6775/Reviewer_NDaJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to pre-train transformers efficiently and proposes a pre-training framework Random Path Training (RAPTR). The main idea of RAPTR is to train random sub-layers of the transformer and progressively increase the number of layers in stages. To further stabilize the training, the authors propose several techniques e.g. scaling the intermediate outputs and fixing the first and last layers. The authors also show the theoretical support for the training stability of RAPTR. Experiments on BERT and UL2 language model pre-training demonstrated the effectiveness of the proposed method. Compared with baselines like progressive stacking and layer-drop, RAPTR achieves lower perplexity under the same computation costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The theoretical analysis and experimental results provide useful insights for pre-training large models.\n- The proposed method is simple and effective and can be adapted to many deep neural networks.\n- The paper is clearly written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The idea of training sub-layers progressively is not novel, which is similar to [1][2].\n- RAPTR introduces many hyper-parameters and it would be difficult to tune these hyper-parameters in the pre-training setup, which would hinder the application of this type of work.\n\n[1] Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping\n\n[2] Towards Efficient NLP: A Standard Evaluation and A Strong Baseline"
                },
                "questions": {
                    "value": "- How to determine the hyper-parameters of RAPTR, e.g. the number of stages and the training steps in each stage? Does it affect the learning rate schedule? Is it affected by the model scale?\n- What is the training setup of baselines and the proposed method in Table 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Reviewer_NDaJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6775/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824563736,
            "cdate": 1698824563736,
            "tmdate": 1699636781575,
            "mdate": 1699636781575,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jZfybXHCr7",
                "forum": "ZyH5ijgx9C",
                "replyto": "fZX8ruyAsh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments on the *simplicity* and *effectiveness* of the proposed method. Please find our responses to your questions below.\n\n***Questions***\n\n**Q1: \u201cHow to determine the hyper-parameters of RAPTR?\u201d**\n \nIn our explorations, we tried simple and intuitive schedules adapted from stacking approaches, and that already provided good results. It is very likely that tuning for the schedule could lead to larger gains. We also explored the effect of different schedules with similar FLOPs in Table 3, and found most of them to be reasonably effective and better than baseline. The schedule we used for most results, and that we prescribe, is: train an L layer network with L/2 - 2L/3 - 5L/6 - L schedule for RAPTR with either uniform schedule (equal time in each stage) with ~1.33x speedup or proportional schedule (time proportional to $i$ in stage $i$) giving ~1.2x speedup. This in addition to other design choices (like scaling, fixing layers, and full training warmup) has been effective for both BERT and UL2.\n\n \n**Q2: \u201cWhat is the training setup of baselines and the proposed method in Table 2?\u201d.**\n\nWe follow the same training setup as Table 1. To equate the flop counts of the baseline and the proposed method, we train the proposed methods longer. For example, when the baseline is trained for 510k steps, stacking and RAPTR (which are supposed to be 1.33x faster) are run for 675k steps (675k $\\approx$ 510k $\\times 4/3$ ). To ensure we have a fair comparison, we run each method at different learning rate scales and report the best eval loss among them. \n\nThe results show that under extensive hyperparameter tuning which allows fair comparison, RAPTR and stacking are better than the baseline model, especially at a shorter horizon (fewer epochs) settings.\n\n\n\n***Weaknesses.***\n\n**W1: \u201cThe idea of training sub-layers progressively is not novel [1,2]\u201d**\n\nWe would argue that RAPTR (and progressive subnetworks) is different from layer-dropping [1] in motivation & concept, implementation and effectiveness. Please see the common response for details on differences. The motivation of [2] is very different, it is to provide efficiency during inference, not training. The aim of [2] is to train a model on which one can perform early exit for inference speeds. However, such training can possibly degrade the performance of the baseline model. \n\n\n**W2: \u201cRAPTR introduces many hyper-parameters and would be difficult to tune.\u201d**\n\nThe important hyperparameters introduced in RAPTR are the number of stages, path lengths, and the number of training steps in each stage. We concur that the number of stages and path lengths in each stage is a heuristic that needs to be pre-decided before training.\n\nWe ran some more ablation studies (table 5 in the revision) on BERT-base that show that RAPTR is robust to stage schedule selection. Since it is computationally expensive to verify in UL2 settings, we continue with the following design principle. train an L layer network with L/2 - 2L/3 - 5L/6 - L schedule for RAPTR with either uniform schedule (equal time in each stage) with ~1.33x speedup or proportional schedule (time proportional to $i$ in stage $i$) giving ~1.2x speedup. For other target speedups, please see section C in the revised version.\n\n\n***References***\n\n1: Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping\n\n2: Towards Efficient NLP: A Standard Evaluation and A Strong Baseline"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6775/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457689846,
                "cdate": 1700457689846,
                "tmdate": 1700457689846,
                "mdate": 1700457689846,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YKQ0B0HzYP",
            "forum": "ZyH5ijgx9C",
            "replyto": "ZyH5ijgx9C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6775/Reviewer_6BSs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6775/Reviewer_6BSs"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new stagewise training approach called \"progressive subnetwork training\" for efficiently pretraining large language models. It introduces an instantiation called Random Path Training (RAPTR) which trains random subsets/paths of layers in the network, progressively increasing the path length over stages. Experiments on BERT and UL2 models show RAPTR can match or improve baseline pretraining loss with 20-33% fewer FLOPs. RAPTR shows improved downstream task performance on UL2, gaining 1-5% on QA/GLUE over baseline and stacking methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and clearly presented; \n- The paper proposes a novel and intuitive stagewise training paradigm with theoretical motivation and it achieves strong empirical gains over baseline and stacking methods, especially for short training regimes.\n- The paper provides theoretical analysis on stability of subnetwork training in residual networks and the algorithm is simple to implement on top of standard training. Detailed ablations regarding the fixed layer and scale have also been presented;"
                },
                "weaknesses": {
                    "value": "- Theoretical results are good while limited to simplified linear residual network settings.\n- The gains in pretraining flops diminish in the asymptotic long training setting. \n- Downstream task improvements lack sufficient analysis on why RAPTR has implicit biases on different tasks, for example, it seems to hurt the multilingual QA performance when adding the 30k. \n- Besides the flops, the real wall-clock time might also be good to provide, given in some cases, flops disagree with wall-clock time for efficiency measurements [1]; \n- Besides different architecture and objectives (BERT/UL2), whether the proposed method scales with model sizes and fit to LM is unknown; \n\n[1] Dehghani, Mostafa, et al. \"The efficiency misnomer.\" arXiv preprint arXiv:2110.12894 (2021)."
                },
                "questions": {
                    "value": "- Could the author elaborate on the real wall-clock time gain with the pretraining experiments; \n- Could the authors explain more on the varied performance gain in Table 3;  \n- Besides the ppl comparison for the ablation studies regarding the fixed layer and scale, could the author also provide the detailed downstream performance to show if the design choice affects both pretraning and downstream;"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6775/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842086253,
            "cdate": 1698842086253,
            "tmdate": 1699636781459,
            "mdate": 1699636781459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uUYvCV7hZ4",
                "forum": "ZyH5ijgx9C",
                "replyto": "YKQ0B0HzYP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their remarks and suggestions. Please find our responses to your questions below.\n\n***Questions***\n\n**Q1: \"...elaborate on the real wall-clock time gain with the pretraining experiments\"**\n\nThe reason we reported FLOPs is because wall-clock time can be highly hardware and implementation-dependent. We tested wall-clock times on the machines we used and found that a theoretical FLOPs speedup of 1.33x for BERT-base was equivalent to a wall-clock speedup of 1.26x. For UL2, FLOPs speedup matches wall-clock speedup (due to the increased depth and width of the model), so 1.2x speedup in the paper is also the wall-clock speedup. Please see section D in the appendix in the revision.\n\n**Q2: \"...explain more about the varied performance gain in Table 3\"**\n\nWe observe that RAPTR improves on some tasks more than others compared to the baseline. The trend is that gain is higher for contextual QA tasks (TydiQA and SQuADv2) and lower for non-contextual QA tasks (TriviaQA). Overall RAPTR improves over the baseline across the board, while requiring ~20% fewer FLOPs. Furthermore, we have added more results on other downstream tasks and higher in-context settings (5-shot) in the revision. RAPTR shows similar improvements over the baseline model in all settings.\n\n\n**Q3. \u201c...show if the design choice affects both pretraining and downstream\u201d (for fixed layers and scaling)**\n \nIn Table 5 in the appendix of the revision, we conduct an ablation study on different fixed player candidate sets for BERT-base training with RAPTR. We observe that fixing the first and the last layers helps pre-training loss. Furthermore, differences in pre-training also show small differences in fine-tuning performance on different downstream evaluations. \n\n\n\n***Weaknesses***\n\n**W1: \u201cTheoretical results are good while limited to simplified linear residual network settings\u201d**\n\nThank you for appreciating the theoretical results. The results aim to understand the importance of layer normalization and residual connection layers in the model, and analysis for Transformers is still in very nascent stages. Thus results in the linear setting allow us to clearly isolate the role of these design choices.\n\n\n**W2: \u201cGains in pretraining flops diminish in the asymptotic long training setting.\u201d**\n\nThat is correct, this observation aligns with those made in [1] for BERT training. However recent language models (like GPT, UL2, Llama) only make 1 (or few) passes over a huge dataset, and are not in the \u201casymptotic\u201d setting. In this setting our paper shows that RAPTR (and even stacking) improves training time by 20% or more, which could be a significant saving in such settings where models are trained for a few months.\n\n**W3: \u201cDownstream task improvements lack sufficient analysis on why RAPTR has implicit biases on different tasks\u201d, and \u201churt the multilingual QA performance when adding the 30k\u201d**\n\nThis is indeed an interesting question, and we could not find a simple explanation for these findings. Perhaps it deserves a separate exploration like some recent papers that are dedicated to this topic of inductive bias [2, 3]. \n\n**W4: \u201c...whether the proposed method scales with model sizes and fits to LM is unknown\u201d**\n\nIn this paper we have tried 3 different model sizes, 100M, 300M, and 1B, and also two kinds of language models: encoder-only MLM and the more recent decoder-only causal LM. In both cases, we find RAPTR provides speedup and downstream benefits. This exploration is more diverse than many prior papers on efficient pretraining.\n\nOur 1B baseline model already demonstrates good downstream 1-shot performance, and thus we considered it an appropriate setting. Exploring scaling behaviors for pretraining beyond 1B parameters can get very expensive. However, if the reviewer believes that this will add more value, we can include some experiments on a 3B scale in the final version.\n\n\n***References***\n\n1: No train no gain: Revisiting efficient training algorithms for transformer-based language models. Kaddour et al.'23\n\n2: Understanding contrastive learning requires incorporating inductive biases. Saunshi et al.'22\n\n3: Same pre-training loss, better downstream: Implicit bias matters for language models. Lieu et al'22"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6775/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700456605320,
                "cdate": 1700456605320,
                "tmdate": 1700456605320,
                "mdate": 1700456605320,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TJq91VZ9sf",
            "forum": "ZyH5ijgx9C",
            "replyto": "ZyH5ijgx9C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6775/Reviewer_9zZw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6775/Reviewer_9zZw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to train an increasingly larger subset of layers of an LLM with a manual schedule of layer dropout with the motivation of using less compute to obtain the same pretraining performance. This work shows computational saving and a slight boost in downstream performance of BERT and UL2 pretraining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "RAPTR is a simple and effective method for training progressively larger networks and saving compute. It's also interesting that layer dropout as a form of regularization can improve stability and even downstream performance."
                },
                "weaknesses": {
                    "value": "BERT baselines seem strong but it's unclear how competitive the UL2 baselines are. The soundness of baseline might be important to show as weak baselines can make any results possible. A reference on these hyperparameters would be helpful.\n\nOne unsatisfying aspect of this work is that it does not analyze how the compute-efficient frontier changes with this pretraining procedure. Model sweep would also help quantify the amount of improvement in downstream performance as we can see an overall trend more clearly."
                },
                "questions": {
                    "value": "."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6775/Reviewer_9zZw"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6775/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890472424,
            "cdate": 1698890472424,
            "tmdate": 1699636781355,
            "mdate": 1699636781355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LblVUShFNe",
                "forum": "ZyH5ijgx9C",
                "replyto": "TJq91VZ9sf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6775/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and suggestions. Please find our responses below.\n\n**How competitive are UL2 baselines? A reference on hyperparameters will be useful.**\n\nWe use the default settings from [1] and do not optimize for hyperparameters. The mixture rates for the dataset are used from [2]. We tuned hyperparameters for the learning rate and learning rate schedule (square-root decay used in the original paper, and cosine decay).\n\n**How does the compute-efficient frontier change with this pre-training procedure? Model sweep will help quantify the amount of improvement in downstream performance.**\n\nWe are sorry but we're having difficulty understanding the question. If it pertains to how RAPTR behaves concerning model scale, scaling behaviors for pretraining beyond 1 billion parameters can become quite costly. In the eventual version, we may incorporate additional experiments on a 3 billion scale if the reviewer deems the results worthwhile. Investigating the scaling laws for RAPTR could be a subsequent exploration following this research.\n\n\n\n[1] UL2: Unifying Language Learning Paradigms. Tay et al.\u201922 \n\n[2] LLaMA: Open and Efficient Foundation Language Models. Touvron et al.\u201923"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6775/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489943916,
                "cdate": 1700489943916,
                "tmdate": 1700489943916,
                "mdate": 1700489943916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]