[
    {
        "title": "Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy"
    },
    {
        "review": {
            "id": "NYVjdgowRk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6785/Reviewer_zDGs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6785/Reviewer_zDGs"
            ],
            "forum": "eFWG9Cy3WK",
            "replyto": "eFWG9Cy3WK",
            "content": {
                "summary": {
                    "value": "The paper distills a large body of experts (in a mixture-of-experts model) into a few experts. This saves memory and improves fine-tunability of the final model. The core idea is to repermute neurons, group based on neuron \"routes\", then merge models. The authors show memory and FLOP reductions with negligible quality loss."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The figures are clear, the motivation is well-written, and the paper is overall well put together. \n- The paper presents a large array of experimental results and ablations; the results are convincing.\n- Figure 1: Impressive results. (A tradeoff curve might be nice? e.g., lines connect your points. also nit: The legend kinda blends in. Maybe give it a strong border or clearer background?)\n- Figure 2 is likewise well done. The lighted dotted lines are and spacing clearly separate the three parts, and the illustration of \"highly frequent\" to \"cluster center\" is helpful. (nit: I wish the fonts and sketch-esque style was applied to everything)\n- Figure 3's insight and accompanying visualizations are clear and insightful. Is this used later on anywhere? e.g., model can be compressed more aggressively for SST2 than for COPA."
                },
                "weaknesses": {
                    "value": "- Experts permutation alignment (then computing similarity based on routes) is a big part of the paper, but the details are a bit lost on me. It could be worth adding more to 3.1, covering the basics of how the different possible permutations are searched.\n- In 3.1, it *seems like there's a chicken and egg problem -- we need alignment to know which experts are more similar *but we need to know which expert is the \"reference\" to re-align. How is that resolved?"
                },
                "questions": {
                    "value": "- nit: the changing underline baseline is visually unappealing. not sure if there's a way to glue underlines to the bottoms of letters. this is a super-nit of course, doesn't really matter at all"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6785/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697275511271,
            "cdate": 1697275511271,
            "tmdate": 1699636783515,
            "mdate": 1699636783515,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RSEoCOAbBX",
                "forum": "eFWG9Cy3WK",
                "replyto": "NYVjdgowRk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate Reviewer zDGs for their careful review and a great summary of our contributions. We are gratified by your recognition of its strengths, particularly that \"the figures are clear,\" \"the motivation is well-written,\" and \"the paper is overall well put together.\" Your comments about our extensive array of experimental results and the convincing nature of these results are highly encouraging. We appreciate your constructive suggestions, please see our responses in what follows.\n\n**[W1. Details of Experts Permutation Alignment]**\n\nThanks for this very constructive comment about \u201ccovering the basics of how the different possible permutations are searched\u201d. Searching optimal permutation can be formulated as a \u201clinear assignment problem\u201d and efficiently solved, specifically:\n\n-   In detail, the permutation matrix is initially defined as a square matrix with each row and column containing precisely one element valued at $1$, while all remaining elements are $0$. Considering an expert with two distinct weight matrices $\\mathtt{W}\\_{\\text {in}}$ and $\\mathtt{W}\\_{\\text {out}}$, the rearrangement of these weight matrices under permutation is represented as $\\mathtt{W}\\_{\\text{out}}\\mathtt P^{\\mathrm T}$ and $\\mathtt P\\mathtt{W}\\_{\\text {in}}$, respectively, for a chosen permutation matrix $\\mathtt P$.\n\n-   To determine the optimal permutation matrix, our strategy is to minimize the L2 norm of the difference between the permuted matrices. This involves an optimization method formulated as:\n    $$\\mathrm{argmin}\\_{\\mathtt P} \\left\\|\\mathrm{vec}([\\mathtt W\\_{\\text {in}}^{(1)}, \\mathtt W\\_{\\text {out}}^{(1)}]) - \\mathrm{vec}([\\mathtt P\\mathtt W\\_{\\text {in}}^{(2)}, \\mathtt W\\_{\\text {out}}^{(2)}\\mathtt P^{\\text T}])\\right\\|^2 = \\mathrm {argmax}\\_{\\mathtt P} \\left\\langle \\mathtt W\\_{\\text {in}}^{(1)}, \\mathtt P\\mathtt W\\_{\\text {in}}^{(2)} \\right\\rangle\\_{\\text F} + \\left\\langle \\mathtt W\\_{\\text {out}}^{(1)}, \\mathtt W\\_{\\text {out}}^{(2)}\\mathtt P^{\\text T}\\right\\rangle\\_{\\text F}$$ \n\nThe process of optimizing this is known as a 'linear assignment problem' (LAP), for which the Hungarian Algorithm ([R1]) provides an effective and feasible solution. More details are included in the last paragraph of Appendix 2, and the Python-style pseudo-code is provided in Appendix 3, in our revised paper.\n\n**[W2. Experts Permutation and Grouping]**\n\nThank you for your insightful question. We address your concerns from several aspects: \n\n-   Your point about the \u201cchick and egg problem\u201d is interesting, but factually we do not \u201cneed alignment to know which experts are more similar\u201d. This is because we compute pairwise similarity among the experts only based on the router\u2019s output logits while permuting expert weight matrices does not influence it. Therefore, it is appropriate to conduct permutation alignment of experts before calculating their similarities.\n-   Yes, aligning experts within a group together is optimal for merging. However, computing the optimal solution for the permutation alignment of all experts simultaneously would be extremely time-consuming. Therefore, we align all experts in a single SMoE layer with the first expert. \n-   To further address your concerns, we conduct additional experiments to compare the performance of within-layer aligned (ours) M-SMoE and within-group aligned M-SMoE. As shown in Table R1, the performance difference between the two is negligible. \n\nTable R1: M-SMoE performance on the COPA and MultiRC tasks with the *switch-base-32* model.\n\n| Method                        | COPA  | MultiRC |\n| ----------------------------- | ----- | ------- |\n| Within-layer Alignment (ours) | $68.00$ | $75.57$   |\n| Within-group Alignment        | $68.00$ | $75.53$   |\n\n**[Formatting Issues]** \n\nWe appreciate your attention to detail and agree that the visual appeal of the document is important. We have looked into all the formatting issues you mentioned and ensured a more consistent and aesthetically pleasing presentation in our revised paper. Specifically:\n\n*   The issue that \u201cthe legend kinda blends in\u201d in Figure 1 has been fixed by giving it \u201ca strong border\u201d and a \u201cclearer background\u201d. Thanks for the constructive suggestion.\n*   Thanks for pointing out that \u201cthe changing underline baseline is visually unappealing\u201d. We have fixed this issue by replacing the \u201c\\underline\u201d latex command with \u201c\\uline\u201d provided by the *ulem* package in our revised paper. \n\n**[Leveraging the Insights Gained from Figure 3 Analysis]** \n\nGreat catch. Thanks for recognizing that the \u201caccompanying visualizations are clear and insightful\u201d. Yes, it is used later to drive our design of adaptive merging ratio, which suggests a diverse number of dominant experts and corresponding groups. For instance, in Figure 3, the latter half of SMoE layers in the left two models will undergo more aggressive merging compared to the right two models.\n\n[R1] https://web.eecs.umich.edu/~pettie/matching/Kuhn-hungarian-assignment.pdf"
                    },
                    "title": {
                        "value": "Responses to Reviewer zDGs"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389571689,
                "cdate": 1700389571689,
                "tmdate": 1700389838058,
                "mdate": 1700389838058,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aPKlLLxMBj",
            "forum": "eFWG9Cy3WK",
            "replyto": "eFWG9Cy3WK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6785/Reviewer_FC53"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6785/Reviewer_FC53"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a mechanism for merging multiple experts by merging redundant experts while preserving as much as knowledge as possible. It is achieved by:\n1. Aligning the weights of any two given experts at a time using a permutation matrix since two models that are merely weight permutations of each other are equivalent. \n2. The router policy is used to group experts into groups of similar experts. Based on activation frequency, weight permutation-corrected experts in a group are merged together.  \n3. The final merged expert is further compressed by a low-rank decomposition and pruning of the incoherent part."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The authors perform an extensive experimental analysis of each of their design decisions for:\n1. Their averaging strategy (Tab. 8)\n2. Need for permutation alignment (Tab. 6)\n3. Similarity function and the superiority of use of router logits (Tab. 4)\n\nThe authors also provide comparisons on multiple text datasets"
                },
                "weaknesses": {
                    "value": "A theoretical proof of either 1. Optimality of their expert merging algorithm or 2. An error bound on either the information loss/performance degradation based on their proposed algorithm would have significantly helped this work.\nAn analysis of the computational cost as the number of SMoE layers increases would be helpful."
                },
                "questions": {
                    "value": "1. Could you provide some theoretical insights into why the merging of experts should be done before compression since compression might be able to get rid of irrelevant information and make it more convenient to compare experts later for merging?\n2. Could you provide some theoretical background for your claim that similar experts would show similar router logits?\n3. An interesting line of investigation is the long-term scalability of how one could add more experts later during the life-cycle after applying M-SMoE."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6785/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6785/Reviewer_FC53",
                        "ICLR.cc/2024/Conference/Submission6785/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6785/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698486164795,
            "cdate": 1698486164795,
            "tmdate": 1700579896529,
            "mdate": 1700579896529,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mWw6Xj4w3m",
                "forum": "eFWG9Cy3WK",
                "replyto": "aPKlLLxMBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer FC53 for their positive feedback on \u201cthe extensive experimental analysis\u201d, \u201ccomparisons on multiple text datasets\u201d and a great summary of our approach. Your recognition of our efforts is appreciated. Please find our detailed response below.\n\n**[W1. Lack of Theoretical Proof]**\n\nThanks for the suggestions. While the absence of theoretical analysis is noted, it does not diminish the value of our contributions. The idea of providing theoretical proof is appreciated and constructive, however, it falls beyond the scope of our present work. The theoretical justification of model merging is an open research question, and some early explorations include [R1] and [R2]. \n\n\n\n**[W2. Computational Cost Analysis]**\n\nThank you for your question. We present analysis and empirical results of the \u201ccomputational cost as the number of SMoE layers increase\u201d of merging. \n\n-   *<The computational cost analysis for each stage.>* Thanks for the constructive suggestion. We present a detailed computational cost analysis for each stage. Our M-SMoE merging method involves three key processes: expert permutation alignment, expert grouping, and expert weight merging. __Firstly__, expert permutation alignment is executed independently within each SMoE layer, making its computational cost linearly proportional to the number of SMoE layers. __Secondly__, expert grouping involves model inference to assess activation frequency and router logits, followed by calculating pair-wise similarity among experts. Given that the model's inference cost remains constant across different numbers of SMoE layers, due to SMoE's sparse activation computation, the primary factor contributing to a linear increase in computational costs is the similarity computation within each SMoE layer. __Lastly__, the process of expert weight merging, carried out within each SMoE layer, further contributes to this linear escalation in computational costs. __In summary__, while certain components of our method exhibit constant computational demands, the overall cost analysis reveals a linear growth pattern.\n\n*   *<Evaluation of computational costs.>* To further address your concerns, we conduct extra experiments for the computational costs of merging. We evaluate the *switch-base-32* model\u2019s computation time cost of expert permutation alignment, expert grouping, and expert weight merging respectively. We maintained a constant ($24$) total number of Transformer layers while varying the number of SMoE layers. The results shown in Table R1 confirm our analysis, indicating that the primary bottleneck in terms of time cost is rooted in the expert permutation alignment, while the bulk of memory cost is attributed to model inference.\n\nTable R1: Evaluation of time cost and memory cost for each stage of M-SMoE.\n\n| Stage                        | Metric      | 2          | 4          | 6          | 8          | 10          | 12          |\n| ---------------------------- | ----------- | ---------- | ---------- | ---------- | ---------- | ----------- | ----------- |\n| Expert Permutation Alignment | Time Cost   | $2.35$ min | $4.61$ min | $6.54$ min | $8.40$ min | $10.30$ min | $12.30$ min |\n|                              | Memory Cost | $1.23$ GB  | $2.36$ GB  | $3.48$ GB  | $4.61$ GB  | $5.73$ GB   | $6.86$ GB   |\n| Expert Grouping              | Time Cost   | $8.03$ s   | $8.21$ s   | $8.22$ s   | $8.26$ s   | $8.20$ s    | $8.24$ s    |\n|                              | Memory Cost | $4.19$ GB  | $5.29$ GB  | $6.39$ GB  | $7.48$ GB  | $8.58$ GB   | $9.68$ GB   |\n| Expert Weight Merging        | Time Cost   | $23$ ms    | $44$ ms    | $66$ ms    | $87$ ms    | $109$ ms    | $139$ ms    |\n|                              | Memory Cost | $1.33$ GB  | $1.83$ GB  | $2.32$ GB  | $2.82$ GB  | $3.31$ GB   | $3.81$ GB   |\n\n\n[R1] https://openreview.net/pdf?id=CQsmMYmlP5T\n\n[R2] https://arxiv.org/pdf/1910.05653.pdf"
                    },
                    "title": {
                        "value": "Responses to Reviewer FC53 [Weakness]"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388636070,
                "cdate": 1700388636070,
                "tmdate": 1700389800290,
                "mdate": 1700389800290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bVgiY79cqe",
                "forum": "eFWG9Cy3WK",
                "replyto": "aPKlLLxMBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[Q1. Theoretical Insights of Merging Before Compression]**\n\nThank you for this excellent question. The reasons lie in several aspects:\n\n-   Our initial intention was to demonstrate that merging can encourage low-rank properties, hence we performed compression after merging. \n-   Moreover, it's important to note that merging will disrupt the sparsity and low-rank patterns established by compression. This is because two compressed experts typically do not exhibit identical patterns. \n\nYour suggestion was insightful, and it led us to carry out further experiments to compare two approaches: 'merging, then compression' (MC-SMoE, ours) and 'compression, then merging' (CM-SMoE) on the COPA and MultiRC tasks, as detailed in Table R2. The results indicate that MC-SMoE not only surpasses CM-SMoE in terms of task performance metrics but also shows advantages in model size and FLOPs. This advantage is primarily due to the preservation of the compression pattern, which is otherwise disrupted when merging follows compression.\n\nTable R2: Comparison between MC-SMoE and CM-SMoE on the COPA and MultiRC tasks with the *switch-base-32* model.\n\n| Method         | COPA    | MultiRC | Model Size | TFLOPs |\n| -------------- | ------- | ------- | ---------- | ------ |\n| MC-SMoE (Ours) | $67.00$ | $73.98$ | $381$M     | $3.83$ |\n| CM-SMoE        | $64.00$ | $73.63$ | $733$M     | $4.65$ |\n\n**[Q2. Theoretical Background of Similarity of Experts and Router Logits]**\n\n-   Thank you for your suggestions. The theoretical justification remains an open research question, and we greatly appreciate your constructive input. We look forward to addressing this aspect in our future work with a thoughtful approach.\n\n-   The insight here is: firstly, SMoE routing tends to stabilize in the early stages of pretraining (which can be shown by Figure 6 in [R3]); therefore, if two experts have similar router logits, it is more likely that they have been trained on similar data, making the experts more alike\n\n**[Q3. Long-term Scalability and Adding More Experts Post-M-SMoE]**\n\nThis comment is insightful. The issue of 'long-term scalability and the addition of more experts during the life-cycle' has been explored in [R4]. It investigates the dynamic addition of experts complemented by regularized pre-training for extra model capacity. Our paper, meanwhile, focuses on the expert merging method, specifically M-SMoE. A synthesis of these two approaches could effectively address your concerns.\n\n[R3] https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pd\n\n[R4] https://arxiv.org/pdf/2305.12281.pdf"
                    },
                    "title": {
                        "value": "Responses to Reviewer FC53 [Questions]"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388717076,
                "cdate": 1700388717076,
                "tmdate": 1700462469715,
                "mdate": 1700462469715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "elnfmIzYVz",
                "forum": "eFWG9Cy3WK",
                "replyto": "bVgiY79cqe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Reviewer_FC53"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Reviewer_FC53"
                ],
                "content": {
                    "title": {
                        "value": "Response to author rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their answers to my concerns. They have adequately answered every question I raised to my satisfaction, and hence, I have chosen to increase my rating to 8.\nThanks"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580158650,
                "cdate": 1700580158650,
                "tmdate": 1700580158650,
                "mdate": 1700580158650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dKeNcFBXvR",
                "forum": "eFWG9Cy3WK",
                "replyto": "aPKlLLxMBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for increasing the score"
                    },
                    "comment": {
                        "value": "Dear Reviewer **FC53**,\n\nWe sincerely appreciate all the helpful feedback and highly positive evaluations provided by reviewer **FC53**.\n\nWe are again very thankful for your time and support, and for increasing our score!\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583526537,
                "cdate": 1700583526537,
                "tmdate": 1700583612691,
                "mdate": 1700583612691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uJsSssSi3x",
            "forum": "eFWG9Cy3WK",
            "replyto": "eFWG9Cy3WK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6785/Reviewer_5S2f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6785/Reviewer_5S2f"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes MC-SMoE, which is a compression method for Mixture-of-Experts models. The idea is to split experts into several groups, where only the most important expert is kept in each group. The authors further propose an algorithm to compress the merged experts. Experiments are provided to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* In this work, the authors study how to consolidate experts in MoE models. This is a very important topic since one of the major bottlenecks of deploying MoE models is the memory usage.\n\n* The problem studied in the paper is well-motivated. It is well-known that there are redundancies in MoE models. This paper leverages this finding and propose algorithms to compress MoE models."
                },
                "weaknesses": {
                    "value": "Concerns and questions about presentation:\n\n* What is the intuition behind experts permutation alignment? Specifically, how is \u201calignment\u201d defined? I can understand from performance-wise (Table 6) that alignment is needed. However, I do not understand how two experts are \u201caligned\u201d using the proposed alignment method. To make the paper self-contained, please include the detailed algorithm used for this.\n\n* How are the results in Figure 3 computed? From my understanding, a Switch-base-32 model is first fine-tuned on each individual task, and then the activation frequencies are computed. Are the models fine-tuned with the load balancing loss [1]? It seems in Figure 3, loads of different experts are extremely unbalanced.\n\n* How is the stable-rank computed? For example, in a specific layer, the 32 experts are compressed into 6 experts. Do you compute the average stable-rank of the 32 experts as \u201cbefore\u201d in Figure 4, and the average stable-rank of the 6 experts as \u201cafter\u201d?\n\n* I do not fully understand how experts are grouped. It is mentioned that \u201ceach non-dominant expert gravitates toward and joins the group led by its most similar dominant expert\u201c. For example, suppose we have two dominant experts $E_1$ and $E_2$, then for a non-dominant expert $E$, do you calculate the similarity of $E$ with $E_1$ and $E_2$, and then assign $E$ to the more similar one? If this is true, is it possible that nearly all non-dominant experts are assigned to the same group?\n\n* The pruning of $S$ needs more justification. It is mentioned that \u201cthe weight columns with the lowest cumulative scores will be removed\u201d. Why are weights pruned according to cumulative importance scores instead of importance scores? The pruning procedure in Appendix A2 also seems ad-hoc. How is this particular pruning schedule chosen?\n\n\nConcerns about experiments:\n\n* I would like to further understand the role of knowledge distillation. The authors mention that all the models (including the baselines) in Table 2 use knowledge distillation. Could the authors provide some results of the dense and full-SMoE models without distillation?\n\n* From Table 2, it seems performance of the model considerably drops after applying the compression technique (M-SMoE vs. MC-SMoE). The authors should provide more detailed analysis on the design of the compression method. For example, will different pruning strategies/schedules work better?\n\n* The authors mention \u201cfurther memory and parameter efficiency\u201d in the paragraph above Algorithm 1. However, no experiments are conducted to evaluate the speed and memory of the MC-SMoE models. The latency results in Table A10 indicate that there is only marginal speed gain of M-SMoE compared with full-SMoE. The authors should benchmark the inference speed (throughput) and memory usage of M-SMoE and MC-MoE, and compare the metrics with the dense and the full MoE models.\n\n[1] https://arxiv.org/pdf/2101.03961.pdf"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6785/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699320976122,
            "cdate": 1699320976122,
            "tmdate": 1699636783244,
            "mdate": 1699636783244,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xmEYSLDImq",
                "forum": "eFWG9Cy3WK",
                "replyto": "uJsSssSi3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to reviewer 5S2f for recognizing the importance (\u201ca very important topic\u201d) and motivation (\u201cwell-motivated\u201d) behind our work on compressing Mixture-of-Experts (MoE) models. To address reviewer 5S2f\u2019s questions, we provide pointwise responses below.\n\n**[Cons 1. Experts Permutation Alignment]** \n\nThe intuition behind permutation alignment is to pull all the models (i.e. experts within one SMoE layer in our setting) into the same local loss basin, as introduced in the third line of the last paragraph in Section 2 of our paper. Specifically:\n\n-   *<Before alignment.>*  As mentioned in the last three lines in the third paragraph of Section 2 in our paper, the experts in SMoE are basically trained on different data subsets and starting from different initializations, resulting in them converging to different basins of the loss landscape[R3]. There existing loss barrier between each pair of basins, which can be measured via the LMC (Linearly Mode Connected) metric (details are included in the last paragraph of Appendix 2 in our revised paper). Such a barrier hinders the merging of experts before alignment.\n-   *<During alignment.>* Alignment permutes the neurons within the expert to minimize the loss barrier between each expert pair while preserving the original functionality. As illustrated in the last few lines in the first paragraph of Section 3.1 in our paper, for a two-layer MLP, both the output channels of its first layer and the input channels of the second layer are permuted via the same order, thus its output embeddings are exactly the same as the one without permutation.\n-   *<After alignment.>*  Via expert alignment, the loss barrier between each two experts is significantly reduced, benefiting the further merging process ([R4, R5, R6, R7]).\n-   *<Existence of the loss barrier in the SMoE case.>* Moreover, we have carried out additional experiments to validate the presence of the loss barrier in the SMoE scenario. More specifically, for each SMoE layer containing $2N$ experts, we modified the weights of the $i$th expert, $\\mathtt{E}\\_i$, to become an interpolation of $\\lambda \\mathtt{E}\\_i + (1-\\lambda)\\mathtt{E}\\_{(i+N)\\\\%N}$ , where $\\lambda$ ranges from $0.5$ to $1$. We then compared the performance of the fine-tuned SMoE, respectively with and without permutation alignment, on the COPA task. The performance (accuracy), as detailed in Table R0, corroborates the existence of a loss barrier among the SMoE experts.\n\nTable R0: Accuracy of different values of $\\lambda$ on the COPA task of the *switch-base-32* model.\n| $\\lambda$  | 1.0   | 0.9   | 0.8   | 0.7   | 0.6   | 0.5   |\n| ---------- | ----- | ----- | ----- | ----- | ----- | ----- |\n| **w.** PA  | 68.00 | 64.00 | 47.00 | 34.00 | 30.00 | 28.00 |\n| **wo.** PA | 68.00 | 60.00 | 40.00 | 26.00 | 20.00 | 20.00 |\n\nThanks for your question, we have enriched the description of the expert permutation alignment procedure and added it to the last of Appendix 2 in our paper. To be more specific:\n\n*   *<The permutation matrix.>* First, we introduce the permutation matrix $\\mathtt{P}$, which is a square matrix where each row and column has exactly one element of 1, with all other elements being 0. It perseveres the functionality of the expert, an MLP with two linear layers $\\mathtt W_{\\text{in}}$ and $\\mathtt W_{\\text{out}}$.\n\n*   *<The optimization through L2 distance minimization.>* Second, we minimize the L2 distance between two experts to align them. Given two layer weights of an expert, denoted as $\\mathtt W\\_{\\text{in}}$ and $\\mathtt W{\\text {out}}$, we consider an optimization approach: (More details are included in the last paragraph of Appendix 2 in our revised paper.)\n    $$\\mathrm{argmin}\\_{\\mathtt P} \\left\\|\\mathrm{vec}([\\mathtt W\\_{\\text {in}}^{(1)}, \\mathtt W\\_{\\text {out}}^{(1)}]) - \\mathrm{vec}([\\mathtt P\\mathtt W\\_{\\text {in}}^{(2)}, \\mathtt W\\_{\\text {out}}^{(2)}\\mathtt P^{\\text T}])\\right\\|^2 = \\mathrm {argmax}\\_{\\mathtt P} \\left\\langle \\mathtt W\\_{\\text {in}}^{(1)}, \\mathtt P\\mathtt W\\_{\\text {in}}^{(2)} \\right\\rangle\\_{\\text F} + \\left\\langle \\mathtt W\\_{\\text {out}}^{(1)}, \\mathtt W\\_{\\text {out}}^{(2)}\\mathtt P^{\\text T}\\right\\rangle\\_{\\text F}$$\n\n*   *<Solving the LAP optimization.>* Finally, the optimization constitutes a \u201clinear assignment problem\u201d (LAP), which can be efficiently and practically solved by the Hungarian Algorithm ([R8]). \n*   *<Detailed revision.>* All explanations above are included in the sixth paragraph of Appendix 2 in our revised paper. We have further provided the Python-style pseudo code for solving the permutation matrix in Appendix 3 of our revised paper.\n\n[R2] https://arxiv.org/pdf/2306.11222.pdf\n\n[R3] https://arxiv.org/pdf/2110.06296.pdf\n\n[R4] https://openreview.net/pdf?id=CQsmMYmlP5T\n\n[R5] https://arxiv.org/pdf/2211.08403.pdf\n\n[R6] https://arxiv.org/pdf/1910.05653.pdf\n\n[R7] https://arxiv.org/pdf/2212.12042.pdf\n\n[R8] https://web.eecs.umich.edu/~pettie/matching/Kuhn-hungarian-assignment.pdf"
                    },
                    "title": {
                        "value": "Responses to Reviewer 5S2f [Cons 1]"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384086829,
                "cdate": 1700384086829,
                "tmdate": 1700389680793,
                "mdate": 1700389680793,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eYMhGw4M0N",
                "forum": "eFWG9Cy3WK",
                "replyto": "uJsSssSi3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[Cons 2. Results in Figure 3]** \nYes, your interpretation that \u201ca switch-base-32 model is first fine-tuned on each individual task, and then the activation frequencies are computed\u201d is correct (which is illustrated in the last two lines of the caption of Figure 3 in our paper). During the fine-tuning process, we did not use load balancing loss, for two reasons:\n*   *<Performance-driven concerns.>* Based on our previous investigation, enabling load balancing regularization will degrade the model performance. As shown in Table R1, after carefully sweeping the coefficient of load balance regularization, we can observe a **consistent** performance degradation.\n\nTable R1: Performance evaluation on the COPA and MultiRC tasks of the *switch-base-32* model fine-tuned with different load balancing loss coefficients.\n\n| Coefficient | COPA (Accuracy) | MultiRC (F1-score) |\n| ----------- | --------------- | ------------------ |\n| $0$         | $68.00$         | $76.19$            |\n| $0.001$     | $63.00$         | $74.41$            |\n| $0.01$      | $62.00$         | $69.75$            |\n| $0.1$       | $58.00$         | $69.09$            |\n| $1.0$       | $58.00$         | $68.07$            |\n| $10$        | $56.00$         | $59.93$            |\n\n*   *<Redundancy-driven concerns.>* Employing load balancing loss brings more redundancy in SMoE. We assess such redundancy via the performance gap between the fine-tuned model and the model where non-dominant experts are pruned. Specifically, large redundancy indicates the encoded information of pruned experts is also contained in the remaining experts, resulting in a smaller performance drop after pruning. Table R2 demonstrates load balancing regularization will cause more redundancy in SMoEs. (i.e., only $4.00$ performance gap v.s. the original $21.00$)\n\nTable R2: The *switch-base-32* model fine-tuned on the COPA task with the load balancing loss coefficient of $0$ and $10$. The larger performance gap of the model with coefficient=$0$ (ours) between pruning indicates the existence of its less redundancy.\n\n| Model          | Coefficient = $10$ | Coefficient = $0$ |\n| -------------- | ---------------- | --------------- |\n| Before pruning | $56.00$            | $68.00$           |\n| After pruning  | $52.00$            | $47.00$           |\n| $\\Delta$       | $-4.00$            | $-21.00$          |\n\n**[Cons 3. Stable Rank Computation]**\n\n>   \u201cFor example, in a specific layer, the 32 experts are compressed into 6 experts. Do you compute the average stable rank of the 32 experts as \u201cbefore\u201d in Figure 4, and the average stable rank of the 6 experts as \u201cafter\u201d?\u201d \n\nNo, we compute the stable rank of these 6 dominant experts both \"before\" and \"after\" the merging process. Following this, we compute the average of the ratio $\\frac{\\text{after} - \\text{before}}{\\text{before}}$ across these 6 experts. A further refinement has been included in Figure 4 of our revised paper.\n\n**[Cons 4. Expert Grouping Process]**\n\nIn your example \u201csuppose we have two dominant experts E1 and E2, then for a non-dominant expert E\u201d, yes, we \u201ccalculate the similarity of E with E1 and E2, and then assign E to the more similar one\u201d. We recapitulate the grouping procedure and explain the corner case you mentioned as below:\n\n-   *<How to group?>* As illustrated in the third paragraph of Section 3.1 and line 9 of Algorithm 1 of our paper. The grouping process includes: (1) selecting the most frequently activated experts as dominant experts; (2) calculating the pairwise similarity among the experts; and (3) for each non-dominant expert, assigning it to the most similar dominant expert. \n-   *<The corner case.>* Thanks for pointing it out. It is a corner case that \u201cnearly all non-dominant experts are assigned to the same group\u201d, which is reasonable due to the redundancies in SMoE models and confirms our insights. We also provide expert grouping results in Appendix 4.1 of our revised paper and show that this corner case is very rare."
                    },
                    "title": {
                        "value": "Responses to Reviewer 5S2f [Cons 2-4]"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700384607279,
                "cdate": 1700384607279,
                "tmdate": 1700389703362,
                "mdate": 1700389703362,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "edrRDd7ElG",
                "forum": "eFWG9Cy3WK",
                "replyto": "uJsSssSi3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[Cons 5. Pruning of S and Pruning Schedule]**\n\n**[Details of Cumulative Importance Score]** Thank you for your question. As described in the ninth line of the last paragraph of our paper, it's important to clarify that we prune weight matrix columns based on their 'cumulative importance scores', not just individual 'importance scores' of weights. \n\n*   *<What is the cumulative importance score?>* The 'cumulative importance score' of a column is a summation of the 'importance scores' of all individual weights in that column. \n*   *<Why cumulative importance score?>* This cumulative approach gives a more comprehensive assessment of the overall significance of a weight matrix column. By considering the collective impact of all weights in a column, we can more accurately determine which columns are less critical to the model's performance and therefore suitable for pruning. In contrast, pruning based on individual weight scores might overlook the collective contribution of weights within a column, potentially leading to less effective pruning decisions. \n\n**[Details of Pruning Schedule]** We use a cubic schedule of pruning ratio, which is widely applied in many existing methods, such as [R10, R11, R12]. It includes initial and final warmups where the ratio stays constant respectively, while in the middle iterations, the pruning ratio gradually increases following a cubic trend. Details are included in the third paragraph in Appendix 2 of our paper.\n\n**[Experiments - Comparison Between Pruning Schedules]** To address your concerns about the pruning schedule, we conduct additional experiments for comparison between linear, quadratic, and cubic (ours) pruning ratio schedules on the COPA and MultiRC tasks. The results are shown in Table R3, indicating a performance ordering of __cubic (ours) > quadratic > linear__. This is potentially because, in the early stages of pruning, an aggressive pruning schedule is less likely to lose useful information in the weights; while it is the opposite in the later stages of pruning. (This comparison and its analysis have been included in Appendix 1.4 of our revised paper.)\n\nTable R3: MC-SMoE Comparison between {linear, quadratic, cubic (ours)} schedules on the COPA and MultiRC tasks with the *switch-base-32* model.\n\n| Schedule     | COPA    | MultiRC |\n| ------------ | ------- | ------- |\n| Linear       | $59.00$ | $73.83$ |\n| Quadratic    | $61.00$ | $73.92$ |\n| Cubic (ours) | $67.00$ | $73.98$ |\n\n\n**[Cons 6. Role of Knowledge Distillation]**\n\nNo, the knowledge distillation is not applied to the dense and full-SMoE models, and it is only applied to \u201call merged and compressed SMoEs\u201d (which is illustrated in the fourth line of the third paragraph of Section 4.3 in our paper). It is to encourage the merged or compressed SMoE models to \u201cimitate the outputs generated by the full SMoE model on the training dataset\u201d (which is included in the eighth line of the last paragraph of Section 4.1 in our paper).\n\nTo further address your concerns, we conduct two additional experiments on COPA and MultiRC tasks: \n\n-   Knowledge distillation from full SMoE to dense model\n-   Self-knowledge-distillation of full SMoE, i.e. at the iteration $t$, distilled from the full SMoE itself at the iteration $(t-1)$.\n\nThe results are shown in Table R5, comparing them with that of our dense and full-SMoE without KD. It demonstrates a clear performance decline when applying knowledge distillation to dense and full-SMoE models. \n\nTable R5: Comparison between {dense, distilled dense, full-SMoE, self-distilled full-SMoE} models on the COPA and MultiRC tasks.\n\n| Model                    | COPA  | MultiRC |\n| ------------------------ | ----- | ------- |\n| Dense (Ours)             | 58.00 | 74.25   |\n| Distilled dense          | 58.00 | 73.70   |\n| Full-SMoE (Ours)         | 68.00 | 76.19   |\n| Self-distilled Full-SMoE | 62.00 | 73.93   |\n\n\n[R10] https://arxiv.org/pdf/1710.01878.pdf\n\n[R11] https://arxiv.org/pdf/2005.07683.pdf\n\n[R12] https://arxiv.org/pdf/2111.05754.pdf"
                    },
                    "title": {
                        "value": "Responses to Reviewer 5S2f [Cons 5-6]"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700385740101,
                "cdate": 1700385740101,
                "tmdate": 1700389725833,
                "mdate": 1700389725833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tjxHIyg40o",
                "forum": "eFWG9Cy3WK",
                "replyto": "uJsSssSi3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[Cons 7. Performance Drop Post-Compression]**\n\n**[Motivation of the Post-merging Compression Design]** In our MC-SMoE, we approximate a weight matrix $\\mathtt{W}$ using a combination of a low-rank matrix $\\mathtt{U}\\mathtt{V}$ and a sparse matrix $S$.\n\n-   As depicted in Figure 4 of our paper, we note that M-SMoE encourages a reduced dimensionality in the weight space of merged experts. This reduction suggests that neurons within a weight matrix share a common subspace, akin to the coherent parts of these neurons. Consequently, low-rank matrices are well-suited for approximating these coherent parts effectively. \n-   A structured-pruned sparse matrix is frequently used for compressing $\\mathtt{W}$ ([R14]). This process benefits from the separation of low-rank and sparse matrices, which facilitates easier pruning. The low-rank matrix effectively segregates the coherent and incoherent parts of neurons. By introducing a new matrix S, we can approximate the residual incoherent parts. This approach allows for the precise approximation of the remaining incoherent parts by adding a new matrix S, which is then pruned to remove the non-expressive, incoherent components.\n\n**[Experiment - Comparison of Pruning Strategies]** We conduct extra comparison experiments with the following two other pruning strategies on the COPA and MultiRC tasks:\n\n-   *Magnitude pruning*, proposed by [R14], preserves weights with high absolute values and is the most widely used method for weight pruning.\n-   *Iterative pruning*, as introduced in [R15], targets the direct removal of neurons that have importance scores below a predefined threshold during each iteration.\n\nThe results are shown in Table R6, indicating that our pruning strategy in MC-SMoE demonstrates the best performance among the three strategies.\n\nTable R6: Comparison between {magnitude pruning, iterative pruning, ours} pruning methods on the COPA and MultiRC tasks with the *switch-base-32* model.\n\n| Method            | COPA    | MultiRC |\n| ----------------- | ------- | ------- |\n| Magnitude Pruning | $61.00$ | $73.51$ |\n| Iterative Pruning | $66.00$ | $72.63$ |\n| Ours              | $67.00$ | $73.98$ |\n\n**[Experiment - Comparison of Different Schedules]** We conduct extra comparison experiments with two other pruning ratio schedules, i.e. the linear, quadratic pruning ratio schedules, and our cubic schedules, on the COPA and MultiRC tasks. The outcomes, shown in Table R7, reveal a performance hierarchy with our cubic method outperforming the quadratic and linear approaches.\n\nTable R7: Comparison between different schedules on the COPA and MultiRC tasks with the *switch-base-32* model.\n\n| Schedule     | COPA    | MultiRC |\n| ------------ | ------- | ------- |\n| Linear       | $59.00$ | $73.83$ |\n| Quadratic    | $61.00$ | $73.92$ |\n| Cubic (ours) | $67.00$ | $73.98$ |\n\n**[Cons 8. Evaluation of Speed and Memory]**\n\nWe appreciate the constructive suggestion of evaluating \u201cthe inference speed (throughput) and memory usage\u201d of the models. We politely address your concerns from several aspects:\n\n-   *<Expert merging improves \u201cmemory efficiency\u201d.>* Due to the sparse activation characteristic of SMoE models, where each SMoE layer chooses one or two experts for the computation of each token, merging experts leads to a reduction in memory usage but not in computational cost (specifically FLOPs). In Table 2 of our paper, we showcase the model size and inference FLOPs for M-SMoE, MC-SMoE, and all other baseline models, illustrating the improvement in memory efficiency brought by both M-SMoE and MC-SMoE.\n-   *<Theoretical speedup of expert merging exists.>* This is because, in conventional SMoE implementation, the routing process involves (1) a layout transform of the tensors (to arrange tokens targeting the same experts into a continuous memory buffer) and its reverse operation ([R13]), (2) breaking down one large matrix block GEMM operation into many smaller matrix block GEMM operations (each corresponding to an individual expert), leading to less efficient utilization of modern computational hardware's advantages. These factors lead to a decrease in real throughput for the sparsely activated computation in SMoE when the number of experts rises, a topic that remains an open area for research ([R13]) and is earmarked for exploration in our future studies. In other words, reducing the number of experts can yield improvements in actual inference speed, even while maintaining roughly constant FLOPs.\n-   *<Why there is a \u201cmarginal speed (latency) gain of M-SMoE\u201d?>* It is mainly because of the challenge of trimming the router's output channels without altering its routing functionality. Therefore, we execute the merging of experts by substituting all the expert weights in a group with a single, merged expert weight. This throughout analysis is included in the eighth line of Appendix 1.2 in our paper. \n\n[R14] https://arxiv.org/pdf/1506.02626.pdf\n\n[R15] https://arxiv.org/pdf/1906.10771.pdf"
                    },
                    "title": {
                        "value": "Responses to Reviewer 5S2f [Cons 7-8]"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387104274,
                "cdate": 1700387104274,
                "tmdate": 1700389743191,
                "mdate": 1700389743191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G8QRkatn63",
                "forum": "eFWG9Cy3WK",
                "replyto": "uJsSssSi3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[Cons 8. Evaluation of Speed and Memory]**\n\n*   *<Acceleration results with a vanilla implementation.>* Thanks for the constructive suggestion. To better address your concerns, we present a comprehensive evaluation of latency, throughput, FLOPs, memory, and model size for the full-SMoE, M-SMoE, and MC-SMoE models. The results are shown in **Table R8**, we observed that the throughput of MC-SMoE is lower than that of M-SMoE, despite it consuming less memory and FLOPs. This is due to our lack of specialized sparse matrix support software or hardware for MC-SMoE, which encourages our future work.\n\nTable R8: The evaluation is carried out using the *switch-base-32* model, utilizing an input batch size of $8$ and a sequence length of $64$.\n\n| Model     | Throughput (tokens per ms) | Latency (ms) | GFLOPs | Memory   | Model Size |\n| --------- | -------------------------- | ------------ | ------ | -------- | ---------- |\n| Full-SMoE | $\\mathbf{4.47}$                    | $\\mathbf{114.3}$      | $232$  | $3895$MB | $2.0$B     |\n| M-SMoE    | $\\mathbf{4.82}$                     | $\\mathbf{106.2}$      | $232$  | $1456$MB | $733$M     |\n| MC-SMoE   | $\\mathbf{2.71}$                    | $\\mathbf{189.0}$      | $186$  | $715$MB  | $381$M     |\n\n*   *<Acceleration results with a specialized implementation.>* To further illustrate the efficiency gains from expert merging, we conduct an additional evaluation with specialized implementation for merged expert weights. Our approach involves gathering tokens routed to all experts of one group and processing them through one single expert, leveraging the shared weights within the group. This method capitalizes on the hardware accelerator's (typically a GPU) capacity for parallel processing. The outcomes, as detailed in Table R9, substantiate our hypothesis, showing that M-SMoE and MC-SMoE with optimized inference design outperform the full-SMoE in terms of throughput. We believe that these encouraging preliminary findings will inspire our future research.\n\nTable R9: The evaluation of specialized M-SMoE inference is carried out using the *switch-base-32* model, utilizing an input batch size of $8$ and a sequence length of $64$. Notably, with the specialized implementation, both M-SMoE and MC-SMoE demonstrate inference speedup over full-SMoE.\n\n|           | Throughput   (tokens per ms) | Latency (ms) | GFLOPs | Memory   | Model Size |\n| --------- | ---------------------------- | ------------ | ------ | -------- | ---------- |\n| Full-SMoE | $\\mathbf{4.47}$                       | $\\mathbf{114.3}$      | $232$  | $3895$MB | $2.0$B     |\n| M-SMoE    | $\\mathbf{7.91}$                       | $\\mathbf{64.7}$       | $232$  | $1456$MB | $733$M     |\n| MC-SMoE   | $\\mathbf{6.27}$                       | $\\mathbf{81.6}$       | $186$  | $715$MB  | $381$M     |\n\nThanks for the valuable suggestion to extend our assessment. We have incorporated the aforementioned additional evaluation and analysis into Appendix 1.2 of our revised manuscript."
                    },
                    "title": {
                        "value": "Responses to Reviewer 5S2f [Cons 8]"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700387620921,
                "cdate": 1700387620921,
                "tmdate": 1700389756779,
                "mdate": 1700389756779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sDCnwdUVn7",
                "forum": "eFWG9Cy3WK",
                "replyto": "uJsSssSi3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are keen to discuss further with you"
                    },
                    "comment": {
                        "value": "Dear Reviewer **5S2f**,\n\nWe thank reviewer **5S2f** for the time of reviewing and the constructive comments again. We really hope to discuss further with you to see if our response solves your concerns.\n\nWe genuinely hope reviewer **5S2f** could kindly check our response. Thank you!\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578694328,
                "cdate": 1700578694328,
                "tmdate": 1700583518478,
                "mdate": 1700583518478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V83SAlAyUt",
                "forum": "eFWG9Cy3WK",
                "replyto": "uJsSssSi3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are keen to discuss further with you"
                    },
                    "comment": {
                        "value": "Dear Reviewer **5S2f**,\n\nWe genuinely thank you again for your time & efforts and your constructive comments.\n\nIn our response, we have (1) added more details about expert permutation alignment; (2) conducted additional evaluation of inference cost for all models; (3) conducted additional ablation about the pruning ratio schedule; (4) clarified all other confutions.\n\nAs the discussion period is approaching its end, we would really appreciate it if you could kindly let us know whether there are any further questions. We will be more than happy to address them.\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638964645,
                "cdate": 1700638964645,
                "tmdate": 1700638964645,
                "mdate": 1700638964645,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bJRRsbmvQ5",
                "forum": "eFWG9Cy3WK",
                "replyto": "uJsSssSi3x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6785/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are keen to discuss further with you"
                    },
                    "comment": {
                        "value": "Dear Reviewer **5S2f**,\n\nThank you for your valuable time and the constructive feedback you have provided once again. We are eager to engage in further discussions to see if our response solves your concerns.\n\nAs the **deadline** for the discussion period is nearing, we would greatly appreciate it if you could kindly let us know whether there are any further questions. Thank you for your attention to our work.\n\nBest wishes,\n\nAuthors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6785/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707664995,
                "cdate": 1700707664995,
                "tmdate": 1700707664995,
                "mdate": 1700707664995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]