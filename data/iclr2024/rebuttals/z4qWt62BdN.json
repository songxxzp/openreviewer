[
    {
        "title": "DSparsE: Dynamic Sparse Embedding for Knowledge Graph Completion"
    },
    {
        "review": {
            "id": "0DqMxgjMFh",
            "forum": "z4qWt62BdN",
            "replyto": "z4qWt62BdN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4899/Reviewer_uERB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4899/Reviewer_uERB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a knowledge graph link prediction model DSparsE, in which the dynamic layer and residual structure are incorporated to achieve higher efficiency. Experimental results on two datasets show its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The motivation of this paper is easy to understand.\n2. The proposed model seems technically reasonable."
                },
                "weaknesses": {
                    "value": "1. The difference and the superiority of DSparsE compared with baseline models are unclear. \n2. The contribution of this paper is limited. It only combines some existing techniques.\n3. The current version of this paper is not easy to follow. There are some uncommon words without explanation, such as \"reminiscent\".\n4. The citation and analysis of previous models are insufficient.\n5. More datasets are required to verify the effectiveness of the proposed model.\n6. More experimental results about the scalability of the proposed model should be provided."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698644938888,
            "cdate": 1698644938888,
            "tmdate": 1699636474596,
            "mdate": 1699636474596,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SgWWdzqiWu",
                "forum": "z4qWt62BdN",
                "replyto": "0DqMxgjMFh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**We appreciate your valuable comments. We have revised our manuscript and clarified the contributions in the revised version. We sincerely hope you will find the time to review our improved manuscript and look forward to your valuable comments.**\n\n\n---\n\n>* **W1: The difference and the superiority of DSparsE compared with baseline models are unclear.**\n>* **W2: The contribution of this paper is limited. It only combines some existing techniques.**\n\n**Response**\nThank you for your comment. The key advantages of DSparsE can be summarized in three points:\n\n1. Dynamic Characteristics: The relation-aware layer in ComDensE[1] focuses only on the dynamic nature of network weights as relationships change, neglecting the dynamics associated with node changes. Indeed, designing a separate weight matrix for each node would lead to an explosive increase in parameters with the scale of the graph. Therefore, we adopt a hypernetwork structure to build a network where parameter weights can change with the input entity-relation pairs. This network can consider both entity and relation node information, and works well in conjunction with the relation-aware layer. Additional ablation studies in the revised version of the paper prove that parallel dynamic and relation-aware layers can achieve better performance.\n\n    | Dynamic+Relation-aware | Dynamic layer only | Relation-aware layer only |\n    |:-------:|:-------:|:-------:|\n    | **0.272**| 0.252 | 0.264 |\n\n2. Sparse Characteristics: Traditional neural networks face issues of overfitting and generalization, which are more severe in knowledge graph link prediction models based on neural networks. This is due to the open-world assumption[2] in knowledge graphs, meaning the graphs are incomplete and the dataset knowledge is extremely limited compared to the actual distribution of knowledge. Thus, our model introduces a fixed sparse structure, different from the dropout feature, which is determined at model initialization. Our inspiration comes from convolutional neural networks like ConvE[3] and InteractE[4], which have sparse connections with weight sharing. We consider random sparse connections between neurons without weight sharing and use a hyperparameter alpha to control the density of feature interactions. Further experiments also show that sparse connections on top of dropout can optimize the network model's predictive performance. The results below show the effectiveness of sparse structure. Note that extra dropout represents setting dropout rate $\\hat{p}$ $=$ $p + \\alpha(1-p)$, where $\\alpha$ denotes the sparsity degree and $p$ denotes the original dropout rate. Here we set $\\alpha$ to 0.5. The results indicate that sparse structure can effectivly enhance the performance.\n\n| model | Dropout + sparse | Extra dropout |\n| :---: | :---: | :---: |\n| DSparsE | **0.272** | 0.266 |\n| ComDensE | 0.267 | 0.258 |\n\n\n3. Residual Characteristics: Existing research has shown that deepening network layers can enhance expressiveness[5], but this comes with convergence difficulties, a problem also present in knowledge graph link prediction models. For example, the authors of ComDensE found that increasing the number of layers actually reduced network performance. We also found that simply adding front-end fully connected layers in matrix decomposition or similar models reduces their expressiveness. However, the only way for shallow models to maintain effectiveness with increasing graph scale is to increase the embedding dimension, leading to greater computational costs. If the embedding dimension is kept constant, performance declines with increasing graph scale (especially the number of entities), such as RESCAL's performance on WNRR18 being worse than on FB15k-237 with the same embedding dimension. In DSparsE, we applied expert layers to maximize feature extraction and fusion, requiring a powerful decoding module. In experiments, we found that 3-5 layers of residual neural networks provided good results on Fb15k-237 and YAGO3-10. To some extent, the introduction of residual layers allows the network to deepen further, maximizing its expressive potential. This is beneficial for large knowledge graphs like YAGO3-10.\nWe have provided a table of the optimal number of residual layers for three models, which shows that the depth of the decoding layer needs to increase with the scale of the dataset. The introduction of residual connections helps to stabilize the network training process and maintain the predictive results (the experimental results have been presented in the paper).\n\n| Dataset | Entity | Relation | Train | optimal residual layer depth |\n| :---: | :---: | :---: |:---:|:---:|\n| FB15k-237 | 14,541 | 237 | 272,115 | 3 |\n| WNRR18 | 40,943 | 11 | 86,835 | 1 |\n| YAGO3-10 | 123,182 | 37 | 1079,040 | 5 |"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362319978,
                "cdate": 1700362319978,
                "tmdate": 1700363112252,
                "mdate": 1700363112252,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yAlI8sD1kU",
                "forum": "z4qWt62BdN",
                "replyto": "0DqMxgjMFh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are eager for your reply!!"
                    },
                    "comment": {
                        "value": "**We have carefully considered your feedback and updated our manuscript. Could you please take a moment to review our changes and let us know if they resolve your queries? Your feedback is invaluable to us!**"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548775305,
                "cdate": 1700548775305,
                "tmdate": 1700548775305,
                "mdate": 1700548775305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b4tMZOGNQl",
                "forum": "z4qWt62BdN",
                "replyto": "yAlI8sD1kU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Reviewer_uERB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Reviewer_uERB"
                ],
                "content": {
                    "title": {
                        "value": "Thank the authors for response"
                    },
                    "comment": {
                        "value": "Thank you for clarifying w2 and providing more experimental results. However, the technique contributions of this paper are still limited, and the superiority of the proposed method in efficiency is still not convincing. Based on my initial review, your response, and the comments of other reviewers. I am inclined to keep my score."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728155877,
                "cdate": 1700728155877,
                "tmdate": 1700728155877,
                "mdate": 1700728155877,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GY7CunAOWP",
            "forum": "z4qWt62BdN",
            "replyto": "z4qWt62BdN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4899/Reviewer_S6XC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4899/Reviewer_S6XC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DSparsE, a novel knowledge graph link prediction model.  This model introduces a dynamic layer into the encoding end and a residual structure into the decoding end. Moreover, this model achieves a significant reduction in the number of parameters and a significant improvement in the efficiency by substituting the fully connected layer with a sparse layer. Extensive experiments demonstrate that DSparsE consistently outperforms existing state-of-the-art methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is clearly written and easy to follow.\n2. This model combines dynamic layer and residual structure, wwhich enables neural networks to better perform information fusion"
                },
                "weaknesses": {
                    "value": "1.\tThe number of datasets is only 2, relatively limited.\n2.\tThe supplementary experiments in this article are not sufficient. For instance, Figure 4 provides a comparison between DSparseE and one baseline on a single dataset. Both the number of baselines for comparison and the diversity of datasets should be increased. Additionally, Figure 5 only illustrates three scenarios of expert quantity, making it challenging to discern a clear and definitive trend. In Figure 6, the blue and orange lines are difficult to interpret. The author mentions that these are results with different numbers of residual layer depth but fails to provide specific numerical values for these quantities. The inadequacy of supplementary experiments has compromised the rigor and credibility of the results.\n3.\tIn the \"Contribution\" section, the authors claim that their model achieves a significant reduction in the number of parameters and a significant improvement in model efficiency. However, the subsequent text does not provide a particularly clear substantiation of this claim. It would be beneficial to provide more explicit details, such as in terms of runtime or the exact number of parameters, to support this assertion."
                },
                "questions": {
                    "value": "1.\tCan you add more datasets?\n2.\tCan you improve the supplementary experiments?\n3.\tCan you provide more explicit details of the improvement of the efficiency?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4899/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4899/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4899/Reviewer_S6XC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698666999647,
            "cdate": 1698666999647,
            "tmdate": 1699636474511,
            "mdate": 1699636474511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xVBrkP4m6g",
                "forum": "z4qWt62BdN",
                "replyto": "GY7CunAOWP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**We would like to sincerely thank you for the time and effort you have dedicated to carefully reading our manuscript and providing insightful comments and suggestions. We have thoroughly considered each point raised and have addressed them in the following sections, aiming to improve and refine our manuscript based on your valuable input.**\n\n---\n\n>* **W1: The number of datasets is only 2, relatively limited.**\n>* **Q1: Can you add more datasets?**\n\n**Response**\nThank you for your comment! We have added a new dataset, YAGO3-10[1], the details of which are as follows in the table below:\n| Dataset | Entity | Relation | Train | Valid | Test |\n| :---: | :---: | :---: |:---:|:---:|:---:|\n| YAGO3-10 | 123,182 | 37 | 1079,040 | 5000 | 5000 |\n\nAccordingly, we have conducted experiments on each metric and compared them with the baseline. Due to the fewer baselines of this dataset, we only selected models with data and presented the results in the table below.\n\n| Dataset | Hits@1 | Hits@10 | MRR |\n| :---: | :---: | :---: |:---:|\n| DistMult | 0.240 | 0.540 | 0.340 |\n| ConvE | 0.350 | 0.620 | 0.440 |\n| ComplEx | 0.260 | 0.550 | 0.360 |\n| RotatE | 0.402 | 0.670 | 0.495 |\n| InteractE | 0.462 | 0.687 | 0.541 |\n| **DSparsE** | **0.464** | **0.690** | **0.544** |\n\nNote that the result on this dataset may change in subsequent versions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361318065,
                "cdate": 1700361318065,
                "tmdate": 1700361318065,
                "mdate": 1700361318065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lkDy9whotH",
                "forum": "z4qWt62BdN",
                "replyto": "GY7CunAOWP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "\uff08Following the previous text\uff09Regarding the original Figure 6, we apologize for the confusion caused by inaccurate markings. Our intention was to distinguish a sudden change in the horizontal axis values from 5 to 100 using a dashed line and a different color. However, this approach now seems redundant. In our revised paper, we have improved the expression of this figure and the interpretation of the results. Additionally, if you have doubts about whether the network should continue to deepen, you can refer to our response to reviewer jcNV's question 5, which we will include in our revised version.\n\nFurthermore, we have added some comparative experiments, such as ablation studies of dynamic layers and relation-aware layers, as well as comparisons of the effects of sparsity structure with dropout and downscaling of the network. Interestingly, by studying the hypernetwork formed by the gated layers' responses to different entity-relation pairs, we discovered that the hypernetwork can effectively model semantic information in the latent space. Along with our analysis, we also provided visualized results.\n\n\n\n---\n\n>* **W3: In the \"Contribution\" section, the authors claim that their model achieves a significant reduction in the number of parameters and a significant improvement in model efficiency. However, the subsequent text does not provide a particularly clear substantiation of this claim. It would be beneficial to provide more explicit details, such as in terms of runtime or the exact number of parameters, to support this assertion.**\n>* **Q3: Can you provide more explicit details of the improvement of the efficiency?**\n\n**Response**\n\nThank you for your comment. Indeed, increasing the number of expert blocks results in reduced parameter efficiency, and the fixed structure sparsification does not significantly reduce the number of parameters in the current computing architecture. This is because non-structured pruning does not alter the number of matrix multiplications, and the optimal sparsity of 0.5 mentioned in our paper is not high enough to utilize sparse matrix techniques for optimization. To avoid misleading information, we have removed the statements about parameter reduction from the contributions section of our paper.\n\nIn the table below, we present the parameter count and corresponding runtime. Several points should be noted regarding these results:\n\n| Number of parameters | FB15k-237 | WN18RR |\n| :---: | :---: | :---: |\n| InteractE | 18M | 60M |\n| ComDensE | 66M | 33M |\n| DSparsE | 69M | 29M |\n\n|  model | Running time (per batch, $batch size = 128$) on FB15k-237 | on WN18RR |\n| :---: | :---: |:---:|\n| InteractE | 2.58s | 3.43s |\n| ComDensE | 2.61s | 3.60s |\n| DSparsE | 2.67s | 3.44s |\n\n1. Increasing the number of expert cores does not significantly increase the number of parameters.\n2. Relatively speaking, increasing the number of expert cores has a minimal impact on computational speed.\n3. For hardware that supports unstructured pruning, introducing sparsity can save storage space and computational resources to a certain extent.\n\n---\n\n**We would like to sincerely thank you for your valuable comments, and we hope that you are satisfied with our responses. We appreciate for your reconsideration of our revised manuscript. Thank you very much for your time and effort in reviewing our work!!**\n\n---\n[1] Suchanek F M, Kasneci G, Weikum G. Yago: a core of semantic knowledge[C]//Proceedings of the 16th international conference on World Wide Web. 2007: 697-706."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361488069,
                "cdate": 1700361488069,
                "tmdate": 1700413378652,
                "mdate": 1700413378652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v7Ur1ufNIu",
                "forum": "z4qWt62BdN",
                "replyto": "GY7CunAOWP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We're grateful for your help in improving our work!"
                    },
                    "comment": {
                        "value": "**We have attempted to address all the points you raised in your review. Could you please confirm whether our revised submission meets your expectations and resolves the issues you highlighted? We appreciate your guidance in this process.**"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548603382,
                "cdate": 1700548603382,
                "tmdate": 1700548603382,
                "mdate": 1700548603382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wr8vawxNNF",
            "forum": "z4qWt62BdN",
            "replyto": "z4qWt62BdN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4899/Reviewer_jcNV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4899/Reviewer_jcNV"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel dynamic sparse embedding method DSparsE for knowledge graph completion task. The DSparsE is proposed to solve the drawbacks of prone to overfitting and constraints on network depth of ConDensE and the limitation in feature interaction and interpretability of InteractE. DSparsE includes three main modules, the dynamic MLP layer and the relation-aware MLP layer in the encoder, and residual blocks in the decoder, and it named as DSparsE because the sparse MLP is applied. DSparsE is evaluated on two common KGC benchmarks, FB15k-237 and WN18RR. The results show DSparsE is effective for KGC task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Authors tried to investigate how powerful the MLP is for KGC task by developing model based on pure MLP layers. This is significantly different to existing methods and very interesting. \n2. Though the experiment results of DSparsE is not comparable to state-of-the-art. But it shows DSparsE is effective for KGC tasks."
                },
                "weaknesses": {
                    "value": "1. Though the overall model architecture is novel, the key advantage of pure MLP-based model such as DSparsE is unclear. As I understand, it is not efficiency since 15(35) experts are set for FB15k-237(WN18RR) with each expert represented by an MLP layer, which will introduce a lot extra parameters compared to 1 expert. It is also not superior performance, since the link prediction results of DSparsE is comparable to existing methods, such as RESCAL and ComDensE. \n2. How do the different modules affect the performance of DSparsE is not well illustrated. For example, the output vectors from Dynamic MLP layer and the Sparse Relation-aware Layer are concatenated into one vector as the input of the decoder. It is unclear the output vector of which affects the results more significantly. \n3. Some minor points:\n* In Table 2, the best MRR result on FB15k-237 should be 0.396 from ConvKB. \n* Both the blue line and orange dashed line represents marked as \"with residual structure\", it is a bit confusing."
                },
                "questions": {
                    "value": "1. What is the key advantages of designing KGC models with pure MLP layers compared to the existing methods, such as tensor-decomposition models and translational models introduced in the related works? \n2. The definition of deep learning models is unclear. Is the graph neural network model be deep learning models? Why are translation models not deep learning models?\n3. The output vectors from Dynamic MLP layer and the Sparse Relation-aware Layer are concatenated into one vector as the input of the decoder. Which vector affect the results more? \n4. What is the number of parameters of DSparsE model for FB15k-237 and WN18RR?  Is the number of parameters of DSparsE significantly more than baseline methods?\n5. The results of 100 depth of the residual structures. Is this mean that 100 residual MLP blocks are used in the model? If yes, how long does it take to train the model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769732710,
            "cdate": 1698769732710,
            "tmdate": 1699636474432,
            "mdate": 1699636474432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8TYb5dte8e",
                "forum": "z4qWt62BdN",
                "replyto": "wr8vawxNNF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Thank you very much for your insightful comments on our manuscript. We have addressed your comments and made revisions in our new manuscript. Below are detailed responses to your questions:**\n\n---\n>* **W1: Though the overall model architecture is novel, the key advantage of pure MLP-based model such as DSparsE is unclear. As I understand, it is not efficiency since 15(35) experts are set for FB15k-237(WN18RR) with each expert represented by an MLP layer, which will introduce a lot extra parameters compared to 1 expert. It is also not superior performance, since the link prediction results of DSparsE is comparable to existing methods, such as RESCAL and ComDensE.**\n\n>* **Q1: What is the key advantages of designing KGC models with pure MLP layers compared to the existing methods, such as tensor-decomposition models and translational models introduced in the related works?**\n\n\n\n**Response**\n\nThank you for your comments. The key advantages of DSparsE can be summarized in three points:\n\n1. Dynamic Characteristics: The relation-aware layer in ComDensE[1] only focuses on the dynamic nature of network weights as relationships change, neglecting the dynamics associated with node changes. Indeed, designing a separate weight matrix for each node would lead to an explosive increase in parameters with the scale of the graph. Therefore, we adopt a hypernetwork structure to build a network where parameter weights can change with the input entity-relation pairs. This network can consider both entity and relation node information, and works well in conjunction with the relation-aware layer. Additional ablation studies in the revised version of the paper demonstrate that parallel dynamic and relation-aware layers can achieve better performance. \n\n    | Dynamic+Relation-aware | Dynamic layer only | Relation-aware layer only |\n    |:-------:|:-------:|:-------:|\n    | **0.272**| 0.252 | 0.264 |\n    \n\nFurthermore, we have investigated how the hypernetwork affects the weights of the expert blocks. Here, we will quote a portion of our revised version:\n>Moreover, our investigation into the gating layer's outputs has unveiled some intriguing insights. Each entity-relation pair in the dataset, upon processing through the gating layer, yields an output vector $\\bm{o}$. These high-dimensional vectors were subjected to tSNE reduction, with the resultant visualization displayed in Figure 8 and Figure 9. Each point in this figure represents a unique entity-relation pair, distinguished by varying colors corresponding to different relationships. The visualization result reveals the following observations:\n>1. A tendency for entity-relation pairs of the same relationship type to cluster together, indicating proximity within the output space of the gated layer outputs.\n>2. The spatial distribution of clusters is significantly influenced by the nature of the relationships. For instance, relationships denoting inverse meanings (e.g. nominee\\_inv and nominee) or semantic opposites (e.g. place of birth vs. place of burial) exhibit a tendency to spatially diverge, exhibiting a unique central symmetry characteristic in the reduced dimensional space, Conversely, relationships with similar semantics (e.g. nationality and city town) are observed to be proximate in the latent space. This proved that DSparsE can capture various associations between entities and relations.\n>3. Alterations in the head entity of a relation pair result in minor shifts within the vector output, confined to a limited scope. Within a fixed relation, the relative positioning of nodes within its corresponding cluster does not display a discernible pattern. This phenomenon can be attributed to the relatively lower frequency of triples involving individual nodes compared to those associated with a particular relation type, posing challenges in accurately modeling semantic information for nodes. Still, certain examples, such as Mariah Carley and Dmitri Shostakovich\u2014both notable in the music domain\u2014demonstrate proximity within clusters pertaining to specific relations."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700360719706,
                "cdate": 1700360719706,
                "tmdate": 1700360719706,
                "mdate": 1700360719706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K84mUbhFK5",
                "forum": "z4qWt62BdN",
                "replyto": "wr8vawxNNF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">* **Q5: The results of 100 depth of the residual structures. Is this mean that 100 residual MLP blocks are used in the model? If yes, how long does it take to train the model?**\n\n**Response**\nWe apologize for any confusion caused by our unclear description. It should be clarified that for a small dataset like FB15k-237, a massive network with 100 layers is extremely redundant. For this dataset, the optimal depth of the decoding layers is around 3 layers.\n\nWe fully understand your concerns. Your core question is:\n* **_Is it really necessary to deepen the network?_**\n\nIn fact, we have observed that the larger the dataset, the deeper the required depth of the decoding layer. Blow is the relationship between the scale of the dataset and the corresponding optimal number of layers:\n\n| Dataset | Entity | Relation | Train | optimal residual layer depth |\n| :---: | :---: | :---: |:---:|:---:|\n| FB15k-237 | 14,541 | 237 | 272,115 | 3 |\n| WN18RR | 40,943 | 11 | 86,835 | 1 |\n| YAGO3-10 | 123,182 | 37 | 1079,040 | 5 |\n\n\nBased on this fact, we make a reasonable conjecture that for super-large-scale knowledge graphs (like FreeBase, DBPedia, etc.) with tens of billions of knowledge triples, the required number of decoding layers will increase further. Under this assumption, our experiments prove that incorporating residual connections in MLPs can effectively maintain the network's decoding performance and maximize the network's expressive power.\n\nYou also mentioned the issue of training time for deep networks. We have conducted corresponding experiments, and the results are shown in the following table. Interestingly, stacking a large number of linear layers does not significantly increase the network's computational time. This may be because computational resources are mainly consumed during preprocessing and information transmission stages. To ensure rigor, we have specified the models of the GPUs and CPUs used for reference.\n\n| CPU | GPU | Running time for $depth = 3$ | running time for $depth = 100$ |\n| :---: | :---: |:---:|:---:|\n| AMD Ryzen 9 7945HX | NVIDIA GeForce RTX 4080 Laptop GPU | 10.2s per batch(512) | 13.7s per batch(512) |\n\n\n---\n**Again, we respectfully request your review of the latest revision. We are eager to hear your comments on our revised manuscript to further improve our work.**\n\n**Thank you once again for your valuable comments on our research. It is truly rewarding that you find our work to be _very interesting_! Wish you have a nice day! :D**\n\n---\n\n[1] Kim M, Baek S. Comdense: combined dense embedding of relation-aware and common features for knowledge graph completion[C]//2022 26th International Conference on Pattern Recognition (ICPR). IEEE, 2022: 1989-1995.\n\n[2] Drummond N, Shearer R. The open world assumption[C]//eSI Workshop: The Closed World of Databases meets the Open World of the Semantic Web. 2006, 15: 1.\n\n[3] Dettmers T, Minervini P, Stenetorp P, et al. Convolutional 2d knowledge graph embeddings[C]//Proceedings of the AAAI conference on artificial intelligence. 2018, 32(1).\n\n[4] Vashishth S, Sanyal S, Nitin V, et al. Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(03): 3009-3016.\n\n[5] Chatterjee S, Zielinski P. On the generalization mystery in deep learning[J]. arXiv preprint arXiv:2203.10036, 2022.\n\n[6] Bordes A, Usunier N, Garcia-Duran A, et al. Translating embeddings for modeling multi-relational data[J]. Advances in neural information processing systems, 2013, 26.\n\n[7] Wang Z, Zhang J, Feng J, et al. Knowledge graph embedding by translating on hyperplanes[C]//Proceedings of the AAAI conference on artificial intelligence. 2014, 28(1).\n\n[8] Lin Y, Liu Z, Sun M, et al. Learning entity and relation embeddings for knowledge graph completion[C]//Proceedings of the AAAI conference on artificial intelligence. 2015, 29(1).\n\n[9] Ji G, He S, Xu L, et al. Knowledge graph embedding via dynamic mapping matrix[C]//Proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: Long papers). 2015: 687-696.\n\n[10] Jacobs R A, Jordan M I, Nowlan S J, et al. \u00aaAdaptive Mixtures of Local Experts, \u00ba Neural Computation, vol. 3[J]. 1991.\n\n[11] Vashishth S, Sanyal S, Nitin V, et al. Composition-based multi-relational graph convolutional networks[J]. arXiv preprint arXiv:1911.03082, 2019.\n\n[12] Schlichtkrull M, Kipf T N, Bloem P, et al. Modeling relational data with graph convolutional networks[C]//The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3\u20137, 2018, Proceedings 15. Springer International Publishing, 2018: 593-607."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361050419,
                "cdate": 1700361050419,
                "tmdate": 1700555983176,
                "mdate": 1700555983176,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ebKFIHVf8g",
                "forum": "z4qWt62BdN",
                "replyto": "vckCdOfYHU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Reviewer_jcNV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Reviewer_jcNV"
                ],
                "content": {
                    "title": {
                        "value": "Thank the authors for response"
                    },
                    "comment": {
                        "value": "Thank the authors for their detailed response. I would like to keep my initial score."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658596756,
                "cdate": 1700658596756,
                "tmdate": 1700658596756,
                "mdate": 1700658596756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rkM2iId88n",
            "forum": "z4qWt62BdN",
            "replyto": "z4qWt62BdN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4899/Reviewer_5L59"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4899/Reviewer_5L59"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new architecture of neural networks, the DSparsE, for graph completion. It is a link prediction model structure that uses only MLP layers and employs sparse and residual structures to alleviate overfitting, and reduce the difficulty of training deep networks. The paper provides performance comparison of various knolwege graph embedding techniques across two datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed architecture utilizes various methods to prevent some of the well known problems, especially the overfitting problem which is important for knowledge graph completion.\n- The paper provides ablation studies to show the benefit of implementing proposed methods."
                },
                "weaknesses": {
                    "value": "- Better explanations can be included. For instance, the paper states the expert kernels, but does not explicitly explain what is the expert kernels or why it is \"expert\".\n- Better figures and figure captions can be written. The architecture figures are small and the explanations in the captions is very limited.\n- Some statistical testings (or critical plots) for the comparing methods would make the arguments of the paper stronger.\n- Comparison of computational time would be necessary to address the computational complexity problems in the existing methods."
                },
                "questions": {
                    "value": "- What is an expert kernel?\n- How does the model do in the case of no experts? Or simple mean of the outputs of k different mlps.\n- How is imposing sparse structure different from the drop-connect (dropout on the weights)\n- Why does the dynamic structure enhance the robustness of the model? Is it by some form of ensembling?\n- How statistically different are the results between the models (or at least the top competing ones).\n- How does the model perform in terms of computational complexity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4899/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4899/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4899/Reviewer_5L59"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698832231728,
            "cdate": 1698832231728,
            "tmdate": 1699636474365,
            "mdate": 1699636474365,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kXKk9KYl5k",
                "forum": "z4qWt62BdN",
                "replyto": "rkM2iId88n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**We would like to sincerely thank you for the time and effort you have dedicated to carefully reading our manuscript and providing insightful comments and suggestions. Here is our response.**\n\n---\n\n>* **W1: Better explanations can be included. For instance, the paper states the expert kernels, but does not explicitly explain what is the expert kernels or why it is \"expert\".**\n>* **Q1\uff1aWhat is an expert kernel?**\n>* **Q2: How does the model do in the case of no experts? Or simple mean of the outputs of k different mlps.**\n>* **Q4: Why does the dynamic structure enhance the robustness of the model? Is it by some form of ensembling?**\n\n**Response**\n\n\nThank you for your comment. As per your suggestion regarding the term \u201cexpert kernel,\u201d we have revised it to \u201cexpert block\u201d for clarity and added explanatory sentences. We have also included a figure specifically for the dynamic layer section for better understanding. The revised statements concerning the experts are as follows:\n\n>The encoding part consists of an MLP layer with $k$ expert blocks (i.e., structurally similar MLP sub-blocks) and a relation-aware MLP layer.\n\nFor an explanation of the term _expert_, you can refer to the paper _Adaptive mixtures of local experts_[1] as well as _Outrageously large neural networks: The sparsely-gated mixture-of-experts layer_[2]. The explanation provided therein is as follows:\n>_We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning[1]._\n\n> _The experts are themselves neural networks, each with their own parameters. Although in principle we only require that the experts accept the same sized inputs and produce the same-sized outputs, in our initial investigations in this paper, we restrict ourselves to the case where the models are feed-forward networks with identical architectures, but with separate parameters[2]._\n\nFrom the explanation, it becomes evident that what is referred to as an _expert_ is essentially a sub-neural network with similar (or the same) structure. Training a large number of such neural networks in parallel can enhance the model's robustness. This aspect has also been elaborated in the experimental section of the revised paper (which includes comparisons with a pure MLP layer without dynamic adjustments). Moreover, reviewer S6XC mentioned in W2 that the original Figure 5 lacked sufficient experimental data on the number of expert blocks. We have now increased the number of data points, which reveals a rather distinct trend. The revised elucidation for this part of the experiment is as follows:\n\n>The experimental results indicate that the predictive performance first increases and then decreases with the rising number of expert blocks. Initially, the addition of expert blocks can effectively enhance feature fusion capabilities. This can be explained from two perspectives. Firstly, in contrast to a non-partitioned fully connected structure (i.e., a very wide fully connected layer), the expert blocks in the dynamic layer represent a form of regular sparse connections (See Appendix D for details). These sparse connections are further integrated through a decision layer, namely a gating layer, forming a hyper-network structure, which brings robustness to the entire network. Secondly, the expert blocks in the dynamic layer can be viewed as sub-modules in an ensemble learning framework. This ensemble learning architecture can effectively suppress the propagation of errors, reducing the variance in prediction results. Under the hypernetwork's constraints, the multi-modular architecture can evolve towards the optimal direction (See Appendix E for details). \n\n>\n>However, when the number of expert blocks becomes excessive, the performance deteriorates, which can be attributed to two factors. First, an increase in network parameters introduces additional training complexity, diminishing the network's generalization performance. Second, the gating network is fundamentally a multi-classifier. An excessive number of categories increases the decision-making complexity of the network, making it more prone to difficulties.\n\n>\n>Another important influencing factor is the temperature of the dynamic layer. High temperature values make the weight combinations tend towards an average, leading to weight homogenization. Conversely, low temperature values can render many experts ineffective in learning, thus impacting the results."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361755507,
                "cdate": 1700361755507,
                "tmdate": 1700363219757,
                "mdate": 1700363219757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "izR2ghZv5m",
                "forum": "z4qWt62BdN",
                "replyto": "rkM2iId88n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "(Following the previous text) In addition, we further investigate how the hypernetwork affects the weights of the expert blocks. In this regard, we have made some interesting observations. Here we will quote a part of the modified version:\n\n>Moreover, our investigation into the gating layer's outputs has unveiled some intriguing insights. Each entity-relation pair in the dataset, upon processing through the gating layer, yields an output vector $\\bm{o}$. These high-dimensional vectors were subjected to tSNE reduction, with the resultant visualization displayed in Figure 8 and Figure 9. Each point in this figure represents a unique entity-relation pair, distinguished by varying colors corresponding to different relationships. The visualization result reveals the following observations:\n>1. A tendency for entity-relation pairs of the same relationship type to cluster together, indicating proximity within the output space of the gated layer outputs.\n>2. The spatial distribution of clusters is significantly influenced by the nature of the relationships. For instance, relationships denoting inverse meanings (e.g. nominee\\_inv and nominee) or semantic opposites (e.g. place of birth vs. place of burial) exhibit a tendency to spatially diverge, exhibiting a unique central symmetry characteristic in the reduced dimensional space, Conversely, relationships with similar semantics (e.g. nationality and city town) are observed to be proximate in the latent space. This proved that DSparsE can capture various associations between entities and relations.\n>3. Alterations in the head entity of a relation pair result in minor shifts within the vector output, confined to a limited scope. Within a fixed relation, the relative positioning of nodes within its corresponding cluster does not display a discernible pattern. This phenomenon can be attributed to the relatively lower frequency of triples involving individual nodes compared to those associated with a particular relation type, posing challenges in accurately modeling semantic information for nodes. Still, certain examples, such as Mariah Carley and Dmitri Shostakovich\u2014both notable in the music domain\u2014demonstrate proximity within clusters pertaining to specific relations.\n\nFor the images mentioned in the text, please refer to our revised version of the paper.\n\n---\n\n>* **W2: Better figures and figure captions can be written. The architecture figures are small and the explanations in the captions is very limited.**\n\n**Response**\nThank you for your comment. we have added detailed captions to all images and provided thorough descriptions within the text in the revised version. Regarding the figures, we have made the revisions as follows:\n\n1. We enlarged the overall architecture diagram and bolded the key text within it. If you still see any issues, please do not hesitate to point them out, and we will continue to modify it to meet your requirements.\n2. We updated the architecture diagram of the residual layer (changed from horizontal to vertical orientation) and added a dynamic layer architecture diagram to aid understanding.\n3. We moved the original image for intuitive understanding of the sparse layer to the appendix due to space constraints.\n\nIn addition, we have completely revamped the structure of the paper and optimized the language expression in key sections. Specifically, some of the main changes are as follows:\n\n\n1. We have polished and updated the language of the article, especially the explanations in the abstract and the contributions section.\n2. We have modified and added several necessary charts to aid understanding and enriched the captions for these charts.\n3. We have added the YAGO3-10 dataset and provided corresponding baseline data, demonstrating that our model outperforms all similar models.\n4.We have added multiple comparative experiments and ablation studies, elucidating the mechanisms of each part.\n5. We have included additional data on the number of parameters and runtime.\n6. Our study of the latent space output has yielded interesting findings, which were mentioned in response to the first question. However, we hope that you will review them in conjunction with the revised version's images.\n7. Due to space constraints, we have placed some additional content in an appendix to aid understanding. Except for the parts related to the number of parameters, all other sections are referenced in the main text."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361858577,
                "cdate": 1700361858577,
                "tmdate": 1700361976916,
                "mdate": 1700361976916,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wn0kvFRpu0",
                "forum": "z4qWt62BdN",
                "replyto": "rkM2iId88n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "---\n\n>* **W3: Some statistical testings (or critical plots) for the comparing methods would make the arguments of the paper stronger.**\n>* **Q3: How is imposing sparse structure different from the drop-connect (dropout on the weights)**\n>* **Q5: How statistically different are the results between the models (or at least the top competing ones).**\n\n**Response**\nThank you for your constructive suggestions. In the revised version of the paper, we have added a dataset, YAGO3-10. Now we present the experimental results of DSparsE on the YAGO3-10 dataset across various metrics:\n\n| Dataset | Entity | Relation | Train | Valid | Test |\n| :---: | :---: | :---: |:---:|:---:|:---:|\n| YAGO3-10 | 123,182 | 37 | 1079,040 | 5000 | 5000 |\n\n\n| Dataset | Hits@1 | Hits@10 | MRR |\n| :---: | :---: | :---: |:---:|\n| DistMult | 0.240 | 0.540 | 0.340 |\n| ConvE | 0.350 | 0.620 | 0.440 |\n| ComplEx | 0.260 | 0.550 | 0.360 |\n| RotatE | 0.402 | 0.670 | 0.495 |\n| InteractE | 0.462 | 0.687 | 0.541 |\n| **DSparsE** | **0.464** | **0.690** | **0.544** |\n\nWe also give the detailed results as well as the comparison on FB15k-237:\n\n|                    |      | MRR   | Hits@10 | Hits@1 | MRR   | Hits@10 | Hits@1 | MRR   | Hits@10 | Hits@1 |\n|--------------------|------|-------|---------|--------|-------|---------|--------|-------|---------|--------|\n|                    | InteractE  |  |    | ComDensE  |  |    |   | DSparsE |   | |\n| **Pred Head**      | 1:1  | 0.386 | 0.547   | 0.245  | 0.422 | 0.557   | 0.349  | **0.434** | **0.572**   | **0.358**|\n|                    | 1:N  | **0.106** | **0.192**   | 0.043  | 0.084 | 0.181   | 0.043  | 0.101 | 0.185   | **0.044**|\n|                    | N:1  | 0.466 | 0.647   | 0.369  | 0.466 | 0.649   | 0.372  | **0.467** | **0.655**   | **0.376**|\n|                    | N:N  | 0.276 | 0.476   | 0.164  | 0.279 | 0.476   | 0.187  | **0.287** | **0.494**   | **0.195**|\n| **Pred Tail**      | 1:1  | 0.368 | 0.547   | 0.229  | 0.422 | 0.563   | 0.349  | **0.428** | **0.570**   | **0.351**|\n|                    | 1:N  | 0.777 | 0.708   | **0.881** | **0.779** | 0.884   | 0.717  | 0.778 | **0.886**   | 0.796|\n|                    | N:1  | 0.074 | 0.141   | 0.034  | 0.084 | 0.169   | **0.043** | **0.088** | **0.171**   | 0.042|\n|                    | N:N  | 0.395 | 0.617   | 0.272  | **0.396** | 0.618   | 0.285  | 0.395 | **0.624**   | **0.286**|\n\nFor the original Figure 4, we added a comparison with InteractE, demonstrating the monotonic decline of InteractE with increasing sparsity. We have explained this in the revised version of the paper as follows:\n\n>On the other hand, the performance of the InteractE model demonstrates a consistent decrease with increasing levels of sparsity. This trend is attributed to the model architecture of InteractE, where only the final feature decoding layer is an MLP layer. Experimental results indicate that introducing increased sparsity over the sparse interactions already captured by the earlier convolutional layers adversely impacts the model's predictive performance.\n\nWhile investigating the impact of sparsity, we also conducted partial tests of DSparsE on the WN18RR dataset. The experiments are still ongoing, but we provide some of the data as follows, which is consistent with the analytical conclusions in the original text:\n\n| Sparsity | Hits@1 |\n| :---: | :---: |\n| 0. | 0.438 |\n| 0.3 | 0.441 |\n| 0.5 | 0.443 |\n| 0.8 | 0.421 |\n\nAdditional ablation studies in the revised version of the paper prove that parallel dynamic and relation-aware layers can achieve better performance. \n\n| Dynamic+Relation-aware | Dynamic layer only | Relation-aware layer only |\n|:-------:|:-------:|:-------:|\n| **0.272**| 0.252 | 0.264 |\n\n\n\nThe Dropout method applies different masks with each training iteration, which adds to the instability of the training. In our method, the pattern of sparse connections is already determined at initialization. Essentially, our method cuts the connections between neurons, weakening the intensity of feature interactions. through empirical evidence, it was demonstrated that:\n\n1. Our method has advantages over simply reducing the network size (i.e., the number of output neurons).\n2. Our method has advantages over the use of Dropout alone."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362044791,
                "cdate": 1700362044791,
                "tmdate": 1700362791966,
                "mdate": 1700362791966,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cF1ISsF9xP",
                "forum": "z4qWt62BdN",
                "replyto": "rkM2iId88n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Did we fully address your concerns?"
                    },
                    "comment": {
                        "value": "**We appreciate your insightful comments and suggestions. Could you kindly confirm if our responses have adequately addressed your concerns and queries? We aim to ensure clarity and completeness in our revisions. Your feedback is highly valuable to us.**"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548351549,
                "cdate": 1700548351549,
                "tmdate": 1700548351549,
                "mdate": 1700548351549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]