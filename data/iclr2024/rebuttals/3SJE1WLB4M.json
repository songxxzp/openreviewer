[
    {
        "title": "Generalization error of spectral algorithms"
    },
    {
        "review": {
            "id": "INZjLVEmoG",
            "forum": "3SJE1WLB4M",
            "replyto": "3SJE1WLB4M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9152/Reviewer_wTWc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9152/Reviewer_wTWc"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a general framework that expresses the generalization error in spectral algorithms. A generic formula is derived for an arbitrary choice of kernel and profile. Then, the analysis is specialized to account for the circle model and the Wishart model. The theory can explain the KRR saturation phenomenon. Finally, a simplified naive model for noisy observations (NMNO) was proposed, for which the generalization error can be neatly expressed, and it is shown that under certain conditions, the circle model and the Wishart model collapse into NMNO."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper takes a generic point of view. It seems that the result can be applied to many different situations, which improves the power of the result.\n* The theory is nicely applied to provide a new perspective to already observed phenomena.\n* Overall, the paper is clearly written and most results are motivated."
                },
                "weaknesses": {
                    "value": "* The analysis in the paper does not account for the errors induced by discretization. For instance, although it is claimed at the beginning of the paper that the theory covers gradient descent, what is really done is for gradient flow. Also, when the kernel $K$ is discretized using the samples, the following assumptions are all made about this discrete kernel, and it is not obvious how it can be connected to the continuous kernel.\n* Certain parts of the analysis do not obtain nice closed formulas. For example, those in `section 4` for the Wishart model and those in `section 5` for the noise-free case.\n* Although this is a theoretical paper, it would be helpful to include some experiments, even if they are on synthetic datasets. This helps explain (and justify) the theory."
                },
                "questions": {
                    "value": "* On page 3, when you define $\\boldsymbol{\\Lambda}$, what is the number $P$? Is it the exact rank of $\\mathbf{K}$ or its numerical rank, or it does not matter too much?\n* In the statement of Proposition 1, I do not think $\\rho$ has been defined before and I do not think it is a standard notation outside this community. What is the precise definition of it?\n* Can you make your conjecture on page 6 slightly more formal?\n* It's not a question, but I think your discussion of the circle model in the GF case might be related to this line of work below. Did you happen to come across these works and see some potentially interesting connections to your paper?\n    * Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang, Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks, International Conference on Machine Learning, pp. 322-332, PMLR, 2019.\n    * Ronen Basri, David Jacobs, Yoni Kasten, and Shira Kritchman, The convergence rate of neural networks for learned functions of different frequencies, Neural Information Processing Systems 32, 2019\n    * Annan Yu, Yunan Yang, and Alex Townsend, Tuning frequency bias in neural network training with nonuniform data, International Conference on Learning Representations, 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697689439739,
            "cdate": 1697689439739,
            "tmdate": 1699637151638,
            "mdate": 1699637151638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MO4f6fucPb",
                "forum": "3SJE1WLB4M",
                "replyto": "INZjLVEmoG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9152/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments and questions and for the overall positive evaluation of our work! Let us address your question separately below\n\n> The analysis in the paper does not account for the errors induced by discretization. For instance, although it is claimed at the beginning of the paper that the theory covers gradient descent, what is really done is for gradient flow. Also, when the kernel \n is discretized using the samples, the following assumptions are all made about this discrete kernel, and it is not obvious how it can be connected to the continuous kernel.\n\nRegarding the first discretization of Gradient Flow (GF) to Gradient Descent (GD), you are right that in the paper, we demonstrate results only for GF. To address this, in the revised version of the paper we double-checked that all claims including gradient-based algorithms are made about GF and not GD. However, are loss functionals derived for Wishart and circle model do not assume any specific shape of the profile, and therefore GD profile $h_t(\\lambda)=1 - (1-\\alpha \\lambda)^t$ can be substituted in the functional to characterize GD generalization. We focused on GF due to its simplicity, and because we do not expect significant differences between GD and GF results in the setting of our paper(but we acknowledge the significant difference in other settings, e.g. for non-linear optimization or when convergence-divergence questions are involved).   \n\nRegarding the second discretization of the kernel using samples, I believe that we describe jointly the properties of the continuous kernel via mentioning population spectrum $\\lambda_l$ and features $\\phi_l(\\mathbf{x})$, and discrete empirical kernel matrix $\\mathbf{K}$ by mentioning how the training inputs $\\mathbf{x}_i$ are generated. For Circle model this is done explicitly. For Wishart model, this is done rather implicitly by the statistics of $\\phi_l(\\mathbf{x}_i)$ without defining separately features $\\phi_l(\\mathbf{x})$ and distribution of inputs $\\mathbf{x}_i$. Yet, such implicit characterization is sufficient for generalization error in eq. (3) to be well defined, and we can even access it experimentally as we describe in a new appendix section G.1. \n\n> Although this is a theoretical paper, it would be helpful to include some experiments, even if they are on synthetic datasets. This helps explain (and justify) the theory.\n\nThat is an excellent comment! During the rebuttal time, we have primarily focused on performing numerical experiments validating our theoretical results. Please see our general response for more details and the revised version of the paper for the new experiment results.\n\n> On page 3, when you define $\\mathbf{\\Lambda}$, what is the number $P$? Is it the exact rank of  $\\mathbf{K}$ or its numerical rank, or it does not matter too much?\n\nThroughout the paper, we use both finite and infinite values $P$. For example, all the theoretical calculations use the value $P=\\infty$, while in the numerical experiments we take large enough but finite $P$ to stay close to the theoretical setting. While accurate and rigorous treatment of $P=\\infty$ requires specific settings for it to be well defined, in our theoretical analysis we follow the \"physical level of rigor\", prioritizing shorter and more accessible derivations in favor of \u0435\u0440\u0443 accurate mathematical definitions of all the objects involved.  \n\n> In the statement of Proposition 1, I do not think $\\rho$ has been defined before and I do not think it is a standard notation outside this community. What is the precise definition of it?\n\nThank you for mentioning this! Indeed, the version of proposition 1 in the submission was not clear, leaving some ambiguity in the status of measures $\\rho$. Conceptually, we believe that these learning measures are new objects introduced in our work (which are essentially equivalent to the loss functional itself). In the revised version of the paper, we improved the wording of proposition 1, including the reference to a general expression of these measures through the expectations over the training dataset $\\mathcal{D}_N$. \n\n> Can you make your conjecture on page 6 slightly more formal?\n\nIndeed, that would be a great addition to the work. However, at the moment we feel we are not ready to formulate a rigorous version yet. This would require defining a family of kernel/data models and providing the analysis able to cover all models within the family. It is difficult to do at the moment since our current approach relies on explicit derivation of generalization error, with techniques tailored towards to data models we considered. Intuitively, we expect the condition of Theorem 2 regarding the absence of localization on the largest scale $s=\\nu$ to be the principal requirement for the model equivalence."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739712346,
                "cdate": 1700739712346,
                "tmdate": 1700739712346,
                "mdate": 1700739712346,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bkW1LVzt2y",
                "forum": "3SJE1WLB4M",
                "replyto": "INZjLVEmoG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continuation of the response"
                    },
                    "comment": {
                        "value": "> It's not a question, but I think your discussion of the circle model in the GF case might be related to this line of work below. Did you happen to come across these works and see some potentially interesting connections to your paper?\n\nThank you for pointing us to these important works. While we have encountered papers [1,2] before we were not aware of the work [3]. Generally, there is a significant connection in the sense that for both works (ours and [1,2,3]) the characterization of the spectrum of the underlying kernel method plays the central role in the analysis. Moreover, [2] also focuses on the Fourier analysis on circle and sphere, which is also at the heart of our Circle model. On the other hand, there seem to be several important differences in our approaches and results:\n1. We aim to develop explicit expressions for the risk, and not upper bounds as is mostly done in the mentioned papers.\n2. We do not require spectral gap assumptions as our main setting of interest is the power-law distribution of the eigenvalue which does not have a gap. \n3. We do not limit our analysis to particular types of kernel. For example, papers [1,2] focus on the NTK of infinitely wide two-layer ReLU network (denoted as $\\mathbf{H}^\\infty$). The work [2] shows that such kernel has the spectrum behaving as $\\lambda_k \\sim k^{-2}$ as $k\\to\\infty$, which roughly corresponds to fixing $\\nu=2$ in the setting of our paper. We chose to consider population spectrum $\\lambda_l,c_l$ as free variables so that we can access a wide range of values of $\\nu,\\kappa$, which makes it possible to discuss effects like saturation transition at $\\kappa=2\\nu$ and overlearning transition at $\\kappa=\\nu-1$.\n\nIt would be nice to do a deeper comparison of the results of our work and papers [1-3] under the conditions on the model and data that make jointly valid the results of both these papers and our paper so that the results can be compared directly. We leave it for future work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740848974,
                "cdate": 1700740848974,
                "tmdate": 1700741638045,
                "mdate": 1700741638045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sDUcWIyXdc",
            "forum": "3SJE1WLB4M",
            "replyto": "3SJE1WLB4M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9152/Reviewer_jkPS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9152/Reviewer_jkPS"
            ],
            "content": {
                "summary": {
                    "value": "This paper defines the spectral algorithms for regression task with Kernel Ridge Regression (KRR) and Gradient Descent (GD) as its special case. Then they derive and compute the generalisation error as a functional of the learning profile $h(\\lambda)$ where $\\lambda>0$ is the ridge and $h$ a function acting point-wisely on diagonal matrices. \n\nThen the paper assumes the two data models: high-dimensional Gaussian and low-dimensional translation-invariant model to further derive and interpret the generalisation error, recovering the result from previous study, as well as explaining some phenomenons in a new perspective, for example the saturation effect of KRR."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality: The idea of spectral algorithm is novel and can be seen as unifying different common machine learning methods like KRR and GD. \n\nQuality: The paper contains both theoretical deductions and experimental validations. The citation is also very up-to-date. \n\nClarity: The paper is organised in a clear manner by mentioning the motivation at first and postponing the technical details in the appendix.\n\nSignificance: Although the computation limit on the two simple models, this paper is a first step into understanding the generalisation error of spectral algorithm in a unifying way."
                },
                "weaknesses": {
                    "value": "There is almost no weakness except a few typos, for example, on section 4 EXPLICIT FORMS OF THE LOSS FUNCTIONAL (Circle model), on the fourth line it should \n$$\n\\hat{\\lambda}\\_k = \\sum\\_{n=-\\infty}^\\infty \\lambda\\_{k+Nn}.\n$$\n\nAlso, due to the complex form of the generalisation error, only two simple data models are considered."
                },
                "questions": {
                    "value": "I really like the idea of the generalisation error of spectral algorithm. What are the biggest obstacle to generalise the argument of this paper to more kernels?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697720625163,
            "cdate": 1697720625163,
            "tmdate": 1699637151452,
            "mdate": 1699637151452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9ZBb312mnx",
                "forum": "3SJE1WLB4M",
                "replyto": "sDUcWIyXdc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by authors"
                    },
                    "comment": {
                        "value": "Thank you very much for emphasizing the strong sides of our work, and we are glad you found the ideas of the paper interesting.   \n\nAlso, thank you for pointing out the typo, which we might have missed otherwise.  \n\n**Extension to more general kernel/data models.** Our approach in this paper is based on explicitly computing the generalization error (or its leading term in the $\\tfrac{1}{N}$-expansion in the case of Wishart model). One relatively straightforward generalization would be the high-dimensional version of the circle model (i.e. a grid on a torus). Besides that, we are currently unaware of any other models that could potentially admit an analytical solution on the same level of detail as the two currently considered models. One potentially interesting direction is related to symmetries: one may argue that analytical solutions for our two data models is due to high level of symmetry in both of them. Then, one can wonder whether there are some other symmetries that would allow for analytical characterization of the loss functional of the underlying model.\n\nHowever, if one is concerned only about the equivalence result of data models and NMNO, more options for generalization potentially appear. The equivalence is mainly based on the argument that at empirical eigenvalues far away from the tail (i.e. on the scale $s<\\nu$), the empirical quantities are close to the population ones with the difference between the two being sufficiently small. Thus an upper bound on this difference will suffice and the exact value is not required. We hope that it is possible to provide such an upper bound for a broader class of models, but this would require a very different set of technical tools than those we used in the current work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739576156,
                "cdate": 1700739576156,
                "tmdate": 1700739576156,
                "mdate": 1700739576156,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nxSJSwCT3C",
            "forum": "3SJE1WLB4M",
            "replyto": "3SJE1WLB4M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9152/Reviewer_v7ue"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9152/Reviewer_v7ue"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops optimization algorithms for KRR estimators using a new meta function termed ``spectral profile'', and shows its versatility to generalize to settings beyond KRR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Equation (7) in introducing the profile $h(\\lambda)$ seems new and interesting.\n\n* The theory appears to be rich, reasonable and self-contained."
                },
                "weaknesses": {
                    "value": "* The paper may seem a bit hard to follow at the beginning due to certain missing definitions (say, $\\nu$ in equation (2)), which need further context derived from reading Section 2."
                },
                "questions": {
                    "value": "* Is Figure 2 mentioned in the context? I'm kind of missing what those curves represent here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9152/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698294768853,
            "cdate": 1698294768853,
            "tmdate": 1699637151320,
            "mdate": 1699637151320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3hcyw7lYuE",
                "forum": "3SJE1WLB4M",
                "replyto": "nxSJSwCT3C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9152/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9152/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response by Authors"
                    },
                    "comment": {
                        "value": "Thank you for the positive evaluation of our work!\n\nWe agree that our introduction section has some things that are properly defined only in the next setting section. While we are also concerned about this issue, we intentionally decided to sacrifice some sequentiality of exposition in favor of a shorter informal discussion covering the main points.\n\nThank you for your question about Figure 2. While there was a reference to it in the text, your concern is indeed valid and touches a bigger problem of Section 5.3. being too compressed and, therefore, hard to understand. We tried to improve this point in the revised version of the paper - please see our general response for more details."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9152/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739462095,
                "cdate": 1700739462095,
                "tmdate": 1700739462095,
                "mdate": 1700739462095,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]