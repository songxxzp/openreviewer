[
    {
        "title": "Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction"
    },
    {
        "review": {
            "id": "dOejWOwboX",
            "forum": "M0MF4t3hE9",
            "replyto": "M0MF4t3hE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4489/Reviewer_t19r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4489/Reviewer_t19r"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel detection task, called IOD, which requires the detector to detect objects related to the human instruction. To unravel the problem, the authors first created a dataset called IOD-Bench, and then designed an Ins-DetCLIP model. The idea is simple and straightforward -- by incorporating an LLM with an open-vocabulary detector. Experiments on the IOD-Bench show that Ins-DetCLIP outperforms the baseline methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed IOD task is novel and has practical values.\n- The method is simple and easy to understand."
                },
                "weaknesses": {
                    "value": "- Instruction types 1, 2, and 3 seem to be variants of traditional detection or OV detection. With the help of LLM, it should be easy to convert these instructions to traditional detection or OV detection problems. Intuitively, I believe that using off-the-shelf detectors and LLM will achieve good results on these types of instructions. If not, a discussion of the reason is expected.\n- Instruction type 4 is abstract enough to meet my expectations of IOD. However, the objective of instruction type 4 is to detect related objects, which is vague. For instance, should cutleries be detected if the instruction is \"Prepare a healthy salad for lunch\"?\n- In Phase 2 of Ins-DetCLIP training, the visual encoder is frozen. This suggests that the LLM can only pick objects from the object proposals given by DetCLIP. However, DetCLIP is unaware of the instructions. As the instruction becomes more complex, DetCLIP will be unable to recall the target objects with a fixed number of proposals. \n- There have been several multimodal LLMs that are capable of detecting objects, i.e. Shikra[1], Kosmos[2]. The proposed Ins-DetCLIP should be compared with such methods."
                },
                "questions": {
                    "value": "- As for the evaluation metrics, the authors said that they use BERT to compute the similarities between the predicted categories and the GT categories. What if the prediction is completely irrelevant to the GT categories? In this case, the similarity score distribution will likely to be random. Will this cause the metrics to be unstable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4489/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742641189,
            "cdate": 1698742641189,
            "tmdate": 1699636424951,
            "mdate": 1699636424951,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IrElDkPAyr",
                "forum": "M0MF4t3hE9",
                "replyto": "dOejWOwboX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t19r (1)"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer t19r for your review and are grateful for the time you spent on our submission. We are also glad you think our proposed IOD task is novel and has practical values. Below we would like to give detailed responses to each of your comments.\n\n**1. Instruction types 1, 2, and 3 seem \u2026 using off-the-shelf detectors and LLM will achieve good results.**\n\nThanks for your question. Even though instruction types 1, 2, and 3 look like traditional detection tasks, they are fundamentally different: our task involves identifying items based on any free-form instruction with unlimited categories, as opposed to traditional detection methods, which are limited to a fixed number of classification heads.  We also prevent the need for the category names as in open-vocabulary detection. Due to the nature of our tasks, using off-the-shelf detectors and LLM is hard to convert these instructions tasks to traditional detection. We wish to clarify from two aspects:\n\n**(1) Task 1,2 and 3 are not merely traditional detection tasks.** \n\nWe wish to first recall the definition of the first 3 tasks:\n- task 1 requires  identifying all visible objects; \n- task 2 requires identifying certain objects given their category names;\n- task 3 needs to discover objects that fall under a parent category and predict their exact categories. \n\nWe wish to note that although task 1 and task 3 look like traditional detection tasks, e.g., detecting all 80 categories on COCO dataset, they are fundamentally different, since our tasks are open-world, rather than limited to a fixed number of classes as in traditional detection tasks. Meanwhile, Task 2 requires detecting specific objects given their categories, which is similar to open-vocabulary object detection and is also a frequent case of instruction. \n\n**(2) Off-the-shelf LLM+OV detector can not adequately address IOD task** \n\nCombining only the off-the-shelf  LLM with open-vocabulary detector for IOD task would be either one of the two approaches: 1) the detector first discovers all the objects in the image given a list of categories, then the LLM selects the objects that match the user's instruction; 2) the LLM first selects the object categories that are relevant to the instruction, then the detector discovers those objects based on the selected categories. However, the first approach requires a list of object names that is comprehensive enough to cover all the objects that may appear, which is impractical;  In the second approach, without knowing what objects might exist in the image, it is infeasible for the LLM to select objects only given the user instruction.\n\nHowever, your suggestion about using off-the-shelf detectors and LLM raises a very good point. Our 2-stage baseline approach is also inspired from this view, which prevents the need for the category list by leveraging the multimodal LLM to list the related object names based on the input image and instruction. The experimental results of the 2-stage baselines are shown in Table 1 of the main paper. We copy the table below for reference:\n\n| MLLM                           | Detector        | In domain                                      | Out of Domain                                   |\n|--------------------------------|-----------------|------------------------------------------------|------------------------------------------------|\n| MiniGPT4-Vicunna-7B            | DetCLIP         | 8.29 (12.3 / 10.3 / 7.51 / 3.05)               | 6.35 (10.4 / 7.13 / 5.29 / 2.57)               |\n| MiniGPT4-Vicunna-7B            | GroundingDino   | 8.22 (11.7 / 10.8 / 7.29 / 3.10)               | 6.34 (10.6 / 7.32 / 5.04 / 2.39)               |\n| LLAVA-Vicunna-7B               | GroundingDino   | 8.86 (14.5 / 11.2 / 6.46 / 3.30)               | 8.91 (14.2 / 11.5 / 6.02 / 3.94)               |\n| Ins-DetCLIP-FlanT5-base        | DetCLIP         | **15.3** (24.5 / 15.3 / 11.3 / 10.0)           | **13.7** (24.2 / 16.0 / 8.62 / 5.90)           |\n| Ins-DetCLIP-FlanT5-large       | DetCLIP         | **16.2** (25.6 / 16.4 / 11.7 / 11.0)           | **14.4** (25.4 / 16.2 / 9.65 / 6.50)           |\n\n\nThe two-stage baselines demonstrate inferior performances due to the following drawbacks:\n- input images of the VIT encoder typically have low resolution, which makes them unable to identify small objects;\n- the MLLM are known to suffer from hallucination, which often mention objects that do not appear in the image;\n- the LLM encounter difficulty in naming all the relevant objects sequentially when the number of objects grows large. \n\nOn the other hand, our approach demonstrates advantage in the following aspects: 1) we adopt DetCLIP to extract fine-grained object level features from high-resolution images, which prevents  the issue with small objects;  2) the LLM directly performs reasoning directly for each object feature in parallel, which reduces the difficulty for generation, achieves better results and boosts the efficiency."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204802322,
                "cdate": 1700204802322,
                "tmdate": 1700204802322,
                "mdate": 1700204802322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R1hKp2Jqyl",
                "forum": "M0MF4t3hE9",
                "replyto": "dOejWOwboX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t19r (2)"
                    },
                    "comment": {
                        "value": "**2. The objective of instruction type 4 is vague.**\n\nThank you for pointing out this potential confusion. In the context of our problem definition, the term \"relatedness\" refers to objects that are relevant or connected to the purpose described in the instruction. The related objects should be ones that can be used or involved in accomplishing the specific purpose outlined in the instruction. For example, if the purpose of an instruction is to assemble a piece of furniture, related objects could include tools, screws, and various components needed for assembly. Similarly, if the purpose is to prepare a meal, related objects could include ingredients, utensils, and cooking appliances. Therefore, in your question, the cutleries should be considered as \"related objects\" to the instruction \"prepare healthy salad for lunch\". \n\nIn our constructed dataset, the \"related\" objects of an instruction are comprehensive, which enables our model to produce all the objects that may be associated with the instruction. Since instruction-oriented detection is an intermediate step for building an intelligent system, in actual deployment, the predicted objects may be passed to another downstream module to decide what objects to be used exactly, and in which order they should be used. We have updated our paper in Section C of the Appendix and hope to better clarify this issue.\n\n**3. The visual encoder is frozen. This suggests that the LLM can only pick objects from the object proposals given by DetCLIP.**\n\nThank you for the question. During fine-tuning, we enable the DetCLIP to output object features in a class-agnostic manner by training a lightweight classification head to distinguish the foreground proposals from the background (elaborated in Section 4.2 of the updated paper). This is to ensure that the features produced by DetCLIP can encompass the majority of the visible objects, as comprehensively as  possible. Afterwards, the reasoning and decision-making processes are conducted by the LLM. In this way, DetCLIP behaves like an instruction-agnostic visual feature extractor on the object-level, which is analogous to the VIT-based image-level visual feature extractors used in multi-modal large language models, such as MiniGPT-4 and LLAVA. However, we agree that enabling different visual features to be extracted given various instructions could be an interesting direction to explore, which may potentially lead to higher performance. InstructBLIP (cited in related work of the updated version) has made attempts to achieve this for image-level multimodal understanding. We will leave this exploration to future work.\n\n**4. Comparison with Shikra and Kosmos2.**\n\nThank you very much for the suggestion. We first update these methods in the related work section of our paper. Then, we perform extra experiments to compare with the mentioned approaches on our benchmark, and the results are demonstrated below (updated in Table 10 of the Appendix). \n\nEven though these approaches excel at tasks such as referring expression comprehension or spot caption, we observe that such methods demonstrate lower performances on our benchmark, which may be attributed to the following two reasons: \n- these methods adopt low resolution input images due to the VIT image encoders, which makes the small objects difficult to be recognized; \n- the sequence representation of bounding boxes make it difficult for the LLM to output all the coordinate locations when there are many target object related to the instruction. \n\nHowever, we believe these approaches that integrate the fine-grained perception abilities into LLMs are promising directions, which deserve to be explored more in the future.\n\n| Model                     | In domain                                      | Out of Domain                                   |\n|---------------------------|------------------------------------------------|------------------------------------------------|\n| Kosmos-2 [1]                  | **7.51** (11.5 / 9.61 / 6.30 / 2.64)           | **6.98** (11.0 / 9.13 / 5.30 / 2.47)           |\n| Shikra [2]                    | **7.10** (9.57 / 9.20 / 6.43 / 3.22)           | **6.82** (9.10 / 8.93 / 6.21 / 3.04)           |\n| Ins-DetCLIP-FlanT5-base    | **15.3** (24.5 / 15.3 / 11.3 / 10.0)           | **13.7** (24.2 / 16.0 / 8.62 / 5.90)           |\n| Ins-DetCLIP-FlanT5-large   | **16.2** (25.6 / 16.4 / 11.7 / 11.0)           | **14.4** (25.4 / 16.2 / 9.65 / 6.50)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204993160,
                "cdate": 1700204993160,
                "tmdate": 1700205800040,
                "mdate": 1700205800040,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5dXaSJcsTQ",
                "forum": "M0MF4t3hE9",
                "replyto": "dOejWOwboX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer t19r (3)"
                    },
                    "comment": {
                        "value": "**5. As for the evaluation metrics, What if the prediction is completely irrelevant to the GT categories?**\n\nThank you for this question. We wish to note that the similarity score calculated using BERT is leveraged to map the model's prediction to the ground truth category of the dataset. If the score is lower than a preset threshold (in our case, we  set it  to be 0.4), the prediction is going to be treated as false positive and decrease the mAP score. We update this missing information in our paper. Afterwards, we follow the conventional evaluation pipeline for object detection. Since we follow the same evaluation protocol for all the methods, the results are comparable and are able to reflect the true performance.\n\nHowever, we agree that there is no standard and well established evaluation metric for generation-based tasks. For example, many recent works use powerful LLMs such as GPT-4 to evaluate the correctness of the generated answers. Unfortunately, such evaluation may not be scalable, especially in the case of IOD tasks, where the number of object predictions that need to be evaluated is huge. How to properly conduct evaluation for such tasks is indeed a research topic that has profound impact. We will leave its investigation in our future work.\n\nOverall, we sincerely thank you for your insightful points and your time spend on our paper. We have added the experiments and revised statements accordingly in our current version. We hope our answers have addressed each of your concerns. If you have any further questions, we are happy to address them.\n\n**References:**\n\n[1] Kosmos-2: Grounding Multimodal Large Language Models to the World\n\n[2] Shikra: Unleashing Multimodal LLM\u2019s Referential Dialogue Magic"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205076691,
                "cdate": 1700205076691,
                "tmdate": 1700205782669,
                "mdate": 1700205782669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dqPFWbnZtg",
                "forum": "M0MF4t3hE9",
                "replyto": "dOejWOwboX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Waiting for further discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer t19r,\n\nWe are grateful for your valuable time during the reviewing process, and we appreciate all your constructive comments on our work. We are also very glad that you acknowledge that our proposed IOD task is novel and has practical values, and the method is simple and easy to understand.\n\nSince it has come to the end of the first rebuttal phase, we sincerely hope that our response has addressed your problems and will continue to address further questions if you have any.\n\nThank you!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533081197,
                "cdate": 1700533081197,
                "tmdate": 1700576468377,
                "mdate": 1700576468377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "27TRnFUq0Q",
            "forum": "M0MF4t3hE9",
            "replyto": "M0MF4t3hE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4489/Reviewer_Y4Gg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4489/Reviewer_Y4Gg"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce a novel task named Instruction-oriented Object Detection (IOD). In the course of developing an IOD system, an innovative dataset, IOD-Bench, is presented, along with a proposed model named Ins-DetCLIP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea that the author solves real-world problems through diverse instructions is interesting."
                },
                "weaknesses": {
                    "value": "1-I understand the authors' aspiration to address a wide range of real-world scenarios through diverse instructions. However, I find the motivations behind the four tasks to be somewhat unclear. The similarity amongst the first three tasks appears quite pronounced, and it seems that existing datasets already cater to these scenarios to a certain extent.\n\n2-In Section 5.2, the authors described integrating BLIP2 and MiniGPT4 with open-vocabulary detectors (OVD) in a two-stage sequential manner to construct baseline models. I'm curious as to why such a sequential approach was chosen for building the baseline models. Would it not be more straightforward to employ the OVD method for direct open-vocabulary detection? My concern is that a two-stage sequential manner could inherently limit the system's performance to the accuracy of both models, seeming like a suboptimal choice."
                },
                "questions": {
                    "value": "see Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4489/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760353808,
            "cdate": 1698760353808,
            "tmdate": 1699636424883,
            "mdate": 1699636424883,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jgum3cB3Nu",
                "forum": "M0MF4t3hE9",
                "replyto": "27TRnFUq0Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Y4Gg (1)"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer Y4Gg for the review and are grateful for the time you spent with our submission. We are glad for the acknowledgement that our proposed idea is interesting, and we wish to address your concerns by giving detailed responses to each of your comments as follows:\n\n**1. The similarity amongst the first three tasks appears quite pronounced, existing datasets already cater to these scenarios.**\n\nThanks for pointing out this potential confusion. We wish to clarify that even though the first three tasks may look similar, they are fundamentally different, which would expect different instructions from the user,  and the existing datasets alone do not cater for our needs. \n\nWe wish to recall the definition for each task first: \n- task 1 requires  identifying all visible objects; \n- task 2 requires identifying certain objects given their category names;\n- task 3 needs to discover objects that fall under a parent category and predict their exact categories. \n\nTo enable the object detection system to flexibly perform such tasks given natural language instructions,  we not only need the existing datasets containing the box annotations for objects, but also the natural language instructions that indicate which task to complete. Therefore, the key is to design <instruction>-<objects of interest> pairs to help training and evaluation. Specifically, we design such pairs for each task by leveraging the generative power of ChatGPT as follows:\n\n\n* for task 1, we let ChatGPT produce instructions that have the same meaning as \u201cdetecting all objects in an image\u201d. In this case, all the objects that exists in the image are considered as objects of interest; \n* for task 2, we let ChatGPT generate more templates similar to \u201cdetect <obj_names> in the image\u201d, where <obj_names> is a placeholder for a list of objects and are considered as objects of interest; \n* for task 3, we generate the instruction-objects pairs such as \u201cLook for all electronic devices: [\u2019Moniter/TV\u2019, \u2019Laptop\u2019, \u2019Cell Phone\u2019, \u2019Camera\u2019, \u2019Computer Box\u2019, \u2019Tablet\u2019, \u2019Keyboard\u2019, \u2019Mouse\u2019, \u2019Printer\u2019, \u2019Projector\u2019, \u2019Telephone\u2019, \u2019Head Phone\u2019, \u2019Remote\u2019, \u2019Microphone\u2019, \u2019Speakers\u2019, \u2019Surveillance Camera\u2019, \u2019Air Conditioner\u2019, \u2019Fan\u2019, \u2019Router/modem\u2019]. The objects mentioned in the list will be considered as objects of interest for given the instruction.\n\nCombining the collected <instruction>-<objects of interest> pairs and the bounding box annotations of existing detection datasets, we are able to curate the IOD-Bench to facilitate the research in instruction-oriented object detection. We greatly appreciate the advice and update the detailed motivation and explanation for the construction of IOD-Bench in Section C of the Appendix."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203676981,
                "cdate": 1700203676981,
                "tmdate": 1700205178436,
                "mdate": 1700205178436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4FnKrsSoEY",
                "forum": "M0MF4t3hE9",
                "replyto": "27TRnFUq0Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Y4Gg (2)"
                    },
                    "comment": {
                        "value": "**2. Why not use OVD method for direct open-vocabulary detection?**\n\nThank you for the question. We agree that employing the OVD method for IOD tasks is the most straight-forward baseline to think of. However, we wish to clarify that open-vocabulary detectors can only detect objects given the exact category names, and do not have the capability to detect objects of interest given an open-ended user instruction. For instance, to detect the 80 categories of COCO, those category names need to be directly provided to the detector. The detector can not work with language instructions such as \"detect all visible objects\", \"detect all the sports equipment\", or \"detect a place where I can rest comfortably\", since the object names are not directly given to the detector. \n\nTo verify this, we directly use open-vocabulary object detectors for the four tasks, and demonstrate the results as follows:\n\n| Model                     | In domain                                      | Out of Domain                                   |\n|-----------------|------------------------------------------------|------------------------------------------------|\n| DeCLIP                  | **2.81** (5.35 / 4.33 / 1.29 / 0.28)           | **2.97** (5.49 / 4.96 /1.10 / 0.31)           |\n| GroundingDino        | **4.08** (4.49 / 9.33 / 2.29 / 0.21)           | **3.96** (4.26 / 8.97 / 2.41 / 0.19)          |\n| Ins-DetCLIP-FlanT5-base    | **15.3** (24.5 / 15.3 / 11.3 / 10.0)           | **13.7** (24.2 / 16.0 / 8.62 / 5.90)           |\n| Ins-DetCLIP-FlanT5-large | **16.2** (25.6 / 16.4 / 11.7 / 11.0)           | **14.4** (25.4 / 16.2 / 9.65 / 6.50)           |\n\nWe observe that directly using the OVD methods for IOD tasks achieve poor performances, since they are not able to interpret natural language instructions.\n\nTherefore, we choose the 2-stage sequential baselines due to the following reason: the Multimodal LLMs have shown superior ability in terms of instruction-following and reasoning, which makes it natural to utilize them to first name all the objects of interest given the instruction, then perform localization with the open-vocabulary detectors.\n\nHowever, we agree that the two-stage approaches are indeed sub-optimal, which suffer from the following problems: \n- input images of the VIT encoder typically have low resolution, which makes them unable to identify small objects; \n- the MLLM are known to suffer from hallucination, which often mention objects that do not appear in the image; \n- the LLM encounter difficulty in naming all the relevant objects sequentially when the number of objects grows large.  \n\nThis is where our approach demonstrates advantage, since 1) we adopt DetCLIP to extract fine-grained object level features from high-resolution images, which prevents  the issue with small objects;  2) the LLM directly performs reasoning directly for each object feature in parallel, which reduces the difficulty for generation, achieves better results and boosts the efficiency.\n\nOverall, many thanks for your valuable comments.  We have updated our paper with the experiments and detailed explanations accordingly. We hope our answers have addressed each of your concerns. If you have any further questions, we are happy to address them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203832876,
                "cdate": 1700203832876,
                "tmdate": 1700205266033,
                "mdate": 1700205266033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "auZDBx6k4C",
                "forum": "M0MF4t3hE9",
                "replyto": "27TRnFUq0Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Waiting for further discussion."
                    },
                    "comment": {
                        "value": "Dear Reviewer Y4Gg,\n\nWe are grateful for your valuable time during the reviewing process, and we appreciate all your constructive comments on our work. We are also very glad that you acknowledge that our work solves real-world problems through diverse instructions to be interesting.\n\nSince it has come to the end of the first rebuttal phase, we sincerely hope that our response has addressed your problems and will continue to address further questions if you have any.\n\nThank you!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533005135,
                "cdate": 1700533005135,
                "tmdate": 1700576411013,
                "mdate": 1700576411013,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AZHQhRf4kC",
            "forum": "M0MF4t3hE9",
            "replyto": "M0MF4t3hE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4489/Reviewer_tem2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4489/Reviewer_tem2"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Instruction-oriented Object Detection (IOD), a novel task aimed at improving human-computer interaction by enabling object detectors to interpret user instructions for identifying specific objects. IOD necessitates the understanding of natural-language instructions and contextual reasoning to provide the name and location of the desired objects, posing new challenges to current object detection systems. To address this, the authors develop a dataset called IOD-Bench, consisting of instruction-guided detections and specialized evaluation metrics, and leverage large-scale language models (LLMs) to generate a diverse set of instructions based on existing public object detection datasets. The proposed model, Ins-DetCLIP, utilizes the knowledge within LLMs to enable instruction-following capabilities in the detector. It employs a visual encoder, DetCLIP, to extract object-level features and aligns them with the input instructions through a cross-modal fusion module integrated into a pre-trained LLM. The experimental results on IOD-Bench demonstrate that Ins-DetCLIP consistently outperforms baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Overall, the task presented in this manuscript is captivating and aligns well with the practical applications of an intelligent detection system. The instructions have been well-crafted, bolstering my view on this matter.\n\n2. The clarity of the writing in this submission is commendable, making the content generally straightforward to grasp.\n\n3. The impressive performance of Ins-DetCLIP underscores the effectiveness of the proposed instruction tuning paradigm. Furthermore, the authors have done a good providing an extensive range of experiments to scrutinize all the design choices, which is highly valuable."
                },
                "weaknesses": {
                    "value": "1. I would recommend that the authors take another pass at proofreading the manuscript to ensure consistent and correct formatting throughout. For instance, there is a conventional practice of inserting a space before references in both the main body text and tables.\n\n2. With respect to the main results showcased in Table 1, while they are compelling and seem to align with the authors' motivations, I note that the comparison methods, such as BLIP-2 and MiniGPT-4, do not incorporate human instructions interns of object detection. I presume that performing instruction tuning on these models could enhance them and provide insights into the generalizability of the proposed tuning method. I am curious about the feasibility of this approach.\n\n3. In terms of the dense captioning tasks, despite the impressive performance demonstrated, the comparison is made with somewhat outdated methods. There are contemporary methods, building on SAM or other models, capable of performing this task as well. A performance comparison with Ins-DetCLIP would be insightful. While Ins-DetCLIP may not outperform these methods, including such results or providing justification would help to elucidate the performance gap, potentially laying the groundwork for future enhancements."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4489/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4489/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4489/Reviewer_tem2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4489/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770261518,
            "cdate": 1698770261518,
            "tmdate": 1699636424796,
            "mdate": 1699636424796,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NUOf1A3ApE",
                "forum": "M0MF4t3hE9",
                "replyto": "AZHQhRf4kC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tem2"
                    },
                    "comment": {
                        "value": "Thank you very much for your insightful suggestions. We are glad that you find our proposed task to be captivating and align well with the practical applications of an intelligent detection system, the instructions are well crafted, and the experiments to be thorough. We wish to address your concerns by giving detailed responses to each of your comments as follows:\n\n**1. Reference formats are wrong.**\n\nThank you so much for pointing out this careless mistake. We have corrected the citations in the updated version.\n\n\n**2. Instruction tuning on the two-stage baselines.**\n\nThank you for pointing out this potential confusion. For experiments in Table 1, We indeed perform instruction tuning on the LLM part of the two-stage baselines, which aims to enable the LLM to elicit the target object categories corresponding to the user's instruction. Such categories are then given to the open-vocabulary object detector (OVOD) to perform localization. For completeness, we further compare with the two-stage baselines without instruction tuning in the table below, and update it to Table 8 of the Appendix. The results show that tuning with our dataset also boosts the performances of two-stage baselines, which proves instruction tuning indeed generalizes well to other approaches. However, due to the design choices, such methods are still inferior to end2end instruction tuning of Ins-DetCLIP:\n\nThe reasons for the inferior performance of two-stage baselines in IOD tasks are as follows: \n- Input images of the VIT encoder typically have low resolution, which makes them unable to identify small objects. In contrast, the input resolution of our DetCLIP backbone has high resolution, which prevents the problem; \n- The LLMs are known to suffer from hallucination, which often mention objects that do not appear in the image;\n- The LLM encounter difficulty in naming all the relevant objects sequentially when the number of objects grows large. \n\n| MLLM                                   | Detector | Tuned | In domain                                     | Out of Domain                                   |\n|----------------------------------------|----------|-------|-----------------------------------------------|------------------------------------------------|\n| BLIP2-FlanT5-XL | DetCLIP | N     | 3.06 (4.16 / 3.95 / 2.34 / 1.80)           | 3.03 (4.27 / 3.70 / 2.45 / 1.68)               |\n| BLIP2-FlanT5-XL          | DetCLIP  | Y     | 3.95 (5.37 / 4.51 / 3.04 / 2.89)           | 3.71 (5.21 / 4.40 / 2.79 / 2.43)               |\n| MiniGPT4-Vicunna-7B| DetCLIP  | N     | 5.57 (10.5 / 7.20 / 2.62 / 1.94)           | 5.45 (9.63 / 7.31 / 2.69 / 2.18)               |\n| MiniGPT4-Vicunna-7B| DetCLIP  | Y     | 8.29 (12.3 / 10.3 / 7.51 / 3.05)           | 6.35 (10.4 / 7.13 / 5.29 / 2.57)               |\n| Ins-DetCLIP-Opt1.3b                     | DetCLIP  | Y     | **14.9 (22.9 / 14.7 / 11.5 / 10.4)**          | **11.4 (20.4 / 13.6 / 7.42 / 4.10)**           |\n| Ins-DetCLIP-FlanT5-base                 | DetCLIP  | Y     | **15.3 (24.5 / 15.3 / 11.3 / 10.0)**          | **13.7 (24.2 / 16.0 / 8.62 / 5.90)**           |\n| Ins-DetCLIP-FlanT5-large                | DetCLIP  | Y     | **16.2 (25.6 / 16.4 / 11.7 / 11.0)**          | **14.4 (25.4 / 16.2 / 9.65 / 6.50)**           |\n\n\n\n**3. Comparison with more up-to-date methods on dense-captioning tasks.**\n\nThank you for pointing this out. We further compare with more contemporary methods on dense caption tasks, which include CapDet (CVPR2023) [1] and Grit (Arxiv) [2]. Among them, CapDet is based on the image-language foundation model BLIP. The results are shown in the following table, where our method still achieves slightly better performance. Even though we were not able to discover new dense captioning methods that are based on powerful vision experts such as SAM, it is indeed a viable approach that could lead to strong performance. We will leave this investigation to future research. We also updated this comparison in Table 1 of the main paper.\n\n| Method                    | VG V1.2 mAP (%) | Method                    | VG COCO mAP (%) |\n|---------------------------|-----------------|---------------------------|-----------------|\n| CapDet   | 15.44 | CapDet  | 13.98 |\n| Grit    | 15.50 | Grit |  -    |\n| Ins-DetCLIP-Flanbase      | **15.70**       | Ins-DetCLIP-Flanbase      | **14.35**       |\n| Ins-DetCLIP-Flanlarge     | **16.13**       | Ins-DetCLIP-Flanlarge     | **15.01**       |\n\nOverall, many thanks for your insightful points and suggestions. We have added the experiments and revised our paper accordingly in our latest version.  We hope our answers have addressed your concerns. If you have any further questions, we are happy to address them.\n\n**References:**\n\n[1] CapDet: Unifying Dense Captioning and Open-World Detection Pretraining\n\n[2] GRiT: A Generative Region-to-text Transformer for Object Understanding"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203411592,
                "cdate": 1700203411592,
                "tmdate": 1700205978940,
                "mdate": 1700205978940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BUjaujRena",
                "forum": "M0MF4t3hE9",
                "replyto": "NUOf1A3ApE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Reviewer_tem2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Reviewer_tem2"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the authors' rebuttal"
                    },
                    "comment": {
                        "value": "Many thanks for your comprehensive and prompt rebuttal. I think the experiments that you provided are convincing. I strongly recommend adding them to the main paper. \n\nWith respect to the experiments on adding instruction tuning to BLIP2 and MiniGPT4, why would BLIP2-FlanT5-XL and Ins-DetCLIP-FlanT5 have such a huge performance gap although they adopt almost identical LLMs?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549124770,
                "cdate": 1700549124770,
                "tmdate": 1700549124770,
                "mdate": 1700549124770,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vQUeYXqyFB",
                "forum": "M0MF4t3hE9",
                "replyto": "AZHQhRf4kC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tem2"
                    },
                    "comment": {
                        "value": "Thank you very much for your response! Since we need to stick to the original requirement for 9-page-limit, we will carefully rearrange the paper's contents and update the new results to the main paper in the final version.\n\n**Why would BLIP2-FlanT5-XL and Ins-DetCLIP-FlanT5 have such a huge performance gap although they adopt almost identical LLMs?**\n\nThank you for this interesting question, the reasons are as follows:\n\n- For the 2-stage baselines, the LLM needs to output all the object names that satisfy the user's instruction. This is particularly challenging, especially for weaker LLMs such as FlanT5-XL. We observe that they tend to miss many objects in the generated response. On the other hand, our method processes each object in parallel, and the LLM only needs to predict the category for a single object, which greatly alleviates the difficulty;\n\n- For two stage baselines, the input images to the MLLM (e.g., BLIP2) typically have low resolution, which makes them unable to identify small objects. In contrast, the input resolution of our DetCLIP backbone has high resolution, which prevents the problem.\n\nThank you again for your valuable time spent on reviewing our paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551178051,
                "cdate": 1700551178051,
                "tmdate": 1700551344147,
                "mdate": 1700551344147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RpDDlcjUSW",
            "forum": "M0MF4t3hE9",
            "replyto": "M0MF4t3hE9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4489/Reviewer_rzsX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4489/Reviewer_rzsX"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents a new task called Instruction-oriented Object Detection (IOD), which aims for the model to accept human instruction and generate corresponding detection results. For training the model on such a task, the authors first construct a dataset termed IOD-Bench and the corresponding evaluation metrics. Based on DetCLIP, Ins-DetCLIP is presented for making the open-vocabulary object detector able to follow instructions, where an LLM is attached to the frozen visual encoder of pretrained DetCLIP.\n\nEmpirically, Ins-DetCLIP notably outperforms baseline approaches such as BLIP-2, MiniGPT-4 and LLaVA. Additionally, the model shows the capability of performing captioning for the relevant objects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The manuscript goes beyond image-level captioning/question answering and proposes to use LLM for instructed object detection, which is novel.\n- The approach provides new insights into what a LLM could do when it is connected to an open-vocabulary object detector. \n- Compared to existing approaches such as BLIP-2, MiniGPT-4 and LLaVA, Ins-DetCLIP demonstrates an outstanding capability of performing instruction guided object detection. \n- Increasing the size of LLM could benefit all the tasks, showing good scalability."
                },
                "weaknesses": {
                    "value": "- It is unclear how the object bounding boxes and the features are generated in the first place before object-level cross modal fusion.\n- It is not shown to what extent is the approach dependent on the performance/quality of the phase-1 training, which would be an important aspect for understanding the approach that requires two-phase training.\n- The inference speed comparison seems unfair, since the model sizes are different. It is difficult to harness whether the approach is slow or fast.\n- Citation formats are wrong."
                },
                "questions": {
                    "value": "- It would be interesting to know how much resources and GPU days are required to train such a model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4489/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699127251866,
            "cdate": 1699127251866,
            "tmdate": 1699636424726,
            "mdate": 1699636424726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kJAtE00jlg",
                "forum": "M0MF4t3hE9",
                "replyto": "RpDDlcjUSW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4489/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rzsX"
                    },
                    "comment": {
                        "value": "We sincerely thank Reviewer rzsX for your positive feedback and are grateful for the time you spent on our submission. We are also glad you think our idea is novel and provides new insights. Below we would like to give detailed responses to each of your comments.\n\n**1. How the object bounding boxes and the features are generated before object-level cross-modal fusion.**\n\nThank you for pointing out this potential confusion. To enable generating the object features and bounding boxes, we introduce a foreground-background classification head, similar to the spirit of the Region Proposal Network (RPN) [1]. In our original submission, we located the corresponding details in the appendix due to the space limitation in our initial submission. However, your comment prompted us to realize that we have missed this important detail in the main paper. We have moved this information to Section 4.2 of the updated paper.\n\n**2. The importance of the phase-1 training.**\n\nThank you for the suggestion! In order to validate the significance of the phase-1 training, we conduct experiment by engaging in the second stage of instruction tuning without pretraining. Note that we continue to employ ImageNet-pretrained Swin-Transformer weights for initializing the vision backbone, alongside a FILIP-pretrained text encoder in this experiment.  Our observations indicate a substantial decline in the model's performance, primarily attributed to its diminished capability in extracting high-quality object features. More specifically, the model's performance experienced a marked reduction, dropping from 15.3 to 8.46 for in-domain instructions, and from 13.7 to 8.00 for out-of-domain instructions. These findings have been incorporated into Table 9 in the Appendix of our revised manuscript.\n\n| Model                    | Pretrain | In domain                                      | Out of Domain                                    |\n|--------------------------|----------|------------------------------------------------|-------------------------------------------------|\n| Ins-DetCLIP-FlanT5-base  | N        | **8.46** (12.6 / 10.5 / 7.16 / 3.53)            | **7.75** (11.3 / 10.3 / 6.10 / 3.31)            |\n| Ins-DetCLIP-FlanT5-base  | Y        | **15.3** (24.5 / 15.3 / 11.3 / 10.0)            | **13.7** (24.2 / 16.0 / 8.62 / 5.90)            |\n\n**3. The inference speed comparison seems unfair.**\n\nGreat suggestion! Indeed, the LLMs used in the two-stage baselines (size > 7B ) are larger than the ones used in Ins-DetCLIP (size < 1B). This is because such methods rely heavily on the quality of  LLM's generated object names, switching to a small model hurts the performance greatly. As suggested, we further show the performance and inference speed of a smaller counterpart, where the performance is severely degraded from 4.32 to 3.14 mAP (updated in Table 1 of the main paper), and the inference speed (1.4 fps) is still slower than Ins-DetCLIP-FlanT5-Base (2.2 fps) and Ins-DetCLIP-FlanT5-Large (1.6 fps), which is updated in Table 7 of the Appendix and shown by the table below. This is because the two-stage baselines require the LLM to first sequentially output all the category names of the objects before providing them to the OV detector. On the other hand, Ins-DetCLIP handles the generation for each target object in parallel.\n\n| Method                 | Inference Speed (FPS) |\n|------------------------|----------------------|\n| Blip2-FlanBase         | 1.4                  |\n| Ins-DetCLIP-FlanBase   | **2.2**              |\n| Ins-DetCLIP-FlanLarge  | **1.6**              |\n\n**4. Citation formats are wrong.**\n\nThank you so much for pointing out this careless mistake. We have corrected the citations in the updated version.\n\n**5. Resources and GPU days required to train the model.**\n\nFor the first stage, the training of DetCLIP-T with Swin-T backbone takes 63 hours on 32 V100 GPUs. The resulting model from the pretraining stage can already serve as a powerful open-vocabulary object detector. In the second stage, it takes only around 24 hours on 16 V100 GPUs for instruction tuning. We have added this information in the Appendix of our updated paper.\n\nThank you very much for the constructive comments, which really help us further improve our work. We hope our answers have addressed your concerns. If you have any further questions, we are happy to address them\n\n**References:**\n\n[1] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4489/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202943198,
                "cdate": 1700202943198,
                "tmdate": 1700205714918,
                "mdate": 1700205714918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]