[
    {
        "title": "Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision"
    },
    {
        "review": {
            "id": "7qs6BpkZpo",
            "forum": "elMKXvhhQ9",
            "replyto": "elMKXvhhQ9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9315/Reviewer_tbTo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9315/Reviewer_tbTo"
            ],
            "content": {
                "summary": {
                    "value": "This paper has addressed the challenge of graph anomaly detection under limited supervision by introducing a novel model, which effectively leverages the abundance of unlabeled data for consistency training by incorporating a novel learnable data augmentation mechanism. Furthermore, CONSISGAD exploits the disparity in homophily distribution between normal and anomalous nodes, enabling the formulation of a refined GNN backbone that enhances its discriminatory power between the two classes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The logic of the whole paper is relatively clear and there are no obvious grammatical errors.\n\n2. There are quite sufficient experiments to verify the proposed method."
                },
                "weaknesses": {
                    "value": "1) why Eq4 can achieve the edge-level homophily representation? where does it differ from GCN aggregation? \n\n2) as you said, GAD has low homophily distribution, whether  the homophily-aware neighbourhood aggregation you propose will be effective for abnormal nodes, I deeply doubt it.\n\n3) I don't find  the definition of Vhq in Section 2,  whiat is the meaning of Vun?\n\n4) please compare  your learnable augmentation with other augmentation methods  \n\n5) please give complexity analysis"
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9315/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698497928586,
            "cdate": 1698497928586,
            "tmdate": 1699637173063,
            "mdate": 1699637173063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RjcdiUmfuK",
                "forum": "elMKXvhhQ9",
                "replyto": "7qs6BpkZpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer tbTo,\nThank you for your thoughtful and constructive feedback on our submission 9315. We appreciate the time you have taken to review our work and provide detailed insights. Below, we address each of the concerns and questions you raised.\n\n**1. Why Eq4 can achieve the edge-level homophily representation? Where does it differ from GCN aggregation?**\n\n*For the first question: Why Eq4 can achieve the edge-level homophily representation?*\n\nIn Eq.(4), the representation of an edge (e.g., $(v,u)$) is calculated using the representations of its two end nodes (e.g., $v$ and $u$) through the function $\\delta(\\cdot,\\cdot;\\theta_{\\delta})$. Furthermore, Eq.(4) aggregates these edge-level representations into the node representation of node $v$. Notably, the node representation ($\\mathbf{h}^l_v$) is calculated based on the distinctions in homophily distribution between normal and anomalous nodes, as discussed in the Introduction.\n\nThe cross-entropy loss function elicits inverse predictions for normal and anomalous nodes. Specifically, this loss function steers the edge representations of edges surrounding a normal node away from those surrounding an anomalous node, emphasizing their distinct homophily distributions. As a result, edges with high homophily around normal nodes share similar representations, while edges with low homophily around anomalous nodes exhibit analogous representations. Therefore, through this optimization process, Eq.(4) achieves disparate representations for high-homophily and low-homophily (high-heterophily) edges, ensuring edge-level homophily representations for edges around different nodes.\n\nTo validate the ability of the edge representations to capture edge-level homophily distribution, we conducted an analysis and visualized the results in Figure 3. The visualizations demonstrate that edges with the same homophily or heterophily labels can be well-clustered together. This indicates that the edge-level homophily representation adeptly reflects the type of edge homophily, forming a robust foundation for aggregation to represent the contextual homophily distribution of each node.\n\n*For the second question: Where does it differ from GCN aggregation?*\n\nOur backbone GNN model, as outlined in Eq.(4), diverges from the generic GCN model. Typically, GCNs utilize the fundamental operation of neighborhood aggregation to consolidate information from neighboring nodes and generate the representation of the target node. In contrast, our backbone GNN model emphasizes edge-level representation learning by modeling the homophily distribution of each edge for aggregation to generate node representations. Therefore, relying on the intuition of distinctions in homophily distribution, as discussed in the Introduction (Figure 1(b)), our model places greater emphasis on edge-level homophily representation to address the classification of normal and anomalous nodes. Furthermore, Tables 1 and 2 in the Experiments consistently demonstrate that our backbone GNN model outperforms the GCN model, underscoring its effectiveness in handling anomaly detection on graphs."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343653789,
                "cdate": 1700343653789,
                "tmdate": 1700343653789,
                "mdate": 1700343653789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y7hhC23MSN",
                "forum": "elMKXvhhQ9",
                "replyto": "7qs6BpkZpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**2. As you said, GAD has low homophily distribution, whether the homophily-aware neighbourhood aggregation you propose will be effective for abnormal nodes, I deeply doubt it.**\n\n\nWe sincerely apologize for any confusion that may have arisen, and we appreciate the opportunity to clarify this aspect of our work. In our study, we use the term \"homophily\" as a conceptual label, and it is important to note that \"heterophily\" could be interchangeably used in this context. Our model's primary goal with the homophily-aware neighborhood aggregation is to differentiate between the distinct distributions of homophily (or equivalently, heterophily) that are observed in the edges surrounding normal and anomalous nodes. Specifically, edges in the neighborhood of normal nodes typically exhibit high homophily (or low heterophily), whereas those around anomalous nodes tend to show low homophily (or high heterophily). The cross-entropy loss function in our model facilitates inverse predictions for normal and anomalous nodes. This means that the loss function strategically guides the edge representations surrounding a normal node to diverge from those around an anomalous node, aligning with their distinct homophily (or heterophily) distributions. Consequently, edges with high homophily around normal nodes share similar representations, while those with low homophily around anomalous nodes form analogous representations. This approach is critical for enhancing the discriminatory power of our model between normal and anomalous nodes, aligning with the core objectives of our research.\n\n**3. I don't find the definition of Vhq in Section 2, what is the meaning of Vun?**\n\nThank you for highlighting the need for clarity regarding the definition of $V_{hq}$ and $V_{un}$ in our paper. We appreciate the opportunity to elucidate this aspect of our work. $V_{un}$ is defined as the set of unlabeled nodes.\nIn the paragraph preceding Eq.(3), $V_{hq}$ represents the set of high-quality nodes, defined as $V_{hq}=\\\\{v \\mid v\\in V_{un} \\land \\exists \\mathbf{p}_v[k]\\ge\\tau\\\\}$. This set comprises unlabeled nodes for which there exists at least one predicted score in their predictions greater than the threshold $\\tau$. We have clarified this definition in our paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343706275,
                "cdate": 1700343706275,
                "tmdate": 1700344149875,
                "mdate": 1700344149875,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "118ajYjDLh",
                "forum": "elMKXvhhQ9",
                "replyto": "7qs6BpkZpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**4. Please compare your learnable augmentation with other augmentation methods.**\n\nThank you for your insightful suggestion.\nIn Appendix E.4, we benchmark our learnable data augmentation against several stochastic augmentation methods. Specifically, we substitute our augmentation module with widely-recognized techniques in the consistency training framework, including *DropNode* (Feng et al., 2020a), *DropEdge* (Rong et al., 2020), and *Dropout* (Srivastava et al., 2014). We administer Dropout to both input features, denoted as *DropInput*, and intermediate features, referred to as *DropHidden*. It is noteworthy that DropHidden serves as a non-learnable analogue to our augmentation module. For a fair comparison, we determine the optimal drop rate for each method within the interval of $(0, 0.5]$, with a step size of $0.1$. We list the optimal drop rate in Table 6. For your ease of reference, we also reported these optimal rates in the table below.\n\n| Methods| Amazon | YelpChi | T-Finance | T-Social |\n| --- | --- | --- | --- | --- |\n| ConsisGAD (DropInput) | 0.1 | 0.1 | 0.1 | 0.4 |\n| ConsisGAD (DropNode) | 0.2 | 0.1 | 0.1 | 0.5 |\n| ConsisGAD (DropEdge) | 0.2 | 0.1 | 0.1 | 0.3 |\n| ConsisGAD (DropHidden) | 0.3 | 0.1 | 0.2 | 0.1 |\n\nThe outcomes, depicted in Figure 9 through the three metrics, underscore the preeminence of our proposed learnable data augmentation over traditional stochastic alternatives.\nA salient observation is that among conventional augmentation methods, employing DropHidden in consistency training usually yields more substantial and stable performance improvements across all datasets. This empirical insight substantiates our decision to integrate learnable data augmentation at intermediate states and suggests the potential exploration of applying such augmentation to graph topology (Xia et al., 2022). For your easy reference, we also summarize the numerical results in the following table.\n| Dataset    | Metric   | ConsisGAD | ConsisGAD (DropInput) | ConsisGAD (DropNode) | ConsisGAD (DropEdge) | ConsisGAD (DropHidden) |\n| --- | --- | --- | --- | --- | --- | --- |\n| Amazon     | AUROC    | **93.91\u00b10.58** | 89.16\u00b10.53 | 91.47\u00b10.73 | 92.63\u00b10.45 | 92.71\u00b10.38 |\n|            | AUPRC    | **83.33\u00b10.34** | 74.01\u00b11.30 | 77.84\u00b11.03 | 80.22\u00b10.89 | 81.03\u00b11.00 |\n|            | Macro F1 | **90.03\u00b10.53** | 86.22\u00b10.93 | 85.58\u00b10.68 | 88.03\u00b10.81 | 87.78\u00b10.91 |\n| YelpChi    | AUROC    | **83.36\u00b10.53** | 81.68\u00b10.21 | 79.85\u00b10.26 | 80.70\u00b10.47 | 80.27\u00b10.39 |\n|            | AUPRC    | **47.33\u00b10.58** | 42.14\u00b10.52 | 40.14\u00b10.67 | 43.54\u00b10.27 | 43.50\u00b10.21 |\n|            | Macro F1 | **69.72\u00b10.30** | 68.22\u00b10.37 | 66.31\u00b10.47 | 67.34\u00b10.29 | 67.32\u00b10.44 |\n| T-Finance  | AUROC    | **95.33\u00b10.30** | 95.17\u00b10.23 | 95.07\u00b10.40 | 95.13\u00b10.49 | 94.58\u00b10.45 |\n|            | AUPRC    | **86.63\u00b10.44** | 84.87\u00b10.61 | 84.78\u00b10.31 | 85.43\u00b10.47 | 85.59\u00b10.43 |\n|            | Macro F1 | **90.97\u00b10.63** | 90.16\u00b10.33 | 90.02\u00b10.66 | 90.16\u00b11.02 | 90.31\u00b11.05 |\n| T-Social   | AUROC    | **94.31\u00b10.20** | 94.23\u00b10.16 | 94.04\u00b10.34 | 94.10\u00b10.25 | 94.13\u00b10.32 |\n|            | AUPRC    | **58.38\u00b12.10** | 55.32\u00b12.00 | 54.00\u00b12.11 | 55.33\u00b12.39 | 53.58\u00b12.41 |\n|            | Macro F1 | **78.08\u00b10.54** | 76.86\u00b10.98 | 76.35\u00b10.71 | 77.75\u00b10.57 | 77.48\u00b11.03 |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344220640,
                "cdate": 1700344220640,
                "tmdate": 1700344220640,
                "mdate": 1700344220640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uZy0MlRkKx",
                "forum": "elMKXvhhQ9",
                "replyto": "7qs6BpkZpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**5. Please give complexity analysis.**\n\nAs illustrated in Algorithm 1, our model is composed of two primary components: consistency training and the training of the learnable data augmentation module. These components operate iteratively in each iteration of the process. This approach potentially incurs a higher computational cost compared to the standard training procedures of graph neural networks. In this part, we give a detailed complexity analysis and analyze the possibility of using it on large graphs.\n\nAt the outset, we focus on analyzing the complexity of our backbone GNN model, as outlined in Equation (4). This model is a critical component in Algorithm 1 for calculating node embeddings. Considering a target node $v$, in the first GNN layer, the function $\\delta(\\cdot,\\cdot;\\theta_{\\delta})$ is implemented as $\\text{MLP}(\\mathbf{h}^{l-1}_v||\\mathbf{h}^{l-1}_u)$. A typical example of this MLP is $\\sigma(\\mathbf{W}(\\mathbf{h}^{l-1}_v \\|\\| \\mathbf{h}^{l-1}_u)+\\mathbf{b})$. The computational complexity in this first layer is $O(2dd_X+\\bar{N}d+2d)$, where $d_X$ represents the dimension of the input feature vector, $d$ the intermediate dimension of the embeddings, and $\\bar{N}$ the average node degree. Each subsequent layer contributes an additional complexity of $O(2d^2+\\bar{N}d+2d)$. Given $L$ total GNN layers, the overall complexity of the GNN backbone sums up to $O(2dd_X+\\bar{N}d+2d+(L-1)(2d^2+\\bar{N}d+2d))=O(2dd_X+2Ld^2+2Ld+L\\bar{N}d-2d^2)$.\nIn every iteration of Algorithm 1, we sample a batch of labeled and unlabeled nodes for subsequent computations. These batches are of sizes $B$ (for labeled nodes) and $\\mu B$ (for unlabeled nodes), respectively. We can split the whole process into three step as follows.\n- We begin by selecting high-quality nodes from the sampled batch of unlabeled nodes. In line 6, the GNN backbone introduces a complexity of $O(2dd_X+2Ld^2+2Ld+L\\bar{N}d-2d^2)$, as previously analyzed. The prediction step in line 7 has a complexity of $O(Kd+2K)$, where $K$ is the number of classes. Lines 8-10 involve checking the high-quality nodes with a complexity of $K$. Overall, for a batch size of $B$, lines 5-10 collectively result in a complexity of $O(\\mu B(2dd_X+2Ld^2+2Ld+L\\bar{N}d-2d^2+Kd+3K))$.\n- Each high-quality node is augmented and predicted, forming the basis for the consistency and diversity loss calculations. For line 12, the complexity of the Sharpen function in Algorithm 2 involves $O(8\\lfloor \\xi d \\rfloor d+d)$ across its steps. Consequently, line 12 in Algorithm 1 incurs a complexity of $O(8\\lfloor \\xi d \\rfloor d+d^2+3d)$. Line 13, similar to line 7, involves a complexity of $O(Kd+2K)$. Line 14 includes forming both the consistency and diversity losses, with complexities of $O(\\mu BK^2)$ and $O(2\\mu Bd)$, respectively. Summing up, lines 11-14 entail a complexity of $O(\\mu B(8\\lfloor \\xi d \\rfloor d+d^2+3d + Kd+2K) + \\mu BK^2 + 2\\mu Bd)$.\n- The consistency training involves complexity calculations similar to the earlier sections. Lines 17 and 18, as previously analyzed, involve a complexity of $O(2dd_X+2Ld^2+2Ld+L\\bar{N}d-2d^2+Kd+2K)$. Lines 19 and 20, pertaining to the loss calculations, have complexities of $O(BK^2)$ and $O(\\mu BK^2)$, respectively. Thus, the total complexity for lines 16-20 over $B$ iterations is $O(B(2dd_X+2Ld^2+2Ld+L\\bar{N}d-2d^2+Kd+2K)+BK^2+\\mu BK^2)$.\n\nIn each iteration of Algorithm 1, we aggregate the complexities from the previous sections to derive the overall complexity. This can be expressed as $O(2B(\\mu+1)dd_X + B(2\\mu L+2L-\\mu-2)d^2 + (2\\mu BL+\\mu BL\\bar{N}+\\mu BK+8\\mu B\\lfloor \\xi d \\rfloor+5\\mu B+K\\mu B+2BL+BL\\bar{N}+BK)d + 5\\mu BK+2\\mu BK^2+2BK+BK^2)$.\n\nParticularly in our anomaly detection scenario, where typically $K = 2$ due to the binary classification nature, and $L$ generally ranges from 1 to 3 as the number of GNN layers, the complexity of our model is predominantly influenced by the feature dimension $d_X$, the hidden dimension $d$, and the average node degree $\\bar{N}$. This indicates that with appropriately set hyper-parameters, our model shows promising potential for application on large-scale graphs. To enhance the clarity of our paper, we have included this detailed complexity analysis in Appendix A of our revised manuscript.\n\n\nWe hope that these clarifications and additional details address your concerns and strengthen the paper's contributions. Your insightful feedback has been instrumental in refining the quality and coherence of our manuscript. Should there be any more comments or issues from your side, we remain open and willing to make the requisite modifications."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700344806726,
                "cdate": 1700344806726,
                "tmdate": 1700344806726,
                "mdate": 1700344806726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GdjJZJgprY",
                "forum": "elMKXvhhQ9",
                "replyto": "7qs6BpkZpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer tbTo,\n\nThank you once again for your insightful feedback on our submission. We would like to remind you that the discussion period is concluding. To facilitate your review, we have provided a concise summary below, outlining our responses to each of your concerns:\n\n- **Part 1**: We provided further details on how our homophily-aware neighborhood aggregation (Eq.4) achieves edge-level homophily representation and explained its distinctions from GCN aggregation.\n- **Part 2**: We offered additional clarifications on our homophily-aware neighborhood aggregation.\n- **Part 3**: We clarified the definitions of $V_{hq}$ and $V_{un}$.\n- **Part 4**: We conducted a comparative analysis of our learnable data augmentation against four stochastic augmentation methods for graph anomaly detection.\n- **Part 5**: We presented a detailed complexity analysis of our algorithm.\n\nWe are grateful for your insightful comments and are eager to confirm whether our responses have adequately addressed your concerns. We look forward to any additional input you may provide.\n\nWarm regards, \\\nThe Authors of Submission 9315."
                    },
                    "title": {
                        "value": "A Kind Reminder to Reviewer tbTo"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567358692,
                "cdate": 1700567358692,
                "tmdate": 1700567639677,
                "mdate": 1700567639677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZzWw7rvL59",
                "forum": "elMKXvhhQ9",
                "replyto": "7qs6BpkZpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Further Kind Reminder to Reviewer tbTo"
                    },
                    "comment": {
                        "value": "Thanks a lot for your time in reviewing and insightful comments. We sincerely understand you are busy. But since the discussion due is approaching, would you mind checking the response to confirm where you have any further questions?\n\nWe are looking forward to your reply and happy to answer your further questions.\n\nWarm regards, \\\nThe Authors of Submission 9315"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703824482,
                "cdate": 1700703824482,
                "tmdate": 1700703849462,
                "mdate": 1700703849462,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4lYXOAmO5O",
            "forum": "elMKXvhhQ9",
            "replyto": "elMKXvhhQ9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9315/Reviewer_RZbU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9315/Reviewer_RZbU"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the challenge of graph anomaly detection with limited supervision. To address this, the authors introduce ConsisGAD, a novel model rooted in consistency training. ConsisGAD leverages variance in homophily distribution between normal and anomalous nodes to craft an effective GNN backbone. The authors also propose a learnable data augmentation mechanism for consistency training using unlabeled data. The paper presents comprehensive experiments on benchmark datasets to validate the effectiveness of the proposed model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-structured and presented in a clear manner. It effectively communicates the motivation behind the proposed model, making it accessible to readers. \n\n2. The proposed model, comprising homophily-aware neighborhood aggregation and consistency training with learnable data augmentation, is innovative. The homophily-aware aggregation method is straightforward yet effective, especially for edge-level homophily representation, enhancing the anomaly detection process. The proposed label consistency and distribution diversity concepts are intriguing, guiding the optimization of learnable data augmentation.\n\n3. The paper demonstrates a thorough evaluation through extensive experiments, which strengthens the empirical foundation and validates the performance across different datasets."
                },
                "weaknesses": {
                    "value": "Weaknesses:\n\n1. The paper should acknowledge related works that are pertinent to the proposed learnable data augmentation, such as [a] and [b]. It is crucial to cite and discuss the distinctions between these works and the proposed approach, providing readers with a clear understanding of the novel contributions made by this study.\n\n2. The paper predominantly explores the concept of applying learnable data augmentation for graph anomaly detection. While this is valuable, investigating its applicability in broader graph learning tasks, such as node classification with contrastive learning, could significantly expand its scope. For example, how about its benefits to generic graph contrastive learning tasks, compared to existing contrastive techniques?\n\n3. While consistency training might usually be deployed on unlabeled data, I wonder if it would be beneficial to utilize labeled data for consistency training as well. Specifically, labeled data has exact labels, which might provide effective information for consistency training the model in dealing with the taks of graph anomaly detection.\n\n\n[a] Graph Contrastive Learning Automated. Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang. ICML 2021\n[b] Graph Contrastive Learning with Adaptive Augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang. WWW 2021."
                },
                "questions": {
                    "value": "Please see the Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9315/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9315/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9315/Reviewer_RZbU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9315/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699032805651,
            "cdate": 1699032805651,
            "tmdate": 1700750102765,
            "mdate": 1700750102765,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ILAVDg6J2T",
                "forum": "elMKXvhhQ9",
                "replyto": "4lYXOAmO5O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer RZbU,\n\nThank you for your insightful comments and constructive feedback on our submission 9315. We appreciate the time and effort you have invested in reviewing our work. Below, we address each of your comments to clarify and improve our manuscript:\n\n**1. The paper should acknowledge related works that are pertinent to the proposed learnable data augmentation, such as [a] and [b]. It is crucial to cite and discuss the distinctions between these works and the proposed approach, providing readers with a clear understanding of the novel contributions made by this study.**\n\nWe acknowledge the importance of referencing and discussing relevant related works. In Appendix F of our revised manuscript, we have included a discussion comparing our work with the studies of [a] and [b]. For your convenience, we summarize the distinctions between these two works and our proposed learnable data augmentation:\n\n- **JOint Augmentation Optimization (JOAO) [a]**: This work learns a sampling distribution to sample augmentation pairs for graph contrastive learning. It instantiates the learning process as a min-max optimization process, iteratively optimizing the GNN encoder and the learnable sampling distribution. Our work is different from JOAO in two aspects:\n  - **Evaluation of augmentation quality**: JOAO is based on the adversarial training framework and assumes that a good augmentation will cause a high contrastive learning loss. Intuitively, increasing contrastive loss is to increase the distribution diversity of augmentations. However, the label consistency is overlooked, which may cause excessive noise injected into augmentations and lead to suboptimal performance, as indicated in prior studies [c,d]. In contrast, our work assumes that a good augmentation will increase the diversity of a node without changing its label (i.e., high label consistency). To this end, we formulate differentiable distribution diversity and label consistency metrics to evaluate augmentation quality and to guide the training of our augmentation module.\n  - **Learning objectives**: The learning objective of JOAO is to pick the optimal augmentation pair from a pre-defined pool of augmentation types for graph contrastive learning. It requires domain knowledge to manually construct and configure the augmentation pool for selection. By contrast, our work introduces a novel learnable augmentation module through learnable masking. Our module learns to synthesize node-specific augmentations, evolving dynamically with the dataset and GNN encoder. \n\n- **Graph Contrastive learning with Adaptive augmentation (GCA) [b]**: GCA introduces a general graph contrastive learning framework and generates augmented graphs by adaptively dropping edges and node features. The drop rates of edges and node features are based on node centrality scores, incorporating topological and semantic priors in a graph. The node centrality scores include degree centrality, eigenvector centrality, and PageRank centrality. Our work is different from GCA in two aspects:\n  - **Evaluation of augmentation quality**: GCA assumes that a good augmentation should drop unimportant edges and features while keeping important ones. The importance is measured through node centrality scores. In this way, edges whose ending nodes have low centrality scores are more likely to be dropped, and feature dimensions appearing less frequently on important nodes are more likely to be masked. However, this heuristic approach may not guarantee high consistency and diversity in augmentations. On the other hand, our proposed model does not rely on node importance metrics but focuses on maximizing distribution diversity and label consistency in augmentations.\n  - **Learning objectives**: The adaptive augmentation module in GCA, when presented with an input graph, remains fixed and non-learnable throughout the training process. This is due to the static nature of node centrality scores, resulting in unchanging dropping rates for edges and node features. Moreover, this module operates independently of the specific GNN encoder employed. On the contrary, our learnable augmentation module is dynamic, updating continuously throughout training to adapt to the dataset and the evolving GNN encoder."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343527893,
                "cdate": 1700343527893,
                "tmdate": 1700343527893,
                "mdate": 1700343527893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xEwmWFJkSm",
                "forum": "elMKXvhhQ9",
                "replyto": "4lYXOAmO5O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**2. The paper predominantly explores the concept of applying learnable data augmentation for graph anomaly detection. While this is valuable, investigating its applicability in broader graph learning tasks, such as node classification with contrastive learning, could significantly expand its scope. For example, how about its benefits to generic graph contrastive learning tasks, compared to existing contrastive techniques?**\n\nThank you for your insightful suggestion regarding the potential applicability of our learnable data augmentation in generic graph contrastive learning tasks. We acknowledge the value of this idea. However, our current model is not straightforwardly applicable to such tasks due to the specific requirements of our label consistency metric. In graph contrastive learning, which focuses on pre-training a GNN encoder without specific label information, our method faces challenges, as it depends on label predictions for unlabeled nodes to measure label consistency.\n\nOne potential approach to address this could involve clustering nodes in the input graph and then using these clusters as proxy labels to assess label consistency. Nevertheless, due to time constraints, we have not been able to explore this adaptation in the current study. We appreciate your suggestion and agree that expanding the applicability of our augmentation approach represents an exciting and valuable direction for future research. This would indeed enhance the versatility and impact of our learnable augmentation module and the proposed metrics for label consistency and distribution diversity. We are eager to explore this avenue in our future research, aiming to further broaden the scope of our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343568836,
                "cdate": 1700343568836,
                "tmdate": 1700343568836,
                "mdate": 1700343568836,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6n8pWUREIj",
                "forum": "elMKXvhhQ9",
                "replyto": "4lYXOAmO5O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**3. While consistency training might usually be deployed on unlabeled data, I wonder if it would be beneficial to utilize labeled data for consistency training as well. Specifically, labeled data has exact labels, which might provide effective information for consistency training the model in dealing with the task of graph anomaly detection.**\n\nThank you for highlighting the potential benefits of utilizing labeled data for consistency training. In our current settings, both labeled and unlabeled data are used for consistency training, but labeled samples are treated as unlabeled, using their predicted labels instead of exact labels.\n\nEncouraged by your suggestion, we explore the impact of using exact labels for labeled nodes in consistency training. The results of this additional experiment are detailed in Appendix E.9 of our revised manuscript. For your convenience, we provide a summary in the table below:\n\n| Dataset    | Metric   | ConsisGAD (Predicted Label) | ConsisGAD (Exact Label) |\n|------------|----------|-----------------------------|-------------------------|\n| Amazon     | AUROC    | 93.91\u00b10.58                  | 93.82\u00b10.55              |\n|            | AUPRC    | 83.33\u00b10.34                  | 83.16\u00b10.46              |\n|            | Macro F1 | 90.03\u00b10.53                  | 89.88\u00b10.30              |\n| YelpChi    | AUROC    | 83.36\u00b10.53                  | 82.91\u00b10.36              |\n|            | AUPRC    | 47.33\u00b10.58                  | 47.23\u00b10.98              |\n|            | Macro F1 | 69.72\u00b10.30                  | 69.44\u00b10.29              |\n| T-Finance  | AUROC    | 95.33\u00b10.30                  | 95.09\u00b10.27              |\n|            | AUPRC    | 86.63\u00b10.44                  | 86.94\u00b10.13              |\n|            | Macro F1 | 90.97\u00b10.63                  | 91.32\u00b10.18              |\n| T-Social   | AUROC    | 94.31\u00b10.20                  | 93.87\u00b10.30              |\n|            | AUPRC    | 58.38\u00b12.10                  | 56.54\u00b11.86              |\n|            | Macro F1 | 78.08\u00b10.54                  | 77.42\u00b10.67              |\n\nOur findings indicate that using exact labels in consistency training does not significantly improve performance on most datasets. This lack of improvement may stem from the fact that labeled data has already been utilized in the cross-entropy loss, and their reuse in the consistency training does not contribute significantly to performance enhancement. Notably, a decrease in performance is observed in the T-Social dataset. We hypothesize this could be attributed to the model overfitting to label noise, which could be mitigated by the use of predicted labels.\n\nWe hope these revisions to our submission adequately address your concerns and enhance its overall contributions. Your thoughtful feedback has been crucial in elevating the quality and coherence of our manuscript. Should you have additional comments or concerns, we remain receptive and prepared to undertake any further modifications.\n\n[a] Graph Contrastive Learning Automated. Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang. ICML 2021.\n\n[b] Graph Contrastive Learning with Adaptive Augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang. WWW 2021.\n\n[c] Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets. Yogesh Balaji, Tom Goldstein, and Judy Hoffman. arXiv preprint arXiv:1910.08051, 2019.\n\n[d] Robustness May Be at Odds with Accuracy. Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. ICLR, 2018."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343603442,
                "cdate": 1700343603442,
                "tmdate": 1700343603442,
                "mdate": 1700343603442,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fXPWtxr6qD",
                "forum": "elMKXvhhQ9",
                "replyto": "4lYXOAmO5O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer RZbU,\n\nThank you once again for your insightful feedback on our submission. We would like to remind you that the discussion period is concluding. To facilitate your review, we have provided a concise summary below, outlining our responses to each of your concerns:\n\n- **Part 1**: We elucidated the distinctions between our learnable data augmentation and other automated data augmentation methods, such as JOAO and GCA.\n- **Part 2**: We explored the application of our learnable augmentation module in generic graph contrastive learning tasks.\n- **Part 3**: We conducted further analysis on the effectiveness of utilizing the exact labels of labeled nodes in consistency training.\n\nWe are grateful for your insightful comments and are eager to confirm whether our responses have adequately addressed your concerns. We look forward to any additional input you may provide.\n\nWarm regards, \\\nThe Authors of Submission 9315."
                    },
                    "title": {
                        "value": "A Kind Reminder to Reviewer RZbU"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567303835,
                "cdate": 1700567303835,
                "tmdate": 1700567598416,
                "mdate": 1700567598416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DAthg10twp",
                "forum": "elMKXvhhQ9",
                "replyto": "4lYXOAmO5O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Further Kind Reminder to Reviewer RZbU"
                    },
                    "comment": {
                        "value": "Thanks a lot for your time in reviewing and insightful comments. We sincerely understand you are busy. But since the discussion due is approaching, would you mind checking the response to confirm where you have any further questions?\n\nWe are looking forward to your reply and happy to answer your further questions.\n\nWarm regards, \\\nThe Authors of Submission 9315"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703722770,
                "cdate": 1700703722770,
                "tmdate": 1700703867576,
                "mdate": 1700703867576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "quyBPeaSTj",
            "forum": "elMKXvhhQ9",
            "replyto": "elMKXvhhQ9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9315/Reviewer_ZNLh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9315/Reviewer_ZNLh"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors tackle the problem of anomaly detection in graphs by introducing a novel approach---consistency training coupled with a learnable data augmentation mechanism. Their method involves leveraging a simple GNN backbone that emphasizes edge-level homophily representation for anomaly detection. By exploiting disparities in homophily distribution, the proposed model achieves enhanced anomaly detection capabilities. Additionally, the authors introduce two evaluation metrics designed to optimize their innovative learnable data augmentation technique, guiding its effectiveness in data synthesis. In their experimental evaluation, the authors assess the performance of their model across four benchmark datasets commonly used for graph-based anomaly detection, along with a private dataset. The proposed model generally outperforms existing state-of-the-art baselines across three key evaluation metrics."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. I find the proposed learnable data augmentation mechanism intriguing. Intuitively, this approach appears effective, as it can be finely tuned through the optimization of two core metrics: label consistency and data distribution. Through iterative optimization of these components---the GNN module and the learnable data augmentation module---it seems both elements can be well-trained, enhancing the overall performance of the model.\n\n2. The node clusters displayed in Figure 3 provide a compelling visual representation of the effectiveness of the proposed GNN backbone. The well-separated node clusters, especially concerning different edge relations, serve as a strong validation of the necessity of leveraging distinctions in homophily distribution for anomaly detection. This visualization underscores the robustness of the proposed method.\n\n3. The paper is skillfully composed, ensuring clarity and coherence in conveying the research concepts. Additionally, the experiments conducted are generally comprehensive, offering substantial evidence to support the effectiveness of the proposed model."
                },
                "weaknesses": {
                    "value": "1. Regarding consistency training, the proposed model employs thresholds to identify \"high-quality nodes\" from the unlabeled data for training. This step, akin to semi-supervised learning, assigns synthetic labels to unlabeled data for consistency training. The performance of these \"high-quality nodes\" is pivotal for downstream iterative training, emphasizing the significance of their accuracy. Hence, the authors should analyze the actual quality of these \"high-quality nodes\" and the accuracy of their predicted labels. This aspect holds significant importance in evaluating the reliability and effectiveness of the model.\n\n2. The paper introduces essential hyperparameters, including $\\alpha$ and $\\xi$. While the authors provide their specific settings in the experiments, the impact of these parameters on the performance of the model remains unexplored. The authors should perform a sensitivity analysis for the crucial hyperparameters in the proposed model.\n\n3. The proposed GNN backbone shows promise for anomaly detection by capturing distinctions in the homophily distribution of nodes in graphs. However, it raises intriguing questions about its performance on graphs with varying levels of heterophily. Specifically, how well does the proposed GNN backbone perform on graphs with low or high heterophily when applied to generic multi-class node classification tasks?"
                },
                "questions": {
                    "value": "1. How about the correctness of the \"high-quality nodes\"?\n\n2. The authors should perform a sensitivity analysis for the crucial hyperparameters in the proposed model.\n\n3. Can the proposed GNN backbone effectively handle generic multi-class node classification tasks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9315/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9315/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9315/Reviewer_ZNLh"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9315/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699144275704,
            "cdate": 1699144275704,
            "tmdate": 1700574329237,
            "mdate": 1700574329237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pswQIlyRi3",
                "forum": "elMKXvhhQ9",
                "replyto": "quyBPeaSTj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and insightful comments on our submission. We appreciate the time you invested in evaluating our work and offering constructive feedback. Below, we address your comments to clarify and improve our manuscript.\n\n**1. How about the correctness of the \"high-quality nodes\"?**\n\nThanks for highlighting the significance of evaluating the quality of \"high-quality nodes\" in our consistency training approach. Considering your valuable suggestion, we have conducted a detailed experiment to assess this aspect and include the results in Appendix E.6 of our revised manuscript. In this experiment, we visualize Macro F1 scores of \"high-quality nodes\" at each epoch during training and compare these with scores from the test set. For your reference, we report the scores with an interval of 10 epochs in the following table. For more details, please kindly refer to the Appendix E.6.\n\n|                    | Epoch 10 | Epoch 20 | Epoch 30 | Epoch 40 | Epoch 50 | Epoch 60 | Epoch 70 | Epoch 80 | Epoch 90 | Epoch 100 |\n| :----------------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: | :-------: |\n|       Amazon       |          |          |          |          |          |          |          |          |          |           |\n| High-quality Nodes |  90.79   |  88.89   |  91.06   |  92.18   |  90.91   |  90.76   |  90.76   |  91.43   |  89.99   |   89.29   |\n|     Test Nodes     |  87.56   |  88.13   |  90.10   |  90.96   |  90.98   |  90.92   |  90.04   |  90.60   |  90.07   |   90.43   |\n|       Diff.        |   3.23   |   0.76   |   0.96   |   1.23   |  -0.07   |  -0.17   |   0.72   |   0.83   |  -0.08   |   -1.14   |\n|      YelpChi       |          |          |          |          |          |          |          |          |          |           |\n| High-quality Nodes |  67.23   |  69.23   |  68.09   |  69.11   |  68.49   |  68.98   |  69.21   |  70.02   |  69.28   |   68.76   |\n|     Test Nodes     |  66.72   |  67.56   |  68.49   |  67.76   |  68.13   |  68.12   |  67.42   |  67.96   |  68.25   |   68.42   |\n|       Diff.        |   0.51   |   1.68   |  -0.40   |   1.35   |   0.36   |   0.86   |   1.79   |   2.06   |   1.03   |   0.33    |\n|     T-Finance      |          |          |          |          |          |          |          |          |          |           |\n| High-quality Nodes |  90.82   |  90.27   |  90.15   |  90.12   |  88.50   |  91.33   |  91.11   |  91.48   |  91.14   |   91.09   |\n|     Test Nodes     |  89.28   |  90.22   |  89.39   |  90.44   |  91.16   |  91.05   |  90.90   |  90.87   |  90.62   |   90.39   |\n|       Diff.        |   1.54   |   0.05   |   0.76   |  -0.31   |  -2.66   |   0.28   |   0.21   |   0.62   |   0.52   |   0.71    |\n|      T-Social      |          |          |          |          |          |          |          |          |          |           |\n| High-quality Nodes |  75.68   |  76.35   |  76.92   |  75.82   |  76.09   |  77.53   |  75.48   |  74.07   |  76.11   |   73.59   |\n|     Test Nodes     |  72.14   |  73.56   |  76.07   |  74.17   |  74.23   |  76.10   |  73.13   |  71.52   |  73.81   |   72.58   |\n|       Diff.        |   3.53   |   2.80   |   0.85   |   1.65   |   1.87   |   1.43   |   2.35   |   2.54   |   2.30   |   1.01    |\n\nThe result demonstrates that \"high-quality nodes\" consistently achieve higher Macro F1 scores than test nodes across various datasets. On Amazon, YelpChi, T-Finance, and T-Social, the average performance improvement of \"high-quality nodes\" over test nodes is 1.01%, 0.83%, 0.71%, and 0.64%, respectively. Generally, as depicted in Figure 10, both \"high-quality nodes\" and test nodes exhibit a gradual increase in performance, with the former driving the improvement of the latter. An exception lies in the T-Social dataset, on which we observe some performance fluctuations. This is possibly due to the much smaller batch size and training data relative to the whole dataset size. Overall, our analysis showcases the accuracy of the predicted labels and the robustness of our selection criteria for \"high-quality nodes\"."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343160831,
                "cdate": 1700343160831,
                "tmdate": 1700345006390,
                "mdate": 1700345006390,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dzHagp9Q0c",
                "forum": "elMKXvhhQ9",
                "replyto": "quyBPeaSTj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**2. The authors should perform a sensitivity analysis for the crucial hyperparameters in the proposed model.**\n\nTaking your valuable advice, we have performed a sensitivity analysis on hyper-parameters and included the results in Appendix E.7 of our revised manuscript. We vary the weights of the label consistency loss $\\alpha$ among \\{0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0\\} and the drop ratio $\\xi$ among \\{0.0, 0.1, 0.2, 0.3, 0.4, 0.5\\}, and measure the corresponding model performance across the Amazon, YelpChi, and T-Finance datasets. For your ease of reference, we have summarized the results in the tables below. Additionally, we have included in Appendix E.7 a sensitivity analysis of the normal threshold $\\tau_n$ and anomalous threshold $\\tau_a$, providing a more comprehensive understanding of our model's behavior under different hyper-parameter settings.\n\n| $\\alpha$  |    0.1     |    0.2     |      0.5       |      1.0       |    2.0     |      5.0       |    10.0    |\n| :-------: | :--------: | :--------: | :------------: | :------------: | :--------: | :------------: | :--------: |\n|  Amazon   |            |            |                |                |            |                |            |\n|   AUROC   | 93.16\u00b10.56 | 93.55\u00b10.56 |   93.68\u00b10.63   | **93.91\u00b10.58** | 93.73\u00b10.54 |   92.89\u00b10.60   | 93.31\u00b10.58 |\n|   AUPRC   | 82.47\u00b10.48 | 82.38\u00b10.35 |   82.76\u00b10.51   | **83.33\u00b10.34** | 83.13\u00b10.69 |   80.37\u00b10.84   | 79.21\u00b11.04 |\n| Macro F1  | 89.50\u00b10.81 | 89.27\u00b10.66 |   89.03\u00b10.69   | **90.03\u00b10.53** | 89.77\u00b10.27 |   89.56\u00b10.40   | 88.65\u00b10.65 |\n|  YelpChi  |            |            |                |                |            |                |            |\n|   AUROC   | 81.86\u00b10.25 | 81.76\u00b10.41 | **83.36\u00b10.53** |   82.16\u00b10.63   | 82.94\u00b10.62 |   82.01\u00b10.81   | 81.34\u00b10.50 |\n|   AUPRC   | 45.33\u00b10.33 | 45.00\u00b10.52 | **47.33\u00b10.58** |   45.49\u00b11.08   | 46.27\u00b10.82 |   44.88\u00b11.19   | 43.47\u00b10.26 |\n| Macro F1  | 68.73\u00b10.33 | 68.63\u00b10.52 | **69.72\u00b10.30** |   68.81\u00b10.37   | 69.11\u00b10.50 |   68.55\u00b10.43   | 68.25\u00b10.36 |\n| T-Finance |            |            |                |                |            |                |            |\n|   AUROC   | 95.06\u00b10.31 | 95.10\u00b10.22 |   95.15\u00b10.48   |   95.01\u00b10.40   | 95.26\u00b10.17 | **95.33\u00b10.30** | 95.13\u00b10.45 |\n|   AUPRC   | 84.77\u00b10.47 | 85.37\u00b10.37 |   85.71\u00b10.77   |   85.88\u00b11.23   | 86.10\u00b10.75 | **86.63\u00b10.44** | 83.87\u00b10.39 |\n| Macro F1  | 89.57\u00b10.68 | 89.74\u00b10.41 |   90.29\u00b10.69   |   90.93\u00b10.55   | 90.74\u00b10.68 | **90.97\u00b10.63** | 89.17\u00b10.84 |\n\n\n|   $\\xi$   |    0.0     |      0.1       |      0.2       |      0.3       |    0.4     |    0.5     |\n| :-------: | :--------: | :------------: | :------------: | :------------: | :--------: | :--------: |\n|  Amazon   |            |                |                |                |            |            |\n|   AUROC   | 92.07\u00b10.29 |   92.93\u00b10.15   |   93.54\u00b10.47   | **93.91\u00b10.58** | 93.05\u00b10.82 | 92.43\u00b11.13 |\n|   AUPRC   | 78.79\u00b10.42 |   80.20\u00b10.74   |   81.58\u00b10.31   | **83.33\u00b10.34** | 81.66\u00b11.33 | 80.46\u00b11.82 |\n| Macro F1  | 87.25\u00b11.21 |   87.26\u00b10.99   |   88.93\u00b11.15   | **90.03\u00b10.53** | 89.38\u00b10.90 | 89.48\u00b10.87 |\n|  YelpChi  |            |                |                |                |            |            |\n|   AUROC   | 80.36\u00b10.62 | **83.36\u00b10.53** |   82.45\u00b10.65   |   81.93\u00b10.83   | 81.66\u00b10.63 | 81.82\u00b11.02 |\n|   AUPRC   | 42.60\u00b11.18 | **47.33\u00b10.58** |   45.47\u00b11.12   |   44.59\u00b10.99   | 43.80\u00b10.65 | 43.65\u00b11.28 |\n| Macro F1  | 67.22\u00b10.76 | **69.72\u00b10.30** |   68.89\u00b10.66   |   68.21\u00b10.72   | 67.87\u00b10.27 | 68.10\u00b10.58 |\n| T-Finance |            |                |                |                |            |            |\n|   AUROC   | 95.18\u00b10.56 |   95.24\u00b10.27   | **95.33\u00b10.30** |   95.14\u00b10.33   | 95.13\u00b10.39 | 95.20\u00b10.24 |\n|   AUPRC   | 83.57\u00b10.64 |   86.55\u00b10.82   | **86.63\u00b10.44** |   86.39\u00b10.88   | 85.29\u00b10.71 | 85.10\u00b10.73 |\n| Macro F1  | 88.79\u00b11.21 |   90.89\u00b10.73   | **90.97\u00b10.63** |   90.73\u00b11.03   | 89.69\u00b11.14 | 88.98\u00b11.09 |\n\nFor the weight of the label consistency loss $\\alpha$, our results indicate that an $\\alpha$ value of around 1.0 yields optimal performance for the Amazon and YelpChi datasets, whereas a value of around 5.0 is preferable for the T-Finance dataset. Overall, the model performance remains stable with a moderate $\\alpha$ value. If $\\alpha$ is set to an excessively high value, the performance will be greatly impaired. This is likely due to the diminished diversity in generated augmentations, which is crucial for effective consistency training. \n\nRegarding the drop ratio $\\xi$, our results reveal that maintaining $\\xi$ within a range of 0.1 to 0.3 offers consistent benefits across all datasets. A smaller $\\xi$ reduces ConsisGAD to its backbone model, limiting its ability to harness unlabeled data effectively. Conversely, a higher $\\xi$ could lead to significant information loss, making it hard for the model to discern valuable patterns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343240427,
                "cdate": 1700343240427,
                "tmdate": 1700343240427,
                "mdate": 1700343240427,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xAVY6fyVhN",
                "forum": "elMKXvhhQ9",
                "replyto": "quyBPeaSTj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**3. Can the proposed GNN backbone effectively handle generic multi-class node classification tasks?**\nPursuant to your advice, we have extended our experiments to include graphs with low and high heterophily and discussed the results in Appendix E.8 of our revised manuscript. In this experiment, we adhere to the experimental procedure of a recent work, the Feature Selection Graph Neural Network (FSGNN) [a], and use their public repository for experimentation. Our evaluation encompasses three homophily graphs and six heterophily graphs, with homophily ratios ranging from 0.11 to 0.81. This range provides a comprehensive spectrum for performance assessment of our model. For your easy reference, we list the dataset statistics in the following table:\n\n|                |      Cora      |  Citeseer  |     Pubmed     |   Chameleon    |   Wisconsin    |     Texas      |    Cornell     |    Squirrel    |     Actor      | \n| :------------- | :------------: | :--------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: |\n| Hom. ratio     |      0.81      |    0.74    |      0.80      |      0.23      |      0.21      |      0.11      |      0.30      |      0.22      |      0.22      |\n| \\# Nodes       |     2,708      |   3,327    |     19,717     |     2,277      |      251       |      183       |      183       |     5,201      |     7,600      |\n| \\# Edges       |     5,278      |   4,732    |     44,338     |     36,101     |      499       |      309       |      295       |    198,353     |     26,659     |\n| \\# Features    |     1,433      |   3,703    |      500       |     2,325      |     1,703      |     1,703      |     1,703      |     2,089      |      932       |\n| \\# Classes     |       7        |     6      |       3        |       4        |       5        |       5        |       5        |       5        |       5        |\n\nWe maintain a consistent architecture for our GNN backbone, setting it to two layers, and conduct fine-tuning for learning rate and weight decay on each dataset. The model is evaluated in ten publicly available data splits, and the mean test accuracy is reported. For your convenience, we list the results as below:\n\n|                |      Cora      |  Citeseer  |     Pubmed     |   Chameleon    |   Wisconsin    |     Texas      |    Cornell     |    Squirrel    |     Actor      | Mean Acc. |\n| :------------- | :------------: | :--------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :------------: | :-------: |\n| GCN            |   87.28\u00b11.26   | 76.68\u00b11.64 |   87.38\u00b10.66   |   59.82\u00b12.58   |   59.80\u00b16.99   |   59.46\u00b15.25   |   57.03\u00b14.67   |   36.89\u00b11.34   |   30.26\u00b10.79   |   61.62   |\n| GAT            |   82.68\u00b11.80   | 75.46\u00b11.72 |   84.68\u00b10.44   |   54.69\u00b11.95   |   55.29\u00b18.71   |   58.38\u00b14.45   |   58.92\u00b13.32   |   30.62\u00b12.11   |   26.28\u00b11.73   |   58.55   |\n| GraphSAGE      |   86.90\u00b11.04   | 76.04\u00b11.30 |   88.45\u00b10.50   |   58.73\u00b11.68   |   81.18\u00b15.56   |   82.43\u00b16.14   |   75.95\u00b15.01   |   41.61\u00b10.74   |   34.23\u00b10.99   |   69.50   |\n| Cheby+JK       |   85.49\u00b11.27   | 74.98\u00b11.18 |   89.07\u00b10.30   |   63.79\u00b12.27   |   82.55\u00b14.57   |   78.38\u00b16.37   |   74.59\u00b17.87   |   45.03\u00b11.73   |   35.14\u00b11.37   |   69.89   |\n| MixHop         |   87.61\u00b10.85   | 76.26\u00b11.33 |   85.31\u00b10.61   |   60.50\u00b12.53   |   75.88\u00b14.90   |   77.84\u00b17.73   |   73.51\u00b16.34   |   43.80\u00b11.48   |   32.22\u00b12.34   |   68.10   |\n| GEOM-GCN       |     85.27      | **77.99**  |     90.05      |     60.90      |     64.12      |     67.57      |     60.81      |     38.14      |     31.63      |   64.05   |\n| GCNII          |   88.01\u00b11.33   | 77.13\u00b11.38 | **90.30\u00b10.37** |   62.48\u00b12.74   |   81.57\u00b14.98   |   77.84\u00b15.64   |   76.49\u00b14.37   |      N/A       |      N/A       |     -     |\n| H2GCN-1        |   86.92\u00b11.37   | 77.07\u00b11.64 |   89.40\u00b10.34   |   57.11\u00b11.58   |   86.67\u00b14.69   |   84.86\u00b16.77   |   82.16\u00b14.80   |   36.42\u00b11.89   |   35.86\u00b11.03   |   70.71   |\n| WRGAT          |   88.20\u00b12.26   | 76.81\u00b11.89 |   88.52\u00b10.92   |   65.24\u00b10.87   |   86.98\u00b13.78   |   83.62\u00b15.50   |   81.62\u00b13.90   |   48.85\u00b10.78   |   36.53\u00b10.77   |   72.93   |\n| GPRGNN         | **88.49\u00b10.95** | 77.08\u00b11.63 |   88.99\u00b10.40   |   66.47\u00b12.47   |   85.88\u00b13.70   |   86.49\u00b14.83   |   81.89\u00b16.17   |   49.03\u00b11.28   |   36.04\u00b10.96   |   73.37   |\n| FSGNN          |   88.23\u00b11.17   | 77.40\u00b11.93 |   89.78\u00b10.38   | **78.95\u00b10.86** | **88.43\u00b13.22** | **87.57\u00b14.86** | **87.84\u00b16.19** | **74.10\u00b11.89** |   35.75\u00b10.96   | **78.67** |\n| CONSISGAD(GNN) |   86.32\u00b11.72   | 75.83\u00b11.81 |   89.39\u00b10.34   |   44.82\u00b12.96   |   86.47\u00b14.51   |   83.24\u00b15.77   |   83.51\u00b16.89   |   33.08\u00b10.79   | **37.38\u00b11.55** |   68.89   |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343364642,
                "cdate": 1700343364642,
                "tmdate": 1700343364642,
                "mdate": 1700343364642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WhxgXlINZh",
                "forum": "elMKXvhhQ9",
                "replyto": "quyBPeaSTj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "It is important to note that our proposed GNN backbone, ConsisGAD(GNN), is specifically tailored to exploit the homophily distribution differences between normal and anomalous nodes. Such a phenomenon is characteristic in the graph anomaly detection task (i.e., the imbalanced binary classification task) where normal nodes have high homophily distribution while anomalous nodes have low homophily distribution. This prominent distribution discrepancy lays down the foundation of our GNN backbone. When it comes to multi-class node classification tasks, the homophily-aware neighborhood aggregation (Equation (4)) in our GNN backbone will model the neighbor label distribution within a node's neighborhood. If this neighborhood label distribution exhibits distinguishable patterns for different classes, we would expect our backbone model to function well. \n\nFrom the results, we observe that our GNN backbone model performs comparably to existing baselines in the three homophily graphs. Our GNN backbone is built upon the Message-Passing Neural Network (MPNN) framework, which allows handling homophily information naturally. For heterophily graphs, our backbone model yields promising results in the Wisconsin, Texas, and Cornell datasets, but encounters challenges in the Chameleon, Squirrel, and Actor datasets. Our further investigations in Figure 15 reveal that, in Wisconsin, Texas, and Cornell, nodes of different classes have distinct neighbor label distribution within their neighborhood, which allows our backbone model to distinguish different classes effectively. However, such distinct patterns are absent in Chameleon, Squirrel, and Actor, explaining the struggle of our backbone model on them. Notably, despite a low accuracy on the Actor dataset, our model achieves a new state-of-the-art result, underscoring its potential in handling heterophily graphs in generic multi-class node classification tasks. We recognize the opportunity for further enhancements, particularly on datasets where distinct neighborhood patterns are less pronounced. Future research may explore synergies between our GNN backbone model and other advanced techniques to elevate performance on heterophily graphs.\n\nWe hope these revisions and additional analyses address your concerns and strengthen the contribution of our work. We appreciate the constructive guidance you have provided, which has substantially improved the manuscript. If you have additional comments or concerns, we welcome your input and are ready to make any necessary adjustments. \n\n[a] Simplifying approach to node classification in graph neural networks. Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Journal of Computational Science, 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700343403059,
                "cdate": 1700343403059,
                "tmdate": 1700343474059,
                "mdate": 1700343474059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1NsimUs0Mp",
                "forum": "elMKXvhhQ9",
                "replyto": "quyBPeaSTj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9315/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer ZNLh,\n\nThank you once again for your insightful feedback on our submission. We would like to remind you that the discussion period is concluding. To facilitate your review, we have provided a concise summary below, outlining our responses to each of your concerns:\n\n- **Part 1**: We conducted additional experiments to assess the quality of high-quality nodes. The results consistently demonstrated superior performance of high-quality nodes compared to test nodes.\n- **Part 2**: We conducted additional experiments to assess the sensitivity of our hyperparameters: the weight of the label consistency loss ($\\alpha$) and the drop ratio ($\\xi$).\n- **Part 3**: We evaluated our GNN backbone on a generic multi-class node classification task. The experiments comprised three homophily graphs and six heterophily graphs, with homophily ratios ranging from 0.11 to 0.81. Additionally, we conducted a detailed performance comparison analysis.\n\nWe are grateful for your insightful comments and are eager to confirm whether our responses have adequately addressed your concerns. We look forward to any additional input you may provide.\n\nWarm regards, \\\nThe Authors of Submission 9315."
                    },
                    "title": {
                        "value": "A Kind Reminder to Reviewer ZNLh"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9315/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567225436,
                "cdate": 1700567225436,
                "tmdate": 1700567613679,
                "mdate": 1700567613679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]