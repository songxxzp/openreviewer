[
    {
        "title": "An Invex Relaxation Approach for Minimizing Polarization from Fully and Partially Observed Initial Opinions"
    },
    {
        "review": {
            "id": "DnWiG3J9qd",
            "forum": "cI7WAadODh",
            "replyto": "cI7WAadODh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6576/Reviewer_f5dH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6576/Reviewer_f5dH"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an invex relaxation approach for minimizing polarization over a network. It is proved in Section 4 that many types of polarization all fall into the invex function class, whose local minimum is a global minimum. Then this paper proposes to use projected gradient descent to solve a relaxed problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written.\n2. Invexity is provably identified for many types of polarization. It shows that polarization minimization regardless of constraints is similar to convex optimization."
                },
                "weaknesses": {
                    "value": "My main concern is on the contribution of the relaxation and the framework to solve it.\n 1. The relaxation seems to be straightforward. It is standard in optimization to relax $\\ell_0$-norm into $\\ell_1$-norm. And I think it cannot be viewed as a contribution of this work. Other modifications, including replacing the adjacency matrix with Laplacian (then the variable in the loss function and in the constraint become the same), as well as relaxing the constraint from $\\le 2k$ to $\\le 4k$, are also very slight, from my point of view.\n2. What is the contribution of the proposed framework to solve this problem? It seems to be the use of projected gradient descent. But I think the projected gradient descent is also very standard in optimization. So what is the novelty of this method?  \n3. It is my first time to see polarization minimization. So my confidence is only 2."
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6576/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6576/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6576/Reviewer_f5dH"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6576/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697724179399,
            "cdate": 1697724179399,
            "tmdate": 1699636746690,
            "mdate": 1699636746690,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qfLqVd7Htv",
                "forum": "cI7WAadODh",
                "replyto": "DnWiG3J9qd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6576/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6576/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications"
                    },
                    "comment": {
                        "value": "Firstly, we thank the reviewer for taking their valuable time to provide feedback.\n\nRunning first-order optimization algorithms on non-convex problems only guarantees us a local minimum. Studying the properties of these local minima and proving an even stronger result that there exists a global minima improves our theoretical understanding of these problems. This, in turn, motivates the design of efficient algorithms for such problems.   There is a wide range of non-convex practical problems which have been studied in this context. These include, but are not limited to, principal components analysis, canonical correlation analysis, orthogonal tensor decomposition, phase retrieval, dictionary learning, matrix sensing, matrix completion, and other nonconvex low-rank problems. A few references are provided below:\n\nLee, J.D., Simchowitz, M., Jordan, M.I. and Recht, B., 2016, June. Gradient descent only converges to minimizers. In Conference on learning theory (pp. 1246-1257). PMLR.\n\nGe, R., Huang, F., Jin, C. and Yuan, Y., 2015, June. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In Conference on learning theory (pp. 797-842). PMLR.\n\nOur contribution here is similar in this aspect, where we theoretically demonstrate the characteristics of polarization function under the popular  FJ dynamics, which is believed to be non-convex. By showing that polarization (and the general class of $M \\rightarrow$ $s^TM^{-k}s$ functions) are invex, a special class of non-convex functions, we ensure there are no saddle points, and every local minimum is a global minimum for this class of functions. This information is valuable in algorithm design as it ensures that first-order methods are not stuck at local minima or saddle points. The objective function for $k=2$, $M \\rightarrow$ $s^TM^{-2}s$, is conjectured to have only one local minimum [Citation: Xi Chen et.al., Page 10]. Ours is the first theoretical result that validates this conjecture. \n\nNot only do we theoretically demonstrate the global optimality scenarios for various practical cases of polarization, including stubborn actors and multi-period polarization, but we also deal with scenarios where only partial information about initial opinions is available. Our relaxations provided in equations (5), (6), and (24) aim to identify the edges that provably minimize polarization. \n\nAlso, observe that most of the literature is focussed on minimizing convex objectives such as Polarization and Disagreement index or understanding Disagreement among users (references are provided in Section 3: Prior Work). \n\n[Citation: Miklos Z. R\u00e1cz et al., page 12] studied this problem in a discrete setting and provided heuristics to minimize polarization. \nOurs is the first continuous optimization study that provides theoretical guarantees in this aspect and outperforms the existing approaches. \n\nThus, our contributions are multi-fold: \n\n1) Rather than modelling the problem in a discrete setting, we provide a continuous relaxation backed up with theoretical guarantees for polarization under known initial opinions, including the realistic cases of stubborn actors and multi-period.\n2) We also study a realistic problem setting that has not been previously studied, wherein the observer has access to only a subset of initial opinions. We show that in these cases, too, PGD can attain the global minimum. Note that this scenario can also be extended to stubborn actors and multi-period. \n\n\nWe hope to have addressed the reviewer's questions. We would be happy to answer any further questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6576/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699908730413,
                "cdate": 1699908730413,
                "tmdate": 1699966196155,
                "mdate": 1699966196155,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7Z58SbCoIm",
            "forum": "cI7WAadODh",
            "replyto": "cI7WAadODh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6576/Reviewer_pnnF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6576/Reviewer_pnnF"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new approach for two problems related reducing polarization in a network. In the one variant, opinions are assumed to be observed for all participants in the network, while in another variant, only a subset of opinions are observed. There are assumed to be weights between pairs of users that can be modified by the social network platform, and opinions are assumed to evolve via the Friedkin-Johnsen model. The goal is to minimize the polarization of the network by making changes to the weights of the network, subject to a budget constraint. The authors show that polarization is an invex function, and develop an invex relaxation approach to solve this problem. Computational results are presented on both synthetic and real data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method provided by the authors is original, and addresses an interesting problem. The computational experiments are reasonable, and demonstrate that the method provides value.The paper is mostly clearly written, other than a couple of points that I mention in the weaknesses."
                },
                "weaknesses": {
                    "value": "It was unclear to me exactly which optimization problem the authors are trying to solve. Is it problem (3) or is it problem (5)? The problem (5) is presented as a relaxation of problem (3), so I am assuming that this work is ultimately intended to solve problem (3). However, as far as I can tell, the procedure proposed by the authors does not guarantee that the resulting solution is feasible for problem (3). The authors should clarify this. \n\nSome of the content presented in the paper seems superfluous, including the material related to polarization under stubbornness and multi-period polarization.\n\nThe assumptions that the authors make about the distribution of the unknown opinions seems to be quite strong. The authors could make their work stronger by providing stronger justification for this assumption or by examining how this assumption affects their results. For example, the authors could provide computational experiments where these assumptions are violated.\n\nThe authors do not report required computational time of their method.\n\nThe computational experiments in the case where some opinions are unknown could be stronger. The only comparison method that the authors provide is one that ignores all known opinions. It would be good to also apply some of the other existing methods, such as the coordinate descent approach where the unknown opinions are mean imputated."
                },
                "questions": {
                    "value": "What, exactly is the optimization problem that you are trying to solve?\nIf you are trying to solve problem (3), how do you ensure feasibility?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6576/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6576/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6576/Reviewer_pnnF"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6576/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698423434392,
            "cdate": 1698423434392,
            "tmdate": 1699636746576,
            "mdate": 1699636746576,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pMxFRnJYeI",
                "forum": "cI7WAadODh",
                "replyto": "7Z58SbCoIm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6576/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6576/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications"
                    },
                    "comment": {
                        "value": "Firstly, we thank the reviewer for taking their valuable time to provide feedback.\n\nWe are solving the problem (5), which is the relaxation of the problem (3). We have now mentioned this explicitly in our paper (page 7, below equation 6 in the rebuttal edition of the paper). \n\n**Regarding the distribution of unknown opinions:** We have provided new proof in Appendix F where the invexity is attained without any assumptions on the distribution of initial opinions (in the rebuttal version of the paper). Thus, the global minimum guarantees are maintained even under the unknown distribution of initial opinions. \n\nWe are currently working on the suggested experimentation for the baseline coordinate descent approach where the unknown opinions are mean imputed. We will add a follow-up to this update soon. \n\nWe hope to have addressed the reviewer's questions. We would be happy to answer any further questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6576/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699908736692,
                "cdate": 1699908736692,
                "tmdate": 1700011857327,
                "mdate": 1700011857327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qBBEdzYQ16",
                "forum": "cI7WAadODh",
                "replyto": "7Z58SbCoIm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6576/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6576/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Experiments"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe have incorporated experiments on coordinate descent (by mean imputation of opinions) and compared the results with Trace and nonconvex relaxations. These experiments are detailed on page 19 of supplementary material under the section labeled \"Additional experimentation for partially observable initial opinions.\""
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6576/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700143427644,
                "cdate": 1700143427644,
                "tmdate": 1700143746584,
                "mdate": 1700143746584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1S8mk7sEqD",
                "forum": "cI7WAadODh",
                "replyto": "7Z58SbCoIm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6576/Reviewer_pnnF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6576/Reviewer_pnnF"
                ],
                "content": {
                    "title": {
                        "value": "Clarifications"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications and additional experiments.\n\nI appreciate the expanded theoretical result showing that the expected objective is invex for arbitrary initial distributions of opinions, which I feel strengthens the paper. However, if I am understanding correctly, you would have to know what the initial distribution is in order to apply your method. In practice, this initial distribution is unlikely to be known precisely, and would be estimated somehow. How sensitive is your method to inaccuracies in estimating this initial distribution?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6576/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496793884,
                "cdate": 1700496793884,
                "tmdate": 1700496812985,
                "mdate": 1700496812985,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4mhHPW6vn0",
            "forum": "cI7WAadODh",
            "replyto": "cI7WAadODh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6576/Reviewer_XHJ6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6576/Reviewer_XHJ6"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of minimizing polarization in Friedkin-Johnson (FJ) model, where polarization simply measures how close the given network is to consensus. In particular, given an adjacency matrix on an undirected graph, the problem at hand is to find a new adjacency matrix which only differs from the original by a given budget and minimizes the polarization. It is expected that this problem is difficult in nature (due to the sparse/zero norm constraints), which is what is stated. The authors then provide a nonconvex relaxation and show that this relaxation falls into the category of an invex function minimization, and naturally use this to provide a trackable formulation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is not suitable for this venue."
                },
                "weaknesses": {
                    "value": "Regardless of the merits of the contributions, the paper is not suitable for ICLR. \n\nThe problem is also not well motivated, and does not appear to be addressing a fundamental issue or question; the problem seems to be defined in a way that its relaxation fits to an invex function minimization problem. The related literature is not well surveyed; there is a wide range of optimization problems on graph Laplacian learning that could be relevant here, and the literature on Friedkin-Johnson (FJ) model is far from complete."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6576/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699049867121,
            "cdate": 1699049867121,
            "tmdate": 1699636746464,
            "mdate": 1699636746464,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "U9PcpASrVJ",
                "forum": "cI7WAadODh",
                "replyto": "4mhHPW6vn0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6576/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6576/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\n**Relevance to ICLR:** The URL for the ICLR conference has a \u201cgeneral machine learning\u201d track, which is where we submitted our paper. Also, the conference has non-convex optimization listed in the subject areas. The webpage also says, \u201cA non-exhaustive list of relevant topics.\u201d So, we believe the conference is open to other ML topics, such as ours, where we characterize the objective function followed by a non-convex relaxation pertinent to the problem under study. \n\nWe strongly believe that we addressed all the relevant state-of-the-art literature pertinent to the problem in the prior work section. We also provided an additional literature review in Appendix A of the paper that was submitted for review.  \n\n**Motivation:** The topic of polarization has been motivated in the introduction section of the paper. The formal Instance of the polarization minimization problem is provided on Page 2 of our paper.  Also, we would like to point out that we are not the first to study the polarization minimization problem. This problem has been initially proposed in the [Citation: Xi Chen et al. on Page 10, Cameron Musco et al. on Page 11 of references].   \n\n**Laplacian Learning:** Note that the current literature on Laplacian Learning, such as the ones listed below, does not have the negative exponent of \u201c-2\u201d in their objective, unlike what we have for polarization minimization ($s^T(I+L)^{-2}s$). So, the current literature on Laplacian learning is ***not*** directly applicable. \n\nChanghao Shi and Gal Mishne, 2023. Graph Laplacian Learning with Exponential Family Noise. Fourteenth International Conference on Sampling Theory and Applications.\n\nEgilmez, H.E., Pavez, E. and Ortega, A., 2017. Graph learning from data under Laplacian and structural constraints. IEEE Journal of Selected Topics in Signal Processing, 11(6), pp.825-841.\n\nKumar, S., Ying, J., de Miranda Cardoso, J.V. and Palomar, D., 2019. Structured graph learning via Laplacian spectral constraints. Advances in neural information processing systems, 32."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6576/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699908748149,
                "cdate": 1699908748149,
                "tmdate": 1699966704420,
                "mdate": 1699966704420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]