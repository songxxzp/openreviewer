[
    {
        "title": "BOWLL: A DECEPTIVELY SIMPLE OPEN WORLD LIFELONG LEARNER"
    },
    {
        "review": {
            "id": "gG8k10bxSi",
            "forum": "WReszdNNdP",
            "replyto": "WReszdNNdP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1218/Reviewer_wS2P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1218/Reviewer_wS2P"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript delineates a simple method, termed as BOWLL, which is devised as a baseline for the evaluation of open world lifelong learning - a conjunction of open-set recognition, active learning, and continual learning. The BOWLL exhibits an innovative usage of the Batch Normalization layer - a commonly-used component of neural networks,  along with Out-of-Distribution Detection module, Active Query module, Memory Buffer, and Pseudo Data that endow the proposed method with competitive performance for open world lifelong learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The paper's significance is underscored by its motivation to facilitate future research in this domain.\n\nS2. The BOWLL model's novelty is encapsulated in its innovative usage of the Batch Normalization layer, along with Out-of-Distribution Detection module, Active Query module, Memory Buffer, and Pseudo Data.\n\nS3. The evaluation is comprehensive, with comparisons to baseline methods and ablation study providing a compelling demonstration of the promising performance of the BOWLL method.\n\nS4. The clarity of the manuscript enhances accessibility for readers, facilitating a straightforward understanding of the proposed approach."
                },
                "weaknesses": {
                    "value": "W1.  Although the paper provides a comprehensive explanation of the methodology, further technical insights regarding the implementation and each module within the BOWLL method would be beneficial.\n\nW2. The paper falls short in providing a detailed analysis of the limitations of the proposed BOWLL, a factor which could be significant for future research and practical applications.\n\nW3. The computational complexity of the BOWLL algorithm, especially for those selection and replacement strategies, which could be a concern for large-scale datasets or practical applications, is not discussed in the manuscript.\n\nW4.  Although the paper employs sound-good methodology and achieves competitive performance,  further efforts regarding the technical innovation and methodological novelty would be beneficial.\n\nW5. The manuscript could delve deeper into the Continual Train Step, a factor which could be pivotal for understanding the pipeline of open-world lifelong learning.\n\nW6. A more detailed exposition of the datasets used in the evaluation, including their characteristics and potential biases, would enrich the manuscript."
                },
                "questions": {
                    "value": "C1. How does the method balance the data in the memory buffer and pseudo-images?\n\nC2. What is the formulation of $R_{TV}()$ and $R_{l_2}()$ respectively in Eq. (7)?\n\nC3. What is the meaning of $\\beta$ in the evaluation metric LCA?\n\nC4. Haven't the model used those discarded data?\n\nC5. What is the relationship between open-world learning and open-world lifelong learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698607276210,
            "cdate": 1698607276210,
            "tmdate": 1699636048163,
            "mdate": 1699636048163,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gO2GdzFWgA",
                "forum": "WReszdNNdP",
                "replyto": "gG8k10bxSi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to reviewer wS2P"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their assessment and **appreciate the additional feedback** on how to improve our paper. We have uploaded a **revised version of our paper,** that already incorporates the mentioned suggestions and provides clarifications with respect to the remaining concerns and questions. For convenience, the **changes** and additions are highlighted **in blue** in the pdf. We have also posted a **summary of all changes**, spanning all reviewers\u2019 feedback **at the top**. In the following we provide additional short statements and responses to the specific reviewer\u2019s points. \n\n**W1 & W4: \u201cfurther technical insights regarding the implementation and novelty\u201d**\n\nWe agree that more details were necessary. Whereas earlier we have just had a technical description of the OoD module (A.2), added appendices A.3 and A.4 now provide the full details for the active query and Deep Inversion. With respect to novelty, we emphasize that there exists no \u201cGDUMB equivalent\u201d as the default in open world learning. BOWLL provides this first baseline with a single easy to use common component, by identifying the three-fold use of batch-normalization. Both the OoD detector and the active query are novel by themselves.\n\n**W2: limitations and prospects of BOWLL**\n\nWe agree that these are valuable. We now provide a brief summary in the conclusion and point to a new appendix A.10 for an extended account. In summary, we discuss the diagonal Gaussian covariance in batch-norm to render the easiest baseline, discuss potential caveats of Deep Inversion, and highlight how BOWLL can easily be used in other scenarios without supervision.  \n\n**W3: \u201ccomputational complexity of BOWLL\u201d**\n\nWe now provide an intuitive walkthrough of the computational aspect of BOWLL\u2019s components at the end of new appendix A.4.. In summary, the main critical component is Deep Inversion, not the active query step, as DI requires many updates on a synthetic image before converging. However, we also alleviate this concern by proceeding to discuss the trade-off in light of our findings of figure 3, where BOWLL* (without Deep Inversion) shows only around a 3% accuracy decrease. We agree that it was helpful to expose this to prospective readers in the pdf. \n\n**W5: \u201c\u201ddelve deeper into the continual learning step**\n\nIn addition to the new appendix sections to provide more details for the individual components, we have revised the main body\u2019s sections 3.3 and 3.4, where we now more precisely describe the use of the memory and the update step. This should improve clarity and remove potential prior ambiguity. \n\n**C1: \u201cBalance data in memory buffer and pseudo-images\u201d**\n\nWe now describe that we only generate as many pseudo-images as in the memory buffer, a 1:1 balance that is technically a prospective hyper-parameter, discussed in appendix A.4 together with computational trade-offs.  \n\n**C2: \u201dFormulation of equation 7 - priors\u201d**\n\nWe now state the intuition behind the priors in the main body and provide exhaustive definitions  in appendix A.4. In short, The l2 term ensures that the range of values of the synthetic data remains in a reasonable range and the total variation (TV) prior ensures that pixels in a vicinity are related (to form entities in the specific case of images) - as motivated by cited prior work.\n\n**C3: \u201cmeaning of beta in LCA\u201d**\n\nLCA sums over mini-batches of data trained on so far. In essence, it asses how quickly the model learns. For instance, beta=1 is one-shot learning. To clarify, we now provide an extended description of definitions and their interpretation of all metrics in appendix A.6.\n\n**C4: \u201cHasn\u2019t the model used those discarded data\u201d**\n\nWe have revised the text to more precisely point out that data that has been rejected in the OoD step is never queried by the active learner and that data that is not in the memory buffer, is not included in optimization steps. As such, all data may have been passed through the model to figure out whether to include it, but only data that makes it into the memory buffer is ultimately trained on. Table 2 shows the number of data points that are being used in optimization. \n\n**C5: \u201dRelationship between open world learning and open world lifelong learning\u201d**\n\nWe realize we have not pointed this out precisely before, but open world learning only  mentions that data is included or rejected, then queried/labelled and an incremental learning step is conducted. Often this leads to an interpretation that novel identified data is simply concatenated to existing datasets over time (inspired by active learning). To disambiguate this from a continual learning step (where we do not store old data), we have included the term lifelong. We have included a statement in the related work section, pointing to a new appendix A.1 with prior work\u2019s definitions.\n\nFinally, we thank the reviewer again for the feedback and invite them to confirm that our revised manuscript has adequately included their points."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240930080,
                "cdate": 1700240930080,
                "tmdate": 1700240930080,
                "mdate": 1700240930080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2zCbss5tJW",
            "forum": "WReszdNNdP",
            "replyto": "WReszdNNdP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1218/Reviewer_SRy8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1218/Reviewer_SRy8"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses open-world continual learning, an emerging research area, and suggests leveraging Bayesian Network statistics to enhance various phases of open-world learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Applying BN statistics for OOD detection, active learning, and continual learning is a novel and unified approach.\n2. The significance of the problem is notable.\n3. It surpasses a strong baseline, GDUMB."
                },
                "weaknesses": {
                    "value": "I believe this paper might overstate its contributions for the following reasons:\n\n1. It seems to focus solely on the class-incremental learning scenario in continual learning, despite claiming to address various types of continual learning settings. How about, for example, Task-incremental learning [1]? \n\n2. The paper claims that BOWLL can achieve OOD detection, active learning, and continual learning, but I only see a comparison in final and LCA performance in table 2. This falls short of adequately demonstrating the model's superiority in all three objectives.\n\n[1]: Continual learning of a mixed sequence of similar and dissimilar tasks. Ke et al., NeurIPS 2020"
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1218/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1218/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1218/Reviewer_SRy8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698628897539,
            "cdate": 1698628897539,
            "tmdate": 1699636048095,
            "mdate": 1699636048095,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BNCX2jpgCT",
                "forum": "WReszdNNdP",
                "replyto": "2zCbss5tJW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to reviewer SRy8"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their assessment and **appreciate the additional feedback** on how to improve our paper. We have uploaded a **revised version of our paper,** that already incorporates the mentioned suggestions and provides clarifications with respect to the remaining concerns and questions. For convenience, the **changes** and additions are highlighted **in blue** in the pdf. We have also posted a **summary of all changes**, spanning all reviewers\u2019 feedback **at the top**. In the following we provide additional short statements and responses to the specific reviewer\u2019s points. \n\n**\u201cOverstated contributions: paper focuses only on class-incremental learning in continual learning\u201d**\n\nWe believe there is a misconception of what the paper is doing and trying to achieve, but we taken the point as feedback to improve and have extended the writing to avoid this in the future. \nTo first clarify, the paper is not tackling a purely continual learning problem. In fact, the experiments follow a logical sequence (see next point) to step by step lead the reader to open world learning, where the data does not only change over time, but may also include both irrelevant or corrupt data. Open world learning is a very challenging realistic scenario, that requires the model to predict whether a data point should be rejected/accepted, how informative it is, and how to continue training. Traditional continual learning only focuses on the last aspect. We are not investigating task-incremental learning because it assumes one knows \u201cwhich task to predict\u201d during inference through a provided label. This is a step back in the opposite direction of open world learning, where we don\u2019t even assume that the data must be related at all to any of the tasks. \nAs mentioned, we have however taken the feedback to improve the paper and now include a) a detailed introduction and definition to open world learning in appendix A.1; b) an improved experimental setup section to clarify our experiments (see also below) c) a more detailed description of the data and training details in appendix A.7; d) extended motivation and discussion of the active query\u2019s formulation in appendix A.3 (active query).  \n\n**\u201cPaper claims can achieve OOD detection, active learning, continual learning but doesn\u2019t adequately demonstrate the model\u2019s superiority\u201d**\n\nWe believe the paper shows this already, as the experiments are meant to showcase individual components in a cascade of insights, first on learning speed, then on forgetting, and ultimately in the full open world learning experiment (third experiment). Especially the last, realistic scenario without curated data shows that BOWLL is the only method to succeed here, making it a very reasonable future baseline. However, we have taken the reviewer\u2019s feedback to improve presentation and more rigorously highlight the individual aspects. Primarily we have improved the paper in the following: \n\n* Each experimental subsection has been reworded to be more precise on the experimental take-away and now clearly states which component is supported by the found experimental evidence. In short:\n  1. Table 2 focuses on highlighting that BOWLL achieves the same performance in the particular domain incremental setting as GDUMB, but does so with less data and massively faster learning speed (LCA) - highlighting the \u201cactive\u201d part. \n  2. Figures 2 and 3 support that BOWLL features less catastrophic forgetting through the way it maintains its memory - highlighting the \u201ccontinual\u201d part.\n  3. Figures 4 highlights how BOWLL\u2019s OoD detector is crucial to perform meaningful in the open scenario, where the scenario actually contains non-curated data. \n* Based also on reviewer BmgJ\u2019s feedback, we have included two additional baselines in the second and third experiment: experience replay (which visits all task\u2019s data first and then keeps a memory as it proceeds) and Softmax based OoD detection, to highlight that BOWLL is a very meaningful baseline beyond GDUMB alone. The discussion and plots are updated respectively.   \n\n**Comment: \u201cBayesian Network Statistics\u201d** \n\nWe are unsure if there may be a misunderstanding from our side on the reviewers remark \u201csuggests leveraging Bayesian Network statistics\u201d, so we would like to clarify that BN in our case is batch-normalization. While batch-normalization may certainly be framed in a Bayesian Network context and may even be sampled from, we want to emphasize that BOWLL is a simple, yet highly performative baseline that does not require a framing of Bayesian Neural Networks or their use. Any non-Bayesian neural network that contains batch-normalization is convertible into a BOWLL learner, which makes it a meaningful baseline for open world learning. \n\nFinally, we thank the reviewer again for the feedback and invite them to confirm that our revised manuscript has adequately included their points. If the reviewer has any remaining questions, we are happy to clarify them further."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240810138,
                "cdate": 1700240810138,
                "tmdate": 1700240859171,
                "mdate": 1700240859171,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TJySpF3Ay8",
            "forum": "WReszdNNdP",
            "replyto": "WReszdNNdP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1218/Reviewer_BmgJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1218/Reviewer_BmgJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a baseline method for open-world lifelong methods that relies on the idea of batch normalization. The method relies on three important components: 1) detection of out-of-distribution examples using batch-norm statistics, 2) active querying of remaining examples by using batch-norm statistics, 3) continual training using both example replay and generated pseudo-examples that also rely on information coming from batch-norm statistics. Experiments are run on three benchmark datasets, and compared to strategies such as joint learning, funetuning and GDUMB, using metrics such as backward transfer and accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper intends to tackle the very important problem of open-world lifelong learning by exploiting a simple yet effective strategy of batch-normalization statistics. These statistics are exploited in several parts of the learning process, including discarding OOD examples, actively selecting most effective examples, and actually learning from these selected examples in a continual learning setting. \n- Experiments show that the proposed baseline is quite competitive, in particular in terms of backward transfer (Table 2)\n- The paper is well-written and easy to follow. The components of the solution are clearly explained, and the diagram in Fig. 1 is very self-explanatory."
                },
                "weaknesses": {
                    "value": "- The main weakness that I see in the paper is the limitation of the experiments. I would have expected more robust experiments in more varied datasets, and a larger number of datasets and tasks. \n- Similarly, I would have expected more comparisons with other SOTA methods that, although not originally open-world learning, perhaps could be slightly modified for the sake of comparison."
                },
                "questions": {
                    "value": "- Table 2 shows quite a remarkable good performance of the proposed method in the case of backward transfer, which is a very challenging problem in continual learning, and is difficult to achieve. Could you provide more insights as to why this would be the case?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718424796,
            "cdate": 1698718424796,
            "tmdate": 1699636048005,
            "mdate": 1699636048005,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GIPLPeoSEg",
                "forum": "WReszdNNdP",
                "replyto": "TJySpF3Ay8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to reviewer BmgJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their assessment and **appreciate the additional feedback** on how to improve our paper. We have uploaded a **revised version of our paper,** that already incorporates the mentioned suggestions and provides clarifications with respect to the remaining concerns and questions. For convenience, the **changes** and additions are highlighted **in blue** in the pdf. We have also posted a **summary of all changes**, spanning all reviewers\u2019 feedback **at the top**. In the following we provide additional short statements and responses to the specific reviewer\u2019s points. \n\n**\u201cThe main weakness that I see in the paper is the limitation of the experiments\u201d**\n\nWe have provided a more detailed description of our experiments and have clarified their purpose more thoroughly in the main body and appendix. In addition, we agree that there may not exist any SOTA methods designed specifically for open world learning, but that some other methods are interesting for readers to see in this context. To this end, we have run and added two further popular baselines to our experiments: experience replay (ER) and Softmax based OoD detection (according to the published works of Rollick et al and Hendrycks et al). Whereas GDUMB actively fills a memory buffer at random and thus is the closest in spirit to BOWLL, ER is a more intuitive comparison in experiment 2\u2019s focus on forgetting, as ER first trains on all data before maintaining a random memory buffer over time. In figure 3, we can see that this training on all data does not lead to ER outperforming BOWLL. Naturally ER fails in the experiment with corrupted and OoD data. Here, Softmax based OoD detection provides a baseline for the open world learning scenario of the third experiment. We further note that appendix A.8 contains an ablation study, demonstrating the meaningfulness of BOWLL\u2019s components and showing how BOWLL performs when individual parts are stripped away.\n\n**Question: could you provide more insights in the case of backward transfer**\n\nWe agree with the reviewer that observing positive backward transfer is remarkable and interesting. In general, and particularly in the mentioned specific scenario of the sequence of multiple digit based datasets (table 2), we attribute the high performance to the fact that BOWLL contains an active learning query that balances data novelty with similarity (rather than for instance querying based on high entropy alone or doing so at random as in GDUMB). As such, new data that features some form of similarity is prioritized and thus contains partial information to further improve retrospectively. In line with the question of reviewer ELi2, we understand that the precise benefit of the exact active query formulation may not have been fully clear enough in the present manuscript. We have made the wording more precise and now provide an extended discussion of how and why the active query has been conceived in appendix A.3. \n\nFinally, we thank the reviewer again for the feedback and invite them to confirm that our revised manuscript has adequately included their points."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240673735,
                "cdate": 1700240673735,
                "tmdate": 1700240673735,
                "mdate": 1700240673735,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0UDvLJHIoV",
            "forum": "WReszdNNdP",
            "replyto": "WReszdNNdP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1218/Reviewer_ELi2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1218/Reviewer_ELi2"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce the first monolithic baseline for open world lifelong learning, which remedies the lack of well suited baselines for evaluation. Particularly, the simple batch normalization technique is repurposed for 3 subtasks in lifelong learning: open-set recognition, active learning and continual learning. Through extensive empirical evaluation, the resulting approach proves simple yet highly effective to maintain past knowledge, selectively focus on informative data, and accelerate future learning. The proposed method also compares favorably to other related baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- A simple and reliable baseline is always valuable, especially for the less-studied open world lifelong learning area. The method seems competitive on the benchmarked datasets.\n- The unified use of the batch norm statistics for the 3 components in lifelong learning is interesting and promising. The ablation in Table 3 of the appendix is nice, indicating the involved components are indispensable."
                },
                "weaknesses": {
                    "value": "- One main concern of this paper is the missing analysis for some components of the proposed lifelong learner (see questions below)."
                },
                "questions": {
                    "value": "- The image synthesis method based on Deep Inversion seems interesting. All it's doing is to generate class-conditioned pseudo-images using past representations (the running mean and variance from the batch normalization layers). How much cost will such image synthesis incur? How faithful are the generated images? Why not opt for feature synthesis which seems natural and efficient given the maintained feature mean and variance?\n- For active query, the acquisition function is designed using entropy weighted with sample similarity. How important is such weighting? Is this the best way to strike a good tradeoff between exploration and similarity? Any other formulations for ablation/comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1218/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777037458,
            "cdate": 1698777037458,
            "tmdate": 1699636047935,
            "mdate": 1699636047935,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SSf5zuAE7a",
                "forum": "WReszdNNdP",
                "replyto": "0UDvLJHIoV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1218/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1218/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official response to reviewer ELi2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their assessment and **appreciate the additional feedback** on how to improve our paper. We have uploaded a **revised version of our paper,** that already incorporates the mentioned suggestions and provides clarifications with respect to the remaining concerns and questions. For convenience, the **changes** and additions are highlighted **in blue** in the pdf. We have also posted a **summary of all changes**, spanning all reviewers\u2019 feedback **at the top**. In the following we provide additional short statements and responses to the specific reviewer\u2019s points. \n\n**\u201cHow faithful are the generated images of Deep Inversion (a); Why not opt for feature synthesis, which seems natural and efficient given the maintained feature mean and variance (b). \u201d**\n\na) We remark that the generated images do not necessarily need to be faithful in the sense of the original data, as the intend is to mitigate catastrophic forgetting only and not train on them from scratch or reconstructing natural images. However, the included L2 and TV priors (now explained better in the main body and in much more detail in appendix A.4) regulate the image values to stay within a meaningful range and ensure that pixels in a local vicinity retain similarity, in order to ensure that entities are formed rather than just producing any (adversarial) noise that fits the statistics. \n\nb) We agree that feature synthesis is a highly intriguing direction, and have included a short discussion and motivation for our choice in the new appendix A.4. In essence, we believe that a strong baseline needs to both be performant but also simple to implement. Using synthesized data points from Deep Inversion matches this requirement, as it allows us to straightforward interleave these instances with the real memory buffer, populated by the active learner, in optimization. It is thus trivial to code and employ in any system that already contains batch-norm, the majority of current NNs. We do however agree that feature synthesis is intriguing and now explicitly point to it, referring also to works such as Pellegrini et al (IROS 2020) that have already demonstrated the efficacy of \u201clatent replay\u201d techniques. (We are happy to include more references here, if the reviewer has explicit pointers). We are excited for future work to pick up on this direction and showcase how such a more involved approach could beat the current BOWLL baseline, in similar spirit to how future work may extend the use of diagonal covariance towards full measures.  \n\n**\u201cFor the active query, \u2026 is this the best way to strike a good tradeoff between exploration and similarity?\u201d**\n\nYes indeed, the reviewer\u2019s intuition is correct and the design is about the tradeoff between exploration and similarity. We noticed that we could perhaps highlight this aspect more prominently and are now doing so in the main body, pointing further to a new appendix section A.3. Similar to the extended description of the conception of the OoD detector in A.2, we now describe the conception of the active query in much more detail. Here, we also point out the failure mode of excluding the similarity term. That is, informativeness gauged by the entropy term alone might yield highly novel data that is unrelated to the task or undesired (e.g. very noisy, perturbed, corrupted or fully unrelated data). The memory replacement step follows a similar logic.  At the end, we now also note that there could technically be a more elaborate weighting of the exploration and similarity terms, but that we did not introduce respective weights to analyze this option further.   \n\nFinally, we thank the reviewer again for the feedback and the already highly positive perception of our work. Nevertheless, we invite them to read our overall additions to the manuscript and to confirm that our revised manuscript has adequately included above points."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1218/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240601142,
                "cdate": 1700240601142,
                "tmdate": 1700240601142,
                "mdate": 1700240601142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]