[
    {
        "title": "On the Limitations of Temperature Scaling for Distributions with Overlaps"
    },
    {
        "review": {
            "id": "2OCHeM4dUj",
            "forum": "zavLQJ1XjB",
            "replyto": "zavLQJ1XjB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3617/Reviewer_JXm6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3617/Reviewer_JXm6"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies when and why temperature scaling may not work. This paper showed theoretically that when the supports of classes have overlaps, , the performance of temperature scaling degrades with the amount of overlap between classes, and asymptotically becomes no better than random when there are a large number of classes. This paper suggests that Mixup data augmentation technique can lead to reasonably good calibration performance, which are supported by the experiments conducted in the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper identifies theoretical limitations of a widely used calibration technique, temperature scaling. The paper is technically solid and clearly written, with aligned theory and experiments.\n\nOriginality: The paper offers novel theoretical results on the inherent limitations of temperature scaling based on distributional assumptions. While temperature scaling is widely used, formal characterization of when it provably fails is new. The conditions identified are also intuitive and realistic.\n\nQuality: The theoretical results are informative. [Note: I am not an expert in theory though, and I didn't check the proof]. The experiments cleanly validate the theory on both synthetic data and real image benchmarks. The proposed d-Mixup method is interesting. Overall the paper reflects quality research.\n\nClarity: The problem is motivated well and background provided. The writing clearly explains the theories, assumptions, experiments, and connections between them. Figures aid understanding. The paper is well organized.\n\nSignificance: Calibration is critical for uncertainty aware models, but little theory exists. This paper significantly advances understanding of an important technique. The insights on training procedures are impactful for future work."
                },
                "weaknesses": {
                    "value": "1. The scope is limited to temperature scaling and Mixup. Discussing connections to other calibration methods could broaden impact.\n2. It would be better to have more real data experiments. In the \"IMAGE CLASSIFICATION BENCHMARKS\", the overlap is introduced rather artificially."
                },
                "questions": {
                    "value": "1. \"We also trained d-Mixup models on the same data, but we found that the confidence regularization effect for d > 2 led to underconfidence on these datasets, so we report just the results for Mixup.\": do we know why this may happen?\n2. In the image experiments, for CIFAR-100, why having label noise makes NLL worse but ACE / ECE better? This makes the experiments less convincing if we don't have a solid explanations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3617/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3617/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3617/Reviewer_JXm6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3617/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698651626894,
            "cdate": 1698651626894,
            "tmdate": 1699636316934,
            "mdate": 1699636316934,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wHWBpGR9Ou",
                "forum": "zavLQJ1XjB",
                "replyto": "2OCHeM4dUj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3617/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3617/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JXm6"
                    },
                    "comment": {
                        "value": "We would like to thank Reviewer JXm6 for reviewing our paper, and we are grateful that they found our work to be clear and technically well-founded. We hope to address the raised weaknesses and questions below.\n\n1. **Scope of discussed methods.** We definitely agree that it would be possible to extend the ideas from our theory to other data augmentation/training-time regularization approaches, and we have now emphasized this more in Section 1.1 (outline) in addition to stressing it in the conclusion.\n\n2. **Real data experiments.** We also agree that having a more canonical \"overlap\" setting would be useful, however we felt that label noise was a natural way to introduce this overlap that also is readily encountered in practice (since samples are often mislabeled). Our results in Sections 5.2 and B.3 also include the 0\\% label noise setting, which involves no artificial modification.\n\n3. **Explaining regularization effect of $d$-Mixup.** When mixing several points with a uniform mixing distribution, mixtures in which one point gets a label very close to its original label (i.e. a mixing weight very close to 1) become less likely, and as a result we get even less spiky predictions (much closer to uniform probabilities).\n\n4. **NLL gets worse but ECE/ACE get better.** It is possible to have very poor NLL while still having good (or even perfect) ECE, so worse NLL does not necessarily imply worse ECE performance. For example, a model that predicts uniformly at random can achieve zero ECE in the balanced class regime despite having poor NLL. Additionally, the ECE results in Table 4 for Mixup on CIFAR-100 are roughly within the 1 standard deviation error bounds of each other (so this does not represent a marked improvement), which we have also now pointed out in the surrounding discussion.\n\nThank you again for your feedback and comments and we are happy to answer any further questions you may have."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3617/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700005762880,
                "cdate": 1700005762880,
                "tmdate": 1700005762880,
                "mdate": 1700005762880,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iqiBA6z903",
            "forum": "zavLQJ1XjB",
            "replyto": "zavLQJ1XjB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3617/Reviewer_dtwE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3617/Reviewer_dtwE"
            ],
            "content": {
                "summary": {
                    "value": "The paper demonstrated how temperature scaling has subpar performance in the case of overlapping classes, and proposed mixed up as an effective alternative to improve model calibration. The paper considers the ERM interpolator set of models, where there's clear separation between the prediction of top-class and the rest. In this case temperature scaling is failing to produce the desired behavior of equal prediction on the overlapping portion of the two classes. On the other hand, training with mixing loss is able to capture the overlapping behavior and significantly improve ECE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written with clear presentation of references on background, assumption, and key results.\n- There are theoretical results backing up the observations made in the toy examples and experiments.\n- The few experiments shows good evidence supporting the conclusion that mixup is effective under the overlapping classes scenario."
                },
                "weaknesses": {
                    "value": "- The mixing training introduces an extra degree of freedom (i.e. d-mixing). Based on table 2, we see that mixing is actually negative for NLL when classes are relatively separate, but only show performance improvement as the overlap increase. I also believe that the model performance would not be strictly better as we increase the degree of mixing, not to mention the additional computational complexity. Intuitively, the optimal d should have to do with the structure of overlapping in the dataset. I think it would be beneficial for the authors to have a more in-depth discussion on the choice of mixing in practice. Discounting the additional regularization effect, is it reasonable to only have the regular mixup when only two classes overlap at a time?"
                },
                "questions": {
                    "value": "- Does it make sense to generate additional classes for the overlapping case (y=1, y=2, y=1&2)? In that case would temperature scaling still work and what is the tradeoff here?\n- Is ERM interpolator the best model class to capture the datasets with overlaps? The properties of the interpolator seem to be naturally mismatched."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3617/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743247009,
            "cdate": 1698743247009,
            "tmdate": 1699636316852,
            "mdate": 1699636316852,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3YRZTm6wK0",
                "forum": "zavLQJ1XjB",
                "replyto": "iqiBA6z903",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3617/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3617/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dtwE"
                    },
                    "comment": {
                        "value": "We would like to thank Reviewer dtwE for reviewing our paper, and we are happy to see that they found our work to be well-written and our claims to be well-evidenced. We hope to address the mentioned weaknesses and questions below.\n\n1. **Amount of mixing in $d$-Mixup.** We definitely agree that the optimal mixing choice depends on the underlying data distribution being considered, and it is certainly true that mixing more points is not necessarily better (as we mention, this can lead to over-regularization of the predicted probabilities). However, it is tricky to analyze the optimal mixing choice in practice outside of treating it as an additional hyperparemeter. This is due to the fact that we do not know the underlying ground truth distribution - if we did have some way to quantify the amount of overlap in the ground truth, then it is certainly conceivable that we could develop theoretical guidelines for the optimal mixing number (although this would be separate from the theory in our work, which mixes $d + 1$ points since we do not make assumptions on the model class). Additionally, even when only two classes are overlapping there can still be significant benefit to mixing more than two points, which is demonstrated by the high overlap case in our experiments in Section 5.1 (4-Mixup performs the best in terms of NLL and ECE). The intuition here is again that the number of mixed points corresponds to the amount of regularization, and in the cases of high overlap we want to make sure the predicted probabilities are far away from being spiky (i.e. close to point masses).\n\n2. **Generating additional class labels.** Generating synthetic labels as a means of regularization is definitely a useful idea, and in fact this is already done by methods such as label smoothing and Mixup (which drives the theory behind Mixup in our paper). The main difficulties with what you are suggesting (constructing a new class entirely) is the lack of knowledge of the ground truth distribution required to do so (that is, the input data does not specify which samples are in the overlapping part between class 1 \\& 2). We need to specify some strategy for determining which samples get split off into the newly constructed class, and any such strategy will necessarily come with some baked-in biases about how we expect the ground truth distribution to behave. Naive strategies such as randomly assigning samples to the new class probably fall within the realm of existing label augmentation techniques, such as variants of label smoothing and Mixup.\n\n3. **ERM interpolator drawbacks.** It is true that the properties of the ERM interpolators that we study are not ideal for calibration, which also drives the failure of temperature scaling in this case. However, we focus on this class of interpolating models as they are incredibly common in practice, and for these models we observe empirically the kind of assumptions we make in our theory (spiky probabilities that are well-separated). It is definitely possible that training ERM models with early stopping can provide sufficient regularization for better calibration (especially when combined with temperature scaling), which would be an interesting avenue to explore.\n\nIf you have any further questions we are happy to answer them; thank you again for your useful feedback and remarks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3617/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700005535096,
                "cdate": 1700005535096,
                "tmdate": 1700005535096,
                "mdate": 1700005535096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DTMySkmQdH",
                "forum": "zavLQJ1XjB",
                "replyto": "iqiBA6z903",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3617/Reviewer_dtwE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3617/Reviewer_dtwE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for the response. This might be somewhat a naive question. Your response mentioned that we do not know the ground truth distribution and quantification of the overlap, but shouldn't we have the estimation from the empirical distribution through the training data and labels?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3617/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467041750,
                "cdate": 1700467041750,
                "tmdate": 1700467065555,
                "mdate": 1700467065555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pAENTEEiwR",
            "forum": "zavLQJ1XjB",
            "replyto": "zavLQJ1XjB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3617/Reviewer_UALj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3617/Reviewer_UALj"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the limitations of the widely used temperature scaling for post-hoc uncertainty calibration. The authors find that under the specific assumption, i.e., the datasets follow a general set of distributions in which the supports of classes have overlaps, the temperature scaling method cannot perform well. Since the temperature scaling has been very successful in post-hoc calibration, this paper is interesting for pointing out its limitations and find under some conditions it provably fails to achieve good calibration. Furthermore, the authors find that the performance of temperature scaling degrades with the amount of overlap between classes, and asymptotically becomes no better than random when there are a large number of classes. This paper also studies a specific training-time calibration technique mixup and finds that it can lead to reasonably good calibration performance under the same conditions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well motivated. The studied point is interesting. It is empirically found that temperature scaling is good for calibrating deep models. However, as pointed out in this paper, it may harm calibration under some conditions.\n\n2. The empirical study shows very supportive results for the theoretic results. Both experiments on synthetic data and real-world data show positive results that temperature scaling cannot work under some conditions.\n\n3. The writing and organization of this paper is very good."
                },
                "weaknesses": {
                    "value": "1. It is commonly believed that temperature scaling is very effective for post-hoc calibration scaling, although it is found that this technique cannot be used for all the cases. Can you explain what causes this gap between your theoretic results and the commonly observed empirical success?\n\n2. The main experimental results in tables only show the comparison between ERM+TS vs Mixup. I think that the results of ERM baseline and Mixup+TS should be presented at least. Moreover, is the results influenced by the training schemes used for training models (such as learning epochs, learning rate and regularization)?"
                },
                "questions": {
                    "value": "Please refer to Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3617/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698974711159,
            "cdate": 1698974711159,
            "tmdate": 1699636316786,
            "mdate": 1699636316786,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jKRYDLKFjD",
                "forum": "zavLQJ1XjB",
                "replyto": "pAENTEEiwR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3617/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3617/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UALj"
                    },
                    "comment": {
                        "value": "We would like to thank Reviewer UALj for taking the time to review our paper, and we are glad they found the work to be well-motivated and interesting. We hope to address their main questions below.\n\n1. **Reconciling theory with common temperature scaling wisdom.** As you correctly point out, temperature scaling has been demonstrated repeatedly since [1] to be useful for calibration performance. Our theory does not contradict the *relative benefit* of temperature scaling (i.e. ERM + TS will almost certainly be better than ERM alone), but rather says that in *absolute* terms that there are classes of data distributions for which the improvement due to temperature scaling is simply not enough to achieve good calibration. This is made clearer by a new set of comparisons (Tables 6-8) that we have added as part of Appendix Section B.4, which show the relative gain from temperature scaling for both ERM and Mixup. As can be seen in this table, temperature scaling improves ERM calibration performance, but nowhere near enough to be even close to competitive with Mixup alone. Also worth pointing out is that in our setting, we do not train our ERM models with additional regularization (data augmentation, weight decay, etc.) as was done in prior work (so as to stay within the setting of our theory), and these almost certainly play a non-trivial role in how much temperature scaling can improve the base model (since the aforementioned modifications can lead to less spiky predictions and also better test accuracy).\n\n2. **Non-temperature-scaling comparisons and effect of hyperparameters.** Thank you for the useful point; as mentioned above, we have added ERM vs ERM + TS and Mixup vs Mixup + TS comparisons to the Appendix as part of Section B.4. The main obversation from these comparisons is that while TS can significantly improve ERM performance (at least with respect to negative log-likelihood), it simply does not make a substantial difference with respect to the gap in performance between ERM and Mixup (exactly as suggested by our theory). With regards to the question about hyperparameters - calibration performance will certainly be affected heavily by hyperparameter tuning, with early stopping (among the modifications you mention) in particular having a significant effect (since this can prevent predicted probabilities from becoming too spiky for ERM). Our results are constrained to the common interpolation regime in which models are trained to have very low training loss, and in this regime we found that minor tweaks to hyperparameters such as learning rate did not have a significant effect (since we are training for a long enough time horizon). Additionally, as mentioned above, in this regime it is also harder to improve calibration performance since model predictions are even closer to point masses (which drives the failure of temperature scaling in our theoretical results).\n\nWe are happy to answer any further questions you may have - thank you again for your review and helpful comments.\n\n[1] https://arxiv.org/abs/1706.04599"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3617/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700005218062,
                "cdate": 1700005218062,
                "tmdate": 1700005218062,
                "mdate": 1700005218062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]