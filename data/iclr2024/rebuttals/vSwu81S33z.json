[
    {
        "title": "Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling"
    },
    {
        "review": {
            "id": "xBsovmBrPN",
            "forum": "vSwu81S33z",
            "replyto": "vSwu81S33z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6913/Reviewer_ZDrq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6913/Reviewer_ZDrq"
            ],
            "content": {
                "summary": {
                    "value": "In the context of transfer learning, the choice of the prior distribution for downstream data is crucial when employing Bayesian model averaging (BMA). Prior strategies have limitations in handling distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL) that addresses distribution shift issues within the framework of nonparametric learning. The nonparametric learning (NPL) method, which uses a nonparametric prior for posterior sampling, efficiently deals with model misspecification scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper proposes the use of nonparametric learning techniques to address the issue of distribution shift in transfer learning. The writing in the article is of high quality and is easy to understand. I partially reviewed the mathematical derivations in the paper, and the technical aspects appear to be sound."
                },
                "weaknesses": {
                    "value": "I did not find any significant drawbacks in the paper, except for some confusion in the nonparametric learning section. Please see below for details."
                },
                "questions": {
                    "value": "I feel confused about the authors' claim that \"the NPL posterior is robust to model misspecification through the adoption of a nonparametric model and gives asymptotically superior predictions to the regular Bayesian posterior on $\\theta$\".\n\nIn my understanding, NPL is just another way to compute the posterior for $\\theta$. In parametric Bayesian methods, a prior $p(\\theta)$ is placed on $\\theta$, and various methods are employed to compute the corresponding posterior $p(\\theta|\\mathcal{D})$. NPL, on the other hand, establishes a deterministic mapping between the data distribution $F$ and $\\theta$ using MLE (as shown in Equation 2). Then, a prior $p(F|\\alpha, F_\\pi)$ is placed on the data distribution $F$ to analytically derive the posterior $p(F|\\mathcal{D})$, which subsequently leads to the posterior $p(\\theta|\\mathcal{D}$).\n\nBoth of the above methods assume a parameterized model with parameters $\\theta$. In other words, the assumed parameterized model is still susceptible to potential discrepancies with the true data distribution, which could result in model misspecification. Therefore, if my understanding is correct, NPL may not inherently address the issue of model misspecification."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Reviewer_ZDrq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6913/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698376282447,
            "cdate": 1698376282447,
            "tmdate": 1700448076630,
            "mdate": 1700448076630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sHSBmciDhJ",
                "forum": "vSwu81S33z",
                "replyto": "xBsovmBrPN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and positive comment with an important question.\n\n__Q1 Is the NPL posterior robust to model misspecification compared to the regular Bayesian posterior on $\\theta$?__\n\nThank you to the reviewer for posing an important and insightful question. You are indeed correct that the parametric posterior and NPL posterior target the same parameter, that is $\\theta^*$ which minimizes the KL divergence between the true $F^*$ and the model $F_\\theta$ [Walker (2013)]. \n\nHowever, we highlight that NPL elicits a nonparametric prior, which induces a \nposterior distributions on $\\theta^*$ that has superior asymptotic properties to the regular Bayesian posterior when the model is misspecified. Intuitively, the reason for this subpar performance for parametric Bayes is that the posterior is computed assuming there exists $\\theta^*$ such that $F_{\\theta^*} = F^*$. On the other hand, NPL does not make this assumption (i.e. model misspecification is acknowledged), and updates the posterior distribution $\\pi_n(F)$ in a nonparametric fashion. This in turn leads to more robust posterior inferences on $\\theta^*$, as well as asymptotically superior predictions. This improvement in prediction is indeed observed practically as well [Fong et al. (2019)].\n\nWe now outline what we mean specifically by superior asymptotic posteriors and predictions.  Lyddon et al. (2018) show that the Bayesian bootstrap posterior (which has the same limit as NPL) asymptotically has the sandwich covariance matrix, which is known to be robust. On the other hand, the parametric posterior does not obtain this variance asymptotically.\n\nFor prediction, we are interested in the posterior predictive density,\n$p_n(y) = \\int f_\\theta(y) \\pi_n(\\theta)$, where $\\pi_n$ is either the Bayesian or the NPL posterior. Theorem 1 of Lyddon et al. (2018) shows that asymptotically, the KL divergence between $F^*$ and $P_n$ is smaller for the NPL posterior compared to the Bayesian posterior. This asymptotic improvement is indeed due to the robust sandwich covariance matrix [Muller (2013)].\n\nThank you again for highlighting this important point. We will add a detailed discussion of how NPL addresses model misspecification in the revision paper. And if there are no additional questions or uncertainties, we kindly request you to contemplate revisiting your evaluation to ensure it accurately reflects the situation.\n \n__References__\n\n[1] Walker, S. G. (2013). Bayesian inference with misspecified models. Journal of statistical planning and inference, 143(10), 1621-1633.\n\n[2] Lyddon, S. P., Holmes, C. C., & Walker, S. G. (2019). General Bayesian updating and the loss-likelihood bootstrap. Biometrika, 106(2), 465-478.\n\n\n[3] Lyddon, S., Walker, S., & Holmes, C. C. (2018). Nonparametric learning from Bayesian models with randomized objective functions. Advances in neural information processing systems, 31.\n\n[4] M\u00fcller, U. K. (2013). Risk of Bayesian inference in misspecified models, and the sandwich covariance matrix. Econometrica, 81(5), 1805-1849.\n\n[5] Fong, E., Lyddon, S., & Holmes, C. (2019, May). Scalable nonparametric sampling from multimodal posteriors with the posterior bootstrap. In International Conference on Machine Learning (pp. 1952-1962). PMLR."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699688277565,
                "cdate": 1699688277565,
                "tmdate": 1699861895587,
                "mdate": 1699861895587,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6OiFvb84Gt",
                "forum": "vSwu81S33z",
                "replyto": "xBsovmBrPN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response request"
                    },
                    "comment": {
                        "value": "Your contribution to reviewing our paper is much appreciated. With the discussion period deadline nearing, could you kindly respond to our rebuttals? It would greatly assist us if you could indicate any further inquiries or uncertainties you might have regarding our paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447575959,
                "cdate": 1700447575959,
                "tmdate": 1700447575959,
                "mdate": 1700447575959,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UNVhfVbvM1",
                "forum": "vSwu81S33z",
                "replyto": "6OiFvb84Gt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_ZDrq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_ZDrq"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the authors' feedback. I do not have any further concern and I am satisfied with authors' answers in this rebuttal. I raise the score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700448042053,
                "cdate": 1700448042053,
                "tmdate": 1700448042053,
                "mdate": 1700448042053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nyWr6KSeIO",
            "forum": "vSwu81S33z",
            "replyto": "vSwu81S33z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6913/Reviewer_izux"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6913/Reviewer_izux"
            ],
            "content": {
                "summary": {
                    "value": "In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in Bayesian model averaging (BMA). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning (NPTL), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. Through extensive empirical validations, the authors demonstrate that the approach surpasses other baselines in BMA performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The authors maintain high quality of presentation. The motivation and algorithms are clearly explained. \n\n(2) Extensive experiments and ablation studies are conducted in this paper. The authors considered different model architectures, datasets, evaluation metrics, tasks."
                },
                "weaknesses": {
                    "value": "(1) A detailed discussion of limitations is lacking. A possible aspect could be the computation cost. A careful discussion of pros and cons could be very helpful to the community."
                },
                "questions": {
                    "value": "(1) Following the experiments of [1], could the proposed method also uses self-supervised pretrained models and include experiments about segmentation?\n\nreferences:\n\n[1] Shwartz-Ziv, Ravid, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, and Andrew G. Wilson. \"Pre-train your loss: Easy bayesian transfer learning with informative priors.\" Advances in Neural Information Processing Systems 35 (2022): 27706-27715."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6913/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722011230,
            "cdate": 1698722011230,
            "tmdate": 1699636804428,
            "mdate": 1699636804428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zUSnXGB7Oh",
                "forum": "vSwu81S33z",
                "replyto": "nyWr6KSeIO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your constructive and positive comments. We will summarize and respond to the questions below:\n\n__W1 Discussion of limitations is lacking.__\n\nAs you pointed out, the apparent downside of the suggested NPTL approach is the extra training cost. For instance, applying the proposed NPTL algorithm involves additional computational expenses to obtain $f_{\\text{probed}}$ through linear probing for the provided downstream data. Nevertheless, the computational cost associated with linear probing is minimal in contrast to training the complete network. We will add a discussion on such limitations regarding computational costs in the next version of the paper.\n\n__Q1 (1) Using self-supervised pre-trained models__\n\nThank you for posing an important question about constructing pre-trained models. Since the informative base measure $F_\\pi(x, y)$ could be any probability measure representing our initial assumption about $F_0$ based on our prior knowledge of the actual data distribution, we have the flexibility to employ linear probed self-supervised pre-trained models as our model $f_{\\text{probed}}. Indeed, our preliminary experimental results on Caltech101, utilizing ResNet-50-SimCLR as the pre-trained model, are as follows: Ensemble + L2SP attains ACC=0.875 and NLL=0.441, whereas NPTL attains ACC=0.879 and NLL=0.410. We believe that integrating the outcomes of self-supervised pre-training will undoubtedly enhance the paper, and we are grateful for constructive feedback. Additional results regarding self-supervised pre-training will be available in the camera-ready version.\n\n__Q1 (2) Experiments on semantic segmentation__\n\nUnderstanding semantic segmentation as pixel-level classification would simplify the implementation of the proposed NPTL in semantic segmentation experiments. More precisely, we first obtain $f_{\\text{probed}}$ by exclusively training additional modules for semantic segmentation tasks while keeping the fixed (self-supervised) pre-trained backbone network. Subsequently, we proceed to train the entire semantic segmentation model, incorporating the proposed NPTL term.\n\nIn this manner, there is no obstacle to the conceptual application of our NPTL methodology to semantic segmentation tasks. We are of the opinion that providing additional experimental results for semantic segmentation, as per your suggestion, will enhance the emphasis on our contribution. Nevertheless, due to practical limitations like restricted computational resources and time constraints, we regretfully acknowledge that we may be unable to provide additional experiments on semantic segmentation during the rebuttal period. Even if the results are not ready by then, we will be committed to incorporating them into the camera-ready version. We earnestly request your understanding and consideration of this matter for your assessment."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699937687027,
                "cdate": 1699937687027,
                "tmdate": 1699937687027,
                "mdate": 1699937687027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wp6XmT6og3",
                "forum": "vSwu81S33z",
                "replyto": "nyWr6KSeIO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Additional Results for semantic segmentation experiments"
                    },
                    "comment": {
                        "value": "Following your constructive suggestion, we expanded our experiments on semantic segmentation using the PASCAL VOC 2012 dataset, building upon [1]. Our backbone model was the pre-trained ResNet-50, while we adopted the DeepLabv3+ model as our complete model. Our experiments, apart from our model, were consistent with those in [1]. Table R.3 presents conclusive evidence that NPTL surpasses other baseline models.\n\n__Table R.3.__ Additional BMA Mean IoU performance of (a) SGLD with Non-Learned Prior, (b) SGLD with Learned Prior Supervised, (c) SGLD with Learned Prior SSL, and (d) NPTL with ResNet-50.\n|  Dataset   | Backbone | (a) | (b) | (c) |(d)|\n| :-: | :-:          | :-:          | :-:          | :-:          |:-:          |\n| PASCAL VOC 2012 | ResNet-50 | 73.16 | 73.72 | 74.15 | 74.72|\n\nIt would greatly assist us if you could indicate any further inquiries or uncertainties you might have regarding our paper.\n\n__References__\n\n[1] Shwartz-Ziv, Ravid, Micah Goldblum, Hossein Souri, Sanyam Kapoor, Chen Zhu, Yann LeCun, and Andrew G. Wilson. \"Pre-train your loss: Easy bayesian transfer learning with informative priors.\" Advances in Neural Information Processing Systems 35 (2022): 27706-27715."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539407913,
                "cdate": 1700539407913,
                "tmdate": 1700540363837,
                "mdate": 1700540363837,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vphz5EdGvl",
                "forum": "vSwu81S33z",
                "replyto": "nyWr6KSeIO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_izux"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_izux"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "I have read the response and I will keep my score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612385358,
                "cdate": 1700612385358,
                "tmdate": 1700612385358,
                "mdate": 1700612385358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "htTjAstfRF",
                "forum": "vSwu81S33z",
                "replyto": "nyWr6KSeIO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your positive and constructive comments. We'll add our additional experiment results in new version of our paper."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621837535,
                "cdate": 1700621837535,
                "tmdate": 1700621837535,
                "mdate": 1700621837535,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3uyNRM4aFV",
            "forum": "vSwu81S33z",
            "replyto": "vSwu81S33z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the applicability of nonparametric transfer learning (NPTL) in the context of Bayesian NNs and transfer learning. The general idea is that having pre-trained models based on NNs, once can obtain good performance on Bayesian model averaging (BMA) prediction on different tasks. For instance, pre-training on Imagenet and testing performance on CIFAR or similar vision datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "In general, I think it is a good paper with positive ideas and contributions. I particularly see the interest behind the application of NPTL in this context for transfer learning. To me, the paper is clear in the details concerning the sampling methodology, and perhaps not that much in the problems related to scalability or computational cost (see my comments below)."
                },
                "weaknesses": {
                    "value": "In my opinion, I think there are several points that are not clear enough while reading the manuscript and they could be also potential weaknesses of the method.\n\n[w1] --- Access to the posterior distribution given pre-trained models. In general, I see the idea, but it is not entirely clear to me how we can get posterior samples from any model that has been pre-trained without a particular prior before. Are we in the MAP solution? similarly to the Laplace approximation. Are there conditions or requirements for pre-training the models?\n\n[w2] --- I appreciate the details and the sincere comments on the heuristics used and so on. However, I do not get a good feeling on the scalability and the computational cost. Is really the methods playing a role given huge models with large number of parameters? or is it just bc the pre-trained models are doing still well on similar vision tasks. In that regard, I'm not entirely convinced by the empirical results."
                },
                "questions": {
                    "value": "see my prev. comments"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6913/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761592473,
            "cdate": 1698761592473,
            "tmdate": 1700222241247,
            "mdate": 1700222241247,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bH8W0gNqJ0",
                "forum": "vSwu81S33z",
                "replyto": "3uyNRM4aFV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review confirmation request"
                    },
                    "comment": {
                        "value": "We appreciate your dedication to reviewing our paper. However, it seems there might be a confusion with other paper, as we don't have any anonymous references in our paper or reliance on a theory submitted to ICLR 2024 in another submission. Could you please double-check and confirm your observations? Thank you."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699657577033,
                "cdate": 1699657577033,
                "tmdate": 1699658440901,
                "mdate": 1699658440901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YEnCw76YuJ",
                "forum": "vSwu81S33z",
                "replyto": "3uyNRM4aFV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "As, we have identified what seems to be the accurate review for our paper in this link: https://openreview.net/forum?id=AweVGJeW47&noteId=4ovgy5lYGf.\n\nWe write responses based on that review.\n\nThank you for your constructive and positive comment. We will summarize and respond to the questions below:\n\n__W1 How we can get posterior samples from any model that has been pre-trained without a particular prior before. Are there conditions or requirements for pre-training the models?__\n\nThank you for pointing out an important question. As we commented on the question from reviewer izux, there is no restriction to constructing pre-trained models. Since the informative base measure $F_\\pi(x, y)$ could be __any probability measure representing our initial assumption about $F_0$ based on our prior knowledge of the actual data distribution__, we have the flexibility to employ any type of pre-trained models that contain information of generating process of the upstream dataset (MAP solution from supervised tasks or solution from self-supervised).\n\n__W2 (1) Is the method really playing a role given huge models with a large number of parameters?__\n\nAs you pointed out, there is a trend in modern transfer learning to assume large pre-trained models on extensive upstream data. Consequently, it would be essential to assess the scalability of transfer learning approaches. To this end, we conducted experiments employing the sizable ViT-B/16 and RoBERTa-Base models (Specifically, ResNet-50 has approximately 23 million parameters, whereas ViT-B/16 and RoBERTa-Base have 86 million and 123 million parameters, respectively). We believe that our experiments using ViT and RoBERTa adequately validate the scalability of the proposed NPTL methodology.\n\n__W2 (2) Is it just because the pre-trained models are doing still well on similar vision tasks?__\n\nYour question brings up an intriguing point. Indeed, a well-pretrained model tends to perform effectively across related tasks. Yet, if the prior distribution isn't suitable, it can significantly impact the performance of the Bayesian model averaging (BMA) with posterior samples. To demonstrate this, we showcase the BMA performance using the zero-mean isotropic Gaussian prior, denoted as $\\mathcal{N}(0,\\sigma^2 * I)$. The empirical results in Table R.1. and Table R.2. distinctly illustrate how an unsuitable prior distribution can markedly degrade performance.\n\n__Table R.1.__ Additional BMA accuracy performance of (a) SGHMC with zero-mean isotropic Gaussian prior and (b) NPTL with ResNet-20x4.\n|     | CIFAR-10 | CIFAR-100 | Food-101 | Dogs-120 |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| (a) | 0.872 \u00b1 0.000 | 0.625 \u00b1 0.001 | 0.550 \u00b1 0.001 | 0.235 \u00b1 0.001 |\n| (b) | 0.964 \u00b1 0.001 | 0.818 \u00b1 0.000 | 0.644 \u00b1 0.002 | 0.634 \u00b1 0.001 |\n\n__Table R.2.__ Additional BMA negative log-likelihood performance of (a) SGHMC with zero-mean isotropic Gaussian and (b) NPTL with ResNet-20x4.\n|     | CIFAR-10 | CIFAR-100 | Food-101 | Dogs-120 |\n| :-  | :-:          | :-:          | :-:          | :-:          |\n| (a) | 0.381 \u00b1 0.001 | 1.321 \u00b1 0.001 | 1.733 \u00b1 0.003 | 3.071 \u00b1 0.002 |\n| (b) | 0.102 \u00b1 0.001 | 0.606 \u00b1 0.002 | 1.347 \u00b1 0.001 | 1.297 \u00b1 0.003 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700020599108,
                "cdate": 1700020599108,
                "tmdate": 1700020599108,
                "mdate": 1700020599108,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "klyAsU43aV",
                "forum": "vSwu81S33z",
                "replyto": "YEnCw76YuJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
                ],
                "content": {
                    "title": {
                        "value": "Apologies and correction of \"swapped\" review"
                    },
                    "comment": {
                        "value": "I sincerely want to apologize to the authors (AC, PCs and the rest of reviewers) for my mistake on the initial review, which I accidentally swapped with another submission with \"similar\" ID number. Since my original (draft) review (out of the openreview system) was rather positive (soundness, presentation, and contribution were good in my opinion with an acceptance score recommendation), I also want to apologize to the authors for receiving a review with a reject recommendation, particularly, if this has caused a bad impression of the reviewing process of ICLR, which I remark is still rigorous and aware of the mistake I accidentally committed. Last but not least, I want to remark authors' effort to find the correct review in the openreview blind system and make a rebuttal according to the comments I included there.\n\n---------------------------------------\n\nJust for future reference, the correct review of this paper was initially this one and is the one the authors used for the rebuttal (it is also updated as main review)\n\n**Summary:** The paper studies the applicability of nonparametric transfer learning (NPTL) in the context of Bayesian NNs and transfer learning. The general idea is that having pre-trained models based on NNs, once can obtain good performance on Bayesian model averaging (BMA) prediction on different tasks. For instance, pre-training on Imagenet and testing performance on CIFAR or similar vision datasets.\n\n**Strengths:** In general, I think it is a good paper with positive ideas and contributions. I particularly see the interest behind the application of NPTL in this context for transfer learning. To me, the paper is clear in the details concerning the sampling methodology, and perhaps not that much in the problems related to scalability or computational cost (see my comments below).\n\n**Weaknesses:** In my opinion, I think there are several points that are not clear enough while reading the manuscript and they could be also potential weaknesses of the method.\n\n[w1] --- Access to the posterior distribution given pre-trained models. In general, I see the idea, but it is not entirely clear to me how we can get posterior samples from any model that has been pre-trained without a particular prior before. Are we in the MAP solution? similarly to the Laplace approximation. Are there conditions or requirements for pre-training the models?\n\n[w2] --- I appreciate the details and the sincere comments on the heuristics used and so on. However, I do not get a good feeling on the scalability and the computational cost. Is really the methods playing a role given huge models with large number of parameters? or is it just bc the pre-trained models are doing still well on similar vision tasks. In that regard, I'm not entirely convinced by the empirical results."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215161614,
                "cdate": 1700215161614,
                "tmdate": 1700215161614,
                "mdate": 1700215161614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HKuctm1Cle",
                "forum": "vSwu81S33z",
                "replyto": "3uyNRM4aFV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_bi4C"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' rebuttal."
                    },
                    "comment": {
                        "value": "Thanks to the authors for the answers to my questions included in the (correct) review. Some short comments to their points to make a follow up message on this rebuttal period.\n\n[W1] --- Ok, I see it now, also after section (2) and using section 2.3 and 2.4 from Fong et al. (2019). It makes sense to me, thank you.\n\n[W2A] --- I agree that the order of parameters considered makes sense given the proposed method and approach for probabilistic transfer learning. To me sounds convincing. Additionally, the sort of models considered is also in line with other recent papers doing transfer learning approaches in similar scenarios like in (Matena and Raffel, NeurIPS 2022). Thanks for clarifying this out.\n\n[W2B] --- Thanks for running out additional experiments to address my question. I see the point of author's response and makes sense to me. I also liked the experiment conducted. It would be definitely interesting to test this sort of impact with pre-trained models in the future. But I also understand it could be out of the paper's current scope. \n\nIn general, I am satisfied with authors' contributions and answers in this rebuttal. Therefore, I will raise my score accordingly.\n\n\n\nMatena and Raffel. Merging Models with Fisher-Weighted Averaging. NeurIPS 2022"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222197901,
                "cdate": 1700222197901,
                "tmdate": 1700222217340,
                "mdate": 1700222217340,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RD2MMLB7ry",
            "forum": "vSwu81S33z",
            "replyto": "vSwu81S33z",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6913/Reviewer_vw7S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6913/Reviewer_vw7S"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces nonparametric transfer learning (NPTL), which uses a Dirichlet process prior with centering measure $F_{\\pi}$ defined by downstream samples and their outputs from a linear-probed model. Then each posterior sample defines a weighted loss of the downstream samples and pseudo samples from the linear-probed model. The weighted loss is optimized for each posterior sample. Then Bayesian model averaging is performed over $M$ posterior samples. Extensive experiments show the superior performance of the proposed method over traditional Bayesian sampling methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Using downstream samples and a linear prob model to construct prior for transfer learning is an interesting idea, and with Dirichlet Processes, the posterior gives a simple form. The paper provides extensive numerical experiments to support the superior performance of the proposed method. The limitation of Bayesian model averaging: the computational cost is also properly discussed."
                },
                "weaknesses": {
                    "value": "I found some details of the proposed algorithm a bit confusing, especially in the context of transfer learning. I want to clarify the following points\n\n1. For the linear prob model, my understanding is that $\\phi^*$ is from the pre-trained model, how do we get $W^*$, is it obtained by only fitting the last fully connected layer on the downstream task?\n\n2. In step 7 of algorithm 1, how is $\\theta$ initialized, randomly or by $(\\phi^*, W^*)$, i.e. is the pre-trained model only used to create pseudo samples for the prior, or is it also used to initialize parameter as well. Looking at the training setting, e.g. ResNet-50, with a cosine learning rate decay schedule, it looks like the parameters are trained from scratch, otherwise, I imagine the large initial learning rate would drive parameters away from the initialization. \n\n3. For the SGHMC baseline, how is the prior specified and how does the pre-trained model help the SGHMC method? And how does the pre-trained model help the ensemble baseline?"
                },
                "questions": {
                    "value": "Please see the questions in the weakness above. Some minor questions\n\n1. In section 2.1 paragraph 1. the zero mean isotropic Gaussian is called a non-informative prior. If I remember correctly, the non-informative prior usually refers to something else.\n\n2. For the Bayesian inference, one criterion of setting prior can be ensuring posterior concentration, e.g. in the sense of theorem 2.1 in [1]. Can the authors comment a bit on how the proposed prior related to those concepts?\n\n### Reference\n[1] Ghosal, Subhashis, Jayanta K. Ghosh, and Aad W. Van Der Vaart. \"Convergence rates of posterior distributions.\" Annals of Statistics (2000): 500-531."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6913/Reviewer_vw7S"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6913/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821250287,
            "cdate": 1698821250287,
            "tmdate": 1699636804096,
            "mdate": 1699636804096,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NoyV4rspCQ",
                "forum": "vSwu81S33z",
                "replyto": "RD2MMLB7ry",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your positive and constructive reviews and comments. We will summarize and respond to individual comments below: \n\n__W1 How do we get W*?__\n\nYour understanding is correct. We specifically fit the last fully-connected layer for the downstream task, ensuring that our $(\\phi^\\ast, W^\\ast)$ retains the essence of the upstream data-generating process in $\\phi^\\ast$ and transfers the knowledge from the upstream through $W^\\ast$.\n\n__W2 Initialization and learning rate.__\n\nOur proposed posterior sampling method is tailored for transfer learning scenarios, and we initiated the model parameters as $\\mathbf{\\theta} = (\\phi^\\ast, W)$, where $\\phi^\\ast$ represents a pre-trained feature extractor parameter, and $W$ is the randomly initialized task-specific head parameter. The linear-probed head parameter $W^\\ast$ is solely utilized in constructing the informative base measure for NPTL (although it could potentially be initialized with $W^\\ast$, we assert that using the same random initialization for all methods offers a more equitable basis for experimental comparison).\n\nAs you pointed out, when fine-tuning from a pre-trained solution, it is crucial to be cautious about avoiding substantial deviations from it. In our ResNet-50 experiments, we opted for a learning rate of 0.01, smaller than the conventional value used in from-scratch training (e.g. 0.1 in practice). Consequently, we did not encounter situations where the model significantly deviated from the pre-trained initialization. Additionally, when testing larger learning rate values such as 0.1 and 0.03, we observed a significant performance decline in line with your concern, indicating a departure from the pre-trained initialization.\n\n__W3 For the SGHMC baseline, how is the prior specified and how does the pre-trained model help the SGHMC method? And how does the pre-trained model help the ensemble baseline?__\n\nFor the SGHMC baseline, we employed two distinct weight prior distributions as outlined below:\n1. **L2SP:** Gaussian prior with a mean set as the pre-trained weight and a variance of $\\sigma^2 * I$, where $I$ represents the identity matrix.\n2. **PTYL:** Gaussian prior with a mean derived from the pre-trained weight, further trained on the upstream dataset (acknowledged as challenging to utilize, as emphasized in the paper). The variance is a non-diagonal matrix $\\Sigma$ constructed with empirical covariance using weights gathered during additional training on the upstream dataset.\n\nConsidering the context of posterior sampling in the transfer learning scenario, both the ensemble baselines and the SGHMC method adopt the same initialization approach. Specifically, we initialize our method and all the baselines with pre-trained feature extractor parameters $\\phi$ and randomly initialized task-specific parameters $W$.\n\n\n__Q1 The zero mean isotropic Gaussian is called a non-informative prior.__\n\nThank you for bringing this confusion to our attention. We will seek more suitable language and make the necessary corrections.\n\n__Q2 For the Bayesian inference, one criterion of setting prior can be ensuring posterior concentration. Can the authors comment a bit on how the proposed prior related to those concepts?__\n\nThank you for highlighting the importance of posterior concentration. As NPL is based on the Dirichlet Process, we can leverage standard results. In fact, more can be said than the posterior contraction rate - for example, Theorem 12.2 of Ghosal & Van der Vaart (2017) gives a Bernstein-von Mises theorem for the Dirichlet process.   \n\n__References__\n\n[1] Ghosal, S., & Van der Vaart, A. (2017). Fundamentals of nonparametric Bayesian inference (Vol. 44). Cambridge University Press."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699860937096,
                "cdate": 1699860937096,
                "tmdate": 1699861913835,
                "mdate": 1699861913835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cjIttGVW4M",
                "forum": "vSwu81S33z",
                "replyto": "RD2MMLB7ry",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response request"
                    },
                    "comment": {
                        "value": "Your contribution to reviewing our paper is much appreciated. With the discussion period deadline nearing, could you kindly respond to our rebuttals? It would greatly assist us if you could indicate any further inquiries or uncertainties you might have regarding our paper."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700447677496,
                "cdate": 1700447677496,
                "tmdate": 1700447677496,
                "mdate": 1700447677496,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "okSihFAp7J",
                "forum": "vSwu81S33z",
                "replyto": "NoyV4rspCQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_vw7S"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Reviewer_vw7S"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' detailed response. The clarification of the algorithm makes sense in the transfer learning context. I don't have further concerns, and I will keep my positive rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453380155,
                "cdate": 1700453380155,
                "tmdate": 1700453380155,
                "mdate": 1700453380155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xjQ0M74JIp",
                "forum": "vSwu81S33z",
                "replyto": "RD2MMLB7ry",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6913/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comments by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your valuable and positive comments. We will make the correction in the revised version of our paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6913/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453965815,
                "cdate": 1700453965815,
                "tmdate": 1700453965815,
                "mdate": 1700453965815,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]