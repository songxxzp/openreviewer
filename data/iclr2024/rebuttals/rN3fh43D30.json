[
    {
        "title": "Enhancing Length Extrapolation in Sequential Models with Pointer-Augmented Neural Memory"
    },
    {
        "review": {
            "id": "8MMKcUONvI",
            "forum": "rN3fh43D30",
            "replyto": "rN3fh43D30",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4232/Reviewer_VDhU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4232/Reviewer_VDhU"
            ],
            "content": {
                "summary": {
                    "value": "* First, I want to highlight that I served as a reviewer for this manuscript during its submission to NeurIPS 2023. It is noticeable that several amendments, which were assured to the four reviewers, are absent in the current version presented at ICLR 2024. I strongly suggest that the authors should revise the manuscripts as promised either in a camera-ready version if accepted, or the next submission or arxiv if rejected this time.\n\nIn this paper, the capabilities of the \"Pointer-Augmented Neural Memory (PANM)\" system are extensively explored, demonstrating its proficiency in managing long data sequences through the integration of an external memory that operates on physical addressing. The system significantly enhances the performance of RNNs/Transformers in reasoning tasks. PANM achieves this by employing pointers for memory content retrieval and mastering a variety of operations via comprehensive end-to-end training, as is validated by experimental results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper introduces a novel method for symbolic processing in neural sequence modeling, utilizing \"physical pointers\" for enhanced generalization to longer sequences and manipulating pointers for symbolic processes. This versatile plug-and-play module seamlessly integrates with various neural models.\n\n2. The innovative approach of employing a pointer to govern memory decoding stands out, with even a straightforward GRU model yielding impressive performance, showcasing the method's effectiveness and potential applications."
                },
                "weaknesses": {
                    "value": "1. The paper's limited evaluation tasks hinder its research impact, despite effectively enhancing Transformers and LLMs with symbolic reasoning. Its performance in broader applications is unconvincing, focusing mainly on QA tasks already validated by other networks. A wider range of NLU and NLG tasks is needed to prove its versatility and value.\n\n2. The baselines used for comparison are outdated, and large language models (LLMs) with over 10B parameters could outperform the proposed method on reasoning tasks. The paper should either compare the method with these LLMs or show how PANM enhances LLMs' reasoning abilities. \n\n3. The paper downplays the similarities between PANM and the Transformer, particularly in terms of positional encodings and attention computations. The current notation makes these connections unclear, and the paper should work to make them more explicit. By presenting PANM as a modification of the Transformer, rather than something entirely different, the distinctions and innovations of PANM could be more easily understood. The choice of notation, such as using h and g instead of query and key, further obscures the connection, and should be addressed to enhance clarity."
                },
                "questions": {
                    "value": "1. Why not conduct an experiment on LLM? I do think some billion-level network could be used in normal research lab, espcially with advanced packages such as DeepSpeed, etc."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4232/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698605771593,
            "cdate": 1698605771593,
            "tmdate": 1699636390355,
            "mdate": 1699636390355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KsuxyTPd85",
                "forum": "rN3fh43D30",
                "replyto": "8MMKcUONvI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4232/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4232/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer VDhU"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review. We address your concerns and questions point by point below.\n### **Summary**\n**\"First, I want to highlight ...\"**. Could you specify the amendment you expected to see in our paper? We will try to address them if they are appropriate. \n### **Weaknesses**\n1.  Thank you for sharing your opinions. We appreciate your insights and agree that expanding the evaluation tasks to a wider range of NLU and NLG applications would be beneficial. However, we do not think that lacking wider NLP evaluations is a good reason to reject a paper that focuses on improving the symbol-processing capability of general deep learning models. Generalization to longer sequences is a crucial aspect with far-reaching implications in various AI applications, particularly in addressing the core limitation of current deep learning architecture. While we acknowledge the importance of broader evaluations, we believe that our paper makes a meaningful contribution by emphasizing and advancing the field's understanding of symbol processing and length extrapolation within the context of general deep learning models. Further, we included experiments on question-answering and machine translation to prove the broad application of our approach. In total, we have conducted experiments on **4 major categories of tasks** with **12 different datasets** and compared our method with **16 baselines** where our method consistently outperforms other models. Therefore, we believe that our evaluation is convincing and comprehensive. \n2. As explained in Appendix D's Baseline choice section, our baselines are competitive with SOTA in the considered tasks. Regarding your mention of LLM, could you clarify the setting when LLM with 10B could outperform our method?  If the LLM is not finetuned on the task, we have shown that Chat-GPT with 175B still fails in our task, and can be considered to underperform our method (Appendix D6). In case the setting is to finetune LLM to the task, we need to conduct experiments to confirm that the finetuned LLM could perform our PANM+LLM, and this task is not trivial because finetuning LLM of 10B requires huge computing resources. \n3. We spent a paragraph in Appendix A to compare positional encoding and our physical pointers. Similarly, we highlighted the difference between our attention and softmax attention used by Transformer in the introduction and Sec. 2.3.1. In this revision, we have added another table (Appendix A Table 3) to summarize the difference between Transformer and our method (see below).\n| Difference | Transformer |\tPANM (Our) |\n| ----------- | ----------- |-----------|\n| Key Generation  | Keys are computed based on input data. Hence, when meeting novel  data during testing, Transformer will observe novel keys, and cannot work properly. | The keys in our approach are  generated as fixed numbers,  specifically physical memory addresses. These keys are entirely separate from the data.|\n|Extendable to Longer Sequences|The dimension of attention weights varies with input length, making arithmetic transformations on these attention weights infeasible as the sequence length increases.|The fixed nature of our physical addresses allows our pointers to be easily manipulated and extendable to longer sequences.|\n|Symbol Processing Advantages| The use of attention weights as implicit pointers may lack the explicitness needed for effective symbol processing.| Using physical addresses as keys in our approach is crucial for symbol processing as it explicitly allows pointer assignment, dereference, and arithmetic operations.|\n|Physical Address vs Positional Encoding| Positional encoding can be generated independently from data. However, they are not separated from the input data as our physical addresses. There is no explicit mechanism in Transformer to attend only to these | Our physical addresses are detached from the data, supporting the transformation of pointers through timesteps and isolating pointer manipulation from the input.|"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4232/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105898359,
                "cdate": 1700105898359,
                "tmdate": 1700105898359,
                "mdate": 1700105898359,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "THotHskBvm",
            "forum": "rN3fh43D30",
            "replyto": "rN3fh43D30",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4232/Reviewer_2DCJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4232/Reviewer_2DCJ"
            ],
            "content": {
                "summary": {
                    "value": "Systematic generalization is the ability of a model to recognize and apply abstract rules to novel contexts that are semantically similar to the training data. Existing models tend to overfit the training data and perform poorly on out-of-distribution samples, such as longer sequences or sequences with novel compositions. The paper discusses the challenge of achieving compositional generalization in neural networks and the proposal of a novel memory architecture called Pointer-Augmented Neural Memory (PANM) to address this issue. \n\nThe work proposes a pointer-based mechanism inspired by both the human brain's symbolic processing through variable binding and computer programs' use of pointers to dynamically access data and programs. PANM explicitly models pointers as physical addresses and strictly isolates pointer manipulation from input data. PANM incorporates a memory structure similar to RAM in Von Neumann's architecture, where each memory slot consists of data and address components. The addresses are generated to explicitly simulate physical memory addresses, allowing for better generalization to longer sequences. To manipulate a pointer, the authors introduce an address bank containing physical addresses corresponding to the input sequence and a neural network called the Pointer Unit responsible for transforming pointers based on attention to the address bank. The paper describes two modes of pointer-based access: pointer dereference (Mode-1) and relational access (Mode-2). \n\nThe authors demonstrate the effectiveness of PANM in symbol-processing domains, including algorithms and context-free grammars, where it works seamlessly with LSTM and StackRNN. PANM is also applied to improve generalization in Transformer models on tasks such as compositional learning, SCAN, and mathematics datasets. The paper concludes the importance of integrating fundamental generalizing principles into existing deep learning models to address their limitations in systematic generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a novel memory architecture, Pointer-Augmented Neural Memory (PANM), which explicitly models pointers as physical addresses. This architecture provides a new approach to enhance compositional generalization in neural networks, addressing a significant challenge in deep learning.\n\nThe method demonstrates improved generalization, especially in processing tasks, such as algorithms, context-free grammars, and compositional learning. It enhances the model's ability to work with longer sequences and compositions.\n\nThe model designed two explicit modes for both of easy and complex ways, and the paper provided the analysis. \n\nThe paper showcases the versatility of PANM by applying it to different tasks, including question-answering and machine translation."
                },
                "weaknesses": {
                    "value": "While PANM is proposed as a solution to compositional generalization, the paper does not provide a comprehensive comparison with existing memory architectures or models designed for similar purposes. It would be helpful to assess its performance against competing approaches.\n\nThe paper focuses on demonstrating the improvements achieved by PANM in specific tasks, such as algorithms and instruction generalization, but it is unknown if the method could be extended to practical adoption in natural language domains.\n\nMore related work for systematic generalization or compositional generalization in language or multimodal perspectives should be discussed. \nE.g.,\nCompositional generalization through meta sequence-to-sequence learning 2019\nA systematic assessment of syntactic generalization in neural language models 2020\nAnalogical reasoning for visually grounded language acquisition 2020\nGeneralization in multimodal language learning from simulation 2021"
                },
                "questions": {
                    "value": "Refer to above comments"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4232/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4232/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4232/Reviewer_2DCJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4232/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699471457576,
            "cdate": 1699471457576,
            "tmdate": 1701034680576,
            "mdate": 1701034680576,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O1tOH5macU",
                "forum": "rN3fh43D30",
                "replyto": "THotHskBvm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4232/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4232/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 2DCJ"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback.  We address your concerns and questions point by point below.\n- **\"While PANM is proposed ...\"**. As explained in Appendix D's Baseline choice section, we have compared with competitive and SOTA baselines in the task we considered. In particular, we have compared our method with **16 baselines** across tasks. Most of them are memory-based models and designed for symbolic generalization. We note that, as explained in Related Work's second paragraph, we have not compared our model with task-specific methods using reinforcement learning or neural-symbolic architecture. Our focus is improving the systematic generalization of fundamental differentiable models.\n- **\"The paper focuses on ...\"**. The reason we focus more on symbolic processing tasks is that these tasks remain very challenging for current deep neural networks and urgently demand new approaches like our solution. That said, as shown in Sec 3.4, we have tested our methods on practical NLP tasks such as Question Answering (QA) and Machine Translation (MT). The significant gains, ranging from 1-6% in QA and 5-40 perplexity scores in MT, show the potential of extending our method to practical NLP domains. \n- **\"More related work for ...\"**. Thank you for your suggestions on the related works. We have incorporated the below discussion in our section \"More Discussion on Related Works\" (Appendix A). Among your suggested papers, the last two works on reasoning and generalization using image inputs, which is beyond the scope of our paper only targeting generalization for longer sequences of text/discrete inputs. The other two papers are more relevant. The approach by Lake et al., 2019 addresses systematic generalization with meta-learning training while our method adopts standard supervised training. The two approaches are complementary rather than competing.  By focusing on improving the model architecture rather than the training procedures, our method can be used in various settings, not just SCAN tasks. The study conducted by Hu et al., 2020 targets a different problem-syntactic generalization, while our paper focuses on length extrapolation with different benchmarks. Some baselines such as LSTM and Transformer examined in the paper have been chosen as baselines in our paper."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4232/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105403029,
                "cdate": 1700105403029,
                "tmdate": 1700105403029,
                "mdate": 1700105403029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MkZXkGgGSj",
            "forum": "rN3fh43D30",
            "replyto": "rN3fh43D30",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4232/Reviewer_fEJf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4232/Reviewer_fEJf"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an external memory module called Pointer Augmented Neural Memory (PANM), leveraging two observations\u2014 1. from neuroscience, for showing that the brain performs symbol processing using neural pointers and variable binding\u2014(so, maybe, it is the architecture that needs an update), and that, 2. computers perform well at symbol processing. PANM is made up of three sub-modules\u2014 1. memory slots, each with a data and an associated address, which the authors represent using incremental binary numbers. They claim this leads to better generalization to longer sequences over approaches that rely on softmax based attention for addressing, 2. a GRU based pointer unit for pointer operations, 3. a controller, which can produce outputs based on the pointer values and (optionally) the input. The aim is to improve the  generalization abilities of conventional sequential models, and this is evaluated on mathematics datasets, more realistic question answering, and machine translation tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Using explicit pointers made of addresses can by design help with generalization to longer sequences by being independent of the sequence length.\n- Experimental: the generalisation results on algorithmic reasoning are stronger than other benchmarks by a huge margin of 10-20%.\n- The paper is extremely readable and well-presented."
                },
                "weaknesses": {
                    "value": "- I didn\u2019t understand the different choices for the baselines for different tasks. For question answering tasks\u2014 why not consider models such as the Universal Transformer, which perform as well as the PANM augmented transformer on SCAN and also perform well on question answering? Is the point here to show PANM also helps transformers on NLP tasks? Or, can a broader claim be made that PAN can also perform better than existing transformer based approaches that perform well on the considered NLP tasks?\n- How many parameters does PANM add? The authors state that DNC and NTM have ~1.2 million parameters, but I think I am missing something, I cannot see the parameter count for PANM.\n- Say I am a practitioner and I want to decide if I want to choose PANM or U. TRM\u2014 looking at the performance of both the models, it is not obvious to me why I should prefer PANM over U. TRM since U. TRM performs very similarly to PANM on the tasks considered\u2014 could you highlight its advantages a bit more strongly over U. TRM, not necessarily in an experimental sense, but intuitively speaking what the benefits/non-benefits of an explicit pointer based memory module system are vis-a-vis a generalised transformer model design wise?\n- The comparison with Chat-GPT on algorithmic tasks is interesting\u2014 in Table 3, what would the failure examples of PANM look like, as are shown for Chat-GPT? Also, I maybe missing something\u2014 but are there any comparison tables on the other tasks apart from the couple of examples\u2014 mathematics, and SCAN? You can just point me to them, or maybe state why looking at them would be or not be relevant.\n\nIt is quite possible that I didn't understand or missed some things, and I am willing to adjust my score accordingly on the resolution of these doubts."
                },
                "questions": {
                    "value": "- Just a small suggestion, but could be helpful to see what p_t^a, *p_t^a, and z_t correspond to in the input sequence in Figure 1 as an example.\n- Possible typo in the first line of section 2.3.1\u2014 PANC instead of PANM.\n- The authors state that to use PANM in a plug-and-play fashion with an encoder-decoder based architecture, one can reduce the number of parameters in the decoder to reduce the overall number of parameters, which will increase by introducing PANM. It is not obvious to me what effects this will have\u2014 could you please elaborate on it? And what does it mean that the parameter counts of PANM and the Transformer baseline are kept the same when PANM is added as an external module\u2014 does this happen by cutting down on the decoder parameters? If so, what parameters are cut down? Let me know if this is a standard practice.\n- Also, is their name for the combined module consisting of the address bank and the data memory?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4232/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699571844098,
            "cdate": 1699571844098,
            "tmdate": 1699636390230,
            "mdate": 1699636390230,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hIUqIYX1HL",
                "forum": "rN3fh43D30",
                "replyto": "MkZXkGgGSj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4232/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4232/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer fEJf"
                    },
                    "comment": {
                        "value": "Thank you for your positive review. We address your concerns and questions point by point below.\n#### **Weaknesses**\n- **\"I didn\u2019t understand ...\"**. Firstly, we would like to correct that our method outperforms Universal Transformer.  Even with Relational Position Encoding, Universal Transformer (UT) falls short of our PANM in SCAN's most challenging scenarios. Specifically, when training lengths are set at 22, 24, and 25, PANM exhibits performance advantages of 2%, 35%, and 29%, respectively. Similarly, in more demanding mathematical tasks, PANM outshines UT+RPE by 11%. I (Universal Transformer without RPE is even worse). In the QA task, we opted against using UT as the backbone primarily due to its slow speed resulting from recurrent computation. Additionally, in our bAbI setting, Transformer and UT+RPE demonstrate minimal performance differences (see our revised Appendix Table 12), thus, we chose to integrate PANM with Transformer. In the SQUAD task, it is well known that BERT serves as a much stronger baseline compared to Universal Transformer. The successful enhancement of a potent baseline like BERT with PANM substantiates the practical efficacy of our approach. Our objective aligns closely with your second hypothesis: to demonstrate that PANM *\"can also perform better than existing transformer based approaches that perform well on the considered NLP tasks\"*\n- **\"How many parameters ...\"**. It depends on the task and choice of the Controller. Usually, PANM's module's size is similar to the size of an attention-layer in Transformer.  We report PANM model size in Appendix D1, the final sentence of the fifth paragraph. In the considering task, the whole architecture with PANM has \u22481.1 million parameters.\n- **\"Say I am a ...\"**.  As clarified earlier, PANM outperforms U. TRM (with and without RPE) in the toughest generalization testbed. We will clarify the benefit of our approach vs Transformer:\n| Difference | Transformer |\tPANM (Our) |\n| ----------- | ----------- |-----------|\n| Key Generation  | Keys are computed based on input data. Hence, when meeting novel  data during testing, Transformer will observe novel keys, and cannot work properly. | The keys in our approach are  generated as fixed numbers,  specifically physical memory addresses. These keys are entirely separate from the data.|\n|Extendable to Longer Sequences|The dimension of attention weights varies with input length, making arithmetic transformations on these attention weights infeasible as the sequence length increases.|The fixed nature of our physical addresses allows our pointers to be easily manipulated and extendable to longer sequences.|\n|Symbol Processing Advantages| The use of attention weights as implicit pointers may lack the explicitness needed for effective symbol processing.| Using physical addresses as keys in our approach is crucial for symbol processing as it explicitly allows pointer assignment, dereference, and arithmetic operations.|\n|Physical Address vs Positional Encoding| Positional encoding can be generated independently from data. However, they are not separated from the input data as our physical addresses. There is no explicit mechanism in Transformer to attend only to these | Our physical addresses are detached from the data, supporting the transformation of pointers through timesteps and isolating pointer manipulation from the input.|\n- **\"The comparison with Chat-GPT ....\"** PANM failure will focus on the end of the sequence. When sequence length increases, more characters at the end of the output sequence are likely wrong. For example, in the reverse task, input 123456789, true output 987654321, PANM output can be 987654322. The reason for this failure pattern is that PANM uses a Pointer Unit to recurrently transform the pointer across timestep. As the transformation is not always perfect, there could be error accumulation as the number of timesteps increases. Looking at Fig 2c as an example, attention errors (red) become clearer at the end of the decoding. Although in this example, the majority of attention is still correct, pointing to the right address. As the test length increases, these errors will amplify and possibly make the final prediction incorrect. We explained the reason for not carefully testing ChatGPT on other tasks in Appendix D6's first paragraph. There are two main reasons: (1) the task input is not suitable for text representation. (2) the test data is public and can be seen by ChatGPT. Another reason is the cost of systematically evaluating ChatGPT on these tasks: we have to manually extract the relevant answer from ChatGPT's output to have a reliable evaluation."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4232/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104679852,
                "cdate": 1700104679852,
                "tmdate": 1700104679852,
                "mdate": 1700104679852,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tSYD2wfFOb",
                "forum": "rN3fh43D30",
                "replyto": "Pd1sadHT7e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4232/Reviewer_fEJf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4232/Reviewer_fEJf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifying comments on my questions and considering the minor edits. I have more clarity on the implementation of the PANM module now."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4232/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333447708,
                "cdate": 1700333447708,
                "tmdate": 1700333447708,
                "mdate": 1700333447708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]