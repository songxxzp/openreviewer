[
    {
        "title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research"
    },
    {
        "review": {
            "id": "8MnoPYR9Lg",
            "forum": "clU5xWyItb",
            "replyto": "clU5xWyItb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8010/Reviewer_vdwn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8010/Reviewer_vdwn"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose an agent-based scientific multiple choice QA system, mostly powered by LLMs and search APIs. They also contribute LitQA - 50 multiple choice questions written by experts."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I have read the author responses and have increased the score by 1 point.\n-------------------------------------------------------\n\nThanks to the authors for their hard work on this agent and dataset. \n\nStrengths:\n- Good performance on the proposed dataset\n- Clear writing\n- Extensive ablations"
                },
                "weaknesses": {
                    "value": "- Is it likely you overfit the entire PaperQA system? There are only 50 questions. Was PaperQA developed after the data was collected? Did you make important system choices based on outcomes on the 50 questions? I think you need a development and test set split and I worry that the 50 questions in LitQA are actually the development set. Table 5 suggests that your system's advantage over GPT + search is actually much smaller than suggested by the deltas in Table 2.\n- Multiple-choice is not realistic. Scientists don't already know what they're trying to solve when they are doing research. I think the entire paper should be the \"No MC options\" row from Table 3. I realize that MC means evaluation is easier, but with only 50 questions, manual evaluation of the various systems in Table 2 is feasible. Please consider rerunning the entirety of table 2 with the \"No MC options\". \n- How *did* you evaluate the No MC options? Exact string overlap? Manually? I think it should be the latter.\n- As I'm sure you know, Table 4 is hard to believe. Could it be because the MC options are included? What happens to the hallucination rate when the MC options are removed? I have done a lot of manual evaluation of many LLMs over the past year and have never seen a 0 hallucination rate. This needs to be more thoroughly understood as a part of this paper."
                },
                "questions": {
                    "value": "- Will you be open sourcing your agent system? \n- In table 3, what is the \"Samples\" column? Is this how many times you ran the entire end-to-end experiment? If so, can you include the standard deviations?\n- Are Perplexity, Scite, and Elicit made to answer multiple-choice questions? I don't believe so. How do they compare in the \"No MC options\" setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8010/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8010/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8010/Reviewer_vdwn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8010/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698090590009,
            "cdate": 1698090590009,
            "tmdate": 1700850098546,
            "mdate": 1700850098546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FeBYmYyIiX",
                "forum": "clU5xWyItb",
                "replyto": "8MnoPYR9Lg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**LitQA - Dataset Size**\n> Is it likely you overfit the entire PaperQA system? There are only 50 questions. Was PaperQA developed after the data was collected? Did you make important system choices based on outcomes on the 50 questions? I think you need a development and test set split and I worry that the 50 questions in LitQA are actually the development set.\n\nThis is a very reasonable concern. However, LitQA was only introduced as a performance metric after the PQA system was built. Thus, the performance on LitQA was not used for any hyperparameter optimization. \n\n**Table 5 - Clarification**\n> Table 5 suggests that your system's advantage over GPT + search is actually much smaller than suggested by the deltas in Table 2.\n\nTo clarify, Table 2\u2019s GPT row is an experiment without any search. We prompt GPT to answer a question from LitQA. In Table 5, we never use GPT-4 + search, there was a typo and we have fixed it in the new version of the paper. GPT-4 + PaperQA search referred to GPT-4 + PaperQA a priori reasoning, where GPT-4 is prompted to either answer the question directly, or use PaperQA as an oracle to help it answer the question.\n\n**LitQA - Multiple Choices**\n> Multiple-choice is not realistic. Scientists don't already know what they're trying to solve when they are doing research. I think the entire paper should be the \"No MC options\" row from Table 3. I realize that MC means evaluation is easier, but with only 50 questions, manual evaluation of the various systems in Table 2 is feasible. Please consider rerunning the entirety of table 2 with the \"No MC options\".\n\nThank you for your feedback on our use of multiple-choice questions. We chose this approach for its ability to provide standardized, objective comparisons across systems, and to manage the practical constraints of our study's scale.\n\nIn addition, while multiple choice questions may seem simplistic compared to the open-ended nature of scientific research, they are designed to encapsulate key elements of the problem-solving process. Each option represents a plausible hypothesis or outcome, mirroring the decision-making process scientists engage in when evaluating multiple possibilities.\n\nNonetheless, we acknowledge the limitations of this method and will consider incorporating more open-ended questions in future research to address these concerns. Poor performance in Table 3\u2019s ablation with No MC options - as you highlight - is a clear motivation to investigate new ways of evaluating open-ended QA settings.\n\n**LitQA - Evaluation**\n> How did you evaluate the No MC options? Exact string overlap? Manually? I think it should be the latter.\n\nWe used GPT-4 to compare provided answer against reference answer. We manually checked one 50 question run to ensure it agreed with experts and validate this method.\n\n**Hallucination**\n> As I'm sure you know, Table 4 is hard to believe. Could it be because the MC options are included? What happens to the hallucination rate when the MC options are removed? I have done a lot of manual evaluation of many LLMs over the past year and have never seen a 0 hallucination rate. This needs to be more thoroughly understood as a part of this paper.\n\nThank you for this comment. We have revised the manuscript to reflect that the dataset on which the hallucination evaluation was conducted was long-form, with no multiple choice options given. The 0% hallucination rate is largely influenced by the RAG step.\n\n**Open-Sourcing**\n> Will you be open sourcing your agent system?\n\nYes, the original PaperQA system without an agent is already open-sourced on one of the author\u2019s GitHub, where the agent version will be open-sourced thereafter.\n\n**Table 2 Clarification**\n> In table 3, what is the \"Samples\" column? Is this how many times you ran the entire end-to-end experiment? If so, can you include the standard deviations?\n\nThat is indeed correct - \u201cSamples\u201d refers to the number of times the experiment with the agent was run end-to-end. We keep averages for \u201cResponse\u201d columns, but will include standard deviations for the \u201cScore\u201d in the camera-ready paper.\n\n**Industrial Competitors**\n> Are Perplexity, Scite, and Elicit made to answer multiple-choice questions? I don't believe so. How do they compare in the \"No MC options\" setting?\n\nWhen we performed these experiments, we prompted PaperQA and all other models identically, meaning that each had the question, along with the multiple choice options. \n\nThe mentioned models are not necessarily designed to answer multiple choice questions, but the multiple choice options serve to provide the model with additional context, rather than limitation in this case. Often the competitor models would answer a long-form answer that matched the context of a provided answer, and of course, in this case, the manual evaluation shows correct."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698809037,
                "cdate": 1700698809037,
                "tmdate": 1700698809037,
                "mdate": 1700698809037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j9F1pNGBQr",
            "forum": "clU5xWyItb",
            "replyto": "clU5xWyItb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8010/Reviewer_CSpW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8010/Reviewer_CSpW"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes PaperQA, an agent that answers questions about scientific literature according to the search results. The agent is composed of three tools: search, gather evidence, and answer the question. It can find and parse relevant full-text research papers, identify specific sections in the paper that help answer the question, summarize those sections with the context of the question (called evidence), and then generate an answer based on the evidence. Compared to a standard retrieval-augmented generative (RAG) agent, PaperQA decomposes parts of a RAG and provides them as tools to an agent. It can adjust the input to paper searches, gather evidence with different phrases, and assess if an answer is complete."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. PaperQA decomposes parts of a RAG and provides them as tools to an agent, and it can adjust the input to paper searches, gather evidence with different phrases, and assess if an answer is complete. \n2. PaperQA makes use of a priori and a posteriori prompting, tapping into the latent knowledge in LLMs.\n3. PaperQA outperforms all models tested and commercial tools, and is comparable to human experts on LitQA on performance and time."
                },
                "weaknesses": {
                    "value": "1. The paper has some innovation, but it still feels limited. Firstly, the dynamic use of the three tools is quite similar to the ReAct framework, all of which are dynamically autonomous in determining whether to retrieve them again. Secondly, the number of benchmarks constructed is relatively small, with only 50 questions and a multiple-choice format. Existing research has shown that the form of multiple choice questions has limitations in evaluating model performance, and the model is more often used to generate longer texts. Therefore, there is a significant gap between the form of multiple choice questions and practical applications.\n2. In the experiment, there is a lack of comparison with some advanced agent frameworks, which often consider the dynamic nature of intermediate steps. Therefore, it is necessary to increase the comparison with these frameworks, such as ReAct and Reflexion. The main experiment is conducted on multiple choice questions, which has limitations because hallucinations typically occur when the generated text is long. During the hallucination evaluation experiment, some details were not clearly written, such as whether other LLMs used search tools."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8010/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698504484648,
            "cdate": 1698504484648,
            "tmdate": 1699636987484,
            "mdate": 1699636987484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XxPtd5WW9J",
                "forum": "clU5xWyItb",
                "replyto": "j9F1pNGBQr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Other Agent Frameworks**\n> Firstly, the dynamic use of the three tools is quite similar to the ReAct framework, all of which are dynamically autonomous in determining whether to retrieve them again. \nIn the experiment, there is a lack of comparison with some advanced agent frameworks, which often consider the dynamic nature of intermediate steps. Therefore, it is necessary to increase the comparison with these frameworks, such as ReAct and Reflexion.\n\nWe thank the reviewer for this comment. We do not necessarily need to compare with other agent frameworks, because we are not proposing a new agent framework. We rather propose a system built on top of an arbitrary agent framework, able to think and use tools. \n\nMore specifically, we use the OpenAIFunctionsAgent from LangChain. Nevertheless, we also tested the ReAct agent without any major difference in performance. If required, we are happy to provide those results as an ablation in the final version of the paper.\n\n**LitQA**\n> Secondly, the number of benchmarks constructed is relatively small, with only 50 questions and a multiple-choice format. Existing research has shown that the form of multiple choice questions has limitations in evaluating model performance, and the model is more often used to generate longer texts. Therefore, there is a significant gap between the form of multiple choice questions and practical applications.\n\nThe 50 question LitQA dataset was laboriously created to evaluate PaperQA  on a small subsection of papers that the underlying LLMs had not seen in training data. We also evaluated PaperQA on several existing evaluation benchmarks that are multiple choice. \n\nWhile we agree that multiple choice evaluations have their limitations, the multiple choice format allows us to properly evaluate performance across multiple models, as well as humans, without any bias. Thus, this evaluation metric has benefits that outweigh the mentioned limitations. \n\n**Hallucination**\n> The main experiment is conducted on multiple choice questions, which has limitations because hallucinations typically occur when the generated text is long. During the hallucination evaluation experiment, some details were not clearly written, such as whether other LLMs used search tools.\n\nThe hallucination evaluation was carried out in long-form, requiring all models to give citations to support all claims. Then, every citation (with respect to the corresponding claim, when applicable) was evaluated manually to determine whether it was hallucinated or not. \n\nThe hallucination experiment was conducted to determine hallucinated citations, showing the power of PaperQA to produce claims, backed by correct and valid citations, compared to simple LLMs (no tools)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698627043,
                "cdate": 1700698627043,
                "tmdate": 1700698627043,
                "mdate": 1700698627043,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cQs6IGInBd",
            "forum": "clU5xWyItb",
            "replyto": "clU5xWyItb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8010/Reviewer_afKA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8010/Reviewer_afKA"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents PaperQA, a tool developed with retrieval-augmented generation (RAG) technique to answer science questions. They also proposed LitQA, new benchmark to assess the performance of RAG models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Authors introduce new components to the standard RAG pipeline (e.g., search, map-reduce the summary, repeat for more evidence)\n* Adaptive and modular framework and an implementation with open source libraries."
                },
                "weaknesses": {
                    "value": "* This paper appeared to be more product or application specific than focused on the underlying research problems. Unfortunately, no research problem was mentioned in the text.\n* Ask LLM prompt assess the parametric knowledge, which is feed to the evidence contexts. Authors found this knowledge is helpful. But I do not agree with the reasons provided. For example, what would happen if there are knowledge conflicts raised with the parametric knowledge and retrieved knowledge?\n> \u201cSurprisingly enough, not using the LLM\u2019s latent knowledge (no ask LLM) also hurts performance, despite the benchmark being based on information after the cutoff date \u2013 we suggest that the useful latent knowledge we find LLMs to possess in Table 5 helps the agent use the best pieces of evidence.\u201d \n* I don\u2019t think the following claim is true. Having a low rate of incorrect answers does not suggest that the model is certain, in fact, one needs to measure the uncertainty in the generated answers to make such a claim.\n\u201cFurthermore, we see the lowest rate of incorrectly answered questions out of all tools, which rivals that of humans. This highlight\u2019s PaperQA\u2019s ability to be certain about its answers.\u201d\n* This is a bad analogy, I don\u2019t think that human judgmental time should need to correlate with the time taken to complete OpenAI API calls.\n> \u201cIt took PaperQA on average about 2.4 hours to answer all questions, which is also on par with humans who were given 2.5 hours.\u201d"
                },
                "questions": {
                    "value": "* What kind of reasoning required in this case? I can only find the task is to measure the relevance of the query to the retrieved passages. It is intriguing why authors opt out the model to explain the score.\n> \u201cthe LLM\u2019s ability to reason over text and provide numerical relevance scores for each chunk of text to the query question.\u201d\n> \u201cAt the end of your response, provide a score from 1-10 on a newline indicating relevance to question. Do not explain your score\u201d\n* How authors reliably make the claim of the GPT4 cut off date? Which GPT4 version used in the study?\n> \u201cWe take special care to only collect questions from papers published after the GPT-3.5/4 cutoff date in September 2021\u201d\n* Is it expected that biomedical researchers cannot answer these question without internet? Or is this setup introduced to mimic the RAG styled QA by asking only to answer given what they find on the internet?\n> \u201cWe recruited five biomedical researchers with an undergraduate degree or higher to solve LitQA. They were given access to the internet and given three minutes per question (2.5 hours in total) to answer all questions\u201d\n* This is unexpected, any reasons? Is the context length a factor to correlate with the summarization performance?\n> \u201cInterestingly, we observe that using GPT-4 as the summary LLM worsens overall performance.\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8010/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797491655,
            "cdate": 1698797491655,
            "tmdate": 1699636987361,
            "mdate": 1699636987361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gj2EFxhhfF",
                "forum": "clU5xWyItb",
                "replyto": "cQs6IGInBd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**No Research Problem**\n> This paper appeared to be more product or application specific than focused on the underlying research problems. Unfortunately, no research problem was mentioned in the text.\n\nWe appreciate this critique and agree that a clear problem statement should be defined. Thus, we have adjusted our manuscript to reflect this. We tackled the three following problem statements:\n1. *Does the inclusion of a LLM-driven, non-linear RAG workflow aid in carrying out scientific question-answering?*\nIn this question, we were particularly interested in PaperQA as an agent system, where the pieces of RAG could be performed modularly and iteratively as needed, without human interaction. We found in our ablation study that PaperQA, which was allowed to follow any ordering and number of tools, outperformed Vanilla RAG, which was required to follow a linear workflow. \n2. *Are LLMs augmented with RAG-style tools superior on established benchmarks?* \nFor this question, we evaluated our method, as well as other methods, on established benchmarks and found that PaperQA indeed does reach higher performance, suggesting the RAG-style tools do indeed boost performance. \nA sub-question to this is whether our full-text document and chunk summary methods boost performance, which we indeed confirm in our manuscript.\n3. *Does hierarchical RAG with access to full-text scientific articles lead to robust answers without hallucinated references?*\nWe evaluated this and compared to LLMs with no tools, and we found that indeed, while plain LLMs do hallucinate citations (which is expected), our method is robust.\n\n**Ask LLM**\n> Ask LLM prompt assess the parametric knowledge, which is feed to the evidence contexts. Authors found this knowledge is helpful. But I do not agree with the reasons provided. For example, what would happen if there are knowledge conflicts raised with the parametric knowledge and retrieved knowledge?\n\nThis is an excellent point. We have added an example outlining this in the appendix (Appendix G).\n\nIn summary, the role of background information is crucial in shaping the model's responses: when it contradicts the context, the model may refrain from answering due to the conflict, whereas accurate and supportive background information enables the model to provide detailed and informed responses. \n\n**Precision & Certainty**\n> I don\u2019t think the following claim is true. Having a low rate of incorrect answers does not suggest that the model is certain, in fact, one needs to measure the uncertainty in the generated answers to make such a claim. \u201cFurthermore, we see the lowest rate of incorrectly answered questions out of all tools, which rivals that of humans. This highlight\u2019s PaperQA\u2019s ability to be certain about its answers.\u201d\n\nWe agree that our wording was imprecise. We do not mean to say that PQA is more certain, but rather that it is better calibrated to express uncertainty when it is uncertain. In comparison, GPT-4\u2019s RLHF training results in poor calibration in this respect, as it tries to give full answers to please the asker. This means that it is more confident than it should be in some cases. Our system, with suitable prompt engineering, alleviates this. We have changed the section highlighted to the following: \n\n\u201cFurthermore, we see the lowest rate of incorrectly answered questions out of all tools, which rivals that of humans. This emphasizes that PaperQA is better calibrated to express uncertainty when it actually is uncertain.\u201d\n\n**Human vs. PQA Time**\n> This is a bad analogy, I don\u2019t think that human judgmental time should need to correlate with the time taken to complete OpenAI API calls.\n\nWe understand the reviewers concern, but here we do not claim that these two correlated, but that rather PQA does not necessarily take longer and thus can be competitive with humans on performance, as well as time."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698446657,
                "cdate": 1700698446657,
                "tmdate": 1700698446657,
                "mdate": 1700698446657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d4t4oPhQSj",
                "forum": "clU5xWyItb",
                "replyto": "cQs6IGInBd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Summary LLM**\n> What kind of reasoning required in this case? I can only find the task is to measure the relevance of the query to the retrieved passages. It is intriguing why authors opt out the model to explain the score.\n> \u201cthe LLM\u2019s ability to reason over text and provide numerical relevance scores for each chunk of text to the query question.\u201d \u201cAt the end of your response, provide a score from 1-10 on a newline indicating relevance to question. Do not explain your score\u201d\n\nDue to context length limits, we provided 8 contexts for the final answer. To populate these 8, we performed the summary over 20 potential source documents. This is because some will likely be irrelevant. However, to downsample from 20 to 8, we need a way to rank the documents to choose the best 8. One method would be to have multiple LLM calls to provide relative rankings while avoiding context limits. However, our approach was to have the summary LLM to provide an estimated numerical ranking that aids in ranking the summaries.  We did not explore the calibration or agreement with experts on this number - the scores are a quick concurrent method used to downselect the summaries to 8.\n\n**GPT Version**\n> How authors reliably make the claim of the GPT4 cut off date? Which GPT4 version used in the study? \u201cWe take special care to only collect questions from papers published after the GPT-3.5/4 cutoff date in September 2021\u201d\n\nThe GPT-3.5 version used was gpt-3.5-turbo-0613 and GPT-4 version was gpt-4-0613. We have added technical details for all accessed and utilized models in the SI.\n\nIn terms of the cutoff date, we rely on OpenAI\u2019s documentation where they specifically mention all training data originates from before September 2021. Moreover, the poor results of Claude-2 and GPT-4 in Table 2 show that most of the papers were unlikely to be in the training dataset. We still however see that both models appear to have some inner knowledge to help them be biased towards correct answers on average, which is expected.\n\n**LitQA**\n> Is it expected that biomedical researchers cannot answer these question without internet? Or is this setup introduced to mimic the RAG styled QA by asking only to answer given what they find on the internet?\n\nAs the questions require a detailed and specific retrieval of very recent knowledge from the literature, we do not expect any biomedical researcher to be able to answer these without the use of the internet. The answers for the LitQA questions are also not answered in the abstract of the papers, by design, requiring the researcher to dig into the paper for the information. LitQA is thus used to evaluate the process of scientific literature retrieval itself, comparing between RAG and human researchers.\n\n**Summary LLM**\n> This is unexpected, any reasons? Is the context length a factor to correlate with the summarization performance? \u201cInterestingly, we observe that using GPT-4 as the summary LLM worsens overall performance.\u201d\n\nThank you for pointing this claim out. As it is unsupported in our manuscript and not central to the purpose of the manuscript, we have removed this claim altogether."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698543559,
                "cdate": 1700698543559,
                "tmdate": 1700698543559,
                "mdate": 1700698543559,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lY7aRHT4TI",
            "forum": "clU5xWyItb",
            "replyto": "clU5xWyItb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8010/Reviewer_1BNP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8010/Reviewer_1BNP"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents PaperQA, a Retrieval-Augmented Generation (RAG) agent developed to enhance question-answering in the scientific domain by mitigating issues of hallucinations and uninterpretability associated with Large Language Models (LLMs). Unlike other LLMs, PaperQA searches and retrieves information from full-text scientific papers to generate more accurate and interpretable responses. The authors showcase PaperQA's performance over existing LLMs on science QA benchmarks and introduce a new benchmark called LitQA, designed to simulate the complex task of human literature research. PaperQA is said to perform on par with expert human researchers when evaluated against the LitQA benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The concept introduced in the paper is promising, as it aims to develop a framework for retrieving literature to facilitate the answering of questions within scientific texts. The authors propose a novel approach that breaks down the QA task into three primary components: identifying relevant papers from online databases, extracting text from these papers, and synthesizing the information into a coherent final answer.\n\nThe paper introduces a new dataset, LitQA, which necessitates the retrieval and synthesis of information from full-text scientific papers. This is a notable effort to replicate the complexity of real-world scientific inquiry.\n\nThe study compares the proposed PaperQA system against multiple baselines. The results indicate that PaperQA outperforms these baselines and is on par with human experts."
                },
                "weaknesses": {
                    "value": "Lack of Novelty:\n\nThe methodology presented in this paper follows the established pipeline of retrieval, reading, and answering, which has been extensively explored in prior literature. The paper does not adequately differentiate the proposed model from existing work in the field. For this approach to be considered a substantial contribution, it would require either a novel application of these methods or significant improvements over existing models, neither of which are sufficiently demonstrated in the current paper.\n\nInsufficient Dataset Size:\n\nThe introduction of the LitQA dataset is an interesting addition; however, with only 50 examples, it is far too limited to serve as a robust benchmark for this area of research. Benchmarks require extensive and diverse examples to evaluate the generalizability and effectiveness of the proposed approach and to provide a reliable comparison with other baselines. The dataset, as it stands, does not meet these criteria.\n\nTechnical Feasibility and Lack of Detail:\n\nThere are concerns regarding the technical feasibility of some experimental settings described. Specifically, the instruction for the summary LLM to score relevance from 1 to 10 is not grounded in a clearly defined metric, raising questions about the model's capacity to interpret and apply these scores accurately.\n\nMoreover, the paper omits crucial details necessary for the reproducibility of the results and the clarity of the methods used. For instance, the base LLM utilized for PaperQA is not specified, leaving a gap in understanding the foundation upon which the system is built. Similarly, the engines powering GPT-3.5 and GPT-4 are not clearly defined. The configurations and model setups for the tools Elicit, Scite_, Perplexity, and Perplexity (Co-pilot) are insufficiently detailed. This lack of clarity hinders the assessment of the methods and the comparison of the results."
                },
                "questions": {
                    "value": "Missing related work:\n\n- Are You Smarter Than a Sixth Grader? Textbook Question Answering for Multimodal Machine Comprehension\n- Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering\n\nTypos:\n- \u201cThis implementation decision is explained\u201d -> \u201cThe implementation details are explained\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8010/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698888505265,
            "cdate": 1698888505265,
            "tmdate": 1699636987260,
            "mdate": 1699636987260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x3sQJ3mkV4",
                "forum": "clU5xWyItb",
                "replyto": "lY7aRHT4TI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8010/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Lack of Novelty**\n> The methodology presented in this paper follows the established pipeline of retrieval, reading, and answering, which has been extensively explored in prior literature. The paper does not adequately differentiate the proposed model from existing work in the field. For this approach to be considered a substantial contribution, it would require either a novel application of these methods or significant improvements over existing models, neither of which are sufficiently demonstrated in the current paper.\n\nThank you for this comment. We have added a few details to the paper to address this comment directly, highlighting the novelty and value of PaperQA more clearly. PaperQA exceeds existing LLM-only models like GPT-4 by 29 points on PubMedQA, a closed-book benchhmark, and PaperQA beats all existing models/agents/commercial projects on our smaller dataset. The dataset had to be small because it required dozens of experiments - we had to manually enter and evaluate questions (e.g., using Perplexity.AI), and we had to use large amounts of expert time to generate questions, where one needs to check \u201chas anyone reported X in the literature\u201d  across the scientific literature.  We believe to have reported a model better than all existing LLMs and agent architectures on the class of doing scientific research, which we hope would be significant enough for acceptance.\n\nRegarding what is novel about PaperQA: PaperQA is novel in three ways. \n\n1. First, we break the steps of RAG into tools that are accessible by the agent, allowing the agent to iterate and repeat steps as needed, in any order, in order to fully answer the question. \n2. In addition, PaperQA has access to the majority of all available scientific literature, going beyond just PubMed, or just Arxiv, which has been done before. \n3. Last but not least, we aim to show that using LLMs for difficult scientific questions is insufficient and that fine-tuning on specific scientific tasks may be unnecessary when compared to augmenting LLMs with the appropriate tools. \n\n**Insufficient Dataset Size**\n> The introduction of the LitQA dataset is an interesting addition; however, with only 50 examples, it is far too limited to serve as a robust benchmark for this area of research. Benchmarks require extensive and diverse examples to evaluate the generalizability and effectiveness of the proposed approach and to provide a reliable comparison with other baselines. The dataset, as it stands, does not meet these criteria.\n\nWe agree that 50 questions is a limited dataset. Those 50 questions were evaluated in multiple experiments with various commercial products and LLMs, requiring a great deal of human time to enter questions.  \n\nThe focus of this paper was primarily the development and use of PaperQA. The LitQA dataset was an additional set that also was explicitly out of pre-training data of GPT-4/3.5. We believe that beating GPT-4 by 30 points on PubMedQA is a good assessment on a larger dataset.\n\n**Summary LLM**\n> There are concerns regarding the technical feasibility of some experimental settings described. Specifically, the instruction for the summary LLM to score relevance from 1 to 10 is not grounded in a clearly defined metric, raising questions about the model's capacity to interpret and apply these scores accurately.\n\nWe appreciate this concern that the ability to assign this relevance only arises from the inherent capability of the base LLM, GPT-4. The relevance scoring metric is used to rank chunks, and the aim of this paper is to show that even with a general purpose model like GPT-4, the entire PQA workflow is able to find the relevant chunks and outperform other models on benchmarks. \n\n**Industrial Competitors**\n> The configurations and model setups for the tools Elicit, Scite_, Perplexity, and Perplexity (Co-pilot) are insufficiently detailed.\n\nWe agree that the technical details of these are not disclosed, as most information regarding the model setups are proprietary. Thus, we have provided all information we have access to. In the spirit of transparency, we clarify that we prompted these models the same way that we prompted PaperQA and evaluated manually.\n\n**Reproducibility**\n> Moreover, the paper omits crucial details necessary for the reproducibility of the results and the clarity of the methods used. For instance, the base LLM utilized for PaperQA is not specified, leaving a gap in understanding the foundation upon which the system is built. Similarly, the engines powering GPT-3.5 and GPT-4 are not clearly defined.\n\nThe GPT-3.5 version used was gpt-3.5-turbo-0613 and GPT-4 version was gpt-4-0613. These details have been added to the appendix (Appendix A), along with package version specifications.\n\n**Missing Related Work**\n\nThank you for pointing us to these related works. We have added them in our manuscript.\n\n**Typos**\n\nFixed."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8010/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698093229,
                "cdate": 1700698093229,
                "tmdate": 1700698093229,
                "mdate": 1700698093229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]