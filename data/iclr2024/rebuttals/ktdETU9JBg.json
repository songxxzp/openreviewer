[
    {
        "title": "Towards image compression with perfect realism at ultra-low bitrates"
    },
    {
        "review": {
            "id": "7jNjDn2h0o",
            "forum": "ktdETU9JBg",
            "replyto": "ktdETU9JBg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2661/Reviewer_Z1Pi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2661/Reviewer_Z1Pi"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use diffusion models as the decoder.  \nThe diffusion decoder is conditioned on both a vector-quantized latent image representation and a textual image description (losslessly coded separately).  \nThe proposed methods achieve realistic reconstructions at bitrates as low as 0.003 bits per pixel, significantly outperforming previous works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The technical contribution is novel to me. I like the idea of conditioning the generative codec on a vector-quantized image representation along with a global textual image description to provide additional context.  \n2. The paper is well written and easy to follow. Enough background information is provided to general readers. It is a great work bridging learnt image compression and AIGC.  \n3. The results are significantly better than previous works.   \n4. Ablation studies are thorough and convincing."
                },
                "weaknesses": {
                    "value": "1. Though the perceptual quality is SOTA, the decoding complexity is not compared. It is better to show the decoding complexity since diffusion models tend to be slower. Comparing the decoding complexity (i.e. latency and flops) with previous perceptual image compresion methods can provide more helpful information to the community.   \n2. Missing citations. Following Hoogeboom et al 2023, previous SOTA PO-ELIC should be compared or at least discussed if it is hard to make direct comparison: He, Dailan, et al. \"PO-ELIC: Perception-Oriented Efficient Learned Image Coding.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. \n3. Other Suggestions. The idea of conditioning the generative codec on image representation along with an extra descriptor (which is losslessly coded) is very interesting. I noticed that the following work provides a theoretical perspective on this conditional coding setting and also explains that the perceptual quality measured by FID is not affected by bitrate, please consider discussing this to provide more information to readers.\nXu, Tongda, et al. \"Conditional Perceptual Quality Preserving Image Compression.\" arXiv preprint arXiv:2308.08154 (2023)."
                },
                "questions": {
                    "value": "Overall I think this is a good work. Please refer to the weakness part for the discussion phase"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2661/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697530765783,
            "cdate": 1697530765783,
            "tmdate": 1699636206605,
            "mdate": 1699636206605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "magenf74ka",
                "forum": "ktdETU9JBg",
                "replyto": "7jNjDn2h0o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2661/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and constructive comments and address the questions below:\n\n> Though the perceptual quality is SOTA, the decoding complexity is not compared. It is better to show the decoding complexity since diffusion models tend to be slower. Comparing the decoding complexity (i.e. latency and flops) with previous perceptual image compresion methods can provide more helpful information to the community.\n\nAs stated in the joint answer, we performed tests for encode and decode speed and added a table with the results to the Appendix.\n\n> Missing citations. Following Hoogeboom et al 2023, previous SOTA PO-ELIC should be compared or at least discussed if it is hard to make direct comparison: He, Dailan, et al. \"PO-ELIC: Perception-Oriented Efficient Learned Image Coding.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\nThank you for pointing out to this relevant reference. In the related work section of the revised manuscript, we add a discussion on PO-ELIC which builds upon ELIC by adding a mixture of losses including adversarial ones to achieve better perceptual quality. They evaluate at bigger bitrates than us (their minimum bitrate is 0.075), and they do not provide code so it is difficult for us to include it in our benchmark. We already compare to three other adversarial-based codecs: HifiC as well as MS-ILLM and Multi-Realism which are more recent than PO-ELIC.\n\n> Other Suggestions. The idea of conditioning the generative codec on image representation along with an extra descriptor (which is losslessly coded) is very interesting. I noticed that the following work provides a theoretical perspective on this conditional coding setting and also explains that the perceptual quality measured by FID is not affected by bitrate, please consider discussing this to provide more information to readers. Xu, Tongda, et al. \"Conditional Perceptual Quality Preserving Image Compression.\" arXiv preprint arXiv:2308.08154 (2023).\n\nThank you for providing this relevant reference. This work introduces the notion of conditional perceptual quality, by extending perceptual quality to be conditioned on a specific side information. Given that we condition our generative codec on a losslessly encoded text description with the objective of achieving better preservation of text and local semantic information (measured by CLIP score/mIoU) and optimal image quality (FID), our work is very related to the idea of designing an optimal conditional perceptual quality preserving codec. We include this discussion in the related work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979488409,
                "cdate": 1699979488409,
                "tmdate": 1699979488409,
                "mdate": 1699979488409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "avdJY1sVFK",
                "forum": "ktdETU9JBg",
                "replyto": "magenf74ka",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2661/Reviewer_Z1Pi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2661/Reviewer_Z1Pi"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the reply, I keep my score"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700300086328,
                "cdate": 1700300086328,
                "tmdate": 1700300086328,
                "mdate": 1700300086328,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4mHh0evD8o",
            "forum": "ktdETU9JBg",
            "replyto": "ktdETU9JBg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2661/Reviewer_p6Yn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2661/Reviewer_p6Yn"
            ],
            "content": {
                "summary": {
                    "value": "## Summary\n* The authors present a technique to compress images into 0.003 bbp by text-to-image latent diffusion models. They show that in very low bitrate, pretrained caption becomes a very efficient latent representation for image compression."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "## Strength\n* There has been several diffusion based image compression works. A major problem of diffusion based compression is that what special advantage can diffusion brings with the cost of decoding time. This work find a specific scenario (ultra low bitrate) that is hard to be achieved by GAN models, which justifies the adoptation of diffusion over GAN, and I like it a lot.\n* The visual results look strikingly good. No zoom-up is needed to see the obvious advantage over other methods.\n* It is also interesting to see that text is a efficient representation for image compression."
                },
                "weaknesses": {
                    "value": "## Weakness\n* I am not really sure what is the point of evaluating LPIPS in Fig.3. Obviously MS-ILLM and HiFiC are trained with LPIPS, and this brings unfair advantage over the proposed approach. And as the authors are optimizing their approach towards divergence based perceptual quality [Blau 2018], using FID and KID should be enough. Reporting MS-SSIM and mIoU is also weird. \n* The authors think achieveing a low MS-SSIM, PSNR and LPIPS is a weakness, which I can not agree. By RDP theory [Blau 2019], it is absolutely normal for perceptual codec being outperformed on image-wise distortion metrics.\n* The authors are enouraged to present results in their original aspect ratio. Currently some results in Fig. 1 looks weird as they are squeezed horizontally."
                },
                "questions": {
                    "value": "## Questions\n* In MS-ILLM, the VQ-VAE is forzen once after pre-trained. While in this paper the VQ-VAE participate the end-to-end training. Then a natural question to ask is whether include the VQ-VAE of MS-ILLM in training improves the result?\n* A recent preprint [Conditional Perceptual Quality Preserving Image Compression] also justifies the adoptation of side information in image compression. The authors can probably connect their work to rate-distortion-perception trade-off with the approach similar the in preprint."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2661/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2661/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2661/Reviewer_p6Yn"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2661/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697781606536,
            "cdate": 1697781606536,
            "tmdate": 1699636206487,
            "mdate": 1699636206487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MZdsgzHzfH",
                "forum": "ktdETU9JBg",
                "replyto": "4mHh0evD8o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2661/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and constructive comments and address the questions below:\n\n> I am not really sure what is the point of evaluating LPIPS in Fig.3. Obviously MS-ILLM and HiFiC are trained with LPIPS, and this brings unfair advantage over the proposed approach. And as the authors are optimizing their approach towards divergence based perceptual quality [Blau 2018], using FID and KID should be enough. Reporting MS-SSIM and mIoU is also weird.\n\nWe agree with the reviewer that our goal is to design a perceptual codec and that FID/KID should be sufficient to evaluate the perceptual quality of our model. As we wanted to compare in the setting of RDP theory [Blau 2019], we included an evaluation on image similarity metrics LPIPS and MS-SSIM because they are widely used in the compression litterature. As rightly pointed out, LPIPS and MS-SSIM are not very meaningful in the low bitrate regime (see Fig. 14 in Appendix), so we included mIoU and CLIP score as they offer alternative ways of assessing distribution matching (mIoU having spatial localization and CLIP using a different image recognition mechanism). In the revised manuscript we further clarify the selection of metrics for evaluation.\n\n> The authors think achieveing a low MS-SSIM, PSNR and LPIPS is a weakness, which I can not agree. By RDP theory [Blau 2019], it is absolutely normal for perceptual codec being outperformed on image-wise distortion metrics.\n\nWe agree with the reviewer. We include this discussion in 'Limitations' paragraph of the paper.\n\n> The authors are enouraged to present results in their original aspect ratio. Currently some results in Fig. 1 looks weird as they are squeezed horizontally.\n\nThanks for the valuable remark. We update all figures in the paper with their original aspect ratio. \n\n> In MS-ILLM, the VQ-VAE is forzen once after pre-trained. While in this paper the VQ-VAE participate the end-to-end training. Then a natural question to ask is whether include the VQ-VAE of MS-ILLM in training improves the result?\n\nIn our model, we use an autoencoder that maps RGB images to the latent space of the diffusion model. This autoencoder is pretrained and frozen when training our models and we train a small hyper-encoder in the latent space for vector quantization (see paragraph 'Implementation details' in Section 4.1). In MS-ILLM, the VQ-VAE is only used as a continuous-to-discrete mapping for the purpose of training an adversarial loss. Our model does not use an adversarial loss, so our assessment is the two are not comparable. If we misunderstood the reviewer\u2019s intent, we could reconsider a rephrasing of this request.\n\n> A recent preprint [Conditional Perceptual Quality Preserving Image Compression] also justifies the adoptation of side information in image compression. The authors can probably connect their work to rate-distortion-perception trade-off with the approach similar the in preprint.\n\nThank you for providing this relevant reference. This work introduces the notion of conditional perceptual quality, by extending perceptual quality to be conditioned on a specific side information. Given that we condition our generative codec on a losslessly encoded text description with the objective of achieving better preservation of text and local semantic information (measured by CLIP score/mIoU) and optimal image quality (FID), our work is very related to the idea of designing an optimal conditional perceptual quality preserving codec. We include this discussion in the related work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979475639,
                "cdate": 1699979475639,
                "tmdate": 1699979475639,
                "mdate": 1699979475639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DyPgHnhcEC",
                "forum": "ktdETU9JBg",
                "replyto": "MZdsgzHzfH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2661/Reviewer_p6Yn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2661/Reviewer_p6Yn"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your rebuttal, most of my concerns are addressed. The images look better in their original aspect ratio.\n\np.s. the abstract is repeated twice in openreview."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700140332861,
                "cdate": 1700140332861,
                "tmdate": 1700140332861,
                "mdate": 1700140332861,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DVkMiGC1iN",
            "forum": "ktdETU9JBg",
            "replyto": "ktdETU9JBg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2661/Reviewer_FDrq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2661/Reviewer_FDrq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an image generative compression framework called PerCo. This model applies the diffusion model to the field of image compression and achieves visually good performance at extremely low bit rate compression. Compared to previous methods, this work leads to better visual quality as measured by subjective metrics like FID and KID."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method combines text and image description for the diffusion model to improve compression performance. Compared to previous works, it achieves higher quality and more realistic image representation at very low bit rates. The model proposed is valuable for future exploration of applying diffusion models to the field of image compression.\nMoreover, the semantic logic and argumentation of this article are clear. The extensive comparative analysis also makes the article more convincing."
                },
                "weaknesses": {
                    "value": "1\uff09The training and inference complexity of such diffusion-based methods is usually large. The authors should provide some preliminary results like running time or memory consumption.\n\n2\uff09Another existing diffusion-based image compression framework [1] combining text and image information can be discussed.\n\n[1] Pan, Zhihong, Xin Zhou, and Hao Tian. \"Extreme Generative Image Compression by Learning Text Embedding from Diffusion Models.\" arXiv preprint arXiv:2211.07793 (2022)."
                },
                "questions": {
                    "value": "A minor question: on page 4, in \u201cLocal spatial encoding\u201d paragraph line 6, \u201cThird, we proceed to quantization of $H_s$ to obtain $z_g$ via vector quantization\u201d.  From my understanding, $z_g$ should be $z_l$ ? (ps, a typo here: \u201cquatization\u201d to \u201cquantization\u201d )"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2661/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643880376,
            "cdate": 1698643880376,
            "tmdate": 1699636206424,
            "mdate": 1699636206424,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cSRmAQ8oFK",
                "forum": "ktdETU9JBg",
                "replyto": "DVkMiGC1iN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2661/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and constructive comments and address the questions below:\n\n> The training and inference complexity of such diffusion-based methods is usually large. The authors should provide some preliminary results like running time or memory consumption.\n\nAs stated in the joint answer, we performed tests for encode and decode speed and added a table with the results to the Appendix.\n\n> Another existing diffusion-based image compression framework [1] combining text and image information can be discussed. [1] Pan, Zhihong, Xin Zhou, and Hao Tian. \"Extreme Generative Image Compression by Learning Text Embedding from Diffusion Models.\" arXiv preprint arXiv:2211.07793 (2022).\n\nThanks for pointing out this relevant reference. In [1], they optimize a textual embedding on top of a pretrained text-to-image diffusion model. They also design a compression guidance method used at each denoising step during inference to better reconstruct original images. Their lowest compression rate is 0.042 bits per pixel which is lower than most of the compression baselines, but still much higher than 10x our minimum bitrate of 0.0031 bits per pixel. We include this paper in the related work.\n\n> A minor question: on page 4, in \u201cLocal spatial encoding\u201d paragraph line 6, \u201cThird, we proceed to quantization of $H_s$\n to obtain $z_g$ via vector quantization\u201d. From my understanding, $z_g$ should be $z_l$ ? (ps, a typo here: \u201cquatization\u201d to \u201cquantization\u201d )\n\nThanks a lot for pointing out the two typos, we correct them in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979458907,
                "cdate": 1699979458907,
                "tmdate": 1699979458907,
                "mdate": 1699979458907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QHHJcVLXFZ",
                "forum": "ktdETU9JBg",
                "replyto": "cSRmAQ8oFK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2661/Reviewer_FDrq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2661/Reviewer_FDrq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I will keep my rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466908094,
                "cdate": 1700466908094,
                "tmdate": 1700466908094,
                "mdate": 1700466908094,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T4l1DZBVzl",
            "forum": "ktdETU9JBg",
            "replyto": "ktdETU9JBg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2661/Reviewer_6W37"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2661/Reviewer_6W37"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new image compression method, which decodes compressed latent features with a iterative diffusion model, instead of a single-pass feed-forward decoder. Specifically, the introduced diffusion compressor conditions on two kinds of features:  (1) a global textual image description obtained from a image captioning model; (2) a vector-quantized image representation in latent space.  According to the experimental results, the proposed method beats the state-of-the-art methods in visual quality at ultra-low bitrate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper introduces a new image compression architecture. It proposes to combine a global textual image description and a vector-quantized latent image representation as the conditions of the latent diffusion model to decode the images.\n- The reconstruction results are much better than the exisiting works at ultra-low bitrates."
                },
                "weaknesses": {
                    "value": "- The idea of applying diffusion model on image compression is not new (as reviewed in the paper). But at least for me, combining it with textual information is interesting and motivating for image compression task.\n- The proposed method only optimizes the distortion/perceptual term but drops the rate term. It is not justified why the rate term can be dropped.\nWhat impact will have if include the rate term into optimization? \n- One limitation of diffusion model is the inference speed. This paper do not provide the running time of the proposed method, especifically the decoding time. A fair comparsion of running time (both encoding and decoding ) with the competing methods (both learned and traditional) should be provided and discussed."
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2661/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2661/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2661/Reviewer_6W37"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2661/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699343113129,
            "cdate": 1699343113129,
            "tmdate": 1699636206362,
            "mdate": 1699636206362,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pRWF7YmZIf",
                "forum": "ktdETU9JBg",
                "replyto": "T4l1DZBVzl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2661/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2661/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their time and constructive comments and address the questions below:\n\n> The proposed method only optimizes the distortion/perceptual term but drops the rate term. It is not justified why the rate term can be dropped. What impact will have if include the rate term into optimization?\n\nIn our approach, the rate is controlled through two hyper-parameter settings: the spatial resolution of the hyper-latents, and the vocabulary size. Therefore, the rate can be considered fixed when training our models, and therefore dropped from the objective function. To clarify this, we rephrase the first paragraph of section 3.1.\n\n> One limitation of diffusion model is the inference speed. This paper do not provide the running time of the proposed method, especifically the decoding time. A fair comparsion of running time (both encoding and decoding ) with the competing methods (both learned and traditional) should be provided and discussed.\n\nAs stated in the joint answer, we performed tests for encode and decode speed and added a table with the results to the Appendix."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979435547,
                "cdate": 1699979435547,
                "tmdate": 1699979435547,
                "mdate": 1699979435547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JE7sehUN4e",
                "forum": "ktdETU9JBg",
                "replyto": "pRWF7YmZIf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2661/Reviewer_6W37"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2661/Reviewer_6W37"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. My concerns are well addressed. I still stick with my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2661/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661554494,
                "cdate": 1700661554494,
                "tmdate": 1700661554494,
                "mdate": 1700661554494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]