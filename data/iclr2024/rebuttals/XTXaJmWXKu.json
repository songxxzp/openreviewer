[
    {
        "title": "Continual Nonlinear ICA-Based Representation Learning"
    },
    {
        "review": {
            "id": "LzDrf5DZrL",
            "forum": "XTXaJmWXKu",
            "replyto": "XTXaJmWXKu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4487/Reviewer_k5LY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4487/Reviewer_k5LY"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies an interesting topic: causal representation learning. Recent works in nonlinear Independent Component Analysis (ICA) provide a promising causal representation learning framework by separating latent sources from observable nonlinear mixtures. This paper introduces a new approach that optimizes the model by satisfying two objectives: (1) reconstructing the observations within the\ncurrent domain, and (2) preserving the reconstruction capabilities for prior domains through gradient constraints. Experiments show that the proposed approach can achieve good performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written.\n2. The research topic in this paper is interesting."
                },
                "weaknesses": {
                    "value": "1. It is not clear how you address continual learning.\n2. What is the actual form for the domain variable u?\n3. This paper only considers a general continual learning setting, which relies on the task information. However, the proposed approach can not be used in task-free continual learning.\n4. The theoretical framework is based on the existing work (Kong et al., 2022).\n5. The number of baselines in the experiment is small and more continual learning experiments should be performed.\n6. Since this paper employs the generative model. The lifelong generative modelling experiments should be provided."
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4487/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4487/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4487/Reviewer_k5LY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4487/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775226606,
            "cdate": 1698775226606,
            "tmdate": 1699636424773,
            "mdate": 1699636424773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SEGqRyCuqo",
                "forum": "XTXaJmWXKu",
                "replyto": "LzDrf5DZrL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4487/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4487/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are very grateful for your time, insightful comments, and encouragement"
                    },
                    "comment": {
                        "value": "We are very grateful for your time, insightful comments, and encouragement. Below please see our point-by-point response.\n\n> It is not clear how you address continual learning.\n\nThank you for your question. As detailed in Section 3.3, we address continual learning through pursuing two objectives: (1) the reconstruction of observations within the current domain and (2) the preservation of reconstruction capabilities for preceding domains. Specifically, we use GEM algorithm to enforce that the movement of network parameters learning a new domain should not result in an increased loss for the previous domains.\n\n> What is the actual form for the domain variable $u$?\n\n$u$ is an indicator of domain which can be represented with a vector or a scalar. \n\n> This paper only considers a general continual learning setting, which relies on the task information. However, the proposed approach can not be used in task-free continual learning.\n\nThank you for your question and we would like to re-emphasize the difference between ours with traditional continual classification tasks. In the traditional continual classification task, each task is independent. The purpose of continual learning is to make the model remember each task as much as possible. In this situation, not knowing the identities of tasks will indeed affect testing. However, for our identifiable nonlinear ICA learned in sequentially arrived domains, we need multiple domains to make the model identifiable. Once we have enough domains, knowing the identity of the domain (task) or not is not important at all for the test phase. Regardless of which domain the tested data comes from, even a completely unseen domain, we can still recover meaningful latent variables (component-wise identifiable). Figure 5 (a) also validates this point as the model is always tested on all 15 domains while the training data increases gradually from 1 domain to 15 domains.\n\nFor the training phase, it should be noted that our observations are generated from changing distribution through **fixed** transformations. Thus, it would be easy to capture the boundary of each domain for model training.\n\n> The theoretical framework is based on the existing work [1].\n\nThanks for your comment. Compared with the framework from [1], we introduced the subspace identifiability for changing variables, which is essential to investigate the case where few domains are observed. Beyond that, we have demonstrated that, in the context of continual learning, the identifiability of certain variables becomes compromised with the introduction of new domains, as outlined in Proposition 2 and empirically shown in Figure 6.\n\n> The number of baselines in the experiment is small.\n\nThe reason we have not conducted comparisons of our method with other nonlinear ICA approaches is due to the distinctive structure of our model. Specifically, our approach involves partitioning latent variables into two distinct categories: changing and invariant components. Thus, other nonlinear ICA frameworks cannot be directly compared as a baseline. \n\n> More continual learning experiments should be performed.\n\nWe are currently conducting the experiment on image datasets and will report the results once we get them.\n\n> The lifelong generative modeling experiments should be provided.\n\nWe appreciate your comment, but we would like to clarify our understanding of your question to provide a more accurate response. Section 4 of our paper is dedicated entirely to the lifelong generative modeling experiment. If you could provide more specific details or context regarding your query, it would greatly assist us in addressing your concerns or questions more effectively. \n\n[1]. Kong. et al. Partial Disentanglement for Domain Adaptation, 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516755121,
                "cdate": 1700516755121,
                "tmdate": 1700516755121,
                "mdate": 1700516755121,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7RnRVWoVC0",
            "forum": "XTXaJmWXKu",
            "replyto": "XTXaJmWXKu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4487/Reviewer_iGtV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4487/Reviewer_iGtV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies identifiability of VAE models trained on a stream of different data domains. The ground truth generative model assumes that one part of the latents is domain-dependent, and another part domain-independent."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I really enjoyed reading the paper. I think the motivation is clear, the problem interesting and relevant. Some theoretical and algorithmic innovation is given, and the theory is validated by synthetic experiments (but see weaknesses below)."
                },
                "weaknesses": {
                    "value": "The biggest weakness by far is the empirical investigation. The theory, while interesting and relevant, seems too incremental to justify acceptance just based on the theoretical contribution --- more empirical validation and comparisons to prior work is needed. The main weaknesses are:\n\n- There is no discussion of potentially competing ICA approaches in the literature, and no comparison to baseline algorithms. The evaluation is only with regards to the proposed setting, no external validation takes place. Especially the joint training setup is applicable to a variety of non-linear ICA methods, so a better empirical comparison would greatly enhance the positioning w.r.t prior work.\n- It is a bit tricky to connect the sections in the appendix to the theorems/proposition of the paper. It would enhance readability if there are clear headings in the appendix, and/or the authors would re-state the results from the paper.\n- In the derivation in A1 and the following sections, the arguments to the Jacobians, e.g. $J_h$, are dropped. I find it not always clear from the context to infer the arguments. I am especially wondering (but might be wrong) whether the Jacobian $J_h$ depends on $\\mathbf u$, see my question below. In any case, stating the argument of $J_h$ would improve readability of the proof.\n- between Eqs (21), (22), is it necessary to keep the $\\mathbf 0$ argument? I find this more confusing/uncessary than helpful, but might overlook something.\n- The method is purely validated on synthetic toy datasets. There's a wealth of e.g. image datasets available (dsprites and the like) to validate the approach on more complex distributions, without drastically increasing the number of latent factors. Such an exploration would improve the paper a lot, I would be happy to discuss a choice of datasets with the authors before running experiments. This could be especially interesting to \"convert\" an existing benchmark into the continual ICA setting.\n- There are a lot of typos in the appendix and proofs, in general more care could be taken with spellchecking and typesetting. A common typesetting issue is missing spaces, or inconsistent upper/lowercasing (assumption / Assumption, etc). This should be fixed.\n- A2, Proposition 2, typesettng errors ($<\\le$ etc).\n\nMy current assessment is based on the current state of the paper, which can be improved in terms of clarity in the theory (esp. in the appendix) and the experimental results (comparisons to more baseline methods from the literature, scaling beyond synthetic data). I think with good execution and improvement along these dimensions, the current paper story and problem setting could easily get a 6 or even 8 in score, and I expect to re-adapt my evaluation during the rebuttal phase based on the the improvements made."
                },
                "questions": {
                    "value": "- Figure 5: The error bars are quite large between baseline and joint --- did you run a test whether the improvements observed are signficant?\n- Performance of the empirical validation is far from optimal, MCCs are substantially smaller than 1. What would it take to observe a result close to the \"theoretical limit\"? Have you considered how different components (number of latents, number of samples in the dataset, ...) influence the final result?\n- In A1, does $J_h$ depend on $\\mathbf u$?\n- \"the distribution estimate variable $\\tilde z_j$ doen't change across all domains\" -> Can you clarify why this is? That statement is not obvious to me in the context of the proof.\n- \"Similarly, $q_i \\dots$ remainds the same for ...$ -> same concern, not obvious, maybe a ref is needed.\n- Is there a reason why $\\mathbf u$ is assumed to be a vector? for the purpose of the proof, isn't it sufficient to assume an integer value? (the function $f_u$ might still map it to a vector internally, I am just not sure why that assumption is needed)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4487/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698947788514,
            "cdate": 1698947788514,
            "tmdate": 1699636424692,
            "mdate": 1699636424692,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fyaX70nGcd",
                "forum": "XTXaJmWXKu",
                "replyto": "7RnRVWoVC0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4487/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4487/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are very grateful for your time, insightful comments, and encouragement - Part 1"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the time dedicated to reviewing our paper, the constructive suggestions, and encouraging feedback. Please find the response to your comments and questions below.\n\n> In the derivation in A1 and the following sections, the arguments to the Jacobians, e.g. $J_h$, are dropped. Is $u$ related to $J_h$?\n\nThank you for your question. $u$ is not related to $J_h$. The transformation $h = g^{-1} \\circ g$ describes the relationship between the true latents $z$ and the estimated variables $\\hat{z}$, which will not be influenced by the change of domains $u$. For different $u$, Jocabian $J_h$ is the same and the existence of $u$ is to help us to remove the intractable term $J_h$. We will make this clear in the revised manuscript.\n\nLet us summarize the overall logic of the proof: we can easily construct the relationship between estimated variables $z$ and true latents $\\hat{z}$ like Eq(16) as both $g$ and $\\hat{g}$ is invertible. The challenge is that we do not know about $J_h$ (once we know, everything is solved), while we want to investigate the relationship between $\\hat{z}$ with $z_s$. To solve the untractable term $J_h$, we introduce the domain $u$ and produce multiple equations like Eq~(17). Then, we can remove the intractable Jocabian term by taking the difference for every equation with the equation where $u=u_0$. This is why the $u$ is not related to $J_h$.\n\n>\"the distribution estimate $\\hat{z}_j$ variable doesn't change across all domains\" and \"Similarly, q_i remains the same for\"\n\nThe reason that why  $\\hat{z}_j$ doesn't change across all domains is as follows: the $\\hat{z}_j$ is defined to be the estimated invariant variable that will not be affected by domain $u$, for $j \\in [n_s+1, \\dots, n]$. Similarly, $q_i(z_i, u)$ for $i \\in [n_s+1, \\dots, n]$ represents the log density of groundtruth invariant variables, and thus it also does not change across all domains.\n\n> Is there a reason why $ u$ is assumed to be a vector? for the purpose of the proof, isn't it sufficient to assume an integer value?\n\nThanks for your insightful observation. We totally agree with you that it is sufficient to assume an integer value for $u$ for the purpose of the proof. In our framework, $u$ serves as an indicator to denote different domains, and it can indeed be represented either as a vector or a scalar. We opted for a vector representation to maintain generality (when dimension reduces to one, a vector degrades to a scalar). We will update the manuscript to include this discussion.\n\n> between Eqs (21), (22), is it necessary to keep the argument $0$ ?\n\nThanks for your suggestion which helps improve the presentation and lighten the notations. We have updated the manuscript to remove the argument $0$.\n\n> The error bars are quite large between baseline and joint --- did you run a test whether the improvements observed are significant?\n\nWe appreciate your insightful observation. All experiments shown in Figure~5 are run with seeds set at 1,2,3. The error bars of baselines are indeed large--a possible reason is that the performance of the baseline is heavily influenced by the last domain, giving rise to the large variance. In contrast, the error bars or variances of our proposed method and the joint approach are much smaller, which further indicates the importance of a continual learning approach in this setting. We will include this observation and discussion in the experiment section.\n\n> Performance of the empirical validation is far from optimal, MCCs are substantially smaller than 1. What would it take to observe a result close to the \"theoretical limit\"? \n\nThanks for your question. A possible reason is that the estimation procedure involves neural networks, and the overall optimization problem is nonconvex, as is typical for methods involving deep learning. Thus, the estimation process, even for the joint training approach, may never achieve the global optimum. Another reason is that we divide the latent variables into invariant and changing parts, which may be more challenging than regular nonlinear ICA where all latents are changing. In fact, similar observations have been reported in [1].\n\n> Have you considered how different components (number of latents, number of samples in the dataset, ...) influence the final result? \n\nThanks for your question. Kindly refer to Figure 4 in the paper which shows the influence of different components ($n_s=4,n=8$ and $n_s=2, n=4$) on the final MCC results. We also conducted an ablation study to investigate the influence of $\\hat{n}_s$ as shown in Table~1. \n\nFor different number of samples, in light of your suggestion, we conducted additional experiment results. For the case of $n_s=4,n=8$, we conducted training using 5000 samples and testing with 1000 samples for each domain. For the scenario of $n_s=2, n=4$, we conducted training using 10000 samples and testing with 1000 samples for each domain. We will add these details in the appendix of the final version."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515885529,
                "cdate": 1700515885529,
                "tmdate": 1700515885529,
                "mdate": 1700515885529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZBd4h21PKi",
                "forum": "XTXaJmWXKu",
                "replyto": "7RnRVWoVC0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4487/Reviewer_iGtV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4487/Reviewer_iGtV"
                ],
                "content": {
                    "title": {
                        "value": "Quick response to rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors, thanks a lot for your comments. I will go through your comments regarding the theory in more detail (from skimming them, they seem to address many of my open Qs).\n\nI wanted to quickly chime in and confirm that additional (convincing) empirical results would be the main reason for me to change my evaluation of the paper. I appreciate that you decided to run additional experiments, and am happy to engage in further discussion once first results are in."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517249480,
                "cdate": 1700517249480,
                "tmdate": 1700517249480,
                "mdate": 1700517249480,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bqUlHVy2xp",
            "forum": "XTXaJmWXKu",
            "replyto": "XTXaJmWXKu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4487/Reviewer_r8sE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4487/Reviewer_r8sE"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the identifiability of nonlinear ICA in the continual learning setting i.e. under a changing and partially changing domains. The identifiability theorems show that under a sufficiently large number of domains with significant changes, the latent components can be identified up to component-wise nonlinearity. For a lower number of domains, subspace identifiability is still guaranteed as long as there are more domains than changing variables. A learning algorithm based on VAE and GEM (method for continual learning) is introduced and its performance (on identifiability) is presented on simulated data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper's idea to identify nonlinear ICA in terms of sequential learning is novel. The authors nicely show how as the number of domains increases, the identifiability improves which is an interesting, albeit expected, result."
                },
                "weaknesses": {
                    "value": "There are some major weaknesses and questions.\n\n**1. Contribution is not clear since relevant previous work is ignored:**:\n\na). You claim that *\"[Nonlinear ICA] still relies on observing sufficient domains simultaneously...\"*. -- Not true necessarily: for example [1], shows the identifiability of hidden Markov nonlinear ICA. As is well known, HMMs can be learned sequentially / in online fashion with the latent states analogous to different domains. Authors have not covered relation to this work.\n\nb). Related to above: [1] and [2] give a broad identifiability theorem that applies to situation with a changing latent variable that can be interpreted as the changing domain that alters the probability distributions i.e. it seems to already to cover mostly what is considered in the identifiability theorems here, and provides stronger results in a more general setting. It is therefore important to contrast to this work and explain what is novel here. Note that I understand that the sequential learning is different, but here I am only talking about the identifiability theorem. Other important and similar work is also ignored and need to be discussed such as [3]-[6]. Note also that most of these works *do not* assume auxiliary variables.\n\nc). The paper is frames itself as \"Causal Representation Learning\" but it seems like it's much more related to nonlinear ICA and doesn't learn latent causal relationships -- see [6] and [7]. I recommend the authors to reconsider the use of this term.\n\n**2. Experimental evaluation is lacking**\n\na). The author's only evaluate their model on synthetic data, no real data is used and the importance of this method to practicable applications is not clear\n\nb.) Baseline does not include any of relevant previous works e.g. [1] could be reasonable adapted to this data\n\n\n**3. Several issues on theory and estimation algorithm**\n\na.) The identifiability theorem appears not to consider observations noise -- yet the estimation method VAE clearly includes that. This mismatch and its impact on identifiability can be significant (Theorems 1, 2 in [2]) but this is ignored here\n\nb.) The identifiability theorem does not appear to consider dimension reduction into the latent space, which is in practice very important. Without that you are usually stuck with high dimensional latent space -- and the theorems presented here are unlikely to hold with high dimensional latent variables making it unclear how useful the work is in practice. \n \n\n**Other issues:**\n\na). You use \"component-wise identifiability\" but really this is inaccurate, you do not identify the elements component-wise, rather you identify them up to component-wise *nonlinearity*. This needs to be fixed in order to make sure the reader doesn't misunderstand and think that you can exactly identify each components. \n\nb). \"Importantly, the guarantee of identifiability persists even when incoming domains do not introduce substantial changes for partial variables\". Please clarify what is meant by partial variables -- the term has not been defined by this point.\n\nc). It is not explained clearly enough that the conditions in Theorems 1 and 2 are *sufficient* conditions, not *necessary* and I feel there is some confusing writing related to this. You for example say that \"However, when considering domains u0, u1, u2, the component-wise identifiability for z1 disappears, and instead, we can only achieve subspace identifiability for both z1 and z2.\" This is not necessarily true -- according to your own theorems you can only *guarantee* subspace identifiability (sufficiency) but since theorem 1 does not give *necessary conditions* you can not say that \"the component-wise identifiability for z1 disappears,\" it could still be component-wise identified,\n\nmisc.:\n- the z in Figure 2 should not be bold if I'm correct\n- poor grammar: \"will contribute to both domains and we remain the direction.\", \"where no loss increment for previous domains.\"\n- Figure 5 coming before Figure 4 is extremely confusing\n- \"We evaluate the efficacy of our proposed approach by comparing it against the same model trained on sequentially arriving domains and multiple domains simultaneously, referred to as the baseline and theoretical upper bound by the continual learning community.\" Could you please clearly define \"joint\" and \"baseline\" so that the reader can understand them.\n- I dont see \"Table 1\" mentioned anywhere in the text, making it hard to understand \n\n[1]. H\u00e4lv\u00e4 and Hyv\u00e4rinen, Hidden Markov Nonlinear ICA: Unsupervised Learning from Nonstationary Time Series, 2020\n\n[2]. H\u00e4lv\u00e4 et al. Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA, 2021\n\n[3]. Klindt et al. Towards Nonlinear Disentanglement in Natural Data with Temporal Sparse Coding, 2020\n\n[4]. Gresele et al. The Incomplete Rosetta Stone Problem: Identifiability Results for Multi-View Nonlinear ICA, 2019\n\n[5]. Gresele et al. Independent mechanism analysis, a new concept?, 2021\n\n[6]. Hyv\u00e4rinen et al. Identifiability of latent-variable and structural-equation models: from linear to nonlinear, 2023\n\n[7]. Sch\u00f6lkopf et al. Towards Causal Representation Learning, 2021"
                },
                "questions": {
                    "value": "Q1. You write that \"Thus, a fully identifiable nonlinear ICA needs to satisfy at least two requirements: the ability to reconstruct the observation and the complete  consistency with the true generating process. Unfortunately, current research is far from achieving this level of identifiability.\" Could you please explain what you mean by \"far from achieving this level\"?\n\nQ2. Could you please explain how realistic the assumption in equation (3) is? After all, the method here is somewhat heuristic so we can not expect MLE-style guarantees\n\nQ3. Am I correct understanding that this model does not allow dimension reduction into latent space?\n\nQ4. The two paragraphs relating to Figure 2 are very hard to understand. In fact, the paragraph break between \"Given our subspace identifiability theory, z1 can achieve subspace identifiability.\" and \"As there is no change in the other variables in those two domains, this subspace identifiability is equal to competent-wise identifiability.\" doesn't seem to make sense. Why is there a paragraph break here? Further why is there $u_3$ if it's not mentioned in the text?\n\nQ5. You write \"Contrasted with the traditional joint learning setting, where the data of all domains are overwhelmed, the continual learning setting offers a unique advantage. It allows for achieving and maintaining original identifiability, effectively insulating it from the potential \"noise\" introduced by newly arriving domains.\" I think this is really a key, and very interesting if true, but as far as I understand, there is no theoretical way of proving this exactly? Am I correct in understanding this is more what you believe Theorem 1 implies? \n\nQ6. \"Assuming the function is invertible, we employ a flow model to obtain the high-level variable\"-- Supposedly this high-level variable is not identifiable however? What is the impact of its unidentifiability?\n\nQ7. What does the apostrophe in eq (6) mean?\n\nQ8. I am bit confused by the lack of detail in the experiments section -- how is the experiment in Figure 5 different from the top-right one in Figure 4. \n\nQ9. \"We evaluate the efficacy of our proposed approach by comparing it against the same model trained on sequentially arriving domains and multiple domains simultaneously, referred to as the baseline and theoretical upper bound by the continual learning community.\" Could you please clearly define \"joint\" and \"baseline\" so that the reader can understand them. \n\nQ10. \"model achieves the component-wise identifiability and the extra domains (from 9 to 15) do not provide further improvement.\" How can you be sure it really is component-wise identifiable? Are all the components around similarly identified or is there a lot of variance?\n\nQ11. why is there so much variance in the other methods in Figure 5b.)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4487/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4487/Reviewer_r8sE",
                        "ICLR.cc/2024/Conference/Submission4487/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4487/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699290926099,
            "cdate": 1699290926099,
            "tmdate": 1699957130237,
            "mdate": 1699957130237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JuZ3FG3oGv",
                "forum": "XTXaJmWXKu",
                "replyto": "bqUlHVy2xp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4487/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4487/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely thank the reviewer for the time dedicated to reviewing our paper, the constructive suggestions, and encouraging feedback - Part 1"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the time dedicated to reviewing our paper, the constructive suggestions, and encouraging feedback. Please find the response to your comments and questions below.\n\n> You claim that \"[Nonlinear ICA] still relies on observing sufficient domains simultaneously...\". -- Not true necessarily: for example [1], shows the identifiability of hidden Markov nonlinear ICA. As is well known, HMMs can be learned sequentially / in online fashion with the latent states analogous to different domains.\n\nThank you for your insightful observation. It is indeed correct that Hidden Markov Models (HMMs) can be learned sequentially. However, as delineated in reference [1], the identifiability of the model is contingent upon the concurrent observation of a sufficient number of latent states or domains. On one hand, the validity of Equation (22) is compromised in scenarios where the model experiences 'forgetting', particularly when only a single latent state is observed at any given time. This scenario presents a stark contrast to the framework and contributions of our study, which focuses on a genuinely continual learning approach. On the other hand, to preserve the identifiability of the model in a sequential learning context (as proposed in [1]), where only one latent state is observed at a time, the algorithm we have used in our paper could be adapted as it only focuses the optimization procedure.\n\n> it seems to already to cover mostly what is considered in the identifiability theorems here, and provides stronger results in a more general setting. It is therefore important to contrast to this work and explain what is novel here.\n\nThank you for your feedback. We agree that [1] and [2] are highly relevant, which we will discuss further in the revised manuscript. At the same time, we would also like to point out the differences with our setting (and therefore one is not \"stronger\" than another): \n\n(1) While [2] is predicated on exploiting the temporal structure of latent variables, our paper delves into a scenario where the latent variables are independently and identically distributed (i.i.d) within each domain. This distinction is crucial as the reliance on the use of temporal structures in [1] and [2] can indeed facilitate stronger identifiability results.\n\n(2) Our papers separate the latent variables into invariant and changing part, which is practical in real-world scenarios. We thoroughly investigated the conditions under which these two groups can be segregated (requiring at least $n_s+1$ domains) and also provided the sufficient condition that each changing variable is identified up to a nonlinear transformation ($2n_s+1$ domains). This exploration into the separability and identification of latent variables underscores the novelty and depth of our study. \n\n(3) In contrast to [1], which presupposes the conditional distribution of latent variables within the exponential family framework, our work does not make such an assumption.\n\n> Other important and similar work is also ignored and need to be discussed such as [3]-[6]. Note also that most of these works do not assume auxiliary variables.\n\nThank you for sharing the references. We have provided a brief discussion on related works that do not assume auxiliary variables; see the third paragraph of Section 1. Given your suggestion, we will cite and discuss [3]-[6] in the revised version of our paper. We will also modify the sentence \"it still relies on observing sufficient domains simultaneously\" to \"many of them still rely on observing sufficient domains simultaneously\", to avoid any possible confusion.\n\n> The paper frames itself as \"Causal Representation Learning\" but it seems like it's much more related to nonlinear ICA and doesn't learn latent causal relationships -- see [6] and [7]. I recommend the authors to reconsider the use of this term.\n\nThanks for your suggestion. We have reconsidered the use of the term and will use \"continual identifiable representation learning\" instead."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517365558,
                "cdate": 1700517365558,
                "tmdate": 1700517365558,
                "mdate": 1700517365558,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kMebJVQkRo",
                "forum": "XTXaJmWXKu",
                "replyto": "oxoGU239dB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4487/Reviewer_r8sE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4487/Reviewer_r8sE"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the reviewers for their rebuttal and it has cleared up several things but not the big problems -- thus I will hold my score as it is. I think there are currently too many issues with the paper.\n\nOn the theoretical side, I am not convinced that this idea of \"new domains\" brings anything substantial new e.g. with respect to earlier work. In partcular as response to the author comments:\n\n>(1) While [2] is predicated on exploiting the temporal structure of latent variables, our paper delves into a scenario where the latent variables are independently and identically distributed (i.i.d) within each domain. This distinction is crucial as the reliance on the use of temporal structures in [1] and [2] can indeed facilitate stronger identifiability results.\n\nTo above: actually, there is nothing to prevent this type of situation in [2] and it's in fact whathappens in [1].\n\n>(2) Our papers separate the latent variables into invariant and changing part, which is practical in real-world scenarios. We thoroughly investigated the conditions under which these two groups can be segregated (requiring at least domains) and also provided the sufficient condition that each changing variable is identified up to a nonlinear transformation ( domains). This exploration into the separability and identification of latent variables underscores the novelty and depth of our study.\n>(3) In contrast to [1], which presupposes the conditional distribution of latent variables within the exponential family framework, our work does not make such an assumption.\n\nTo above: I don't see why this idea isn't immediately from previous works. e.g. [1] we can assume that the domains are such that only some of the components distribution changes and we would have the same result as far as I can tell? As you say [1] does assume exponential family, which may be restrictive but [2] for example does not and seems to also naturally include the identifiability result of this type of changing and constant variables with domain changes. \n\n>Thank you for your insightful observation. It is indeed correct that Hidden Markov Models (HMMs) can be learned sequentially. However, as delineated in reference [1], the identifiability of the model is contingent upon the concurrent observation of a sufficient number of latent states or domains\n\nTo above: I don't agree with this. Theoretical identifiability is a property of the model. How it is estimated is a separated a concept. In practice of course the two sides interact and may produce different results. But the identifiability theory of the HMM-NICA model ought to hold regardless of what type of algorithm is used. \n\nFinally, the experimentation is lacking as described and I propose the authors take in feedback from all the reviewers to improve their work. I would like to see for example how models from [1] or [2], after being adapted to this scenario, perform because I can't see them being different currently."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682461907,
                "cdate": 1700682461907,
                "tmdate": 1700682461907,
                "mdate": 1700682461907,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]