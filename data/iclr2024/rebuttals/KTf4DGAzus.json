[
    {
        "title": "$\\textbf{\\textit{M}}^\\textbf{\\textit{3}}$: Towards Robust Multi-Modal Reasoning via Model Selection"
    },
    {
        "review": {
            "id": "eiHf8nNVt6",
            "forum": "KTf4DGAzus",
            "replyto": "KTf4DGAzus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4586/Reviewer_v23h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4586/Reviewer_v23h"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied the model selection problem in multi-modal reasoning. It first formulated this problem, and then proposed $M^3$ framework as an initial attempt in this field. This paper also created a new dataset, MS-GQA, as a benchmark to compare different methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper formulated the model selection problem in multi-modal reasoning, which is a new direction worth investigating.\n\n2. This paper made an initial yet comprehensive effort to study this problem, including a model-selection framework, MS-GQA datasets, as well as a comparison between possible baselines."
                },
                "weaknesses": {
                    "value": "1. The significance of the problem is not well illustrated. While the paper has shown the existence of model selection problem, I am not aware of how important this problem is. There can be lots of problems in multi-modal reasoning, but some may not be of much value. Specifically, is there a huge gap between an oracle model selection and a random selection? Is there a na\u00efve solution that can approach the oracle performance? The authors are suggested to add these preliminary experiments to illustrate the significance of the problem.\n\n2. Lack of ablation study on model inputs. The paper claims that other model selection methods do not take subtask dependency into account. However, the ablation study does not show the effect of using subtask dependency as input. More broadly, because the framework uses various inputs, including multi-modal inputs, node embedding, subtask dependency, a more extensive ablation can be done by removing each component successively. This will show the importance of each component."
                },
                "questions": {
                    "value": "1. Presentation should be improved. I find some of the notations are used without introducing first, which hinders a smooth reading. \nEspecially in Section 3.2:\n* Page 4, bottom line, $\\phi \\circ \\psi$ is used but they have not been introduced.\n* Page 5, Section 3.2.1, Para 1, $\\psi := [\\psi_1, \\psi_2]$ is used without explanation.\n\n2. Why choose SER as the metric rather than accuracy? From Figure 1, I thought a wrong model selection will mainly cause the system to give false answer. But the main metric adopted is to measure the successful execution rate. I think there is a difference between successful execution and final accuracy.\n\n3. What is the difficulty level in tab2? How do the authors define the difficulty level?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4586/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698633350464,
            "cdate": 1698633350464,
            "tmdate": 1699636436760,
            "mdate": 1699636436760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LqGkHgYjwM",
                "forum": "KTf4DGAzus",
                "replyto": "eiHf8nNVt6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v23h (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer v23h:\n\nThank you very much for the review and feedback. We kindly address your questions as follows:\n\n> **W1:** *The significance of the problem is not well illustrated. While the paper has shown the existence of model selection problem, I am not aware of how important this problem is. There can be lots of problems in multi-modal reasoning, but some may not be of much value. Specifically, is there a huge gap between an oracle model selection and a random selection? Is there a na\u00efve solution that can approach the oracle performance? The authors are suggested to add these preliminary experiments to illustrate the significance of the problem.*\n> \n\nThe significance of the problem we introduced to the community and studied in this manuscript can be identified by the performance gap between oracle and our current SOTA method, as illustrated in Section 4.2. In detail, \n\n1. In our constructed MS-GQA, every sample has at least one model selection choice that would output the correct answer. Therefore, the theoretical upper bound for the entire test set is 100% SER (Successful Execution Rate, where a successful execution in our context indicates that the agent could output the correct answer by executing the selected models);\n2. As shown in Table 1, a model selector employing random model selection can only achieve approximately 56% SER, while a rigid model selector like VisProg also achieves only 59%, which is not very satisfactory;\n3. Our method (M3) achieves a SER of 68.7%, significantly surpassing other baselines but still maintaining a gap from the oracle model selector.\n\nIn summary, a good, robust model selector can significantly improve SER compared to simple strategies (~56% to 68.7%), but there is still considerable room for improvement compared to the Oracle model selector. This indicates the value of the current research direction.\n\n---\n\n> **W2:** *Lack of ablation study on model inputs. The paper claims that other model selection methods do not take subtask dependency into account. However, the ablation study does not show the effect of using subtask dependency as input. More broadly, because the framework uses various inputs, including multi-modal inputs, node embedding, subtask dependency, a more extensive ablation can be done by removing each component successively. This will show the importance of each component.*\n> \n\nAs discussed in Section 3.2, M3 unifies the key components that emerged in the unexamined scenario of multi-step multi-modal reasoning procedure, namely multi-modal inputs, model (node) embedding, and subtask dependency (see Figure 3).\n\nBelow we explain why removing any specific component would make our M3 framework ineffective:\n\n1. **Multi-modal inputs and model embeddings are irremovable.** Multi-modal inputs and model (node) embeddings are fundamental and basic inputs for model selection, which essentially involves finding the most suitable model for each sample;\n2. **Subtask dependency is also irremovable.** The representation of subtask dependency through edges in the computation graph is an indispensable component of the computation graph;\n\nIn our initial manuscript, we extensively explored the impact of the replaceable components in M3. Specifically, in Appendix D, the ablation study encompasses experiments that span the selection of multi-modal feature extractors to the determination of the objective function.\n\nBesides, we have separately studied the effect of subtask dependency in the conventional model selection methods (namely NCF and MetaGL). It again reveals the uniqueness of our methodology by taking all components as a complete unit.\n\n- Section 4.4 elucidates the advantage of M3 in leveraging subtask dependency information compared to the NCF++ and MetaGL++."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295644589,
                "cdate": 1700295644589,
                "tmdate": 1700298862152,
                "mdate": 1700298862152,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dC7GHdDrNP",
                "forum": "KTf4DGAzus",
                "replyto": "eiHf8nNVt6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer v23h,\n\nThank you for your valuable feedback on our manuscript. Upon your request, in our previous response, we have:\n\n- thoroughly discussed the research significance (see Appendix E for more details).\n- provided additional explanations for the settings in the existing ablation study.\n- clarified certain definitions in the original text.\n- rectified instances of non-standard notation usage in the original manuscript.\n\nWe would appreciate it if you could let us know if our response has addressed your concerns, and we kindly request a reconsideration of the score.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625576548,
                "cdate": 1700625576548,
                "tmdate": 1700625588827,
                "mdate": 1700625588827,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H0nWbr0Avu",
            "forum": "KTf4DGAzus",
            "replyto": "KTf4DGAzus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4586/Reviewer_1wtR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4586/Reviewer_1wtR"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the importance of model selection in multi-modal agents, where Large Language Models (LLMs) play a central role in orchestrating various tools for collaborative multi-step task solving. Unlike traditional methods that use predefined models for subtasks, these multi-modal agents excel by integrating diverse AI models for complex challenges. However, existing multi-modal agents tend to overlook the significance of dynamic model selection, focusing primarily on planning and execution phases, which can make the execution process fragile.\n\nThe paper introduces the M3 framework as a plug-in with a small runtime overhead at test time. This framework aims to enhance model selection and improve the robustness of multi-modal agents in multi-step reasoning scenarios. In the absence of suitable benchmarks, the authors create a new dataset called MS-GQA, designed to investigate the model selection challenge in multi-modal agents. The experiments demonstrate that the M3 framework enables dynamic model selection by considering both user inputs and subtask dependencies, ultimately enhancing the overall reasoning process. The authors plan to make their code and benchmark publicly available."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper provides a clear analysis of the challenges.   \nBesides the method, the paper also provides a dataset as one of the contributions.  \nThe experimental results show significant improvements."
                },
                "weaknesses": {
                    "value": "The method uses a heuristic process to perform selection which the capacity is relying on the pre-trained models themselves.  \nHow about the generalization capacity for the zero-shot tasks?"
                },
                "questions": {
                    "value": "refer to the above content."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4586/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820847153,
            "cdate": 1698820847153,
            "tmdate": 1699636436684,
            "mdate": 1699636436684,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZySh3PnoHx",
                "forum": "KTf4DGAzus",
                "replyto": "H0nWbr0Avu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1wtR"
                    },
                    "comment": {
                        "value": "Dear reviewer 1wtR:\n\nThank you very much for the review and feedback. We kindly address your questions as follows:\n\n> **W1.1**: *The method uses a heuristic process to perform selection which the capacity is relying on the pre-trained models themselves.*\n> \n\nWe are uncertain about the concept of \u201c**heuristic process\u201d** and \u201c**pre-trained models\u201d**  mentioned by the reviewer. It would be very appreciated if the reviewer could elaborate on it a bit.  A more detailed explanation would greatly assist us in addressing any potential misunderstandings and further refining our work accordingly.\n\nBesides, we would like to clarify that our study isolates the effect of multi-step reasoning quality\u2014-caused by the LLMs\u2014-and focuses on the unexplored model selection scenario for the given reasoning paths. We aim to utilize model selection for each sub-task to further improve the robustness of the multi-modal agent system.\n\n> **W1.2**: *How about the generalization capacity for the zero-shot tasks?*\n> \nAs highlighted in [here](https://openreview.net/forum?id=KTf4DGAzus&noteId=wuDiUQJVxC), creating a novel model selection benchmark is a non-trivial task. Because constrained by the resources, we paid our efforts only on collecting an MS-GQA dataset and haven\u2019t tested the generalization capacity for other zero-shot tasks. We acknowledge the importance of assessing generalization across diverse datasets and plan to incorporate widely recognized datasets in our future research to thoroughly investigate M3's performance in handling zero-shot tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295512533,
                "cdate": 1700295512533,
                "tmdate": 1700298832075,
                "mdate": 1700298832075,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AorMzzBhW2",
            "forum": "KTf4DGAzus",
            "replyto": "KTf4DGAzus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4586/Reviewer_JtbH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4586/Reviewer_JtbH"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the need for improved model selection in multi-modal agents to enhance their robustness in multi-step reasoning tasks. The authors introduce the M3 framework to facilitate dynamic model selection, considering user inputs and subtask dependencies, and they present the MS-GQA dataset as a benchmark for evaluating their framework's performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Identification of Critical Challenge: The paper recognizes and addresses a significant challenge in multi-modal agents, which is the selection of appropriate models for subtasks, a crucial aspect often overlooked in prior research.\n\nIntroduction of the M3 Framework: The paper presents the M3 framework, which aims to improve model selection by considering user inputs and subtask dependencies. The framework is designed with negligible runtime overhead at test-time, making it practical for real-world applications.\n\nCreation of the MS-GQA Dataset: The authors introduce the MS-GQA dataset, specifically designed for investigating model selection challenges in multi-modal agents. This dataset is a valuable resource for benchmarking and advancing research in this area.\n\nExperimental Findings: The paper provides experimental evidence that the M3 framework enhances dynamic model selection and, as a result, bolsters the overall robustness of multi-modal agents in multi-step reasoning tasks."
                },
                "weaknesses": {
                    "value": "Limited Baseline Comparison: The paper could benefit from a more comprehensive comparison of the M3 framework with existing methods. While it claims to outperform traditional model selection methods, a detailed comparison with state-of-the-art techniques would provide a more robust evaluation.\n\nInsufficient Experimental Discussion: The discussion of experimental results could be more in-depth. The paper does not thoroughly analyze the scenarios where the M3 framework performs exceptionally well or falls short. A deeper dive into the results would provide valuable insights into the framework's strengths and limitations.\n\nReal-World Application Discussion: While the paper discusses the practicality of the M3 framework, it could delve further into real-world applications or use cases where this framework could be deployed effectively. This would provide a clearer vision of its potential impact."
                },
                "questions": {
                    "value": "Please see weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4586/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699121543186,
            "cdate": 1699121543186,
            "tmdate": 1699636436386,
            "mdate": 1699636436386,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AaX4iFrgbF",
                "forum": "KTf4DGAzus",
                "replyto": "AorMzzBhW2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JtbH (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer JtbH:\n\nThank you very much for the review and feedback. We kindly address your questions as follows:\n\n> **W1**: *Limited Baseline Comparison: The paper could benefit from a more comprehensive comparison of the M3 framework with existing methods. While it claims to outperform traditional model selection methods, a detailed comparison with state-of-the-art techniques would provide a more robust evaluation*.\n> \n\nWe investigate model selection in multi-step multi-modal reasoning scenarios, a novel research direction that has not been explored previously. Currently, there are no existing methods available for direct use; therefore, all model selection methods in our current baselines are adapted from related domains. We have organized the baselines as follows:\n\n1. For training-free methods, we include classic approaches like GLOBALBEST and other simplistic model selection strategies employed by existing multi-modal agents.\n2. For training-based methods, we present two representative model selection baselines\u2014collaborative filtering-based NCF and meta-learning-based MetaGL. Additionally, for a fairer comparison, we extend NCF and MetaGL to NCF++ and MetaGL++, examining the significance of subtask dependency.\n\nIt would be appreciated if the reviewer could specify suitable references as baselines, and we will include them upon request.\n\n> **W2**: *Insufficient Experimental Discussion: The discussion of experimental results could be more in-depth. The paper does not thoroughly analyze the scenarios where the M3 framework performs exceptionally well or falls short. A deeper dive into the results would provide valuable insights into the framework's strengths and limitations*.\n> \n\nWe have added a more in-depth experimental analysis to reveal the framework's strengths and limitations in our new manuscript. More analysis can be found in Appendix G. We also summarize some key points below.\n\n1. When does M3 perform exceptionally well:\n    \n    **Observations:**\n    \n    - M3 exhibits the best performance on the complete test set (Full) of MS-GQA.\n    - M3 continues to perform exceptionally well on the sub-test sets of MS-GQA.\n    \n    **Analysis:**\n    \n    Compared with other methods, M3 framework's success can be attributed to its unique approach of integrating multi-modal inputs, model (node) embedding, and subtask dependency to model the intricate information flow within the multi-step reasoning process. \n    \n\n2. When does M3 fall short: \n\n    **Observations:**\n\n    - M3 may perform less effectively than some training-free baselines in some sub-test sets of MS-GQA.\n\n    **Analysis:**\n\n    - The training dataset size may affect the performance of training-based methods including M3. As shown in Figure 5 (b), despite M3 performing better than baselines in scenarios with varying degrees of data missing, there is an absolute decline in performance.\n    - Preferences for models vary across different sub-test sets and subtask types. The training-free baseline tends to fixate on a specific model for each subtask type, meaning some training-free methods may perform well on one sub-test set (but may poorly on others).\n\n    We apologize that constrained by the resources, we only collected an MS-GQA dataset with ~8k samples. However, according to Figure 5 (b), we have reason to believe that expanding the size of the dataset further will improve the performance of M3."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294976631,
                "cdate": 1700294976631,
                "tmdate": 1700298801670,
                "mdate": 1700298801670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0wZ8DYeAMU",
                "forum": "KTf4DGAzus",
                "replyto": "AorMzzBhW2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JtbH (2/2)"
                    },
                    "comment": {
                        "value": "> **W3**: *Real-World Application Discussion: While the paper discusses the practicality of the M3 framework, it could delve further into real-world applications or use cases where this framework could be deployed effectively. This would provide a clearer vision of its potential impact.*\n>\nThe practical utility of M3 lies in enhancing the reasoning robustness of multi-modal agents, thereby facilitating their application across various popular domains in the real world. In detail,\n\n- Breaking down and solving complex multi-modal tasks step by step using agents is a leading research approach in addressing multi-modal challenges.\n- [1,2] highlight the prospective applications of multi-modal approaches in areas such as video understanding, finance, and education. Meanwhile, the advancements presented in [3,4,5] have given rise to the potential application of multi-modal agents with per-step tool reliance in domains like robotics and embodied intelligence.\n- When agents call upon different multi-modal AI models to tackle various subtasks in reasoning, it gives rise to the need for model selection techniques. Specifically, considering:\n    1. the richness and abundance of existing multi-modal model types;\n    2. the extensive candidate models;\n    3. the reliability and feasibility demonstrated by model selection in other domains;\n    4. the overly simplistic or less effective model selection strategies employed by current multi-modal agents;\n    \n    Researching model selection in the context of multi-modal reasoning is highly promising and practically valuable in this new scenario.\n\n[1] Gao, Difei, et al. \"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn.\"\u00a0*arXiv preprint arXiv:2306.08640*\u00a0(2023).\n\n[2] Lu, Pan, et al. \"Chameleon: Plug-and-play compositional reasoning with large language models.\"\u00a0*arXiv preprint arXiv:2304.09842*\u00a0(2023).\n\n[3] Yang, Jingkang, et al. \"Octopus: Embodied Vision-Language Programmer from Environmental Feedback.\"\u00a0*arXiv preprint arXiv:2310.08588*\u00a0(2023).\n\n[4] Dalal, Murtaza, et al. \"Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks.\"\u00a0*CoRL 2023 Workshop on Learning Effective Abstractions for Planning (LEAP)*. 2023.\n\n[5] Wen, Licheng, et al. \"On the Road with GPT-4V (ision): Early Explorations of Visual-Language Model on Autonomous Driving.\"\u00a0*arXiv preprint arXiv:2311.05332*\u00a0(2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295054927,
                "cdate": 1700295054927,
                "tmdate": 1700295189080,
                "mdate": 1700295189080,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n7us7zcLIz",
                "forum": "KTf4DGAzus",
                "replyto": "AorMzzBhW2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer JtbH,\n\nWe appreciate your valuable feedback on our manuscript. Upon your request, in our previous response, we have:\n\n- provided explanations for the settings in the existing baselines.\n- expanded our discussions on the experiments.\n- delved deeper into real-world applications and research significance.\n\nWe would appreciate it if you could let us know if our response has addressed your concerns, and we kindly request a reconsideration of the score.\n\nBest, \n\nAuthors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625439707,
                "cdate": 1700625439707,
                "tmdate": 1700625439707,
                "mdate": 1700625439707,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fK1XHflWlm",
            "forum": "KTf4DGAzus",
            "replyto": "KTf4DGAzus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4586/Reviewer_JZBq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4586/Reviewer_JZBq"
            ],
            "content": {
                "summary": {
                    "value": "This paper concentrates on the issue of model selection for multi-modal reasoning tasks. It introduces the Model Selector for the Multi-Modal Reasoning (M3) framework, designed to model the dependencies among subtasks and enhance model selection in multi-modal reasoning. To explore the model selection challenge in multi-modal tasks, the authors have created the MS-GQA dataset. The experiments demonstrate that the M3 framework improves the robustness of reasoning on the MS-GQA dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper adeptly formulates the model selection problem within multi-modal reasoning contexts and constructs the MS-GQA dataset.\n2. The paper is well-founded in its pursuit to address the overlooked subtask dependencies in previous works. The proposed M^3 framework innovatively and effectively models the relationship between samples, selected models, and subtask dependencies.\n3. The experiments conducted on MS-GQA demonstrate the efficiency and efficacy of the M^3 framework."
                },
                "weaknesses": {
                    "value": "1. The primary concern is that model selection is a small part of multi-modal reasoning. It remains to be seen whether it is important for the entire task and how it can benefit real-world applications. The selection method proposed in this paper involves complex proxy training and may need to be more universally applicable or scalable for different reasoning tasks.\n\n2. Lack of reproducibility: The paper must include crucial details, such as the LLM used. The constructed MS-GQA dataset is not yet open-sourced, and the paper fails to provide even a single example of the dataset. Furthermore, the paper does not demonstrate how the proposed methods can improve various reasoning tasks and whether they can be applied to open-source models like LLaMA.\n\n3. The implementation of the baselines is weak: The original HuggingGPT paper dynamically selected models for tasks through in-context task-model assignment, yet this paper describes it as only using \"external metrics\" and implements it as \"choosing the most recently published one for each subtask\", which is misleading and causes unfair comparisons.\n\n4. The experiments could be more convincing: This paper only reports results on a newly created MS-GQA dataset. Even though it's compared to simple baselines that do not require training, like directly using the latest or best models (as shown in Table 2), the proposed M^3 method does not show consistent improvements and may even significantly degrade performance. It would be more convincing if experiments were conducted on more reasoning tasks, as done in VisProg and HuggingGPT."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4586/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699368279350,
            "cdate": 1699368279350,
            "tmdate": 1699636436319,
            "mdate": 1699636436319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DJMjFhaGgk",
                "forum": "KTf4DGAzus",
                "replyto": "fK1XHflWlm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JZBq (1/3)"
                    },
                    "comment": {
                        "value": "Dear reviewer JZBq:\n\nThank you very much for the review and feedback, we kindly address your questions as follows.\n\n> **W1.1:** *The primary concern ... can benefit real-world applications.*\n> \nWe summarize and elucidate the value of exploring model selection in multi-modal reasoning scenarios from four specific perspectives:\n\n1. In Section 2.2, we demonstrate the proven value of model selection in other domains (e.g. outlier detection, time-series prediction, and graph learning);\n2. Table 1 provides experimental evidence, showing that model selection methods successful in other domains do not perform well in multi-modal reasoning scenarios under the agent. Additionally, our analysis in Section 4.3 further indicates that effectively leveraging subtask dependency, unique to multi-step reasoning, significantly improves the successful execution rate of the overall system;\n3. Existing methods still have a significant gap from the oracle model selector, see Appendix E.1 for more details;\n4. A newly added Appendix E.4 and our joint reply discusses the necessity for a reliable model selector in current multi-modal agents. This enhances the overall robustness of the agent system, facilitating their application in real-world domains like finance, education, robotics, and embodied intelligence.\n\n> **W1.2:** *The selection method proposed in this paper involves complex proxy training*\n> \nCertainly, proxy training introduces additional overhead, but:\n1. As indicated in our analysis in Section 4.3, training-free methods exhibit a noticeable performance gap compared to training-based approaches.\n2. Still in Section 4.3, the SOTA model selection methods in other domains, though being training-based, fall short of addressing the multi-modal reasoning scenarios.\n3. As an initial attempt in the field, our proposed framework (M3) incorporates all necessary components required to model the dependent reasoning process for model selection. Our simple design takes less than an hour of training. Section 4.5 further clarifies the efficiency of our method during test-time.\n4. As our next step, we are working on reducing the overhead of proxy training from the aspect of ability assignment and in-context learning. This exploration is beyond the scope of our current manuscript.\n\n> **W2.4:** *whether they can be applied to open-source models like LLaMA*\n> \n> **W2.1:** *The paper must include crucial details, such as the LLM used*\n> \nIn a multi-modal agent scenario, the LLM's role is to decompose GQA questions into AI subtasks and determine their execution order. In our case, we utilize the LLM to construct the MS-GQA dataset, and our method design and experimental results are irrelevant to the LLM. Note that,\n- Our study isolates the effect of multi-step reasoning quality\u2014-caused by the LLMs\u2014-and focuses on the unexplored model selection scenario for the given reasoning paths.\n- As explained in our Section 3, M3 relies on the generated reasoning path from a LLM, and performs the model selection along the path per subtask node.\n- We have elaborated the LLM details in the Appendix A (of the original submission), where we use **GPT-3.5 turbo** as our default LLM.\n\nWe agree with the reviewer that the significance of model selection would be more pronounced when the quality of the reasoning path cannot be guaranteed, e.g., with open-source models. We leave this part for future work.\n\n> **W2.2:** *The constructed MS-GQA dataset is not yet open-sourced, and the paper fails to provide even a single example of the dataset*\n> \nWe have open-sourced our constructed dataset and source code in the supplementary materials.\n\nStatistical information about MS-GQA can be found in Sec 4.1 and Appendix A. Additionally, we have updated the introduction of dataset files and provided examples of samples in Appendix A.2 and A.3. The dataset files are also available in the supplementary materials.\n\nWe also commit to promptly releasing the code and dataset as indicated in our abstract.\n\n> **W2.3:** *the paper does not demonstrate how the proposed methods can improve various reasoning tasks*\n>\nTo showcase the robustness of our method in handling different reasoning tasks:\n1. We categorized the existing MS-GQA dataset into five subgroups/tasks (Query, Choose, Compare, Verify, and Logical) based on question structures, each with distinct reasoning characteristics (see Appendix A.3 for examples).\n2. We also determined the question difficulty by considering the number of candidate models capable of solving each question. Questions solvable by any model selection choices were deemed *the easiest*, while those with limited viable choices were considered *challenging*. Then, questions were categorized by their difficulty levels.\n\nIn both scenarios, the results in Tables 1 and 2, along with the analysis in Section 4.3, demonstrate the robust performance of our method across various reasoning tasks, surpassing other baselines."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700293995524,
                "cdate": 1700293995524,
                "tmdate": 1700298752077,
                "mdate": 1700298752077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wuDiUQJVxC",
                "forum": "KTf4DGAzus",
                "replyto": "fK1XHflWlm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JZBq (2/3)"
                    },
                    "comment": {
                        "value": "> **W1.3:** *may need to be more universally applicable or scalable for different reasoning tasks*\n> \n> **W4.2:** *It would be more convincing if experiments were conducted on more reasoning tasks, as done in VisProg and HuggingGPT.*\n> \nAs the very initial exploration of the community, we established a benchmark and introduced MS-GQA, a significant dataset derived from GQA.\n\n- We assert MS-GQA's representativeness for model selection for multi-modal reasoning' given GQA's wide recognition in the Visual Question Answering community [3,4,5] and its use in testing multi-modal agents like VisProg [1] and ViperGPT [2].\n- Additionally, to validate our framework on diverse test data, we separately divided MS-GQA into five sub-datasets with different question structures and five sub-datasets with varying difficulty levels. (see Sec 4.3)\n- Furthermore, creating a novel model selection dataset is a non-trivial task involving several steps:\n    1. finding a publicly accessible multi-modal reasoning dataset covering various reasoning types; \n    2. identifying the dataset's subtask types (e.g., object detection, segmentation, and visual question answering); \n    3. deploying popular candidate models for different subtask types; \n    4. utilizing existing multi-modal agents to execute the dataset with diverse model selection choices; \n    5. post-processing the results, discarding or revising samples with unsuccessful executions due to engineering problems or errors in the LLM-generated task execution graph. \n    \n    This entire process is laborious and time-consuming, not to mention the significant costs associated with invoking LLM (GPT-3.5-turbo).\n    \n\nWhile we recognize the limitation of conducting experiments solely on MS-GQA, it is important to note that: 1) MS-GQA serves as a representative dataset, 2) our method consistently performs well across various test scenarios, highlighting its robustness, and 3) developing a new model selection dataset presents substantial challenges. Due to these factors, we cannot provide experimental results for different multi-modal reasoning tasks at the moment. \n\nWe are planning to include other widely recognized datasets in future research to strengthen the persuasiveness of our findings, considering our resource limitations.\n\n[1] Gupta, Tanmay, and Aniruddha Kembhavi. \"Visual programming: Compositional visual reasoning without training.\"\u00a0*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2023.\n\n[2] Sur\u00eds, D\u00eddac, Sachit Menon, and Carl Vondrick. \"Vipergpt: Visual inference via python execution for reasoning.\"\u00a0*arXiv preprint arXiv:2303.08128*\u00a0(2023).\n\n[3] Li, Junnan, et al. \"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.\"\u00a0*arXiv preprint arXiv:2301.12597*\u00a0(2023).\n\n[4] Tiong, Anthony Meng Huat, et al. \"Plug-and-play vqa: Zero-shot vqa by conjoining large pretrained models with zero training.\"\u00a0*arXiv preprint arXiv:2210.08773*\u00a0(2022).\n\n[5] Tan, Hao, and Mohit Bansal. \"Lxmert: Learning cross-modality encoder representations from transformers.\"\u00a0*arXiv preprint arXiv:1908.07490*\u00a0(2019).\n\n> **W4.1:** *Even though it's compared to simple baselines that do not require training, like directly using the latest or best models (as shown in Table 2), the proposed M^3 method does not show consistent improvements and may even significantly degrade performance.*\n> \n\nTraining-free methods may perform well on one sub-test set (If their choice happens to align with the preference) but poorly on others, while the M3 framework consistently performs well across all sub-test sets, as mentioned in our paper Section 4.3:\n\n- First of all, preferences for models vary across different sub-test sets and subtask types.\n- The training-free baseline tends to fixate on a specific model for each subtask type, meaning some training-free methods may perform well on one sub-test set (e.g., VisProg performs best in the Compare sub-test set) but poorly on others (e.g., VisProg performs worst in the Query and Choose sub-test sets).\n- Tables 1 and 2 demonstrate that M3 performs best on many sub-test sets and still exhibits strong performance on the remaining sets, showing its robustness.\n\nThe experimental results in Section 4.4 also demonstrate a notable improvement in all training-based methods with an increase in the number of samples. Therefore, although M3 may **occasionally** perform less effectively than training-free methods on the current scale of the MS-GQA dataset, we have reason to believe that M3 will exhibit better performance on larger datasets in the future."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294196770,
                "cdate": 1700294196770,
                "tmdate": 1700295460621,
                "mdate": 1700295460621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1HU0jyBLYU",
                "forum": "KTf4DGAzus",
                "replyto": "fK1XHflWlm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JZBq (3/3)"
                    },
                    "comment": {
                        "value": "> **W3:** The implementation of the baselines is weak: The original HuggingGPT paper dynamically selected models for tasks through in-context task-model assignment, yet this paper describes it as only using \"external metrics\" and implements it as \"choosing the most recently published one for each subtask\", which is misleading and causes unfair comparisons.\n> \n\nThank you for highlighting the issue in our HuggingGPT baseline implementation. We agree with the reviewer that the original HuggingGPT uses in-context learning for model assignment, rather than directly selecting models based on \u201cexternal metrics\u201d. We have made revisions in blue in the new version.\n\nMoreover, we conducted experiments on the \"in-context task-model assignment\" model selection strategy of HuggingGPT, and displayed the partial results in the table below: \n- The selected model consistently remained the same despite changes in question descriptions or structures.\n- The consistency indicates that leveraging the in-context learning capability of LLM currently falls short of achieving genuine and effective dynamic model selection.\n|  | model 1: vilt-b32-finetuned-vqa | model 2: git-base-textvqa | model 3: blip-vqa-base  | model 4: blip2-opt-2.7b | model 5: blip2-flan-t5-xl | model 6: instructblip-vicuna-7b | model 7: instructblip-flan-t5-xl |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Query | 100% | 0% | 0% | 0% | 0% | 0% | 0% |\n| Choose | 100% | 0% | 0% | 0% | 0% | 0% | 0% |\n| Compare | 100% | 0% | 0% | 0% | 0% | 0% | 0% |\n| Logical | 100% | 0% | 0% | 0% | 0% | 0% | 0% |\n| Verify | 100% | 0% | 0% | 0% | 0% | 0% | 0% |\n\nThe corresponding experimental details and observations are outlined below:\n\n- Settings. We randomly selected 100 questions from GQA, and constructed prompts following HuggingGPT's description. 100 questions cover 5 different question structures, and each question structure typically has distinct reasoning characteristics. The corresponding experimental code is included in the supplementary materials.\n- Observations. All questions, irrespective of task type and question itself, are assigned to the first model (vilt-b32-finetuned-vqa) with five 100% values in the first model column.\n- Comment. The experiment above was conducted only in a one-step scenario, where one model is selected for a single task. Though simple, we believe this case is sufficient to state the fact that \u201cin-context task-model assignment\u201d may not be particularly effective in a multi-step setting."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294389433,
                "cdate": 1700294389433,
                "tmdate": 1700294502878,
                "mdate": 1700294502878,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UoiVSRI5k9",
                "forum": "KTf4DGAzus",
                "replyto": "fK1XHflWlm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4586/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear Reviewer JZBq,\n\nWe appreciate your valuable feedback on our manuscript. Upon your request, in our previous response, we have:\n\n- delved deeper into real-world applications and research significance;\n- provided code and dataset examples in the supplementary materials and the revised manuscript to enhance reproducibility;\n- included additional details regarding baseline settings;\n- expanded our discussions on the experiments;\n\nWe would appreciate it if you could let us know if our response has addressed your concerns, and we kindly request a reconsideration of the score.\n\nBest, \n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4586/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579854546,
                "cdate": 1700579854546,
                "tmdate": 1700579907922,
                "mdate": 1700579907922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]