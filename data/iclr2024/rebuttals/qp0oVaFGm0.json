[
    {
        "title": "Iterative Graph Neural Network Enhancement Using Explanations"
    },
    {
        "review": {
            "id": "FB4WhRZV6r",
            "forum": "qp0oVaFGm0",
            "replyto": "qp0oVaFGm0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7471/Reviewer_9PFw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7471/Reviewer_9PFw"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to enhance Graph Neural Networks (GNN) using Explanatory Artificial Intelligence (XAI). Specifically, the EXPLANATION ENHANCED GRAPH LEARNING (EEGL) strategy is introduced for node classification tasks in GNNs. It utilizes frequent connected subgraph mining to identify and analyze patterns in explanatory subgraphs. And iteratively update feature matrix by annotating subgraph information."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Introducing interpretation into graph neural network architecture is interesting.\n2. The research topic is important."
                },
                "weaknesses": {
                    "value": "1. My first concern is the complexity of the model. In every iteration, the approach starts by utilizing a node explainer to derive all explanation graphs. Subsequently, it calculates the maximal frequent rooted subgraphs for all labels. The algorithm then selects the top-k rooted subgraphs by processing them through the GNN and determining the F1-score. Finally, a feature annotation module updates the feature matrix X. Notably, in the experiments, each iteration on one fold takes about 1 hour and 20 minutes. I urge the authors to provide a thorough analysis of their method's complexity and compare its runtime to other baseline models.\n2. My second point of contention lies in the experimental setup. The absence of baseline models for comparison in the authors\u2019 work makes it hard  to convincingly showcase the efficacy of their proposed method.\n3. My third concern is that the datasets are all Synthetic Data. I highly recommend authors to incorporate more real-world dataset for experiments.\n4. The paper's presentation lacks clarity, particularly in section 3 where the notation introduction feels disorganized. I suggest that the authors compartmentalize definitions into distinct sections or blocks for better readability. Furthermore, the method's description relies solely on textual explanations. Incorporating a diagram or figure to illustrate the workflow of the proposed method would enhance understanding."
                },
                "questions": {
                    "value": "Please refer to Weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7471/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7471/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7471/Reviewer_9PFw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698182702476,
            "cdate": 1698182702476,
            "tmdate": 1699636901077,
            "mdate": 1699636901077,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mXJKSBjCFt",
                "forum": "qp0oVaFGm0",
                "replyto": "FB4WhRZV6r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7471/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their review of our paper. In the response we refer to weaknesses by W.\n\n**Answer to W1:**  In the updated version we are going to provide details.\n\n**Answer to W2:** Thank you very much for this question. We consider it to be a reformulation of Q2 in the introduction. We interpret the question as comparisons to single runs of GNN involving various non-adaptive initialization techniques. Answers are given on page 8.\n\n**Answer to W3:** The choice of using synthetic data is explained in detail in Section 4.1 (page 5), building upon the negative findings of [18] regarding real-world benchmarks for GNN node classification.\n\n**Answer to W4:** Thanks for suggesting compartmentalization. As the notation is standard, we tried to present it as compactly as possible in order to save space. We will introduce subsections. \n\nAlgorithm 1 on page 3 contains the pseudocode of the method. As noted on page 3, a diagram is included as Figure 4 in  Appendix B. Both could not be included in the main text due to lack of space."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700139076508,
                "cdate": 1700139076508,
                "tmdate": 1700139076508,
                "mdate": 1700139076508,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FJenHpJOVO",
            "forum": "qp0oVaFGm0",
            "replyto": "qp0oVaFGm0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7471/Reviewer_FGzN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7471/Reviewer_FGzN"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers using explanation to update input graphs iteratively, guiding the model to rely on only identified critical elements for node classification. Particularly, it want to discover frequent subgraph structures, and extend node attributes with indicators of subgraph isomorphism existence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It explores using GNN explanations to augment node attributes, and test its potential to improve GNN performance for node classification. It is an interesting and challenging direction\n2. It designs EEGL, containing both subgraph pattern extraction module and feature annotation module to iteratively examine explanations and augment node representations. Algorithms are provided for each components.\n3. It shows promising results in experiments."
                },
                "weaknesses": {
                    "value": "1. The basic assumption seems incomplete. When GNN models can not go beyond 1-WL algorithm, why augmenting graphs using their explanations can help? Their explanations would be the same for nodes with 1-WL isomorphism.\n2. Experiments are incomplete. No comparisons are made with other graph augmentation techniques. No comparisons are made with other explanation-guided learning strategies. And its influence to different GNN architectures are not tested.\n3. Time complexity is missing. The pattern detection and node attribution modules may require a lot of time to run. Authors discussed about two techniques used, and talked about running time for one iteration. But the complexity analysis and comparisons are needed for fully understand its computation cost."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801964138,
            "cdate": 1698801964138,
            "tmdate": 1699636900900,
            "mdate": 1699636900900,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AgwPnde9Wh",
                "forum": "qp0oVaFGm0",
                "replyto": "FJenHpJOVO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7471/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their review of our paper. In the response we refer to weaknesses by W.\n\n**Answer to W1:** The explainer returns a rooted subgraph for each node (for example, a triangle). This subgraph is found by a sensitivity-type analysis. It is unrelated to the 1-WL algorithm. Thus explanations for ``non-isomorphic'' nodes that are 1-WL indistinguishable can be different. \n\n**Answer to W2:** Graph augmentation techniques (see Sec. 5.2 in Morris et al., 2021) use subgraph augmentation for graph classification problems. The only exception we are aware of is the paper by Zeng et al. (2022). We plan to make a comparison to this technique. As far as we know, explanation-guided GNN learning has not been considered before. Our approach can be used with any GNN learner and explainer. The detailed study of the effect of using different systems requires a separate paper, which is in progress.\n\n**Answer to W3:** The Reviewer is right, this was indeed the case. In the updated version we are going to provide details."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138859243,
                "cdate": 1700138859243,
                "tmdate": 1700138859243,
                "mdate": 1700138859243,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F1gLSqtRaJ",
            "forum": "qp0oVaFGm0",
            "replyto": "qp0oVaFGm0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7471/Reviewer_dVAw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7471/Reviewer_dVAw"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a framework, EEGL, to enhance graph neural networks by incorporating the explanations. The effectiveness of the proposed work has been demonstrated on the synthetic datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of using explanation to enhance graph neural networks is convincing."
                },
                "weaknesses": {
                    "value": "1.\tThe paper demands a meticulous review by a native English speaker, to refine its clarity and presentation. While numerous areas require attention, highlighted below are some specific instances. It should be noted that this isn't an exhaustive list:   \na.\tIt will be better to provide the full term before transitioning to an abbreviation. For instance, before using 'XAI', its complete form should be mentioned.  \nb.\tIn the abstract, the statement \u201cEEGL is an iterative algorithm\u2026.. in the node neighborhoods\u201d is too long to understand. Following that, the sentence \u201cGiving an application-dependent algorithm for such an extension of the Weisfeiler-Leman (1-WL) algorithm has been posed as an open problem.\u201d appears disjointed from the context and needs rephrasing for clarity.  \nc.\tThe term 'MPNN' typically stands for \"message passing neural networks.\" Instead of \u201cMessage-passing GNN\u201d.  \nd.\tThe commonly-used abbreviation for graph neural networks is 'GNNs.' If the paper opts to use 'GNN' as the abbreviation, phrasing like \"GNN form\" in the introduction should be revised to \"The GNN forms.\u201d  \ne.\tIn Figure 1, the meaning of designations such as \u201cM1, M1\u2019, M1\u2019\u2019, M2, M2\u2019, M2\u2019\u2019\u201d should be elucidated within the caption.   \nf.\tThe current citations format does not adhere to the official template provided.   \n2.\tThe exclusive reliance on synthetic datasets without any inclusion of real-world datasets diminishes the empirical strength of the paper. While the authors have proffered reasons for this choice, the graph generation model utilized is rather simple. Incorporating a more enhanced model, such as Stochastic Block Model, might enhance the quality and representativeness of the generated graphs, allowing for a more robust validation of the proposed method.\n\nIn summary, while the paper holds potential, its current presentation hinders a comprehensive review. I'm eager to provide a more detailed review if the authors could provide an updated version."
                },
                "questions": {
                    "value": "See the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699252465978,
            "cdate": 1699252465978,
            "tmdate": 1699636900773,
            "mdate": 1699636900773,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1jZ2xnKjTF",
                "forum": "qp0oVaFGm0",
                "replyto": "F1gLSqtRaJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7471/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their review of our paper. In the response we refer to weaknesses by W.\n\n**Answer to W1:** Weaknesses a)-f) are all minor notation, terminology and style issues, which will be addressed in the updated version. \n\n**Answer to W2:** A detailed justification for considering synthetic data is given in the first paragraph of Sect. 4.1. The particular model we use is a variant which has been considered in many forms since its introduction in (Ying et al., 2019). Increasing motif complexity, the number of motifs, and the graph-theoretical properties of the labeling can make it very complex and challenging. Our experiments are about exploring this challenge. On the other hand, models, such as the stochastic block model, are useful\nwhen the underlying graph serves as a similarity relation (homophily, heterophily) for the node feature vectors, but not for node classification tasks, where the class labels are determined by graph patterns. Thus the stochastic bloc model is not relevant in our context. The working assumptions discussed in the first paragraphs on p. 2 imply that our focus is orthogonal to this situation. \n\n**It is not clear why the apparently minor issues listed in Weakness 1 lead to the Reviewer's withholding their more detailed review.**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138673437,
                "cdate": 1700138673437,
                "tmdate": 1700138673437,
                "mdate": 1700138673437,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iJ1GpQVirf",
            "forum": "qp0oVaFGm0",
            "replyto": "qp0oVaFGm0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7471/Reviewer_CfEt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7471/Reviewer_CfEt"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new XAI-based iterative approach called Explanation Enhanced Graph Learning (EEGL) to enhance the performance of node classification by focusing on explanation subgraph structures. The proposed method applies frequent subgraph mining to explanation graphs to find helpful patterns in each class. Then, discovered patterns are used to annotate the feature matrix for GNN. Experiments show that the proposed algorithm can improve the performance on various synthetic data that the 1-WL label cannot distinguish."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed algorithm EEGN is a novel approach that uses frequent subgraph mining to XAI of GNNs for node classification.\n\n- EEGL can improve the performance of node classification by iteratively annotating the information of explanation subgraphs obtained by frequent subgraph mining into a feature matrix.\n\n- This paper empirically shows the effect of feature initialization of GNNs by comparing randomly assigned labels and labels obtained from EEGL-GNN, which can predict labels of synthetic datasets that 1-WL cannot distinguish."
                },
                "weaknesses": {
                    "value": "- It requires additional computational cost to update the feature matrix in larger graphs when finding induced subgraphs. This additional cost can be high, hence it should be empirically evaluated.\n\n- In the pattern extraction module, hyperparameters are determined by the rule of thumb. It is unclear how sensitive the frequent threshold $\\tau$ and an upper bound $N$ are in frequent subgraph mining and how much subgraph structures affect the predictive performance."
                },
                "questions": {
                    "value": "- Does R0 means that the feature matrix is initialized with one? It seems that this initialization causes the fact that GCN cannot train well and predict a label as an almost random class in the M2 dataset. Is it due to ill-initialization or the nature of the 1-WL algorithm of GNN?  \n\n- As the number of iterations increases, does the annotated feature matrix X converge to that equivalent to the label encoder, and the subgraph structures obtained from GNNExpiner will be equivalent to explanation subgraphs when initialized by the feature matrix with the label encoder?\n\n- There are some typos in the paper, such as in Sec 4.2, CGN (GCN?), the subscript of R, Table 4.2 (no need), and the label in Figure 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7471/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7471/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7471/Reviewer_CfEt"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7471/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699490825700,
            "cdate": 1699490825700,
            "tmdate": 1699636900513,
            "mdate": 1699636900513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dHS21SnkK0",
                "forum": "qp0oVaFGm0",
                "replyto": "iJ1GpQVirf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7471/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7471/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their review of our paper.\nIn the response we refer to weaknesses by W, and questions by Q.\n\n**Answer to W1:** \nThe Reviewer is right, this was indeed the case. In the updated version we are going to provide details.\n\n**Answer to W2:** The effect of subgraph structures has been the focus of our experiments in the current paper, and Section 4.2 contains several qualitative observations beyond the quantitative results. The choice of the graph mining hyperparameters may be application-dependent and it would require detailed further study.\n\n**Answer to Q1:** Thank you very much for this valid question. Regarding your first question, the answer is yes. This is the vanilla setting in the sense that (effectively) no node feature is used in GCN. For this case, the poor predictive performance of GNN on 1-WL-indistinguishable nodes follows from its inherent limitation. \nThus the answer to the second question is that it is the nature of 1-WL.\n(In fact, EEGL iteratively turns an ill initialization into a non-ill one.) \nAs an example, consider motif $M_2'$ (cf. Fig.~1). Note that vanilla GCN correctly classifies all nodes of labels 4 (cf. the confusion matrix for Round-0 in Fig. 2) and that all other nodes are classified with a label different from 4, except for the 22 nodes with label 8. The reason is that although nodes with label 4 and 8 are probably 1-WL distinguishable in the entire graph due to the presence of the base graph, they are hard to distinguish for GNN. We recall that the limitations of the distinguishing power of 1-WL provide only an upper bound on that of GNN.\n\n**Answer to Q2:** Thank you very much for this great question! This is exactly what we plan to do in future work described as Q5 at the very end of the paper. As discussed in that paragraph, there are several interesting challenges in this direction, starting with defining the proper metric (some kind of graph distance) for convergence. \n\n**Answer to Q3:** They will be fixed in the updated version."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7471/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700138503927,
                "cdate": 1700138503927,
                "tmdate": 1700138503927,
                "mdate": 1700138503927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]