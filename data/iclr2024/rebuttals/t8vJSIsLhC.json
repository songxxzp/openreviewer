[
    {
        "title": "SMPE: A Framework for Multi-Dimensional Permutation Equivariance"
    },
    {
        "review": {
            "id": "I9bSha9UJQ",
            "forum": "t8vJSIsLhC",
            "replyto": "t8vJSIsLhC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5733/Reviewer_y2bA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5733/Reviewer_y2bA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multi-dimensional permutation equivariance framework named SMPE to pave the way for the design of multi-dimensional permutation equivariant networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This work provides the first exact algebra-based definition of multi-dimensional permutation equivariance.\n\n- The experiments are extensive, including contextual auction design, pseudo inverse computation, and typical wireless communication tasks.\n\n- The research can benefit many real applications including point cloud analysis, graph analysis, pseudo inverse computation, typical wireless communication, etc.\n\n- The proposed technique seems sound, and its effectiveness is verified by experiments."
                },
                "weaknesses": {
                    "value": "- The running time of all the methods could be compared to show the efficiency of the proposed method.\n\n- The hyperparameter settings of the model should be listed in the text.\n\n- Two related works [1,2] about graph permutation invariance are missing in the discussion.\n\n- The source codes and the datasets could be provided to facilitate the reproducibility of this work.\n\nRefs:\n\n[1] [arXiv 2019] PiNet: A Permutation Invariant Graph Neural Network for Graph Classification\n\n[2] [TKDE 2022] Graph Substructure Assembling Network with Soft Sequence and Context Attention"
                },
                "questions": {
                    "value": "Please respond to the weaknesses listed in the previous text box."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have not identified any ethical concerns about this work."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697725109169,
            "cdate": 1697725109169,
            "tmdate": 1699636600361,
            "mdate": 1699636600361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RJ4yGwplNh",
                "forum": "t8vJSIsLhC",
                "replyto": "I9bSha9UJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparison of running times to show efficiency"
                    },
                    "comment": {
                        "value": "Many thanks for your careful reviews and constructive comments.\n\nThe running times of all the methods have been compared in Appendix F.2. Specifically, the following table provides a comparison of the runtime for each network across different tasks. The runtime refers to the average time (ms) taken for a batch during GPU inference, with a batch size of 1000. In the first row, '1',  '2' and '3' denote experiments in Sections 4.1, 4.2, and 4.3, respectively. ${\\mathcal{A}}$, ${\\mathcal{B}}$, and ${\\mathcal{C}}$ represent the scenario configurations for each task during inference. It can be observed that the running time of $\\mathtt{SMPEN-DS}$ is comparable to that of $\\mathtt{ETN}$, and the running time of $\\mathtt{SMPEN2D-TF}$ is similar to that of $\\mathtt{CITransNet2D}$.\n\n|                                | 1$\\\\mathcal{A}$ | 1$\\\\mathcal{B}$ | 1$\\\\mathcal{C}$ | 2$\\\\mathcal{A}$ | 2$\\\\mathcal{B}$ | 2$\\\\mathcal{C}$ | 3$\\\\mathcal{A}$ | 3$\\\\mathcal{B}$ | 3$\\\\mathcal{C}$ |\n| :----------------------------: | :---------------: | :---------------: | :---------------: | :---------------: | :---------------: | :---------------: | :---------------: | :---------------: | :---------------: |\n| $\\\\mathtt{SetTrans1D}$       | 61\\.7             | 67\\.5             | 72\\.1             | 16\\.4             | 18\\.6             | 24\\.3             |                   |                   |                   |\n| $\\\\mathtt{CITransNet2D}$     | 87\\.9             | 106\\.0            | 119\\.1            | 48\\.2             | 53\\.8             | 64\\.4             | 34\\.8             | 34\\.9             | 63\\.2             |\n| $\\\\mathtt{ETN2D}$            | 61\\.9             | 69\\.2             | 75\\.8             | 18\\.7             | 21\\.9             | 28\\.4             | 13\\.8             | 13\\.8             | 27\\.1             |\n| $\\\\mathtt{SMPEN2D\\\\!-\\\\!DS}$ | 65\\.2             | 75\\.6             | 80\\.4             | 21\\.3             | 24\\.6             | 31\\.4             | 12\\.8             | 12\\.9             | 24\\.7             |\n| $\\\\mathtt{SMPEN2D\\\\!-\\\\!TF}$ | 87\\.8             | 109\\.1            | 129\\.0            | 51\\.2             | 59\\.9             | 69\\.1             | 35\\.0             | 35\\.1             | 63\\.4             |\n| $\\\\mathtt{ETN3D}$            |                   |                   |                   |                   |                   |                   | 25\\.6             | 25\\.7             | 46\\.9             |\n| $\\\\mathtt{SMPEN3D\\\\!-\\\\!DS}$ |                   |                   |                   |                   |                   |                   | 23\\.1             | 23\\.2             | 42\\.5             |\n| $\\\\mathtt{SMPEN3D\\\\!-\\\\!TF}$ |                   |                   |                   |          |                   |                    | 55\\.4             | 55\\.5             | 99\\.3             |"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493385081,
                "cdate": 1700493385081,
                "tmdate": 1700493949479,
                "mdate": 1700493949479,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4A9w2ODeId",
                "forum": "t8vJSIsLhC",
                "replyto": "I9bSha9UJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Listing of network hyperparameter configurations"
                    },
                    "comment": {
                        "value": "The hyperparameters used in our experiments are listed in Appendix E.4\nof the revised paper. For example\n\n-   Pooling functions: Without loss of generality, all the pooling\n    operations are set to be the mean function.\n\n-   Training strategy: We perform batch iterations for $8\\times 10^{4}$\n    times with a batch size of 512, and reduce the learning rate by half\n    after $4\\times 10^{4}$ iterations. This strategy for three tasks is\n    the same. For the design task of MIMO and cell-free MIMO system\n    transmission, we randomly assign SNR values to each training and\n    testing sample.\n\n-   Activation & normalization: To accelerate convergence and enhance\n    the stability of the training process, we applied the ReLU\n    activation and LN to $\\mathtt{ETN2D}$ and $\\mathtt{ETN3D}$.\n\n-   Monte-Carlo simulation: Each experimental result is the average of\n    three valid experiments."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493502976,
                "cdate": 1700493502976,
                "tmdate": 1700493502976,
                "mdate": 1700493502976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DQYYlblYEZ",
                "forum": "t8vJSIsLhC",
                "replyto": "I9bSha9UJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion about the two related works [1][2]"
                    },
                    "comment": {
                        "value": "In \\[1\\], a **one-dimensional** invariant network was constructed based\non the proposed variable node attention pooling mechanism, which\neffectively addressed **graph classification problems**. Due to its\npermutation invariance, it is applicable to graphs of different sizes.\n\nThe work in \\[2\\] introduces the Substructure Assembling Network (SAN),\na graph neural network model that effectively learns interpretable and\ndiscriminative graph representations for **classification**. It\novercomes challenges by hierarchically assembling substructures using\nthe Substructure Assembling Unit (SAU) and enhancing performance with\nSoft Sequence with Context Attention (SSCA).\n\nThe goal of these two works is to model **one-dimensional** invariant\nmappings, which is relevant to our work but not the primary focus of our\ninvestigation. We have incorporated these two works into the 'Related\nWork' section and provided additional discussions.\n\n\\[1\\] Meltzer P, Mallea M D G, Bentley P J. Pinet: A permutation\ninvariant graph neural network for graph classification\\[J\\]. arXiv\npreprint arXiv:1905.03046, 2019.\n\n\\[2\\] Yang Y, Guan Z, Zhao W, et al. Graph substructure assembling\nnetwork with soft sequence and context attention\\[J\\]. IEEE Transactions\non Knowledge and Data Engineering, 2022, 35(5): 4894-4907."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493599154,
                "cdate": 1700493599154,
                "tmdate": 1700493599154,
                "mdate": 1700493599154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gzHfAHlPDw",
                "forum": "t8vJSIsLhC",
                "replyto": "I9bSha9UJQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The source codes and the datasets for reproducibility"
                    },
                    "comment": {
                        "value": "The source code and associated datasets will be provided soon, and we\nare currently making efforts to organize the code and add comments to\nimprove reproducibility."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700493620072,
                "cdate": 1700493620072,
                "tmdate": 1700493620072,
                "mdate": 1700493620072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lK9qTRf22e",
            "forum": "t8vJSIsLhC",
            "replyto": "t8vJSIsLhC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5733/Reviewer_QBbk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5733/Reviewer_QBbk"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method for obtaining neural networks with higher-order permutation equivariances. This concerns tensors where multiple dimensions are equivariant. The main approach uses a composition of first order permutation equivariant functions. Features are pooled from all subsets of dimensions to facilitate exchange of information between the different dimensions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Straightforward and easy to understand method, well-written\n- I believe the method to be novel\n- While there is existing work handling this subject, I believe that this is a useful *practical* contribution to the literature.\n- Interesting applications used in experiments, solid results"
                },
                "weaknesses": {
                    "value": "- I believe that this topic has been studied before as part of more general treatments of permutation equivariance, such as in [1] under the name of higher-order permutations. As such, the techniques used themselves are fairly standard and novelty therefore limited, even if their exact combination is new.\n- The experiments could be made stronger by comparing on existing settings, so that it's possible to compare against existing numbers in papers. Some of the performance benefits of the proposed method could be explained simply by insufficient tuning of the baseline models.\n- It would be good to see further ablation to test some of the claims made in the paper. For example, it would be good to check the performance impact of the parallel computation vs sequential computation.\n\n[1] https://arxiv.org/pdf/2004.03990.pdf"
                },
                "questions": {
                    "value": "It seems like using all possible subsets can be limiting in terms of scalability. Can you think of more efficient alternatives that don't sacrifice too much in model capacity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698182591315,
            "cdate": 1698182591315,
            "tmdate": 1699636600233,
            "mdate": 1699636600233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4LaFaODBY6",
                "forum": "t8vJSIsLhC",
                "replyto": "lK9qTRf22e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The difference between high-order permutation equivariance and high-dimensional permutation equivariance"
                    },
                    "comment": {
                        "value": "Thank you for your thorough reviews and valuable feedback. \n\nThe **high-order** permutation equivariance considers the high-order\nfeatures of a single type of object. Assuming there are $K$ objects,\ntheir $N$th-order features among each other are denoted as\n${{{\\bf{\\mathsf{X}}}}}\\in{\\mathbb R}^{K\\times K\\times \\cdots\\times K\\times D\\_X}$,\nwhere $K$ is repeated $N$ times in the dimension, so it can also be\nwritten as ${\\bf{\\mathsf{X}}}\\in{\\mathbb R}^{K^N\\times D\\_X}$ \\[1\\]. The\nfunction $f$ exhibits $N$th-order permutation equivariance if it\nsatisfies $$\\begin{aligned}\n    f(\\sigma\\circ{\\bf{\\mathsf{X}}})=\\sigma\\circ f({\\bf{\\mathsf{X}}}),\\ (\\sigma\\circ{\\bf{\\mathsf{X}}})\\_{[k\\_1,...,k\\_N,:]} = {\\bf{\\mathsf{X}}}\\_{[\\sigma^{-1}\\circ k\\_1,...,\\sigma^{-1}\\circ k\\_N,:]},\\ \\sigma\\in {\\mathbb S}\\_K.\n    \\end{aligned}$$ It can be observed that the same permutation\n$\\sigma$ is employed across all dimensions. **High-order** permutation\nequivariance is an extension of **one-dimensional** permutation\nequivariance \\[1\\]\\[2\\]. The relevant literature on high-order\nequivariance also includes references \\[3\\], \\[4\\], \\[5\\], and so on.\nThe **multi-dimensional** equivariance considers features among multiple\ntypes of objects, i.e.,\n${{{\\bf{\\mathsf{X}}}}}\\in{\\mathbb R}^{K\\_1\\times K\\_2\\times \\cdots\\times K\\_N\\times D\\_X}$.\nA mapping $f$ is said to be $N$-dimensional equivariant if\n$$\\begin{aligned}\n    f\\left({\\pi}^{\\mathbb N}\\circ{{{\\bf{\\mathsf{X}}}}}\\right) = {\\pi}^{\\mathbb N}\\circ f({{{\\bf{\\mathsf{X}}}}}),\\ ({\\pi}^{\\mathbb N}\\circ {\\bf{\\mathsf{X}}})\\_{[k\\_1,k\\_2,...,k\\_N,:]} = {{\\bf{\\mathsf{X}}}}\\_{[\\pi^{-1}\\_{K\\_1}(k\\_1),\\pi^{-1}\\_{K\\_2}(k\\_2),...,\\pi^{-1}\\_{K\\_N}(k\\_N),:]},\\ \\forall {\\pi}^{\\mathbb N}\\in {\\mathbb S}^{\\mathbb N}.\n    \\end{aligned}$$ **multi-dimensional** permutation equivariance\nconsiders different permutations $\\pi\\_{K\\_1},...,\\pi\\_{K\\_N}$ across\nmultiple dimensions, which is clearly distinct from high-order\npermutation equivariance.\n\n\\[1\\] Maron H, Ben-Hamu H, Shamir N, et al. Invariant and equivariant\ngraph networks\\[J\\]. arXiv preprint arXiv:1812.09902, 2018.\n\n\\[2\\] Thiede E H, Hy T S, Kondor R. The general theory of permutation\nequivarant neural networks and higher order graph variational\nencoders\\[J\\]. arXiv preprint arXiv:2004.03990, 2020.\n\n\\[3\\] Keriven N, Peyr\u00e9 G. Universal invariant and equivariant graph\nneural networks\\[J\\]. Advances in Neural Information Processing Systems,\n2019, 32.\n\n\\[4\\] Pan H, Kondor R. Permutation equivariant layers for higher order\ninteractions\\[C\\]. International Conference on Artificial Intelligence\nand Statistics. PMLR, 2022: 5987-6001.\n\n\\[5\\] Kim J, Oh S, Hong S. Transformers generalize deepsets and can be\nextended to graphs & hypergraphs\\[J\\]. Advances in Neural Information\nProcessing Systems, 2021, 34: 28016-28028."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490497655,
                "cdate": 1700490497655,
                "tmdate": 1700490497655,
                "mdate": 1700490497655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PPM39JIxXN",
                "forum": "t8vJSIsLhC",
                "replyto": "lK9qTRf22e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The novelty of the proposed framework"
                    },
                    "comment": {
                        "value": "In this work, **we propose a plug-and-play paradigm for the construction\nof arbitrary high-dimensional equivariant networks with well-designed\none-dimensional equivariant networks**. Specifically, we first provide a\ndefinition of multi-dimensional equivariance based on algebraic\ndefinitions. Based on this, we further propose two methods to combine\none-dimensional equivariant functions, which have been mathematically\nproven to satisfy the equivariant property in multiple dimensions.\nMoreover, we employed feature reuse and introduced global information at\neach layer to enhance performance while maintaining the\nmulti-dimensional equivariance. Additionally, it has been proven that\nthe constructed equivariant layer can degenerate into the\nmulti-dimensional equivariant linear layer characterized by a complex\nmathematical expression. The substitutability of one-dimensional\nequivariant functions in the proposed framework implies its enhanced\ngenerality, effectively transforming the design problem of\nmulti-dimensional equivariant networks into the selection of\none-dimensional equivariant networks."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490518014,
                "cdate": 1700490518014,
                "tmdate": 1700490518014,
                "mdate": 1700490518014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PVOWqHzKVv",
                "forum": "t8vJSIsLhC",
                "replyto": "lK9qTRf22e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparison of experimental setting"
                    },
                    "comment": {
                        "value": "The hyperparameters used in our experiments are listed in Appendix E.4\nof the revised paper. These hyperparameters of the baseline models have\nbeen fine-tuned, and we reported the best results for comparison. For\nexample:\n\n-   Pooling functions: Without loss of generality, all the pooling\n    operations are set to be the mean function.\n\n-   Activation & normalization: To accelerate convergence and enhance\n    the stability of the training process, we applied the ReLU\n    activation and LN to $\\mathtt{ETN2D}$ and $\\mathtt{ETN3D}$.\n\n-   Parameter initialization: With the exception of networks\n    $\\mathtt{SMPEN2D-TF}$ and $\\mathtt{SMPEN3D-TF}$ in the task\n    of Section 4.3, which were initialized using the default method, we\n    employed Kaiming initialization for the weight parameters of the\n    linear layers in networks used for other tasks.\n\n-   Monte-Carlo simulation: Each experimental result is the average of\n    three valid experiments."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490726161,
                "cdate": 1700490726161,
                "tmdate": 1700490726161,
                "mdate": 1700490726161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NvJ7eAU2Ab",
                "forum": "t8vJSIsLhC",
                "replyto": "lK9qTRf22e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ablation experiments for the parallel computation and sequential computation"
                    },
                    "comment": {
                        "value": "The ablation experiments have been included in Appendix F.1 of the\nrevised paper. Specifically, to validate the superiority of\n$g\\_{\\rm serial}$ (represented by $\\mathtt{ParaN2D-TF}$) over\n$g\\_{\\rm para}$ (represented by $\\mathtt{SerialN2D-TF}$), as well as\nthe superiority of $g\\_{\\rm SMPE}$ over $g\\_{\\rm serial}$, we compared the\nperformance of these three methods on the task in Section 4.2. In most scenarios, as observed from the table below, $g\\_{\\rm SMPE}$ outperforms $g\\_{\\rm serial}$, which, in turn, outperforms $g\\_{\\rm para}$.\n\n| **Test/Train Setting**           | $\\\\mathcal{A}$/$\\\\boldsymbol{\\\\mathcal{A}}$ | $\\\\mathcal{A}$/$\\\\mathcal{B}$ | $\\\\mathcal{A}$/$\\\\mathcal{C}$ | $\\\\mathcal{B}$/$\\\\mathcal{A}$ | $\\\\mathcal{B}$/$\\\\boldsymbol{\\\\mathcal{B}}$ | $\\\\mathcal{B}$/$\\\\mathcal{C}$ | $\\\\mathcal{C}$/$\\\\mathcal{A}$ | $\\\\mathcal{C}$/$\\\\mathcal{B}$ | $\\\\mathcal{C}$/$\\\\boldsymbol{\\\\mathcal{C}}$ |\n| :------------------------------: | :---------------------------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: | :---------------------------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: | :---------------------------------------------: |\n| $\\\\mathtt{ParaN2D\\\\!-\\\\!TF}$   | 20\\.49                                          | 20\\.39                            | 20\\.33                            | 21\\.72                            | 21\\.87                                          | 21\\.62                            | 21\\.99                            | 21\\.61                            | 21\\.97                                          |\n| $\\\\mathtt{SerialN2D\\\\!-\\\\!TF}$ | 20\\.75                                          | 21\\.48                            | 19\\.08                            | 21\\.91                            | 22\\.94                                          | 20\\.31                            | 19\\.26                            | 20\\.18                            | 24\\.33                                          |\n| $\\\\mathtt{SMPEN2D\\\\!-\\\\!TF}$   | **22\\.21**                                      | 22\\.70                            | 20\\.69                            | 23\\.78                            | **24\\.71**                                      | 22\\.30                            | 21\\.80                            | 23\\.17                            | **24\\.44**                                      |"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492656533,
                "cdate": 1700492656533,
                "tmdate": 1700492656533,
                "mdate": 1700492656533,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c7JpbARY82",
                "forum": "t8vJSIsLhC",
                "replyto": "lK9qTRf22e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "More efficient alternatives without sacrificing too much model capacity"
                    },
                    "comment": {
                        "value": "First, we would like to illustrate the primary sources of complexity in\nthe framework. The computational complexities and parameter quantities\nare shown as follows\n\n| Layer Type              | Complexity                                                                             | Parameters                             |\n|-------------------------|----------------------------------------------------------------------------------------|----------------------------------------|\n| $\\mathtt{SMPEL-DS}$ | $\\mathcal{O}\\left((\\Pi^{N}_{n=1}K_n)\\cdot(2^N+N+1)\\cdot D^2\\right)$                    | $\\mathcal{O}\\left((2^N-1+N)D^2\\right)$ |\n| $\\mathtt{SMPEL-TF}$ | $\\mathcal{O}\\left((\\Pi^{N}\\_{n=1}K_n)\\cdot[(2^N+N-1)D+\\sum_{n=1}^{N}K_n]\\cdot D\\right)$ | $\\mathcal{O}\\left((2^N-1+N)D^2\\right)$ |\n| $\\mathtt{ETL}$          | $\\mathcal{O}\\left((\\Pi^{N}_{n=1}K_n)\\cdot2^N\\cdot D^2\\right)$                          | $\\mathcal{O}(2^ND^2)$                  |\n\nThe total number of input features is $\\Pi\\_{n=1}^NK\\_n$. This implies\nthat under the condition of a fixed input dimensions $N$, the\ncomputational complexity orders of $\\mathtt{SMPEL-DS}$ and\n$\\mathtt{SMPEL-TF}$ are lower than the second order of\n$\\Pi\\_{n=1}^NK\\_n$, and the parameter quantity is also independent of\n$\\Pi\\_{n=1}^NK\\_n$. Therefore, the main factor that limits the scalability\nof the framework is the exponential expansion of network size with the\ngrowth of dimension $N$. The number $2^N$ in this table arises from the\nalgorithm's requirement to obtain pooled information for every subset in\nthe power set of ${\\mathbb N}\\backslash\\{\\emptyset\\}$ and incorporate it\ninto the computation.\n\nThe exploration of efficient alternatives that do not significantly\ncompromise model capacity is an interesting topic. We are currently\nconducting further research about this problem. From a practical\nimplementation perspective, the following method can achieve a trade-off\nbetween performance and complexity: Train the network for multiple\ntimes, with each training randomly selecting $H$ subsets from the\n$2^N-1$ subsets to include in the computation. Then, the best performing\nnetworks are retained. This approach allows for the identification of\nthe most important $H$ combinations, effectively reducing the scale from\n$2^N-1$ to $H$."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492892573,
                "cdate": 1700492892573,
                "tmdate": 1700492892573,
                "mdate": 1700492892573,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IV5yNuHJaL",
            "forum": "t8vJSIsLhC",
            "replyto": "t8vJSIsLhC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5733/Reviewer_9EiR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5733/Reviewer_9EiR"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces SMPE, a multi-dimensional permutation equivariant framework. The framework combines 1D equivariant functions applied to individual dimensions to preserve higher-dimensional equivariance. By incorporating feature reuse and cross-dimensional global information, SMPE facilitates interactions among objects in all dimensions, enhancing expressivity. Additionally, the framework can be extended to achieve multi-dimensional invariance using pooling operations. Experimental results show that networks constructed using the SMPE framework outperform other approaches and can operate on sets of varying sizes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* It is valuable to preserve higher-dimensional equivariance.\n* The theoretical analyses are provided. \n* The experimental results show the method have significant improvements over the baselines."
                },
                "weaknesses": {
                    "value": "1. One concern is about the evaluation. 1) The adopted tasks in this paper consider the dimensions no more than 3D. This may not reflect the effectiveness of the proposed method, since the paper claims its contribution in **high-dimensional** permutation equivariance. 2) The tasks seem simple that they only consider a linear mapping. It would be better to evaluate the method in real-world datasets, where the mapping from inputs to outputs could be complex. It would be better to show the method can be an effective module in neural networks for complex tasks.\n\n2. Another concern is about the novelty. The idea seems straightforward to combine 1D equivariant functions applied to individual dimensions to preserve higher-dimensional equivariance."
                },
                "questions": {
                    "value": "1. Can this method act a high-dimensional permutation equivariant module in other neural networks for more complex tasks?\n\n2. Most graph neural networks are known as permutation equivariant, and most pooling methods are permutation invariant. What about the comparisons of this method with these works?\n\n3. What is the distance metric for calculating MAE&MSE in the experiments? \n\n4. What about the tasks with more dimensions, e.g., more than 3D?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5733/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5733/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5733/Reviewer_9EiR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815918647,
            "cdate": 1698815918647,
            "tmdate": 1699636600112,
            "mdate": 1699636600112,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UXPxfAwDYG",
                "forum": "t8vJSIsLhC",
                "replyto": "IV5yNuHJaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The effectiveness of the proposed method on tasks with more than 3D"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your careful reviews and constructive comments.\n\nThe proposed framework can be easily extended to higher dimensions, and\nwe illustrate the framework of network construction by using a task of\n4D mapping as an example.\n\nConsider the precoding design for cell-free multi-user multiple-input\nmultiple-output (MU-MIMO) systems There is a CPU controlling $M$\ndistributed APs to support $K$ UEs, where each AP is equipped with\n$N\\_{T}$ antennas and each AP is equipped with $N\\_{R}$ antennas. The\nchannel between the $m$-th AP and the $k$-th UE is denoted as\n${{\\bf H}}\\_{k,m}\\in{\\mathbb{C}}^{N\\_{T}\\times N\\_{R}}$, and the\ncorresponding precoding matrix is\n${\\bf W}\\_{k,m}\\in{\\mathbb{C}}^{N\\_{T}\\times N\\_{R}}$. By stacking the\nchannel and precoding matrices belonging to all APs and UEs, we have\n${{\\bf{\\mathsf{H}}}}\\in{\\mathbb{C}}^{M\\times K\\times N\\_{T}\\times N\\_{R}}$\nand\n${{\\bf{\\mathsf{W}}}}\\in{\\mathbb{C}}^{M\\times K\\times N\\_{T}\\times N\\_{R}}$.\nSimilar to the task in Section 4.3, we consider the optimization problem\nof maximizing the sum rate under transmit power constraints. It can be\nproved that the mapping from ${{\\bf{\\mathsf{H}}}}$ to certain optimal\n${\\bf{\\mathsf{W}}}^{\\star}=f({{\\bf{\\mathsf{H}}}})$ is 4D permutation\nequivariant.\n\nFor this task, we concatenate the real parts and imaginary parts of\n${{\\bf{\\mathsf{H}}}}$, and the shapes of the input and the target output\nare\n${\\widetilde {{\\bf{\\mathsf{H}}}}}\\in{\\mathbb R}^{M\\times K\\times N\\_{T}\\times N\\_{R}\\times 2}$\nand\n${\\widetilde {{\\bf{\\mathsf{W}}}}}\\in{\\mathbb R}^{M\\times K\\times N\\_{T}\\times N\\_{R}\\times 2}$,\nrespectively. For this, the architecture of layer\n${\\rm SMPEL}:{\\mathbb R}^{M\\times K\\times N\\_{T}\\times N\\_{R} \\times D}\\to {\\mathbb R}^{M\\times K\\times N\\_{T}\\times N\\_{R}\\times D}$\ncan be represented as follows: $$\\begin{aligned}\n        {\\rm SMPEL}({\\bf{\\mathsf{X}}}) = {\\rm FC}\\_1(\\sigma({\\rm LN}({\\bf{\\mathsf{M}}}))),\\\\\n        {\\bf{\\mathsf{M}}} = {\\rm FC}\\_2([{\\bf{\\mathsf{O}}}\\_1, {\\bf{\\mathsf{O}}}\\_2, {\\bf{\\mathsf{O}}}\\_3, {\\bf{\\mathsf{O}}}\\_4, {\\bar {\\bf{\\mathsf{X}}}}\\_{{\\mathbb P}\\_1}, ..., {\\bar {\\bf{\\mathsf{X}}}}\\_{{\\mathbb P}\\_{15}}]),\\\\\n        {\\bf{\\mathsf{O}}}\\_0 = {\\bf{\\mathsf{X}}},\\ {\\bf{\\mathsf{O}}}\\_{n} = {\\rm PE1D}\\_{n}({\\bf{\\mathsf{O}}}\\_{n-1}),\\ n\\in\\{1, 2, 3, 4\\},\n        \\end{aligned}$$ where\n${\\rm FC}\\_1:{\\mathbb R}^{D}\\to{\\mathbb R}^{D}$ and\n${\\rm FC}\\_2:{\\mathbb R}^{(4+15)D}\\to{\\mathbb R}^{D}$ represent the\nlinear layer applied at the last dimension.\n${\\rm PE1D}\\_1:{\\mathbb R}^{M\\times D}\\to {\\mathbb R}^{M\\times D}$,\n${\\rm PE1D}\\_2:{\\mathbb R}^{K\\times D}\\to {\\mathbb R}^{K\\times D}$,\n${\\rm PE1D}\\_3:{\\mathbb R}^{N\\_{T}\\times D}\\to {\\mathbb R}^{N\\_{T}\\times D}$,\nand\n${\\rm PE1D}\\_4:{\\mathbb R}^{N\\_{R}\\times D}\\to {\\mathbb R}^{N\\_{R}\\times D}$\ndenotes the 1D equivariant layer operating at the 1,2,3, and 4-th\ndimensions, respectively.\n\nThen, the four-dimensional equivariant neural network used to model $f$\ncan be constructed as follows $$\\begin{aligned}\n        {\\widetilde {{\\bf{\\mathsf{W}}}}} = \\sigma\\_{\\rm out}({\\rm FC}\\_{\\rm out}({\\rm SMPEN}({\\rm FC}\\_{\\rm in}({\\widetilde {{\\bf{\\mathsf{H}}}}})))),\n        \\end{aligned}$$ where\n${\\rm FC}\\_{\\rm in}:{\\mathbb R}^{2}\\to {\\mathbb R}^{D}$ and\n${\\rm FC}\\_{\\rm out}:{\\mathbb R}^{D}\\to {\\mathbb R}^{2}$ are used to is\nused to change the feature dimensions. $\\sigma\\_{\\rm out}(\\cdot)$ is\ndesigned to ensure that ${\\widetilde {{\\bf{\\mathsf{W}}}}}$ satisfies the\npower constraints. At this point, the construction of the\nfour-dimensional permutation-equivariant network from\n${\\widetilde {{\\bf{\\mathsf{H}}}}}$ to ${\\widetilde {{\\bf{\\mathsf{W}}}}}$\nis completed. Following this approach, the framework can be easily\nextended to higher dimensions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490120767,
                "cdate": 1700490120767,
                "tmdate": 1700494024173,
                "mdate": 1700494024173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QxpFGp7qEf",
                "forum": "t8vJSIsLhC",
                "replyto": "IV5yNuHJaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The nonlinearity and complexity of the mappings considered in the tasks"
                    },
                    "comment": {
                        "value": "The involved mappings are relatively complex non-linear mappings. A\nlinear mapping between two vector spaces needs to satisfy additivity,\nwhich means $f(X+Y)=f(X)+f(Y)$. Clearly, the pseudo-inverse and the\noptimal solution for downlink transmission design do not satisfy this\nproperty. Specifically, for the transmission design task in Sections 4.2\nand 4.3, finding the optimal results requires solving Problem (26) and\nProblem (28), respectively. These two problems are typical NP-hard\nproblems and are difficult to solve \\[1\\]. The typical solving\nalgorithms usually involve iterative operations on high-dimensional\nmatrices and require multiple initializations to select the optimal\nresults \\[2\\], which is very time consuming and can not guarantee\noptimality.\n\n\\[1\\] Zhao X, Lu S, Shi Q, et al. Rethinking WMMSE: Can its complexity\nscale linearly with the number of BS antennas?\\[J\\]. IEEE Transactions\non Signal Processing, 2023, 71: 433-446.\n\n\\[2\\] Christensen S S, Agarwal R, De Carvalho E, et al. Weighted\nsum-rate maximization using weighted MMSE for MIMO-BC beamforming\ndesign\\[J\\]. IEEE Transactions on Wireless Communications, 2008, 7(12):\n4792-4799."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490203474,
                "cdate": 1700490203474,
                "tmdate": 1700490203474,
                "mdate": 1700490203474,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FMnJ2zKKVM",
                "forum": "t8vJSIsLhC",
                "replyto": "IV5yNuHJaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Evaluation of the proposed method on real-world datasets"
                    },
                    "comment": {
                        "value": "In order to enhance the practicality and complexity of the task, we\nincorporated real-world datasets and conducted experiments on the\ntransmission design task discussed in Section 4.3. We replaced the\noriginal results in Table 3 with the new results and move the original\nTable 3 to the appendix as supplementary results.\n\nThe new dataset is provided by QuaDRiGa \\[1\\]. QuaDRiGa, short for QUAsi\nDeterministic RadIo channel GenerAtor, is used for generating realistic\nradio channel impulse responses for system-level simulations of mobile\nradio networks. These simulations are used to determine the performance\nof new digital-radio technologies in order to provide an objective\nindicator for the standardization process in bodies like 3rd Generation\nPartnership Project (3GPP). Furthermore, the channel generator is\ncontinuously updated with the evolution of wireless communication\nstandards \\[2\\]. For more details, please refer to the official website\nof QuaDRiGa: https://quadriga-channel-model.de.\n\nThe dataset configuration we used is as follows\n\n|                              |                                            |                    |        |   |\n|:----------------------------:|:------------------------------------------:|:------------------:|:------:|:---:|\n| Scenario                     | 3GPP_38.901_UMa_NLOS                       | Center Frequency   | 3.5GHz |   |\n| Transmit Antenna             | 3gpp-3d                                    | Receive Antenna    | omni   |   |\n| AP Height                    | 25m                                        | User Height        | 1.5m   |   |\n| Transmitter Antenna Downtilt | 10                                         | Subcarrier Spacing | 30kHz  |   |\n| AP/User Distribution         | Uniformly distributed within a 500m radius |                    |        |   |\n|                              |                                            |                    |        |   |\n\n\nwhere the scenario '3GPP\\\\_38.901\\\\_UMa\\\\_NLOS ' refers to a specific\nconfiguration defined by the 3GPP in technical specification 38.901\n\\[3\\]. This scenario is designed to model wireless communication in\nurban macro environments where non-line-of-sight (NLOS) conditions are\nprevalent. In this scenario, QuaDRiGa simulates a multi-path channel\nmodel that includes the effects of buildings, structures, and other\nobstacles typically found in urban macro environments. The model\nconsiders the propagation of radio waves in NLOS conditions, accounting\nfor reflections, diffractions, and scattering from surrounding objects.\n\nThe dataset from the generator closely approximates real-world\nscenarios, ensuring the practical value of experimental results.\n\n\\[1\\] Jaeckel S, Raschkowski L, B\u00f6rner K, et al. QuaDRiGa: A 3-D\nmulti-cell channel model with time evolution for enabling virtual field\ntrials\\[J\\]. IEEE transactions on antennas and propagation, 2014, 62(6):\n3242-3256.\n\n\\[2\\] Wang C X, Bian J, Sun J, et al. A survey of 5G channel\nmeasurements and models\\[J\\]. IEEE Communications Surveys & Tutorials,\n2018, 20(4): 3142-3168.\n\n\\[3\\] 3GPP TR 38.901 v16.1.0, \"Study on channel model for frequencies\nfrom 0.5 to 100 GHz,\\\" Tech. Rep., 2019."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490271934,
                "cdate": 1700490271934,
                "tmdate": 1700490271934,
                "mdate": 1700490271934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0dEauxfxPE",
                "forum": "t8vJSIsLhC",
                "replyto": "IV5yNuHJaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The novelty of the proposed framework that seems straightforward to combine 1D equivariant functions"
                    },
                    "comment": {
                        "value": "There are various ways to apply one-dimensional equivariant functions to\nmultiple dimensions, but not all of them can maintain the equivariant\nproperty in multiple dimensions. For example, one can flatten the\nequivariant dimensions of the multi-dimensional data and apply\none-dimensional equivariant functions. Alternatively, one can fix one\nequivariant dimension and flatten the remaining dimensions along with\nthe feature dimension to create a new feature dimension. However, these\napproaches do not satisfy the multi-dimensional equivariance.\n\nIn our work, **we propose a plug-and-play paradigm for the construction\nof arbitrary high-dimensional equivariant networks with well-designed\none-dimensional equivariant networks**. Unlike the examples mentioned,\nthese proposed combination methods have been mathematically proven to\nsatisfy the equivariant property in multiple dimensions. Moreover, we\nemployed feature reuse and introduced global information at each layer\nto enhance performance while maintaining the multi-dimensional\nequivariance. Additionally, it has been proven that the constructed\nequivariant layers can degenerate into the multi-dimensional equivariant\nlinear layers described in \\[1\\], which is characterized by a complex\nmathematical expression. In summary, while our proposed method has a\nstraightforward expression, its foundation and underlying analysis are\nnot straightforward. The concise expression reduces the cost of further\nexploitation and analysis, reflecting the principle we advocate.\n\n\\[1\\] Hartford J, Graham D, Leyton-Brown K, et al. Deep models of\ninteractions across sets\\[C\\]. International Conference on Machine\nLearning. PMLR, 2018: 1909-1918."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490318542,
                "cdate": 1700490318542,
                "tmdate": 1700490318542,
                "mdate": 1700490318542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qqZH2DyQ3A",
                "forum": "t8vJSIsLhC",
                "replyto": "IV5yNuHJaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comparison of the proposed method with equivariant graph neural networks and invariant pooling methods"
                    },
                    "comment": {
                        "value": "About graph neural networks: Graph neural networks are designed to solve\ngraph-related problems, which often consider modeling of nodes and edges\n\\[1\\]. Due to the unordered nature of nodes, graph neural networks\npossess permutation equivariance. It is worth noting that most graph\nneural networks can be viewed as **one-dimensional** equivariant\nnetworks, as evidenced by references \\[2\\] and \\[3\\]. As our work\nprimarily focuses on the construction of multi-dimensional equivariant\nnetworks, our approach is more general than graph neural networks that\ntarget **one-dimensional** equivariant network design.\n\nAbout pooling methods: The pooling function is utilized to aggregate\nmultiple features, and this characteristic can be employed to construct\npermutation-invariant networks. In Sections 3.2 and 3.3 of the paper,\npooling functions are respectively introduced to acquire global\ninformation and provide invariance. Most pooling functions can be\napplied within the proposed framework, as the multi-dimensional\nequivariance maintained by our framework does not depend on the\nselection of pooling function. Therefore, designing pooling functions is\nbeyond the scope of this paper.\n\n\\[1\\] Wu Z, Pan S, Chen F, et al. A comprehensive survey on graph neural\nnetworks\\[J\\]. IEEE transactions on neural networks and learning\nsystems, 2020, 32(1): 4-24.\n\n\\[2\\] Vignac C, Loukas A, Frossard P. Building powerful and equivariant\ngraph neural networks with structural message-passing\\[J\\]. Advances in\nneural information processing systems, 2020, 33: 14143-14155.\n\n\\[3\\] Velickovic P, Cucurull G, Casanova A, et al. Graph attention\nnetworks\\[J\\]. stat, 2017, 1050(20): 10-48550."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490386879,
                "cdate": 1700490386879,
                "tmdate": 1700490386879,
                "mdate": 1700490386879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5IxBeZpo9Q",
                "forum": "t8vJSIsLhC",
                "replyto": "IV5yNuHJaL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The distance metric for calculating MAE&MSE in the experiments"
                    },
                    "comment": {
                        "value": "The computation methods of these two distance metrics are as follows\n$$\\begin{aligned}\n    {\\rm MAE} = (\\|\\Re({{\\bf X}\\_1})-\\Re({\\bf X}\\_2)\\|\\_1+\\|\\Im({\\bf X}\\_1)-\\Im({\\bf X}\\_2)\\|\\_1)/KN,\\\\\n    {\\rm MSE} = (\\|\\Re({\\bf X}\\_1)-\\Re({\\bf X}\\_2)\\|^2\\_F+\\|\\Im({\\bf X}\\_1)-\\Im({\\bf X}\\_2)\\|^2\\_F)/KN,\n    \\end{aligned}$$ where ${\\bf X}\\_1\\in\\mathbb{C}^{N\\times K}$ and\n${\\bf X}\\_2\\in\\mathbb{C}^{N\\times K}$ are two matrices that need to be\nmeasured for their distance. It is worth noting that the upper and lower\nparts of Table I in the paper correspond to two separate experiments\nconducted for training networks with respect to these two metrics.\n\nThese details are provided in Appendix E.1."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490423124,
                "cdate": 1700490423124,
                "tmdate": 1700490423124,
                "mdate": 1700490423124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0cRyVNby1U",
            "forum": "t8vJSIsLhC",
            "replyto": "t8vJSIsLhC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5733/Reviewer_4oi5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5733/Reviewer_4oi5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel serial multi-dimensional permutation equivariance framework called SMPE by serially composing multiple one-dimensional equivariant layers and incorporating dense connections for feature reuse to enable multi-dimensional interactions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed SMPEL seems to be reasonable.\n\n2. The experimental results demonstrate the effectiveness of the proposed SMPE."
                },
                "weaknesses": {
                    "value": "1. The pooling operation for multi-dimensional permutation invariance is not clear. Which type of pooling do you use in the proposed method? How does the pooling layer help with the multi-dimensional permutation invariance?\n\n2. Some experimental setups are not clear. For instance, in Section 4.2, the authors do not explicitly mention what evaluation metric is used in Table 2. It's unclear which type of pooling is used in the proposed method."
                },
                "questions": {
                    "value": "1. How does $\\bar{X}_\\mathbb{P}$ with the weight $\\mathcal{w}^{GI}_P$ to preserve the global information? How do you learn the weights $\\mathcal{w}^{GI}_P$ and $\\mathcal{w}^{PE}_P$? Can you elaborate on this?\n\n2. The pooling operation for multi-dimensional permutation invariance is not clear. Which type of pooling do you use in the proposed method? How does the pooling layer help with the multi-dimensional permutation invariance?\n\n3. In the experiment, the authors provide various variants of SMPEN. For instance, in Table 3, SMPEN2D-TF sometimes outperforms SMPEN3D-TF, while SMPEN3D-TF achieve the best performance in different training settings. Given a task, how to determine which type of methods should be chosen to achieve the best performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5733/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5733/Reviewer_4oi5",
                        "ICLR.cc/2024/Conference/Submission5733/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5733/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698876476921,
            "cdate": 1698876476921,
            "tmdate": 1700686125131,
            "mdate": 1700686125131,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KMTDnI3cYF",
                "forum": "t8vJSIsLhC",
                "replyto": "0cRyVNby1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The type of pooling function used for invariance"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your thorough reviews and insightful comments.\n\nIn this work, pooling functions are used in two instances: Firstly, in\nSection 3.2, a pooling function is utilized to extract global\ninformation from the input of each layer. Secondly, in Section 3.3,\npooling functions are introduced across multiple dimensions to transform\nmulti-dimensional equivariant functions into multi-dimensional invariant\nfunctions. Without loss of generality, both of the two types are set to\nbe the mean function in the experiments, since the design of the pooling\nfunction is not the focus of this work.\n\nWe have supplemented this information in Section 4 and Appendex E.1."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489236151,
                "cdate": 1700489236151,
                "tmdate": 1700494187935,
                "mdate": 1700494187935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hnamqSNbe2",
                "forum": "t8vJSIsLhC",
                "replyto": "0cRyVNby1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The pooling layer's contribution to multi-dimensional invariance"
                    },
                    "comment": {
                        "value": "Reference \\[2\\] demonstrates that a general invariant function can be\ndecomposed into\n$f({\\bf x})=\\rho\\left({\\rm sum}(\\phi(x\\_1),...,\\phi(x\\_K))\\right)$, where\n$\\phi$ and $\\rho$ are the suitable transformations. More generally,\nreference \\[1\\] reformulates this as\n$f({\\bf x})=\\rho\\left({\\rm pool}(\\phi(x\\_1),...,\\phi(x\\_K))\\right)$.\nFurthermore, it was mentioned in \\[1\\] that if the transformation $\\phi$\nappied at ${\\bf x}$ is a stack of permutation equivariant layer, the\nmodel remains permutation invariant, and it is easy to prove that\n$f({\\bf x})={\\rm pool}({\\rm PEL}({\\bf x}))$ is a permutation invariant\nfunction, where ${\\rm PEL}(\\cdot)$ is the equivariant layer. Based on\nthis, it can be demonstrated that a multi-dimensional equivariant\nfunction can be transformed into a multi-dimensional invariant function\nby pooling the output across multiple dimensions. Thus, we transform\nSMPEN into SMPIN in Section 3.3, expressed as\n${\\rm SMPIN}({\\bf{\\mathsf{X}}}) = {\\rm Pool}\\_{{\\mathbb N}}\\left({\\rm SMPEN}({\\bf{\\mathsf{X}}})\\right)$,\nwhere ${\\rm Pool}\\_{{\\mathbb N}}$ represents the pooling function\noperating on dimensions in ${\\mathbb N}$.\n\n\\[1\\] Lee J, Lee Y, Kim J, et al. Set transformer: A framework for\nattention-based permutation-invariant neural networks\\[C\\].\nInternational conference on machine learning. PMLR, 2019: 3744-3753.\n\n\\[2\\] Zaheer M, Kottur S, Ravanbakhsh S, et al. Deep sets\\[J\\]. Advances\nin neural information processing systems, 2017, 30."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489322970,
                "cdate": 1700489322970,
                "tmdate": 1700489322970,
                "mdate": 1700489322970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "orHP8X0lgQ",
                "forum": "t8vJSIsLhC",
                "replyto": "0cRyVNby1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The experimental setup, including the evaluation criteria in Table 2 and the used pooling layer"
                    },
                    "comment": {
                        "value": "Due to the limitations of the main text, we have added Appendix E titled\n\\\"Experimental Setup\\\" in the revised version, which includes the\nspecific mathematical expressions for the evaluation metric of each\nexperiment and the selection of used pooling operations. For example,\nthe evaluation metric ${\\bar R}\\_{\\rm M}$ in Table 2 is defined as the\naverage of $R\\_{\\rm M}$ across seven SNR values\n$\\{0, 5, 10, 15, 20, 25, 30\\}$, where the expression of $R\\_{\\rm M}$ is\ngiven by $$\\begin{aligned}\n    R\\_{\\rm M}=\\sum\\_{k=1}^{K}\\log\\_2\\left(1+\\frac{|{\\bf h}\\_k^H{{\\bf w}}\\_k|^2}{\\sigma^2+\\sum\\_{i\\neq k}|{\\bf h}^H\\_k{\\bf w}\\_i|}\\right),\n    \\end{aligned}$$ \nwhere the definitions of symbols can be found in\nSection D.2."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489460407,
                "cdate": 1700489460407,
                "tmdate": 1700489460407,
                "mdate": 1700489460407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J0n8pOCQnl",
                "forum": "t8vJSIsLhC",
                "replyto": "0cRyVNby1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The approach for ${\\bar{\\bf{\\mathsf{X}}}}\\_{{\\mathbb P}}$ and weights to preserve global information"
                    },
                    "comment": {
                        "value": "${\\bar{\\bf{\\mathsf{X}}}}\\_{{\\mathbb P}}$ is obtained from pooling\nfunctions, which means it is aggregated from the information of each\nobject. Its invariance to objects implies that it contains the global\ninformation of all object features. In each layer of the network,\n${\\bar{\\bf{\\mathsf{X}}}}\\_{{\\mathbb P}}$ is connected to the output after\nbeing weighted by learnable parameters $w^{\\rm GI}$. This means that\nafter passing through this layer, the global information of the input is\nstill preserved in the output of this layer. Similarly, by stacking\nlayers, the global information of the input is preserved in the network."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489522472,
                "cdate": 1700489522472,
                "tmdate": 1700489522472,
                "mdate": 1700489522472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hc86fowrhW",
                "forum": "t8vJSIsLhC",
                "replyto": "0cRyVNby1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "The learning approach for weights $w^{GI}$ and $w^{PE}$"
                    },
                    "comment": {
                        "value": "The weighted sum of the processed features and the global information is\nperformed through equations (5) in the paper. The processing of the\nlinear layer ${\\rm FC}\\_2$ can be rewritten as $$\\begin{aligned}\n    {\\rm FC}\\_2([{\\bf{\\mathsf{O}}}\\_1, ..., {\\bf{\\mathsf{O}}}\\_N, {\\bar{\\bf{\\mathsf{X}}}}\\_{{\\mathbb P}\\_1}, ..., {\\bar{\\bf{\\mathsf{X}}}}\\_{{\\mathbb P}\\_{2^N-1}}])=\\sum\\_{n=1}^{N}{\\bf{\\mathsf{O}}}\\_n\\times\\_{(N+1)}{\\bf W}^{\\rm PE}\\_n + \\sum\\_{i=1}^{2^N-1}{\\bar{\\bf{\\mathsf{X}}}}\\_{{\\mathbb P}\\_i}\\times\\_{(N+1)}{\\bf W}^{\\rm GI}\\_i +\\_{(N+1)} {\\bf b},\n    \\end{aligned}$$\n\nWhere ${\\bf W}^{\\rm PE}\\_n\\in{\\mathbb R}^{D\\times D}$ and\n${\\bf W}^{\\rm GI}\\_i\\in{\\mathbb R}^{D\\times D}$ are the matrix weights\nrepresenting $w^{\\rm PE}\\_n$ and $w^{\\rm GI}\\_{{\\mathbb P}\\_i}$, and\n${\\bf b}\\in{\\mathbb R}^{D}$ is the bias of this layer. The operation\n'$\\times\\_{(N+1)}$' denotes that the multiplication acts on the\n$(N+1)$-th dimension of the tensor (i.e., the last dimension) \\[1\\].\nSimilarly, the bias is added along the last dimension, which does not\naffect the equivariance. By training this linear layer ${\\rm FC}\\_2$, the\nweights $w^{\\rm PE}\\_n$ and $w^{\\rm GI}\\_{{\\mathbb P}\\_i}$ can be learned.\n\n\\[1\\] Kolda, Tamara G., and Brett W. Bader. Tensor decompositions and\napplications. SIAM review 51(3), 455-500, 2009."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489572024,
                "cdate": 1700489572024,
                "tmdate": 1700489572024,
                "mdate": 1700489572024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CmFc1TmZM7",
                "forum": "t8vJSIsLhC",
                "replyto": "0cRyVNby1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Selection of the network with the best performance for a given task"
                    },
                    "comment": {
                        "value": "This is an interesting topic that is currently being considered.\nAlthough we are unable to provide rigorous mathematical conclusions at\nthis moment, we can analyze the following points from an experimental\nperspective and offer practical recommendations.\n\n1\\. The impact of the number of objects $K\\_1,...,K\\_N$ on performance:\\\nWe consider the performance of a trained network from two perspectives:\nthe testing performance in the training configuration and in other\nconfigurations. During testing in the training configuration, the number\nof of objects has a negligible impact on the network performance.\nHowever, when tested in other configurations, if the number of a certain\ntype of objects in the training configuration is small, the change of\nthis number will have a significant impact on the network performance.\nFor example, in Table 3, the performance of SMPEN3D-TF trained under\nconfiguration ${\\mathcal{B}}$ shows poor performance when tested under\nconfiguration ${\\mathcal{A}}$. Conversely, if the number of such objects\nis large in the training configuration, minor variations in number have\na negligible impact on the performance. For instance, in Table 2,\nSMPEN2D-TF trained under configuration ${\\mathcal{B}}$ exhibits\nexcellent performance when tested under configuration ${\\mathcal{C}}$.\nIntuitively, when the number difference is small, the modeled\nequivariant mappings are similar, while the distinctions in the modeled\nmappings become more significant as the number difference grows large.\n\n2\\. The impact of the relationship between the number of object types $N$\nand the network dimensionality on performance:\\\nWhen the dimension of the equivariant network is equal to the number of\nobject types in the input, the input matches the network. With adequate\ntraining, the network can achieve good performance under the training\nconfiguration. When the dimension of the equivariant network is smaller\nthan the number of object types in the input, multiple types of objects\nare reshaped into a single dimension for input. Due to equivariance, the\nnetwork loses its ability to distinguish the merged types, resulting in\na decrease in performance. For example, in Table 3, SMPEN3D-TF trained\nunder configuration ${\\mathcal{A}}$ performs better than SMPEN2D-TF when\ntested under configuration ${\\mathcal{A}}$. However, merging multiple\ntypes of objects into a single type will increase the number of the new\ntype of objects. According to the analysis of the first point, this\noperation may lead to better performance of the network when tested\nunder other configurations. For instance, in Table 3, SMPEN2D-TF trained\nunder configuration ${\\mathcal{B}}$ performs better than SMPEN3D-TF when\ntested under configuration ${\\mathcal{A}}$.\n\nIn conclusion, we provide the following recommendations for network\nselection.\n\n-   If the number of each type of objectis is relatively large, choose\n    an equivariant network with the same dimension as the number of\n    types. For example, for the task in Table 2, we choose SMPEN2D-TF\n    instead of SetTrans1D.\n\n-   If the number of each type of objectis is relatively small, to\n    achieve better testing performance under the training configuration,\n    it is still advisable to choose an equivariant network with the same\n    dimension as $N$. To achieve better testing performance under other\n    configurations, selecting a network with lower equivariant dimension\n    can be considered. For instance, for the task in Table 3, SMPEN3D-TF\n    performs better under the training configuration, while SMPEN2D-TF\n    performs better under other configurations.\n\nFinally, many thanks for your careful reviews and constructive comments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489680903,
                "cdate": 1700489680903,
                "tmdate": 1700489680903,
                "mdate": 1700489680903,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8RpI3zT0Au",
                "forum": "t8vJSIsLhC",
                "replyto": "CmFc1TmZM7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5733/Reviewer_4oi5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5733/Reviewer_4oi5"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for the detailed clarification and I would like to raise my score to 6."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5733/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686108118,
                "cdate": 1700686108118,
                "tmdate": 1700686108118,
                "mdate": 1700686108118,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]