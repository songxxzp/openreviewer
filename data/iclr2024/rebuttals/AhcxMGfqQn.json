[
    {
        "title": "Collaborative World Models: An Online-Offline Transfer RL Approach"
    },
    {
        "review": {
            "id": "vEyWijNgmV",
            "forum": "AhcxMGfqQn",
            "replyto": "AhcxMGfqQn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission537/Reviewer_77pB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission537/Reviewer_77pB"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied two challenges of training offline reinforcement learning models with visual inputs: 1) overfitting in representation learning and 2) overestimation issue. The basic idea is to harness a readily available RL simulator such that the offline agent can interact with it while the offline RL can be treated as an online-to-offline transfer learning problem. The interaction with the auxiliary RL agent allows the offline agent to assess the target policy by introducing a  regularization term in the training. Meanwhile, the online interaction is also designed to facilitate the learning of more generalized representations from the visual input. Experimental results presented in the paper demonstrate the effectiveness of this approach, comparing with the models such as CQL and offline DV2 in six robot manipulation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well-motivated, and itaims to study two challenges in offline RL.\n\n+ The paper is overall well-written and provides a detailed description of the proposed collaborative world models (CoWorld).\n\n+  Based on the experimental studies presented in the paper, the proposed CoWorld seems to be a promising approach for offline RL by providing better performance compared with other offline RL methods. Especially it shows to outperform offline DV2 by a large margin."
                },
                "weaknesses": {
                    "value": "One main challenge in the proposed approach is how to choose a source RL simulator. Clearly, the key of the online-to-offline transfer lies in the quality of the source agent or the alignment between the source and target tasks. A priori,  how do you set the criteria for choosing the source task and how do you ensure the source task is sufficiently informative for the target tasks? Needless to say, one cannot  choose it arbitrarily. So how do you specify the threshold according to the values in the transfer matrix?  Also, what is the impact of the source task quality on the learning performance.  Many of these details are missing.\n\n Some details of the proposed method are missing or simply punted to another paper, e.g., the objective functions in Eqn. (2), Eqn. (6). In order to make this paper self-contained, it is necessary to include those details. \n \n This work is compared with the fine-tune method, where in offline RL, there are many possible ways to fine-tune the model. It is important to specify the details of these methods, e.g., how many steps for fine-tuning for each task?"
                },
                "questions": {
                    "value": "1. When computing the transfer matrix, how many steps of online fine-tune and online learning are used, respectively? Meanwhile, how many steps of fine-tune in Table 1 and Figure 4?\n\n2. What is the variance in Figure 3(b)?\n\n3. In both Figure 5(a), would you explain why in iteration around 150k, the CoWorld performance is worse or similar compared with CoWorld w/o State A/B/C?\n\n4. What is the computational complexity of the training phase of the auxiliary agent?\n\n5. In Figure 5(b), the CQL (Kumar et al.) method seems to overestimate the value function for all the tasks, why is this the case considering that CQL is a conservative way for offline RL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission537/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698554395553,
            "cdate": 1698554395553,
            "tmdate": 1699635980957,
            "mdate": 1699635980957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jts8JiCljZ",
                "forum": "AhcxMGfqQn",
                "replyto": "vEyWijNgmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 77pB (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments. We hope our responses below can help address your concerns on this paper.\n\n> Q1: How to choose a source RL simulator\uff1f How do you set the criteria for choosing the source task and how do you ensure the source task is sufficiently informative for the target tasks?\n\nWe understand the reviewer's concern regarding the impact of the source task quality on the learning performance. We would like to address this question from two perspectives:\n\n(1) CoWorld's performance with a random source domain:\n \nIn the revised paper, we have updated **Figure 3(a)** to include the Transfer Matrix among the 6 tasks on the Meta-World benchmark. Within all domain transfer cases, there are instances of pairs with less related source and target tasks. Despite these challenging scenarios, our observations indicate that CoWorld outperforms Offline-DV2 in the majority of cases (26 out of 30).\n\n(2) Results with the **REVISED** \"source domain selection\" method:\n\nAdmittedly, our initial proposal of the \"source domain selection\" method is not practical in real-world scenarios, as $R_\\text{online}$ should not be available during training in the offline RL setup. To address this issue, we have thoroughly revised this method and introduced a new adaptive domain selection approach for scenarios with multiple source domains available. \n\nFor detailed technical insights and corresponding results, please refer to our **General Response (Q2)**. In summary, the \"adaptive source domain selection\" method yields results comparable to models trained with manually designated online simulators. This highlights our approach's ability to automatically identify a useful online simulator from a set of both related and less related source domains, thereby expanding its applicability in practical scenarios.\n\nWe have included the aforementioned results in the revised paper. \n    \n> Q2: Some details of the proposed method are missing or simply punted to another paper.\n\nThank you for your suggestion. We have incorporated additional details about the proposed method for world model learning and behavior learning in Appendix A. This includes further explanations regarding the insight behind the objective functions and a more in-depth description of the network architecture.\n\n> Q3: How many steps for fine-tuning for each task? \n\nBelow, we present the finetuning steps of the \"Finetune\" baseline model compared with the training steps of CoWorld in the target dataset. Please refer to Appendix B for more details of the \"Finetune\" method.\n\n| Method   | Meta-World | DMC  | Meta-World $\\rightarrow$ RoboDesk |\n| -------- |:----------:|:----:|:---------------------:|\n| Finetune |    300k    | 600k |    300k    |\n| CoWorld  |    300k    | 600k |   300k   |\n    \n> Q4: What is the variance in Figure 3(b)?\n\nWe trained the compared models with three random seeds and presented the mean results along with standard deviations. The experimental results in Figure 3(b) have been updated in the revised paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission537/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728580932,
                "cdate": 1700728580932,
                "tmdate": 1700734409472,
                "mdate": 1700734409472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jcW14cMyRL",
                "forum": "AhcxMGfqQn",
                "replyto": "vEyWijNgmV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 77pB (Part 2)"
                    },
                    "comment": {
                        "value": "> Q5: In Figure 5(a), why CoWorld performance is worse or similar compared with CoWorld w/o State A/B/C?\n\nAt approximately 150k iterations, the convergence of the world model and policy in CoWorld is not complete, resulting in unstable performance. We observed a significant decrease in reward variance and evaluation returns around this point, suggesting a potential overfitting process within the reward predictor. However, by continuing the training of CoWorld, we successfully mitigated this overfitting issue. Additionally, around 170k iterations in CoWorld without Stage B, a similar decrease was noted, although without a subsequent increase in reward variance. This discrepancy might have contributed to the final result decline in CoWorld without Stage B.\n    \n> Q6: What is the computational complexity of the training phase of the auxiliary agent?\n\nWe have provided the comparisons of the training time and the inference time of the compared models in the above **General Response (Q4)**. Please note that our approach yields comparable inference time to the baseline models such as LOMPO.\n  \n> Q7: In Figure 5(b), the CQL (Kumar et al.) method seems to overestimate the value function for all the tasks, why is this the case considering that CQL is a conservative way for offline RL?\n\nAs there is no official code for CQL with visual inputs, we adopted the DrQ-v2+CQL implementation from the work of V-D4RL (Lu et al., 2023). Below, we compare the value estimated by DrQ-v2+CQL and its base DrQ-v2 model:\n\n| Task | DrQ-v2 Value | DrQ-v2+CQL Value |\n| --- | :---: | :---: |\n| Door Close | 959 | 639 |\n| Button Press | 836 | 766 |\n| Handle Press | 1104 | 921 |\n| Window Close | 903 | 892 |\n| Button Topdown | 1211 | 1076 |\n| Drawer Close | 1001 | 924 |\n| Avg. | 1002 | 870 |\n\nWe have used the above results to update Figure 5(b). In comparison to the naive DrQ-v2, using the CQL regularization term can partly mitigate the issue of overestimated values. However, there are still overestimates due to the complexity of offline visual RL problems. \n\nReference: Lu et al. \"Challenges and opportunities in offline reinforcement learning from visual observations\", TMLR, 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission537/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728642098,
                "cdate": 1700728642098,
                "tmdate": 1700734421691,
                "mdate": 1700734421691,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EU7vY80skq",
            "forum": "AhcxMGfqQn",
            "replyto": "AhcxMGfqQn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission537/Reviewer_wybs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission537/Reviewer_wybs"
            ],
            "content": {
                "summary": {
                    "value": "This paper utilizes an online RL simulator to facilitate offline RL. The main idea is to learn aligned world models for both source and target domains, and then use the source model and critic to regularize the target critic. This method is supposed to address the trade-off between over-estimation of values and over-conservatism in offline RL. Experiments on cross-task and cross-environment show that the algorithm can transfer knowledge from the source task to the target one."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of online-to-offline transfer learning is interesting and sounds novel to me. The exprimental results verify the effectiveness of the proposed method. It is also promising to see the method work for large discrepancy between the source task and the target task."
                },
                "weaknesses": {
                    "value": "- It is not clear how the source domain should be selected in practice. Although the paper provides a selection metric based on the ratio between the finetune model and the online model, it is not practical because we do not have access to the test-time domain (if you have them, why do you do offline learning?) Here the $R_{Online}$ should not be available during training time. It makes the experiment results less convincing, as the source domain selection is actually **leaking information from test to train**.  \n- The learning process involves alternation between online and offline agents until convergence, which can be expensive and unstable in practice. Can you provide some evidence for the time complexity of the method compared to baselines?\n- From the formulation and the algorithm, it is not clear how the source and the target domains are related, which makes it hard to justify the feasibility of the method."
                },
                "questions": {
                    "value": "- For latent state alignment, it is confusing to me why you can feed the same target domain observations into the two world models and close the distance of latent states. Based on the formulation provided, the source domain and the target domain may have different observation spaces $O^{(S)}$ and $O^{(T)}$. Why does it make sense to align $p(s_t^{(S)}|o_t^{(T)})$ and $p(s_t^{(T)}|o_t^{(T)})$? Does it mean you assume the similarity between $s_t^{(S)}$ and $s_t^{(T)}$? The relation between two domains are not clear to me (e.g. how similar are $O^{(S)}$ and $O^{(T)}$). The same issue exists across multiple equations, where the regularization is to align the t-th step of the source and the target, without explaining why they CAN be aligned. \n- Can you evaluate the performance of the algorithm without the designed \"source domain selection\" (for the reason I specified in weakness)? What if the source domain does not match the requirement? Will it hurt the performance by a lot?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission537/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698728078496,
            "cdate": 1698728078496,
            "tmdate": 1699635980889,
            "mdate": 1699635980889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E9HmviMsla",
                "forum": "AhcxMGfqQn",
                "replyto": "EU7vY80skq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer wybs"
                    },
                    "comment": {
                        "value": "We appreciate your great efforts in reviewing our paper and hope that the following responses can address most of your concerns.\n    \n> Q1: It is not clear how the source domain should be selected in practice, as the source domain selection is actually leaking information from test to train.\n\nYes, indeed, our initial proposal of the \"source domain selection\" method is not practical in real-world scenarios. We greatly appreciate that the reviewer pointed it out. To address this problem, we have thoroughly modified the domain selection method, which can extend CoWorld to scenarios with multiple source domains available. This is achieved by measuring the distance of the latent states between the **OFFLINE** target dataset and each source domain provided by different world models. We have included more technical details and corresponding results in **General Response (Q2)**. \n\nIn summary, the \"adaptive source domain selection\" method yields results comparable to models trained with manually designated online simulators. This highlights our approach's ability to automatically identify a useful online simulator from a set of both related and less related source domains, thereby expanding its applicability in practical scenarios.\n\nWe have included the aforementioned results in the revised paper. \n\n> Q2: Can you evaluate the performance of the algorithm without the designed \"source domain selection\"? What if the source domain does not match the requirement? Will it hurt the performance by a lot?\n\n(1) CoWorld's performance with a random source domain:\n\nIn the revised paper, we have updated **Figure 3(a)** to include the Transfer Matrix among the 6 tasks on the Meta-World benchmark. Within all domain transfer cases, there are instances of pairs with less related source and target tasks. Despite these challenging scenarios, our observations indicate that CoWorld outperforms Offline-DV2 in the majority of cases (26 out of 30).\n\n(2) Results with the **REVISED** \"source domain selection\" method:\n\nAs discussed above, we have revised the source domain selection method. The updated method can easily extend CoWorld to scenarios with both related and less related source domains by automatically identifying a useful domain as the auxiliary simulator. We observe that:\n- In **Table 1** in the revised manuscipt, the multi-source CoWorld achieves comparable results to the models trained with manually designated online simulators.\n- In **Figure 3(a)**, the multi-source CoWorld achieves positive improvements over Offline-DV2 in all cases, approaching the best results of models using each individual source task as the auxiliary domain.\n- In **Figure 3(b)**, the multi-source CoWorld consistently outperforms the Finetune baseline, even when the single-source CoWorld faces challenges with specific undesirable source domains.\n\nThese results demonstrate our approach's ability to operate without strict assumptions about domain similarity.\n\n> Q3: Time complexity of the method compared to baselines.\n\nPlease refer to the comparisons of the training/inference time complexity in our **General Response (Q4)**.\n    \n> Q4: From the formulation and the algorithm, it is not clear how the source and the target domains are related.\n\nWe propose the following two techniques to establish a relationship between the source and target MDPs, mitigating cross-domain discrepancies:\n- **Latent Space Alignment:** To mitigate the cross-domain discrepancies in visual observations. This is achieved by closing the distance of latent states produced by the world models with the same visual inputs.\n- **Target-Inclined Source Model Tuning:** To mitigate the cross-domain discrepancies in reward functions. This is achieved by incorporating the target reward information into the learning process of the source reward predictor. As we assume the latent states in different domains have been successfully aligned, the objective in this stage is to enable the source reward predictor to estimate the target returns based on the aligned state inputs.\n    \n> Q5: For latent state alignment, it is confusing to me why you can feed the same target domain observations into the two world models and close the distance of latent states.\n\nIn \"model transfer\" setups, as suggested by Knowledge Flow (Liu et al., 2019) and Transferrable Memory Unit (Yao et al., 2020), it is a common practice to pretrain a teacher model in the source domain, and subsequently transfer the knowledge from the teacher model to the student model in the target domain by providing the teacher model with target domain data. These existing approaches typically assume the presence of distribution shifts between $O^{(S)}$ and $O^{(T)}$.\n  \nReferences: \n- Liu et al. \"Knowledge flow: Improve upon your teachers\", ICLR, 2019. \n- Yao et al. \"Unsupervised transfer learning for spatiotemporal predictive networks\", ICML, 2020."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission537/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734361795,
                "cdate": 1700734361795,
                "tmdate": 1700734808769,
                "mdate": 1700734808769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "omMvFCiE0y",
            "forum": "AhcxMGfqQn",
            "replyto": "AhcxMGfqQn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission537/Reviewer_MD4J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission537/Reviewer_MD4J"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new problem statement in which we have access to an online simulator that does not match the distribution of target offline data. The presented CoWorld approach iterates between online training in a source environment and offline training in the target environment and regularizes target world models with critics learned in the source. The authors evaluate both cross-task and cross-environment transfer settings using Meta-World, DeepMind Control Suite, and RoboDesk. CoWorld outperforms offline-only training with DreamerV2, DrQ+BC, CQL, CURL, and LOMPO."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper includes an interesting discussion on the transfer learning problem within RL and presents a realistic and relevant problem statement that is currently understudied. The CoWorld training framework enables the author to study interesting questions about transfer learning for RL, especially around the difficulty in transferring from one environment or task to another (see Figure 3). The paper shows that for Dreamer-V2 the CoWorld approach improves final performance in the target environment."
                },
                "weaknesses": {
                    "value": "It's not clear is CoWorld is a method or a new problem statement. In my opinion the paper presents two things: \n\n1. A new _problem statement_ where we have a simulator where we can do online RL where our objective is to perform well on an offline dataset from a target environment\n2. A method for doing well in this new problem statement\n\nCurrently in the paper 1 and 2 are both presented as a method. I think it would be more straightforward and fair to separate these contributions. \n\nThis is important because only one baseline is fine-tuned from the source online simulator to the offline data. However, based on prior work on off-line to online fine-tuning, I would expect different RL baselines to have different levels of success when provided access to a online training in a source environment. For example, the IQL [Kostrikov et. al., 2021] paper shows that different offline RL algorithms perform differently when fine-tuned online. I think to have proper baselines, all methods should be able to access the online simulator. It is also important to add baselines that focus specifically on the offline to online problem statement (namely, IQL).\n\n- Kostrikov et. al. Offline Reinforcement Learning with Implicit Q-Learning. 2021."
                },
                "questions": {
                    "value": "To confirm, are the DrQ+BC, CQL, CURL, and LOMPO baselines all trained fully offline without access to the online source environment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission537/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790681226,
            "cdate": 1698790681226,
            "tmdate": 1699635980802,
            "mdate": 1699635980802,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VMdos2KZDN",
                "forum": "AhcxMGfqQn",
                "replyto": "omMvFCiE0y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer MD4J"
                    },
                    "comment": {
                        "value": "Thank you for the insightful comments.\n\n> Q1: It's not clear is CoWorld is a method or a new problem statement. I think it would be more straightforward and fair to separate these contributions.\n\nFollowing the reviewer's suggestion, we have refined our introduction to elaborate on the paper's contributions in two key aspects:\n* First, we propose a novel online-to-offline transfer RL problem, which aims to improve offline visual RL by leveraging an online simulator as the auxiliary domain.\n* Second, we present CoWorld, a new model-based RL approach tailored for the online-to-offline setup. CoWorld effectively transfers domain-sharing knowledge by addressing cross-domain discrepancies. This is achieved through:\n    * Latent Space Alignment: To mitigate the domain shifts in visual observations;\n    * Target-Inclined Source Model Tuning: To mitigate the domain shifts in rewards;\n    * Min-Max Value Regularization: To enable mildly-conservative value estimation.\n\n> Q2: Only one baseline is fine-tuned from the source online simulator to the offline data.\n\nFirst, we have compared CoWorld with more baseline models that are pretained with source domain data and then finetuned in the target dataset. Please refer to our **General Response (Q3)** for the results. We found that all of the baseline models suffer from the so-called \"negative transfer\" effect when incorporating a source pretraining stage. This indicates that a naive transfer learning scheme might degenerate the target performance by introducing unexpected bias. \n\nFurthermore, we implemented another baseline model by combining the Finetuning method with elastic weight consolidation (EWC) (Kirkpatrick et al., 2017). EWC allows the model for preserving source domain knowledge. However, we found that, without additional model designs for target domain adaptation, maintaining knowledge from the source domain could affect the performance in the target domain. Detailed quantitative comparisons across different Meta-World tasks can be found in the **General Response (Q3)** and **Table 1** in the revised paper.\n\nFor the suggested method, IQL, we did not find a well-performed off-the-shelf implementation that allows for visual inputs. We tried the official code for the work from Cho et al. (2022), which includes an implementation of image-based IQL, but found that it did not converge well. Besides, we should note that IQL was originally designed for the offline-to-online problem, which is inherently different from our problem setup.\n\nReferences:\n- Kirkpatrick et al. \"Overcoming catastrophic forgetting in neural networks\", Proceedings of the national academy of sciences, 2017.\n- Cho et al. \"S2P: State-conditioned image synthesis for data augmentation in offline reinforcement learning\", NeurIPS, 2022.\n\n    \n> Q3: Are the DrQ+BC, CQL, CURL, and LOMPO baselines all trained fully offline?\n\nYes, they were. However, as discussed above, we have provided new results by pretraining these models in the source domain. Please refer to the quantitative comparisons in our **General Response (Q3)**."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission537/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728389362,
                "cdate": 1700728389362,
                "tmdate": 1700728389362,
                "mdate": 1700728389362,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hD5UAKYCvT",
            "forum": "AhcxMGfqQn",
            "replyto": "AhcxMGfqQn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission537/Reviewer_mTFC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission537/Reviewer_mTFC"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Collaborative World Models (CoWorld), a novel approach to offline reinforcement learning (RL) with visual inputs. CoWorld uses an existing online RL simulator as a 'test bed' for offline policies, allowing for moderate value estimation without hindering the exploration of potentially beneficial actions. \n\nCoWorld treats offline visual RL as an online-to-offline transfer learning problem and is designed to address the trade-off between overestimation and over-conservatism in value functions. The approach involves three stages: training world models and aligning latent state spaces, incorporating target reward information into the source model and performing model-based behavior learning in the offline domain.\n\nExperiments demonstrate that CoWorld significantly outperforms existing methods in offline visual control tasks across various environments, including DeepMind Control, Meta-World, and RoboDesk."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes an online-to-offline transfer learning approach to tackle offline RL with visual inputs. To my best knowledge, this setting is relatively new and understudied.\n\n2. The methodology proposed seems to adequately addresses the main challenges in offline RL with visual inputs. Specifically, DOMAIN-COLLABORATIVE REPRESENTATION LEARNING (stage A) alleviates the overfitting issue in representation learning, and stage B \\& C attempts to strike a balance between value function overestimation and over-conservatism.\n\n3. The experiments are relatively comprehensive. In particular, the paper demonstrates results on three RL benchmarks: Meta-World, RoboDesk and DeepMind Control Suite. Moreover, both cross-task and cross-environment domain transfer experiments are conducted."
                },
                "weaknesses": {
                    "value": "1. **Unrealistic assumptions**.  The paper tries to tackle offline RL by assuming the existence of an off-the-shelf simulator from a source domain. On one hand, as discussed in part 4.2, the success of this online-to-offline paradigm highly hinges on the similarity between the source and target domains. On the other hand, offline RL research is motivated by the assumption that online data collection can be too costly or dangerous on the target domain. These two conditions seem contradictory since if we cannot query large amount of online data from a target domain due to safety or economic issues, it's unlikely that we can build a high-quality simulator on a very similar source domain.\n\n2. **Marginal or unclear performance improvement of the method**.  The paper compares CoWorld with a number of offline RL methods trained **on the target domain only** with only one exception, which is a simple transfer learning strategy which finetunes the source model on the target domain. I find the performance improvement of CoWorld not convincing enough due to the following considerations:\n\n-  There are many potentially better baseline methods that can leverage data from both domains, which the paper doesn't compare. To name a few: finetuning the model with early stopping/soft update/elastic weight consolidation [1], or finetuning the model and source agent simultaneously, or simply co-training a policy on both domains simultaneously.\n\n- The performance of CoWorld seems to be highly sensitive to hyperparameters such as the Target-inclined reward factor $k$. According to Figure 11, by simply changing $k=0.3$ to $k=0.7$ results in about 600 decline to ~3000 in episode return. In this case, CoWorld is inferior to both Finetune and Offline DV2 on DC $\\rightarrow$ BP task according to Table 1. The improvement in Figure 9 also looks very marginal compared to the naive Finetune baseline.\n \n3. **Computational complexity**: As highlighted in section 6, CoWorld achieves a marginal improvement at the cost of increase computational complexity, which is a notable limitation.\n\nMinor issues:\n\n1. The paper seems rushed. What follows requires further clarification:\n\n- In figure 4, what are the source and target domains for (a) and (b)? Why do you conclude that \"performance experiences a notable decline in scenarios involving a significant data distribution shift between the source and the target domains, such as in the following cross-environment experiments from Meta-World to RoboDesk\"?\n\n[1] Kirkpatrick, James, et al. \"Overcoming catastrophic forgetting in neural networks.\" Proceedings of the national academy of sciences 114.13 (2017): 3521-3526."
                },
                "questions": {
                    "value": "1. Your method relies on the availability of an online RL simulator. Could you elaborate on how dependent the performance of CoWorld is on the quality of this simulator? How would the method perform if the simulator is not a perfect representation of the real-world task?\n\n2. For all experiments, competitive model-free methods such as CQL consistently underperform model-based methods by a large margin. However, we generally expect that model-free methods achieve higher asymptotic performance compared to model-based methods at the cost of sample efficiency. Any explanations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission537/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission537/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission537/Reviewer_mTFC"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission537/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842498554,
            "cdate": 1698842498554,
            "tmdate": 1701011735427,
            "mdate": 1701011735427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GTmSMA3wBF",
                "forum": "AhcxMGfqQn",
                "replyto": "hD5UAKYCvT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer mTFG (Part 1)"
                    },
                    "comment": {
                        "value": "We greatly appreciate your valuable comments. We hope the responses below can help address your concerns.\n\n> Q1: In Figure 4, what are the source and target domains for (a) and (b)?\n    \nFigure 4 provides cross-environment results, where the source domains (a: Press Button, b: Close Window) are from the Meta-World environment, while the target domains (a: Push Button, b: Open Slide) are from another environment named Robodesk. \n\nThe data distribution shifts between the source domain and the target domain reside in visual observation, physical dynamics, reward definition, and even the action space of the robots. Let's take the experimental setup in Figure 4(a) for example:\n\n|              | Source: Meta-World                                   | Target: RoboDesk             | Similarity / Difference                                                                     |\n|:------------ |:---------------------------------------------------- |:---------------------------- |:---------------------------------------------------------------------------------- |\n| Task         | Window Close                                        | Open Slide                   | Related manipulation tasks |\n| Dynamics     | Simulated Sawyer robot arm      | Simulated Franka Emika Panda robot arm      | Different                                                                   |\n| Action space | Box([-1. -1. -1. -1.], [1. 1. 1. 1.], (4,), float64) | Box(-1, 1, (5,), np.float32) | Different                                                                   |\n| Reward scale | [0, 1]                                               | [0, 10]                      | Different                                                                   |\n| Observation  | Right-view images                               | Top-view images         | Different viewpoints, scene object, and backgrounds |\n\n> Q2: Could you elaborate on how dependent the performance of CoWorld is on the quality of this simulator? How would the method perform if the simulator is not a perfect representation of the real-world task?\n\nWe understand the reviewer's concern regarding the dependencies between the source domain and the target domain. We would like to address this question from two perspectives:\n\nFirst, it is important to emphasize that the cross-environment experiments (Meta-World to RoboDesk) we clarified in the above response are **INDEED** the scenarios that \"the simulator is not a perfect representation of the real-world task\". Our experimental results in **Figure 4** reveal that **our approach can effectively mitigate the cross-domain discrepancies** in visual observation, physical dynamics, reward definition, or even the action space of the robots. In methodology, this is achieved by 1) Latent Space Alignment and 2) Target-Inclined Source Model Tuning.\n\nSecond, in **Figure 3(a)** in the revised paper, we have updated the Transfer Matrix among the 6 tasks on the Meta-World benchmark. Among all domain transfer cases, there exist pairs of seemingly unrelated source and target tasks. However, we observe that our approach outperforms Offline-DV2 in the majority of scenarios (26/30 cases).\n> Q3: Unrealistic assumptions --- the success of this online-to-offline paradigm highly hinges on the similarity between the source and target domains... It's unlikely that we can build a high-quality simulator on a very similar source domain.\n\nAs discussed earlier, CoWorld does not rely on strict assumptions about domain similarity. The results presented in Figure 3 and Figure 4 illustrate CoWorld's effectiveness in mitigating cross-domain discrepancies and utilizing a less related online simulator to improve offline RL. \n\nFurthermore, CoWorld can be easily extended to online-to-offline scenarios with multiple source domains. As detailed in our **General Response (Q2)**, CoWorld achieves comparable results to the models trained with manually designated online simulators. This demonstrates our approach's ability to automatically identify a useful online simulator from a set of both related and less related source domains, thereby expanding its applicability in practical scenarios."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission537/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728079405,
                "cdate": 1700728079405,
                "tmdate": 1700728079405,
                "mdate": 1700728079405,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dvPg9IhmE8",
            "forum": "AhcxMGfqQn",
            "replyto": "AhcxMGfqQn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission537/Reviewer_G38A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission537/Reviewer_G38A"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce Collaborative World Models (CoWorld), a method for offline reinforcement learning (RL) that conceptualizes the challenge as an online-to-offline transfer learning problem. CoWorld aims to reduce the representation learning issue of overfitting that emerges from using limited visual data and mitigate the common tendency of value function overestimation in offline RL.\n\nThe method utilizes an auxiliary online simulator and learns separate source and target world models and source and target actor-critic agents, each with its own policy and value function. Regularization of the target agent value function is designed to prevent over-estimation without being overly conservative. They tackle the representation overfitting problem by aligning the target and source model's latent spaces. CoWorld iteratively performs a three-stage learning procedure: aligning the latent spaces between source and target world models, target-inclined source model tuning, and behavior learning with min-max value regularization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- *Strategic Integration of Transfer Learning*: The authors utilize an online simulator to serve as a 'test-bed\" and gather more data from an analogous task. This parallels multi-task and meta-learning approaches that rely on transfer learning across tasks. \n\n- *Empirical Validation*: Experimental results show that the method works well. They also demonstrate the effectiveness of a simple pretraining with fine-tuning baseline, which suggests that transfer between tasks is crucial. \n\n- *Cohesive Methodological Design*: The components of CoWorld contribute synergistically to the overall performance enhancement, as validated by ablation studies."
                },
                "weaknesses": {
                    "value": "- *Dependence on Auxiliary Simulators:* The CoWorld framework's performance is critically dependent on the availability of an online simulator in a domain that is sufficiently similar to the target domain. This reliance may limit the method's applicability in situations where such simulators are unavailable or where the source and target domains do not share sufficient similarity to ensure sufficient knowledge transfer.\n\n- *Decoupling Rationale*: While it is easy to understand how getting more data from an online simulator in the source domain is beneficial, the rationale behind the decision to maintain two separate world models (rather than a single, jointly trained model on both offline and online data) is not sufficiently explained or supported by evidence."
                },
                "questions": {
                    "value": "- Have you considered training on multiple source domains?\n- I would be curious to see experiments that use only one world model.\n- Were all other approaches you compare to (except for the \"Finetune\" baseline) trained only on the offline data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission537/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission537/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission537/Reviewer_G38A"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission537/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699485057708,
            "cdate": 1699485057708,
            "tmdate": 1699635980639,
            "mdate": 1699635980639,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6NiQhT5QDE",
                "forum": "AhcxMGfqQn",
                "replyto": "dvPg9IhmE8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission537/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer G38A (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your effort in reviewing our paper. Below, you'll find our responses addressing your specific concerns.\n\n> Q1: The CoWorld framework's performance is critically dependent on the availability of an online simulator in a domain that is sufficiently similar to the target domain.\n\nIndeed, it is beneficial to leverage an online simulator as the source domain that is very similar to the target domain. However, it is essential to emphasize that (1) our approach does not assume a strict requirement for domain similarity, and (2) it is not that difficult to find a useful auxiliary simulator in practical applications.\n\n(1) Can CoWorld benefit from a less similar source domain?\n\nOur experimental results in **Figure 4** demonstrate CoWorld's ability to mitigate cross-environment discrepancies and benefit from a weakly related source domain. As presented in our **General Response (Q1)**, notable differences between the source (Meta-World) and target environments (RoboDesk) exist in visual observation ($\\mathcal O$), physical dynamics ($\\mathcal T$), reward definition ($\\mathcal R$), and even the action space of robots ($\\mathcal A$). CoWorld outperforms the second-best approaches by approximately 14%-18% in episode returns. In methodology, this is achieved by 1) Latent Space Alignment and 2) Target-Inclined Source Model Tuning.\n\nFurthermore, in **Figure 3(a)** in the revised paper, we have updated the Transfer Matrix among the 6 tasks on the Meta-World benchmark. There exist pairs of seemingly unrelated source and target tasks. Despite the challenging domain transfer setups, we observe that our approach outperforms Offline-DV2 in the majority of scenarios (26/30 cases).\n\n(2) How to choose the auxiliary online simulator given the offline target domain?\n\nOur results demonstrate the ability of CoWorld to mitigate the cross-domain discrepancies and leverage a less related online simulator to improve offline RL. This enhances the convenience of selecting an auxiliary simulator based on the type of robot (major) and task similarity (minor). For example, the Meta-World/RoboDesk/RLBench simulators can all be directly applied to the classic offline RL dataset, D4RL FrankaKitchen. Please refer to our **General Response (Q1)** for further details.\n\n> Q2: The rationale behind the decision to maintain two separate world models is not sufficiently explained or supported by evidence.\n\n(1) Rationale of separate world models:\n\nIn light of non-identical observation spaces, diverse physics, varying reward scales, and even disparate action spaces between source and target MDPs (e.g., Meta-World and RoboDesk environments using different robotic arms with distinct action dimensions), we train separate world models for different domains. Our approach involves aligning the learned world models in latent state space and reward prediction. This strategy captures both domain-specific and shared information in the online-to-offline setup.\n\n(2) Empirical evidence for using separate world models vs. One world model:\n\nTo investigate the effectiveness of employing separate world models, we conduct an experimental comparison between CoWorld and \"Multi-Task DV2\". The latter involves training DreamerV2 on both offline and online data with a joint world model and separate actor-critic models. Here, we present the mean returns and standard deviations for the cross-environment experiments (Meta-World to RoboDesk) that are calculated over 10 episodes with 3 random seeds. CoWorld consistently performs better.\n\n| Meta-World $\\rightarrow$ RoboDesk               | Button Press  $\\rightarrow$ Push Button | Window Close $\\rightarrow$ Open Silde |\n|:------------------- |:-------------------------------------------- |:------------------------------------- |\n| Multi-Task DV2 (One world model) | 342 $\\pm$ 29                                   | 173 $\\pm$   22                        |\n| CoWorld             | 428 $\\pm$ 42                                 | 202 $\\pm$ 19                          |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission537/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727885081,
                "cdate": 1700727885081,
                "tmdate": 1700727885081,
                "mdate": 1700727885081,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]