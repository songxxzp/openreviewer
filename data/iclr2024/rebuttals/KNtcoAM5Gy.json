[
    {
        "title": "BaFTA: Backprop-Free Test-Time Adaptation for Zero-shot Vision Language Models"
    },
    {
        "review": {
            "id": "4ZDhtap0yZ",
            "forum": "KNtcoAM5Gy",
            "replyto": "KNtcoAM5Gy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6294/Reviewer_JA78"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6294/Reviewer_JA78"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new backpropagation-free method for test-time adaptation in vision-language models. Instead of fine-tuning text prompts, this paper employs online clustering within a projected embedding space that aligns text and visual embeddings to estimate class centroids. To reliably evaluate predictions, the paper utilizes Renyi entropy, which ensures accuracy and robustness. Experiments demonstrate that the method improves zero-shot classification accuracy and outperforms SOTA test-time adaptation methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper provides a clear introduction to the challenges of test-time prompt tuning methods and introduces the state-of-the-art method TPT. To address these issues, this paper proposes BaFTA, which is highly motivated.\n\n2. The algorithm proposed in this paper is simple, lightweight, and its description is also very clear."
                },
                "weaknesses": {
                    "value": "1. In the part of Projected Online Clustering, the main distinction between your approach and ReCLIP[1] is that your clustering is conducted online, while ReCLIP's clustering is done offline. Furthermore, due to your Test-Time adaptation setting, your clustering must be conducted online. So I think this may not represent a particularly solid innovation.\n\n2. The experiments are insufficient. In Table 3 and Table 4, I suggest that the authors include additional methods such as CoCoOp[2], TPT+CoCoOp, BaFTA+CoCoOp, and Sus-X [3] and [4].\n\n3. The ablation study on entropy order \u03b1 should be more comprehensive. In Table 7, when entropy order equals 0.25, the result is 70.52, and when it equals 0.5, the result is 70.53. So, what about when it equals 0.4 or 0.6? Therefore, the authors should conduct more fine-grained experiments.\n\n[1] ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation. arXiv 2023.\n\n[2] Conditional Prompt Learning for Vision-Language Models. CVPR 2022.\n\n[3] SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. CVPR 2023.\n\n[4] Prompt-aligned Gradient for Prompt Tuning. ICCV 2023."
                },
                "questions": {
                    "value": "1. The method proposed in this paper has persistence? In other words, can I save the results at a certain moment for later updates?\n\n2. In Table 6, the authors discuss the effectiveness of projection P*. Is there any explanation regarding how projection P* can improve the embedding distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6294/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6294/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6294/Reviewer_JA78"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698468042477,
            "cdate": 1698468042477,
            "tmdate": 1699636690843,
            "mdate": 1699636690843,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eTsDKO0cDE",
                "forum": "KNtcoAM5Gy",
                "replyto": "4ZDhtap0yZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the valuable feedback! Here are our responses to your concerns:\n1. ReCLIP focuses on source-free adaptation, employing a label propagation algorithm for pseudo label generation. However, this method relies on a substantial number of examples to yield satisfactory results, a condition that may not be met in many practical scenarios. In our exploration of alternatives to the label propagation approach, we found that online clustering performs satisfactorily. This approach alleviates the need for an extensive pool of available test data while maintaining performance standards to a reasonable degree.\n2. In response to your suggestion, we have incorporated additional methods (CoCoOp, CoCoOp+TPT, ProGrad, SuS-X) into the supplementary materials. However, we opted not to include CoCoOp+BaFTA for two reasons: 1) The authors of CoOp/CoCoOp have not released the weights for CoCoOp, preventing us from running BaFTA+CoCoOp; 2) CoOp+TPT outperforms CoCoOp+TPT on both RN50 and ViT-B/16, and we believe that comparing with the stronger CoOp+TPT is more informative. For a comprehensive examination of results and analysis, kindly refer to Section D and Table 4. We are planning to reorganize and integrate some of these methods into the main paper in future revisions.\n3. To offer more insights into the sensitivity of the Renyi Entropy parameter, we have performed extensive experiments with detailed results in the supplementary materials. The analysis suggests that BaFTA's performance is not highly sensitive to the choice of $\\alpha$. Please refer to Section B, Figure 1 and Table 2 for a more detailed analysis.\n4. Yes, BaFTA can be easily saved at any moment. The only evolving parameters are the moving averaged centroids and the cluster member counts, making them space-efficient for storage.\n5. The projection can be conceptually described in two steps: 1) SVD extracts the orthonormal basis U of the text embedding subspace. The projection matrix $P=UU^T$ projects everything onto the text embedding subspace, removing dimensions not participating in the classification calculation, since they are orthogonal to the text embeddings. This step removes the category agnostic information from visual embeddings and makes them easier to cluster; 2) $P^*=U\u2019U\u2019^T$ discards the first axis from U where all text embeddings overlap the most. This step removes shared information between text embeddings from different classes, emphasizing their differences and separating clusters from different classes."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265687732,
                "cdate": 1700265687732,
                "tmdate": 1700265687732,
                "mdate": 1700265687732,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UIjZ8KnEdL",
            "forum": "KNtcoAM5Gy",
            "replyto": "KNtcoAM5Gy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6294/Reviewer_sCvw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6294/Reviewer_sCvw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use centroids of visual embedding to improve the test time performance of CLIP. The proposed method is training-free."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- 1. It is interesting to see such a training-free method achieves good performance compared to previous tuning methods like TPT. \n\n- 2. The proposed method is training-free. The authors managed to combine different components to make them work well."
                },
                "weaknesses": {
                    "value": "- 1. A more comprehensive ablation of different components in the system is needed. In algorithm 1, the authors use $Eq.3$, online clustering, weighted aggregation, etc. I am confused about the contributions of different components. For example, in Table 5, the authors can add results of a simple average of BaFTA-OC + CLIP text predictions. This may help us understand the effectiveness of Renyi Aggregation more clearly. Other detailed ablations are also needed.\n\n- 2. The core idea of the proposed method is to use visual centroids to help classifications. The online updating paradigm also assumes the test data come from the same distribution. In practice, this may not be the case. For instance, test samples may come from different distributions. One possible case is samples come from mixed distributions, e.g., mix of ImageNet-A&ImageNet-V2&ImageNet-R. I think the method will not work in such a circumstance. \n    - 2.1 The proposed method will not work in an open scenario, for example, the number of classes is not fixed.\n\n- 3. How about the inference time of the proposed method? The time cost of one step in Algorithm 1 from $for$ to $end for$."
                },
                "questions": {
                    "value": "Please check Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647380517,
            "cdate": 1698647380517,
            "tmdate": 1699636690731,
            "mdate": 1699636690731,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ADGqfBQEkY",
                "forum": "KNtcoAM5Gy",
                "replyto": "UIjZ8KnEdL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We value your constructive feedback and would like to address your comments with the following responses:\n1. In light of the concern regarding ablation experiments on various components of BaFTA, we've introduced additional ablation results in the supplementary materials. Specifically, we've included BaFTA-single, utilizing a single-template CLIP as the base model, and BaFTA-Avg, which employs a simple average function to merge predictions. Notably, the results from BaFTA-Avg exhibit a 1.68% decrease compared to the regular BaFTA, underscoring the effectiveness of Renyi Entropy Aggregation. For a comprehensive exploration of these results, please refer to Section C.1, Section C.2, and Table 3 in the supplementary materials.\n2. The consistent decent zero-shot classification accuracy achieved by CLIP across ImageNet and ImageNet-A/V2/W/R/Sketch using the same text embeddings generated from the CLIP text encoder suggests that the distributions of these datasets are not distinctly different from each other. Notably, ImageNet-R itself comprises a dataset with mixed distributions, including art, sketch, cartoon, plastic toys, etc. BaFTA demonstrates a significant improvement even in this mixed-distribution scenario. This indicates that BaFTA can effectively handle a certain level of distribution mixture, given that CLIP embeddings being sufficiently accurate, thereby ensuring that the distributions are not too dissimilar. It is possible to improve BaFTA with dynamic clustering mechanisms to handle an indefinite number of clusters or mixtures of severely different distributions. While this modification is beyond the scope of the current paper, we appreciate the suggestion and will consider it in future work.\n3. In response to the request for an inference time analysis, we have included a comparison of the inference time for BaFTA and TPT in our supplementary materials. The results indicate BaFTA is roughly 5 times faster than TPT. For detailed results and analysis, please refer to Section A and Table 1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265684713,
                "cdate": 1700265684713,
                "tmdate": 1700265684713,
                "mdate": 1700265684713,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A7kcobXn7m",
                "forum": "KNtcoAM5Gy",
                "replyto": "ADGqfBQEkY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Reviewer_sCvw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Reviewer_sCvw"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the author's response.\n\nIt seems that the proposed method is a mixture of online clustering+visual text alignment+aggregation with renyi entropy. After checking the newly provided Table 3 in supplementary_material. It seems aggregation with renyi entropy makes a significant contribution in the performance. However, I am not sure why the authors use such an aggregation method and how/why it improves the results. \n\nBesides, the mixture of different methods also makes me confused about the main contribution of the method (Reviewer JA78 points out that clustering appeared in previous works). Thus I think a more dedicated declaration about the contributions is needed.\n\nOverall, I maintain my original score, marginally below."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708185103,
                "cdate": 1700708185103,
                "tmdate": 1700708185103,
                "mdate": 1700708185103,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uwu0BHEbOu",
                "forum": "KNtcoAM5Gy",
                "replyto": "rFzEICYAAU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Reviewer_sCvw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Reviewer_sCvw"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the author's review.\n\nWell, it seems the choice of Renyi Entropy is vital for the whole performance. Some questions are raised here:\n- TPT only makes predictions with a single input view, BaFTA uses aggregation of different data views. How about TPT with aggregations of different data views after its TTA process? Or, what is the performance of BaFTA without aggregation (only a single data view)?\n\n- I am still confused about why Renyi Entropy, and many other entropies can be applied. The authors also list other choices in Table 7. Simple entropy as weight is close to Renyi Entropy, by the way, what is the performance of simply using entropy as weights in Table 3? Plus, the authors say they are motivated by [1]. [1] also mention other entropy choices. The key question is, why Renyi Entropy and how it solves the problems.\n\n- **After re-checking the method, I think the ensemble of different predictions of various data views is vital for the whole performance.** However, this may not be fair for other methods like TPT as they only use the prediction of a single view as the output. So TPT+aggregation-of-different-data-views maybe necessary for us to figure out the effectiveness of online clustering and visual-text alignment. This is something I did not note when I first saw the paper.\n\n\n[1] Aleksandr Laptev and Boris Ginsburg. Fast entropy-based methods of word-level confidence estimation for end-to-end automatic speech recognition. In 2022 IEEE Spoken Language Technology\nWorkshop (SLT), pp. 152\u2013159. IEEE, 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732036093,
                "cdate": 1700732036093,
                "tmdate": 1700732036093,
                "mdate": 1700732036093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N8gLixUAcY",
                "forum": "KNtcoAM5Gy",
                "replyto": "UIjZ8KnEdL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Hi! Thanks for the reply. Since it is approaching the end of the rebuttal period, we will focus on sharing our insights in this response. Hopefully these answers will help you better understand BaFTA!\n\n**Augmentation Views**: Although TPT only uses a single view for prediction, it requires the computation of augmented views for its test-time training before inference for every example. \nBaFTA does not introduce extra computation or memory cost on augmentations in comparison with TPT. Both TPT and BaFTA require augmentations as essential components to enhance performance, while TPT utilizes them during test-time training, BaFTA directly utilizes them during inference. \n\n**Effectiveness of Online Clustering**: The effectiveness of online clustering can be evidenced by results from Table 5 from the main paper. BaFTA-RA uses $p_i$ for prediction (Renyi Entropy aggregated CLIP predictions on augmented views), and BaFTA uses $p_i + \\hat{p_i}$ for prediction (Renyi Entropy aggregated CLIP predictions and Online Clustering predictions, on augmented views). BaFTA exhibits a 1.38% improvement over BaFTA-RA, underscoring the effectiveness of online clustering predictions ($\\hat{p_i}$).\n\n**Why Renyi Entropy**: Entropy functions are good estimators of prediction confidence. For example, an uncertain uniform distribution $p_u = [1/k, 1/k,..., 1/k]$ will have Renyi Entropy $Re(p_u)=-1$ and a confident one-host distribution $p_c = [1, 0, 0, 0, 0]$ will have Renyi Entropy $Re(p_c)=0$. As a general form of entropy functions, we can control the sharpness of Renyi Entropy curves for it to mimic the behaviour of other entropy functions by tuning its entropy order $\\alpha$. Small $\\alpha$ corresponds to more sharpened entropy curves where only extremely confident predictions get high outputs. According to our ablation results in Section B of the supplementary materials, we find $\\alpha=0.5$ reaches a good balance for predictions generated by CLIP embeddings, and therefore we use Renyi Entropy with $\\alpha=0.5$ for prediction aggregation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738562042,
                "cdate": 1700738562042,
                "tmdate": 1700738964147,
                "mdate": 1700738964147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XkGuV65LRy",
                "forum": "KNtcoAM5Gy",
                "replyto": "N8gLixUAcY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Reviewer_sCvw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Reviewer_sCvw"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the author's response.\n\nNow, the key question becomes **\"whether the ensemble of different data views plays a vital role (or the most important role)\"**.\n\nFrom Table 5, we can see that **BaFTA-RA**(ensemble of different data views) achieves huge improvement compared to the baseline (**57.87 vs 49.89**) on ImageNet-A. It is close to the final result **58.19**.\n\nI also run the ensemble of different data views for TPT (without CoOp weights) on ImageNet-A. Following the TTA process of TPT, I choose the confident samples and simply average them and get results in the below table.\n| Method | ImageNet-A |\n|---|---|\n| zero-shot CLIP-ViT-B/16 | 47.87 |\n| TPT | 54.77 |\n| BaFTA | 58.19 |\n| TPT + ensemble-of-confident-data-views | **59.76** |\nThe **TPT+Ensemble-of-data-views** will be better if we also use some other entropies as weight scores.\n\nThe above experiments and Table 5 in the paper both demonstrate \"**the ensemble of different data views plays a vital role (or the most important role)\"**.\n\nThis does not flaw the other merits of the paper, for example, training-free. However, the arguments about online clustering/visual-text alignments/renyi entropy aggregation need further justification.\n\nOverall, I think a score of marginal below is suitable for the paper from my perspective."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740359014,
                "cdate": 1700740359014,
                "tmdate": 1700740359014,
                "mdate": 1700740359014,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zvCWATFQ7e",
            "forum": "KNtcoAM5Gy",
            "replyto": "KNtcoAM5Gy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6294/Reviewer_mkqJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6294/Reviewer_mkqJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of enhancing zero-shot classification in vision-language models (VLMs) during inference. It introduces a novel approach called Backpropagation-Free Test-time Adaptation (BaFTA) that significantly improves zero-shot classification capabilities without requiring labeled examples or backpropagation training. BaFTA leverages online clustering algorithms to directly refine class embeddings within the unified visual-text space of VLMs. The authors also propose a method to combine clustering-based predictions with those from augmented views and evaluate their reliability using Renyi entropy. Comprehensive experiments validate BaFTA's effectiveness in enhancing zero-shot classification accuracy during inference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+This paper excels in originality by introducing the Backpropagation-Free Test-time Adaptation (BaFTA) algorithm, a novel approach to enhancing zero-shot classification in vision-language models without labeled data or backpropagation. \n+ The quality is evident through comprehensive experiments and the sound methodology of the online clustering technique. \n+ Clarity is maintained in the well-structured introduction and clear explanations of the methodology. \n+ The significance of the paper lies in its practical relevance, potential wider applicability, and the introduction of a technique for dynamically aggregating predictions using Renyi entropy, enhancing the robustness and accuracy of predictions in machine learning tasks."
                },
                "weaknesses": {
                    "value": "- Efficiency plays a pivotal role in test-time adaptation, as the ability to swiftly adapt to novel environments is of paramount importance. The paper should explicitly report and compare inference time metrics for BaFTA with existing methods like TPT. This can be done by measuring the time required for BaFTA to adapt to new data during inference and comparing it with the time taken by TPT or other relevant methods. Providing quantitative results and possibly a comparison table would be valuable for the readers.\n- The paper assumes that the visual embeddings from CLIP are often discriminative enough for effective classification. A more detailed discussion or evidence regarding this assumption would strengthen the paper's argument.\n- BaFTA's success seems to depend on parameters such as the projection space and the Renyi entropy threshold. The paper could provide more insights on how these parameters are chosen, tuned, and their sensitivity to results. A sensitivity analysis or parameter ablation study could be included."
                },
                "questions": {
                    "value": "Please refer to weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764237667,
            "cdate": 1698764237667,
            "tmdate": 1699636690605,
            "mdate": 1699636690605,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KHQ0stJxXu",
                "forum": "KNtcoAM5Gy",
                "replyto": "zvCWATFQ7e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments! Here are our responses to your questions:\n1. In response to the request for an efficiency analysis, we have included a detailed comparison of the inference time for BaFTA and TPT in our supplementary materials. Notably, BaFTA demonstrates an approximately 5 times speed improvement over TPT. For detailed results and analysis, please refer to Section A and Table 1.\n2. Regarding the assumption about CLIP visual embeddings, results in Table 2 of the main paper indicate that CLIP's linear evaluation results on all datasets are significantly higher than the corresponding zero-shot results. Linear evaluation is a protocol that assesses the quality of representations.  By freezing the CLIP visual embeddings and training a fully-supervised linear layer classifier, linear evaluation obtains the upper bound accuracy achievable by the frozen visual embeddings. Table 2 results suggest that the CLIP performance is likely constrained by the quality of text-generated classification weights. While CLIP visual embeddings are not perfect, we argue that, in the unsupervised test-time adaptation setting where only minor modifications can be made, focusing on text-side updates is more efficient due to the substantial improvement space available on the text side.\n3. To offer more insights into the sensitivity of the Renyi Entropy parameter, we have added updated plots and results in the supplementary materials. The analysis suggests that BaFTA's performance is not highly sensitive to the choice of $\\alpha$. Please refer to Section B, Figure 1, and Table 2 for a more detailed analysis. In terms of the embedding projection, it does not involve additional hyper-parameters except for the number of basis used to construct the projection matrix. For the majority of datasets, we set this number equal to the number of classes, considering it as the rank and the maximum number of basis for the text embedding subspace. However, for datasets with more than 150 categories, such as SUN397 and ImageNet, we observed that utilizing the top 150 basis is sufficient to capture most important dimensions from the embeddings. Therefore, we use the top 150 basis for SUN397 and ImageNet, as mentioned in the implementation detail section."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265680327,
                "cdate": 1700265680327,
                "tmdate": 1700265680327,
                "mdate": 1700265680327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d7G6sLM2Rp",
                "forum": "KNtcoAM5Gy",
                "replyto": "zvCWATFQ7e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Reviewer_mkqJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Reviewer_mkqJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my main concerns. I will vote for weak accept."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489195359,
                "cdate": 1700489195359,
                "tmdate": 1700489212629,
                "mdate": 1700489212629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ki9YPMy9Zt",
            "forum": "KNtcoAM5Gy",
            "replyto": "KNtcoAM5Gy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6294/Reviewer_mwjV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6294/Reviewer_mwjV"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the challenges faced by test-time prompt tuning methods in enhancing the performance of large-scale pretrained vision-language models. The authors a novel backpropagation-free method for test-time adaptation in vision-language models, focusing on estimating class centroids through online clustering in a projected embedding space that aligns text and visual embeddings. The approach dynamically aggregates predictions from both estimated and original class embeddings, as well as distinct augmented views, while assessing prediction reliability using Renyi entropy. The authors claim that the proposed method reaches the state-of-the-art test-time adaptation methods significantly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors use of a backpropagation-free approach instead of employing prompt-tuning is novel than before approach.\n\n- The main contributions are clearly stated."
                },
                "weaknesses": {
                    "value": "- The experimental setting parameters are not the same with TPT and no fair experiment is conducted. The paper of Implementation Details shows: In line with the TPT implementation, we utilize a simple combination of RandomResizedCrop and RandomFlip to prepare 63 augmented views, constituting a mini-batch of 64 images for each test image. Actually, TPT only uses random resized crops to augmented views. Besides, \u201cand adaptation In line with the TPT\u201d seems less a \u201c.\u201d\n\n- Although the authors' approach can use multiple prompts compared to TPT only use single prompt, the paper lacks ablation experiments on prompt\n\n- Authors can present results in more forms of Effectiveness of Projected Embedding Space, such as t-SNE."
                },
                "questions": {
                    "value": "Although the authors use of a backpropagation-free approach instead of employing prompt-tuning is novel, some experiments are still needed to prove the effectiveness of the method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699593085062,
            "cdate": 1699593085062,
            "tmdate": 1699636690459,
            "mdate": 1699636690459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZeWDf9GFbz",
                "forum": "KNtcoAM5Gy",
                "replyto": "ki9YPMy9Zt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments! Here are our responses to your questions:\n1. Regarding the implementation details, we carefully followed the official implementation from the [GitHub Repository](https://github.com/azshue/TPT/tree/main) provided in the TPT paper. In the actual implementation of TPT, both RandomResizedCrop and RandomFlip augmentations were used to produce the reported scores. Therefore, the comparison of BaFTA and TPT is fair.\n2. In response to the concern about ablation experiments on prompts, we evaluated BaFTA with single-template CLIP as the base model. The BaFTA-single configuration resulted in a noteworthy 6.22% improvement in averaged accuracy across 15 datasets. You can find detailed results and analysis in Section C.1 and Table 3 of the supplementary materials.\n3. To provide a more comprehensive understanding of the effectiveness of the projected embedding space, we have included t-SNE plots of CLIP visual embeddings in the supplementary materials. The t-SNE plot visually demonstrates that the projection space can effectively transform the sparse clusters into denser formation. Please refer to Section E and Figure 2 for detailed results and analysis."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700265672500,
                "cdate": 1700265672500,
                "tmdate": 1700265813934,
                "mdate": 1700265813934,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]