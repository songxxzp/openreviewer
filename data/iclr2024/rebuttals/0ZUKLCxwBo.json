[
    {
        "title": "A simple and interpretable model of grokking modular arithmetic tasks"
    },
    {
        "review": {
            "id": "5N1JGNjUk9",
            "forum": "0ZUKLCxwBo",
            "replyto": "0ZUKLCxwBo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_QK7D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_QK7D"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript introduces a simple setup to reproduce the  grokking phenomenon on modular arithmetic problems. Different from existing works, the major contribution is the authors provide an  analytic solutions for  two-layer quadratic networks of solving modular arithmetic problems. Additionally, the authors show that in experiments, typical algorithms like SGD and Adam indeed find solutions that resemble the analytic ones."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The proposed setup  is simple and interpretable.\n- The analytic solutions could be valuable in analyzing the grokking phenomenon for the tasks of modular arithmetic."
                },
                "weaknesses": {
                    "value": "- The constructed analytic solutions for the tasks of modular arithmetic has potential but specifically, this manuscript does not produce too much new insights for understanding grokking. For instance, one can easily construct analytic solutions for learning k-sparse parity with two-layer ReLU networks, where we can reproduce the grokking phenomenon. \n\n- The authors have empirically shows a peak around 0 for $\\phi_k^{(1)} + \\phi_k^{(2)} - \\phi_k^{(3)}$ in the found solution, satisfying equality they propose. However, the presentation falls short of providing adequate evidence that the found weights have the periodic structure of the analytic solution. It is imperative that the authors supplement their work with further empirical evidence or a comprehensive theoretical analysis to elucidate how the weights progressively evolve toward the analytic solution during the training process.\n- Further investigation into the minimal data amount of grokking occurrences is warranted. Does the order of the minimal amount is $O(p^2)$. If not, it necessitates a more suitable definition of the fraction as presented in equation (3).\n- Some mathematical oversight. \n   - The definition in (6) might lead to the misconception that the weights $W_{kn}^{1}$ form an $N \\times p^2 $ matrix  \n   - The  factor $\\frac{1}{N}$ is missing at the beginning of (11)."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698680869422,
            "cdate": 1698680869422,
            "tmdate": 1699637019432,
            "mdate": 1699637019432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Y8ZgCKHEmv",
                "forum": "0ZUKLCxwBo",
                "replyto": "5N1JGNjUk9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the reviewer for positive comments and questions!\n\n**Strengths:**\n_The proposed setup is simple and interpretable.\nThe analytic solutions could be valuable in analyzing the grokking phenomenon for the tasks of modular arithmetic._\n\n**Weaknesses:**\n\n_The constructed analytic solutions for the tasks of modular arithmetic has potential but specifically, this manuscript does not produce too much new insights for understanding grokking. For instance, one can easily construct analytic solutions for learning k-sparse parity with two-layer ReLU networks, where we can reproduce the grokking phenomenon._\n\nI am not familiar with the result. I suspect that either solution is not found by optimization (of which there are examples) or only makes sense for ReLU networks. \n\nTo further elaborate, the solution I presented works for arbitrary number of variables and over $\\mathbb{Z}_p$ with any $p$. $k$-sparse parity is a particular case of this setting with $p=2$ and linear modular function with $k$ non-zero coefficients. I would be very interested to compare the two solutions if the reviewer can provide a reference where such solution for ReLU network is constructed.\n\n_The authors have empirically shows a peak around 0 for \n in the found solution, satisfying equality they propose. However, the presentation falls short of providing adequate evidence that the found weights have the periodic structure of the analytic solution. It is imperative that the authors supplement their work with further empirical evidence or a comprehensive theoretical analysis to elucidate how the weights progressively evolve toward the analytic solution during the training process._\n\n There must have been a poor explanation on my part: The very notion of a phase is not meaningful _unless_ the weights are periodic. Consequently, the very presence of the phases implies that the rows of the weight matrix are periodic. The phases are extracted by fitting the rows of the weight matrices with $\\cos(\\omega + \\phi)$ and then extracting $\\phi$.\n \n I further demonstrate the periodicity in the Supplementary Material, Section C on dynamics (see general comments above and Fig.6 of Supplementary material), where inverse participation ratio (IPR) of the Fourier-transformed weights is defined and measured as a function of time. Growth of IPR indicates that weights become increasingly more periodic (_i.e._ more localized in Fourier space) over time.\n\n_Further investigation into the minimal data amount of grokking occurrences is warranted. Does the order of the minimal amount is $O(p^2)$ \n. If not, it necessitates a more suitable definition of the fraction as presented in equation (3)._\n\nThe equation (3) defines an $\\alpha$ which is simply a \"dimensionless\" (meaning independent of $p$) measure of how much data was used for training. It is a  hyperparameter that is independent of $p$ by definition. The _minimal_ amount of training data $\\alpha_c$ does weakly depend on $p$ and slowly decreases as $p$ gets larger. This effect has been observed in [1] where linear functions of many variables over $Z_p$ are learnt and larger $p$ lead to \"simpler\" learning task.\n\n**References:**\n\n[1] Emily Wenger, Mingjie Chen, Fran\u00e7ois Charton, Kristin Lauter, ``SALSA: Attacking Lattice Cryptography with Transformers''\n\n\n**Some mathematical oversight.**\n\n_The definition in (6) might lead to the misconception that the weights form an matrix $Nxp^2$._\n\nFixed!\n\n_The factor $\\frac{1}{N}$ is missing at the beginning of (11)._\n\nI thank the reviewer for their diligence! This choice was deliberate to lighten the equations. I state: ``(we drop the normalization factors)'' above Eq.(8) and restore the $\\frac{1}{N}$ factor in Eq.(16).\n\nThe $\\frac{1}{N}$ factors are not critical, unless we wish to have a finite large-$N$ limit, which is the normalization I opted for in the end."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525966857,
                "cdate": 1700525966857,
                "tmdate": 1700525966857,
                "mdate": 1700525966857,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JZb9XzG1yc",
            "forum": "0ZUKLCxwBo",
            "replyto": "0ZUKLCxwBo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_5GzG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_5GzG"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the gokking phenomenon by fitting two-layer MLP on modular arithmetic tasks. The paper obtains explicit periodic features in the solutions, and shows that gokking occurs when the correct features are learned."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This paper is a timely and important contribution to the growing literature on gokking. It offers a class of problems with explicit solutions, so that gokking can be studied in great depth."
                },
                "weaknesses": {
                    "value": "While simplicity and explicit solution are a strength, it also limits the scope of the paper in terms of covering the gokking phenomenon in general. Moreover, it is desirable to study the dynamics of the optimizers in reaching the exact solutions, but the paper did not make such an attempt."
                },
                "questions": {
                    "value": "The transition from memorization to generalization appears to be a continuous process of Occam's razor, i.e., gradually reducing the complexity of the model while maintaining the training error. Converging to periodic features is also of this nature. Is this the correct understanding of gokking?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698907085806,
            "cdate": 1698907085806,
            "tmdate": 1699637019322,
            "mdate": 1699637019322,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "etKqKijHy8",
                "forum": "0ZUKLCxwBo",
                "replyto": "JZb9XzG1yc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the reviewer for the positive comments and questions!\n\n**Strengths:**\n\n_This paper is a timely and important contribution to the growing literature on grokking. It offers a class of problems with explicit solutions, so that grokking can be studied in great depth._\n\n**Weaknesses:**\n\n_While simplicity and explicit solution are a strength, it also limits the scope of the paper in terms of covering the grokking phenomenon in general. Moreover, it is desirable to study the dynamics of the optimizers in reaching the exact solutions, but the paper did not make such an attempt._\n\nI have addressed both of these questions in the general comments part of my response.\n\nI added experiments and some discussion with other architectures: Transformers and pure attention layers (please see general comments for detailed description of those).\n\nDiscussion of the dynamics appears in the Supplementary material. In the case of modular addition, the network starts moving towards a periodic solution right away (as I show by measuring inverse participation ratio of Fourier-transformed weights over time) and this motion accelerates after a certain point, at this point generalization picks up. \n\nMore details about the dynamics are the subject of work in progress.\n\n**Questions:**\n\n_The transition from memorization to generalization appears to be a continuous process of Occam's razor, i.e., gradually reducing the complexity of the model while maintaining the training error. Converging to periodic features is also of this nature. Is this the correct understanding of grokking?_\n\nOne important motivation of the work was to flesh out the final solution in as much detail as possible to address this very question. I am not aware of any quantitative (meaning computable) measure of complexity that monotonically decreases over the course of training for all learnable modular functions and all hyperparameter and initialization choices. In particular, sparsity in Fourier space (which is only relevant for addition) or weight norms (which increase according to Fig.1 of my work) are _not_ relevant measures. Consequently, I am skeptical of explanations based on any notion of complexity. \n\nThe best I can presently offer is: for sufficient amount of data ($\\alpha > \\alpha_c$) most low training loss minima are generalizing, while for insufficient amount of data $(\\alpha < \\alpha_c)$ most low training loss minima are memorizing. The critical amount of data $\\alpha_c$ depends strongly on the modular operation we are trying to learn and weakly on the optimizer. _How_ this transition happens is not clear and is an exciting research direction."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525616900,
                "cdate": 1700525616900,
                "tmdate": 1700525616900,
                "mdate": 1700525616900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xLC8VVDV7h",
                "forum": "0ZUKLCxwBo",
                "replyto": "etKqKijHy8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_5GzG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_5GzG"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for answering my question"
                    },
                    "comment": {
                        "value": "I think this paper should be accepted."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652286277,
                "cdate": 1700652286277,
                "tmdate": 1700652286277,
                "mdate": 1700652286277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XMoUzHHAu7",
            "forum": "0ZUKLCxwBo",
            "replyto": "0ZUKLCxwBo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_PaA4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_PaA4"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a two-layer MLP for solving modular arithmetic tasks. The goal is to study a sudden jump in generalization during training, known as grokking. An analytic solution of the model weight is derived, guaranteeing 100% test accuracy. A general result for  arithmetic addition is also given. The experiments show that the proposed representation is also found by training using gradient descent and AdamW."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The analysis of grokking help to understand and dynamics of model training and how to achieve good generalization\n- The theoretical results are applicable for general modular functions. Follow-up work could leverage on these results."
                },
                "weaknesses": {
                    "value": "- Simple architecture and tasks (two layer MLP, modular arithmetic) could limit the applications and extensions of this work\n- The given analytical solution does not help much in understanding how grokking happens as the latter occurs earlier than achieving 100% test accuracy."
                },
                "questions": {
                    "value": "- How does the analytical solution help understanding grokking ? \n- Neural networks are known to converge to local minima. I wonder if there are potentially other analytical solutions and why it seems that model training leads to the same solution."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698910193066,
            "cdate": 1698910193066,
            "tmdate": 1699637019227,
            "mdate": 1699637019227,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iydALSEOFK",
                "forum": "0ZUKLCxwBo",
                "replyto": "XMoUzHHAu7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the reviewer for the positive comments and questions!\n\n**Strengths:**\n_The analysis of grokking help to understand and dynamics of model training and how to achieve good generalization\nThe theoretical results are applicable for general modular functions. Follow-up work could leverage on these results._\n\n\n**Weaknesses:**\n_Simple architecture and tasks (two layer MLP, modular arithmetic) could limit the applications and extensions of this work\nThe given analytical solution does not help much in understanding how grokking happens as the latter occurs earlier than achieving 100$\\%$ test accuracy._\n\nI have addressed some of these concerns in the general comments above. I have added experiments with transformers and with pure attention layers, and described the results above.\n\nPart of this question is addressed in the Supplementary Material on dynamics (also, see general comments above). In the case of modular addition the network starts moving towards a periodic solution right away (as I show by measuring inverse participation ratio of Fourier-transformed weights over time) and this motion accelerates after a certain point, at this point generalization picks up. The progress measure -- inverse participation ratio -- is motivated by the analytic solution: it measures periodicity of the weights.\n\nMore details about the dynamics are the subject of the work in progress.\n\n**Questions:**\n_How does the analytical solution help understanding grokking ?\nNeural networks are known to converge to local minima. I wonder if there are potentially other analytical solutions and why it seems that model training leads to the same solution._\n\nIn the present work I could only answer where the dynamics converges to. The questions why and how are extremely complicated even in the case of the simplest possible model. Detailed understanding of the dynamics is still work in progress and will appear in a separate paper.\n\nThere are memorizing solutions, however I am not aware of any other generalizing solutions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525427988,
                "cdate": 1700525427988,
                "tmdate": 1700525427988,
                "mdate": 1700525427988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "x9Y1gkwLZd",
            "forum": "0ZUKLCxwBo",
            "replyto": "0ZUKLCxwBo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_UC6H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_UC6H"
            ],
            "content": {
                "summary": {
                    "value": "Using a two-layer MLP, this paper analyzes the phenomenon of grokking on a few modular arithmetic problems. Due to the simple DNN architecture, the weights and features are calculated analytically to solve modular addition problems to provide mechanical details about what was learned by the model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Grokking is an interesting and exciting phenomenon that is worth careful study.\n2. The paper is technically sound.\n3. The presentation and organization is clear."
                },
                "weaknesses": {
                    "value": "1. To provide an analytical solution and interpretability of the model, this paper focuses on a very simple model (definitely not used in practice) and arithmetic function to be learned, which limits its impact on practical models currently in use, such as CNN, Transformer.\n\n2. If the model really learns the arithmetic function, it will be interesting to see whether the model generates accurate results for OOD data, e.g., training with the data from [0, 10], testing with the data from [1000, 1100]."
                },
                "questions": {
                    "value": "1. \"Instead, large width leads to redundant representations: Each frequency appears several times with different random phases ultimately leading to a better wave interference\" Since they are identical frequencies, will combining them provide a more concise representation? \n\n2. Besides the amount of data, how is grokking affected by the training data distribution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698957474732,
            "cdate": 1698957474732,
            "tmdate": 1699637019128,
            "mdate": 1699637019128,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b2zetFYmgM",
                "forum": "0ZUKLCxwBo",
                "replyto": "x9Y1gkwLZd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the reviewer for the positive comments and questions!\n\n**Strengths:**\n_Grokking is an interesting and exciting phenomenon that is worth careful study.\nThe paper is technically sound.\nThe presentation and organization is clear._\n\n**Weaknesses:**\n_To provide an analytical solution and interpretability of the model, this paper focuses on a very simple model (definitely not used in practice) and arithmetic function to be learned, which limits its impact on practical models currently in use, such as CNN, Transformer._\n\nSome of the concerns are addressed in the general comments above.\n\nI have addressed this comment by making further experiments with transformers and pure-attention networks. Initially, I chose not to include these architectures because grokking was first empirically observed in two-layer transformers, while the blogpost [https://www.neelnanda.io/blog/interlude-a-mechanistic-interpretability-analysis-of-grokking] empirically considered one-layer transformer. \n\nMy objective was to distill the _simplest possible model_ of the phenomenon rather than focus on models that are more realistic, but cannot be understood in as much detail. Since the identical effects (jump in generalization as a function of time and amount of data) are present in all cases -- simple and complex --, it is reasonable to make as much progress as possible in the simplest case. (Such research philosophy has been very successful in theoretical physics so far). \n\n\n_If the model really learns the arithmetic function, it will be interesting to see whether the model generates accurate results for OOD data, e.g., training with the data from [0, 10], testing with the data from [1000, 1100]._\n\nOne advantage of the analytic solution is that the reviewer does not need to take my word on it! The learnt feature maps are described by equations and are made from trigonometric functions. In the main text I prove _analytically_ that these features perform modular operations including addition and show empirically that similar features are learnt during optimization.\n\nThe model learns _modular_ arithmetic. That is, arithmetic over a finite field $\\mathbb{Z}_p$ rather than over $\\mathbb Z$. The modulus $p$ appears explicitly in the analytic solution and in the dimension of the input and output of the architecture. In the transformer case, $p$ will appear as vocab_size, unless a representation of integers with base $B<p$ is used (in which case ``tokenization scheme'' is determined by the pair $(B,p)$). If numbers larger than $p$ are presented to the model, it will round them down modulo $p$.\n\nMeta learning many different operations in one network is beyond the scope of this paper.\n\n**Questions:**\n_\"Instead, large width leads to redundant representations: Each frequency appears several times with different random phases ultimately leading to a better wave interference\" Since they are identical frequencies, will combining them provide a more concise representation?_\n\nUnfortunately, combining the neurons with same frequencies is not possible because in addition to frequency the trigonometric functions contain a random phase, which is different for different neurons. To be concrete, neurons with $\\cos(\\omega + \\phi_1)$ and $\\cos(\\omega + \\phi_2)$ cannot be easily replaced by a single neuron.\nWhat is possible, however, is to simply keep erasing the ``redundant'' neurons. This will keep performance as measured by accuracy constant (up to some point), but will decrease the train and test loss.\n\n_Besides the amount of data, how is grokking affected by the training data distribution?_\n\nThis questioned was mentioned as open in the paper: ``The value of $\\alpha_c$ further depends on how training set is sampled from the entire dataset; the appropriate choice of the sampling method may thus improve the data efficiency.'' \n\nUnfortunately, I had very little progress in this direction. Empirically it appears that sampling the finite dataset with the uniform probability gives the best performance. We did some experiments along the lines of work [1], but no benefit was seen.\n\n**References:**\n\n[1] Mansheej Paul, Surya Ganguli, Gintare Karolina Dziugaite, ``Deep Learning on a Data Diet: Finding Important Examples Early in Training''"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525208554,
                "cdate": 1700525208554,
                "tmdate": 1700525208554,
                "mdate": 1700525208554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iFnV14UQet",
                "forum": "0ZUKLCxwBo",
                "replyto": "b2zetFYmgM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_UC6H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_UC6H"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the authors' response."
                    },
                    "comment": {
                        "value": "Based on the authors' response, my rating remains the same."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678098516,
                "cdate": 1700678098516,
                "tmdate": 1700678098516,
                "mdate": 1700678098516,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pmtfIhncw8",
            "forum": "0ZUKLCxwBo",
            "replyto": "0ZUKLCxwBo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of learning modular arithmetic with a two-layer network. It proposes a certain Ansatz for the final weights based on Fourier analysis and experimentally shows that the weights match this Ansatz."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper's presentation is clear and to the point, and the construction of the weights is succinctly explained. The experimental evidence is convincing. Mechanistic interpretability is also a highly interesting direction overall."
                },
                "weaknesses": {
                    "value": "1) Literature review is missing some recent work:\n\n* There is a growing body of work on learning single-index and multi-index functions (see e.g., \"Online stochastic gradient descent on non-convex losses from high-dimensional inference\" by Ben Arous et al., and \"SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics\" by Abbe et al.) which shows similar grokking effects. It could be interesting to understand how these relate to the arithmetic grokking effect.\n\n* More crucially, there was a paper called \"Progress measures for grokking via mechanistic interpretability\" which appeared online in Jan., 2023 and was published in ICLR 2023. This paper also seems to derive the Fourier-based solution to the grokking task. This seems unfortunate, because it seems that at the time that this paper was written either the authors are unaware of this other paper, or that this paper was written concurrently and has been to a good extent subsumed by that other paper. Could the authors comment on this? This is the main weakness in my mind.\n\n2) The analysis only gives an Ansatz for the final solution of the weights, but does not explain why more/less data leads to finding it, and why there is a sharp jump in the algorithm's loss from not finding the Ansatz to finding the Ansatz. In other words, the paper only predicts the final weights but does not give an interpretation of what is driving the dynamics of the grokking process."
                },
                "questions": {
                    "value": "1. What is meant by \"Functions of the form f(n, m) + g(n, m) mod p are more difficult to grok: they require more epochs and larger \u03b1\"? Why do you need both f(n,m) and g(n,m) here?\nTypos:\n\"a lightening\" -> \"an enlightening\"\n\"we do not observer\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8211/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8211/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8211/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699485452484,
            "cdate": 1699485452484,
            "tmdate": 1700700166427,
            "mdate": 1700700166427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ww3JQUmCDD",
                "forum": "0ZUKLCxwBo",
                "replyto": "pmtfIhncw8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I thank the reviewer for the positive comments on my work and questions!\n\n**Strengths:**\n_The paper's presentation is clear and to the point, and the construction of the weights is succinctly explained. The experimental evidence is convincing. Mechanistic interpretability is also a highly interesting direction overall._\n\n**Weaknesses:**\n_Literature review is missing some recent work:\nThere is a growing body of work on learning single-index and multi-index functions (see e.g., \"Online stochastic gradient descent on non-convex losses from high-dimensional inference\" by Ben Arous et al., and \"SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics\" by Abbe et al.) which shows similar grokking effects. It could be interesting to understand how these relate to the arithmetic grokking effect._\n\nI was not aware of that line of work. Presently, it is not clear to me what the relation is. The point I make in my paper is that grokking is tightly connected to feature learning. The exact shape of the learning curve is not sufficient. I now cite this work in the new version. \n\n\n_More crucially, there was a paper called \"Progress measures for grokking via mechanistic interpretability\" which appeared online in Jan., 2023 and was published in ICLR 2023. This paper also seems to derive the Fourier-based solution to the grokking task. This seems unfortunate, because it seems that at the time that this paper was written either the authors are unaware of this other paper, or that this paper was written concurrently and has been to a good extent subsumed by that other paper. Could the authors comment on this? This is the main weakness in my mind._\n\nMy work appeared online **before** \"Progress measures for grokking via mechanistic interpretability\" paper and, consequently, I opted not to cite it in the present version. I have now added the citation with a comment on the timeline.\n\nI would like to further emphasize that the (very interesting) paper \"Progress measures for grokking via mechanistic interpretability\" (2301.05217v1), which appeared _after_ mine, **does not contain the analytic solution I have described in my work.** The tables with trigonometric identities are obtained empirically and serve as interpretation of the activations measured in experiment. The authors of 2301.05217 are aware of my work, but choose to not cite it. \n\n_The analysis only gives an Ansatz for the final solution of the weights, but does not explain why more/less data leads to finding it, and why there is a sharp jump in the algorithm's loss from not finding the Ansatz to finding the Ansatz. In other words, the paper only predicts the final weights but does not give an interpretation of what is driving the dynamics of the grokking process._\n\nThe current version of the paper concludes: ``Given the simplicity of our setup, the basic explanation for grokking must be quite banal: At some point in training (and assuming there is enough data), the only way to decrease the training loss is to start learning the right features.''. This is my current opinion on the dynamics: all low training loss minima generalize if there is enough data.\n\nI do, however, completely agree with the reviewer: there is no clear picture for the fraction of the data and for what exactly changes when this fraction crosses its critical value. It is still work in progress.\n\n**Questions:**\n_What is meant by \"Functions of the form f(n, m) + g(n, m) mod p are more difficult to grok: they require more epochs and larger $\\alpha$\"? Why do you need both f(n,m) and g(n,m) here?_\n\nIndeed, more difficult means more epochs and more data. Two examples are shown in Fig. 10 of the Supplementary Material. \nThe form $f(n, m) + g(n, m) \\quad \\textrm{mod} \\quad p$ is a slight abuse of notation meant to indicate ``functions of $n$ and $m$ that cannot be written as $F(f_1(n) + f_2(m))$''. To be concrete, $n^2 + m^2 + 2nm$ is not in this class, while $n^2 + m^2 + nm$ is.\n\n_Typos: \"a lightening\" -> \"an enlightening\"_\n\nThis is indeed a typo, however I meant a more humble \"a lightning\"! Fixed now!\n\n_\"we do not observer\"_\n\nFixed!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524844598,
                "cdate": 1700524844598,
                "tmdate": 1700604619579,
                "mdate": 1700604619579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G7TcRQlWXb",
                "forum": "0ZUKLCxwBo",
                "replyto": "Ww3JQUmCDD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
                ],
                "content": {
                    "comment": {
                        "value": "> I was not aware of that line of work. Presently, it is not clear to me what the relation is.\n\n\nI actually think that the works are quite similar in a qualitative sense! Networks learn in the multi-index function setting with a grokking phenomenon that accompanies feature learning. In that setting, it has been proved that the neural network training plateaus and then suddenly experiences a sharp drop. This sharp drop in the loss occurs after a predictable number of samples, when feature learning occurs. In that case, feature learning is when weights align with the relevant subspace of the input.\n\n\n> My work appeared online before \"Progress measures for grokking via mechanistic interpretability\" paper and, consequently, I opted not to cite it in the present version. I have now added the citation with a comment on the timeline.\n\nSince this is an anonymous review process, I of course cannot confirm that your work appeared earlier than the arXiv version -- perhaps the Area Chair can do this? I'm unsure how this can be verified.\n\n> I would like to further emphasize that the (very interesting) paper \"Progress measures for grokking via mechanistic interpretability\" (2301.05217v1), which appeared after mine, does not contain the analytic solution I have described in my work. \n\nI also apologize for my oversight in my original review:  indeed, the analytical expression derived in this paper does not appear in  \"Progress measures for grokking via mechanistic interpretability\". Accordingly, I will raise my score to 8 (accept), since this paper has a novel contribution on an interesting topic."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680031801,
                "cdate": 1700680031801,
                "tmdate": 1700680031801,
                "mdate": 1700680031801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mgFVOX1cTy",
                "forum": "0ZUKLCxwBo",
                "replyto": "G7TcRQlWXb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8211/Reviewer_9nDi"
                ],
                "content": {
                    "comment": {
                        "value": "Apologies for the back-and-forth, but I felt quite confused again and wanted to make sure that I understand correctly the situation.\n\nI looked at the Nanda-Lieberum blogpost as it appeared in August 2022 via the wayback machine: https://web.archive.org/web/20220817015241/https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking\n\nIn this blogpost the observation that the networks implement the Discrete Fourier transform is stated. The equations for the Discrete Fourier transformer are given. There is a section on Modular Addition where the network is reverse-engineered.\n\nAfter reading this blog post more carefully, it seems that the principal contribution of the present manuscript is in Section 3, which writes an explicit set of the weights under which the network implements the Fourier transform and shows that the quadratic activation works. The main difference with the Nanda-Lieberum blogpost is that the weights are written explicitly and the quadratic activation is shown to work.\n\nAfter this closer analysis, I am moving my score to 6 (between my original 5 and my current 8), since I find the problem studied in this paper valuable, the solution valuable, but think that the contributions beyond the Nanda-Lieberum blogpost are somewhat limited in scope."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8211/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700147521,
                "cdate": 1700700147521,
                "tmdate": 1700700147521,
                "mdate": 1700700147521,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]