[
    {
        "title": "DataFreeShield: Defending Adversarial Attacks without Training Data"
    },
    {
        "review": {
            "id": "21KzZlfvB6",
            "forum": "8giiPtg6rw",
            "replyto": "8giiPtg6rw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3160/Reviewer_nfcV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3160/Reviewer_nfcV"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors focus on improving the robustness of deep models without access to the training data. Specifically, given a trained model, the authors first synthesize images and adopt a soft label loss to finetune the model. With such finetuning, the model's robustness can be enhanced."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n\n2. The problem is very interesting, which enhances the model's robustness without the training data. It might be useful in some cases.\n\n3. The authors have conducted extensive experiments to validate the effectiveness."
                },
                "weaknesses": {
                    "value": "1. It is not clear how you synthesize the samples. Especially, how can you adopt Eq (8) to generate the samples?\n\n2. It is not clear why the authors can adopt synthetic data to improve the model robustness while the images from other domains cannot. In my opinion, the synthetic data is also from different domains.\n\n3. Apart from adversarial training, there are also some defense techniques which does not need training data. For instance, random transforms the input data before feeding them into the model [1]. Also, I am curious if the purifier trained on open datasets, such as ImageNet, can effectively eliminate the adversarial perturbation. There might be other ways that do not need training data to defend against adversarial attacks.\n\n\n[1] Xie et al. Mitigating Adversarial Effects Through Randomization. ICLR 2018."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675203773,
            "cdate": 1698675203773,
            "tmdate": 1699636263435,
            "mdate": 1699636263435,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mRYalVGkHv",
                "forum": "8giiPtg6rw",
                "replyto": "21KzZlfvB6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to questions of Reviewer nfcV"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our work. We sincerely address their comments below.\n\n> **It is not clear how you synthesize the samples. Especially, how can you adopt Eq (8) to generate the samples?**\n\n$\\to$ In essence, random Gaussian noise is fed to a pretrained teacher, and its output/intermediate values are used to compute the loss terms in Eq (8). Then, the computed loss is backpropagated with respect to the input noise, where the pixels are updated using Adam optimizer. Please also see the red arrows depicted in figure2, and the pseudocode in section B of the appendix for more details. \n\n> **It is not clear why the authors can adopt synthetic data to improve the model robustness while the images from other domains cannot. In my opinion, the synthetic data is also from different domains.**\n\n$\\to$ Because synthetic data is always available, while images from other domains are not. To clarify, it is not the matter of whether the data is in-domain or not, but whether the dataset is available or achievable when only a pretrained weight is given. The term \u201cdata-free\u201d used in our problem setting, similar to data-free quantization [1-3] and data-free knowledge distillation problems [4,5], means one is only given a pretrained model with no access to its training data. Thus, under this \u201cdata-free\u201d constraint, synthetic dataset is always accessible (because we generate them) while other domain dataset is not guaranteed. \n\n[1] Nagel, M. et al. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.    \n[2] Cai, Y. et al. ZeroQ: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.    \n[3] Xu, S. et al. Generative low- bitwidth data free quantization. In European Conference on Computer Vision, 2020.     \n[4] Nayak, G.K. et al. \"Zero-Shot Knowledge Distillation in Deep Networks. In International Conference on Machine Learning, 2019.     \n[5] Chen, H. et al. DAFL:Data-Free Learning of Student Networks. In Proceedings of the IEEE International Conference on Computer Vision, 2019.     \n\n> **There might be other ways that do not need training data to defend against adversarial attacks.**\n\n$\\to$ It is important to note that while different adversarial defense techniques have been suggested in the past, only adversarial training(AT) and its variants have survived. Others have been shown to be ineffective or weak, often unable to defend against strong attacks. For example, multiple quantization and randomization techniques have been proposed as viable defense methods, but were quickly deprived of their efficacy and found to be a false sense of defense that can easily be circumvented [1]. For now, AT has become the dominant approach that is shown to be most effective. Thus we adopt the same AT approach in our problem. \nTo support our argument, we provide comparison against test time defense methods, DAD and TTE, in the table below. Notice how they are not competitive against ours in all cases. We will add the following results to Table2 in the revised manuscript.\n\n| Dataset | Method | Clean(%) | PGD(%) | AutoAttack(%) |\n|:-------:|:------:|:--------:|:------:|:-------------:|\n|  Tissue |   DAD  |    53.90 |   3.53 |          3.12 |\n|         |   TTE  |    67.23 |   8.34 |          7.22 |\n|         |  **Ours**  |    32.07 |  31.93 |         31.83 |\n|  Blood  |   DAD  |    81.18 |   6.40 |          6.05 |\n|         |   TTE  |    95.79 |   9.09 |          9.09 |\n|         |  **Ours**  |    49.34 |  19.24 |         18.77 |\n|  Derma  |   DAD  |    67.53 |  11.02 |          6.98 |\n|         |   TTE  |    74.21 |  18.15 |         23.54 |\n|         |  **Ours**  |    66.98 |  66.83 |         66.63 |\n|  OrganC |   DAD  |    72.87 |  38.46 |         34.62 |\n|         |   TTE  |    87.76 |  36.05 |         35.90 |\n|         |  **Ours**  |    76.89 |  46.92 |         45.18 |\n\n[1] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" International conference on machine learning, 2018."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066954103,
                "cdate": 1700066954103,
                "tmdate": 1700067088788,
                "mdate": 1700067088788,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "il4eux4MZG",
            "forum": "8giiPtg6rw",
            "replyto": "8giiPtg6rw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3160/Reviewer_F6PX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3160/Reviewer_F6PX"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a data-free defense against adversarial (evasion) attacks for image classifiers. In cases where availability of the training data of the target model is unavailable, authors propose generating an auxillary dataset composed of synthetic images. These synthetic images are generated using the information captured within the pretrained model, and generated in a way that maximizes diversity. The paper further proposes a training method to make the most out of this synthetic data, which uses the soft predictions of the target model as supervision. This training method enforces the model to learn a flat loss landscape, which promotes adversarial robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Authors do a great job explaining their method, including qualitative results/visualizations wherever possible to convey important points regarding their method. For example, I particularly liked the usage of a toy example to demonstrate how using dynamic loss weights during synthesis process increases diversity of synthetic data. Overall, the writing quality and presentation is great.\n2. In my opinion, the methods proposed by authors are simple, intuitive, and effective. These methods include\n    - the dynamic loss weighting for synthesis loss to increase diversity,\n    - using a regularization term (term 3 in L_DFSshield) to encourage flatness of loss surface, and\n    - using a refinement strategy on top of gradients to further enforce flatness of loss surface."
                },
                "weaknesses": {
                    "value": "1. The explanation of the gradient refinement strategy seems rushed. The authors say that targeting a smoother loss landscape will make it more likely that there is alignment between minima achieved using real and synthetic data. Then, the authors describe the design of their gradient refinement strategy. There is no connection presented between the initial idea (smoother loss landscape) and the proposed method to implement/enforce said idea (gradient refinement). As a result, it is not clear how the gradient refinement strategy will lead to smoother loss landscape.\n2. Furthermore, there is already a term in the training objective (3rd term) that enforces smoother loss landscape. Then, how does the gradient refinement strategy and the third loss term differ in what they are trying to achieve? The distinciton between the two is not clear.\n3. Discussion in the results section is not thorough. Authors provide speculative reasoning behind certain observations, without any support in the form of theoretical/empirical results or references to relevant prior works. In my opinion, such speculative reasoning does not add much value to the paper. For example, in section 5.3. (larger datasets), authors comment that prior works are unable to take advantange of large model capacity due to lack of diversity in synthetic samples generated by them. There are several ways this could have been backed up with results. Off the top of my head, here are few methods: (1) using tSNE plots; (2) performing k-means clustering and measuring sum of squared residuals within clusters; (3) fitting a GMM to the synthetic data generated by different methods and comparing the resulting covariance matrices. I strongly suggest the authors to provide supporting evidence to any speculative claims in the results section.\n4. There are several glaring issues in Table 3, that makes it hard to trust the numbers presented in this table.\n    a. SVHN/DFARD/ResNet-20: A_pgd (weaker attack) is lower than A_aa (stronger attack)\n    b. SVHN/DaST/ResNet-50: A_pgd is lower than A_aa\n    c. Cifar10/DFARD/WRN28-10: A_clean is lower than A_pgd\n5. The authors claim that their work is the \"first data-free adversarial defense\". This is clearly not true as one of the papers that authors cite (Nayak, 2022) proposes a similar method, ie DAD. The authors acknowledge that the only difference between their method and DAD is that DAD assumes availability of a an auxiliary dataset which is from the same domain as the training dataset of the target model. Irrespective, DAD does not require access to the original dataset, making it a data-free defense.\n6. Continuing from the previous point, authors do not compare their method with DAD. I understand the benefit of adapting other data-free methods to the adversarial training regime and comparing against them. But this is not more useful than comparing against a method that already tackles the exact same problem as the one studied in this paper (ie, no adaptation needed). This comparison is crucial for the paper.\n7. Another method that provides a data-free way for improving adversarial defense is the TTE method [b]. The authors neither cite this method nor compare against it. Overall, it is critical to include DAD and TTE as baselines.\n8. Authors do not properly explore adaptive attacks. If a defense flattens/smoothens the loss landscape, it makes gradient based attacks harder to converge (and as result, less effective). AutoAttack circumvents this issue to some extent by including a black-box attack, but this is not enough to establish the true robustness of such defenses in my opinion. Using a latent space attack [a] that doesn't rely on the (flattened) outputs of the final layer will be a more effective attack than something like PGD.\n9. Continuing my point regarding adaptive attacks, there are no results using an attack that targets all the loss terms used during training collectively. Based on my understanding, the authors perform attacks using the cross-entropy loss only. Performing an exploration regarding how the attack effectiveness changes using different combinations of the training loss terms is important in terms of developing an adaptive attack.\n\n**References**\n\n[a] Sabour, S., Cao, Y., Faghri, F., and Fleet, D. J. Adversarial manipulation of deep representations. International Conference on Learning Representations, 2016.\n\n[b] P\u00e9rez, Juan C., et al. \"Enhancing adversarial robustness via test-time transformation ensembling.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."
                },
                "questions": {
                    "value": "1. In case of pre-training with Derma, why is adversarial training with organC better than doing so with Derma itself (figure 1, right)? Authors comment that this occurs in rare cases without any further explanation. However 1 out of 4 is not rare. Where is this conclusion coming from? Are there additional datapoints including dataset combinations other than the ones used in the paper? What can be the reason behind this phenomenon?\n2. In figure 3, for fixed (b) and dynamic (c) coefficient methods, how often do blue points appear in red space and vice versa? If this occurs more often in (c) than (b) due to increased diversity in (c), wouldn't this make it harder for the classifier to learn discriminative features using points generated with (c)? Implying that diversification is counter productive?\n3. In Table 2, for Derma => DFME, why are the A_clean and A_pgd numbers the same?\n4. Can you please explain the issues in Table 3 (listed in Weaknesses section)?\n5. How does the proposed defense compare against DAD and TTE?\n6. How does the proposed defense fare against adaptive attacks (see description in Weaknesses section)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746151723,
            "cdate": 1698746151723,
            "tmdate": 1699636263329,
            "mdate": 1699636263329,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KVQjTXG95D",
                "forum": "8giiPtg6rw",
                "replyto": "il4eux4MZG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to questions of Reviewer F6PX (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for a thorough and detailed review of our work, and the encouraging remarks on the effectiveness of our methods, along with its presentation. We address the concerns and questions provided by the reviewer.\n\n> **DAD does not require access to the original dataset, making it a data-free defense. How does the proposed defense compare against DAD and TTE?** \n\n$\\to$ We respectfully argue that both DAD and TTE are not \u2018data-free\u2019 for the following reasons. DAD heavily relies on the existence of similar dataset (which we think is already not data-free), and it also uses the **original test data** to tune their model, which is a far extreme assumption and could be regarded as data leak. Also, it is difficult to view TTE as \u201cdata-free\u201d because they propose TTE as an enhancement method applied to already robust models trained using the original data. \n\nDespite the unfairness, we compare DAD and TTE with ours. DAD does not perform well and there exists a large performance gap to ours in all cases. Note that DAD is evaluated on the same test set that is used to finetune/adapt the detector module, and yet shows poor results. While TTE is advantageous in preserving clean accuracy, the overall robust accuracy is low and not comparable to ours. Both approaches do not directly train the target model, which makes it vulnerable to gradient-based attacks. Thus, comparison against test time defense methods only enhances our assertion that existing methods are insufficient to guarantee robustness when the train data is unavailable.\n\nWe used the official code provided by the authors. For DAD, we follow their method and retrain a Cifar-10 pretrained detector on MedMNIST-v2 test set. For TTE, where we used +flip+4crops+4 flipped-crops as it is reported as the best setting in the original paper. We will add this comparison to Table 2  in the revision.\n\n| Dataset | Method | Clean(%) | PGD(%) | AutoAttack(%) |\n|:-------:|:------:|:--------:|:------:|:-------------:|\n|  Tissue |   DAD  |    53.90 |   3.53 |          3.12 |\n|         |   TTE  |    67.23 |   8.34 |          7.22 |\n|         |  **Ours**  |    32.07 |  31.93 |         31.83 |\n|  Blood  |   DAD  |    81.18 |   6.40 |          6.05 |\n|         |   TTE  |    95.79 |   9.09 |          9.09 |\n|         |  **Ours**  |    49.34 |  19.24 |         18.77 |\n|  Derma  |   DAD  |    67.53 |  11.02 |          6.98 |\n|         |   TTE  |    74.21 |  18.15 |         23.54 |\n|         |  **Ours**  |    66.98 |  66.83 |         66.63 |\n|  OrganC |   DAD  |    72.87 |  38.46 |         34.62 |\n|         |   TTE  |    87.76 |  36.05 |         35.90 |\n|         |  **Ours**  |    76.89 |  46.92 |         45.18 |\n\n> **In case of pre-training with Derma, why is adversarial training with organC better than doing so with Derma itself (figure 1, right)?**\n\n$\\to$ Because OrganC dataset contains more train samples than Derma. For MedMNIST-v2, each dataset in the collection contains a different number of training samples. Derma provides 7,007 training samples while OrganC provides 13,940, which is nearly double. This is possibly the reason OrganC provides a slightly better performance than using Derma itself, due to having more adversarial samples to learn robustness from. Also, due to the nature of biomedical dataset, the environment in which the dataset was collected could differ between each dataset, leading to difference in quality of the collected samples. The \u201crare cases'' we say in our paper refer to these external causes that we usually do not have control over. \n\n> **In figure 3, for fixed (b) and dynamic (c) coefficient methods, how often do blue points appear in red space and vice versa?**\n\n$\\to$ (b) shows 2.8% and 6.1% misclassified points for each class, while (c) has 5.6% and 9.1%. The table below shows the exact numbers. For each plot we generated 2048 data points, where half is given label 0 (red) and the other half 1 (blue) when generating with cross entropy loss. So ideally, generated data points should have equal distribution in both regions of color. \n\n|          Method         |  Blues in Red  |   Reds in Blue  |\n|:-----------------------:|:--------------:|:---------------:|\n|  fixed coefficient (b)  | 29/1024 (2.8%) | 62/1024  (6.1%) |\n| dynamic coefficient (c) | 58/1024 (5.6%) | 93/1024  (9.1%) |\n\nIn the results, we can see that (c) does show higher error, with 2.8\\% and 3.0\\% increase in each case. However, the difference is not large enough where it can hinder the model\u2019s ability to learn discriminative features. On the other hand, the gained diversity is highly noticeable. The trade-off with discriminative features is not a counter productive factor but something we can leverage, where a small sacrifice in discriminative features leads to huge improvement in diversity."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066609433,
                "cdate": 1700066609433,
                "tmdate": 1700066609433,
                "mdate": 1700066609433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8q6XPSmjer",
                "forum": "8giiPtg6rw",
                "replyto": "il4eux4MZG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to questions of Reviewer F6PX (2/2)"
                    },
                    "comment": {
                        "value": "> **In Table 2, for Derma => DFME, why are the A_clean and A_pgd numbers the same?**\n\n$\\to$ Because most datasets in MedMNIST-v2 have uneven class distributions, and training could easily diverge to one of the classes. In the case of Derma, the class imbalance is severe, where each class contains [66, 103, 220, 23, 223, 1341, 29] samples, 2005 in total. The identical numbers reported in the table is due to the model failing to learn and simply diverging in random guessing one of these classes. For example, if the model diverges towards 5th class, the resulting accuracy is 11.12\\%(=223/2005), which is why ResNet-18 trained using DFME shows this number. And since the model has diverged, all the gradient-based attacks become ineffective and thus the numbers are the same across all settings (clean, PGD, AutoAttack). Thus the results only mean that DFME makes training unstable and difficult to converge when applied to our problem. To make a fair observation, we will revise the table to report F1 scores instead of naive accuracy. We hope this alleviates the reviewer\u2019s concern towards the credibility of the experiment results.\n\n> **Can you please explain the issues in Table 3 (listed in Weaknesses section)?**\n\na. SVHN/DFARD/ResNet-20: A_pgd (weaker attack) is lower than A_aa (stronger attack).       \nb. SVHN/DaST/ResNet-56: A_pgd is lower than A_aa.     \nc. Cifar10/DFARD/WRN28-10: A_clean is lower than A_pgd.    \n \n$\\to$ a & c: are possible outcomes of synthetic data training, and possible signs of gradient obfuscation[1]. The signs of gradient obfuscation include increasing attack strength not necessarily leading to better attack success rate, which is what we can observe from Table 3. We agree the phenomenon is rarely observed in real-data adversarial training, where the train dataset is reliable and does not contain any outliers in terms of distribution or quality. However, all the baseline methods are data-free (See Appendix A.4 for explanation on baseline methods and how we adapt them to our problem), and use synthetic data for training. Learning from these data is tricky and adding adversarial perturbation only makes it harder. Thus, the model often diverges, or simply learns to circumvent gradient-based attacks in an unintended way. \n\n$\\to$ b: We found that for SVHN ResNet56 DaST, there has been an error, where 20.20 was written as 0.20. We apologize for the mistake and causing confusion to the reviewers.\n\n[1] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" International conference on machine learning, 2018.\n\n> **How does the proposed defense fare against adaptive attacks?**\n\n$\\to$  We are to update the results using adaptive attacks. \n\n>  **It is not clear how the gradient refinement strategy will lead to smoother loss landscape. Also, how does the gradient refinement strategy and the third loss term differ in what they are trying to achieve?**\n\n$\\to$  The two methods essentially hold the same objective, but achieves it in a distinct way. Their ultimate goal is to learn adversarial robustness from synthetic samples that can be applied to real attacks. The third term in the training loss penalizes the model when it is highly sensitive to small perturbations. Implicitly, this would make the model favor a set of parameters that have a smoother loss surface with respect to the input. On the other hand, the gradient refinement method explicitly ignores gradients of parameters that are highly fluctuating, pushing the model towards a smoother loss landscape where loss does not fluctuate with small changes in the parameters. The ablation study in Table7 shows that improvement from both the loss term and the gradient refinement method are distinct and best when used together."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066763447,
                "cdate": 1700066763447,
                "tmdate": 1700066763447,
                "mdate": 1700066763447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3qpZm9U4Ln",
            "forum": "8giiPtg6rw",
            "replyto": "8giiPtg6rw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3160/Reviewer_RMmq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3160/Reviewer_RMmq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a data-free adversarial training method, DataFreeShield, which creates a synthetic dataset and performs adversarial training on the synthetic dataset to obtain a robust model. Experiments show that the proposed method outperforms several baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper explores a novel, realistic scenario-based approach to adversarial training.\n2. The motivation of the entire framework is clear.\n3. This paper is richly designed with experiments."
                },
                "weaknesses": {
                    "value": "1. Although this setup is interesting, we have doubts about its actual performance. Compared with the adversarial training model, the adversarial robustness obtained in Table 3 is very low and difficult to use in practice. Especially on CIFAR100, the robust accuracy of ResNet-20 under AA is only 5.97, and the clean accuracy is significantly lower than the normal model (60%+).\n\n2. Please analyze the time complexity of the compared methods, which is important for practical applications.\n\n3. How to extract robust knowledge from clean images is not clearly expressed in this paper. The adversarial robustness of previous work relies on pre-trained robust models, but why can adversarial robustness be obtained using only loss constraints? If this is the case, can it be used in any adversarial training? We believe that this section needs to be described in detail.\n\n4. What is the relationship between adversarial robustness and the amount of generated data?"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3160/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3160/Reviewer_RMmq",
                        "ICLR.cc/2024/Conference/Submission3160/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3160/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751541993,
            "cdate": 1698751541993,
            "tmdate": 1700638402270,
            "mdate": 1700638402270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aR8vHma9St",
                "forum": "8giiPtg6rw",
                "replyto": "3qpZm9U4Ln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to questions of Reviewer RMmq (1/2)"
                    },
                    "comment": {
                        "value": "We thank reviewer RMmq for the encouraging remarks on novelty and the presentation of our work. We address your concerns as follows.\n\n> **Compared with the adversarial training model, the adversarial robustness obtained in Table 3 is very low and difficult to use in practice.**\n\n$\\to$ We respectfully disagree with the reviewer\u2019s comment on the amount of robustness and the practicality of the proposal. The reviewer compares our results against the conventional approach where the original train set is available. We gently remind the reviewer that the purpose of studying this particular setup is not to propose data-free approach\u2019s superiority against data-driven ones, but to provide an alternative solution when none is available. (And currently, there is no known solution to make a neural network robust without using the original dataset.) \nWe provide a few examples from other tasks to show that data-free methods have an inevitable gap with data-driven ones. For knowledge distillation, data-free approach[1] compares up to -10% to the data-driven one by Hinton[6]. This gap is larger for quantization, where data-free approaches show degradation up to -42.67% compared to data-driven quantization work[7] of the same year. \n\n**Table 1 : Data-free vs Data-driven in Knowledge Distillation**\n| | | |  Data-driven | Data-free | |\n|-----------|--------------|-------------|-------------|--------|---|\n| **Dataset** | **Model**| **Teacher Acc.** | **KD (2015)** | **ZSKD (2019)** | **Perf. Gap** |\n| CIFAR-10 | AlexNet | 83.03 | 80.08 | 69.56 | ***-10.52*** |\n\n\n**Table 2 : Data-free vs Data-driven in 4-bit Quantization**\n|             |           |                  | Data-driven         | Data-free      |                  |                 |\n|-------------|-----------|------------------|---------------------|----------------|------------------|-----------------|\n| **Dataset** | **Model** | **Teacher Acc.** | **Adaround (2020)** | **DFQ (2019)** | **ZeroQ (2020)** | **GDFQ (2020)** |\n| ImageNet    | ResNet-18 | 69.68            | 68.71               | 38.98          | 26.04            | 60.60           |\n|             |           |                  | **Perf. Gap**       | ***-29.73***   | ***-42.67***     | ***-8.11***     |\n\n\n\n[1] Nayak, G.K. et al. \"Zero-Shot Knowledge Distillation in Deep Networks. In International Conference on Machine Learning, 2019.     \n[2] Chen, H. et al. DAFL:Data-Free Learning of Student Networks. In Proceedings of the IEEE International Conference on Computer Vision, 2019.     \n[3] Nagel, M. et al. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.      \n[4] Cai, Y. et al. ZeroQ: A novel zero shot quantization framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.      \n[5] Xu, S. et al. Generative low- bitwidth data free quantization. In European Conference on Computer Vision, 2020.     \n[6] Hinton, G. et al. Distilling the Knowledge in a Neural Network. In Advances in Neural Information Processing Systems, Deep Learning Workshop, 2015.     \n[7] Nagel, M. et al. Up or Down? Adaptive Rounding for Post-Training Quantization. In International Conference on Machine Learning, 2020.     \n\n> **Please analyze the time complexity of the compared methods.**\n  \n$\\to$ Time complexity for our method is O(N*(F+B)) for data synthesis and another O(N*(F+B)) for student training. The baseline methods take O(N*(Fg+Bg+F+B)) and O(Fg+F+B) for data synthesis and student training, respectively. We have omitted the number of iterations for each process for clarity.\nLet F,B denote time complexity for each forward and backward computation. In our method, data synthesis requires additional forward and backward computation for optimizing each batch of pixels. Using the same notations, the baseline methods take N*(Fg+Bg+F+B) where Fg and Bg denote forward and backward computation to train the generator. Each generator update requires forward and backward computation of the teacher model, hence (Fg+Bg+F+B). Then, (Fg+F+B) is required to generate the samples, and train the student model. \nWe also report wall clock time in Appendix A.2, where our method takes 0.6 hrs on ResNet-20, 2.6 hrs on ResNet-56, and 3.6 hrs on WRN28-10. The compared methods (DaST, DFME, AIT, DFARD) take 1.5hrs using ResNet-20, 2.75hrs using ResNet-56, and 10hrs using WRN28-10 on RTX3090 under the same environment. The time is measured for generating 10000 synthetic samples on a single GPU."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066327918,
                "cdate": 1700066327918,
                "tmdate": 1700066327918,
                "mdate": 1700066327918,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YZUAuI4UNh",
                "forum": "8giiPtg6rw",
                "replyto": "3qpZm9U4Ln",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3160/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to questions of Reviewer RMmq (2/2)"
                    },
                    "comment": {
                        "value": "> **How to extract robust knowledge from clean images is not clearly expressed in this paper. Why can adversarial robustness be obtained using only loss constraints?**\n\n$\\to$ In our method we do not extract robust knowledge from clean images. Instead, we extract knowledge from a pretrained teacher to generate synthetic samples, which are then used in adversarial training. To be specific, we use a pretrained network as guidance to optimize pixels from random Gaussian noise to image-like distributions. First, a set of random Gaussian noise is forwarded through the teacher network. Then, a set of optimization terms is computed using its output and/or intermediate features and statistics. Lastly, the computed loss is backpropagated all the way up to the input, where the gradient is used to update each pixel using Adam optimizer. Then, the generated samples are used in adversarial training. \n\n> **If this is the case, can it be used in any confrontation training?**\n\n$\\to$ Yes. As shown in Table 6 of the main manuscript, the set of generated images can be used in any adversarial training methods that require a set of input images.  (Is \u2018confrontation\u2019 referring to adversarial? If not, please clarify the meaning of \u201cconfrontation\u201d.) The table shows results for using different variations of adversarial training methods with our generated images.\n\n\n> **What is the relationship between adversarial robustness and the amount of generated data?**\n\n$\\to$ As described in Appendix C, adding more generated training samples increases the robust accuracy, but saturates at some point (5-60,000 samples for CIFAR-10), where further increment in data size does not lead to meaningful performance boost. We have the sample quantity-accuracy plot in Appendix C for the suggested study. Thank you!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700066424721,
                "cdate": 1700066424721,
                "tmdate": 1700066424721,
                "mdate": 1700066424721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rVOzoXdQxi",
                "forum": "8giiPtg6rw",
                "replyto": "YZUAuI4UNh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3160/Reviewer_RMmq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3160/Reviewer_RMmq"
                ],
                "content": {
                    "comment": {
                        "value": "The authors' response resolves most of our concerns, but we still question the practical deployment of limited adversarial robustness. In addition, following Reviewer nfcV, recent DiffPure [1] can defend against adversarial attacks without training data.\n\n[1] Diffusion Models for Adversarial Purification, ICML 2022"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3160/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638384648,
                "cdate": 1700638384648,
                "tmdate": 1700638384648,
                "mdate": 1700638384648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]