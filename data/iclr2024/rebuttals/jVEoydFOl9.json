[
    {
        "title": "Towards Foundation Models for Knowledge Graph Reasoning"
    },
    {
        "review": {
            "id": "oVta8PijA2",
            "forum": "jVEoydFOl9",
            "replyto": "jVEoydFOl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4306/Reviewer_sna7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4306/Reviewer_sna7"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents ULTRA, a single model that can be directly used/finetuned for link prediction over different knowledge graphs. The key is to model the transferrable relationships between different relations across knowledge graphs. Specifically, a NBFNet is used to learn relative relation representations and generate relation embeddings, which is then fed into another NBFNet to perform link predictions. Extensive experiments are performed over many knowledge graph to demonstrate the performance of this model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well written and easy to follow\n- The core method around the relative relationships between relations is clever and interesting.\n- The experiments demonstrate the gains of the method. It is especially impressive to see the competitive zero-shot performance of ULTRA over different knowledge graphs."
                },
                "weaknesses": {
                    "value": "- The proposed method relies entirely on knowledge graph structure and does not consider using node embedding such as textual features of the knowledge graphs. In reality, text embedding of nodes and edges could be a better transferrable embedding. Such transferability has already been demonstrated by PRODIGY (https://arxiv.org/abs/2305.12600) and should be addressed.\n- The model does not scale well as the authors already pointed out.\n- The zero shot and fine-tuning performances are worse or on-par with the per dataset model performance, rendering pretraining not effective performance-wise. \n- Some notations are a bit hard to understand. See questions."
                },
                "questions": {
                    "value": "- What are u and v in h_{u|v} in section 4.2?\n- Why are supervised SOTA baselines only reported for some datasets in Figure 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642346433,
            "cdate": 1698642346433,
            "tmdate": 1699636399067,
            "mdate": 1699636399067,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6PA4rgSm3f",
                "forum": "jVEoydFOl9",
                "replyto": "oVta8PijA2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to sna7 Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for highlighting the proposed method, impressive experimental results, and the quality of writing. Below, we would like to comment on the raised weaknesses and questions. \n\n> **W1. The proposed method relies entirely on knowledge graph structure and does not consider using node embedding such as textual features of the knowledge graphs. In reality, text embedding of nodes and edges could be a better transferrable embedding.**\n\nIt is worth noting that LLM-processed text features are only available for KGs that do have those entity/relation descriptions available, but not all KGs have such text features readily available. In the case of numerical features, LLMs might be even a suboptimal encoder choice, eg, encoding numbers might require a different encoder, and if numerical features adhere to certain symmetries, then geometric models might be a better choice. That\u2019s why in this work we focus on the structural representations and feature-less graphs as this can be applied to any KG with or without features. We briefly cover those reasons as well as some recent text-based approaches in the Related Work -> Text-based methods subsection.\n\nHaving said that, there is some evidence (like in ReFactorGNNs [1]) that concatenating encoded text features (where available) to structural GNN features is likely to further boost the performance in inductive tasks. This is an intriguing direction for the future work and we elaborated on that in the new Appendix E in the updated version.\n\nThank you for mentioning PRODIGY, it is certainly a related work (although focusing on different tasks of node classification and relation prediction) and we added it in the updated version of the manuscript.\n\n> **W2. The model does not scale well as the authors already pointed out.**\n\nWe acknowledge the scaling behavior in the limitations and consider it a very promising avenue for future work. In particular, scaling laws for GNNs and common graph learning tasks (like link prediction) are not derived yet so we can only hypothesize whether there is any connection between GNNs size, dataset size, graph topology, and expected performance. Generally, there is no consensus in the graph learning community on whether deep or wide (non-geometric) GNNs bring immediate benefits - mostly due to the rising issues of oversmoothing and oversquashing (some initial results were recently presented in [2]). In our experiments, we observe that the diversity of graphs in the pre-training mixture plays an important role as well. Therefore, we believe that a brute-force increase of the model size is unlikely to bring benefits unless paired with more diverse training data and more intricate mechanisms for capturing relational interactions.\n\n> **W3. The zero shot and fine-tuning performances are worse or on-par with the per dataset model performance, rendering pretraining not effective performance-wise.**\n\nWe would like to offer a different perspective on this matter (perhaps even the opposite) - if a single pre-trained model can deliver a comparable performance to 57 models trained individually from scratch on each dataset, then it saves a significant amount of compute needed to train those individual models. Zero-shot performance of ULTRA highlights such computational efficiency whereas fine-tuning requires only a fraction of the full training costs, as reported in Table 8 in the Appendix.\nGenerally, we consider the ability of a single model to match the performance of numerous dataset-specific models to be promising. As we are at the beginning of the transfer learning era for KG reasoning, we believe that more fine-grained pre-training and fine-tuning techniques might further improve the compute/performance trade-off and it is an exciting area for the future work.  \n\n> **Q1. What are $u$ and $v$ in $h_{u|v}$ in section 4.2?**\n\nThanks for noticing that. Indeed, in Section 4.2 the notation of the indicator function and message passing can be simplified (we changed that in the updated version of the manuscript). In Section 4.2 which deals with the relations graph, the indicator function says that a node $v$ in the graph of relations will be initialized with some non-zero vector (vector of all ones in our case) if it\u2019s equivalent to the query relation $q$ in the original input query $(h,q,?)$. The notation for a node $u$ is indeed redundant in this case as we can condition both equations on the query relation $q$ directly."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700125507466,
                "cdate": 1700125507466,
                "tmdate": 1700125507466,
                "mdate": 1700125507466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bQmon7YTpX",
                "forum": "jVEoydFOl9",
                "replyto": "eyCwND3oq1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4306/Reviewer_sna7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4306/Reviewer_sna7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. I will maintain the current score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673742344,
                "cdate": 1700673742344,
                "tmdate": 1700673742344,
                "mdate": 1700673742344,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MRrxCuaYHL",
            "forum": "jVEoydFOl9",
            "replyto": "jVEoydFOl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4306/Reviewer_HLbG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4306/Reviewer_HLbG"
            ],
            "content": {
                "summary": {
                    "value": "This work aims to build a foundation model for knowledge graph reasoning tasks, where the authors explore the setting of generalization to any edges and nodes, including unseen, of any multi-relational knowledge graphs without using node and edge features. To this end, the authors first construct a view of a relation-centric graph from an original graph where edges become nodes of this new relation graph, and then, based on this view, the authors represent the relation (node) relative to and conditioned on the query relation. Then, based on this relative relation representation, the authors use existing inductive link prediction methods to perform knowledge graph reasoning. The authors conduct link prediction experiments on various knowledge graphs considering both inductive and transductive settings, and show that the proposed method, namely ULTRA, outperforms other SOTA baselines sometimes without further fine-tuning on target knowledge graphs (i.e., zero-shot)."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* This work studies the very important, challenging, and practical setups of building a foundation model for knowledge graph reasoning, which aims to be generalizable to any other knowledge graphs involving unseen nodes and unseen edges, without leveraging features of nodes and edges. \n* The proposed method works well with different knowledge graphs, on zero-shot transfer learning setups without further fine-tuning on target knowledge graphs, and further shows the boosted performance with task-specific further fine-tuning on them, on most experiment setups.\n* This paper is very well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "* I would like to note that I don't see any major weakness, and below is the minor.\n* In Section 4.2, the explanation about the indicator function with variables $u$ and $v$ is a bit unclear to me. Could you elaborate more on the process and result of the indicator function according to those two variables, perhaps with visuals?\n* Text-based methods (e.g., LM-based methods) can be generalizable to any knowledge graphs including unseen nodes and unseen edges, as long as their nodes and edges are represented with texts. In this vein, I think one potential direction for building a foundation model for knowledge graph-related tasks might be to use the LMs, and the authors may highlight this point more and potentially make comparisons between the proposed approach and text-based methods. I don't think this should be the critical weakness of this paper since text-based methods are limited to knowledge graphs with textual features; meanwhile, given the framing of this work (\"Towards Foundation Models for Knowledge Graph Reasoning\"), this point should be carefully explained."
                },
                "questions": {
                    "value": "* I would like to suggest emphasizing the performance differences between inductive and transductive setups when explaining Table 1. The proposed method w/ 0-shot settings are strong on inductive graphs; meanwhile, previous methods are superior to it on transductive graphs, which are worthwhile to discuss.\n* It may be beneficial to show the results of the ULTRA fine-tuned on the knowledge graphs used for pre-training the ULTRA. I am wondering if there are further performance improvements when further fine-tuning the model on the data used for pre-training."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672313275,
            "cdate": 1698672313275,
            "tmdate": 1699636398956,
            "mdate": 1699636398956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wxqnlo7pjc",
                "forum": "jVEoydFOl9",
                "replyto": "MRrxCuaYHL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to HLbG Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for highlighting the merits of our work as to the importance of the problem, the ability to perform zero-shot inference and fine-tuning, as well as the quality of writing.\n\nPlease find our comments on the weaknesses and questions below.\n\n> **W1. Could you elaborate more on the process and result of the indicator function according to those two variables, perhaps with visuals?**\n\nIndeed, in Section 4.2 the notation of the indicator function can be simplified (we changed that in the updated version of the manuscript). In Section 4.2 which deals with the graph of relations, the indicator function says that a node $v$ in the graph of relations will be initialized with some non-zero vector (vector of all ones in our case) if it\u2019s equivalent to the query relation $q$ in the original input query $(h,q,?)$. The notation for a node $u$ is indeed redundant in this case as we can condition both equations on the query relation $q$ directly.\n\n> **W2. I think one potential direction \u2026 might be to use the LMs, and the authors may highlight this point more and potentially make comparisons between the proposed approach and text-based methods. I don't think this should be the critical weakness of this paper since text-based methods are limited to knowledge graphs with textual features;**\n\nYou correctly pointed out that LLM-processed features are only available for KGs that do have those entity/relation features available, we briefly cover some recent approaches in the Related Work -> Text-based methods subsection. Not all KGs have such text features readily available and in the case of numerical features LLMs might be even a suboptimal encoder choice. That\u2019s why in this work we focus on the structural representations and feature-less graphs as this can be applied to any KG with or without features. \n\nHaving said that, there is some evidence (like in ReFactorGNNs [1]) that concatenating encoded text features (where available) to structural GNN features is likely to further boost the performance in inductive tasks. We consider dataset-specific features complementary to ULTRA representations and hypothesize that such additional features might be particularly useful at the fine-tuning stages. This is an intriguing direction for the future work and we added this discussion in the new Appendix E in the updated version.\n\n> **Q1. Previous methods are superior to ULTRA on transductive graphs, which are worthwhile to discuss.**\n\nThank you for the suggestion, we added more discussion on the performance on larger transductive graphs in Appendix D. Generally, we attribute the performance difference to the following factors:\n* Training data mixture and OOD generalization: the model reported in Table 1 was trained on 3 medium-sized KGs (15k - 40k nodes, 80k - 270k edges) while the biggest gaps are on larger graphs with many more nodes and edges (up to 120k / 1M for YAGO 310), or many more relation types (1600+ in AristoV4), or very sparse (as in ConceptNet100k with 100k edges over 78k nodes). Size generalization issues are common for GNNs as found in [2] and [3]. However, if we take the ULTRA checkpoint pre-trained on 8 graphs (Table 9 in Appendix D) and run evaluation on all 16 transductive graphs, then the average performance is better than supervised SOTA models.\n\n|  | MRR | Hits@10 |\n| --- | --- | --- |\n| ULTRA (8 graphs) | 0.377 | 0.537 |\n| Supervised SOTA | 0.371 | 0.511 |\n\t\nOf course, for those datasets in the pretraining mix, we cannot call this regime a zero-shot inference anymore, but the results hint upon the hypothesis that more diverse graphs in the training mixture are beneficial for alleviating OOD issues.\n* Transductive models have the privilege of memorizing target data distributions (even on large graphs) into entity/relation-specific vectors with overall many millions of parameters, eg, 80M parameters for a supervised SOTA BiQUE on ConceptNet100k. This performance, however, comes with the absence of transferability across KGs. In contrast, all pre-trained ULTRA checkpoints are rather small (~170k parameters) but generalize to any KG. When it comes to scaling, scaling laws for GNNs and common graph learning tasks (like link prediction) are not derived yet so we can only hypothesize whether there is any connection between GNNs size, dataset size, graph topology, and expected performance. Generally, there is no consensus in the graph learning community on whether deep or wide (non-geometric) GNNs bring immediate benefits - mostly due to the rising issues of oversmoothing and oversquashing (some initial results were recently presented in [4]). In our experiments, we observe that the diversity of graphs in the pre-training mixture plays an important role as well. Therefore, we believe that a brute-force increase of the model size is unlikely to bring benefits unless paired with more diverse training data and more intricate mechanisms for capturing relational interactions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700125301610,
                "cdate": 1700125301610,
                "tmdate": 1700125301610,
                "mdate": 1700125301610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gkwQZuBpIc",
                "forum": "jVEoydFOl9",
                "replyto": "q3K8bGL3ts",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4306/Reviewer_HLbG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4306/Reviewer_HLbG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my comments and addressing all of them. After reading other reviews, I have no doubt that this is a good paper with significant contributions and it may be highlighted at the conference."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700303327592,
                "cdate": 1700303327592,
                "tmdate": 1700303327592,
                "mdate": 1700303327592,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ueZCPmCi8S",
            "forum": "jVEoydFOl9",
            "replyto": "jVEoydFOl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4306/Reviewer_c3Sg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4306/Reviewer_c3Sg"
            ],
            "content": {
                "summary": {
                    "value": "Paper claims to propose a foundation model, named Ultra, for knowledge graph representation learning. The proposed model can handle full inductive graphs in which new entities and relations may appear in the test set. To do so, the authors propose to lift the graph to a one with relations as the nodes and design 4 different edge types (head2head, head2tail, tail2head, tail2tail). The relational representations are then learnt using message passing on this graph. The learnt relation embeddings are then used in the original graph to perform inductive link prediction. For the experiments, the authors pre-train their method on 3 KGs and further evaluate in a zero-shot setting and also by fine-tuning the downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper proposes a transductive model that works in settings of new relations and entity nodes.\n- The method obtains good zero-shot pretraining results."
                },
                "weaknesses": {
                    "value": "- The authors have not explicitly stated the computational complexity of the method. From the paper, it seems that the forward pass is run on the entire relational graph to obtain relation representations. This is then used to initialize the node embedding from the query triple and the process is repeated for every triple. Thus it seems that the entire graph is being used for link prediction every triple making the computational complexity O(E^2). This seems limiting for large graphs that have not been explored in the paper (such as wikidata-5m etc.).\n- From Table 2 we can see that finetuning over the pre-trained models helps the results significantly over the 0-shot setting. Also, the fine-tuning steps are too large to claim few shot results. This weakens the claim of the \"foundation model\" for KGs. A fair comparison would be to show the pretraining results for other inductive and transductive methods as well in addition to the SOTA comparison.\n- Another limitation is that of scale. Since the current model has fewer parameters, this would limit learning over larger pretraining datasets as can be seen in Figure 6 and also reported by the authors.\n- SoTA results for transductive models are better than the pre-trained Ultra model in many datasets. Thus the Ultra model seems to work well for the inductive setting rather than transductive. Thus the claim of the \"foundation model\" seems broader in scope.\n- We see that in the metafam dataset, the pretraining results are poor but on finetuning the results are improved drastically. This shows that the method works well in cases where the relational patterns of the downstream datasets are similar to the pre-trained one but when the data distribution changes the results suffer. Moreover, due to limited capacity, the model may not be able to handle such cases by increasing the pretraining datasets calling for downstream finetuning. Thus domain adaptation is not a problem which can be easily overcome by scaling the current model and this further weakens the claim of a \"foundation model\" for KGs."
                },
                "questions": {
                    "value": "- For weakness point 2: Any reason why this was not done by the authors?\n- For weakness point 3: How would this be addressed in future works for the model?\n-  For weakness point 4: Could the authors comment on why this would be the case and how would the model be improved to handle the transductive setting?\n- For weakness point 5: Any reason why the results on this dataset are not good?\n- Considering KGs are a rich source of textual/semantic data along with graph/structured data and the current model does not use this rich source of context information, how can we extend Ultra to incorporate the KG ontology? \n- Considering weaknesses 2,4,5 the claim of the foundation model seems a bit broad as of now and at best the model could be said to be a good inductive learner."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4306/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4306/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4306/Reviewer_c3Sg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800160331,
            "cdate": 1698800160331,
            "tmdate": 1699636398852,
            "mdate": 1699636398852,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UyAuWrVhLV",
                "forum": "jVEoydFOl9",
                "replyto": "ueZCPmCi8S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to c3Sg Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the useful feedback and would like to comment on the weaknesses and questions. \n\n> **W1. The authors have not explicitly stated the computational complexity of the method.**\n\nShort answer: The time complexity is the same as of NBFNet, i.e., it is linear in the number of edges. The memory complexity is linear in the number of nodes when using a fused message passing kernel (already provided by NBFNet) or linear in the number of edges if materializing all edge messages without the kernel.\n\nLonger answer: The time complexity of ULTRA is upper-bounded by the entity-level GNN (because the GNN on the graph of relations has negligible overhead as the number of nodes in this graph is the same as number of unique relation types $|\\mathcal{R}|$, and $|\\mathcal{R}| \\ll |\\mathcal{V}|$ - the number of relation types is usually orders of magnitude smaller than the number of nodes, e.g, 37 relations in YAGO310 with 123k nodes). In our case, the main entity-level GNN is NBFNet, so we mainly refer to the Appendix C of the NBFNet paper [1] for all necessary derivations. \n\nThe time complexity for a single layer is generally linear in the number of edges $O(|\\mathcal{E}|d + |\\mathcal{V}|d^2)$. With $T$ layers, the overall complexity of a single forward pass is $O(T(|\\mathcal{E}|d + |\\mathcal{V}|d^2))$ but $T$ is usually a small constant (6 layers) so the complexity is essentially linear to the number of edges. \nHowever, due to the sparsity of GNNs, they are usually bounded by memory. The memory complexity of the basic NBFNet implementation is $O(T|\\mathcal{E}|d)$ and linear to the number of edges, but thanks to the efficient kernelized implementation of the relational message passing (already provided by NBFNet), the memory complexity is reduced to $O(T|\\mathcal{V}|d)$ and is linear in the number of nodes.\n\nMoreover, the wall clock time and memory efficiency can be further reduced when applying more scalable and optimized versions of entity-level GNN such as AdaProp [2] or A*Net [3]. We include this discussion in the new Appendix F in the updated version of the manuscript.\n\nPredicting all possible triples in a graph is a rather artificial task and is hardly employed even by transductive shallow embedding methods as all such methods would require computing a $O(|\\mathcal{V}| \\times |\\mathcal{R}| \\times |\\mathcal{V}|)$ tensor which is rather impractical.\n\n> **W2. A fair comparison would be to show the pretraining results for other inductive and transductive methods as well in addition to the SOTA comparison.**\n\nShort answer: we report the comparison with the only available pre-trainable baseline (InGram) in Table 3 dubbed as \u201cno etypes, unconditional GNN (random)\u201d and it is about 2x worse than ULTRA. The table caption includes the reference to InGram.\n\nLonger answer: The term \u201cpre-training\u201d cannot be applied to transductive models as they cannot run inductive inference on unseen graphs because their entity/relation embedding vocabularies are fixed to that particular graph. For example, a transductive RotatE trained on FB15k237 can only run inference on FB15k237 and not on other 50+ graphs in our benchmark. \nWe are then left with inductive models which are then categorized into two families:\n* Inductive entity models, eg, NBFNet, RED-GNN, and similar) - they still learn relation embeddings pertaining to a particular data split and cannot be \u201cpretrained\u201d to run inference on unseen graphs, eg, NBFNet trained on FB V1 (one of the GraIL datasets from Teru et al) learns embeddings for 180 relations and can generalize to the test set of FB V1 where a graph has new entities but the same 180 relations. That said, such models still cannot generalize to unseen graphs with different relation vocabularies.\n* Inductive entity and relation models, eg, InGram, ISDEA, and MTDEA are the only models known in the literature at the moment. They can generalize to unseen relation vocabularies and can be \u201cpretrained\u201d to run inductive inference. Among them, ISDEA and MTDEA have issues with computational complexity and cannot scale to the graphs in our benchmark (eg, doing ranking evaluation on the full entity set). The only available baseline is InGram, and we do pretrain such a model on the same pre-training mixture as ULTRA and report its performance in Table 3 under the \u201cno etypes, unconditional GNN (random)\u201d ablation. InGram does not use edge types in the relation graph, does not use conditional GNN encoder, and uses random Glorot initialization for relations in the relation graph. Having the same pretraining mixture, such a model significantly underperforms ULTRA averaged across 54 graphs: 0.192 MRR vs 0.366 MRR of ULTRA."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124391165,
                "cdate": 1700124391165,
                "tmdate": 1700124391165,
                "mdate": 1700124391165,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yla77RuJRg",
                "forum": "jVEoydFOl9",
                "replyto": "ueZCPmCi8S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4306/Reviewer_c3Sg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4306/Reviewer_c3Sg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed reply and also for the additional experiments.\n\nThe responses address some of my concerns.\n\nBefore updating my score, I have further clarification questions/suggestions as below:\n\n1) For the baselines /related works for inductive relation/link prediction there are a few additional works [1,2,3,4,5] that the authors may have missed. It would help to discuss these in the paper.\n\n2) One concern that remains is that the method still needs to be finetuned on the entire dataset in most cases. I get that ULTRA requires 1 epoch (or thousands of steps) vs 10 epochs from scratch. But would it not be expected from a foundation model that the finetuning steps required should be orders of magnitude lesser? It would also help to have this study of the performance of ULTRA with a few examples (in addition to zero-shot performance) to understand the nature of the model in a new setting. This may also help practitioners decide how much training is required for the new dataset.\n\nReferences:\n\n1] https://proceedings.neurips.cc/paper_files/paper/2021/hash/a1c5aff9679455a233086e26b72b9a06-Abstract.html\n\n2] https://proceedings.mlr.press/v162/yan22a.html\n\n3] https://ieeexplore.ieee.org/abstract/document/9534355\n\n4] https://link.springer.com/article/10.1007/s11280-023-01168-w\n\n5] https://ojs.aaai.org/index.php/AAAI/article/view/16779"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683276322,
                "cdate": 1700683276322,
                "tmdate": 1700683321958,
                "mdate": 1700683321958,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QgA77Yx79o",
            "forum": "jVEoydFOl9",
            "replyto": "jVEoydFOl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4306/Reviewer_ebFz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4306/Reviewer_ebFz"
            ],
            "content": {
                "summary": {
                    "value": "The key limitation of designing the foundation models for dealing with the Knowledge Graphs (KGs) is that the KGs have different entities and relations that generally do not overlap. To address this issue, this paper proposes ULTRA, which positively transfers the information of source KG to unseen KG. It constructs relation representations based on the interactions between the relations by introducing the graph of relations. The proposed approach has shown good performance on various tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- From their experiments, the proposed methods have shown good performance on various tasks.\n- Research topics about the foundational models on graph-structured datasets is really interesting and important.\n- The paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "- The authors first pretrain the ULTRA model with the mixture of 3 standard KGs and then fine the model for the downstream task. But, the other supervised SOTA model only uses dataset of the downstream tasks without employing the pre-training datasets. If the supervised SOTA models are designed to deal with transductive settings, they may show worse performance on the downstream tasks. However, if the SOTA models are the models for the inductive setting, I think they may be possible to be pretrained like the ULTRA. So, could you measure the performance of the \"pretrained\" SOTA models on the inductive if possible?"
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4306/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698905077548,
            "cdate": 1698905077548,
            "tmdate": 1699636398789,
            "mdate": 1699636398789,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oMP667iUvi",
                "forum": "jVEoydFOl9",
                "replyto": "QgA77Yx79o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4306/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4306/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to ebFz"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the merits of our work including the importance of the tackled problem, experimental results, and writing quality.\nBelow, we would like to comment on the identified weaknesses.\n\n> **If the supervised SOTA models are designed to deal with transductive settings, they may show worse performance on the downstream tasks**\n\nYes, this is correct. To be even more precise, transductive models trained on one KG cannot run inductive inference on downstream tasks at all because their entity/relation embedding vocabularies are fixed to that particular graph. For example, a transductive RotatE trained on FB15k237 can only run inference on FB15k237 and not on other 50+ graphs in our benchmark. In that sense, the term \u201cpre-training\u201d cannot be applied to transductive models as they only fit one particular graph data distribution with a fixed set of entities and relations.\n\n> **So, could you measure the performance of the \"pretrained\" SOTA models on the inductive if possible?**\n\nShort answer: we report the comparison with the only available baseline (InGram) in Table 3 dubbed as \u201cno etypes, unconditional GNN (random)\u201d and it is about 2x worse than ULTRA. The table caption includes the reference to InGram.\n\nElaborated answer:\nBased on the first paragraph, we cannot pretrain transductive SOTA models to run inductive inference on our benchmark. We are then left with inductive models which are then categorized into two families:\n* Inductive entity models, eg, NBFNet, RED-GNN, and similar) - they still learn relation embeddings pertaining to a particular data split and cannot be \u201cpretrained\u201d to run inference on unseen graphs, eg, NBFNet trained on FB V1 (one of the GraIL datasets from Teru et al) learns embeddings for 180 relations and can generalize to the test set of FB V1 where a graph has new entities but the same 180 relations. That said, such models still cannot generalize to unseen graphs with different relation vocabularies.\n* Inductive entity and relation models, eg, InGram, ISDEA, and MTDEA are the only models known in the literature at the moment. They can generalize to unseen relation vocabularies and can be \u201cpretrained\u201d to run inductive inference. Among them, ISDEA and MTDEA have issues with computational complexity and cannot scale to the graphs in our benchmark (eg, doing ranking evaluation on the full entity set). The only available baseline is InGram, and we do pretrain such a model on the same pre-training mixture as ULTRA and report its performance in Table 3 under the \u201cno etypes, unconditional GNN (random)\u201d ablation. InGram does not use edge types in the relation graph, does not use conditional GNN encoder, and uses random Glorot initialization for relations in the relation graph. Having the same pretraining mixture, such a model significantly underperforms ULTRA averaged across 54 graphs: 0.192 MRR vs 0.366 MRR of ULTRA."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124069311,
                "cdate": 1700124069311,
                "tmdate": 1700124069311,
                "mdate": 1700124069311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "htBj4NIeZw",
                "forum": "jVEoydFOl9",
                "replyto": "oMP667iUvi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4306/Reviewer_ebFz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4306/Reviewer_ebFz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I have read all the reviews and their corresponding responses. \n\nMy all concerns are addressed and I keep my rating."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4306/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631611412,
                "cdate": 1700631611412,
                "tmdate": 1700631611412,
                "mdate": 1700631611412,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]