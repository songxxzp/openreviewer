[
    {
        "title": "It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition"
    },
    {
        "review": {
            "id": "7GoZLS3BVH",
            "forum": "QqjFHyQwtF",
            "replyto": "QqjFHyQwtF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_GWCM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_GWCM"
            ],
            "content": {
                "summary": {
                    "value": "To integrate acoustic information into the speech recognition output error correction using large language models (LLMs), the authors of this paper compare three fusion methods: early, mid, and late fusions. To further improve late fusion, this paper investigates adding uncertainty and proposes a new approach, Uncertainty-Aware Dynamic Fusion (UADF), introduced to integrate acoustic information, significantly enhancing word error rate (WER) and showing potential in audio-visual speech recognition."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Overall, this paper's structure is easy to follow.\nIt reminds me of previous milestone works about RNN decoders: Shallow fusion, Cold fusion, and Deep fusion. And this paper also has the potential to be a milestone. \n\nThe UADF is novel in this paper, and the improvements are guaranteed shown in Tables 1 and 2."
                },
                "weaknesses": {
                    "value": "ICLR conference is less specialized for speech researchers than ICASSP and INTERSPEECH. A brief and precise background introduction to the speech recognition framework is needed.\n\nFor the three frameworks in Figure 1, it needs to be clarified: where is N-best from? The N-best and the X_tok, X_enc, and X_dec are from different types of models according to the descriptions. It is better to mention these differences in the figures.\n\n\"language models have been widely utilized in ASR tasks over the past two decades,\" maybe there is a better way to express it; some earlier efforts are ignored.\n\nAdditionally, I believe you want to demonstrate that the proposed method is also applicable to audio-visual tasks. This requires a more detailed description; merely using a single paragraph here is insufficient."
                },
                "questions": {
                    "value": "The N-best are from WavLM and Whisper in GER-based H2T neurlPS2023. This paper is actually implementing a system combination, which, of course, will bring accuracy improvement. So, this work can be extended to more wider tasks."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698596295748,
            "cdate": 1698596295748,
            "tmdate": 1699636327373,
            "mdate": 1699636327373,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O5Qb045q8Z",
                "forum": "QqjFHyQwtF",
                "replyto": "7GoZLS3BVH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer GWCM"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer 1EWw for considering that this work is novel and easy to follow. Now we respond to your concerns and questions:\n\n- **Q1: More background for ASR and AVSR**\n\n    Thanks for your constructive suggestion. We have modified the related work Section for a more thorough literature review of ASR \n    technique, and more earlier efforts are reviewed. Meanwhile, we add more introduction to AVSR in Appendix A.4.\n\n- **Q2: Where is N-best from?**\n\n    We have modified the caption of Figure 1 to introduce the source of N-best list.\n\n\n- **Q3: Extending the proposed method to wider tasks**\n\n    Thanks for your suggestion. The UADF is potential to be applied in multimodal task with a auto-regressive decoding process, e.g., \n    image captioning, where an image-to-text model is designed to describe the input image with natural language. Since the \n    mainstream image caption models are auto-regressive decoding, we argue that an external language model can be dynamically \n    fused to improve the quality of predicted captions.  We leave it as our future work.\n\nThanks again for your time and patience."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043146720,
                "cdate": 1700043146720,
                "tmdate": 1700043146720,
                "mdate": 1700043146720,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W2izwn2AkT",
                "forum": "QqjFHyQwtF",
                "replyto": "O5Qb045q8Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_GWCM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_GWCM"
                ],
                "content": {
                    "title": {
                        "value": "good paper"
                    },
                    "comment": {
                        "value": "Many thanks for your reply. I have no other questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043249727,
                "cdate": 1700043249727,
                "tmdate": 1700043249727,
                "mdate": 1700043249727,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vlfmncpXsh",
            "forum": "QqjFHyQwtF",
            "replyto": "QqjFHyQwtF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_3ovr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_3ovr"
            ],
            "content": {
                "summary": {
                    "value": "This paper builds upon the recently proposed \u201cgenerative error correction\u201d (GEC) paradigm for speech recognition (ASR). The previous GEC work used a text-only LLM to map from a list of ASR hypotheses to a single transcription. In this paper, the authors propose to add acoustic information to this mapping process by including speech representation in the GEC. They experiment with several such \u201cfusion\u201d strategies (so-called early, mid, and late fusion), and finally show that late fusion with uncertainty awareness works best. Evaluations are conducted on a number of ASR benchmarks, showing significant WER improvements compared to GEC or ASR-only baselines, and the method is also shown to generalize to audio-visual ASR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Using LLMs for ASR error correction is an interesting idea and has shown strong results in the past. ASR models often perform badly on rare words such as named entities, and LLMs can be a useful component to improve recognition of such words. In the earlier GEC work, only the top hypotheses of the ASR model was used for the final transcription. Since these hypotheses may not always contain information from the correct word, it is natural to also use the ASR information more directly. With these considerations, the ideas in this paper are relevant and a logical continuation in this line of work.\n\n2. Incorporating modality-specific uncertainty using the UADF approach is a novel and interesting technique, and seems to provide reasonable improvements compared to static weighting of modalities. For example, in Table 4, we see that particularly on very low SNR conditions, there is a large improvement in WER when using UADF instead of static weights. I also appreciate that the authors show ablation results and analysis for the UADF strategy in Section 5.2.\n\n3. In addition to traditional ASR benchmarks, the authors show that their UADF strategy can be used for audio-visual ASR, where it gives a small improvement over AV-HuBERT."
                },
                "weaknesses": {
                    "value": "My main concerns about the paper are about the presentation (in terms of relation with prior work) and comparison with baselines in the experiments, which I will detail below.\n\n### This model in the context of prior work\n\nIn order to make this point clear, let me briefly summarize some key events in the history of the use of language models (LMs) in ASR.\n- ASR was formulated as a noisy channel model using the Bayes rule P(W|X) = P(X|W)P(W). The two distributions were named an acoustic model and a language model, respectively. The LM was trained separately on source text, and only used for decoding [1].\nWFST-based decoding provided a way to create a incorporate n-gram LMs into the decoding graph for efficient **first-pass decoding** [2].\n- Incorporating larger n-gram LMs was hard (since the decoding graph explodes), so researchers used them instead in **second-pass rescoring** in both offline and on-the-fly settings [3, 4], by subtracting the original LM scores and adding back the larger LM\u2019s scores.\n- The Bayesian formulation still made sense in the era of hybrid HMM-DNN model, even when the acoustic models were discriminatively trained, since the scores could be interpreted as pseudo-likelihoods by subtracting an appropriate prior, and so the same decoding/rescoring framework carried over.\n- In the era of \u201cend-to-end\u201d ASR, the models are trained to directly estimate P(W|X), and so log-linear interpolation with an external LM is not interpretable in the Bayesian sense anymore. Yet, practitioners still do this and call the process \u201cfusion\u201d (e.g. shallow fusion, cold fusion, etc. [5, 6]) \u2014 this is analogous to using LMs in the first-pass decoding.\n- Analogous to the second-pass rescoring, we can obtain candidates using beam search on an ASR model, and then re-rank them with an externally trained LM (e.g. as done in the original LAS paper). So far, this \u201crescoring\u201d step only manipulates the hypotheses, without considering the acoustic information. Let\u2019s call this model (A).\n- More recently, a two-pass E2E ASR model was proposed which has an encoder shared between a streaming RNN-T model and a full-context LAS decoder [7]. The idea was to perform a first-pass decoding with the RNN-T head, and use the decoded hypotheses along with the encoder representations to rescore using the LAS head in a *acoustic-guided rescoring* technique.\n- In [8], the authors further built upon this two-pass model by proposing a **deliberation network**, which is an LAS decoder that attends to both the acoustic representations as well as the first-pass hypotheses, to generate the output. Let\u2019s call this model (B). \n\nThe Generative Error Correction (GER) model (which this paper extends) is analogous to model (A), with the exception that it formulates the rescoring problem as a many-to-one sequence transduction task, as opposed to a simple re-ranking task. \n\nThe proposed model is similar to model (B), as is clear from equation (2). Its goal is to generate a new sequence given acoustic representations and an N-best list. In fact, the \u201cearly\u201d and \u201cmid\u201d fusion methods in the paper are, in my opinion, exactly the same as a deliberation model, with the exception that the decoder is frozen instead of being jointly trained. \n\nSituating late fusion is a little more complicated since the fusion only happens at the scoring stage, and it is quite confusing to me exactly how to think about it. This \u201clate fusion\u201d is, in a way, similar to shallow fusion methods in E2E ASR, with the exception that a GER model is used in place of the external LM. The other view is how the authors present it: GER with additional acoustic information.\n\nWhile it is completely acceptable to use new terminology that is more in keeping with broader advances in LLMs, I think it would be beneficial to situate the proposed method in the context of the above ASR+LM strategies to make the paper more widely accessible. In summary, the use of LMs in ASR can be through either first-pass decoding or second-pass rescoring (this paper falls into the latter). Second pass rescoring may be modeled as re-ranking or sequence generation, and methods may or may not use the original acoustic information. This kind of taxonomy and categorization can be used to clarify the contribution of this work. \n\n[1] Jelinek, Frederick. \u201cContinuous speech recognition by statistical methods.\u201d Proceedings of the IEEE 64 (1976): 532-556.\n\n[2] Mohri, Mehryar et al. \u201cSpeech Recognition with Weighted Finite-State Transducers.\u201d (2008).\n\n[3] Ljolje, Andrej et al. \u201cEfficient general lattice generation and rescoring.\u201d EUROSPEECH (1999).\n\n[4] Sak, Hasim et al. \u201cOn-the-fly lattice rescoring for real-time automatic speech recognition.\u201d Interspeech (2010).\n\n[5] Chorowski, Jan and Navdeep Jaitly. \u201cTowards Better Decoding and Language Model Integration in Sequence to Sequence Models.\u201d Interspeech (2016).\n\n[6] Sriram, Anuroop et al. \u201cCold Fusion: Training Seq2Seq Models Together with Language Models.\u201d Interspeech (2017).\n\n[7] Sainath, Tara N. et al. \u201cTwo-Pass End-to-End Speech Recognition.\u201d ArXiv abs/1908.10992 (2019): n. pag.\n\n### Evaluation problems\n\nIf we accept the above categorization of models, the paper can be viewed in 2 ways: (i) shallow fusion of ASR and GER, and (ii) GER with acoustic information. As such, the proposed method should be compared against the following baselines: (1) end-to-end ASR, (2) shallow fusion of ASR and LLM, and (3) GER-only. The authors have compared against (1) and (3), but not against (2), which may be important to show how using GER improves compared to simply using a LLM with shallow fusion.\n\nThis is important because training the GER itself requires generating N-best hypotheses for the whole training data, which is computationally expensive, and it can only be used in an offline manner, while simple shallow fusion can be done on-the-fly. As such, we expect to see significant WER gains when using the GER module.\n\nAnother concern, which is a common issue when using LLMs, is the following: *How can we ensure that the test data was not seen by LLAMA?* In particular, the authors show in Table 3 that we get a large improvement on CHiME-4 when using the proposed method. CHiME-4 is essentially WSJ read outdoors, and WSJ consists of text from newspapers, which are in the public domain. It would be more useful to conduct experiments on a closed dataset, such as CHiME-5, which is less likely to have been memorized by LLAMA.\n\n### Other minor comments\n\n1. Some sections of the paper use a lot of flowery language which can be avoided. For example, in the last paragraph in Section 1, extraneous terms such as \u201ca pioneering UADF technique\u201d, \u201cadroitly allocates\u201d, and \u201cconspicuously outperforming\u201d can be made concise. In general, authors should stick to reporting rather than embellishing.\n2. In Section 2 (paragraph 1), the authors mention that language models have been used in ASR for two decades, but the oldest citation is only from 2019. I think they should conduct a more thorough literature review in order to perform correct credit attribution.\n3. The use of the terms \u201cearly\u201d, \u201cmid\u201d, and \u201clate\u201d fusion in the paper may be confusing for readers familiar with ASR methods such as shallow, cold, and deep fusion. The authors should consider either changing *fusion* to another word, or making the difference explicit in the paper."
                },
                "questions": {
                    "value": "Some of the proposed methods are specific to particular model choices. For example, (i) \u201cearly fusion\u201d assumes that we are using Wav2Vec 2.0 as the encoder, and (ii) late fusion would only work with an encoder-decoder style model. The dominant ASR modeling strategy in the industry is conformer-transducers, which do not have such speech tokens, and their logit space is 3-dimensional (where vocabulary dimension also includes a blank token). Have the authors considered how the proposed method would work with such models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3711/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3711/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3711/Reviewer_3ovr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698623996716,
            "cdate": 1698623996716,
            "tmdate": 1700502205510,
            "mdate": 1700502205510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NxlP11Tzdu",
                "forum": "QqjFHyQwtF",
                "replyto": "vlfmncpXsh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer 3ovr"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer 3ovr provides professional comments, which is quite constructive for improving this work. Now we respond to your concerns and questions:\n\n- **Q1: A more thorough literature review.**\n\n    Thank you for pointing out our oversight. We have modified this part and please see it in the new revision paper.\n\n- **Q2: The language style in Introduction.**\n\n    Thanks for your comment. We have modified our introduction according to your suggestion. \n\n- **Q3: The name of proposed fusion methods.**\n\n    We fully understand your concern. According to your suggestion, we add a paragraph in Section 3.1 to illustrate the relationship to \n    previous works, which includes the shallow fusion, cold fusion, and deep fusion you metioned.\n\n- **Q4: The evaluation problem of shallow fusion.**\n\n    We apologize for the misunderstanding caused. The \u201cstatic fusion\" baseline in our experiment can be viewed as a kind of \u201cshallow \n    fusion\u201d approach. In static fusion, we conduct the grid search on a small unseen validation (200 utterances) to find the suitable \n    weight to balance the weights of LLM and ASR systems. This process is akin to searching for the optimal hyper-parameters of LM\u2019s \n    weight in shallow fusion.\n\n    Additionally, we have some other thoughts on your point about the comparison with shallow fusion. As you said \u201csimple shallow \n    fusion can be done on-the-fly\u201d, the foundation is that we have a well-trained ASR model and a language model (usually trained with \n    in-domain data). However, since UADF does not need any joint training process, it also can be done on-the-fly based on a well- \n    trained ASR model and GER model. In other words, it just replaces an in-domain LM with an in-domain LLM to independently \n    perform GER. Furthermore, because the GER utilizes the LoRA-tuning, the experiment of a single dataset can be completed in less \n    than one day with a single GPU (Nvidia A40 or 3090). Although this still requires more time than training an in-domain LM, GER is \n    more capable of independently predicting transcription, also resulting in a significant improvement to the ASR performance. \n\n- **Q5: Application in Conformer-transducers models.**\n\n    Thank you for bringing up this topic. We think that you provide a potential solution for a practical scenario: the conformer-transducer \n    model provides prompt response for users, and the LLMs perform error correction for high-quality results. To integrate them, we \n    think an asynchronous decoding strategy should be proposed, where LLM and transducer need to launch two separate decoding \n    iterations. When the transducer decoder predicts a \u201cblank\u201d token, it should skip the fusion process. Otherwise, the logits are passed \n    into LLM\u2019s decoding iteration for dynamic fusion. However, under this setting, a primary challenge in asynchronous decoding we can \n    identify is the mismatch in length. A further alignment operation is required to ensure the temporal consistency between the logits \n    from two decoders.  \n\n- **Q6: How to ensure ASR corpus is unseen for LLaMA.**\n\n    Thanks for your question. This concern has been discussed in Hyporadise paper, we report their observations here: 1) They \n    randomly write some customized sentences with errors that are unseen for LLM, while these errors can be corrected. 2) They \n    employ the T5 model (trained with publicly available C4 dataset) as the error correction backbone model, and it still works on \n    different datasets. \n    To reconfirm this issue, we reviewed the pre-training dataset introduced in LLaMA paper [1], and also checked each subset of \n    training data. No traces of WSJ\u2019s transcription were found in these datasets. Furthermore, considering that WSJ is a paid dataset, its \n    transcription is typically not used for pre-training of LLMs.    \n\n- **Reference**\n\n    [1] Touvron, Hugo, et al. \"Llama: Open and efficient foundation language models.\" arXiv preprint arXiv:2302.13971 (2023).\n\n\nThanks again for your time and patience. We hope our explanation can respond to your concern."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700042152800,
                "cdate": 1700042152800,
                "tmdate": 1700042152800,
                "mdate": 1700042152800,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uhLCP7Y9qX",
                "forum": "QqjFHyQwtF",
                "replyto": "NxlP11Tzdu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_3ovr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_3ovr"
                ],
                "content": {
                    "title": {
                        "value": "Increase score to weak accept"
                    },
                    "comment": {
                        "value": "Thanks for your response to my comments. With the edited manuscript, my concerns about literature review, flowery language in the text, and the possibility for test set leakage in the LLM are alleviated.\n\nRegarding the other concerns, here are my additional comments:\n\nQ4: This is a nice equivalence, and it would be useful to point it out explicitly in the results section.\nQ5: Sounds like a interesting possibility for future work --- I'll let this pass for this review.\n\nBased on the above, I am happy to increase my score to a weak accept."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502180890,
                "cdate": 1700502180890,
                "tmdate": 1700502180890,
                "mdate": 1700502180890,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fuF73OSo5o",
            "forum": "QqjFHyQwtF",
            "replyto": "QqjFHyQwtF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_1EWw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_1EWw"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a late fusion strategy called \"UADF\" that combines the modalities of LLM and ASR to enhance generative error correction. The research demonstrates promising results across multiple datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Integrating acoustic information with LLM is a promising strategy that enhances the capabilities and potential of LLMs in ASR tasks. This approach leverages the strengths of both acoustic processing and advanced language modeling, providing a more robust framework for GER.\n2. The paper conducts a comprehensive examination of different fusion strategies, delving into detailed analyses of each approach.\n3. The proposed UASF yields promising results across datasets with varying conditions, including ASR and VASR."
                },
                "weaknesses": {
                    "value": "I believe it's not entirely fair to make comparisons with GER given the use of acoustic information.  It would be better to add some comparisons of the results with LLM scoring."
                },
                "questions": {
                    "value": "1. What distinguishes the proposed method from shallow fusion using LLM?\n2. In equation 5, do $f^{llm}$ and $f^{asr}$ need to be of the same length? If they do, how to ensure this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3711/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3711/Reviewer_1EWw",
                        "ICLR.cc/2024/Conference/Submission3711/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698628645109,
            "cdate": 1698628645109,
            "tmdate": 1699648252414,
            "mdate": 1699648252414,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "68p1V9tMGW",
                "forum": "QqjFHyQwtF",
                "replyto": "fuF73OSo5o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer 1EWw"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer 1EWw for considering that this work is comprehensive and provides a robust framework. Now we respond to your concerns and questions:\n\n- **Q1: Comparison with GER approach and other LLM rescoring baseline**\n\n    We agree that UADF uses more acoustic information than GER. However, for LLM, this kind of acoustic information is \n    unrecognizable, or even harmful due to the large modality gap, according to the results of early fusion. Our work aims to leverage \n    this acoustic information to improve the ASR results. Therefore, we compared the proposed UADF with early fusion and mid fusion \n    that also utilize the acoustic information. \n\n    For the rescoring baseline, as GER has surpassed the upper bound of the rescoring-based method (as oracle $o_{nb}$ shown in \n    Hyporadise paper), so we directly compare with the result of GER. We add a further LM rescoring baseline and oracle information in \n    Appendix A.1 Table 6.\n\n- **Q2: The length of $f^{llm}$ and $f^{asr}$**\n\n    We wonder the \u201clength\u201d you asked is the length of distribution or decoding sequence, but both of them need to have the same \n    length. For distribution, they are the vocabulary size 1D embedding, since our ASR model has been aligned with LLM. For sequence \n    length, as it is auto-regressive decoding, so they predict the current token together including the <eos> (end of sentence), so the \n    sequence length remains consistent throughout.\n\n- **Q3: The difference with shallow fusion.**\n\n    Thanks for your question. We add a paragraph to illustrate it in Section 3.1. UADF performs fusion at the same place as shallow \n    fusion. However, UADF adopts a dynamic fusion after analyzing the uncertainty of LLMs prediction, while shallow fusion employ a \n    static weight (our static fusion baseline is a kind of shallow fusion). Furthermore, in typical shallow fusion, LM probability usually is \n    assigned a small weight since it is viewed as auxiliary information. In UADF, both systems can independently predict transcription \n    and LLM plays a main role since GER shows better WER results than ASR."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700041324529,
                "cdate": 1700041324529,
                "tmdate": 1700041324529,
                "mdate": 1700041324529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nPvWhtr779",
                "forum": "QqjFHyQwtF",
                "replyto": "68p1V9tMGW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_1EWw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_1EWw"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's response. I don't have any further questions, and I would like to maintain the current score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702398003,
                "cdate": 1700702398003,
                "tmdate": 1700702398003,
                "mdate": 1700702398003,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ec1qg2gFIH",
            "forum": "QqjFHyQwtF",
            "replyto": "QqjFHyQwtF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_2SYt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_2SYt"
            ],
            "content": {
                "summary": {
                    "value": "Recent studies showed that large language models (LLMs) can be successfully used to map the N-best hypotheses list generated by an ASR system to the predicted output transcription to improve ASR accuracy. But LLM is not trained with acoustic information. This paper proposed a method named Uncertainty Aware Dynamic Fusion (UADF) to infuse acoustic information to the auto-regressive decoding process with ASR decoder and LLM predictions. Specifically, it uses late fusion to combine the probability output by ASR decoder and LLM to predict the recognition results. In this process, temperature scales are applied to the logits of ASR decoder and LLM to match up the model confidence and recognition accuracy. Besides, the ASR and LLM combination weights are decided by LLM entropy so when LLM\u2019s uncertainty is larger, more compensation from the ASR model will be used to decide the recognition results. The proposed method showed obvious WER reduction for several ASR tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposed several methods applied to the fusion of ASR decoder and LLM output to improve performance. The paper compared the proposed method with several existing methods and did some ablation study to analyze the effectiveness of the proposed method. It also did experiments to prove the generalization of the proposed method. All the results showed the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "\u2022\tSome contents are not accurate or clear. \no\tIn eq. (9), the entropy of LLM output is not accurate since for a discrete distribution, the entropy should be the sum of -p(y_t,i)logp(y_t,i). \no\tIn figure 2, it\u2019s clear the probability of \u201call\u201d from ASR is high. But it didn\u2019t show where is the probability of \u201call\u201d from LLM ? And what\u2019s the meaning to show the blue cross in the left bottom part and the green cross in the right bottom part? \no\tIn the experiments part, for all late fusion experiments, the ASR model needs to be retrained with the training data. But it\u2019s not clear whether all the training data are used to get a unified model or for each task (WSJ, ATIS, Chime-4), the model is updated only with the training data from this task. \no\tThe results in table 2 showed that without calibration, the proposed UADF did not perform better than the static method (WER 1.39 vs. 1.36). This means calibration is crucial for UADF, so the tuning of temperature t1 and t2 becomes a key to make it work. But the paper didn\u2019t give enough information on these two parameters, such as what\u2019s the optimal value, should we tune them for different tasks and so on."
                },
                "questions": {
                    "value": "1.\tDo we have the WER of the top 1 hypothesis in HyPoradise dataset for different tasks? These values may not be useful to validate the proposed method. But it will make the readers understand better why we need to use LLM to refine the N Hypotheses.  \n2.\tAs noted above, tuning of temperature t1 and t2 becomes a key point to make the proposed method work. The questions about this are: \na.\tDid the author observe the same conclusion for other tasks (WSJ,  Chime-4, LRS3)? \nb.\tDo we need to tune these values for different tasks, or we could get them with one task\u2019s data and applied them to other tasks.\nc.\tHow much minimum data should we use to tune these values? \nd.\tWhat\u2019s the optimal value for these two parameters for the tasks in this paper? \n3.\tIn eq(10), the weight of LM and ASR didn\u2019t sum up to 1.0. Will it matter since this means for different t, the range of P(y_t) will be different. \n4.\tIt\u2019s noted that \u201cbeta\u201d in eq. (10) is set to 0.5 for the experiments. It didn\u2019t show what\u2019s the performance is if we change it to other values. Will the results be sensitive to this value?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3711/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3711/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3711/Reviewer_2SYt"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699238760036,
            "cdate": 1699238760036,
            "tmdate": 1699636327062,
            "mdate": 1699636327062,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kRXLosRARr",
                "forum": "QqjFHyQwtF",
                "replyto": "ec1qg2gFIH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer 2SYt"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer 2SYt for considering that this work is effective, and your  suggestion is constructive. Now we respond to your concerns and questions:\n\n -  **Q1: Explanation of Figure 2.**\n\n    We are sorry for the unclear presentation. In Figure 2, we only show the top-2 candidates from ASR and LLM, where the probability of \n    \u201call\u201d is too low (close to 0) to be presented. For UADF, we present the top-3 candidates after fusion. We have revised the caption to \n    illustrate it. \n\n- **Q2: Training data for ASR model in late fusion.**\n\n    We retrain the ASR model as the tokenizer mismatch between ASR model and LLM. In other words, if Whisper has the same \n    decoding space with LLaMA, this step is not necessary as Whisper can perform as a general ASR engine. For the dataset you  \n    mentioned, we employ a common Whisper encoder and train separated decoder (very small) for each dataset. These decoders \n    share the same decoding space with LLaMA.\n\n- **Q3: The top-1 hypothesis in HyPoradise dataset for different datasets.**\n\n    We have included it in the Appendix A.1 Table 6. Furthermore, we provide a LM rerank baseline and oracle performance for \n    comparison.\n\n- **Q4: The importance of tuning t1 and t2.**\n\n    Thank you for bringing up this topic. Firstly, as we mentioned in this paper, we find t1 and t2 on each dataset with binary search \n    algorithms on a small validation set including only 200 unseen utterances (they can provide thousands of token examples). This \n    search strategy is also used in classification tasks [1]. However, compared with classification tasks, we have indeed found out that \n    the impact of t1 and t2 are intricate to the WER performance. Now we provide some empirical conclusion to illustrate the impact of \n    t1 and t2 according our experiment:\n\n    (1) t1 acts on the LLM to alleviate its overconfidence problem. When WER is low, LLM tends to always output a confidence very \n    close to 1, which is difficult to be corrected if it\u2019s a wrong prediction. Accordingly, t1 would reduce the peak value of the distribution.  \n    (2) t2 acts on the ASR model that also exhibits an overconfidence phenomenon. Meanwhile, the calibration of ASR results in a \n    smoother probability distribution, which encourages more potential candidates to LLMs according to E.q.(10).  \n    (3) In general, the benefits of t1 and t2 vary from different dataset due to their different WER levels. So we think the best solution is \n    employing a small validation set to find them for each dataset. Moreover, static fusion baseline also requires a validation set to find a \n    suitable weight (grid search).   \n\n - **Q5: In eq(10), the weight of LM and ASR didn\u2019t sum up to 1.0.**\n\n    Many thanks for your comments. We understand your concern, and we have added a further softmax function for each step. But \n    please allow us to explain that in E.q.(10) the token index with highest value would be selected as results for each step, so this \n    operation would not affect the decoding strategy.\n\n - **Q6:  The value of \u201cbeta\u201d.**\n    We set beta as a fixed value of 0.5 in the ASR experiments that avoids introducing too many hyper-parameters. In practice, the \n    results are not sensitive to beta when it varies from 0.3 to 0.5 on ATIS, WSJ, and ChiME, as the GER performs better than ASR. \n    However, on other datasets, when two systems achieve comparable WER results on validation set (e.g. low SNR condition in AVSR \n    task), we can reduce the value of beta to improve the importance of the second model in E.q.(10).   \n\n\n\n- **Reference**\n\n    [1] Kumar A, Ma T, Liang P, et al. Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift[C]//Uncertainty in \n    Artificial Intelligence. PMLR, 2022: 1041-1051."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700040714841,
                "cdate": 1700040714841,
                "tmdate": 1700040714841,
                "mdate": 1700040714841,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ibhuxq04ll",
            "forum": "QqjFHyQwtF",
            "replyto": "QqjFHyQwtF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_kWJM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3711/Reviewer_kWJM"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an approach they describe as Uncertainty Aware Dynamic Fusion (UADF) that 1. calibrates the LLM score at the token level accounting for the over-confidence of the neural network when doing auto-regressive decoding, 2. a time-changing uncertainty uncertainty that dynamically adjusts the decision-level fusion of text-based LLM and ASR scores. The results show significant improvement over strong baseline models such as Whisper and wav2vec or Hubert on a variety of tasks. Further the authors demonstrate that the results can be applied in an AVSR task with the dynamic uncertainty improving results in noise over a static fusion."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall the work is well motivated and demonstrates strong empirical results."
                },
                "weaknesses": {
                    "value": "The point about calibration in using LLMs for rescoring of ASR hypotheses may be novel, however both calibration of hybrid or neural network ASR model scores and the use of strong language models combined with ASR scores are not novel approaches and have been widely used within speech. \n\nThe paper seems to desire establishing the term \"modality laziness\" as a technical way of describing when multimodal system underachieve compared to a unimodal system; in this case I would describe this as a simply a poorly designed system. There are multiple papers in the literature that describe effective ways of combining audio and visual modalities to yield an improve AVSR system over any single modality. Look for the term \"modality drop out\" and work on audio-visual conformers or AV-hubert.\n\nApplying the entropy of the LLM predictive posterior eqn (9)  as a weighting factor for the dynamic fusion neatly correlates with the desired property of: when the LLM is more certain, then the ASR weight of the ASR can be less, and when the LLM is uncertain, then the ASR weight is higher.  However, it doesn't stem from the statistical approach of ASR where \nY_T = argmax Y_n p(X|Y_n; M_am) P(Y_n; M_lm ) where the first term in the argmax is the acoustic score and the second term the text-based LLM score, which could be computed with a LLM. In this light, without a Bayesian motivation, one might consider UADF as a hueristic method of combining LLM scores with ASR.\n\nLastly, the results here are all conducted on speech recognition tasks. Its unclear if such an approach can be applied to non-speech tasks or is of interest to the broader ICLR community."
                },
                "questions": {
                    "value": "Can you re-frame UADF in a Bayesian framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699333901717,
            "cdate": 1699333901717,
            "tmdate": 1699636326971,
            "mdate": 1699636326971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jBr6dgVwT0",
                "forum": "QqjFHyQwtF",
                "replyto": "Ibhuxq04ll",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer kWJM"
                    },
                    "comment": {
                        "value": "We sincerely appreciate Reviewer kWJM for considering that this work is well motivated and demonstrates strong empirical results. Now we respond to your concerns and questions:\n\n- **Q1: Discussion on modality laziness problem**\n\n    We agree with your comment about the audio-visual combining approach in AVSR system. However, we need to clarify that our \n    method is complementary to those approaches, e.g., modality dropout. Specifically, take the AV-HuBERT, you mentioned, as an \n    example, the modality laziness still exists: When the SNR is low, bimodal AV-HuBERT performs worse than visual-only modality \n    according to [1]. Therefore, we argue that modality laziness can not be completely avoided by representation learning approaches in \n    AVSR, and our proposed method can work together with modality dropout-like methods to further mitigate this problem and improve \n    the performance. \n\n    With respect to our main task: The motivation of our paper is to explore a suitable strategy to make LLM leverage acoustic \n    information for GER, and modality laziness is a problem when we fuse acoustic information to LLMs - refer to the result with early \n    fusion. This is intuitive because LLMs are very proficient in language but unfamiliar with speech. To this end, we propose the UADF \n    approach that performs a dynamic fusion for each decision step of token prediction, which avoids the joint training of two systems.\n       \n- **Q2: Extent on Non-speech tasks to the broader ICLR community.**\n\n    Thanks for your suggestion. Our motivation is to make the LLMs leverage acoustic information, and the challenge mainly stems from \n    the large modality gap between speech and text. Therefore, we evaluate the UADF on ASR and AVSR tasks. Furthermore, there is a \n    potential application case is image captioning, where an image-to-text model is designed to describe the input image with natural \n    language. Since the mainstream image caption models are auto-regressive decoding, we argue that an external language model can \n    be dynamically fused to improve the quality of predicted captions.  \n\n- **Q3: Re-frame UADF in a Bayesian framework.**\n\n    Eq. (10) can be thought of as a system combination scheme carried out in R^V space, and it is not related with the well-known plug- \n    in MAP decision rule used in speech decoding (plug-in maximum a posteriori decoder), where a likelihood (P(X|Y) and a prior \n    probability P(Y) are combined to obtain the best sequence Y*.\n\n    In the present work, we decided to handle uncertainty leveraging entropy of the posterior distribution given in Eq. (9) for the reasons \n    discussed in the paper. The proposed system combination scheme could eventually be re-formulated and casted into a Bayesian \n    framework following, for example, [2], in future work.\n\n**Reference**\n\n    [1]Chen, Chen, et al. \"Leveraging modality-specific representations for audio-visual speech recognition via reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.\n    [2]Kim, Hyun-Chul, and Zoubin Ghahramani. \"Bayesian classifier combination.\" Artificial Intelligence and Statistics. PMLR, 2012. Available from https://proceedings.mlr.press/v22/kim12.html."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039851731,
                "cdate": 1700039851731,
                "tmdate": 1700039851731,
                "mdate": 1700039851731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YwxSodsXL4",
                "forum": "QqjFHyQwtF",
                "replyto": "jBr6dgVwT0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_kWJM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Reviewer_kWJM"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the reply. Given the responses I wish to keep my score unchanged. Detailed responses below.\n\n* Re Q1 response:\nThis is an unfair comparison in [1]: if you've trained your model only on clean data, then you've taught it to expect clean data. The fairer comparison is to provide noise at similar levels to what you expect to test it in, which is called multi-style or multi-condition training. Of course, you may test with an unseen type of noise, but the level of the added noise in training can be chosen with a distribution that at least exposes the model to the levels you are testing. Without using multi-style training, which is easily applied, the baseline is much weaker than it could be making any compensation technique appear better than it would against a stronger widely used baseline in the field. Of course for the actual paper under review, the Whisper model is a given and the experiments cannot be made to change its training.\n\n* Re Q2 response:\nI agree with the authors that the technique presented could be applied to other domains outside of speech, however potential is different than actual experiments.\n\n* Re Q3 response:\nInteresting, reference! Would be neat to see this reformulated in the Bayesian framework in [2]."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495938582,
                "cdate": 1700495938582,
                "tmdate": 1700495938582,
                "mdate": 1700495938582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "N9gs1L2Uok",
                "forum": "QqjFHyQwtF",
                "replyto": "Ibhuxq04ll",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response for Reviewer kWJM"
                    },
                    "comment": {
                        "value": "Thanks again for your feedback. We fully understand your suggestion in \"Re Q1 response\", but we think there is some misunderstanding about our work and [1]. We think the misunderstanding mainly stems from Table 1 in [1] where WER is extremely high when SNR=-15. However, we **do not add SNR=-15** in our paper, instead, our setting {\u221210, \u22125, 0, 5} exactly follows the AV-Hubert paper [2], which proposed a noise-robust AV-Hubert that contains the techniques you mentioned (e.g. dropout and adding training noise). They define these SNR levels to test the model performance and we think it is reasonable. Here we explain more details about your concern.\n\n- The AV-Hubert model used in [1] and our paper is trained by the \"multi-condition training\" approach you mentioned, as \"Babble\" noise **is included** in the training process and it is **seen noise**. More training details can be found in [2]. \n\n- The modality laziness problem also can be found in the Table 2 (\"Babble\" column) of [2]. As [2] has included the dropout-like technique you mentioned, it shows that our method is complementary to these methods and can further improve the result without further labeled data.\n\n- Finally, besides the noisy condition, the proposed UADF also improves the performance in **clean** and **high SNR** conditions. In other words, UADF can alleviate the modality laziness in low SNR, but our contribution is not limited to it.\n\n\n[1] Chen, Chen, et al. \"Leveraging modality-specific representations for audio-visual speech recognition via reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 37. No. 11. 2023.\n\n[2] Shi B, Hsu W N, Mohamed A. Robust self-supervised audio-visual speech recognition[J]. arXiv preprint arXiv:2201.01763, 2022.\n\nWe sincerely appreciate your time and effort. We look forward to discussing more with you on this issue."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501620307,
                "cdate": 1700501620307,
                "tmdate": 1700502241057,
                "mdate": 1700502241057,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]