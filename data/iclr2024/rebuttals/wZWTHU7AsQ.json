[
    {
        "title": "Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations"
    },
    {
        "review": {
            "id": "K0CSRP4WA9",
            "forum": "wZWTHU7AsQ",
            "replyto": "wZWTHU7AsQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5179/Reviewer_GZCq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5179/Reviewer_GZCq"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of deploying reinforcement learning (RL) systems that can withstand uncertainties, particularly those that are temporally coupled. Recognizing that conventional robust RL methods may falter against such temporally-linked perturbations, the authors introduce a game-theoretic approach called GRAD (Game-theoretic Response approach for Adversarial Defense). GRAD conceptualizes the robust RL problem as a partially observable two-player zero-sum game and uses Policy Space Response Oracles (PSRO) to achieve adaptive robustness against evolving adversarial strategies. The study's experiments confirm GRAD's performance in ensuring RL robustness against both temporally coupled and standard adversarial perturbations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors formulate the robust RL objective as a zero-sum games and demonstrating the efficacy of game-theoretic RL in tackling this objective."
                },
                "weaknesses": {
                    "value": "1. This paper does not have a clear mathematical representation of the problem it intends to address. \n\n2. The article claims its primary contribution lies in using zero-sum games to formulate the robust RL problem. However, employing zero-sum games to account for uncertainties, whether in single-agent or multi-agent RL, is well-established, as seen in works like Robust Adversarial Reinforcement Learning, Robust Reinforcement Learning as a Stackelberg Game via Adaptively-Regularized Adversarial Training, and Robust Multi-Agent Reinforcement Learning with State Uncertainty. Given this widespread application, the paper's stated novelty becomes questionable.\n\n3. In terms of algorithmic design, the proposed method is largely an application of the Policy-Space Response Oracles (PSRO). The novelty seems limited, and it's unclear how PSRO uniquely addresses the issue of temporally-coupled perturbations.\n\n4. Considering that the PSRO algorithm converges to an NE in two-player, zero-sum games and has seen recent extensions to other equilibria types [1, 2], the paper's proposed method, essentially a reiteration of PSRO, makes the convergence proof for GRAD appear somewhat lackluster in its contribution.\n\n5. This paper has ample room for improvement in writing, problem formulation, and the method itself. For instance, by simply using the triangle inequality, we could give the range of $\\bar{\\epsilon}$ that break the temporally-coupled property, rather than merely stating, \"By setting \\epsilon \u0304 to a large value, it converges to the non-coupled attack scenario.\" Additionally, the motivation behind temporally-coupled perturbations lacks clarity and persuasiveness, leaving me unconvinced of its pressing relevance.\n\n[1] Lanctot, Marc, et al. \"A unified game-theoretic approach to multiagent reinforcement learning.\" Advances in neural information processing systems 30 (2017).\n\n[2] McAleer, Stephen, et al. \"Pipeline psro: A scalable approach for finding approximate nash equilibria in large games.\" Advances in neural information processing systems 33 (2020): 20238-20248."
                },
                "questions": {
                    "value": "Please see the Weaknesses, I will decide the final rating after the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Non."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5179/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5179/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5179/Reviewer_GZCq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698068444408,
            "cdate": 1698068444408,
            "tmdate": 1700516652610,
            "mdate": 1700516652610,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fNXuhO0MpG",
                "forum": "wZWTHU7AsQ",
                "replyto": "K0CSRP4WA9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GZCq's concern about problem representation and contributions"
                    },
                    "comment": {
                        "value": "We thank Reviewer GZCq for the valuable feedback on our paper. We have provided detailed responses to the concerns raised by the reviewer and updated our paper, incorporating clarifications and discussions where necessary.\n\n> 1. This paper does not have a clear mathematical representation of the problem it intends to address.\n\nWe appreciate the reviewer's feedback regarding the clarity of our problem formulation. In response to this concern, we have enhanced the paper by providing a more explicit and detailed formulation of our problem in Section 2 based on the zero-sum game. We thank the reviewer for highlighting the mathematical problem formulation.\n\n> 2. The article claims its primary contribution lies in using zero-sum games to formulate the robust RL problem. However, employing zero-sum games to account for uncertainties, whether in single-agent or multi-agent RL, is well-established. Given this widespread application, the paper's stated novelty becomes questionable.\n\nWhile we acknowledge the precedent exploration of zero-sum games in robust RL within existing works, it is crucial to emphasize the distinctive contribution of our method, GRAD.\n\n* **Converge to approximate equilibriums on an adversary population.**\n\nDuring training, our approach achieves an approximate equilibrium on an adversary policy set, which is totally different from other robust RL papers which use the zero-sum game formulation but only learn with an adversary during alternating training. GRAD has the capability to continuously explore and learn new policies as best responses that are not present in the current policy set, which allows for a more thorough exploration of the policy space.\n\n* **Adaptable to different adversaries.**\n\nSignificantly, prior works, such as [1, 2, 3], primarily focus on a specific adversary or only consider the worst-case scenario, often adopting a highly pessimistic outlook that limits their adaptability to adversaries with different constraints. However, GRAD does not specifically target certain adversaries during training. Instead, at each epoch, it learns the best response to the adversary policy sampled from the adversary population, **making it adaptable to different adversarial scenarios, including both temporally-coupled and non-temporally-coupled adversaries**.\n\nFurthermore, previous approaches exclusively consider specific attack domains for adversaries, such as perturbations on environmental dynamics [1, 2] and action perturbations [3]. In contrast, GRAD represents a pioneering effort by introducing the first robust RL framework that accommodates various adversaries including action, state, and transition adversaries, **without any constraints on attack domains**. This broader scope positions GRAD as a more practical and scalable general solution for robust RL.\n* **Solving adversarial RL rather than uncertainty-aware RL.**\n\nOur method is proposed to defend against adversarial perturbations and improve adversarial robustness, while many papers such as [4] only focus on uncertainty-aware RL. Though GRAD exhibits strong performance even under model uncertainty, as shown in our experiments. Adversarial RL where the learning process is intentionally disrupted by an adaptive adversary (as a maxmin problem) is more challenging than uncertainty-aware RL (a max problem under uncertainty). \n\nTo provide further clarity on this aspect, we have included a detailed illustration of robust RL formulated as a zero-sum game in **Appendix B (Additional Related Work)** and improved our writing in sections of introduction and methodology. We believe these additions effectively address your concerns.\n\n[1] Robust Adversarial Reinforcement Learning, Pinto et al. 2017.\n\n[2] Robust Reinforcement Learning as a Stackelberg Game via Adaptively-Regularized Adversarial Training, HUang et al. 2021.\n\n[3] Action Robust Reinforcement Learning and Applications in Continuous Control, Tessler et al. 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700036520716,
                "cdate": 1700036520716,
                "tmdate": 1700036929447,
                "mdate": 1700036929447,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iwHAJvdLQp",
                "forum": "wZWTHU7AsQ",
                "replyto": "aOCy9LkPfm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_GZCq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_GZCq"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the response and all the efforts to resolve my concerns. I have raised my score since the paper writing has been improved."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516982156,
                "cdate": 1700516982156,
                "tmdate": 1700516982156,
                "mdate": 1700516982156,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b0CgLihQsG",
                "forum": "wZWTHU7AsQ",
                "replyto": "K0CSRP4WA9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GZCq"
                    },
                    "comment": {
                        "value": "Dear Reviewer GZCq,\n\nThank you so much for your prompt response and for improving our scores. We sincerely appreciate the valuable insights you provided for our paper, which have significantly contributed to enhancing its quality. If there are any further questions or concerns, we would be more than happy to address them."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540552836,
                "cdate": 1700540552836,
                "tmdate": 1700540569871,
                "mdate": 1700540569871,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jGRwI2L0qv",
            "forum": "wZWTHU7AsQ",
            "replyto": "wZWTHU7AsQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5179/Reviewer_JmX3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5179/Reviewer_JmX3"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a novel-framework of temporally-coupled robust RL problem that is closer to the real-world setting. This work proposes GRAD, a game-theoretic approach to provide robust policies played against an adversary which attacks states and actions fitting in the temporally-coupled robust RL problem setting. This work also gives extensive complementing experiment results."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think this work really pushes the robust RL community research efforts further by answering:\n\n> can we design robust RL algorithms for realistic nature attacks?\n\nThe main contribution of game-theoretic algorithm with temporal-based nature attacks (robust RL problem) is a really nice idea worthy for publication. But the score reflects my weakness section."
                },
                "weaknesses": {
                    "value": "I have only a few weakness for this work as follows:\n\n> The current framework considers robustness against state and action uncertainty. More closer work [1] and thereafter are not included. Model uncertainty is justified in the framework mentioning the evolution of the environment depends on the perturbed actions. Model uncertainty in robust RL is defined in more generality [2-10]. So it will be better to include more detailed Related Works including [2-10] and more relevant works in the revision. I agree this work includes experiments with model uncertainty, but the baselines are also only action robust algorithms. I'd rather see more extensive writing and experiments for model-uncertainty OR the current work just focusing on state-action uncertainty is a big step forwards in itself. I've also stopped at '10' since you get the idea of inadequate related work discussion.\n\n> GRAD shares similar idea of RARL algorithm (Pinto et al., 2017), that is, zero-sum structure to get the robust policy against the nature adversary. More details than below must be added to point out key differences (like state-action uncertainty inbuilt) and due references need to be given.\n` Pinto et al. (Pinto et al., 2017) model the competition between the agent and the attacker as a zero-sum two-player game, and train the agent under a learned attacker to tolerate both environment shifts and adversarial disturbances `\n\nI am open to discussions with the authors and reviewers to increase/maintain (already reflects the positive impact) my score. All the best for future decisions!\n\n[1] Robust Multi-Agent Reinforcement Learning with State Uncertainty Sihong He, Songyang Han, Sanbao Su, Shuo Han, Shaofeng Zou, and Fei Miao. Transactions on Machine Learning Research, June 2023.\n\n[2] Xu. Z, Panaganti. K, Kalathil. D, Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning. Artificial Intelligence and Statistics, 2023.\n\n[3] Nilim, A. and El Ghaoui, L. (2005). Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780\u2013798\n\n[4] Iyengar, G. N. (2005). Robust dynamic programming. Mathematics of Operations Research, 30(2):257\u2013280.\n\n[5] Panaganti, K. and Kalathil, D. (2021). Robust reinforcement learning using least squares policy iteration with provable performance guarantees. In Proceedings of the 38th International Conference on Machine Learning, pages 511\u2013520.\n\n[6] Panaganti, K. and Kalathil, D. (2022). Sample complexity of robust reinforcement learning with a generative model. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, pages 9582\u20139602.\n\n[7] Roy, A., Xu, H., and Pokutta, S. (2017). Reinforcement learning under model mismatch. In Advances in Neural Information Processing Systems, pages 3043\u20133052.\n\n[8] Panaganti, K., Xu, Z., Kalathil, D., and Ghavamzadeh, M. (2022). Robust reinforcement learning using offline data. Advances in Neural Information Processing Systems (NeurIPS).\n\n[9] Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. arXiv preprint arXiv:2208.05767\n\n[10] L Shi, G Li, Y Wei, Y Chen, M Geist, Y Chi  (2023) The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model, NeurIPS 2023"
                },
                "questions": {
                    "value": "-n/a-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698448884350,
            "cdate": 1698448884350,
            "tmdate": 1699636513677,
            "mdate": 1699636513677,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PZ0z6LAHdM",
                "forum": "wZWTHU7AsQ",
                "replyto": "jGRwI2L0qv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JmX3's concern about related discussions for model uncertainty"
                    },
                    "comment": {
                        "value": "Thanks Reviewer JmX3 for acknowledging and encouraging our paper's contributions. We provide a detailed response to your insightful feedback and make detailed revisions to our paper accordingly.\n\n> 1. The current framework considers robustness against state and action uncertainty. It will be better to include more detailed Related Works including and more relevant works in the revision.\n\nWe appreciate your valuable suggestion to provide a more detailed discussion on related works concerning robustness against uncertainty. Following your advice, we have expanded the discussion in **Appendix B to include the mentioned papers**, aiming to make our literature survey more comprehensive. \n\nRegarding the experiments on state robustness, we present results under the most robust state adversary. Other state attack methods or state uncertainty sets are less effective in attacking both robust baselines and our approach. Furthermore, we need to clarify that our method is proposed to **defend against adversarial perturbations and improve adversarial robustness**, which differs from many papers that focus on uncertainty-aware RL. Adversarial RL where the learning process is intentionally disrupted by an adaptive adversary (as a max-min problem) is more challenging than uncertainty-aware RL (a max problem under uncertainty)many papers focus on uncertainty-aware RL. Remarkably, GRAD exhibits strong performance even under model uncertainty, as shown in our experiments. \n\nWe have included **an additional set of experiments and updated results in Appendix C**. These experiments consider model mismatch or transition uncertainty following [1], across three perturbed MuJoCo environments with changes in different physical parameters. The observed performance disparity underscores the effectiveness of GRAD and its robustness against model uncertainty.\n\n|  Perturbed Environments  |                    | RNAC-PPO(DS) | RNAC-PPO(IPM) | **GRAD** |\n|----------------|----------------|------------|------------|-----------------|\n| Hopper | natural reward | **3502 \u00b1 256**       | 3254 \u00b1 138       |  3482 \u00b1 209   | \n|                                   | 'leg_joint_stiffness' value: 30   | 2359 \u00b1 182       | 2289  \u00b1 124       |  **2692  \u00b1 236**     | \n| Walker | natural reward | 4322 \u00b1 289        | 4248 \u00b1 89       |  **4359 \u00b1 141**   | \n|                                   | 'foot_joint_stiffness' value: 30    | 4078 \u00b1 297       |  4129 \u00b1 78       |   **4204 \u00b1 132**     | \n| Halfcheetah | natural reward | 5524 \u00b1 178        | 5569 \u00b1 232      |  **6047 \u00b1 241**   | \n|                                   | Bound on 'back_actuator_range': 0.5   | 768 \u00b1 102       |  1143 \u00b1 45       |   **1369 \u00b1 117**     | \n\n[1] Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation, Zhou et al. 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700032897450,
                "cdate": 1700032897450,
                "tmdate": 1700033703366,
                "mdate": 1700033703366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Wf7zNSrASg",
                "forum": "wZWTHU7AsQ",
                "replyto": "jGRwI2L0qv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JmX3's concern about novelty of zero-sum game formulation"
                    },
                    "comment": {
                        "value": "> 2. GRAD shares similar idea of RARL algorithm (Pinto et al., 2017), that is, zero-sum structure to get the robust policy against the nature adversary. \n\nThis is indeed a valuable feedback. In Appendix B, we have extensively discussed other robust RL works that utilize zero-sum games and highlighted the distinctions between GRAD and these approaches. \n\n* **Converge to approximate equilibriums on an adversary population**\n\nDuring training, our approach achieves an approximate equilibrium on an adversary policy set, which is totally different from other robust RL papers which use the zero-sum game formulation but only learn with an adversary during alternating training. GRAD has the capability to continuously explore and learn new policies as best responses that are not present in the current policy set, which allows for a more thorough exploration of the policy space.\n\n* **Adaptable to different adversaries**\n\nSignificantly, prior works, such as [2, 3, 4], primarily focus on a specific adversary or only consider the worst-case scenario, often adopting a highly pessimistic outlook that limits their adaptability to adversaries with different constraints. However, GRAD does not specifically target certain adversaries during training. Instead, at each epoch, it learns the best response to the adversary policy sampled from the adversary population, **making it adaptable to different adversarial scenarios, including both temporally-coupled and non-temporally-coupled adversaries**.\n\nFurthermore, previous approaches exclusively consider specific attack domains for adversaries, such as perturbations on environmental dynamics [2, 3] and action perturbations [4]. In contrast, GRAD represents a pioneering effort by introducing the first robust RL framework that accommodates various adversaries including action, state and transition adversaries, **without any constraints on attack domains**. This broader scope positions GRAD as a more practical and scalable general solution for robust RL.\n\n\nIn summary, we appreciate the reviewer's identification of weaknesses in our related works discussion and our novelty in problem formulation, while also acknowledging our contributions. We look forward to further discussions.\n\n\n[2] Robust Adversarial Reinforcement Learning, Pinto et al. 2017. \n\n[3] Robust Reinforcement Learning as a Stackelberg Game via Adaptively-Regularized Adversarial Training, HUang et al. 2021.\n\n[4] Action Robust Reinforcement Learning and Applications in Continuous Control, Tessler et al. 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033363337,
                "cdate": 1700033363337,
                "tmdate": 1700036763084,
                "mdate": 1700036763084,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oa2wr0Is7I",
                "forum": "wZWTHU7AsQ",
                "replyto": "jGRwI2L0qv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_JmX3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_JmX3"
                ],
                "content": {
                    "title": {
                        "value": "Ack"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. I do think the authors have improved the manuscript compared to the pre-rebuttal stage and also seem to have addressed the major concerns of all reviewers. I will update my score after authors-reviewers discussions."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547214356,
                "cdate": 1700547214356,
                "tmdate": 1700547625465,
                "mdate": 1700547625465,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5rDurpcmTn",
            "forum": "wZWTHU7AsQ",
            "replyto": "wZWTHU7AsQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
            ],
            "content": {
                "summary": {
                    "value": "The classic robust RL focuses on worst-case scenarios, which may result in an overly conservative policy. Instead, this paper introduces temporally-coupled perturbations. Additionally, this paper proposed an adversarial training approach named the game-theoretic response approach for adversarial defense. Finally, the authors show the robust performance of the proposed methods in several MuJoCo tasks compared with several baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper is easy to follow.\n* This paper does thorough experiments for state attacks and action attacks and compares the proposed method with several baselines. \n* The temporally-coupled adversarial perturbation seems new."
                },
                "weaknesses": {
                    "value": "* Although the temporally-coupled adversarial perturbation seems new, it is quite limited. Definition 3.2 only considers the temporally-coupled perturbation from the last time step. Even if the authors don't consider the general partially observable MDP, they should consider a more general case, e.g., m-order MDP [Efroni et al. 2022, Provable Reinforcement Learning with a short-term memory].\n* The zero-sum game-based approach is not new for robust training in RL, e.g., [Tessler et al., 2019].\n* This paper misses one classic setting of robust MDP (i.e., transition adversaries) [e.g., Iyengar'05, Robust Dynamic Programming] as well as related baselines [e.g., Zhou et al. 2023, Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation].\n* Figure 1 doesn't make sense to me. The robust baseline considers the worst-case scenario, which should be more stable for different kinds of attacks compared with the less conservative model that is proposed in this work."
                },
                "questions": {
                    "value": "Please refer to the \"weakness\" section for further information."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5179/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop",
                        "ICLR.cc/2024/Conference/Submission5179/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5179/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698644320239,
            "cdate": 1698644320239,
            "tmdate": 1700435595691,
            "mdate": 1700435595691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bmSilr22hq",
                "forum": "wZWTHU7AsQ",
                "replyto": "5rDurpcmTn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mjop's concern about attack settings and novelty of zero-sum game formulation"
                    },
                    "comment": {
                        "value": "Thanks Reviewer mjop for the insightful review of our paper. We acknowledge the concern the reviewer raised. We have updated our paper and provided a comprehensive response to address the issues highlighted.\n\n> 1. Limitation of our temporally-coupled settings. Definition 3.2 only considers the temporally-coupled perturbation from the last time step. Even if the authors don't consider the general partially observable MDP, they should consider a more general case, e.g., m-order MDP [Efroni et al. 2022, Provable Reinforcement Learning with a short-term memory].\n\nAccording to the reviewer's suggestion, we have expanded our experiments to include a more general scenario and introduce a memorized temporally-coupled attack. We calculate the mean of perturbations from the 10 past steps and apply the temporally-coupled constraint on this past perturbation mean. We add this experiment in **Appendix C.2** to showcase the effectiveness of GRAD under this extended adversarial setting. The added results indicate that **GRAD exhibits enhanced robustness compared to other robust baselines under the memorized temporally-coupled attack**. We believe this addition contributes valuable insights to the paper and addresses your concerns regarding the temporal scope of perturbations.\n\nMoreover, the primary reason we focused on perturbations from the last time step is grounded in the common practice of state adversaries, which typically perturb the current state without explicitly attacking short-term memory, but we agree that considering the short-term memory is a reasonable and considerable setting for adversaries. We eagerly anticipate any additional discussions or suggestions the review may have. \n\n| Memorized temporally-coupled Attacks| Hopper | Walker2d | Halfcheetah | Ant | Humanoid |\n| -------- | -------- | -------- |-------- | -------- |-------- |\n| PA-ATLA-PPO     | 2334 \u00b1 249     | 2137 \u00b1 258    | 3669 \u00b1 312    | 2689 \u00b1 189     | 1573 \u00b1 232\n| WocaR-PPO     | 2256 \u00b1 332     | 2619 \u00b1 198    | 4228 \u00b1 283     | 3229 \u00b1 178     | 2017 \u00b1 213\n| **GRAD** |  **2869 \u00b1 228**     | **3134 \u00b1 251**    | **4439 \u00b1 287**    | **3617 \u00b1 188** | **2736 \u00b1 269** \n\n\n> 2. The zero-sum game-based approach is not new for robust training in RL, e.g., [Tessler et al., 2019].\n\nWhile we acknowledge the precedent exploration of zero-sum games in robust RL within the existing literature, it is crucial to emphasize the distinctive contribution of our method, GRAD.\n\n* **Converge to approximate equilibriums on an adversary population.**\n\nDuring training, our approach achieves an approximate equilibrium on an adversary policy set, which is totally different from other robust RL papers which use the zero-sum game formulation but only learn with an adversary during alternating training. GRAD has the capability to continuously explore and learn new policies as best responses that are not present in the current policy set, which allows for a more thorough exploration of the policy space.\n\n* **Adaptable to different adversaries.**\n\nSignificantly, prior works, such as [1, 2, 3], primarily focus on a specific adversary or only consider the worst-case scenario, often adopting a highly pessimistic outlook that limits their adaptability to adversaries with different constraints. However, GRAD does not specifically target certain adversaries during training. Instead, at each epoch, it learns the best response to the adversary policy sampled from the adversary population, **making it adaptable to different adversarial scenarios, including both temporally-coupled and non-temporally-coupled adversaries**.\n\nFurthermore, previous approaches exclusively consider specific attack domains for adversaries, such as perturbations on environmental dynamics [1, 2] and action perturbations [3]. In contrast, GRAD represents a pioneering effort by introducing the first robust RL framework that accommodates various adversaries including action, state, and transition adversaries, **without any constraints on attack domains**. This broader scope positions GRAD as a more practical and scalable general solution for robust RL.\n\nTo provide further clarity on this aspect, we have included a detailed illustration of robust RL formulated as a zero-sum game in **Appendix B (Additional Related Work)** and improved our writing in sections of introduction and methodology. We believe these additions effectively address your concerns.\n\n[1] Robust Adversarial Reinforcement Learning, Pinto et al. 2017.\n\n[2] Robust Reinforcement Learning as a Stackelberg Game via Adaptively-Regularized Adversarial Training, HUang et al. 2021.\n\n[3] Action Robust Reinforcement Learning and Applications in Continuous Control, Tessler et al. 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034046795,
                "cdate": 1700034046795,
                "tmdate": 1700036816366,
                "mdate": 1700036816366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m2QBZVnPAP",
                "forum": "wZWTHU7AsQ",
                "replyto": "5rDurpcmTn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mjop's concern about experiments for transition adversaries and Figure 1's illustration"
                    },
                    "comment": {
                        "value": "> 3. This paper misses one classic setting of robust MDP (i.e., transition adversaries) [e.g., Iyengar'05, Robust Dynamic Programming] as well as related baselines [e.g., Zhou et al. 2023, Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation].\n\nWe deeply appreciate the insightful suggestion regarding the incorporation of experiments involving transition adversaries. To attempt to study this great point, we conducted a meticulous comparative study pitting GRAD against the baseline RNAC-PPO trained with DS and IPM uncertainty sets across three perturbed MuJoCo environments with changed physic parameters, following the experimental protocols outlined in [4]. \n\nThe results unequivocally demonstrate that **GRAD outperforms RNAC-PPO across all tasks under transition uncertainty**. This performance disparity underscores the efficacy of GRAD and its adaptability across diverse attack domains. We believe these results findings make substantial empirical contributions to the paper, effectively addressing the concern raised by the reviewer. We also add this experiment in **Appendix C.2**.\n\n|  Perturbed Environments  |                    | RNAC-PPO(DS) | RNAC-PPO(IPM) | **GRAD** |\n|----------------|----------------|------------|------------|-----------------|\n| Hopper | natural reward | **3502 \u00b1 256**       | 3254 \u00b1 138       |  3482 \u00b1 209   | \n|                                   | 'leg_joint_stiffness' value: 30   | 2359 \u00b1 182       | 2289  \u00b1 124       |  **2692  \u00b1 236**     | \n| Walker | natural reward | 4322 \u00b1 289        | 4248 \u00b1 89       |  **4359 \u00b1 141**   | \n|                                   | 'foot_joint_stiffness' value: 30    | 4078 \u00b1 297       |  4129 \u00b1 78       |   **4204 \u00b1 132**     | \n| Halfcheetah | natural reward | 5524 \u00b1 178        | 5569 \u00b1 232      |  **6047 \u00b1 241**   | \n|                                   | Bound on 'back_actuator_range': 0.5   | 768 \u00b1 102       |  1143 \u00b1 45       |   **1369 \u00b1 117**     | \n\n> 4. Figure 1 doesn't make sense. The robust baseline considers the worst-case scenario, which should be more stable for different kinds of attacks compared with the less conservative model that is proposed in this work.\n\nWe have revised the caption of Figure 1 to explain why the baseline is not robust to the temporally-coupled adversaries, \n\nWhile it is true that WocaR-PPO aims to enhance its robustness in the worst-case scenarios or improve its lower bound of performance, **it does not guarantee superiority under other case scenarios, such as temporally-coupled perturbations**. The baseline's focus on worst case estimations and improvement does not inherently ensure robustness across all conceivable situations. It is noteworthy that WocaR-PPO's relatively higher risk of stumbling under temporally-coupled attacks, as demonstrated in our experiments. This is precisely where GRAD excels. GRAD finds the equilibrium with an adversary policy set, showcasing a significant advantage in diverse adversarial settings. We believe these aspects underscore the substantial advantages of GRAD and welcome further discussion on this matter.\n\nIn summary, we have responded to the reviewer's concern by providing additional experiments in temporally-coupled settings and diverse attack domains, illustrating GRAD's robustness. We also discuss our noverlty different from related works to better elucidate GRAD's strengths. Regarding Figure 1, we explained the observations and modified Figure 1's illustration in our paper. We appreciate the valuable feedback, which enhances the quality of our paper, and we look forward to further discussion and additional suggestions.\n\n[4] Natural Actor-Critic for Robust Reinforcement Learning with Function Approximation, Zhou et al. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700035102417,
                "cdate": 1700035102417,
                "tmdate": 1700035365535,
                "mdate": 1700035365535,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rc2Dsv7uGf",
                "forum": "wZWTHU7AsQ",
                "replyto": "5rDurpcmTn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
                ],
                "content": {
                    "comment": {
                        "value": "Based on the author's efforts to solve some concerns, I have increased the score. Additionally, I don't completely agree with the authors' claim that \"game-theoretic RL can be a general and scalable solution for the robust RL objective\". Not only does the game-theoretic method bring instability, but it also makes training take much longer compared with the standard non-robust approach. Instead, there exist some computationally efficient methods to guarantee robustness theoretically and empirically (e.g., RNAC [4]). In other words, I am wondering if it is necessary to make slight improvements for robustness at the expense of significant training time."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700435604800,
                "cdate": 1700435604800,
                "tmdate": 1700435708165,
                "mdate": 1700435708165,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nmStVL4rjW",
                "forum": "wZWTHU7AsQ",
                "replyto": "5rDurpcmTn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mjop"
                    },
                    "comment": {
                        "value": "Thank you very much for your further response and raising a discussion-worthy question. We would like to clarify the following points. \n1. Prior methods, such as RNAC, provide theoretical and empirical guarantees for robustness, focusing on uncertainty-aware RL. In contrast, our method mainly aims to enhance robustness specifically for adversarial RL, while our experiments also demonstrate GRAD's significant robustness against model uncertainty. Adversarial RL involves a max-min problem against a learnable and adaptive adversary, posing a greater challenge than uncertainty-aware RL, which is a max problem under uncertainty. Existing methods for adversarial RL often struggle to provide both theoretical guarantees and adaptability to different adversaries, often at a higher computational cost, such as alternating training with learned adversaries. The potential superiority of game-theoretic methods in adversarial RL arises from the fact that adversaries can adjust/learn their attack strategies based on the agent's policy. Hence, it becomes necessary to find the approximate equilibrium solutions to discover more robust strategies against adversarial perturbations\n\n2. We understand your concern, and if our method were only effective with minor improvements in robustness against a limited set of uncertainties or perturbations, the extensive training time would indeed be unjustifiable. Therefore, we conducted experiments to demonstrate the broad practicality of our method against various adversarial perturbations with different constraints or in different attack domains. We are the first to defend against such a diverse range of perturbations, contributing to adversarial robustness. \n\n3. Additionally, addressing computational efficiency, we propose in the discussion and limitations section that future directions could explore more efficient alternatives to PSRO. Our primary contribution lies in showcasing the efficacy of game-theoretic methods in robust RL, particularly against adversarial perturbations, providing a comprehensive and effective solution. \n\nWe hope our response further addresses your concerns and helps you understand the contribution of our work to the robust RL community. We welcome further discussion, and we will appropriately supplement our discussion in the Appendix."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464127217,
                "cdate": 1700464127217,
                "tmdate": 1700464613064,
                "mdate": 1700464613064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "edb4BJFr6E",
                "forum": "wZWTHU7AsQ",
                "replyto": "nmStVL4rjW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. I don't completely agree with the authors' claim. Robust MDP belongs to distributional robust optimization (DRO) instead of uncertainty-aware RL. Both adversarial training and DRO involve a max-min problem, and they are two different approaches to solving robust RL problems. Additionally, action perturbations can be regarded as a special case of DRO (e.g., [Tessler et al. 2019]). But anyway, due to the author's efforts to solve concerns, I have increased the score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700541622004,
                "cdate": 1700541622004,
                "tmdate": 1700541622004,
                "mdate": 1700541622004,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HdlkXcI8pi",
                "forum": "wZWTHU7AsQ",
                "replyto": "5rDurpcmTn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mjop"
                    },
                    "comment": {
                        "value": "Dear Reviewer mjop,\n\nThank you once again for your response. Perhaps our previous clarification led to some misunderstandings. We want to emphasize that our main goal is to address the adversarial RL problem with an adversary that is adaptive to the agent's policy. Papers like Tessler et al. (2019) on action perturbations and Zhou et al. (2023) on model mismatch uncertainty, are hard to defend against the strongest adversarial perturbations and only empirically evaluated on uncertainty sets. We include Tessler et al. (2019) as an action-robust baseline in our paper, which is not robust under RL-based action adversaries.  This vulnerability arises due to the inherent difficulty in estimating the long-term worst-case value under adaptive adversaries. Distributional robust optimization (DRO) is also challenging to apply to this challenging problem, especially against state adversaries in the high-dimensional state space. At the same time, adversarial training methods also struggle to effectively deal with model uncertainty or model mismatch problems.\n\nExisting adversarial training methods, such as alternating training, often require simultaneous training of an RL-based adversary, leading to increased computational costs. This is worthwhile in many practical settings when we have access to a high-fidelity simulator. Our approach, leveraging game-theoretic methods, demonstrates robustness against adversarial perturbations and model uncertainty, as a more effective and general solution for high-dimensional tasks. We hope this clarifies our contributions to the robust RL community. We further elaborate on the distinctions from DRO in our Appendix. \n\nWhile we fully understand your concerns, it remains challenging for existing methods to effectively address adversarial perturbations, especially when considering different attack constraints and simultaneous perturbations to both state and action spaces. GRAD, however, is applicable to different adversary settings, not limited to specific uncertainty sets or adversaries confined to certain constraints. This adaptability is a key strength that sets GRAD apart from other robust RL methods.\n\nWe greatly appreciate your valuable suggestions and hope for more discussions and considerations of your score before the end of the discussion period."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543305927,
                "cdate": 1700543305927,
                "tmdate": 1700567575212,
                "mdate": 1700567575212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qd1vzVKm61",
                "forum": "wZWTHU7AsQ",
                "replyto": "5rDurpcmTn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mjop"
                    },
                    "comment": {
                        "value": "To provide a clearer and more detailed explanation with further discussion:\n\n1. Regarding Tessler et al., although their formulation considers the worst-case adversary by framing robust RL as a max-min problem, their algorithm updates the adversary using only one-step gradient descent without the guarantee of adversary convergence and makes it hard to estimate the worst-case value, contributing to their lack of robustness. In contrast, our algorithm, by finding the approximate equilibrium with the adversary set, achieves stronger robustness and is more adaptable for different adversaries.\n\n2. We acknowledge the significance of prior works and understand that your question mainly revolves around the trade-off between robustness and computational cost. Our insights center on finding the approximate equilibrium to discover optimal robust models with higher levels of model exploitation. In many practical scenarios like self-driving or robotics, where robust models are crucial, robustness under various perturbations holds more significance than computational efficiency. This is where game-theoretic methods become valuable. In game-theoretic methods, it can also balance robustness and computational efficiency by controlling the level of exploitability.\n\nWe appreciate your continued engagement, and we believe this discussion has further clarified the value of our work. Looking forward to your response!"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574982279,
                "cdate": 1700574982279,
                "tmdate": 1700575036312,
                "mdate": 1700575036312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VEV9rvQ19b",
                "forum": "wZWTHU7AsQ",
                "replyto": "qd1vzVKm61",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Reviewer_mjop"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your further clarification. I will consider increasing the score further during the reviewer-AC discussion."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588069923,
                "cdate": 1700588069923,
                "tmdate": 1700588069923,
                "mdate": 1700588069923,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UgX0JLhvvX",
                "forum": "wZWTHU7AsQ",
                "replyto": "5rDurpcmTn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5179/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mjop"
                    },
                    "comment": {
                        "value": "Thank you for your continued engagement and thoughtful consideration of our paper. Additional clarifications and experiments significantly contribute to the improvement of our work. Your willingness to reconsider the score for our paper is highly valued, and any further feedback you may have is welcomed.\n\nAdditionally, we will express our gratitude in the comments provided to the AC for your diligent review and discussion. We truly appreciate the time, effort, and professional insights you've invested in our discussions, which are crucial for enhancing the quality of our work."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5179/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621669487,
                "cdate": 1700621669487,
                "tmdate": 1700632145449,
                "mdate": 1700632145449,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]