[
    {
        "title": "Graph Representation Learning enhanced Semi-supervised Feature Selection"
    },
    {
        "review": {
            "id": "3AsQr2gw02",
            "forum": "1MRfyGLCcU",
            "replyto": "1MRfyGLCcU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4844/Reviewer_s9w1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4844/Reviewer_s9w1"
            ],
            "content": {
                "summary": {
                    "value": "This article focuses on semi-supervised feature selection. The authors argued that the existing methods can hardly exploit the  relations among samples, so they proposed a  graph representation learning approach named G-FS. G-FS could capture the non-Euclidean relations among features and samples with a a bipartite graph strategy. G-FS achieves significant performance edges in 12 datasets compared with baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well organized.\n2. Feature selection is important for many areas, the motivation is sound.\n3. The experiments are comprehensive."
                },
                "weaknesses": {
                    "value": "1. The proposed model will introduce more parameters by using GNN. The efficiency should be investigated.\n2. Compared with XGB, the improvement brought by G-FS is not that significant. It should be discussed.\n3. The visualization of tSNE has certain degrees of randomness, it is not sure the result in Figure 3 is convincing since the advantage is small."
                },
                "questions": {
                    "value": "1. The proposed model will introduce more parameters by using GNN. The efficiency should be investigated.\n2. Compared with XGB, the improvement brought by G-FS is not that significant. It should be discussed.\n3. The visualization of tSNE has certain degrees of randomness, it is not sure the result in Figure 3 is convincing since the advantage is small."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4844/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4844/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4844/Reviewer_s9w1"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4844/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697524944452,
            "cdate": 1697524944452,
            "tmdate": 1700618974922,
            "mdate": 1700618974922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q1mvKpHpf9",
                "forum": "1MRfyGLCcU",
                "replyto": "3AsQr2gw02",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4844/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer s9w1"
                    },
                    "comment": {
                        "value": "Thank you for all your suggestions, we will answer your questions one by one regarding these weaknesses/problems.\n\n**Weakness 1:** The proposed model will introduce more parameters by using GNN. The efficiency should be investigated.\n\n**Answer to Weakness 1:** Thanks for your suggestions. Using GNN to study the relations indeed introduced certain complexity. We also provided the complexity analysis of the GNN's parameters in the original version of Appendix E. Scalability and robustness analysis. Table 7 and Table 8 shows the feature selection time and self-supervision time. Compared to autoencoder-based self-supervision, GFS demands about 4,5 times of times for training in MNIST which is already a large dataset. Considering the benefits of GNN-based self-supervision and this self-supervision only needs to be performed once, we think this overhead is tolerable.   \n\n**Weakness 2:** Compared with XGB, the improvement brought by G-FS is not that significant. It should be discussed.\n\n**Answer to Weakness 2:** Thanks for your suggestions.\nFor typical supervised classification and regression tasks, G-FS achieves performance improvements. For classification tasks, we achieved the best performance on different types of datasets with an average of 20.32% than all baselines and led the second strong baseline 1.13%\\~19.50%. We guess your concerns are about the results of the regression tasks. The regression task is relatively simple and most FS baselines achieve similar results. However, G-FS still achieves good results, especially for MBGM and Pdgfr (two difficult datasets with dim>300), G-FS leads 2.93%\\~9.92%. For CPU, Protein, and Concrete, they are comparably simple. G-FS still leads 0.82%\\~4.22% in MAE than the second best XGBoost. XGBoost is a very strong baseline in the regression task [1]. Many recent FS do not compare the regression datasets with XGBoost. If XGBoost is excluded, our performance gain can be further improved.\n\n[1] Shwartz-Ziv R, Armon A. Tabular data: Deep learning is not all you need[J]. Information Fusion, 2022, 81: 84-90.\n\n**Weakness 3:** The visualization of tSNE has certain degrees of randomness, it is not sure the result in Figure 3 is convincing since the advantage is small. \n\n**Answer to Weakness 3:** Thank you for your concern. T-SNE is used to demonstrate the effectiveness of G-FS. The advantage of G-FS(about 73%) is much better than the one with raw data(about 60%) without GNN-based self-supervision. We also plot the tSNE graph for the other three datasets in Appendix D. More experiments results with three additional datasets: optdigits, MNIST, and USPS. \n\nTo better evaluate the performance improvements, We transform the figure of the ablation studies with GNN-supervision (GFS) and without GNN-supervision into the results in Table 1 with relative performance. The results show that compared with the version without self-supervised graph representation learning, our G-FS has an improvement of 1.89%~17.06%(Average 6.52%). These improvements are significant which are verified by the tSNE results in Fig. 3 and Fig A.2(appendix) for different datasets. Furthermore, the performance is evaluated in a fully supervised setting with different classifiers. The improvement ratio is limited by the capability of the classifier and the problem's complexity. From Table 1 and Table A.2(Appendix), we shows that G-FS achieves good advantages on the two classifiers, LightGBM and Catboost. Therefore, we believe that the improvements are real and convincing.\n\nTable 1: Performance improvement (%) compared with G-FS-g.\n\n| Dataset | Improvement |\n| --- | --- |\n| MNIST(\u2191) | 3.72% |\n| Optdigits(\u2191) | 1.89% |\n| USPS(\u2191) | 2.07% |\n| Concrete(\u2193) | 12.30% |\n| Protein(\u2193) | 2.12% |\n| CPU_act(\u2193) | 17.06% |\n| **Average**| 6.52% |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699882316092,
                "cdate": 1699882316092,
                "tmdate": 1699886502875,
                "mdate": 1699886502875,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LW1XtqhqKz",
                "forum": "1MRfyGLCcU",
                "replyto": "q1mvKpHpf9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4844/Reviewer_s9w1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4844/Reviewer_s9w1"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for your response. I think my concerns haven't be fully addressed. Considering the efficiency and performance, the improvement of the proposed method is not significant. I will change my rating to 5: marginally below the acceptance threshold."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618951847,
                "cdate": 1700618951847,
                "tmdate": 1700618951847,
                "mdate": 1700618951847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VzLo0fiv7h",
            "forum": "1MRfyGLCcU",
            "replyto": "1MRfyGLCcU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4844/Reviewer_AraV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4844/Reviewer_AraV"
            ],
            "content": {
                "summary": {
                    "value": "This paper study a traditional problem feature selection. It first revisits several remaining issues limit the capability of existing self-supervision enhanced feature selection methods: Then, it proposes a novel method G-FS which performs feature selection based on the discovery and exploitation of the non-Euclidean relations among features and samples by translating unlabeled \u201cplain\u201d tabular data into a bipartite graph. Finally, it conducts some experiments to evaluate the proposed method, showing that the proposed method sometimes outperforms baselines on several tasks across multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1.\tIt tests on several widely-used datasets, and the proposed method can sometimes beat the existing methods."
                },
                "weaknesses": {
                    "value": "1.\tThe core part (the proposed method in Section 3) lacks of sufficient analysis. We know that it is not difficult to put different modules together to form a paper. But we should make sure that the motivation of doing that really makes sense and we should understand what we are doing.\n2.\tConfusion of symbol system. For example, the \u201conehot\u201d in Eq. 5.\n3.\tThe writing needs to be largely improved. The content in introduction is hard to follow. It is also hard to follow the content in Section 3."
                },
                "questions": {
                    "value": "1.\tSee the weakness in the \u201c*Weaknesses\u201d part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4844/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638539014,
            "cdate": 1698638539014,
            "tmdate": 1699636467899,
            "mdate": 1699636467899,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3uVY5cGEjR",
                "forum": "1MRfyGLCcU",
                "replyto": "VzLo0fiv7h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4844/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AraV"
                    },
                    "comment": {
                        "value": "Thank you for all your suggestions, we will answer your questions one by one regarding these weaknesses/problems.\n\n**Weakness 1&3:** The core part (the proposed method in Section 3) lacks of sufficient analysis. We know that putting different modules together to form a paper is not difficult. But we should make sure that the motivation of doing that really makes sense and we should understand what we are doing. The writing needs to be largely improved. The content in introduction is hard to follow. It is also hard to follow the content in Section 3. \n\n**Answer to Weakness 1&3:** Sorry for this concern. However, as agreed by Reviewer KtqB and n2md, the motivation of G-FS is to learn more types of relation in the tabular data to support feature selection. According to your suggestion, we have added a paragraph to better illustrate our motivation and reorder the Sect. 3 to make it consistent with Fig. 2. Detailed changes can be found in Reply to Reviewer n2md. \n1. Thanks for pointing out this, for the introduction, we have modified the Introduction of the paper to make the motivation of the paper more obvious and added more analysis of the proposed method.  \n2. For Section 3, the notation (Section 3.1) and architectural design (Section 3.2) are easy to understand, so we think you are hard to follow about bipartite graph representation learning (Section 3.3). We checked Section 3.3 again and found that the description in this section was inconsistent with the workflow in Figure 2, which made our work difficult to understand. Therefore, we adjusted the order of the chapters and subtitles to make the workflow consistent with Figure 2. \n\nIn the revised version: \n\n**3.3 Bipartite Graph Representation Learning**\n\n&nbsp; **3.3.1 Tabular data to bipartite graph**\n\n&nbsp; **3.3.2 GRL for the bipartite graph**\n\n**3.4 Batch-attention-based feature selection**\n\n**Weakness 2:** Confusion of symbol system. For example, the \u201conehot\u201d in Eq. 5.\n\n**Answer to Weakness 2:** Sorry to make you have this impression. Here, \"onehot\" is a common method for dealing with categorical data in machine learning, in which a unique binary value represents each distinct category. This is a very popular method. Thus, we use it without a formal definition to avoid page limits."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699882312342,
                "cdate": 1699882312342,
                "tmdate": 1699882312342,
                "mdate": 1699882312342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mUF6BV0RoM",
            "forum": "1MRfyGLCcU",
            "replyto": "1MRfyGLCcU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4844/Reviewer_KtqB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4844/Reviewer_KtqB"
            ],
            "content": {
                "summary": {
                    "value": "This article presents an innovative, self-supervised method for augmenting raw features through graph representations. This approach facilitates interaction between samples and features and employs a batch-attention mechanism for feature selection by assigning weights to each dimension of the augmented feature. More specifically, each feature dimension is represented as a unique node, and each sample is also depicted as a unique node with the raw feature of that sample serving as the node feature. Each edge signifies that a sample possesses the edge attribute value at that particular feature dimension. The goal of the self-supervised method is to predict edge attribute, which finally generated augmented features. Experimental results demonstrate that this novel method surpasses traditional benchmarks in most tasks and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.This paper presents a highly innovative and inspiring approach to representing tabular data and implementing self-supervised learning for augmentations.\n\n2.The proposed method primarily consists of two components: sample-sample interaction and sample-feature interaction.\n\n3.Empirical experiments have been meticulously conducted to compare performance with baseline models and to investigate the effectiveness of the two main components."
                },
                "weaknesses": {
                    "value": "1.Certain equations, like Eq (1) where the sum notation of j is missing, and Eq (4) which doesn't accurately reflect its preceding description, may lead to confusion or mistakes.\n\n2.In Table 1, the arrow for the Testator dataset should be pointing upwards.\n\n3.The paper fails to establish a strong connection between the proposed method and semi-supervised learning, as all referenced methods show a similar performance improvement trend with an increase in labeled data.\n\n4.The section titled \"Feature selection under different GNN architecture\" doesn't discuss the number of GNN layers, which could potentially lead to oversquashing issues. Furthermore, given that we have two types of nodes (sample and feature), it may not be equitable to compare this with a homophily-based GNN model such as GCN."
                },
                "questions": {
                    "value": "1.How will the different number of the layer of GNN influence the performance? Is there any possible oversquashing or heterophily related problem for your proposed graph building method?\n\n2.What is difference or significance of this method applied in semi-supervised learning and normal supervised learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4844/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659666513,
            "cdate": 1698659666513,
            "tmdate": 1699636467817,
            "mdate": 1699636467817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uaatZSV1Xq",
                "forum": "1MRfyGLCcU",
                "replyto": "mUF6BV0RoM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4844/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KtqB"
                    },
                    "comment": {
                        "value": "**Weakness 1:** Eq (1) where the sum notation of j is missing, and Eq (4) which doesn't accurately reflect its preceding description.\n\n**Answer to Weakness 1:**  For Eq (1), thanks for pointing out this mistake, we are sorry for this and we have corrected it in the revised version.  For Eq (4), we think it is correct. As the masked tabular data might contain both continuous and discrete values for imputation, we use CE loss for discrete attributes (\u03b1=1) and MSE loss for continuous attributes (\u03b1=0). Thus, we think this formula is consistent with the description.  \n\n**Weakness 2:** In Table 1, the arrow for the Testator dataset should be pointing upwards. \n\n**Answer to Weakness 2:** We guess that you are referring to the Tecator dataset, as we did not use the \"Testator\" dataset. Tecator is a dataset for regression. MAE (lower is better) is used for evaluation. So, the downwards arrow is right. \n\n**Weakness 3:** The paper fails to establish a strong connection between the proposed method and semi-supervised learning, as all referenced methods show a similar performance improvement trend with an increase in labeled data.\n\n**Answer to Weakness 3(Question 2):**\n1. First, our method belongs to semi-supervised feature selection in the feature selection part. The self-supervised part (without labels) is used to learn the rich latent information in the tabular data. Figure 1 in the Introduction section shows the motivation for our work. The supervision part is used to obtain feature importance scores. We added additional experiments on the Optdigits and USPS datasets, and the results show that G-FS achieves similar feature selection performance with 1/10 the number of labels. Therefore, we believe that G-FS belongs to semi-supervised feature selection, which reduces the label dependence of supervised feature selection through self-supervision.  \n2. However, for the evaluation parts, the quality of selected features is evaluated with fully-supervised settings. Here, LightGBM is adopted. Its performance is heavily dependent on the number of labels. Thus, the three compared methods show an increasing trend. However, with the same number of labels for feature selection, G-FS achieves the best performance at different levels of labeled numbers thanks to its rich relations discovery capabilities of graphs, as explained in Section 4.1.1, 'Why does G-FS work'.  \n\n**Weakness 4:** The section titled ... doesn't discuss the number of GNN layers? ...it may not be equitable to compare this with a homophily-based GNN model such as GCN. \n\n**Answer to Weakness 4(Question 1):** \n1. Thanks for your questions. We have added the discussion about feature selection performance under different GNN layers, please refer to Table 1. The results show that the performance reaches the optimal level on the 3-layer GNN. This is because the 3-layer contains all three types of relations (sample-feature-sample-feature ) , much more than the 1-layer can express (sample-feature) relationship. GNN with 4/5 layers is too complex and may cause the GNN over-smoothing problem.\n2. Then, you proposed an important question 'This comparison is not equitable with a homophily-based GNN model such as GCN'. Yes, this does seem unfair, but what we want to illustrate in this experiment is that our method pays more attention to the balance of integrity and consistency than GCN, which is crucial for feature selection. GCN is more inclined to data consistency, while GraphSAGE combines sampling and aggregation to learn richer representations in graphs. So we can see that the result of G2SAT is better.\n3. As far as we know, there are few self-supervised learning methods based on heterogeneous bipartite graphs. Here we compared with a recently heterophily-based GNN model IGRM[1], which is more considered to the diversity of nodes. The result in Table 2 shows that too much bias towards homogeneity and heterophily will lead to a decrease in the performance of feature selection. How to find a suitable balance between homogeneity and heterogeneity will be a very interesting research content.\n\nTable 1: Performance comparison under different GNN layers.\n\n| GNN_layer | Optdigits(\u2191) | USPS(\u2191) | Concrete(\u2193) | Tecator(\u2193) |\n| --- | --- | --- | --- | --- |\n| 1 | 73.46\u00b13.45 | 80.89\u00b12.74 | 6.18\u00b10.52 | 1.16\u00b10.19 |\n| 2 | 76.82\u00b13.01 | 81.65\u00b11.82 | 5.31\u00b10.40 | 1.08\u00b10.11 |\n| 3(Ours) | **79.08\u00b12.57** | **83.10\u00b11.46** | **4.96\u00b10.29** | **1.03\u00b10.17** |\n| 4 | 75.67\u00b12.72 | 81.20\u00b12.61 | 5.46\u00b11.26 | 1.06\u00b10.14 |\n| 5 | 69.90\u00b14.13 | 79.60\u00b12.47 | 7.34\u00b12.53 | 1.11\u00b10.13 |\n\nTable 2: Performance comparison under different GNN structures.\n\n| Structure | Optdigits(\u2191) | USPS(\u2191) | MBGM(\u2193) | Tecator(\u2193) |\n| --- | --- | --- | --- | --- |\n| G-FS(G2SAT) | **79.08\u00b12.57** | **83.10\u00b11.46** | **5.45\u00b10.11** | **1.03\u00b10.17** |\n| G-FS(EGCN) | 72.50\u00b13.56 | 81.14\u00b12.07 | 5.90\u00b10.35 | 1.16\u00b10.14 |\n| G-FS(IGRM) | 75.51\u00b12.02 | 80.71\u00b10.98 | 6.12\u00b10.44 | 1.08\u00b10.21 |\n\n[1] Data imputation with iterative graph reconstruction, AAAI 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699879921608,
                "cdate": 1699879921608,
                "tmdate": 1699879921608,
                "mdate": 1699879921608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bs5g6J4QRp",
            "forum": "1MRfyGLCcU",
            "replyto": "1MRfyGLCcU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4844/Reviewer_n2md"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4844/Reviewer_n2md"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of semi-supervised feature selection. The authors introduce a graph representation learning-enhanced semi-supervised feature selection framework called G-FS, which transforms unlabeled tabular data into a bipartite graph to discover relationships between features and samples. They design a self-supervised edge prediction task to convert rich information on the graph into low-dimensional embeddings, reducing redundant features and noise. Additionally, the authors propose a batch-attention feature weight generation mechanism, which can generate more robust weights."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of enhancing semi-supervised feature selection using graph representation learning is novel. \n- Mapping tabular data to a bipartite graph allows for the exploration of more potential relationships."
                },
                "weaknesses": {
                    "value": "- The writing of the paper can be further improved.\n- The improvements over existing methods are not significant."
                },
                "questions": {
                    "value": "- In Figure 1b, the authors depict the sample-label relationship. However, this relationship is not mentioned in the text. In the introduction, the authors mention three types of relationships, yet in Figure 1c, four types of relationships are referenced.\n- Minor errors: In Section 4.2, the reference to Table 1 is incorrectly written as Figure 1. (To verify the performance of G-FS, G-FS is compared with other feature selection methods on 12 different datasets (refer to Figure 1).)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4844/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4844/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4844/Reviewer_n2md"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4844/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763714270,
            "cdate": 1698763714270,
            "tmdate": 1699636467717,
            "mdate": 1699636467717,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6JcKzb01Sz",
                "forum": "1MRfyGLCcU",
                "replyto": "bs5g6J4QRp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4844/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer n2md"
                    },
                    "comment": {
                        "value": "Thank you for all your suggestions, we will answer your questions one by one regarding these weaknesses/problems. \n\n**Weakness 1:** The writing of the paper can be further improved.\n\n**Answer to Weakness 1:** Thanks for your pointing out this question. We have revised the Introduction and Methods Sections of the manuscript to make the purpose of the paper more evident and the proposed method easier to comprehend.\n\nSpecifically, **1)** we explicitly state our design motivation in a newly introduced subsection, 'motivation', with Figure 1 showing the four types of relations in the tabular data. Thus, we propose using graphs and GNN to represent and learn those relations, as autoencoder-based self-supervision can hardly express and learn those relations. **2)** we adjusted the order of the subsections of Sect. 3.3. By putting the translation process from the tabular data to the bipartite graph upfront, the order of Section 3.3 is consistent with Figure 2. We hope it is easier to understand.   \n\n**Weakness 2:** The improvements over existing methods are not significant.\n\n**Answer to Weakness 2:** Thanks for your questions, we will answer your questions in two parts, overall performance and performance under different scenarios.\n1. For typical supervised classification and regression tasks, G-FS achieves performance improvements.  For classification tasks, we achieved the best performance on different types of datasets with an average of 20.32% than all baselines and led the second strong baseline 1.13%\\~19.50%.  We guess your concerns are about the results of the regression tasks. The regression task is relatively simple and most FS baselines achieve similar results. However, G-FS still achieves good results, especially for MBGM and Pdgfr (two difficult datasets with dim>300), G-FS leads 2.93%\\~9.92%. For CPU, Protein, and Concrete, they are comparably simple. G-FS still leads 0.82%~4.22% in MAE than the second best XGBoost. XGBoost is a very strong baseline in the regression task [1]. Many recent FS do not compare the regression datasets with XGBoost. If XGBoost is excluded, our performance gain can be further improved.   \n2. For other scenarios, such as one-shot, noise disturbance, synthetic data, and limited labeled/unlabeled samples,  G-FS achieves more significant leads compared with other methods. Thus, we think G-FS can be applied to different scenarios and its performance improvements are consistent, albeit with different degrees in different problems. \n\n[1] Shwartz-Ziv R, Armon A. Tabular data: Deep learning is not all you need[J]. Information Fusion, 2022, 81: 84-90.\n\n**Question 1:** In Figure 1b, the authors depict the sample-label relationship. However, this relationship is not mentioned in the text. In the introduction, the authors mention three types of relationships, yet in Figure 1c, four types of relationships are referenced. \n\n**Answer to Question 1:** For the feature-label relations, we did mention this relation(maybe implicit) in the text in the introduction of supervised feature selection 'in discriminative information encoded in class labels or regression targets', we apologize for the ambiguity. We have updated our manuscript to mention it explicitly in the introduction.\n\n**Question 2:** Minor errors: In Section 4.2, the reference to Table 1 is incorrectly written as Figure 1. (To verify the performance of G-FS, G-FS is compared with other feature selection methods on 12 different datasets (refer to Figure 1).)\n\n**Answer to Question 2:** Thanks for pointing out these errors; we are sorry for these errors. The description of 'four types' in Figure 1c is right, we have corrected it and other minor errors in the revised manuscript."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699877259499,
                "cdate": 1699877259499,
                "tmdate": 1699877259499,
                "mdate": 1699877259499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]