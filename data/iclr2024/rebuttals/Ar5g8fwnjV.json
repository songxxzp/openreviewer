[
    {
        "title": "HOVER: Hyperbolic Video-text Retrieval"
    },
    {
        "review": {
            "id": "WVFuTac1sY",
            "forum": "Ar5g8fwnjV",
            "replyto": "Ar5g8fwnjV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4637/Reviewer_BhHY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4637/Reviewer_BhHY"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose Hyperbolic Video-tExt Retrieval to model the Is-A semantic relationships between videos and texts.\nSpecifically, a video with action compositions is first decomposed longitudinally into an action tree with mono-action leaf or child nodes and increasingly complex parent nodes. Then, the is-a semantic relationship in videos/texts is represented in the hyperbolic space by employing hyperbolic norm constraints."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A new method for compositional video retrieval. \n\n- Show somewhat better results than previous methods."
                },
                "weaknesses": {
                    "value": "- What's the difference between Is-A semantic relationship and partially relevant? \nThe authors are recommended to make necessary explanations on this issue. \nMost importantly, the authors should make comprehensive and fair comparisons between partially relevant retrieval[A][B] and multi-event retrieval[C]. \nAdditionally, the HOVER could be performed after the hierarchical datasets were built while partially relevant retrieval did not need the operation. \n\n- No comprehensive experiments on the sensitivity of the proposed parameters. \n\n- Reported as Table 2, larger improvement on non-hierarchical datasets: MSR-VTT and MSVD needs necessary analysis. \n \n- What is the difference between the chain-like struture and tree-like structure in the design of losses? \nThe authors are recommended to make necessary explanations on the non-improvement of the performance when the chain-loss is added, presented as Table 4. \nThe ablation with only Align loss is also recommended to present here for further understanding this work. \n\n- Other minor issues:\n\n     -- No LSMDC in Table 2.\n \n     -- Double quotation marks for is-a are incorrect.\n\n    -- Deleting the extra full stop below the caption of Figure 2.\n\n\n\nreferences:\n\nA. Partially Relevant Video Retrieval, ACM MM 2022\n\nB. Progressive Event Alignment Network for Partially Relevant Video Retrieval, ICME 2023\n\nC. Multi-Event_Video-Text_Retrieval, ICCV 2023."
                },
                "questions": {
                    "value": "Presented as weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4637/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569932589,
            "cdate": 1698569932589,
            "tmdate": 1699636443477,
            "mdate": 1699636443477,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "EIwzN7QkyK",
            "forum": "Ar5g8fwnjV",
            "replyto": "Ar5g8fwnjV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4637/Reviewer_AjjV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4637/Reviewer_AjjV"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses hyperbolic space to represent the Is-A relationship in compositional actions, improving video comprehension. Apart from aligning text and video, the suggested method achieves: 1) Using the Poincar\u00e9 ball, study the representation of video and text while considering their hierarchical relationship. 2) Obtaining information about the ordering of video-text pairs. The suggested model improves across multiple tasks by adequately representing the properties of composing activities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow, and the visualizations are nicely designed.\n2. The proposed ``semantic-chain'' is novel and nicely handles one common ill-conditioned case in hierarchical structure.\n3. Intuitively, the combination of semantic tree, semantic chain, and temporal ordering can nicely capture the structure inherent in compositional actions.\n4. Experimental results across a diverse range of tasks provide sound evidence supporting the efficacy of the proposed model."
                },
                "weaknesses": {
                    "value": "1. The novelty is not clearly stated, as many components (text-visual training, hyperbolic embedding, temporal encoding) have been proposed by other models. The authors should highlight their contribution.\n2. The absence of the base case(s) in Table 4, for example, when only alignment is utilized and only temporal encoding is deployed.\n3. Tables with good numbers are good, but it is less interesting than the analysis and explanation of WHY the proposed model works."
                },
                "questions": {
                    "value": "Please respond to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4637/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4637/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4637/Reviewer_AjjV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4637/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698678654302,
            "cdate": 1698678654302,
            "tmdate": 1699636443381,
            "mdate": 1699636443381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "bq9qvECRFC",
            "forum": "Ar5g8fwnjV",
            "replyto": "Ar5g8fwnjV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4637/Reviewer_MxoF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4637/Reviewer_MxoF"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the challenging task of retrieving complex videos with multiple actions. They introduce HOVER, a method that uses hyperbolic space to model the hierarchical relationships between videos and texts. HOVER outperforms traditional methods, especially when the dataset is small, with a substantial performance gain of 28.83%. It also generalizes well to new datasets with complex videos."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method with novel semantic tree, semantic chain, and temporal encoding is quite interesting.\n2. The proposed method has better performance."
                },
                "weaknesses": {
                    "value": "1. The writing is poor and the paper is not self-contained. To be honest, this paper investigates a novel task instead of the traditional video-text (paragraph) retrieval and video moment retrieval. So first thing the authors should do is actually present the definition of \u2018hierarchical video retrieval\u2019 before introducing any other technical details. \n2. It would be interesting to see whether the use of other balls leads to similar or better performance with the proposed novel semantic tree, semantic chain, and temporal encoding.\n3. The details of the proposed method are missing. For example, the details of hyperbolic layers, exponential map, and how to calculate the similarity after obtaining the tree.\n\nThis paper needs another round of proofreading before submitting to any prestigious conference."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4637/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716615286,
            "cdate": 1698716615286,
            "tmdate": 1699636443301,
            "mdate": 1699636443301,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "vny1je6tzF",
            "forum": "Ar5g8fwnjV",
            "replyto": "Ar5g8fwnjV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4637/Reviewer_P4PE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4637/Reviewer_P4PE"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a hyperbolic feature space to encode the visual and text embedding for the task of video-text retrieval. Given a complex video sequence consisting of multiple mono-actions, this work embeds videos and texts into Poincare ball with the manner of semantic tree and chain. Compared to the baseline method, this work achieves promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The entire paper is well-written, the idea is easy to follow and the motivation is well justified.\n\n2. The idea of well-structuring the visual and linguistic embedding into the hyperbolic manifold is novel and interesting, which constructs discriminative feature representations for the complex videos.\n\n\n3. This work also provides many insights of modeling the long-form sequences in the vision and language feature learning. The temporal dependency in the Poincare ball could potentially improve the longer temporal reasoning in the video understanding. \n\n\n4. This work has done extensive comparison with the baseline method (clip4clip), which clearly demonstrates the effectiveness of each proposed component in the algorithm. Moreover, compared to the clip4clip, the performance of this work looks very promising. \n\n\n5. Code is provided which is a plus for the re-implementation."
                },
                "weaknesses": {
                    "value": "1. In the experiment section, this work only compares with the clip4clip. It will be good to compare with other SOTA methods showing the superior performance of this work.\n\n2. The clip4clip is the video-extension model of clip model. When there are multiple leaves, the clip4clip may not have effective mechanism to model longer sequence/text. However, the proposed HOVER formulates a tree/chain to address this. Is there a better way to verify this hypothesis?\n\n3. In some of previous Riemannian manifold works [1, 2], people may face the non-convex issue when doing the optimization. For the stochastic-based optimization used in this work, how to constraint the learned feature always maintains a hyperbolic manifold?\n\n\n[1] Generalized rank pooling for activity recognition, CVPR17\n\n[2] Discriminative Video Representation Learning Using Support Vector Classifiers, TPAMI"
                },
                "questions": {
                    "value": "Mentioned in the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4637/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784470331,
            "cdate": 1698784470331,
            "tmdate": 1699636443183,
            "mdate": 1699636443183,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]