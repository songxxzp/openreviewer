[
    {
        "title": "Efficacy of Dual-Encoders for Extreme Multi-label Classification"
    },
    {
        "review": {
            "id": "tdXMSVnKXZ",
            "forum": "dNe1T0Ahby",
            "replyto": "dNe1T0Ahby",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission476/Reviewer_1qgK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission476/Reviewer_1qgK"
            ],
            "content": {
                "summary": {
                    "value": "Dual-encoder models have achieved substantial success in dense retrieval tasks for open-domain question answering, especially in zero-shot and few-shot scenarios. However, their performance in many-shot retrieval problems, where abundant training data is available, such as extreme multi-label classification (XMC), has received limited attention. Existing evidence indicates that dual-encoder methods tend to underperform compared to state-of-the-art extreme classification methods that scale the number of learnable parameters linearly with the number of classes in such tasks. Some recent extreme classification techniques combine dual-encoders with a learnable classification head for each class to excel in these scenarios. This paper explores the potential of \"pure\" dual-encoder models in XMC tasks and provides insights for XMC."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Important Research Problem: The paper addresses a significant and relevant problem in the field of machine learning - extreme multi-label classification. This is a challenging task with practical applications, and the choice of topic adds value to the existing literature.\n\nInteresting Approach: The idea of using dual-encoders for solving the problem is intriguing and adds a novel dimension to the research. This innovative approach can potentially open up new avenues for tackling similar problems in the future.\n\nComprehensive Experimental Validation: One of the strengths of the paper is its thorough experimental validation. The fact that the proposed approach has been tested on multiple datasets indicates a comprehensive evaluation of its effectiveness. This enhances the credibility of the findings and their potential applicability to real-world scenarios."
                },
                "weaknesses": {
                    "value": "I do not find obvious flaws. One notable weakness is the lack of clarity in articulating the paper's contributions. It's important for the reader to clearly understand what novel insights or advancements are being offered. The paper should explicitly state the unique contributions and why they matter in the context of extreme multi-label classification. This clarity is crucial for both researchers and practitioners in the field.\nTo enhance the paper, it would be beneficial to provide a more structured and explicit statement of the research's contribution and significance in introduction section. This will help the readers better grasp the key takeaways from the study. Additionally, the paper could benefit from improved organization and flow to ensure that the reader can easily follow the arguments and findings."
                },
                "questions": {
                    "value": "As in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755506173,
            "cdate": 1698755506173,
            "tmdate": 1699635973975,
            "mdate": 1699635973975,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kh058UJLlT",
                "forum": "dNe1T0Ahby",
                "replyto": "tdXMSVnKXZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful review and constructive feedback. We appreciate your positive comments on the importance of the research problem, our approach, and the thorough experimental validation. We have carefully considered your suggestions and propose the following revisions:\n\n**Clarifying Contributions**: Please see our general comment for a more clear discussion on contributions and significance of this work. We acknowledge the need for a clearer articulation of the paper's unique contributions, especially in the introduction. We have revised the introduction to explicitly reflect this (please see the revised pdf)\n\n**Improving Organizational Flow**: We will restructure the paper to enhance the flow and coherence of our arguments and findings. More specifically, taking into account reviewer Smm7\u2019s suggestions, we will introduce a preliminary discussion on inability of BCE loss to train dual encoders and clearly state how we are proposing to overcome the challenges. Please let us know if there are any further suggestions to improve the organization of the paper.\n\nWe hope these revisions will improve the manuscript, addressing your concerns and making our contributions clearer to the readers. We are happy to discuss any further questions about the work and would appreciate an appropriate increase in the score if your concerns were adequately addressed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700196976532,
                "cdate": 1700196976532,
                "tmdate": 1700196976532,
                "mdate": 1700196976532,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UEiIF2zySh",
                "forum": "dNe1T0Ahby",
                "replyto": "kh058UJLlT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission476/Reviewer_1qgK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission476/Reviewer_1qgK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed responses. I decide to maintain my score (6)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636416967,
                "cdate": 1700636416967,
                "tmdate": 1700636416967,
                "mdate": 1700636416967,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WsFffu5bQg",
            "forum": "dNe1T0Ahby",
            "replyto": "dNe1T0Ahby",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission476/Reviewer_jMAd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission476/Reviewer_jMAd"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the performance of dual-encoder models in extreme multi-label classification (XMC) tasks. The authors first reveal the shortcomings of traditional dual-encoder training loss, which may over-penalize the correct prediction of \"easy-positive\" labels. To address this, the Decoupled Multi-label Loss is proposed, aiming to minimize the undesirable correlation between positive labels during training. A memory-efficient training framework is also introduced in this paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors highlight a neglected problem in the XMC dual-encoder training stage. The proposed Decoupled Loss effectively solves this problem.\n- The theoretical part of the paper is well-presented, with Section 3 providing clear symbol definitions."
                },
                "weaknesses": {
                    "value": "- The main motivation for this paper is the imperfect design of current dual-encoder training loss. However, there is a lack of evidence that this has been a general issue for current XMC methods. Most discussions and experiments are designed to compare the Decoupled Loss and regular loss using the authors' own training framework. Some experiments are implemented using a synthetic dataset (Fig. 2) or pre-selected labels (Fig. 3). After reading the entire paper, I believe the proposed loss can solve the mentioned problem, but I am not convinced that this problem is a universal issue in the XMC community.\n- The paper lacks novelty. A minor revision of the training loss may not be sufficient for a top conference paper."
                },
                "questions": {
                    "value": "- Please address the questions mentioned in the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission476/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission476/Reviewer_jMAd",
                        "ICLR.cc/2024/Conference/Submission476/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834169893,
            "cdate": 1698834169893,
            "tmdate": 1700594533487,
            "mdate": 1700594533487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a9XUz3d6nk",
                "forum": "dNe1T0Ahby",
                "replyto": "WsFffu5bQg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the review and feedback. Please find below our response to the concerns:\n\n> The main motivation for this paper is the imperfect design of current dual-encoder training loss. However, there is a lack of evidence that this has been a general issue for current XMC methods...\n\nWe disagree that \u201cthere is a lack of evidence that this has been a general issue for current XMC methods and most discussions and experiments are designed to compare the Decoupled Loss and regular loss using the authors' own training framework\u201d. We perform our evaluation on 4 XMC benchmarks which span the largest available XMC datasets. The main results of our paper (Table 2 and 3) compare our approach against current SOTA XMC approaches [1,2] and show superior performance. Existing XMC papers repeatedly show that dual encoders don\u2019t perform well by themselves and hence need to be augmented with per-label classifiers or auxiliary parameters to achieve the best performance, see table 8 in [1] and table 2 in [2] vs results in [3] which show upto 10% absolute difference in P@1 between vanilla dual-encoder approach and dual-encoder with classifier approach. Quoting from [2], \u201cthis (dual-encoder) approach is expected to suffer if the textual descriptions are not descriptive enough which makes it challenging for an encoder to bring related data points and labels close to each other in the embedding space....To overcome this, XC methods introduce a high-capacity classifier such as 1-vs-all models where each label is endowed with a linear classifier that acts on the embedding of a data point\u201d. In contrast, to the best of our knowledge, our work is the first one to show that vanilla dual-encoders can match or surpass existing SOTA methods when trained appropriately. This is a desirable result because a) dual-encoders can be much more parameter efficient than existing XMC methods that use per-label classifiers; b) this brings promise that dual-encoders can solve both standard retrieval and XMC problems leading to a universally applicable solution and simplifying the landscape of future solutions for these problems. Please see our general comment for a more detailed discussion on contributions and significance of this work. We also acknowledge that although this work shows that dual-encoders are capable of performing well on XMC tasks, classifier based solutions can still be attractive because of their ease of computation (which is just an embedding lookup) as compared to encoders (which requires a relatively expensive encoder forward pass).\n\n> The paper lacks novelty. A minor revision of the training loss may not be sufficient for a top conference paper.\n\nWe would like to emphasize that the technical contributions of our paper goes beyond just introducing a minor revision to existing loss. More specifically, our technical contributions include: \n- Identification of Limitations in Traditional Dual-Encoder Training Losses: we analyze existing multi-label and contrastive training losses like One-versus-All Binary Cross-Entropy (OvA-BCE) and InfoNCE, highlighting how they may be inadequate for training dual-encoder models in XMC. Specifically, we note that OvA-BCE does not train effectively and InfoNCE disincentivizes confident predictions.\n- Decoupled Softmax and SoftTop-k loss: To overcome these limitations, we proposes a simple modification to the InfoNCE/Softmax loss as the Decoupled Softmax Loss and extend the loss design to soft-topk loss which is tailored to optimize prediction accuracy within a specific prediction budget size ('k')\nTo the best of our knowledge, these limitations of standard multi-label and contrastive loss functions and the proposed changes have not been discussed in literature before. Moreover, scaling our experiments to largest XMC benchmarks require non-trivial implementation efforts which is often not highlighted but is an essential component for modern machine learning solutions.\n\n---\n\n[1] NGAME: Negative Mining-aware Mini-batching for Extreme Classification, WSDM 2023\n\n[2] Deep Encoders with Auxiliary Parameters for Extreme Classification, KDD 2023\n\n[3] The Extreme Classification Repository: Multi-label Datasets & Code\n\n---\n\nWe hope our response addressed your concerns and are happy to discuss further questions. Please consider improving your score if you find this work adequate for acceptance after the clarifications and revisions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197882910,
                "cdate": 1700197882910,
                "tmdate": 1700197882910,
                "mdate": 1700197882910,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3ZQa5zfoJu",
                "forum": "dNe1T0Ahby",
                "replyto": "a9XUz3d6nk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission476/Reviewer_jMAd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission476/Reviewer_jMAd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for authors' detailed explanation. I've raised my score to 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594513243,
                "cdate": 1700594513243,
                "tmdate": 1700594513243,
                "mdate": 1700594513243,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cv18MOsqcD",
            "forum": "dNe1T0Ahby",
            "replyto": "dNe1T0Ahby",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission476/Reviewer_BerB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission476/Reviewer_BerB"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the use of dual encoder models for extreme multilabel classification (XMC). Dual encoder models, which as the name suggests, use two encoder models, have been generally effective for a variety of other types of tasks involving zero-shot and few-shot learning, but have underperformed the state-of-the-art on XMC tasks. Dual encoder methods are, on the other hand, desirable for extreme classification in principle because they can be much more parameter efficient than extreme classification methods whose parameter counts have a strong dependence on the number of classes. \n\nThe authors propose a new loss function, the differentiable top-k, for dual encoder models on XMC tasks that make them competitive with the state-of-the-art methods on these tasks--often outperforming them by a significant margin."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The work is well-motivated and addresses the practical problem of getting dual encoder methods to work in the extreme multilabel setting. \n- The proposed contribution is simple, as it is just a loss function paired with either a negative mining approach or a memory-efficient implementation using all negatives. \n- The ablation of the different loss variants -- soft top-5 and soft top-100 -- is compelling and shows that the method can effectively optimize precision or recall at 5 or 100, respectively."
                },
                "weaknesses": {
                    "value": "- I might have missed something, but I think it should be made clearer earlier on in the paper that the differentiable top-k operator had been proposed previously [1]. The authors also link to the author of the stackexchange answer, but it would be ideal to cite the specific answer at the link (please correct me if this appears somewhere in the main text, but I couldn't find it). Relatedly, is there other work that uses the formulation by Thomas Ahle? For example, how does this formulation compare to the one linked in the stackexchange post [2]? In my opinion, this brings the novelty of the contribution *when posed as a new loss function* into question. \n- Again, I might have missed something, but I think the ablation involving the negative mining approach should include a comparison to SOTA methods, as the negative mining approach may be required for scaling the method up even further. \n\n[1] https://math.stackexchange.com/questions/3280757/differentiable-top-k-function/4506773#4506773 \n\n[2] https://arxiv.org/pdf/2002.06504.pdf"
                },
                "questions": {
                    "value": "- Can the authors elaborate on how $t_x$ in the soft top-k formulation is computed via binary search? As this is a crucial hyperparameter of the loss function, does this require a logarithmic number of retraining runs? How many runs are required to set this, what are the compute costs involved with setting this hyperparameter, and how strongly does the final performance depend on this choice? \n- In Table 5, why is precision at 5 reported while recall at 100 is reported? Why not both precision and recall at 5 and 100? Is this a standard choice? \n- Does the loss optimize precision at k or recall at k?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission476/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission476/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission476/Reviewer_BerB"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699016677650,
            "cdate": 1699016677650,
            "tmdate": 1700624515311,
            "mdate": 1700624515311,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qXM2EuCjQU",
                "forum": "dNe1T0Ahby",
                "replyto": "cv18MOsqcD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback. We appreciate your insights and have addressed your concerns as follows:\n>  I might have missed something, but I think it should be made clearer earlier on in the paper that the differentiable top-k operator had been proposed previously...\n\nYou\u2019re right that the application of the differentiable top-k operator as a loss function for optimizing multi-label predictions at fixed prediction budget-size is the novel contribution. The differentiable top-k operator itself has been proposed in the literature, a common approach is to use the optimal transport-based formulation (which you mentioned) but it is very hard to implement in a distributed training setup which is necessary for our experiments. This is why we instead extend the approach presented in the stackexchange answer by Thomas Ahle because of its simplicity and easily distributable operations with low communication overheads. To the best of our knowledge this formulation is not used in any existing work. We do cite the original stackexchange answer but ICLR citation style seems to only display user profile in the main text (the citation in the reference links to the original answer), we will fix this in the revised version based on Reviewer Smm7\u2019s suggestions. We\u2019d also like to highlight that although soft-topk loss function is one of the technical contributions we make, the major contributions of this work is in highlighting that existing popular loss functions can be inadequate for training dual-encoder models for XMC, to overcome this we revise InfoNCE and extend its loss design to SoftTopK leading to much more parameter efficient SOTA XMC solution. Please see our general comment for a more detailed discussion on contributions and significance of this work.\n\n> ablation involving the negative mining approach should include a comparison to SOTA methods\n\nDue to lack of space we did not repeat the rows from the main results table (Table 2) which reports numbers for SOTA methods in the negative mining ablation table (Table 4). We do agree that perhaps having the existing SOTA results in Table 4 might be easier for a reader to quickly compare the approach with negative mining, we will make this change in the final version. In the table below we report the results with negative mining against SOTA methods, note that DE trained with proposed loss functions can still match or come very close to SOTA methods.\n| Method | Hard neg per query | P@1     | P@5     | R@10    | R@100   |\n|--------|-------------------|---------|---------|---------|---------|\n| **LF-Wikipedia-500K** | | | | | |\n| Ours   | 0                 | 77.71   | 43.32   | 69.24   | 88.12   |\n| Ours   | 1                 | 82.85   | 48.84   | 74.47   | 90.08   |\n| Ours   | 2                 | 83.34   | 49.32   | 74.73   | 89.74   |\n| Ours   | 5                 | 83.86   | 49.57   | 74.60   | 89.12   |\n| Ours   | 10                | 84.77   | 50.31   | 75.52   | 90.29   |\n| NGAME (with classifiers) | - | 84.01 | 49.97 | - | - |\n| DEXA (with classifiers) | - | 84.92 | 50.52 | - | - |\n| **LF-AmazonTitles-1.3M** | | | | | |\n| Ours   | 0                 | 42.15   | 32.97   | 29.28   | 57.48   |\n| Ours   | 1                 | 49.16   | 39.07   | 32.76   | 60.09   |\n| Ours   | 2                 | 50.74   | 40.14   | 33.31   | 60.45   |\n| Ours   | 5                 | 52.04   | 40.74   | 33.48   | 60.13   |\n| Ours   | 10                | 54.01   | 42.08   | 34.19   | 61.04   |\n| NGAME (with classifiers) | - | 56.75 | 44.09 | - | - |\n| DEXA (with classifiers) | - | 56.63 | 43.90 | - | - |\n\n> How $t_x$ in the soft top-k formulation is computed via binary search...\n\n$t_x$ is not a hyperparameter, rather $t_x$ is a variable which is a function of the $x$ vector (i.e. the vector of all the scores) such that $\\sum_{i=1}^{N}{s(x_i + t_x) = k}$. Essentially, t_x is a scalar quantity which when added to all the scores $x_i$ makes the sigmoid of new scores ($x_i + t_x$) sum upto $k$. Since sigmoid is a monotonic function, increasing $t_x$ strictly increases the sum of sigmoids and decreasing $t_x$ strictly decreases the sum of sigmoids, hence for a given $x$, binary search can be used to find the $t_x$ which makes the sum of new sigmoids equal to $k$. We hope this clarifies the doubt, we will also revise section 4.3 in the final version to make this more clear."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700197119868,
                "cdate": 1700197119868,
                "tmdate": 1700197119868,
                "mdate": 1700197119868,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CgWpVRskSQ",
                "forum": "dNe1T0Ahby",
                "replyto": "mKN4mqrU6q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission476/Reviewer_BerB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission476/Reviewer_BerB"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "Thank you for your response, which has addressed my questions and concerns -- I will raise my score to a 6 and recommend acceptance."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624501183,
                "cdate": 1700624501183,
                "tmdate": 1700624501183,
                "mdate": 1700624501183,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "evoKkJZmAv",
            "forum": "dNe1T0Ahby",
            "replyto": "dNe1T0Ahby",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission476/Reviewer_Smm7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission476/Reviewer_Smm7"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript considers the use of Dual Encoders (DE) models for Extreme Multi-Label Classification (XMC). In previous work using softmax loss functions, the presence of other positive classes decreases the negative loss that can be gained from a training example. This work considers decoupled softmax, in which the negative loss gained for each positive class is independent of other positive classes. The motivation for this new loss function is that some classes may \"obviously\" apply, which should lead to confident predictions for those \"obvious\" cases. In experiment with synthetic dataset where one of the samples is very obviously marked, both softmax and decoupled softmax methods latch on this obvious annotations early on, but standard softmax approaches then entice the model to give the same score to all positive samples, obvious or not, thus decreasing the model's confidence in the obvious case. In non-synthetic settings, decoupled softmax reduces the variance in gradient feedback.\n\nDirectly training with the full decoupled softmax loss is challenging when there are many labels: appendix presents how to do this training with the full loss in a memory-efficient way. Still, for higher number of labels (e.g., documents retrieval), there is a need to approximate this full loss, and some preliminary results are given in this direction. A natural extension is presented for SoftTop-k.\n\nThe Appendices were not considered as part of this review."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "To the best of my knowledge, the work is original and significant. Although the distinction between using a single multi-class cross-entropy vs using multiple binary cross-entropies (OvA-BCE) have long been well understood, it isn't immediately obvious that the DE setting would be so different, and that the OvA-BCE loss would fail to train.\n\nIn the context of Retrieval-Augmented Generation (RAG), when the retriever is not trained in an end-to-end manner with the answerer, one often conceptualize the retrieval problem as \"get that one good document that is know to suffice to answer the question\". However, there could be multiple such documents that contain the desired answer: this retrieval task can be understood as a particularly extreme case of XMC. The take home message that I personally get from this manuscript is that I should exercise caution along those lines if I ever attempt to train DE retrieval models with contrastive augmentations. (continued in weaknesses)..."
                },
                "weaknesses": {
                    "value": "... (continued from strengths) However, I wouldn't have come to this realization from the manuscript's abstract nor introduction, and that nugget would have been lost on me had I encountered the manuscript outside of a reviewing context. I understand that this work mainly targets the XMC literature (of which - full disclosure - I personally don't know much), but I still believe that the authors should dedicate some of their high-level discussions (i.e., abstract and/or introduction), to the significance of their work for DE-based RAG.\n\nMore generally, the manuscript's main weakness is with organizing the content to give a clear narrative. For example, Equation (2) introduces OvA-BCE, then eschew saying what's wrong with it until Section 5.5. As a reader, I had to go back-and-forth in the document to just understand what the authors are set in doing, what goes wrong with the default approach, and what's the authors' solution. More details in Question 1 below."
                },
                "questions": {
                    "value": "### 1\n(a) What are the assumptions, goals and constraints specifying what this project is about; (b) what would be the default/status quo approach and what's wrong with it; and (c) what is the essence of the solution you propose?\n\nI think that I managed to figure out the answer to those questions, but these things should be clearly identifiable from the introduction (or even abstract). Here's my own crude attempt at it: please complement it, clarify any disagreements, and propose edits to the manuscript.\n\n- 1(a) There are a large number of classes, and many of them can apply to the same sample. The models being considered are DE where the representations from each encoder will be converted to a score using an inner product.\n- 1(b) OvA-BCE won't train; InfoNCE disincentivizes confident predictions.\n- 1(c) Decoupled softmax both trains and allows for confident predictions.\n\n\n### 2\nThe bulk of the manuscript presumes the optimization of the the \"full\" loss function over all positive and negative classes. Is this standard practice in the XMC community? Some experiments sampling hard negatives are presented in Section 5.4, but there is no real discussions besides \"more is better\". Do you have any insights to add? Could those be added to the manuscript?\n\n\n*The remaining points are more minor*\n\n### 3\nFigure 1: why express the x axis in millions, instead of replacing the $10^2$ and $10^3$ ticks by $10^8$ and $10^9$?\n\n### 4\nPlease avoid notations such as \"O(100)\", \"O(million)\" and \"O(billion)\".\n\n### 5\nFigure 2: \"Precision@1\" here has a special meaning. There are 5 samples that are marked as positives, but one of them is \"more positive\", and \"Precision@1\" here means \"the more positive sample must be in first position\". This special setting is relatively clear in the text, but looking quickly at Figure 2 and its caption can be misleading. Please consider inventing a different term/notation.\n\n### 6\nThere are multiple missing punctuations after mathematical expressions. For example, a period should be added at the end of the first paragraph of Section 3, and another one should be added after the equation at the bottom of page 4. As a side note, I would personally add a lot of commas to the English text, but I understand that this may be more a matter of style.\n\n### 7\nThe text appears to use \\cite or \\citep everywhere, but many should actually be \\citet. For example, in the second paragraph of Section 4.2, \"similar to (Xiong et al., 2021)\" should become \"similar to Xiong et al. (2021)\", and \"in (Lindgren et al., 2021)\" should be \"in Lindgren et al. (2021)\".\n\n### 8\nThe style of the stackexchange citation in the paragraph following Equation (6) should be revised. Personally, I would have expected something like \"... on a proposal by Ahle (2022).\"\n\n### 9\nSome variables are overloaded; for example $d$ is used both for documents and dimensions. I thought that there were more cases, but I can't find them anymore, so I may be confused with another paper. Please check."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission476/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission476/Reviewer_Smm7",
                        "ICLR.cc/2024/Conference/Submission476/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699307028610,
            "cdate": 1699307028610,
            "tmdate": 1700662893654,
            "mdate": 1700662893654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XyqV3aaVJB",
                "forum": "dNe1T0Ahby",
                "replyto": "evoKkJZmAv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough and insightful review. We appreciate your recognition of the originality and significance of our work in applying Dual Encoders in Extreme Multi-Label Classification and its implications for Retrieval-Augmented Generation. Please find below our response to the concerns and suggestions raised:\n\n**Implications for RAG**: Retrieval augmented generation (RAG) is an important scenario which we didn\u2019t fully consider at the point of writing this paper, many thanks for bringing it out. In scenarios where multiple documents could contain the desired answer, the retrieval task in RAG aligns closely with extreme multi-label classification (XMC). Our work's exploration of dual-encoder methods in XMC directly informs the appropriate choice for loss formulation in such scenarios. To this end, we perform a preliminary experiment described in \u201cRAG experiment\u201d below, to see the promise of this work in the context of RAG.\n\n**Significance and Contributions**: Please see our general comment for a more clear description of contributions and significance of this work. We acknowledge your concern regarding the lack of clarity in contributions and significance of this work in the introduction of the manuscript. To address this, we have proposed a revision of the introduction (please see the revised pdf). We plan to further refine the introduction in the final version to ensure that the broader implications of our research are clear to a wider audience. Below we slightly complement your answer to the question 1\n- 1(a) There are a large number of classes, and many of them can apply to the same sample. The models being considered are DE where the representations from each encoder will be converted to a score using an inner product.\n- 1(b) DE models empirically don\u2019t perform well on such task but are desirable due to parameter efficiency and generalization to unseen labels; Standard loss function are inadequate - OvA-BCE won't train; InfoNCE disincentivizes confident predictions.\n- 1(c)Decoupled softmax both trains and allows for confident predictions leading to SOTA DE models for XMC tasks\n\n**Narrative Clarity**: We acknowledge that the manuscript would benefit from a clearer narrative structure. We will reorganize the content in the final version to more effectively present our objectives, the limitations of existing approaches, and the essence of our proposed solution. More specifically, in order to address the concern related to the lack of discussion on OvA BCE in section 4, we will introduce a preliminary discussion on the inability of BCE loss to train dual encoders and clearly state how we are proposing to overcome the challenges. We will also take into account all the formatting suggestions to improve the final version.\n\n**Discussion on negatives**: optimization of the \"full\" loss function is not a standard practice in the XMC community but is used sometimes to establish the upper bound performance. In XMC benchmarks, we notice that an extensive use of negatives is required to train properly and is typical for classifier based methods to use ~100 negative labels per training query. In dual encoder training since the most expensive step is the forward and backward pass through the encoder, considering ~100 negative labels per training query for a batch size of 8192 would be computationally almost equivalent to computing the full loss for a dataset with upto 1M labels. Apart from showing that there is a significant dependance of the performance with the number of negatives, in section C.3 of the appendix we note that the quality of negatives can also play a modest role in determining the accuracy (specially P@1) performance of the model. In section C.2.3 we also confirm that increasing negatives for InfoNCE/Softmax loss function on XMC benchmarks doesn\u2019t benefit the performance that much when compared to DecoupledSoftmax loss, this is because as we increase total number of negatives to consider in the loss computation we also increase the undesired competition among the positives in a multi-label setting.\n\n## RAG experiment\nInspired by the open QA benchmarks (NQ/MSMARCO) which can be treated as few-shot variants of RAG task (given a question, retrieve passage using a DE model. then given (question, retrieved passage) a reader model generates answer), we construct a naturally many-shot and multi-label RAG scenario from the existing XMC wikipedia dataset LF-Wikipedia-500K. More specifically, we create the task of generating wikipedia content from a wikipedia page title given some tags associated with the page. The tags are essentially the evidence documents that we retrieve and we use the wikipedia page title as the query and ask a fine-tuned LLM (Llama2-7B) model to generate the corresponding wikipedia article. contd. in further comment..."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700196642891,
                "cdate": 1700196642891,
                "tmdate": 1700196642891,
                "mdate": 1700196642891,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "64WKxCgpf2",
                "forum": "dNe1T0Ahby",
                "replyto": "1KqZvt5Jos",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission476/Reviewer_Smm7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission476/Reviewer_Smm7"
                ],
                "content": {
                    "comment": {
                        "value": "The Authors addressed most of my points, they clarified some points on which I was confused, their new introduction is already much better than it was, and they \"will introduce a preliminary discussion on the inability of BCE loss to train dual encoders and clearly state how we are proposing to overcome the challenges [and] will also take into account all the formatting suggestions to improve the final version.\"\n\nI would raise my score to 7 but it is not among the options, so I raise it to 8 and recommend acceptance."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662926266,
                "cdate": 1700662926266,
                "tmdate": 1700662926266,
                "mdate": 1700662926266,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]