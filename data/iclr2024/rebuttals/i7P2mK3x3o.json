[
    {
        "title": "Computing high-dimensional optimal transport by flow neural networks"
    },
    {
        "review": {
            "id": "JYTOH9yHNT",
            "forum": "i7P2mK3x3o",
            "replyto": "i7P2mK3x3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_qvyZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_qvyZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to learn the dynamic Optimal Transport trajectory between two distributions only known through samples. Authors propose to learn the velocity field by minimizing the dynamical OT problem where the marginals are enforced by minimizing the KL divergence. One of the main contribution of the paper is to propose a new density ratio estimation technique based on a logistic classification network. Then, the method is applied on several tasks, ranging from finding the trajectory between toy data, estimating the Mutual Information, Energy-based modeling and Image-to-Image translation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, the paper is well written and proposes a method to approximate the dynamic OT with Continuous Normalizing Flows. One of the main contribution is the new density ratio estimator which is shown to perform well compared to some baselines. The method is also demonstrated to work on several applications.\n\n- A new Density Ratio Estimator used in order to approximate the dynamic OT, which is to the best of my knowledge original.\n- Use a symmetric loss to better train the velocity field\n- Different strategies to initialize the flow are discussed\n- Several applications demonstrating the superiority of the method compared to others DRE estimators. Notably the experiments are mostly in high dimension."
                },
                "weaknesses": {
                    "value": "- The comparisons seem to be made only with the same method using other density ratio estimation techniques. Other works which could be compared with could be e.g. [1] which propose a flow matching technique which can link arbitrary distributions.\n- The Figures are not all of good quality. Notably, Figure 2 and 4 are a bit too small and we cannot really distinguish the results of Figure 2,b. \n\n[1] Tong, Alexander, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. \"Conditional flow matching: Simulation-free dynamic optimal transport.\" arXiv preprint arXiv:2302.00482 (2023)."
                },
                "questions": {
                    "value": "I think that some related works are not cited. For instance, [1] parameterize Normalizing Flows (NFs) with Monge maps, [2] train NFs using the JKO scheme and the dynamic formulation of OT and [3] improves the OT cost of Normalizing Flows. Also, [4] proposes a way to find a Normalizing Flow between two arbitrary distributions.\n\n\nI found some other works which use Density Ratio Estimators based on Bregman divergences, e.g. [5, 6], and I am wondering whether these methods are competitive or not with the technique used in this paper.\n\nTypos:\n- Above equation (5): \"The inner-loop training of $r_1$ is by\"\n\n\n[1] Huang, Chin-Wei, Ricky TQ Chen, Christos Tsirigotis, and Aaron Courville. \"Convex potential flows: Universal probability distributions with optimal transport and convex optimization.\" arXiv preprint arXiv:2012.05942 (2020).\n\n[2] Vidal, Alexander, Samy Wu Fung, Luis Tenorio, Stanley Osher, and Levon Nurbekyan. \"Taming hyperparameter tuning in continuous normalizing flows using the JKO scheme.\" Scientific Reports 13, no. 1 (2023): 4501.\n\n[3] Morel, Guillaume, Lucas Drumetz, Simon Bena\u00efchouche, Nicolas Courty, and Fran\u00e7ois Rousseau. \"Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows.\" arXiv preprint arXiv:2209.10873 (2022).\n\n[4] Panda, Nishant, Natalie Klein, Dominic Yang, Patrick Gasda, and Diane Oyen. \"Semi-supervised Learning of Pushforwards For Domain Translation & Adaptation.\" arXiv preprint arXiv:2304.08673 (2023).\n\n[5] Feng, Xingdong, Yuan Gao, Jian Huang, Yuling Jiao, and Xu Liu. \"Relative entropy gradient sampler for unnormalized distributions.\" arXiv preprint arXiv:2110.02787 (2021).\n\n[6] Heng, Alvin, Abdul Fatir Ansari, and Harold Soh. \"Generative Modeling with Flow-Guided Density Ratio Learning.\" arXiv preprint arXiv:2303.03714 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5678/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5678/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5678/Reviewer_qvyZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698159733616,
            "cdate": 1698159733616,
            "tmdate": 1699636592953,
            "mdate": 1699636592953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gnH8kfEVEC",
                "forum": "i7P2mK3x3o",
                "replyto": "JYTOH9yHNT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qvyZ"
                    },
                    "comment": {
                        "value": "1. **(Presentation) The Figures are not all of good quality. Notably, Figure 2 and 4 are a bit too small and we cannot really distinguish the results of Figure 2,b.**\n\nThank you. We will re-make the figures in future revisions.\n\n2. **(Citation) I think that some related works are not cited. For instance, [1] parameterize Normalizing Flows (NFs) with Monge maps, [2] train NFs using the JKO scheme and the dynamic formulation of OT and [3] improves the OT cost of Normalizing Flows. Also, [4] proposes a way to find a Normalizing Flow between two arbitrary distributions.**\n\n    [1] Huang, Chin-Wei, Ricky TQ Chen, Christos Tsirigotis, and Aaron Courville. \"Convex potential flows: Universal probability distributions with optimal transport and convex optimization.\" arXiv preprint arXiv:2012.05942 (2020).\n\n    [2] Vidal, Alexander, Samy Wu Fung, Luis Tenorio, Stanley Osher, and Levon Nurbekyan. \"Taming hyperparameter tuning in continuous normalizing flows using the JKO scheme.\" Scientific Reports 13, no. 1 (2023): 4501.\n\n    [3] Morel, Guillaume, Lucas Drumetz, Simon Bena\u00efchouche, Nicolas Courty, and Fran\u00e7ois Rousseau. \"Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows.\" arXiv preprint arXiv:2209.10873 (2022).\n\n    [4] Panda, Nishant, Natalie Klein, Dominic Yang, Patrick Gasda, and Diane Oyen. \"Semi-supervised Learning of Pushforwards For Domain Translation & Adaptation.\" arXiv preprint arXiv:2304.08673 (2023).\n\nThank you. We will add the citations in future revisions. Meanwhile, please see our response to the common questions above regarding the difference of our approach with theirs.\n\n3. **Typos: Above equation (5): \"The inner-loop training of r_1 is by\u201d**\n\nThank you. We will fix this in the revised draft in future revisions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5678/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689421653,
                "cdate": 1700689421653,
                "tmdate": 1700689421653,
                "mdate": 1700689421653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Qa86BEjoYj",
            "forum": "i7P2mK3x3o",
            "replyto": "i7P2mK3x3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_3qrZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_3qrZ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors utilize neural ODE to calculate optimal transport mapping in high-dimensional spaces. The proposed Q-flow model can learn a continuous invertible optimal transport. The Q-flow model is trained using a separate continuous-time neural network work classification loss along the time grid. Overall, this paper proposes a simple method to achieve learning optimal transport in high-dimensional space."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method is simple and effective\n2. The writing is good and easy to follow"
                },
                "weaknesses": {
                    "value": "1. How the proposed two loss function satisfies the condition $\\partial_t\\rho+\\nabla\\cdot(\\rho v)=0$ during training.\n2. Does the proposed loss function affect the optimal transport between $P$ and $Q$? Maybe provide proof of achieving optimal transport via these two loss terms.\n3. The author thinks bi-direction flow can achieve better numerical accuracy, but it seems there are no experiments to demonstrate this statement.\n4. Any theoretical proof of bi-direction flow benefit will be better.\n5. How does the KL loss impact the final training results (i.e., if the terminal condition is not considered, how does the final result become)?\n6. No large-scale/high-resolution image generation experiments.\n7. The authors are encouraged to compare their proposal with recent state-of-the-art diffusion based generation methods."
                },
                "questions": {
                    "value": "Refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698417439912,
            "cdate": 1698417439912,
            "tmdate": 1699636592859,
            "mdate": 1699636592859,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vEFA4jDOj5",
                "forum": "i7P2mK3x3o",
                "replyto": "Qa86BEjoYj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3qrZ"
                    },
                    "comment": {
                        "value": "1. **(Continuity equation) How the proposed two loss function satisfies the condition $\\partial_t \\rho+\\nabla \\cdot (\\rho v)=0$ during training.**\n\nThe condition $\\partial_t \\rho+\\nabla \\cdot (\\rho v)=0$ is the continuity equation which relates the density $\\rho(x,t)$ to the velocity field $v(x,t)$. Specifically, if $x(t)\\sim \\rho(x,t)$ and $\\partial_t x(t) = v(x,t)$, then $\\rho(x,t)$ *will* satisfy the continuity equation. Therefore, we do not need to enforce this condition in practice; during training, we only need to focus on the boundary conditions (i.e., $\\rho(x,0)=p(x), \\rho(x,1)=q(x)$).\n\n2. **(Loss to final result) How does the KL loss impact the final training results (i.e., if the terminal condition is not considered, how does the final result become)?**\n\nIf the terminal condition is not considered, the learnt dynamic trajectory would not be between $P_0=P$ and $P_1=Q$, but between $P$ and $\\hat{P}_1$, where $\\hat{P}_1$ can differ from $Q$. In that case, this is different from our original goal of learning the dynamic OT between $P$ and $Q$ based on the BB-formula (see Eq (2)). In practice, as in the case of handbag -> shoes shown in Section 4.5, this would imply a poor match between the true shoes and generated ones, which are translated from handbags by the trained flow.\n\n3. **(High-dim images) No large-scale/high-resolution image generation experiments.**\n\nThank you. We will consider such examples in future revisions, besides the handbags -> shoes example in Section 4.5 on 64x64 images. We also want to emphasize that the main goal of the work is not to perform image generation, but to learn the dynamic OT leveraging the BB formula in Eq (2). Leveraging the learnt OT, we can thus effectively perform many other tasks such as DRE. \n\n4. **(Against diffusion models) The authors are encouraged to compare their proposal with recent state-of-the-art diffusion based generation methods.**\n\nThank you for the suggestion. We want to emphasize that our main goal is not to train a generative model, but to approximate the dynamic OT between arbitrary data distributions $P$ and $Q$. Doing so can be helpful in statistical inference tasks such as DRE. In comparison, diffusion models have reached SOTA performance on generative modeling (where $Q$ is typically the standard Gaussian), but these models are not learning the dynamic OT."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5678/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689292098,
                "cdate": 1700689292098,
                "tmdate": 1700689292098,
                "mdate": 1700689292098,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8AG0drySQ5",
            "forum": "i7P2mK3x3o",
            "replyto": "i7P2mK3x3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_Q3SS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_Q3SS"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a neural-ODE-based approach with min-max optimization for finding the solution of the dynamic optimal transport (OT) with the quadratic cost between two distributions with sample access. By using the flow-based training methodology, they perform optimization of the flow in both directions from the first distribution (initial) to the second (target) and vice versa. The method is applied to the density ratio estimation (DRE) problem on MNIST dataset, demonstrating improved results in comparisons with some baselines [6], [7]. The authors also apply their method to unpaired image-to-image translation on RGB data."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The proposed method outperforms methods [6] and [7] in the DRE problem on MNIST dataset."
                },
                "weaknesses": {
                    "value": "- At the first glance, the authors position their main optimization objective as a minimization problem. However, having understood the paper better, one may realize that their objective actually constitutes the min-max optimization because it uses the variational (discriminator-based) estimation of KL divergence. It seems like the classification net can be viewed as a discriminator (in accordance with the formula (5) and Table 2 of the paper [9]). The trained flow-based network is used as a generator according to expressions (6) and (3). Unfortunately, the authors did not mention this important fact over the paper, which seems a little bit unfair with respect to the reader.\n- The computation of integral of Neural-ODE along learned trajectories lies at the heart of computation inefficiency of the proposed algorithm in accordance with section 3.2. That is, the method is simulation-based.\n- The authors demonstrate improved performance compared to [6],[7]  in the DRE problem in MNIST dataset. However, considering only the gray-scaled dataset MNIST, it is sufficiently difficult to argue that the proposed approach demonstrates significant enough improvement for this problem. So, I think it may be necessary to consider more high-dimensional and color datasets such as Celeba-64  and CIFAR-10 at least. Overall, it seems to me that the methodology for DRE which the authors use is not their method-specific. It seems like the classification network can be learned with any (e.g., trained with some other algorithm) generator. So it is not crystal clear what exactly the experiment on MNIST demonstrates.\n\nGiven the three prior weaknesses above, I wonder what are actual advantages which the current method provides compared to existing methods. For example, neural adversarial OT methods [10,11] are simulation-free. Existing flow-based methods [1],[2],[3],[4] are (usually) not simulation free but have simpler non-adversarial optimization. On top of each of these groups one seems to be able to learn DRE classifier networks. That is, it seems like the method proposed here combines disadvantages of two areas and overcomplicates the training process. So what is the reason to use this method in practice?\n\nAlso there are limited comparisons (both in terms of number of baselines and datasets) both with flow-based methods and adversarial OT methods. In particular, The authors of the article mention in related works 1.1 there are already many flow-based methods [1],[2],[3],[4]. Nonetheless, there are no comparisons with the aforementioned approaches in section 4.2 as well as 4.5. As for adversarial OT methods, there are only quick comparisons with [10] without any qualitatative analysis and only at 64x64 resolution"
                },
                "questions": {
                    "value": "- Why do you support training in both directions ? Which problems do we have while using the training of the flow-based network and classification net along only the forward trajectories ?\n\n- The formula (7) seems to only be an upper-bound for the true Wasserstein-2 distance but not the exact distance, right?\n\n- Since the proposed method is OT-solver with inserted flows, then it seems reasonable to test the approach on the benchmark [5] from the field.\n\n**Papers:**\n\n[1] - \u201cAction matching: Learning Stochastic Dynamics from Samples\u201d,  Neklyudov et al., 2022\n\n[2] - \u201cBuilding normalizing flows with stochastic interpolants\u201d, Michael S. Albergo et al., 2023\n\n[3] - \u201cFlow matching for generative modeling\u201d, Lipman et al., 2022\n\n[4] - \u201cRectified flow: A marginal preserving approach to Optimal transport\u201d, Liu Qiang, 2022\n\n[5] - \u201cDo neural optimal transport solvers work? A continuous Wasserstein -2 benchmark\u201d, Korotin et al., 2021\n\n[6] - \u201cTelescoping Density-Ratio Estimation\u201d, Rhodes et al., 2020\n\n[7] - \u201dDensity Ratio Estimation via Infinitesimal Classification\u201d, Choi et al., 2021\n\n[8] - \u201cDensity Ratio Estimation and Neyman Pearson Classification with Missing Data\u201d, Givens et al., 2023\n\n[9] - \u201cf-GAN: Training Generative Neural Samplers using Variational Divergence Minimization\u201d, Nowozin et al., 2016\n\n[10] - \u201dNeural Optimal Transport\u201d, Korotin et. al., 2022\n\n[11] - \u201cNeural Monge Map Estimation and Applications\u201d, et. al. 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784266332,
            "cdate": 1698784266332,
            "tmdate": 1699636592748,
            "mdate": 1699636592748,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9nbyKfzVGq",
                "forum": "i7P2mK3x3o",
                "replyto": "8AG0drySQ5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Q3SS"
                    },
                    "comment": {
                        "value": "1. **(Implicit variational learning) At the first glance, the authors position their main optimization objective as a minimization problem. However, having understood the paper better, one may realize that their objective actually constitutes the min-max optimization because it uses the variational (discriminator-based) estimation of KL divergence. It seems like the classification net can be viewed as a discriminator (in accordance with the formula (5) and Table 2 of the paper [9]). The trained flow-based network is used as a generator according to expressions (6) and (3). Unfortunately, the authors did not mention this important fact over the paper, which seems a little bit unfair with respect to the reader\u2026[Meanwhile,] existing flow-based methods [1],[2],[3],[4]...have simpler non-adversarial optimization.**\n\n    [1] - \u201cAction matching: Learning Stochastic Dynamics from Samples\u201d, Neklyudov et al., 2022 \n\n    [2] - \u201cBuilding normalizing flows with stochastic interpolants\u201d, Michael S. Albergo et al., 2023 \n\n    [3] - \u201cFlow matching for generative modeling\u201d, Lipman et al., 2022 \n\n    [4] - \u201cRectified flow: A marginal preserving approach to Optimal transport\u201d, Liu Qiang, 2022\n\nThank you for pointing this out. We will make this implicit variational learning clear in future revisions. We agree that compared to [1-4], we have to use a \u201cdiscriminator\u201d to enforce the boundary condition. However, there are essentially differences in terms of the objective and what we can achieve. First, [1-3] learn interpolations between $P$ and $Q$, but such interpolations are not necessarily the (dynamic) OT between $P$ and $Q$. In fact, such an interpolation can be used as the initialized flow for our OT refinement; we have considered using [2] as an initialization strategy in the handbag -> shoe example shown in Section 4.5. Regarding [4], the method can be viewed as an alternative direction descent of the BB formula (see Section 5.4 therein). However, the linear interpolation process in the alternative scheme \u201cis *not* deterministic or ODE-inducible unless the fixed point is achieved\u201d, making it essentially different from the BB approach that focuses on deterministic ODE solutions. In addition, there lacks empirical evaluation of the proposed rectified flow approach.\n\n2. **(Simulation-based approach) The computation of integral of Neural-ODE along learned trajectories lies at the heart of computation inefficiency of the proposed algorithm in accordance with section 3.2. That is, the method is simulation-based**\n\nThank you for pointing this out. We agree that the method is simulation-based. However, this is because we are directly trying to learn the dynamic OT based on the Benamou-Brenier formula (see Eq (2)). Prior works [Liu et al., 2022, Tong et al., 2023] have tried to learn the dynamic OT in a simulation-free manner, but as explained in the answers to the first common question, these approaches either fail to recover the exact deterministic ODE solution [Liu et al., 2022], or the method only works in theory with infinitely large batches [Tong et al., 2023].\n\nReferences:\n\n[Liu 2022] Rectified flow: A marginal preserving approach to Optimal transport]\n\n[Tong et al., 2023] Improving and generalizing flow-based generative models with minibatch optimal transport\n\n3. **(W2 objective) The formula (7) seems to only be an upper-bound for the true Wasserstein-2 distance but not the exact distance, right?**\n\nThank you for the question. As explained in the paper, \u201cthe population form of (7) in minimization can be interpreted as the discrete-time summed (square) Wasserstein-2 distance\u201d, which is between densities $\\rho(\\cdot, t_{k-1})$ and $\\rho(\\cdot, t_k)$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5678/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689208881,
                "cdate": 1700689208881,
                "tmdate": 1700689208881,
                "mdate": 1700689208881,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N9U3z9pZhE",
            "forum": "i7P2mK3x3o",
            "replyto": "i7P2mK3x3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_MAzo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_MAzo"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed an optimal transport flow method to transform images from one distribution to another distribution. The method trains a neural ODE mapping between two distributions. The loss function includes two parts: KL divergence and a Wasserstein-2 regularization, where the KL divergence relies on a pretrained classifier. The paper conducts experiments with toy data and also with real-world images to show that their method can generate high-quality flowed images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper conducts several experiments showing that the flow can generate flow paths between two distributions.\n2. The paper clearly describes the algorithm and the method. The writing is commendable."
                },
                "weaknesses": {
                    "value": "1. In the training algorithm, there are two neural networks r0 and r1. That will add more complexity and difficulty in parameter tuning to the training scheme. It is a bit unclear on if one model is poorly trained, how would that affect the whole flow quality.\n\n2. There are lot of metrics used in the experiment section: mutual information, FID, and BPD. If you can group them in one table or plot, it would be cleaner to compare the methods with all three metrics.\n\n3. We recommend the authors cite the following two recent works on MMD and gradient flow:\nFan, J. and Alvarez-Melis, D., 2023. Generating synthetic datasets by interpolating along generalized geodesics. arXiv preprint arXiv:2306.06866.\n\nHua, X., Nguyen, T., Le, T., Blanchet, J. and Nguyen, V.A., 2023. Dynamic Flows on Curved Space Generated by Labeled Data. arXiv preprint arXiv:2302.00061."
                },
                "questions": {
                    "value": "1. Have you done an ablation study on different loss functions or their weights?\n2. In section 3.1, is it possible to use MMD in the loss instead of KL divergence? \n3. Similar to question 2, is it possible to compute the KL divergence with the images themselves or embeddings of the images?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5678/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5678/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5678/Reviewer_MAzo"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699150906512,
            "cdate": 1699150906512,
            "tmdate": 1699636592653,
            "mdate": 1699636592653,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wJ4buoYtnE",
                "forum": "i7P2mK3x3o",
                "replyto": "N9U3z9pZhE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MAzo"
                    },
                    "comment": {
                        "value": "1. **(Presentation) There are a lot of metrics used in the experiment section: mutual information, FID, and BPD. If you can group them in one table or plot, it would be cleaner to compare the methods with all three metrics.**\n\nThank you for the suggestion. We would do so in the future revisions, which will include additional experiments to verify the method.\n\n2. **(Cite) We recommend the authors cite the following two recent works on MMD and gradient flow:**\nFan, J. and Alvarez-Melis, D., 2023. Generating synthetic datasets by interpolating along generalized geodesics. arXiv preprint arXiv:2306.06866.\nHua, X., Nguyen, T., Le, T., Blanchet, J. and Nguyen, V.A., 2023. Dynamic Flows on Curved Space Generated by Labeled Data. arXiv preprint arXiv:2302.00061.\n\nThank you, we will cite these works in the future.\n\n3. **(Different objective) In section 3.1, is it possible to use MMD in the loss instead of KL divergence?**\n\nThank you for the question. We agree that the MMD objective can be an alternative relaxation of the terminal condition $\\rho(\\cdot, 1) =q$. We will explore the use of this objective in future works.\n\n4. **(Ablation) Have you done an ablation study on different loss functions or their weights?**\n\nWe performed an ablation study on the weight parameter $\\gamma$ in Table A.2, where the DRE performance barely varies across $\\gamma$. We will consider different loss functions (e.g., the MMD loss) in the future.\n\n5. **(Additional evaluation) Similar to question 2, is it possible to compute the KL divergence with the images themselves or embeddings of the images?**\n\nThank you for the question. We can estimate the KL divergence between generated images and true images using Eq (6). Specifically, assume we have trained a classification network $\\hat r_1$ that is close to the optimal classifier $r_1^*$ between $P_1$ and $Q$. Then, the KL divergence between $P_1$ and $Q$ can be computed as $-\\mathbb{E}_{x\\sim P_1} \\hat{r}_1(x)$, where the expectation is approximated by finite samples."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5678/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689038730,
                "cdate": 1700689038730,
                "tmdate": 1700689038730,
                "mdate": 1700689038730,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mhjsUNBfWL",
            "forum": "i7P2mK3x3o",
            "replyto": "i7P2mK3x3o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_f3T8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5678/Reviewer_f3T8"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider a problem of mapping one high-dimensional distribution to another using a flow in continuous time. To train the flow they proposed a loss function consisting of two terms. The first term is KL divergence between the given second distribution and the distribution, resulting from the flow, which takes the first distribution as a starting point. The second term can be interpreted as the discrete-time summed W2 distance. The authors used some existing approach to compute KL divergence and proposed an algorithm to estimate parameters of the flow. They demonstrated on a number of examples that the proposed approach provides good estimate of log density ration, and also can provide nicely-looking flows with good FID values in case of images."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- sufficiently clearly written paper\n- natural idea of the algorithm\n- detailed description of the algorithm and experimental study \n- discussion of the features of the computational implementation of the algorithm\n- interesting practical results"
                },
                "weaknesses": {
                    "value": "- the title of the paper is \"Computing high-dimensional optimal transport by flow neural networks\". However, a significant part of the paper is devoted to benchmarking of the capability of the algorithm to perform density ratio estimation. So it is not clear what is the main aim of the paper - to compute OT, or to estimate log density ratio\n\n- If the main aim is DRE, then it is necessary to provide detailed comparison with other DRE methods, as there are many papers on this topic. E.g., what is the difference of the proposed approach with the approach https://openreview.net/forum?id=kOIaB1hzaLe\n\n- it is not clear why the proposed algorithm estimates optimal transport\n\n- experimental results to verify efficiency of computed W2 high-dimensional optimal transport are not enough to claim accuracy and efficiency of the proposed approach. E.g. the authors consider some image translation tasks, but FID score used to characterise accuracy does not guarantee that the computed W2 high-dimensional optimal transport map is accurate."
                },
                "questions": {
                    "value": "- it is not clear how to tune a value of gamma in (3). Any recipes for automatic tuning?\n\n- page 9: \"Meanwhile, since our Q-flow model learns a continuous transport map from source to target domains, it directly provides the gradual interpolation between the source and target samples along the dynamic OT trajectory as depicted in Figure 4b.\"\n\nAny comments on why the trajectory corresponds to OT trajectory? To construct a flow the authors optimise (3), which contains two terms, and it is not clear why such optimisation formulation guarantees any optimality or that the mapped distribution coincides with the second distribution q. \n\n- since the authors claim they compute W2 optimal transport, it is important to benchmark their approach on problems with ground truth solutions. There exist such benchmark, see https://github.com/iamalexkorotin/Wasserstein2Benchmark (Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark, NeurIPS 2021)\n\n- page 18 (the second line after the displayed formula 15): Why Q = N(0,I_d) if P = N(0,Sigma)?\n\n- page 19: it is not clear how gamma = 0.5 was selected. Why not 0.6?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5678/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699217373273,
            "cdate": 1699217373273,
            "tmdate": 1699636592494,
            "mdate": 1699636592494,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cv37hSO3CD",
                "forum": "i7P2mK3x3o",
                "replyto": "mhjsUNBfWL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5678/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer f3T8"
                    },
                    "comment": {
                        "value": "1. **(contribution, OT or DRE) the title of the paper is \"Computing high-dimensional optimal transport by flow neural networks\". However, a significant part of the paper is devoted to benchmarking the capability of the algorithm to perform density ratio estimation. So it is not clear what is the main aim of the paper - to compute OT, or to estimate log density ratio**\n\nThank you for the question. We intend to present methods that can be useful for both the OT problem and the DRE task. Specifically, we aim to approximate the dynamic OT solution in BB formula (see Eq. (2)) by a flow model, where the learnt continuous OT trajectory enables more accurate DRE. In future revision, we will make our contribution more clear with better experimental design.\n\n2. **(Not showing OT through FID) experimental results to verify efficiency of computed W2 high-dimensional optimal transport are not enough to claim accuracy and efficiency of the proposed approach. E.g. The authors consider some image translation tasks, but the FID score used to characterize accuracy does not guarantee that the computed W2 high-dimensional optimal transport map is accurate.**\n\nThank you for pointing this out. We agree that a low FID score does not indicate an accurate learnt OT trajectory; we considered this example mainly to illustrate the scalability of our approach to non-trivial high-dimensional image datasets. In the future, we will more quantitatively evaluate our learnt OT trajectory, such as through this benchmark [Korotin et al., 2021]\n\nReference:\n\n[Korotin et al., 2021] Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark\n\n3. **($\\gamma$ choice) it is not clear how to tune a value of gamma in (3). Any recipes for automatic tuning? On page 19: it is not clear how gamma = 0.5 was selected. Why not 0.6?**\n\nThank you for the question. At present, we don\u2019t have a recipe for automatic tuning, but find that using $\\gamma=0.5$ is typically a good starting point. We will explore such recipes in the future. Nevertheless, In Table A.2, we conducted an ablation study on the MNIST example with respect to $\\gamma$, where we verified that the performance is not sensitive to the choice of $\\gamma$.\n\n4. **(Clarification) page 18 (the second line after the displayed formula 15): Why Q = N(0,I_d) if P = N(0,Sigma)?**\n\nThank you for the question. This example is taken from [Rhodes et al., 2020, Appendix E]. Specifically, we note that $\\Sigma$ is a block-diagonal matrix with 2x2 blocks. As a result, for $X\\sim N(0,\\Sigma)$, $U=(x_1,x_3,\\ldots,x_{d-1})$ and $V=(x_2,x_d,\\ldots,x_d)$ are random vectors drawn from $N(0,I_{d/2})$. As a result, $p(U)p(V)=Q(x)$ for $Q=N(0,I_d)$.\n\nReference:\n\n[Rhodes et al., 2020] Telescoping Density-Ratio Estimation"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5678/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688885639,
                "cdate": 1700688885639,
                "tmdate": 1700688885639,
                "mdate": 1700688885639,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]