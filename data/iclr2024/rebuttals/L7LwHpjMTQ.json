[
    {
        "title": "CLIP as Multi-Task Multi-Kernel Learning"
    },
    {
        "review": {
            "id": "tupdi7LSza",
            "forum": "L7LwHpjMTQ",
            "replyto": "L7LwHpjMTQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
            ],
            "content": {
                "summary": {
                    "value": "This paper casts the problem of estimating high dimensional neural networks mappings as selecting an unknown Reproducible Kernel Hilbert Space (RKHS) using the optimal solution of a multi-task multiple kernel learning (MTMKL) optimization problem. Under the setting where both number of covariates and the number of candidate kernels increase with the sample size, the authors show an optimal statistical rate of the MTMKL classifier. The proposed method is successfully applied to embeddings of medical imaging data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Casting the high dimensional neural networks optimization problem as multi-task multiple learning optimization problem\n- Thorough theoretical analysis of the optimal estimator"
                },
                "weaknesses": {
                    "value": "- CLIP is not considered in the experiments! This paper should not be sold as CLIP modelling but as a method that use inner-product based objective.\n- Claim that there were no prior theoretical work on multi-task multiple kernel learning. There are multiple work on the subject (this is an old topic) including theoretical analysis. See (Micchelli, C., & Pontil, M. (2004). Kernels for Multi--task Learning. Advances in neural information processing systems, 17.)"
                },
                "questions": {
                    "value": "- I have a big concern about the title of the paper. I do not see why the results in the paper are only specialized to CLIP models instead of general estimating functionals of high dimensional (features embeddings) inner products. In the experimental section, CLIP is barely used! Some embeddings are just extracted. I think if the paper is around CLIP as Multi-task multiple kernel, there should be a whole study on the architecture of CLIP, which to me seems to be a big task. Could the authors please elaborate more on this as this is really confusing to me about the real contribution of the paper? \n-It is mentioned in the Related literature that no theoretical analysis of MKL has been conductied in the multi-task setting. Could the authors justify that as there is a big literature on the subject. For instance: See (Micchelli, C., & Pontil, M. (2004). Kernels for Multi--task Learning. Advances in neural information processing systems, 17.) which has not been cited/analysed. There are other related works. Could the authors elaborate on this?\n- In the experiments section (section 7.2), the authors do not specify which embeddings are used (are they CLIP embeddings?), justifying again my concerns of why CLIP as a motivation of the paper? If so, more detailed experiments should be performed using CLIP embeddings for large-scale image recognition problem (this is a big literature to be considered).\n\n\n------- After rebuttal ---------------------\nThe work conducted by the authors during the rebuttal phase convinced me to raise my score. My original concern was that the paper was focused on proposing a new way of training CLIP-like models but the experimental sections did not compare with respect to the traditional way of training CLIP. The authors have done this comparison in the rebuttal phase and have shown improvements of their MTMK approach over CLIP, and promised to revise the paper to include that as one of the message of the paper by including it as an algorithm for training CLIP."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6786/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6786/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6786/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698419200631,
            "cdate": 1698419200631,
            "tmdate": 1700647052186,
            "mdate": 1700647052186,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "amCfXAop6l",
                "forum": "L7LwHpjMTQ",
                "replyto": "tupdi7LSza",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6786/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6786/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q1: I do not see why the results in the paper are only specialized to CLIP models instead of general estimating functionals of high dimensional (features embeddings) inner products, ..., Could the authors please elaborate more on this as this is really confusing to me about the real contribution of the paper?\n\nThank you for the question. We appreciate the comment about the interpretation of our method beyond CLIP as general estimating functionals of high-dimensional inner products. Nonetheless, the main motivation for the proposed method is to provide an alternative method to solve the CLIP loss and some theoretical understanding of CLIP from the RKHS perspective instead of the neural network. We acknowledge the complexity of thoroughly analyzing the intricate neural architecture of CLIP, which is why we have focused on solving the loss function of CLIP and attempted to give an alternative interpretation of the essence of CLIP via the kernel method. Therefore, we believe our proposed MTMK method should be considered a novel methodological contribution to the CLIP community.\n\n> Q2: It is mentioned in the Related literature that no theoretical analysis of MKL has been conductied in the multi-task setting. Could the authors justify that as there is a big literature on the subject? \n\nWe appreciate your concern and the reference provided. We apologize for the imprecise wording and the lack of citations in our initial submission. While there is indeed a substantial body of literature discussing kernel constructions, optimization, and computation aspects of MKL in the multi-task setting, we intended to highlight the scarcity of works that specifically analyze the optimal learning rate in multi-task multi-kernel learning. The statement \"no theoretical analysis\" refers to the limited research on the analysis of the statistical rate. In the revised version of our paper, we will include a discussion on the general theoretical research line in multi-task multi-kernel learning, along with relevant citations such as [1-6]. We will also ensure that the wording is accurate and precise.\n\n[1] Micchelli, C., & Pontil, M. (2004). Kernels for Multi--task Learning. *Advances in neural information processing systems*, *17*.\n\n[2] Dinuzzo, F., Ong, C. S., Pillonetto, G., & Gehler, P. V. (2011). Learning output kernels with block coordinate descent. In *Proceedings of the 28th International Conference on Machine Learning (ICML-11)* (pp. 49-56).\n\n[3] Ciliberto, C., Mroueh, Y., Poggio, T., & Rosasco, L. (2015, June). Convex learning of multiple tasks and their structure. In *International Conference on Machine Learning* (pp. 1548-1557). PMLR.\n\n[4] Ciliberto, C., Rosasco, L., & Villa, S. (2015). Learning multiple visual tasks while discovering their structure. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (pp. 131-139).\n\n[5] Jawanpuria, P. K., Lapin, M., Hein, M., & Schiele, B. (2015). Efficient output kernel learning for multiple tasks. *Advances in neural information processing systems*, *28*.\n\n[6] Pentina, A., & Ben-David, S. (2015). Multi-task and lifelong learning of kernels. In *Algorithmic Learning Theory: 26th International Conference, ALT 2015, Banff, AB, Canada, October 4-6, 2015, Proceedings 26* (pp. 194-208). Springer International Publishing.\n\n> Q3: In the experiments section (section 7.2), the authors do not specify which embeddings are used (are they CLIP embeddings?), justifying again my concerns of why CLIP as a motivation of the paper?\n\nThank you for the question. We perceive $\\phi(x)$ as the embedding of $x$ and derive the corresponding inner products between samples via the method described in Section 4.  Therefore, the embedding derived for images and phecodes in Section 7.2 corresponds to the estimated mapping of our MTMK method. As for the initial word embeddings for phecodes, we use the ones provided in the MIKGI dataset described in the third paragraph.\n\n> Q4: More CLIP related experiments?\n\nThanks for the question. To further illustrate the performance of the neural network-based CLIP, we will conduct some new experiments to further showcase the empirical performance. We hope the additional results (to be uploaded) resolve your concerns."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6786/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885468066,
                "cdate": 1699885468066,
                "tmdate": 1699885468066,
                "mdate": 1699885468066,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AW9gSJnsuU",
                "forum": "L7LwHpjMTQ",
                "replyto": "amCfXAop6l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for giving clarification on the questions and concerns. I still believe the scope/message of the paper needs to be changed as the story/theory and practical experiments are not really aligned."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6786/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480818333,
                "cdate": 1700480818333,
                "tmdate": 1700480818333,
                "mdate": 1700480818333,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ADbmdW0nv2",
                "forum": "L7LwHpjMTQ",
                "replyto": "wKNyjoqbvQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for performing these additional experiments to show that the proposed method perform better overall compared to the traditional way of training CLIP.\nQuestions:\n1- What is the computational complexity of the MTMK method compared to the training of CLIP you have performed?\n2- I would stress more as message in the paper that you are proposing a new way of training CLIP and a section on how to do that exactly in the form of an algorithm"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6786/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562681001,
                "cdate": 1700562681001,
                "tmdate": 1700562681001,
                "mdate": 1700562681001,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jlzdCQzG9w",
                "forum": "L7LwHpjMTQ",
                "replyto": "1hpfxRU1cp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6786/Reviewer_ry4r"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6786/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585468270,
                "cdate": 1700585468270,
                "tmdate": 1700585468270,
                "mdate": 1700585468270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Dpti1yPSLq",
            "forum": "L7LwHpjMTQ",
            "replyto": "L7LwHpjMTQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6786/Reviewer_F8DR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6786/Reviewer_F8DR"
            ],
            "content": {
                "summary": {
                    "value": "The paper tries to achieve better understanding and analysis of CLIP using reproducing kernel Hilbert spaces (RKHS). CLIP uses a contrastive loss on mapping of input text and image datasets. The paper argues that the objective function of CLIP can be expressed by using kernels in an RKHS. Then the paper proposes to find the best RKHS that maximizes the contrastive loss of CLIP."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Interpreting the training CLIP as a kernel learning problem is interesting. Due to characteristics of kernel functions analyzing kernel-based models is tractable. This perspective offers a promising avenue for enhancing the understanding of CLIP, a fundamental and widely adopted model.\n2. The paper conducted experiments on real datasets to support the theoretical analysis."
                },
                "weaknesses": {
                    "value": "1. It seems that the goal of the paper is to give readers of better understanding on how to train a better CLIP model. It is more common that neural network models are employed. However, I did not find any discussion in the paper that if their analysis provides some intuition on how  train a better CLIP. Therefore, the contribution of this paper is not super clear to me.\n2. The methods used in this paper is too heuristics. In case of using neural networks, deep neural network models can be interpreted as NTK using some heuristics while in order to train the contrastive loss using kernel learning, the paper performs some relaxations in the objective function.\n3. Experimental section can be improved by adding more datasets and baselines."
                },
                "questions": {
                    "value": "Please see weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6786/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839137316,
            "cdate": 1698839137316,
            "tmdate": 1699636783618,
            "mdate": 1699636783618,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qKmetM4vIM",
                "forum": "L7LwHpjMTQ",
                "replyto": "Dpti1yPSLq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6786/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6786/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the constructive comments, we address the concerns below.\n\n> Q1: How do we interpret the contribution of this work?\n\nThank you for the question. We expect that our work offers some theoretical understanding of CLIP from the RKHS perspective rather than the standard neural network perspective. In essence, we do not aim at proposing a new method to train a better CLIP model. Instead, we attempt to provide an alternative method to solve the CLIP loss, which is parallel to the standard backpropagation training scheme from a higher level. Therefore, we believe our proposed MTMK method should be considered a novel methodological contribution to the CLIP community instead of a remedy to the existing limitations of CLIP.\n\n> Q2: The methods used in this paper is too heuristics.\n\nWe appreciate the reviewer for bringing up this concern. We acknowledge that there is a certain degree of relaxation between the original loss and the objective we have derived. However, we argue that such relaxation is necessary to statistically characterize the complexity and learning rate of the model, as presented in Section 6.\n\n> Q3: Experimental section can be improved by adding more datasets and baselines.\n\nWe appreciate your suggestion on improving the numerical evaluation. However, we would like to emphasize that this work mainly focuses on a theoretical interpretation of CLIP rather than proposing a new method. As CLIP has demonstrated excellent generalizability in broader domains, we believe our work can be extended to various tasks as CLIP. To further illustrate the performance of the neural network-based CLIP, we will conduct some new experiments to further showcase the empirical performance. We hope the additional results (to be uploaded) resolve your concerns."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6786/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885252991,
                "cdate": 1699885252991,
                "tmdate": 1699885252991,
                "mdate": 1699885252991,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]