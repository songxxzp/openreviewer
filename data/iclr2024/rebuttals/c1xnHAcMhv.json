[
    {
        "title": "Generating Less Certain Adversarial Examples Improves Robust Generalization"
    },
    {
        "review": {
            "id": "0moXZLjwuo",
            "forum": "c1xnHAcMhv",
            "replyto": "c1xnHAcMhv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission587/Reviewer_4Bie"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission587/Reviewer_4Bie"
            ],
            "content": {
                "summary": {
                    "value": "This paper present an empirical defense against adversarial (evasion) attacks on image classifiers. The authors build their defense on the premise of improving the overfitting issue experienced during adversarial training. They propose that this overfitting occurs because high confidence adversarial examples are added in the training data of the model during adversarial training. They propose a metric to measure the confidence level of adversarial examples and develop a training method to force the model to learn weights that yield low confidence adversarial examples. By doing so, they reduce the generalization gap in robustness obtained through adversarial training (and other similar methods). Models trained using the proposed method exhibit higher clean and adversarial accuracy than baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors provide novel evidence with regards to the correlation between confidence on adversarial examples and robustness generalization gap: the higher the confidence, the bigger the generalization gap.\n2. Using the above insight, authors devise a novel method for improving the robustness generalization gap. Their method measure the variance of the output distribution across all labels and aims to reduce it during adversarial training.\n3. The proposed method can be combined with any defense method that makes use of the strategy of training on adversarial examples.\n4. The design choice of the proposed method is adequately justified. While reading the paper, one of the things that came to my mind was that the method might be better implemented as a regularization term. I appreciate that the authors compared this obvious design choice with their chosen design, and demonstrated that it works better."
                },
                "weaknesses": {
                    "value": "1. The writing quality of the paper can be improved. It was hard to read through the paper and grasp the information being conveyed, majorly because of awkwardly worded or vague sentences. I highly recommend authors to undertake a major re-writing of the paper and convey the important insights as well as method design discussions in a more articulate manner.\n\n2. The jump from label-level variance metric to adversarial certainity metric is abrupt. The motivation for the adversarial certainity metric is lacking. Ease of optimization is understandable, however it is unclear why one would compute variance using logits instead of softmax outputs.\n\n3. Potential issue with reproducing results using TRADES. The AutoAttack paper reports robustness against AA as 53.08% (for the last checkpoint made publicly available by TRADES authors) while in this paper this number is reported as 46.53%. Therefore, either the model was incorrectly trained or AA wasn't correctly executed. It is important to figure out the source of this discerpancy.\n\n4. No discussion regarding computational overhead of the proposed method. It appears that the proposed method doubles the number of times adversarial examples must be generated during training. Since the generation of adversarial examples is the bottleneck process in adversarial training, it appears that the proposed method comes at a huge cost to computational efficiency. One must ask then, whether the improvements in clean and robust accuracy are worth it? From a research perspective, the results may be intersting but the proposed method seems to be infeasible in real world settings.\n\n5. No discussion regarding adaptive attacks. An adaptive adversary would try to generate low confidence adversarial inputs by including both adversarial certainity and cross-entropy loss in the attack objective. Such an attack has not been discussed in the paper at all. Another effective strategy for an adaptive attack would be to find an adversarial example that matches the latent space feature representations (eg, penultimate layer) of an input from another class (see [a] for more details). Overall, I suspect that because the proposed method 'flattens' the output space, the gradient based attacks might be failing to be effective.\n\n6. The authors haven't taken adequate steps to present the improvements introduced by their method as statistically significant figures. This would entail running PGD/CW/AutoAttack with multiple random restarts (20 to 50), or running the same attack multiple times (using different random inits) and reporting the mean and 95% confidence interval across all runs.\n\n7. Comparison with closely related prior work (Setlur et al., 2022) is missing. The method proposed by this prior work and the one proposed in this paper are based on the same premise: perform regularization (or, reduce overfitting) by reducing confidence on (or, along the direction of) adversarial examples. Both these methods can be combined with prior adversarial training based methods. Therefore, it is very important to compare the proposed method with this prior work, across a range of adversarial training methods.\n\n**Writing Issues**\n1. Distillation is not a secure defense, hence listing it as one (in intro, para 2, line 2) is improper.\n2. In intro, 2nd para, authors list several (dissimilar) defenses and then say that adversarial training is the most effective among them all. This statement is not technically sound. For example, randomized smoothing is a fundamentally different defense than adversarial training in terms of the nature of robustness conferred (certified vs. empirical robust). As such, the robustness of models trained using these two methods can not be directly compared.\n3. Ambiguous statement in intro, 2nd para \"sota adversarial training methods can not achieve satisfactory robustness performance\", what does \"satisfactory\" mean here?\n4. Typo fig 2(a), y axis label: \"Label-Lavel\" => \"Label-Level\"\n\n**References**\n\n[a] Sabour, S., Cao, Y., Faghri, F., and Fleet, D. J. Adversarial manipulation of deep representations. International Conference on Learning Representations, 2016."
                },
                "questions": {
                    "value": "1. The AutoAttack paper has evaluated prior defenses using publicly available checkpoints. In case of TRADES, they have used the WRN34 checkpoint at end of 100 epochs (last checkpoint). For this checkpoint, they report accuracy of 53.08% against AA. However, for the same model/checkpoint, authors report 46.53%. What is the cause of this discrepancy?\n\n2. Seems like the proposed method has poor computational efficiency as compared to adversarial training (which already has very poor computational efficiency). Luckily, there are methods that significantly imporve the computational efficiency of adversarial training [b,c]. Based on the content of the paper, it appears that the proposed method should work in conjunction with these methods too. But are there any results to indicate how effective the combination will be? I highly recommend adding these results (even if in appendix) in a future iteration of the paper.\n\n3. What happens if the the adversary tries to generate an adversarial input that maintains low confidence while trying to fool the classifier, i.e., include both adversarial certainity and cross entropy loss in the attack objective. This would be the most straight forward adaptive attack.\n\n**References**\n\n[b] Wong, Eric, Leslie Rice, and J. Zico Kolter. \"Fast is better than free: Revisiting adversarial training.\" International Conference on Learning Representations. 2019.\n\n[c] Shafahi, Ali, et al. \"Adversarial training for free!.\" Advances in Neural Information Processing Systems 32 (2019)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission587/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610893010,
            "cdate": 1698610893010,
            "tmdate": 1699635986468,
            "mdate": 1699635986468,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "4HcKDQ9YgT",
            "forum": "c1xnHAcMhv",
            "replyto": "c1xnHAcMhv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission587/Reviewer_v7sc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission587/Reviewer_v7sc"
            ],
            "content": {
                "summary": {
                    "value": "This work finds that models exhibit superior robust generalization performance have more even predicted label distributions. Then, they introduce an adversarial training method that uses less certain examples by reducing adversarial certainty. They provide some experimental results to verify the efficacy of their approach in addressing robust overfitting problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Easy to follow, clear presentation.\n- Numerous empirical results."
                },
                "weaknesses": {
                    "value": "- The analysis about even label distribution is limited, e.g., this work does not provide some theoretical analyses.\n- There are numerous experiments, however, I cannot find some impressive empirical results, especially under auto attack. ref: https://arxiv.org/abs/2302.04638; https://arxiv.org/abs/2202.10103; https://arxiv.org/abs/2004.05884\n- All in all, I think this is a borderline paper, it provides a story but the empirical results and the motivation analysis are limited."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission587/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760444208,
            "cdate": 1698760444208,
            "tmdate": 1699635986320,
            "mdate": 1699635986320,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "gGwY6b3HAX",
            "forum": "c1xnHAcMhv",
            "replyto": "c1xnHAcMhv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission587/Reviewer_whj7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission587/Reviewer_whj7"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies on the robust overfitting phenomenon. Specifically, adversarially trained models overfit to the training attacks and fail to generalize during testing. The authors reproduce this phenomenon and propose a novel metric called \"Adversarial Certainty\" to measure how severe the overfitting / overconfidence is. The authors claim that minimizing Adversarial Certainty helps the model generalization and introduce an extra-gradient-like algorithm to minimize Adversarial Certainty during training. Finally, experiments show the proposed method outperforms state-of-the-art. Adversarial Certainty metric is also verified consistent with the claim. In general, this paper is straightforward and easy to follow."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The introduction of \"Adversarial Certainty\" is novel. The authors conduct through experiments to show the correlation between \"Adversarial Certainty\" and \"generalization at testing\". \n\n* The idea of minimizing \"Adversarial Certainty\" is interesting and looks promising given the extensive experiments on CIFAR-10."
                },
                "weaknesses": {
                    "value": "**Technical:**\n\nThe main weakness of this paper is that the algorithm and the following discussions are based on intuitive explanations and are lack of solid support. The authors do provide some support but I am afraid that they are not convincing enough. Specifically there are two questions:\n* Does lower *Adversarial Certainty* imply better robust generalization?\n* Does the proposed algorithm indeed solves the problem (2)? Does it have anything to do with extra-gradient? \n\nRegarding the first question, the authors provide experimental evidence and intuitive justifications. I think this argument my hold after a long period of training but may not be true at any time. The reason is that the definition of *Adversarial Certainty* does not have label $y$ involves but the definition of *robust generalization* has. In other words, think of the extreme case in which we have a random classifier. The *Adversarial Certainty* is very low but *robust generalization* is low as well. Furthermore, Figure 3 (b) shows the trends that when *Adversarial Certainty* gets very small, *robust generalization* starts to decrease. Think of the case at the very beginning of training in which the *Adversarial Certainty* is not very large. Minimizing the *Adversarial Certainty* at the moment might not be beneficial.\n\nRegarding the second question, I am not convinced that proposed algorithm (3) solves problem (2). I understand that (2) is very difficult to solve due to its non-convex and non-concave nature. However even under the sense of approximation, I am not sure how (3) approximates  (2) or whether (3) indeed minimizes *Adversarial Certainty*. The following discussions are intuition based and are not rigorous. For example, the authors claim that *Adversarial Certainty* is decrease after the first step. This is reasonable but how about the second step? How can we show that *Adversarial Certainty* is not increased during the section step? Another question is that, how (3) is related to extra gradient? Both steps in (3) use different objective functions and the second step update is based on $\\theta_{t+0.5}$.\n\n**Some other issues:**\n* In the last paragraph of page 7, I guess I may not understand the setup. Why not just run AT and AT-EDAC and report *Adversarial Certainty* at each epoch?\n* The study of *Alternative Regularization Method* on page 8 is actually very interesting. What is the exact formulation of the regularization term? It seems that the performance gap between EDAC and EDAC_reg is marginal, which strengthens my concern of (3). Then why not simply use regularization?\n\n**Writing:**\n\nAlthough the paper in general is straightforward and easy to follow, I believe the presentation can be greatly improved.\n* There are a bunch of missing definitions, e.g. AT on page 5, $L_\\mbox{rob}$ on page 5. They are actually defined later but make the reader confusing when they first appear without being defined.\n* In paragraph 2, the relegation of definitions make it difficult to follow. Important definitions such as *robust generalization* should remain in the body.\n* There are lots of unnecessarily long sentences making it difficult to read, e.g. first sentence in the second paragraph of section 2 is four-line long.\n\n**Summary**\n* The idea that minimizing *Adversarial Certainty* is interesting, novel and seems promising. The motivation of two steps update of (3) is not  well explained. The relation between (3) and extra gradient in not clear. Given the similar performance of EDAC_reg, the benefit of (3) is unknown. The presentation can be improved by clearing up the definitions and making the explanations succinct."
                },
                "questions": {
                    "value": "Please see above section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission587/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699264173025,
            "cdate": 1699264173025,
            "tmdate": 1699635986233,
            "mdate": 1699635986233,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "tcisH7sLta",
            "forum": "c1xnHAcMhv",
            "replyto": "c1xnHAcMhv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission587/Reviewer_QZ4H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission587/Reviewer_QZ4H"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the concept of robust overfitting, where DNNs overfit when trained on adversarial data, after an initial increase in adversarial robustness. The concept was first found in Rice et al., 2020 - and this paper tries to mitigate it by understanding it through the lens of adversarial uncertainty - i.e the probability of the model on adversarial inputs. They find that the model is overconfident on adversarial inputs at training time, but not on those at test time - which indicates miscalibration of the underlying adversarially trained model. This is then mitigated by adding an extra gradient step to generate inputs with low adversarial certainty during training time to avoid overfitting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The hypothesis that decreasing the gap in certainty of train vs test-time adversarial examples is tested empirically, and across different regularization and adversarial training baselines."
                },
                "weaknesses": {
                    "value": "* The choice of step size is done by optimizing for test-time robustness, and not on robustness metrics on a validation split. Optimizing hyper-parameters based on test-time metrics is not acceptable in ML model selection, and a validation split evaluation is warranted. This also brings into question the soundness of results presented in Tables 1-3, as the step size impacts the test-time robustness quite a bit per Fig 3b\n* Constrained search space of the initial parameter \\theta\u2019 is not well justified as to what the search space is minimizing - there seems to be a trade-off between avoiding overfitting, and optimizing for low Adversarial certainty - which is not validated -but only an intuition is provided based on correlation analysis between train and test time measures.\n* Confidence intervals in the Tables are not presented - which makes it harder to understand if these results are statistically significant. For example, in the results section, the claim of \u201cIn general, the improvement of EDAC on AWP and Consistency is not as significant as previous ones\u201d cannot be verified.\n\nIf the above points around soundness are clarified, I'm willing to increase the score."
                },
                "questions": {
                    "value": "* Notations L, B in Eqn 2, L_{rob} in Eqn 3 are not defined.\n* Numbers in brackets in Table 1 are not explained\n* Figure 5 is missing EDAC versions of the methods - would be helpful to see how the proposed method helps."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission587/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699286136156,
            "cdate": 1699286136156,
            "tmdate": 1699635986164,
            "mdate": 1699635986164,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "CVtHahgZ3G",
            "forum": "c1xnHAcMhv",
            "replyto": "c1xnHAcMhv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission587/Reviewer_aLt2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission587/Reviewer_aLt2"
            ],
            "content": {
                "summary": {
                    "value": "GOAL:\nInvestigate the \u201crobust overfitting phenomenon\u201d\nAfter Adversarial Training, the new robust model outputs a more flat distribution of labels compared to non-robust models. It overfits the difficult adversarial data at the expense of the in distribution data.\n\nMETHOD:\nadd an  extragradient step in the adversarial training framework to search for models that can generate adversarially perturbed inputs with lower certainty\n\nContributions:\n1. Observe that the label distributions predicted on train time adversarial examples are:\n- more even (flat?) for \u201cmodels with better robust generalization ability\u201d\n- over confident in predicting labels for models produced by adversarial training\n\n2. Introduce the concept, \u201cadversarial certainty\u201d: \n- Defined in Eq 1, as what\u2019s the variance in the final logit layer of a NN, when we do an epsilon bounded attack on an input.\n- Describe how does this concept relate to other concepts around robustness (4.2) such as AWP. The main difference between those two is that EDAC contains a half step where the model params are updated to make it easier to reduce adversarial certainty.\n\n3. Propose EDAC method for adversarial training. Procedure: \n- Minimize (delta adversarial certainty / delta model weights) by a two step optimization: first adjust NN weights to decrease adversarial certainty on the training set, and then the second step tries to do a typical loss where you try to still pick the correct answer.\n- They generated adversarial examples during training that are less certain\n- These adversarial examples with low certainty are used to optimize robustness\n- End: Generalization gap between train and test sets is lower"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. EDAC method can be combined with other adversarial training methods.\n2. Figure 1 provides an quick and easy intuitive understanding of the concept. I appreciate the use of the heat maps."
                },
                "weaknesses": {
                    "value": "In this section, I'll first justify my overall negative rating and then list out specific weaknesses.\n\nOVERALL:\nI am not sure that the adversarial robustness angle to the problem changes the core problem of overfitting to training data, and thus not doing so well on the test data. Some of the stated contributions (like showing that high adversarial uncertainty led to better test set performance, or the correlation of the two) were not fully achieved, and have been achieved in the past AWP paper. (Based on my understanding, I'm arguing that AWP's notion of flatness does not materially differ from \"adversarial certainty\" introduced in this paper). Finally, the empirical results do not hold up: EDAC is not better than AWP, and a clear head to head comparison is missing. It's also not clear from Table 3, that EDAC marginally improves AWP performance (you would need confidence intervals because the improvements are so slim).\n\n\nSPECIFIC WEAKNESSES:\n1. A contribution listed was to show that flatness in the logit distribution leads to better generalization. But that\u2019s been shown before in the AWP paper you cite. I\u2019m still not very clear on the difference between AWP and this method: Is it that you generate the adversarial examples after that intermediate step of adjusting the model parameters to themselves increase uncertainty?\n\n2. If I\u2019m understanding the Section 2, subheading, \u201cOur perspective.\u201d you\u2019re arguing that the larger gap between train and test performance on a given adversarial attack limits adversarial robust generalization -- which is defined as the performance against that adversarial attack on a held out set of data. That is a tautology. \n\n3. Also in the same section: I think what you\u2019re trying to get at is to state your hypothesis of  \u201cbecause the model cannot generate perturbed training inputs with low enough certainty, which is different from the typical generated adversarial examples during testing time.\u201d But I\u2019m also not really sure what that sentence means. I take it that there are examples that are \u201cmore certain\u201d (less logit variance) and \u201cless certain,\u201d (more logit variance), and we do worse on the test when it is all \u201cless certain\u201d since we haven\u2019t seen it yet, so we need to explore \u201cless certain examples?\u201d Is that just arguing for active learning by generating more data from a given attack (i.e if we get more examples of the attack where we are right now a bit confused on the label, then we will do better on the test set?\u201d). Because that makes sense, but feels a bit obvious. Further, I don\u2019t understand what the adversarial attack has to do with it -- it would be true of any data generation process. In which case, I don\u2019t know that it\u2019s a significant advancement from other papers that use extra gradient steps? ( I guess, what about the adversarial robustness makes this problem different from any overfitting/generalization problem?)\n\n4. Figure 3 doesn\u2019t show a correlation. I\u2019d want a plot that plots adversarial certainty against robustness accuracy (with and without EDAC) -- or a summary metric. I don\u2019t really see that information presented for the  \u201cwithout EDAC\u201d. Also, you don\u2019t discuss the trend in 3b where the orange line of test set performance is going down as adversarial certainty further decreases. (In fact the AWP paper which tries to make a very similar claim doesn't claim outright correlatio, noting: \"Therefore, a flatter weight loss landscape does directly lead to a smaller robust generalization gap but is only beneficial to the final test robustness on condition that the training process is sufficient (i.e., training robustness is high)\")\n\n5. Table 3: The improvements are very slim (esp. against AWP), such that I\u2019m not sure what\u2019s a significant difference. Confidence intervals needed to conclude that EDAC does have a positive cumulative effect.\n\n6. Table 2 and 3: In fact when you compare row 2 of Table 2 (EDAC only) vs row 1 of Table 3 (AWP only), AWP does very significantly better. (As an aside, please include these head to head comparisons of different methods more clearly).\n\n7. The actual sentence writing is quite hard to parse, and significantly hurts my understanding of the work.\n ex. This sentence in the abstract should be multiple sentences.\n\u201c...t overconfident models produced during adversarial training could be a potential cause, supported by the empirical observation that the predicted labels of adversarial examples generated by models with better robust generalization ability tend to have significantly more even distributions. \u201d\nOR \n\u201cEDAC first finds the steepest descent direction of model weights to decrease adversarial certainty, aiming to generate less certain adversarial examples during training, then the newly generated adversarial examples with lower certainty are used to optimize model robustness.\u201d\nOR\n\u201cBesides, we investigate the cooperation of adversarial certainty with other helpful insights for model robustness, where our method shows further generalizability by compatibly improving them (Section 4.2).\u201d"
                },
                "questions": {
                    "value": "1. What about the *adversarial robustness* makes this problem different from any overfitting/generalization problem that uses extra gradient steps/awp?\n\n\n2. What is the criteria for selecting \u201cbest model\u201d in Fig 1? Is it the one with the best test set performance?\n3. In Figure 2, which is on the train set as well, how does the best model have *lower* label variance? I thought the Best model is the one with less confidence in the label (Fig 1c) and should therefore be more varied in the labels it predicts on the train set?\n4. Eq 2: How do you know what C(\\theta) is? What defines a feasible search region? ex: Is this some meta parameter you use to enforce normed distance?\n5. Eq 2: What is B_{epsilon}(x) on the left side under tha max? You don\u2019t define it I think, but for now I\u2019m presuming that\u2019s the perturbation for x? Is it the attack?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical concerns that are not relevant to all generalization studies."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission587/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission587/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission587/Reviewer_aLt2"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission587/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699492722827,
            "cdate": 1699492722827,
            "tmdate": 1699635986061,
            "mdate": 1699635986061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]