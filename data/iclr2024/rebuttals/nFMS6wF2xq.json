[
    {
        "title": "Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing"
    },
    {
        "review": {
            "id": "3ABuKQfRop",
            "forum": "nFMS6wF2xq",
            "replyto": "nFMS6wF2xq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2579/Reviewer_YRdg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2579/Reviewer_YRdg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a contextualized diffusion model ContextDiff to facilitate the learning capacity of cross-modal diffusion models. It incorporates the cross-modal interactions between text condition and visual sample into both forward and reverse processes, serving as a context-aware adapter to optimize diffusion trajectories. It is also generalized to both DDPMs and DDIMs for benefiting both cross-modal generation and editing tasks with detailed theoretical derivations. Experiments in text-to-image generation and text-to-video editing tasks show its effectiveness. Empirical results reveal that it can successfully improve the semantic alignment between text conditions and synthesis results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This paper for the first time proposes the text-guided visual diffusion model to consider cross-modal interactions in both forwarding and sampling processes.\n\n2) The authors propose their contextualized diffusion model (ContextDiff) and generalize it to DDPMs and DDIMs through the derivations of theoretical formulas.\n\n3) The experiments show that the proposed method has improvements on two tasks: T2I generation and T2V editing, compared with other state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "1) This paper claims the problem that neglecting the cross-model context in the forward process may limit the expression of textual semantics in synthesis results, but there is no clear explanation of the specific reasons, and there is no intuitive and theoretical analysis of the necessity of adding cross-model to the forward process.\n\n2) As claimed by the authors: \u201cThus CONTEXTDIFF is theoretically capable of achieving better likelihood compared to original DDPMs\u201d. Please provide the quantitative results on ELBO/ likelihood compared to the baseline.\n\n3) Since this paper is a general improvement on the conditional diffusion model, more results on different conditional generation tasks, such as class-to-image/layout-to-image\u2026should be provided. \n\n4) Some configurations in T2V editing experiments are confusing, as the experiments based on pre-trained Stable Diffusion v1.4 are not enough to prove that the approach of this paper can enable diffusion models better editing ability."
                },
                "questions": {
                    "value": "1) Are there any more experiments that can prove the text-based editing ability of this approach, for example, conducting T2I editing or T2V editing based on the well-trained T2I generation model of your first experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698219383590,
            "cdate": 1698219383590,
            "tmdate": 1699636195369,
            "mdate": 1699636195369,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BCihpzW4nO",
                "forum": "nFMS6wF2xq",
                "replyto": "3ABuKQfRop",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YRdg (Part 1/2)"
                    },
                    "comment": {
                        "value": "*We thank Reviewer YRdg for the positive review and valuable feedback. We are glad that the reviewer found that the proposed method is innovative, the method has theoretical foundations, and the proposed method consistently improves T2I and T2V tasks. Please see below for our responses to your comments, and **the changes in revised manuscript are marked in blue**.*\n\n**Q1: Intuitive and theoretical analysis of the necessity of adding cross-modal context to the forward process**\n\nA1: \n\n**Qualitative Analysis**: We investigate how our adapter enhances multimodal semantic relevance in the **Figure 8 and Figure 9, marked in blue**. We visualize the heatmaps of text-image cross-attention module in the sampling process of each frame image. We find that the image latents refined by our adapter better attend to the fine-grained semantics in text and sufficiently convey them for generation.\n\n**Theoretical Analysis**:\nWe provide an in-depth analysis on why ContextDiff can better express the cross-modal semantics. Based on objective function in Equation (8), the optimal solution of ContextDiff at time t can be expressed as \n$$\\begin{array}{rl}& \\arg\\min_{\\phi,\\theta}E_{q_\\phi(x_t,x_0|c)}||x_0-f_\\theta(x_t,c)||^2\\\\\\\\\n&=\\arg\\min_{\\phi}E_{q_\\phi(x_t|c)}E_{q_\\phi(x_0|x_t,c)}||x_0-E[x_0|x_t,c]||_2^2 \\\\\\\\\n& = \\phi^*,\\theta^* \\\\\\\\\n\\end{array}$$\n\n\n,since the best estimator under L2 loss is the conditional expectation. As a result, the optimal estimator of ContextDiff for $x_0$ is \n$$E[x_0|k_t r_{\\phi^*}(x_0,c)+\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, c],$$\n\nwhile existing methods that did not incorporate cross-modal contextual information in the forward process have the following optimal estimator:\n$$E[x_0|\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, c].$$\n\nCompared with existing methods, ContextDiff can explicitly utilize the cross-modal relation $r_{\\phi^*}(x_0,c)$ to optimally recover the ground truth sample, and thus achieve better multimodal semantic alignment. \n\nFurthermore, we analyze a toy example to show that ContextDiff can indeed utilize the cross-modal context to better recover the ground truth sample. We consider the image embedding $x_0$ and text embedding $c$, which are generated with the following mechanism:\n$$x_0 = \\mu(c)+\\sigma(c)\\epsilon,$$\nwhere $\\epsilon$ is an independent standard gaussain, $\\mu(c)$ and $\\sigma^2(c)$ are the mean and variance of $x_0$ conditioned on $c$. We believe this simple model can capture the multimodal relationships in the embedding space, where the relevant images and text embeddings are closely aligned with each other. Then $x_t = \\sqrt{\\bar{\\alpha}_t} x_0+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon^{'}$ is the noisy image embedding in original diffusion model. We aim to calculate and compare the optimal estimation error in original diffusion model and in ContextDiff:\n\n $$\\begin{aligned}&min_\\theta E_{q(x_0,x_t|c)}||x_0-f_\\theta(x_0,c)||_2^2 \\\\\\\\\n &=E ||x_0-E[x_0|x_t,c]||_2^2 \\end{aligned}$$\n\nThe conditional expectation as the optimal estimator of DDPMs can be calculated as\n$$\\begin{aligned}E[x_0|x_t,c]\n&= \\mu(c) - Cov(x_0,x_t|c)*Var(x_t|c)^{-1}(\\sqrt{\\bar{\\alpha}_t}\\mu(c)-x_t)\\\\\\\\\n&= \\mu(c) - \\frac{\\sqrt{\\bar{\\alpha}_t}\\sigma(c)^2}{\\bar{\\alpha}_t\\sigma(c)^2+1-\\bar{\\alpha}_t}(\\sqrt{\\bar{\\alpha}_t}\\mu(c)-x_t) \\end{aligned}$$\n\nAs a result, we can calculate the estimation error of DDPMs:\n\n$$\\begin{array}{rl} &E||x_0-E[x_0|x_t,c]||_2^2 \\\\\\\\\n&=E||\\sigma(c)\\epsilon-\\frac{\\sqrt{\\bar{\\alpha}_t}\\sigma(c)^2}{\\bar{\\alpha}_t\\sigma(c)^2+1-\\bar{\\alpha}_t}(\\sqrt{\\bar{\\alpha}_t}\\sigma(c)\\epsilon+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon^{'})||_2^2\\\\\\\\\n& = d*\\frac{(1- \\bar{\\alpha}_t)\\bar{\\alpha}_t\\sigma(c)^4+ \\sigma^2(c)(1-\\bar{\\alpha}_t)^2}{(\\bar{\\alpha}_t \\sigma^2(c)+1- \\bar{\\alpha}_t)^2}\\\\\\\\\n& = d *\\sigma(c)^2 \\frac{1- \\bar{\\alpha}_t}{\\bar{\\alpha}_t \\sigma^2(c)+1- \\bar{\\alpha}_t}\n \\end{array}$$\nNow we use CONTEXTDIFF with a parameterized adapter : $x_t = \\sqrt{\\bar{\\alpha}_t} x_0+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon^{'}+r(\\phi,c,t)x_0$\n\n , where $r(\\phi,c,t)x_0$ is the adapter. We can similarly calculate the conditional mean as the optimal estimator of ContextDiff:\n $$E_\\phi[x_0|x_t,c] = \\mu(c)-\\frac{\\sigma^2(c)(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})}{1-\\bar{\\alpha}_t+(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})^2\\sigma^2}*((r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})\\mu(c)-x_t)$$\n And the estimation error for a given $\\phi$ in ContextDiff is:\n\n$$\\begin{array}{rl}\n&E||x_0-E_\\phi[x_0|x_t,c]||_2^2\\\\\\\\\n&=E||\\sigma(c)\\epsilon-\\frac{\\sigma^2(c)(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})}{1-\\bar{\\alpha}_t+(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})^2\\sigma^2(c)}((r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})\\sigma(c)\\epsilon+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon^{'})||_2^2\\\\\\\\\n&= d \\sigma(c)^2\\frac{1-\\bar{\\alpha}_t}{1-\\bar{\\alpha}_t+(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})^2\\sigma^2(c)}\n \\end{array}$$\n\nComparing the denominators of two estimation errors, we can see that using a non-negative adapter will always reduce the estimation error."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121553394,
                "cdate": 1700121553394,
                "tmdate": 1700121553394,
                "mdate": 1700121553394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PtBBkT6aEx",
                "forum": "nFMS6wF2xq",
                "replyto": "3ABuKQfRop",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YRdg (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Q2: Provide the quantitative results on ELBO/ likelihood compared to the baseline**\n\nA2: We compute NLL (Negaitve Log-Likelihood) for performances of density estimation with Bits Per Dimension (BPD). We train our ContextDiff on CIFAR-10 from scratch and use the same training steps as DDPM, then compute NLL with the uniform dequantization. The results are in the table below, we conclude that our method is empirically capable of achieving better likelihood compared to original DDPMs.\n\n| Method |NLL $\\downarrow$| \n| :-----| :----: | \n|DDPM|3.01|\n|DDPM + Context-Aware Adapter|**2.63**|\n\n**Q3: More results on different conditional generation tasks, such as class-to-image/layout-to-image generation**\n\nA3: We generalize our context-aware adapter into class and layout conditional generation tasks. We replace the text encoder in original adapter with ResNet blocks for embedding classes or layouts, and keep the original image encoder and cross-attention module for obtaining cross-modal context information. We put both quantitative (**Table 3 and Table 4**) and qualitative results (**Figure 10 and Figure 11**) **in the updated manuscript marked in blue**. From the results, we conclude that our context-aware adapter can benefit the conditional diffusion models with different condition modalities and enable more realistic and precise generation consistent with input conditions, demonstrating the satisfying generalization ability of our method. \n\n**Q4: Are there any more experiments that can prove the text-based editing ability of this approach?**\n\nA4: In Section 5.2, we have demonstrate that our context-aware adapter can improve video diffusion models in **text-guided video editing tasks**. Kindly note that existing diffusion-based T2V editing methods (Tune-A-Video [1] and FateZero [2]) are mainly based on a pretrained T2I model (Stable Diffusion v1.4/1.5 or higher) for preliminary appearance generation ability, and they try to develop spatio-temporal approaches to improve the semantic precision of edited frames and the temporal consistency between frames. We base on pretrained Stable Diffusion v1.4 just **for fair comparison**, and both quantitative and qualitative results have already proved the text-based editing ability of our approach. For more insights, the **heatmap visualization results in Figure 8 and Figure 9** of the updated manuscript further show and explain our superior editing ability.\n\n[1] Wu J Z, Ge Y, Wang X, et al. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In IEEE International Conference on Computer Vision. 2023.\n\n[2] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In IEEE International Conference on Computer Vision, 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121626373,
                "cdate": 1700121626373,
                "tmdate": 1700213951047,
                "mdate": 1700213951047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oo3L1Dpc0T",
                "forum": "nFMS6wF2xq",
                "replyto": "3ABuKQfRop",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Reviewer_YRdg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Reviewer_YRdg"
                ],
                "content": {
                    "title": {
                        "value": "Comments by Reviewer YRdg"
                    },
                    "comment": {
                        "value": "Thanks for the feedback. The authors have met most of my concerns. I still lean toward keeping my rating as marginally above the acceptance threshold."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484456606,
                "cdate": 1700484456606,
                "tmdate": 1700484456606,
                "mdate": 1700484456606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "T1QGxCSZ4Y",
            "forum": "nFMS6wF2xq",
            "replyto": "nFMS6wF2xq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2579/Reviewer_zGd6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2579/Reviewer_zGd6"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a novel conditional diffusion model, ContextDiff. Rather than only modeling the cross-modal context in the backward process, ContextDiff propagates the context information to all timesteps in both forward and backward process to adapt the trajectories for facilitating cross-modal conditional generation. The proposed method can be generalized to DDPMs and DDIMs and achieves better results in text-to-image generation and text-to-video editing."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The idea of modeling the cross-modal context in the forward process is interesting, as it differs from previous works that only consider conditional modeling in the backward process.\n* The method has sound theoretical foundations. Furthermore, the generalization to DDIMs is a clear strength that allows fast sampling.\n* The writing of the method section is clear. The adaptation to the previous diffusion process with a bias term is straightforward to understand.\n* The evaluation results show that the model performs better in terms of qualitative metrics of both automated evaluation and user study."
                },
                "weaknesses": {
                    "value": "* Experiments on latent diffusion: the method uses an Imagen-based framework, which generates a low-res image and then performs super-resolution. However, the author does not evaluate the proposed method on latent diffusion architecture. There are mentions of LDM in Sec 5.3 (ablations), but the setting is not clearly described, and comparisons with other works (rather than the baseline) are not offered.\n* The author does not offer an inference latency evaluation. Does the method slow inference down compared to baseline diffusion methods that do not have context-aware adapters?\n* A small typo (which does not affect the rating): \"A red ross\" -> \"A red rose\"?"
                },
                "questions": {
                    "value": "* How does the method compare to the baseline when integrated into latent diffusion (or Stable Diffusion)?\n* How does the method compare to the baseline in terms of inference latency?\n* What is the Stable Diffusion version used in Table 1? How does the method compare with different versions of Stable Diffusion?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2579/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2579/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2579/Reviewer_zGd6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698600592422,
            "cdate": 1698600592422,
            "tmdate": 1699636195285,
            "mdate": 1699636195285,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZE6ryWEbx1",
                "forum": "nFMS6wF2xq",
                "replyto": "T1QGxCSZ4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zGd6"
                    },
                    "comment": {
                        "value": "*We thank Reviewer zGd6 for the positive review and valuable feedback. We are glad that the reviewer found that the idea is interesting, the method has sound theoretical foundations, the writing is clear, and the evaluations are sufficient for demonstrating the effectiveness. Please see below for our responses to your comments, and **the changes in revised manuscript are marked in blue**.*\n\n**Q1: How does the method compare to the baseline when integrated into latent diffusion (or Stable Diffusion)**\n\nA1: Kindly note that our context-aware adapter can improve both text-guided image diffusion models and video diffusion models with latent diffusion architectures. In the ablation studies of Figure 6, we evaluate the FID-5k performance of LDM and LDM + our context-aware adapter, and the adapter is optimized with a pre-trained LDM. For fair comparison, we ensure that two models have the same number of training steps.\nFrom the results, we conclude that our method can comprehensively improve other pretrained latent diffusion models (i.e., Stable Diffusion). In the ablation studies of **Figure 5, 14, 15, and 16** and the quantitative results of **Table 2**, we sufficiently demonstrate our context-aware adapter can also improve other video diffusion models (Tune-A-Video and FateZero), which are all **based on latent diffusion architecture**, and outperform all previous methods. Further, we generalize our latent-based context-aware adapter to class and layout conditional image generation tasks for demonstrating the effectiveness. The results are **in the Table 3 and Table 4 of the revised manuscript marked in blue**, we find that our method not only improves the LDM model, but also surpasses other works in different conditional generation tasks.\n\n**Q2: Inference latency of proposed method and the Stable Diffusion version used for comparison**\n\nA2: We compare our method with other diffusion models regarding model size, training time, inference time and FID performance in the table below:\n\n| Method |Parameters $\\downarrow$| Training Time $\\downarrow$| Inference Time $\\downarrow$|FID $\\downarrow$|\n| :-----| :----: | :----: |:----: |:----:|\n|LDM (Stable Diffusion v1.0)|1.4B|0.39 s/Img|3.4 s/Img|12.64|\n|Stable Diffusion v1.5|0.9B|0.63 s/Img|6.5 s/Img|9.62|\n|Stable Diffusion XL|10.3B|0.71 s/Img|9.7 s/Img|8.32|\n|DALL\u00b7E 2|6.5B|2.38 s/Img|21.9 s/Img|10.39|\n|Imagen |3B| 0.79 s/Img|13.2 s/Img|7.27|\n|Imagen + Our Context-Aware Adapter| 3B+188M|0.83 s/Img|13.4 s/Img|**6.48**|\n\nWe observe that our context-aware adapter (188M) only introduces few additional parameters and computational costs to the diffusion backbone (3000M), and substantially improves the generation performance, achieving a better trade-off than previous diffusion models. In Table 1, we use LDM (Stable Diffusion v1.0) for comparison, and our ContextDiff can consistently outperform different versions of Stable Diffusion as illustrated in the table above.\n\n**Q3: A small typo: \"A red ross\" -> \"A red rose\"**\n\nA3: Thanks for your reminder, we have corrected it in the revised manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121208191,
                "cdate": 1700121208191,
                "tmdate": 1700121208191,
                "mdate": 1700121208191,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C46anMOB33",
                "forum": "nFMS6wF2xq",
                "replyto": "ZE6ryWEbx1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Reviewer_zGd6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Reviewer_zGd6"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the updates in the rebuttal. The comparison and clarifications with Stable Diffusion are indeed very helpful in understanding the method. I still vote for acceptance of this work. Keeping my score as is."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611326246,
                "cdate": 1700611326246,
                "tmdate": 1700611326246,
                "mdate": 1700611326246,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EtHEMvSDEK",
            "forum": "nFMS6wF2xq",
            "replyto": "nFMS6wF2xq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2579/Reviewer_D2VS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2579/Reviewer_D2VS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a  general cross-modal contextualized diffusion model (CONTEXTDIFF) that harnesses cross-modal context to facilitate the learning capacity of cross-modal diffusion models. The cross-modal interactions between text condition and image/video sample are incorperated into the forward process, serving as a context-aware adapter to optimize diffusion trajectories. The context-aware adapter to adapt the sampling trajectories, which facilitates the conditional modeling in the reverse process and aligns it with the adapted forward process. A series of experimental results and mathematical proofs are presented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes a method to enhance the multimodal relevance by incorporating multimodal contextual information during the forward process of the diffusion model.\n\n2. Adequate mathematical proofs are provided for both the forward and backward processes.\n\n3. The experimental results to some extent demonstrate that this method yields a high semantic correlation between the generated images and text."
                },
                "weaknesses": {
                    "value": "1. Compared to the proposed method, existing methods also utilize cross-modal attention during the forward process to control the generated content of images based on information from different modalities.\n\n2. The article does not provide sufficient analysis for why adding textual information during the forward process enhances multimodal semantic relevance. It is based on intuitive reasoning rather than an in-depth analysis.\n\n3. In the provided experimental results, the method proposed in this paper shows limited improvements compared to existing methods (e.g. Imagen) in image generation tasks.\n\n4. The paper does not analyze the limitations of the proposed method."
                },
                "questions": {
                    "value": "1. In Figure 6, the author claims that they conduct ablation study on the trade-off between CLIP and FID scores across a range of guidance weights, however, only FID scores are provided in the figure.\n\n2.  Analysis on why adding textual information during the forward process enhances multimodal semantic relevance.\n\n3. Whether this method can be incrementally trained on other pretrained generative models (e.g. Stable Diffusion), and if doing so would result in improved generation performance and faster convergence, is not discussed in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636325159,
            "cdate": 1698636325159,
            "tmdate": 1699636195127,
            "mdate": 1699636195127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xFIrgwd7IL",
                "forum": "nFMS6wF2xq",
                "replyto": "EtHEMvSDEK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D2VS (Part 1/2)"
                    },
                    "comment": {
                        "value": "*We thank Reviewer D2VS for the valuable feedback. We are glad that the reviewer think that we provide adequate mathematical proofs for our new method and this method yields a high semantic correlation between the generated images and text. Please see below for our responses to your comments, and **the changes in revised manuscript are marked in blue**.*\n\n**Q1: Existing diffusion-based methods utilize cross-modal attention to control the cross-modal generation conditioned on information from different modalities.**\n\nA1: We speculate that you may want to say \"existing methods also utilize cross-modal attention during the **backward (generation)** process\" not \"during the forward process\". Because, to the best of our knowledge, we are the first to incorporate cross-modal context ($c$, $x_0$) (the relationships between text embedding $c$ and image embedding $x_0$/$\\hat x_0$) to adapt both forward and backward processes of diffusion models while existing methods only add text into backward process. Regarding the mechanisms of conditioning, we propose context-aware adapter to directly and effectively adjust both forward and backward trajectories while existing methods only conduct cross attention between text and noisy latent. More direct comparison is in the table below:\n\n| Method |Forward|Backward|Conditioning Mechanism|\n| :-----| :----: |:----:|:----:|\n|Existing Diffusion Models|-|only text $c$|cross attention|\n|Our ContextDiff |context ($c$,$x_0$)|context ($c$,$\\hat x_0$)|directly adjust trajectories\n\n**Q2: Regarding the improvements compared to existing methods (e.g. Imagen) in image generation tasks.**\n\nA2: Our method focuses on improving the cross-modal semantic understanding of diffusion models, which is critical for both text-guided image generation and video editing.  For better understanding of qualitative results, we use **red boxes to highlight critical fine-grained parts of Fig.3 and Fig.13 in the updated paper** where our method substantially surpasses existing methods. Besides, in text-guided video editing tasks, we significantly advance video diffusion models to precisely manipulate videos according to the edited text as in Fig.4, Fig.5, Fig.14, Fig.15, and Fig.16. As for quantitative results, our FID improvement over previous methods is also significant because improving FID performance on MS-COCO is very challenging, and we achieve new SOTA results. We also improve the quantitative results in text-guided video editing tasks with automatic and human evaluations. In conclusion, we are **the first work** to generalize new diffusion models to **both text-guided image generation and video editing** tasks, and **consistently achieve new SOTA results.** \n\n**Q3: Analyze the limitations of the proposed method.**\n\nA3: While our ContextDiff boosts performance of both text-guided image and video diffusion models introducing an efficient cross-modal context-aware adapter, our models still have more trainable parameters than other types of generative models, e.g GANs. Furthermore, we need the longer inference times compared to single step generative approaches like GANs or VAEs due to the iterative diffusion sampling. However, this drawback is inherited from the underlying model class and is not a property of our ContextDiff. For future work, we will try to find more intrinsic context information to preserve for improving existing cross-modal diffusion models, and try to design a non-parametric context-aware adapter.\n\n**Q4: Regarding the ablation study on the trade-off between CLIP and FID scores across a range of guidance weights, only FID scores are provided in the figure**\n\nA4: In fact, the CLIP score is positively correlated with guidance weight in text-to-image generation, as demonstrated in DALL\u00b7E 2 and RAPHAEL. Therefore, following DALL\u00b7E 2, we plot the FID-guidance curve in Figure 6. From Figure 6, we can observe that equipped with our context-aware adapter, LDM can achieve a better FID-guidance (FID-CLIP) trade-off.  \n\n**Q5: Whether this method can be incrementally trained on other pretrained generative models, and result in improved generation performance and faster convergence.**\n\nA5: Kindly note that our context-aware adapter can improve both text-guided image and video diffusion models. In the ablation studies of **Figure 6**, we have proved our method can comprehensively improve other pretrained image diffusion models (i.e., Stable Diffusion). In the ablation studies of **Figure 5, 14, 15, and 16**, we qualitatively demonstrate our method can also improve other pretrained video diffusion models. Besides, in **Figure 7**, we have proved that our context-aware adapter can lead to better video editing performance and faster convergence of other video diffusion models. Finally, for a comprehensive evaluation, we additionally prove our method can improve generation quality and enable faster convergence of other image diffusion models (**Figure 12 in the revised manuscript, marked in blue**)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121035505,
                "cdate": 1700121035505,
                "tmdate": 1700446360041,
                "mdate": 1700446360041,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fyoO3zKmbW",
                "forum": "nFMS6wF2xq",
                "replyto": "EtHEMvSDEK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D2VS (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Q6: Sufficient analysis for why adding textual information during the forward process enhances multimodal semantic relevance**\n\nA6:\n\n**Qualitative Analysis**: We first conduct qualitative analysis on how our context-aware adapter enhances multimodal semantic relevance, the visual results are in the **Figure 8 and Figure 9 of the revised manuscript, marked in blue**. Specifically, we visualize the heatmaps of text-image cross-attention module in the sampling process of each frame image. We find that the image latents refined by our context-aware adapter can better attend to the fine-grained semantics in text prompt and sufficiently convey them in final generation results.\n\n**Theoretical Analysis**:\nWe provide an in-depth analysis on why ContextDiff can better express the cross-modal semantics. Our analysis focuses on the case of optimal estimation, as the theoretical analysis of convergence requires understanding the non-convex optimization of neural network, which is beyond the scope of this paper. Based on objective function in Equation (8), the optimal solution of ContextDiff at time t can be expressed as \n$$\\begin{array}{rl}& \\arg\\min_{\\phi,\\theta}E_{q_\\phi(x_t,x_0|c)}||x_0-f_\\theta(x_t,c)||^2\\\\\\\\\n&=\\arg\\min_{\\phi}E_{q_\\phi(x_t|c)}E_{q_\\phi(x_0|x_t,c)}||x_0-E[x_0|x_t,c]||_2^2 \\\\\\\\\n& = \\phi^*,\\theta^* \\\\\\\\\n\\end{array}$$\n\n\n,since the best estimator under L2 loss is the conditional expectation. As a result, the optimal estimator of ContextDiff for $x_0$ is \n$$E[x_0|k_t r_{\\phi^*}(x_0,c)+\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, c],$$\n\nwhile existing methods that did not incorporate cross-modal contextual information in the forward process have the following optimal estimator:\n$$E[x_0|\\sqrt{\\bar{\\alpha}_t}x_0+\\sqrt{1-\\bar{\\alpha}_t}\\epsilon, c].$$\n\nCompared with existing methods, ContextDiff can explicitly utilize the cross-modal relation $r_{\\phi^*}(x_0,c)$ to optimally recover the ground truth sample, and thus achieve better multimodal semantic alignment. \n\nFurthermore, we analyze a toy example to show that ContextDiff can indeed utilize the cross-modal context to better recover the ground truth sample. We consider the image embedding $x_0$ and text embedding $c$, which are generated with the following mechanism:\n$$x_0 = \\mu(c)+\\sigma(c)\\epsilon,$$\nwhere $\\epsilon$ is an independent standard gaussain, $\\mu(c)$ and $\\sigma^2(c)$ are the mean and variance of $x_0$ conditioned on $c$. We believe this simple model can capture the multimodal relationships in the embedding space, where the relevant images and text embeddings are closely aligned with each other. Then $x_t = \\sqrt{\\bar{\\alpha}_t} x_0+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon^{'}$ is the noisy image embedding in original diffusion model. We aim to calculate and compare the optimal estimation error in original diffusion model and in ContextDiff:\n\n $$\\begin{aligned}&min_\\theta E_{q(x_0,x_t|c)}||x_0-f_\\theta(x_0,c)||_2^2 \\\\\\\\\n &=E ||x_0-E[x_0|x_t,c]||_2^2 \\end{aligned}$$\n\nThe conditional expectation as the optimal estimator of DDPMs can be calculated as\n$$\\begin{aligned}E[x_0|x_t,c]\n&= \\mu(c) - Cov(x_0,x_t|c)*Var(x_t|c)^{-1}(\\sqrt{\\bar{\\alpha}_t}\\mu(c)-x_t)\\\\\\\\\n&= \\mu(c) - \\frac{\\sqrt{\\bar{\\alpha}_t}\\sigma(c)^2}{\\bar{\\alpha}_t\\sigma(c)^2+1-\\bar{\\alpha}_t}(\\sqrt{\\bar{\\alpha}_t}\\mu(c)-x_t) \\end{aligned}$$\n\nAs a result, we can calculate the estimation error of DDPMs:\n\n$$\\begin{array}{rl} &E||x_0-E[x_0|x_t,c]||_2^2 \\\\\\\\\n&=E||\\sigma(c)\\epsilon-\\frac{\\sqrt{\\bar{\\alpha}_t}\\sigma(c)^2}{\\bar{\\alpha}_t\\sigma(c)^2+1-\\bar{\\alpha}_t}(\\sqrt{\\bar{\\alpha}_t}\\sigma(c)\\epsilon+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon^{'})||_2^2\\\\\\\\\n& = d*\\frac{(1- \\bar{\\alpha}_t)\\bar{\\alpha}_t\\sigma(c)^4+ \\sigma^2(c)(1-\\bar{\\alpha}_t)^2}{(\\bar{\\alpha}_t \\sigma^2(c)+1- \\bar{\\alpha}_t)^2}\\\\\\\\\n& = d *\\sigma(c)^2 \\frac{1- \\bar{\\alpha}_t}{\\bar{\\alpha}_t \\sigma^2(c)+1- \\bar{\\alpha}_t}\n \\end{array}$$\nNow we use CONTEXTDIFF with a parameterized adapter : $x_t = \\sqrt{\\bar{\\alpha}_t} x_0+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon^{'}+r(\\phi,c,t)x_0$\n\n , where $r(\\phi,c,t)x_0$ is the adapter. We can similarly calculate the conditional mean as the optimal estimator of ContextDiff:\n $$E_\\phi[x_0|x_t,c] = \\mu(c)-\\frac{\\sigma^2(c)(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})}{1-\\bar{\\alpha}_t+(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})^2\\sigma^2}*((r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})\\mu(c)-x_t)$$\n And the estimation error for a given $\\phi$ in ContextDiff is:\n\n$$\\begin{array}{rl}\n&E||x_0-E_\\phi[x_0|x_t,c]||_2^2\\\\\\\\\n&=E||\\sigma(c)\\epsilon-\\frac{\\sigma^2(c)(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})}{1-\\bar{\\alpha}_t+(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})^2\\sigma^2(c)}((r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})\\sigma(c)\\epsilon+\\sqrt{1-\\bar{\\alpha}_t} \\epsilon^{'})||_2^2\\\\\\\\\n&= d \\sigma(c)^2\\frac{1-\\bar{\\alpha}_t}{1-\\bar{\\alpha}_t+(r(\\phi,c,t)+\\sqrt{\\bar{\\alpha}_t})^2\\sigma^2(c)}\n \\end{array}$$\n\nComparing the denominators of two estimation errors, we can see that using a non-negative adapter will always reduce the estimation error."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700121098242,
                "cdate": 1700121098242,
                "tmdate": 1700121098242,
                "mdate": 1700121098242,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hWzDWW25W7",
                "forum": "nFMS6wF2xq",
                "replyto": "EtHEMvSDEK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Esteemed Reviewer,\n\nWe sincerely appreciate the time and effort you dedicated to reviewing our paper. Your valuable feedbacks have been extremely beneficial. In response, we have prepared both qualitative and theoretical analysis to address your queries and concerns. \n\nAs the discussion period is approaching its conclusion in two days, we kindly request, if possible, that you review our rebuttal at your convenience. Should there be any further points that require clarification or improvement, please know that we are fully committed to addressing them promptly. Thank you once again for your invaluable contribution to our research.\n\nWarm Regards,\n\nThe Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532832833,
                "cdate": 1700532832833,
                "tmdate": 1700532832833,
                "mdate": 1700532832833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fuxGCz184I",
                "forum": "nFMS6wF2xq",
                "replyto": "EtHEMvSDEK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer D2VS:\n\nWe just want to reach out to you again and see if our response addresses your concern. Your comments really inspire us, and we are eager to continue discussing our work with you. Any further discussion before today's deadline would be highly appreciated!\n\nWarm Regards,\n\nThe Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712897240,
                "cdate": 1700712897240,
                "tmdate": 1700712897240,
                "mdate": 1700712897240,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wUHSQM4V9Z",
            "forum": "nFMS6wF2xq",
            "replyto": "nFMS6wF2xq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2579/Reviewer_DNxF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2579/Reviewer_DNxF"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the concept of contextualized forward and reverse diffusion processes, which is interesting. They have made modifications to traditional models and proposed a method that significantly improves semantic alignment. The experimental results section effectively demonstrates the efficacy of this approach. The authors provide detailed theoretical and empirical evidence, which lends strong support to this article."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The novelty of this article lies in the introduction of a new contextualized forward and reverse diffusion processes. They have made improvements upon existing methods and provided theoretical support.\n- The results presented in this paper have shown promising performance when compared to existing models, and the visualization section further supports the efficacy of this meth"
                },
                "weaknesses": {
                    "value": "- Regarding evaluation metrics, while existing metrics are commonly used, aligning text, especially fine-grained text content, with images requires more refined evaluation criteria. I would like to hear the authors' opinions on the need for improved evaluation metrics for more fine-grained, context-aware image and video generation. Additionally, why not incorporate human evaluation of the generated data in this context?\n\n-  The authors have introduced some additional controls, and it would be beneficial to discuss the associated costs, such as extra parameters, training time, and testing time, to aid in understanding their proposed method.\n\n- The authors have achieved promising results in natural images or videos, but there is a need for further discussion regarding more fine-grained context-awareness. For instance, it would be interesting to explore whether the method remains effective when dealing with specific text from within an image, as generating precise text remains a challenge for most methods. Similarly, what happens when modifying specific parts of an image? I would like to see the authors' insights on these issues concerning their context-aware adapter."
                },
                "questions": {
                    "value": "The questions I would like the authors to address have already been raised in the \"weakness\" section. I hope the authors can provide more information on these aspects."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2579/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698973247258,
            "cdate": 1698973247258,
            "tmdate": 1699636195044,
            "mdate": 1699636195044,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l3M57dXqTj",
                "forum": "nFMS6wF2xq",
                "replyto": "wUHSQM4V9Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DNxF"
                    },
                    "comment": {
                        "value": "*We thank Reviewer DNxF for the positive review and valuable feedback. We are glad that the reviewer found that the proposed method is novel, performance improvements are promising, theoretical and empirical evidences are detailed and strong. Please see below for our responses to your comments, and **the changes in revised manuscript are marked in blue**.*\n\n**Q1: Opinions on the need for improved evaluation metrics for more fine-grained, context-aware image and video generation. Why not incorporate human evaluation of the generated data in this context?**\n\nA1: This is a great question for existing text-guided visual generation models, because some fine-grained text contents are often neglected in generation process. Actually, in qualitative comparison, we have noticed this problem and demonstrated our superiority in conveying fine-grained text contents as evidenced by **Fig.3 and Fig.13**. We further update the manuscript and use **red boxes to highlight critical fine-grained parts** where existing methods fail. As for better evaluation metrics, we think a comprehensive benchmark is needed to evaluate the model capacity from different aspects. For example, we need a new text prompt set categorized with color, shape, texture, spatial relationships, non-spatial relationships, and complex compositions for more comprehensive evaluation. We will attempt to build such a benchmark in future work to promote the development of this field. Following your suggestion, we conduct a preliminary fine-grained human evaluation with four kinds of fine-grained text prompts. For each kind, we try two various prompts: color (\"blue\" and \"red\"), spatial relationships (\"on the left of\" and \"on the right of\"), non-spatial relationships (\"watch\" and \"walk with\"), and complex compositions (color+spatial relationships and color+non-spatial relationships). We show the preference rate of 100 generated images with 5 subjects in the following:\n\n| Method |Color| Spatial Relationships| Non-Spatial Relationships|Complex Compositions|\n| :-----| :----: | :----: |:----: |:----: |\n|LDM | 28% | 20% | 16% |14%|\n|Imagen | 34% | 37% | 33% |32%| \n|Our ContextDiff|**38%**|**43%**|**51%**|**54%**|\n\nWe conclude that our ContextDiff consistently outperforms LDM and Imagen across various fine-grained prompts, especially in complex scenarios, demonstrating the superior cross-modal understanding of our model.\n\n**Q2: Discuss the associated costs, such as extra parameters, training time, and testing time, to aid in understanding the proposed method.**\n\nA2: We compare our method with LDM and Imagen regarding parameters, training time, and testing time in the table below:\n\n| Method |Parameters $\\downarrow$| Training Time $\\downarrow$| Inference Time $\\downarrow$|FID $\\downarrow$|\n| :-----| :----: | :----: |:----: |:----:|\n|LDM|1.4B|0.39 s/Img|3.4 s/Img|12.64|\n|SDXL|10.3B|0.71 s/Img|9.7 s/Img|8.32|\n|DALL\u00b7E 2|6.5B|2.38 s/Img|21.9 s/Img|10.39|\n|Imagen |3B| 0.79 s/Img|13.2 s/Img|7.27|\n|Our ContextDiff | 3B+188M|0.83 s/Img|13.4 s/Img|**6.48**|\n\nWe can observe that our context-aware adapter (188M) only introduces few additional parameters and computational costs to the diffusion backbone (3000M), and substantially improves the generation performance, achieving a better trade-off than previous diffusion models.\n\n**Q3: I would like to see the authors' insights on these issues concerning their context-aware adapter.**\n\nA3: Our ContextDiff is good at dealing with specific text within an image or modifying specific parts of an image as illustrated in our text-guided video editing results (Fig.4, Fig.5, Fig.14, Fig.15, and Fig.16), because we need to edit each frame image according to text prompts. To further investigate more fine-grained context-awareness of our model, following your suggestion, we base on our text-to-video editing framework to explore how our context-aware adapter works. We visualize the heatmaps of text-image cross-attention in the sampling process of **each frame image in Fig.8 and Fig.9 of the updated paper**. We find that our context-aware adapter can enable the model to better focus on the fine-grained text semantics and precisely modify specific parts in frame image according to the edited text prompt. The results demonstrate that our context-aware adapter can substantially improve the cross-modal understanding of the diffusion model."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700120838787,
                "cdate": 1700120838787,
                "tmdate": 1700120838787,
                "mdate": 1700120838787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QiKRHPXWmV",
                "forum": "nFMS6wF2xq",
                "replyto": "rKFg9eMVuu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2579/Reviewer_DNxF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2579/Reviewer_DNxF"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the responses."
                    },
                    "comment": {
                        "value": "Thanks for the responses of the authors. Most of them have been addressed and I will keep my original rating of borderline accept."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2579/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712110425,
                "cdate": 1700712110425,
                "tmdate": 1700712110425,
                "mdate": 1700712110425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]