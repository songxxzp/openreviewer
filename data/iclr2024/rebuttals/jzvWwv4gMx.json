[
    {
        "title": "On the Paradox of Generalizable Logical Reasoning in Large Language Models"
    },
    {
        "review": {
            "id": "qUxMQVJelK",
            "forum": "jzvWwv4gMx",
            "replyto": "jzvWwv4gMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_qhHL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_qhHL"
            ],
            "content": {
                "summary": {
                    "value": "I appreciated the related works section. However, I am not sure that the experimental design is up to the standards of a top venue like ICLR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I appreciated the references to the literature. The questions you want to answer are interesting."
                },
                "weaknesses": {
                    "value": "An experiment should try to isolate the hypothesis being tested and removing confounding factors. I am also suspicious your  conclusions; e.g., you claim \"In other words, LLMs show significantly worse performance when semantics are decoupled \" but without error bars, the Symbols and Semantic of Table 2 look the same to me. (I don't think that you should generalize from a single example (Symbol tree), where the results don't hold for other example (ProofWriter).\n\nMuch deductive reasoning is combinatorially difficult, and is difficult even for humans. I'm surprised humans can do the examples B.2 well. (I thought that the psychology literature results are that humans are not good at logical reasoning -- but I am not a psychologist).\n\nYou have a strange definition of abduction. It looks like \"find a proof\" (from the only example given in Figure 1 and on page 4), where the sisterOf and motherOf are given as facts. Abduction in logic means to find a plausible explanation:\ne.g. why did someone cough? An explanation is that they have asthma. Another explanation is they have a cold. The system does not know wether they have asthma or a cold. It is difficult to judge the correctness of an answer.\n\n(see also questions)"
                },
                "questions": {
                    "value": "Why only \"zero-shot Symbols\" for humans? Who are the humans used? Are they trained in logic? (Appendix F1 doesn't provide much details being being diverse college/graduate students). This is not up the standards of a good human-experiment to make any conclusions. Were the humans all the same? Why isn't there a range? The examples you gave on p 36 for humans were for the semantics case (unless I misunderstood appendix I). I wish your appendices gave a few complete examples rather than more abstract examples; what was the actual input and what was the actual output for the computers and the humans? \n\nFor the Symbolic Tree dataset, is the LLM/human told it is a closed-world dataset? What is a false fact? None of the examples seem to rely on negative facts, and none of the examples in appendix B have any. Are there negative examples as well as positive examples?\n\nHow hard are the induction tasks? What bias do you assume for the correct answer?  Why should we believe the \"gold proofs\" are correct?\n\nCan you explain why In Table 1, for ChatGPT Zero-Shot-CoT is better than Zero-Shot for deduction, but in Table 2, it is worse for all depths? Does CoT help?\n\nFor \"Paradox 1\" and \"Paradox 2\" - why are they paradoxes? Maybe use \"hypothesis\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697659037182,
            "cdate": 1697659037182,
            "tmdate": 1699636176763,
            "mdate": 1699636176763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HmlrHONG5V",
                "forum": "jzvWwv4gMx",
                "replyto": "qUxMQVJelK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the reviewer for their comments.\n\n> W1. concern about the experimental results and conclusion\n\nWe have added error bars to the ChatGPT results in the revised version. Due to the cost of GPT-4, we do not provide its error bars. As seen in Table 1, the semantic performance under error bars is still significantly worse than symbolic performance. \n\nAs for Table 2, we observed that the symbol version performs comparably to the semantic version, primarily because the semantics in ProofWriter are irrelevant to commonsense. Essentially, tasks which require less intricate semantic understanding or minimal dependence on common sense knowledge are not significantly impacted by the presence or absence of semantics in the language models' performance, as highlighted in blue.\n\nTo further validate our finding that Language Learning Models serve as in-context semantic reasoners rather than symbolic reasoners, we introduced a counter-commonsense setting and conducted more ablation studies, as shown in Table 3. In addition to deduction tasks, we executed comprehensive experiments in induction and abduction, to provide a holistic illustration of our conclusions.\n\n\n> W2. concern about the results of human study\n\n\nPiaget's theory of formal operations (https://en.wikipedia.org/wiki/Piaget' s_theory_of_cognitive_development) points out that individuals in early to middle adolescence are capable of hypothetical and deductive reasoning that involves assumptions with no necessary relation to reality, also including counterfactual thinking. While not all individuals use formal reasoning in their daily lives [1], most humans possess this skill to some degree. And they possess the cognitive ability to engage in formal reasoning when required. In addition to Piaget's theories, it has been suggested by citations [1][2][3] that humans possess symbolic reasoning abilities. To rigorously verify this claim, we will conduct a human study to measure human performance in these tasks. \n\n[1]\u00a0https://cbmm.mit.edu/research/2013-2018-research/exploring-future-directions/learning-and-reasoning-symbolic-domains\n[2]\u00a0https://www.frontiersin.org/articles/10.3389/fpsyg.2014.00275/full\n[3]\u00a0https://tigerprints.clemson.edu/all_dissertations/2932/\n\n> W3. definition of abduction\n\nAbduction in logic traditionally involves generating plausible explanations for a given observation. Indeed, our approach to abductive reasoning aligns with this definition, particularly within the framework of first-order logic. Based on the definition of Logic-based abduction (https://en.wikipedia.org/wiki/Abductive_reasoning), a proof-theoretical abduction method for first-order logic have been proposed for finding explanations. In first-order logic contexts, abduction typically involves reasoning that is more definitive and less speculative than in commonsense reasoning scenarios. Most abductive reasoning in this domain tends to yield plausible conclusions.\n\nMoreover, Regarding the specific task settings, our methodology follow the approach \"Explanation Generation with Theory\" described in the paper https://arxiv.org/pdf/2303.12023.pdf. This approach involves generating a new hypothetical fact h such that, when combined with a given theory C, can prove an observation O, i.e., $C \\cup {h} |=O$. We also reference \"Explanation Classification,\" where the task is to choose the most plausible hypothesis from given alternatives to explain sequential observations. Considering the complexity of generating new facts and the necessity of a feasible evaluation framework, we adapted our task to involve selecting the most plausible facts from existing ones. This adaptation allows us to maintain the rigors of first-order logic abduction while ensuring that our tasks are feasible and effectively evaluative.\n\n> Q1. the details of human study\n\nWe apologize for any confusion regarding Appendix I, which was designed to examine the influence of irrelevant information on logical reasoning. We will provide more details about the human study in the revised paper. \n\nWe recruited 11 participants from diverse science and engineering backgrounds, including computer science, electronics, artificial intelligence, and automation. Although they have basic understanding of simple logic concepts, they are not experts in logical reasoning. Therefore, we provided them with task instructions that explained the concepts of deduction, induction, and abduction, aligned with the illustrations and definitions of logical reasoning presented in Section 3 of our paper.\n\nWe then presented them with 18 specific tasks, including six tasks for each deductive, inductive, and abductive reasoning type. Each task closely resembled the zero-shot prompts given to LLMs. We refer to this setting as \"zero-shot\" because we did not provide any further specific examples to help participants understand the tasks. The examples can be found in Appendix I and detailed materials have been uploaded in supplementary."
                    },
                    "title": {
                        "value": "Reply to Reviewer qhHL (part 1/3)"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204939888,
                "cdate": 1700204939888,
                "tmdate": 1700205123490,
                "mdate": 1700205123490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YfxGQErm9M",
                "forum": "jzvWwv4gMx",
                "replyto": "qUxMQVJelK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Q2. concern about closed-world setting\nIn our experimental setting, Symbolic Tree is a closed-world dataset, meaning that any facts not presented or inferable in the dataset are assumed to be false. In other words, when the language model (LLM) outputs an \"Unknown\" answer, we interpret it as \"False\". \n\nConsider the following false case example:\n```\nStatement: $r27$(patrick, marko)\nAnswer: We can use logical rule L24, which states that if there is a relationship of type 3 between B and A, and a relationship of type 3 between C and B, and A has property 1, then A has relationship 27 with C. \nFrom the facts, we know that there is a relationship of type 3 between Patrick and Theodor (F43), and a relationship of type 3 between Theodor and Marko (F44). However, we do not have any direct information about a relationship of type 1 involving Patrick. Therefore, we cannot use L24 to prove $r27$(patrick, marko). \nThus, we cannot determine whether the statement $r27$(patrick, marko) is True or False using the given logical rules and facts.\n```\n\n> Q3. the complexity and \"gold proof\" of induction tasks \n\nIn our induction tasks, we utilized the Symbolic Tree dataset, which consists of approximately 99 basic facts including gender information (\"r43\" and \"r44\"), \"parentOf\" (\"r1\"), and \"inverse_parentOf\" (\"r45\") relationships. Additionally, there are about 5-15 inferred facts related to the unknown (to be predicted) rule.\n\nNote that a single rule can be equivalent to multiple rules. For example, the rule $\\forall x,y,z: \\text{parentOf}(x, y) \\land \\text{parentOf}(y, z) \\land \\text{gender}(x, \\text{female}) \\rightarrow \\text{GrandmotherOf}(x,z)$ can be represented as two separate rules: $\\forall x,y,z: \\text{parentOf}(x, y) \\land \\text{parentOf}(y, z) \\rightarrow \\text{GrandparentOf}(x,z)$ and $\\forall x,y,z: \\text{GrandparentOf}(x,z) \\land \\text{gender}(x, \\text{female}) \\rightarrow \\text{GrandmotherOf}(x,z)$. To simplify the induction evaluation process, we rewrite all logical rules by including only the \"parentOf\" and \"gender\" relations in the rule body. Doing so ensures that each inferred relation is implied by a single logical rule. We provide a rule template to define the rule search space, as detailed in Appendix A.2, and specific examples can be found in Appendix E.3.\n\n\n> Q4. Can you explain why In Table 1, for ChatGPT Zero-Shot-CoT is better than Zero-Shot for deduction, but in Table 2, it is worse for all depths? Does CoT help?\n\nIndeed, in Table 1, we observe that the performance of ChatGPT in the Zero-Shot-CoT setting does not show a marked improvement over the Zero-Shot setting, especially when considering error bars. This suggests that the addition of CoT does not significantly enhance the model's ability to solve deduction tasks.\n\nTable 2 presents an even more pronounced discrepancy, where CoT settings perform worse than the Zero-Shot setting. We hypothesize two main reasons for this observed trend:\n\n1. The CoT settings perform worse than the zero-shot setting with a higher frequency of \"Cannot be determined\" answers. We speculate that this may arise from the LLMs struggling to generate explicit reasoning traces effectively.\n2. Utilizing a step-by-step explicit reasoning trace might lead to error propagation, resulting in worse performance than directly obtaining the answer. In addition, this effect seems to be more pronounced in tasks with semantic contexts, as seen in Table 2. The discrepancy between Zero-Shot-CoT and CoT is greater. This observation might indicate that step-by-step reasoning could amplify the distracting effect of weird semantics, such as \"The squirrel needs the dog.\"\n\nConsequently, CoT might not always be helpful for reasoning tasks that involve in-context new knowledge for models with weaker reasoning abilities like ChatGPT."
                    },
                    "title": {
                        "value": "Reply to Reviewer qhHL (part 2/3)"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205023286,
                "cdate": 1700205023286,
                "tmdate": 1700205111705,
                "mdate": 1700205111705,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R0vm2AcW2K",
            "forum": "jzvWwv4gMx",
            "replyto": "jzvWwv4gMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_oqtQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_oqtQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper provided an in-depth evaluation of the reasoning capability of large language models through the language of first-order logic:\n\n- For reasoning tasks, deductive, inductive, and abductive reasoning are conducted.\n- For the representation of language, the pure logic language, natural language, and some corner cases of language inputs with garbled symbols.\n- For LLMs, the in-context learning of ChatGPT and GPT4 as well as the fine-tuning of Llama-13B is discussed.\n\nBy conducting investigations over logical reasoning, the authors identified two major findings and multiple minor findings from their empirical results. It is suggested that the logical reasoning of the large language model is still a challenging task. The good performance of large language models is either mixed results of the semantics of the language, templating matching of the known knowledge, and eventually, the strict logical reasoning ability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "It is praiseworthy that this paper justifies many aspects of logical reasoning.\n\nThe highlighted discussions include\n- the gap between formal and natural language (or the symbolic or semantics referred to in this paper).\n- the impact of in-context knowledge and parameterized knowledge on the commonsense and counter-commonsense settings.\n\nThough there is no innovation from the methodological aspect,  the way of tackling this problem demonstrated by this paper will surely encourage future work."
                },
                "weaknesses": {
                    "value": "Despite the impressive points that the authors intended to make, some facts might undermine the validity of the claims.\n\n1. The first part of the claims are made by direct prompt ChatGPT/GPT4. However, some gaps between the performances are not significant.\n2. Some claims are too general to be valid, please check the question part."
                },
                "questions": {
                    "value": "1. For the claim\n> The length of the context influences reasoning performance, as shorter contexts make it easier to select relevant and useful information while minimizing the impact of unrelated content. \nThe effect is also affected by the semantic language and internal knowledge. Are there any results from the symbolic logic language evaluation?\n\n2. For the claim regarding LLM leverages template matching, why do the garbled symbols decrease the performance of deductive reasoning?\n\n3. For the claim regarding learning to reason, why do authors expect that \"fine-tuning the LLM on symbolic trees\" should lead to good performance in FOLIO and RuDaS?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Reviewer_oqtQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699072084417,
            "cdate": 1699072084417,
            "tmdate": 1699636176696,
            "mdate": 1699636176696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dBBAAIWg6A",
                "forum": "jzvWwv4gMx",
                "replyto": "R0vm2AcW2K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer oqtQ"
                    },
                    "comment": {
                        "value": "Thank you for the reviewer's comments.\n\n> W1. The first part of the claims are made by direct prompt ChatGPT/GPT4. However, some gaps between the performances are not significant.\n\nIn Table 1, considering error bars, semantic performance is still notably worse than symbolic performance.\n\nAs for Table 2, we observed that the symbol version performs comparably to the semantic version, primarily because the semantics in ProofWriter are irrelevant to commonsense. Essentially, tasks which require less intricate semantic understanding or minimal dependence on common sense knowledge are not significantly impacted by the presence or absence of semantics in the language models' performance, as highlighted in blue.\n\nTo further validate our finding that Language Learning Models serve as in-context semantic reasoners rather than symbolic reasoners, we introduced a counter-commonsense setting and conducted more ablation studies, as shown in Table 3. In addition to deduction tasks, we executed comprehensive experiments in induction and abduction, to provide a holistic illustration of our conclusions.\n\n\n> W2. Some claims\n\nRe W2 (1): The results of symbolic reasoning evaluation are provided in Appendix J.2. In this section, we perform deductive and inductive reasoning experiments while providing the models with only relevant facts. The findings demonstrate that LLMs are easily disrupted by irrelevant information, especially when confronted with longer contexts.\n\nRe W2 (2): We suppose the reviewer is concerned about \"rep_all_with_counter\" setting presented in Table 4. We would like to emphasize that even though fine-tuned LLMs can leverage template matching to make predictions, it is still inevitable that they may become distracted by prior knowledge.\n\nRe W2 (3): Our paper focuses on first-order logical reasoning abilities. Unlike the inference of machine learning which usually generalizes to the same distribution, an agent with \u201ctrue\u201d logical reasoning abilities can reason with any given information to solve new, unfamiliar problems, regardless of any prior knowledge or different knowledge representations. In other words, it can generalize across novel rules (Sec 1 Line 13). For instance, consider training on $r1(A, B) \\land r2(B) \\rightarrow r3(B, A)$ and testing on $r1(A, B) \\land r2(B, C) \\rightarrow r3(C, B)$, as depicted in Fig 3. In our experimental setting, RuDaS is the dataset featuring rules with similar forms as $\\text{xx} \\land \\text{xx} \\cdots \\rightarrow \\text{xx}$. FOLIO is a complex and diverse dataset for logical reasoning expressed in natural language. Through our comprehensive experiment results, we find fine-tuned LLMs on symbolic logic datasets cannot generalize to novel rules, highlighting that fine-tuning LLMs fails to achieve generalizable logic reasoning."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204574842,
                "cdate": 1700204574842,
                "tmdate": 1700204574842,
                "mdate": 1700204574842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zzrzzvV5rW",
            "forum": "jzvWwv4gMx",
            "replyto": "jzvWwv4gMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_VR1b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_VR1b"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates whether large language models (LLMs) like ChatGPT and llama have true logical reasoning abilities that can generalize across facts, rules, domains, and representations.\nThe authors evaluate LLMs on deductive, inductive, and abductive reasoning tasks. They find that when semantics are removed from the tasks by replacing words with symbols, the performance of LLMs drops significantly. This suggests LLMs rely on semantic associations rather than formal reasoning.\nThe authors fine-tune an LLM on symbolic reasoning tasks, which improves performance on unseen facts but not novel rules. This indicates the LLM uses template matching rather than truly mastering generalizable reasoning.\nOverall, the paper reveals two paradoxes: 1) LLMs rely on semantics rather than formal reasoning, and 2) Fine-tuning enables shallow generalization via template matching but not true generalization to new rules."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Authors study the reasoning capabilities of LLMs and find an interesting angle. Authors report extensive negative results for future body of work to tackle."
                },
                "weaknesses": {
                    "value": "Authors can be more specific regarding details. Authors can also perform additional interpretability analyses to help the community understand the failure modes."
                },
                "questions": {
                    "value": "- Can authors clarify which version of GPT4 and ChatGPT they use? There are many timestamped versions with differing context length. \n- Can authors provide more study on how GPT4 fails on symbols version of the task?\n\nI read the author response and I am keeping my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Reviewer_VR1b"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699345260398,
            "cdate": 1699345260398,
            "tmdate": 1700723511258,
            "mdate": 1700723511258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qXaAughWI8",
                "forum": "jzvWwv4gMx",
                "replyto": "zzrzzvV5rW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer VR1b"
                    },
                    "comment": {
                        "value": "Thank you for recognizing our contribution.\n\n> additional interpretability analyses about failure cases.\n\nWe provided some failure cases and discussions in Appendix E. These failures can likely be attributed to interference from irrelevant information and the inclusion of hallucinated information.\n\n> the version of ChatGPT and GPT-4\n\nWe used the gpt-3.5-turbo-0301 and gpt-4-0314 versions for this study. We will provide this detail in the revised paper."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204528608,
                "cdate": 1700204528608,
                "tmdate": 1700204528608,
                "mdate": 1700204528608,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zJ4igccFGc",
            "forum": "jzvWwv4gMx",
            "replyto": "jzvWwv4gMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_np2z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_np2z"
            ],
            "content": {
                "summary": {
                    "value": "This paper asks whether LLMs truly understand logical reasoning or is their success on some datasets influenced by linguistic semantics and pattern matching. To this end, they experiment with linguistic logical reasoning datasets both in their original form and in pure symbolic form (e.g., relations r1, r2, ...). The find that there is a substantial performance gap between the two settings for both ChatGPT and GPT-4 in a zero/few-shot setting. Further, fine-tuning closes the gap in the symbolic setting, but there still is a large gap when asked to generalize to rules in a different domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The basic question that the paper is asking is important to study in order to understand the true capabilities of LLMs, especially when it comes to performing logical reasoning. Their approach of decoupling relation/fact semantics from performing deduction/induction/abduction with logical rules is interesting (though likely not the first, but I cannot pin-point a prior work at this time, so I'll give the authors the benefit of doubt).\n\nThe authors conduct a reasonably large study (within the realm of the 2-3 datasets they consider), with many side questions and analyses.\n\nThe paper situates itself in the broader NLP / AI research work, citing a LOT of (perhaps too many?) related papers."
                },
                "weaknesses": {
                    "value": "The overall pitch of the paper is not as convincing as it could be. It's written like the community believes (from prior papers) that LLMs have strong logical reasoning skills, and that the current paper questions this belief and provides evidence against it. However, I don't think it's the case that the community believes logical reasoning is solved by LLMs. E.g., for the datasets considered here, even the baseline performance (in the original, so-called *Semantics* version of the tasks) is not high enough. This makes the motivation of the study relatively weak.\n\nThe pitch is also confusing because of the use of the word \"paradox\". What paradox exactly is being explored here? Reading the title, I was expecting to see something like: LLMs are great at X, which implies they should be great at Y too, but they fail at Y, raising a conundrum. Or some such internal conflict or conflict with commonsense expectations, that would justify the word paradox. I'm not sure what the authors have in mind for a paradox.\n\nOverall, while I thought the study was useful, I didn't find anything subjectively surprising. It is generally accepted that LLMs --- being **language models** --- rely on many linguistic clues and prior knowledge to perform tasks. Taking away these clues is thus expected to drop performance. Similarly, training directly on the so-called *Symbolic* form should help, which they authors also find to be the case. All this is very much aligned with expectation, which makes it difficult to pin point what the new knowledge this paper would bring to the community.\n\nThere are number of additional side experiments in the paper. This, in principle, is nice. However, while reading through those section, I found the large number of questions to be somewhat distracting. At the least, the authors should try to thread the narrative better through these side experiments and analyses, and try to provide a view of them that helps support the overall message of the paper.\n\nIn summary, while it's somewhat useful to see the experiments on *Symbolic* forms of the considered datasets done, the results don't really feel different from what one might expect to see.\n\nMINOR comments:\n\n* The use of *Semantics* when referring to relation names but not when referring to logic is confusing. Logic, of course, by design has a very clear and unambiguous semantics. I think what you mean is *linguistic semantics* of predicate names. If so, please be sure to clarify this and emphasize the *linguistic* aspect.\n\n* Your related work section (as well as the introduction) has a **lot** of citations, almost too many to be meaningfully valuable. E.g., near the top of page 3, you have 12+ citations around ICL, without any explanation of the connection between these prior works and what's in your paper. As a general rule, it's more valuable for the related work section to point out the few most related works AND articulate a clear connection of the current work to them, as opposed to dumping a huge list of papers just to cover every possible connection.\n\n* The last sentence of page 2 (\"Wei et al propose symbolic tuning, which ....\") is very long and hard to parse."
                },
                "questions": {
                    "value": "* What exactly is the paradox (i.e., some form of commonsense contradiction) that you are referring to? Or if *paradox* is not the right word, please replace it with something else.\n\n* Looks like you forgot to discuss Table 2 (results on ProofWriter) in the main paper. What are the main take-aways from this table and how do they support your claims? E.g., it appears the going from the Semantics setting to the Symbolic setting does *not* reduce the performance of the models substantially; in fact, the performance goes up in many cases. How does this align with your claims from Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699396528891,
            "cdate": 1699396528891,
            "tmdate": 1699636176496,
            "mdate": 1699636176496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gyOc4oLTKk",
                "forum": "jzvWwv4gMx",
                "replyto": "zJ4igccFGc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer np2z (part 1/2)"
                    },
                    "comment": {
                        "value": "Thank the reviewer for their comments.\n\n> W1. The motivation behind the study is weak due to the discrepancy in baseline performance on logical reasoning tasks in question.\n\nIndeed, Large Language Models have achieved impressive performance on a variety of reasoning tasks[1][2], such as arithmetic and commonsense reasoning. However, the extent to which LLMs have mastered formal logical reasoning remains unclear. Previous studies have shed light on LLMs\u2019 reasoning abilities to some extent, but a deeper exploration of their underlying reasoning mechanisms is still necessary. Our work aims to fill this gap by providing a novel perspective, going beyond mere performance metrics to understand the essence of LLMs' reasoning processes.\n\nThe performance on the Symbolic Trees semantics version has achieved commendable results (91% accuracy on GPT-4). However, performance on reasoning datasets with semantics does not necessarily reflect a deep understanding or mastery of formal reasoning. When the semantics are decoupled, LLMs tend to perform much worse. Thus, our primary goal is to investigate whether LLMs possess genuine formal reasoning abilities or simply exploit superficial semantic associations between questions and answers to make intuitive predictions. This investigation is of great importance for advancing the development of artificial general intelligence (AGI).\n\nFurthermore, we also experiment with fine-tuning Llama-2 on pure symbolic reasoning tasks to bridge the gap. Although it appears to close the gap, this is achieved through template matching rather than invoking the fundamental formal reasoning abilities. We identify this phenomenon to provide a clear understanding of the boundaries within which LLMs currently solve logical reasoning tasks.\n\nTherefore, the interesting findings aim to motivate further investigation into unveiling the inner workings of black-box LLMs.\n\n[1] Wei, Jason, et al. \"Chain-of-thought prompting elicits reasoning in large language models.\" Advances in Neural Information Processing Systems 35 (2022): 24824-24837.\n[2] OpenAI. Gpt-4 technical report, 2023. 1\n\n> W2. the use of the word \"paradox\" is confusing and unclear.\n\nWe would like to explore and highlight two key paradoxical observations in the capabilities of Large Language Models (LLMs) with respect to logical reasoning.\n\n**Paradox 1** revolves around the unexpected discrepancy in LLMs' performance in logical reasoning tasks. While LLMs like GPT-4 show high accuracy (91.1%) in tasks rich in semantics, their performance significantly declines when semantic cues are replaced with symbolic representations. This observation is paradoxical because, according to the formal definition of logical reasoning, LLMs' reasoning ability should rely purely on the form or syntax of premises and conclusions, independent of semantic content. Yet, our findings indicate that LLMs heavily rely on semantics, suggesting that current logical reasoning tasks may not fully reflect the true extent of LLMs' formal reasoning capabilities.\n\n**Paradox 2** emerges from our fine-tuning experiments on symbolic reasoning tasks. While supervised fine-tuning narrows the performance gap between semantic and symbolic settings, enabling LLMs to perform logical reasoning seemingly agnostic to semantics, their ability to generalize to unseen logical rules remains limited. This indicates that LLMs primarily use template matching mechanisms for reasoning, rather than engaging in genuine formal reasoning processes. This paradox challenges the common perception that fine-tuning effectively enhances LLMs' logical reasoning skills, suggesting that the improvement may be superficial rather than indicative of true reasoning abilities.\n\nIn summary, our paper identifies two paradoxes: 1) **LLMs' reliance on semantics rather than engaging in formal reasoning, despite seeming logical reasoning proficiency**, and 2) **the superficial nature of improvements in logical reasoning through fine-tuning, which fails to achieve deep generalization to new rules**. These paradoxes question the true logical reasoning capabilities of LLMs and emphasize the need for more nuanced understanding and development in this area."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194455902,
                "cdate": 1700194455902,
                "tmdate": 1700204487666,
                "mdate": 1700204487666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wOCXa6lp6X",
            "forum": "jzvWwv4gMx",
            "replyto": "jzvWwv4gMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_nbkm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_nbkm"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides an experimental evaluation of logical reasoning abilities of large language models. The authors first evaluate pre-trained models (GPT-4, GPT-3.5 Turbo) on logical reasoning tasks (deduction, induction, abduction) on both problems expressed with symbols and with words. They observe a large gap in some of the tasks, with even GPT-4 performing generally very poorly on induction with symbols, but much better with words that carry commonsense semantics. The authors then try fine-tuning LLaMA 2 on these tasks, observing that while it is able to match the training rules very well, it still cannot generalize to novel logical rules at inference time."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The main motivating question here is interesting, of whether there are fundamental limitations for logical reasoning in LLMs.\n\nThe authors run human comparisons, which is good to sanity check the feasibility of the tasks.\n\nThe paper tries to do a very through experimental evaluation, considering both prompting and fine-tuning, and on a range of tasks -- synthetic and existing ones from prior work.\n\nThe paper is generally easy to follow."
                },
                "weaknesses": {
                    "value": "The paper has two main sets of results: with prompting and with fine-tuning. I'll give separate comments on those.\n\n## Results with prompting\n\nThe main result here was that models performed significantly worse when symbols were used to described the rules, instead of meaningful words. In a broad sense, this question has been studied before in both papers that observed content effects in LLM reasoning that the authors cite (PrOntoQA and Dasgupta et al). In those papers, they used made-up words, whereas here the authors used short symbols (A, B, etc), but I believe the insight is the same. So, if the authors believe this says something that hasn't been said before, I don't think it came across in the paper.\n\nFinally, the gap in these results is significantly larger in the induction and abduction tasks. The results of GPT-4 in induction (< 10% with symbols) do make me wonder whether this was due to the specific way the task was set up, or whether these are honest failures. It would be interesting if this was the latter case, since induction and abduction haven't really gotten as much attention from prior work. However, the paper has little detail about these tasks besides the general description (I have some specific questions below). It would have helped to have seen many examples of the problems and of GPT-4 responses, to make sure that the task was set up properly and that this is actually due to GPT-4 having a surprisingly bad performance. I tried to find such examples in the Appendix, but couldn't (it's possible I just missed them because there's a lot there! In that case, please point me to it).\n\n## Results with fine-tuning\n\nFor fine-tuning, the main result was that models can internalize rules seen during training, but fail to generalize to novel rules. But if I understand, the total number of rules in training and testing was extremely small (5 in training, 3 in testing). Indeed, we would not expect to see generalization from these many examples. There are many other works showing that you do need a certain minimal level of task diversity in the training to get in-context learning in LMs [1,2]. In order to draw this strong conclusion that Transformers might have fundamental limitations to generalizing to unseen logical rules, you would have to train with a much larger number of training rules (e.g. hundreds of thousands) to make this argument convincing. If _even then_ you see a large gap, then it starts to look more like scaling the data is not leading to significant improvements, suggesting that such limitation might be more fundamental. But, at the current scale, the negative result is to be expected, and does not lead to insights into the broader motivating question.\n\n[1] Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. Allan Ravent\u00f3s, Mansheej Paul, Feng Chen, Surya Ganguli, 2023\n[2] Data Distributional Properties Drive Emergent In-Context Learning in Transformers. Chan et al, 2022."
                },
                "questions": {
                    "value": "- Can you point to specific examples of GPT-4 failures in induction and abduction?\n- Generally, your few-shot numbers seem worse than zero-shot. Why would that be the case? That might point to not giving good examples of reasoning.\n-- In particular, looking at some of the examples of the appendix, I don't think they contain valid reasoning. For example, this one in Appendix H:\n```\nStatement: r8(Elena, Nina)\nAnswer: We can use logical rule L5: \u2200A, B, C : r3(A, B) \u2227 r3(B, C) \u2227 r2(A) \u2192 r8(A, C) to deduce whether the statement r8(Elena, Nina) is true or false. [...]\n```\nThis is not the complete problem, but I don't think this is correct. Rule L5 might only be used to prove that r8(A, C) is true (which in this case it does), but if its premises are not satisfied it does not say anything about r8(A, C) being false. Thus, this example is misleading - this reasoning template does not generalize. In fact, all of the other examples below this one proceed like this, and conclude \"true\". Do you also give examples of \"false\" cases?\n- Why are there missing entries in induction in Table 1?\n- What do you think are the novel insights in your experiments with words <--> symbols compared to results in prior works around content effects in LLM reasoning?\n- For induction and abduction, what was the complexity of the held-out premises or rules? How did you make sure the answer was unique, since this is logically non-trivial? (in fact impossible, since formally there will be an infinite set of hypothesis in first-order logic that could be used to derive any given conclusion)\n- For fine-tuning, would you be able to provide specific fine-tuning examples, besides just the prompts?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2413/Reviewer_nbkm"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699501788767,
            "cdate": 1699501788767,
            "tmdate": 1699636176432,
            "mdate": 1699636176432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "41vdY022nV",
                "forum": "jzvWwv4gMx",
                "replyto": "wOCXa6lp6X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer nbkm (part 1/2)"
                    },
                    "comment": {
                        "value": "Thank the reviewer for their comments.\n### Results with prompting\n\n> W1. similar question about content effect in LLMs' reasoning has been studied before.\n\nWhile they conclude that \"Language models show human-like content effects on reasoning\", this is only a subordinate conclusion that suggests LLMs are in-context semantic reasoners influenced by semantic associations. However, their conclusion does not directly imply that \"LLMs are not symbolic reasoners,\" as suggested by Paradox 1: \"LLMs are in-context semantic reasoners rather than symbolic reasoners\".\n\n\nAccording to the definition of deductive reasoning from Wikipedia (https://en.wikipedia.org/wiki/Deductive_reasoning), logical reasoning should be formal and depends only on the form or the syntax of the premises and the conclusion, irrespective of the specific contents of this argument. To this end, our task decouples semantics and focuses on pure symbolic reasoning (only provided syntax). When we feed these to LLMs, they show significantly worse performance compared to when normal semantic words are fed. This phenomenon indicates that LLMs fail to invoke the basic formal reasoning abilities of humans but instead rely on shallow semantic associations for prediction. \n\nMoreover, we conduct comprehensive experiments on various logical reasoning, including deduction, induction and abduction tasks and find that they perform notably worse in these types of reasoning tasks compared to deduction, regardless of whether semantics or symbols are used. When semantics are decoupled, the drop in performance is even more significant. These findings highlight the\u00a0considerable room for improvement\u00a0in LLMs' reasoning abilities and suggest that relying solely on semantics to achieve symbolic reasoning is challenging.\n\n> W2. specific examples of GPT-4 failures in induction and abduction\n\nWe provided some failure cases and discussions in Appendix E. These failures can likely be attributed to interference from irrelevant information and the inclusion of hallucinated information.\n\n\n\n### Results with fine-tuning\n\n> W3. the main result found models can internalize rules during training but fail to generalize to novel rules, with a limited number of rules tested. To convincingly conclude fundamental limitations in Transformers, a much larger number of training rules should be used.\n\nWe appreciate the reviewer's insightful comment on the necessity of task diversity and the scale of training data for effective in-context learning in language models. However, we would like to emphasize that our Symbolic Tree dataset included 28 logical rules (not \"5 in training, 3 in testing\" as the reviewer mentioned). Moreover, in our extended dataset, RuDaS, we significantly increased the diversity with up to 1,000 different rules. Despite this expansion in rule variety, the models still struggled to generalize to novel rules.\n\nAdditionally, we attempted to scale up the training data and extend training time to enhance the models' logical reasoning capabilities. As shown in Figures 5 and 6 of our paper, these efforts did not yield substantial improvements in performance. This suggests that simply scaling data and training duration may not be sufficient to overcome certain inherent limitations in transformers regarding logical rule generalization.\n\nIn the \"Limitations and Future Work\" section of our paper, we discuss these points, acknowledging that while our findings strive to address the limitations of scalability and diversity to obtain the most convincing results possible, they might not fully realize the potential of training with large-scale and highly diverse data.\n\nWith these considerations in mind, we will carefully refine and moderate our statements to ensure the precision and reliability of our paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700194170457,
                "cdate": 1700194170457,
                "tmdate": 1700194170457,
                "mdate": 1700194170457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IhEiPbHlYH",
                "forum": "jzvWwv4gMx",
                "replyto": "BahXDsTjhI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Reviewer_nbkm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Reviewer_nbkm"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response, and the pointers to the relevant examples in the appendices.\n\nAbout comparison with respect to the literature on content effects:\n\n> While they conclude that \"Language models show human-like content effects on reasoning\", this is only a subordinate conclusion that suggests LLMs are in-context semantic reasoners influenced by semantic associations. However, their conclusion does not directly imply that \"LLMs are not symbolic reasoners,\" as suggested by Paradox 1: \"LLMs are in-context semantic reasoners rather than symbolic reasoners\".\n\nThe fact that LLMs show content effects does imply that they are not symbolic reasoners (like people aren't). As you mentioned, symbolic reasoning \"depends only on the form or the syntax of the premises and the conclusion, irrespective of the specific contents of this argument\". Content effects are the negation of this: the prior work on this shows exactly that LLM reasoning does not \"depend only on the form or the syntax of the premises and the conclusion\". Therefore, LLMs are not symbolic reasoners. As such, this point (Paradox 1) is implied by prior work.\n\nAfter seeing the proper failure examples in Appendix E, I would not expect any LLM to succeed at this task in any case, at least not in this current format. The context is extremely long, and the LLM has to guess in one step what is the relevant rule and the list of facts (when it is predicting the token coming after `To prove the statement ..., we can use the rule `). This is a case where you'd certainly expect to need some trial-and-error and possible backtracking, including for a trained human. To argue that LLMs really cannot do this task at all, it would be more compelling to find much simpler examples where they still fail at it, not one that spans 2 pages.\n\nThe clarifications alleviate my concerns about the novel rule generalization experiments to some extent (I was confused by the \"5 training Symbolic Trees / 3 for testing\" in Page 7). However, I do think the negative result from Figure 6 is missing some positive control to be convincing. From what we have observed in all the literature on training Transformers, we'd expect that you should see _some_ generalization to novel rules with enough training tasks and diversity, and if enough information is given in context, even if not nearly as strong as a symbolic reasoner. Having just one experiment where you fail to see any generalization (3.2.3 is quite short in detail) still leaves the doubt on whether the experiment makes sense. I strongly encourage the authors to find a positive control first, where varying a specific axis of the setup (e.g., perhaps the maximum depth of the logical rules) makes generalization harder. In such a case, we'd be more confident that we're seeing the influence of a particular variable on the results.\n\nFor these reasons, I'd like to keep my original score."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715009122,
                "cdate": 1700715009122,
                "tmdate": 1700715009122,
                "mdate": 1700715009122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lhPLEzkIDp",
            "forum": "jzvWwv4gMx",
            "replyto": "jzvWwv4gMx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_Zwsu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2413/Reviewer_Zwsu"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies whether the logical reasoning capability of large language model generalizes. They evaluate deductive, inductive, and abductive reasoning.\n\nFirst, they replaced semantic words with pure symbols and found that LLMs perform much worse on the Symbolic Tree dataset which consists of family tree relations. In contrast, there's no drop for ProofWriter which consist of fictional facts and rules.\n\nSecond, they finetuned Llama2 on symbolic reasoning tasks from one domain (Symbolic Tree), which made the gap disappear in domain, but found that the finetuned model cannot generalize to other domains (ProofWriter, RuDaS, FOLIO).\n\nThey concluded that the reasoning abilities of LLMs were confounded by memorizing the semantics, and even if finetuned, it uses template matching instead of truly learning the rules."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The writing is relatively easy to understand.\n2. There are a few interesting empirical findings from the carefully designed experiments, e.g.\n(1) Finetuning on symbolic reasoning generalizes to unseen facts, but finetuning on semantics doesn't.\n(2) Finetuning on symbolic reasoning can help with generalization in semantic reasoning.\n3. The paper found previous works either focusing on a single domain or are confounded by semantics, and try to address their shortcomings."
                },
                "weaknesses": {
                    "value": "1. I think the major weakness is the lack of novelty. Previous works [e.g. Saparov & He] already showed that semantics affects LLMs's reasoning ability, and that if we give new fact and rules contrary to the pretraining prior, the model struggles with learning those new rules. I think the main message of this paper is the same thing and not very new.\n2. While I agree that looking at test performance on multiple OOD datasets is important, I hope the authors can explain more clearly whether the datasets contain the same logic rules as the training dataset (LogicTree). If they're different, why do we expect finetuning on LogicTree would generalize at all? Requiring the model to generalize to any novel symbolic rule OOD doesn't seem reasonable to me. Usually for domain generalization one has to specify the boundary of domains. Is this all first-order logic or propositional logic? The delineation seems unclear to me, and I'm not sure inductive reasoning is comparable to deductive reasoning, since we would also not want the model to learn spurious correlations in context. I think the authors should clarify the exact scope of expected generalization in mathematical language. For example, we may want to train on 2-hop but generalize to multi-hop problems, etc.\n3. Some minor issues:\n(1) All tables and plots: missing error bars\n(2) Tables 4, 5, 6 can have more informative row names. The current row names are hard to parse.\n(3) Table 6 is lacking context. What is the baseline we are comparing to?"
                },
                "questions": {
                    "value": "1. Table 1: Why does induction column miss some values?\n2. LLMs can perform simple arithmetics and it couldn't have seen all possible additions / multiplications etc. during training. Doesn't this show it have some ability to learn rules beyond sematics?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2413/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699595789109,
            "cdate": 1699595789109,
            "tmdate": 1699636176358,
            "mdate": 1699636176358,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jw4DU4ereh",
                "forum": "jzvWwv4gMx",
                "replyto": "lhPLEzkIDp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Zwsu (part 1/2)"
                    },
                    "comment": {
                        "value": "Thank the reviewer for their comments.\n\n\n>  W1. lack of novelty\n\nWe would like to address your concerns regarding the novelty of our work and stress how our findings on paradox 1 differ from the conclusions drawn in prior works. We will discuss paradox 2 subsequently.\n\nWhile Saparov and He (2022) conclude that \"Language models show human-like content effects on reasoning\", this is only a subordinate conclusion that suggests LLMs are in-context semantic reasoners influenced by semantic associations. However, their conclusion does not directly imply that \"LLMs are not symbolic reasoners,\" as suggested by Paradox 1: \"LLMs are in-context semantic reasoners rather than symbolic reasoners\".\n\n\nAccording to the definition of deductive reasoning from Wikipedia (https://en.wikipedia.org/wiki/Deductive_reasoning), **logical reasoning should be formal and depends only on the form or the syntax of the premises and the conclusion, irrespective of the specific contents of this argument.** To this end, our task decouples semantics and focuses on pure symbolic reasoning (only provided syntax). When we feed these to LLMs, they show significantly worse performance compared to when normal semantic words are fed. This phenomenon indicates that LLMs fail to invoke the basic formal reasoning abilities of humans but instead rely on shallow semantic associations for prediction. \n\nMoreover, we conduct comprehensive experiments on various logical reasoning, including deduction, induction and abduction tasks and find that they perform notably worse in these types of reasoning tasks compared to deduction, regardless of whether semantics or symbols are used. When semantics are decoupled, the drop in performance is even more significant. These findings highlight the\u00a0considerable room for improvement\u00a0in LLMs' reasoning abilities and suggest that relying solely on semantics to achieve symbolic reasoning is challenging.\n\n[1]Saparov, Abulhair, and He He. \"Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.\"\u00a0*arXiv preprint arXiv:2210.01240*\u00a0(2022).\n\n\n\n> W2. concern about the generalization\n\n\nWe would like to clarify that our paper focuses on first-order logical reasoning abilities, which will be emphasized in the revised version. Unlike the inference of machine learning which usually generalizes to the same distribution, an agent with \"true\" logical reasoning abilities can reason with any given information to solve new, unfamiliar problems, regardless of any prior knowledge or different knowledge representations. In other words, it can also generalize across facts, rules and representations (Sec 1 Line 13): \n\n1. Facts: We test the same rules as in the training but with different facts (Sec 3.2.1 Line 4), as shown in the testing settings in Table 4.\n2. Representations: Logical rule reasoning should be independent of different semantic content, including the rep_xx_with_xx examples in Table 4.\n3. Rules: We test different first-order rules, such as training on $r1(A,B) \\land r2(B) \\rightarrow r3(B,A)$ and testing on $r1(A,B) \\land r2(B,C) \\rightarrow r3(C,B)$, as demonstrated in Fig 3. Different hop rules are also considered as different rules. Relevant settings include pf-sym-depth1, pf-sym-depth2, and Rudas in Table 5.\n\nOur comprehensive experimental results show that LLMs fine-tuned on symbolic logic datasets can achieve generalization in (1) and (2) to some extent with the help of template matching. However, they struggle to generalize to novel rules, emphasizing that fine-tuning LLMs does not achieve generalizable logical reasoning.\n\n> W3. some minor issues: (1) All tables and plots: missing error bars (2) Tables 4, 5, 6 can have more informative row names. The current row names are hard to parse. (3) Table 6 is lacking context. What is the baseline we are comparing to?\n\nRe W3 (1) and (2): We have included error bars and refined the row names in Tables 4, 5, and 6 to be more descriptive and concise in the revised version of our paper, with the error bars highlighted in red. Thank you for your suggestions.\n\nRe W3 (3): Table 6 represents the performance of RuDaS and FOLIO of the non-fine-tuned LLaMA2-13B-chat model. The results reveal that, although FOLIO's performance surpasses random, it remains inferior to the non-fine-tuned LLaMA2-13B-chat. This further emphasizes our finding that fine-tuned LLMs struggle to generalize to novel rules."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193366321,
                "cdate": 1700193366321,
                "tmdate": 1700193443394,
                "mdate": 1700193443394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RN8SZe091w",
                "forum": "jzvWwv4gMx",
                "replyto": "lhPLEzkIDp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2413/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Zwsu (part 2/2)"
                    },
                    "comment": {
                        "value": "> Q1: Table 1: Why does induction column miss some values?\n\nThis is because we did not perform a few-shot setting. Symbolic Tree dataset has a limited number of rules, and these rules exhibit a degree of interconnectedness. For instance, a rule like 'grandAuntOf' can be derived through the composition of rules like 'sisterOf' and 'grandparentOf'. The potential overlap and dependencies among the rules could skew the assessment of the model's true inductive reasoning capabilities. Consequently, we chose to only conduct a zero-shot setting where we provide facts to infer new rules without exposing the model to similar examples.\n\n> Q2: LLMs can perform simple arithmetics and it couldn't have seen all possible additions / multiplications etc. during training. Doesn't this show it have some ability to learn rules beyond sematics?\n\nThe review's observation about LLMs performing simple arithmetic tasks raises an intriguing point about their ability to learn rules beyond semantics. However, our research suggests that this ability may not be as robust as it appears. The performance in simple arithmetic tasks could be attributed to data leakage or the application of pre-learned arithmetic solution templates, rather than a genuine understanding or mastery of arithmetic rules.\n\nFor instance, when testing GPT-4 with a large number arithmetic problem, such as \"128373 + 2879321873=\", the model provided an incorrect answer (2879449246 instead of the correct answer 2879450246). This error indicates a limitation in the model's ability to process and apply arithmetic rules, especially in the context of large numbers. In other words, we think if it truly master arithmetics rules (carry or borrow operations), it should process large number. It suggests that while LLMs may display a superficial capability to perform arithmetic operations, they may not truly master the underlying rules.\n\nTherefore, while LLMs exhibit some level of proficiency in arithmetic, this should not be mistaken for a deep or comprehensive understanding of arithmetic rules. Our findings point towards a need for further development in logical reasoning area to enhance the true rule-learning capabilities of LLMs beyond semantic associations."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2413/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700193468618,
                "cdate": 1700193468618,
                "tmdate": 1700193498458,
                "mdate": 1700193498458,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]