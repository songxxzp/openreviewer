[
    {
        "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models"
    },
    {
        "review": {
            "id": "cbWPcV04A9",
            "forum": "gfFVATffPd",
            "replyto": "gfFVATffPd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1031/Reviewer_MD6P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1031/Reviewer_MD6P"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an interesting finding that the magnitude of attention contribution to \u201cconstraint entities\u201d (something like key entities associated with the prediction) is well-correlated with popularity of the constraint entity, the constrainedness of the query, and more importantly, the correctness of the LLM prediction. The authors define metrics to quantify such attention contribution and expect to use the metric to predict factual errors of LLM predictions. On experiments across multiple datasets, the proposed method demonstrates comparable factual error prediction performance to the confidence (i.e. logits) baseline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper presents interesting and novel findings of a phenomenon in the hidden representations of LLMs, that the magnitude of attention contribution is correlated with factualness of the prediction. It may inspire future researchers to discover and probe more interpretable inner patterns of LLMs, which could potentially help researchers understand and explain models\u2019 behaviours.   \n2. The authors perform comprehensive analysis to verify the correlation of attention contribution with various properties such as popularity, constrainedness, and factualness. These analyses are convincing and the different types of visualization are informative and efficient to convey the messages.  \n3. The paper is very well-written."
                },
                "weaknesses": {
                    "value": "1. Although the findings are interesting to know, I doubt the practicality of the proposed method and the contribution it makes from a practical perspective:  \n    a. Attention contribution is computed assuming access to the constraint entities, which are not available in most of cases  \n    b. The proposed method is not better on factual error prediction than the simple confidence baseline. It seems to me it does not bring any new information beyond the confidence score\n\n2. I think this paper focuses on and only studies a narrow notion of factuality, where constraint entities exist as in a world knowledge domain. It is unclear whether attention contribution is a valid metric to explain other factual errors, such as mathematical errors and inference errors where the traditional notion of \u201centities\u201d may not always exist. This is important since researchers talk about factuality and hallucination actually in a quite broad scope in the LLM context nowadays, not limited to entity-based knowledge.\n\n3. From Figure 14-23, all the datasets look simple and toy. Although it appears that the experiments are conducted on a diverse range of datasets, these datasets turn out to be very similar style-wise \u2013 they are very similar QA tasks with one-entity answers. While I understand that it is difficult to perform experiments on more realistic data since the constraint entities will become unavailable, I am not sure how general the proposed findings are for LLMs given these toy datasets."
                },
                "questions": {
                    "value": "1. In section 5.1 the authors mention fitting Logistic Regression, does that mean you need a training dataset with labels to learn the parameters w and b? If so, how large is the training data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698851348304,
            "cdate": 1698851348304,
            "tmdate": 1699636029345,
            "mdate": 1699636029345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PfOjJtuiHD",
                "forum": "gfFVATffPd",
                "replyto": "cbWPcV04A9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MD6P 1/2"
                    },
                    "comment": {
                        "value": "Dear Reviewer MD6P,\n\nThank you so much for your detailed insights and the time you took to review our paper. We are extremely excited that you find our findings novel, and find that our work can inspire many future works! At a high level, we provide new results that\n- (i) we show that the errors that attention and confidence make are different\n- (ii) this lets us come up with a method that combines confidence and attention, which works better than confidence alone.\n\nWe further incorporated the suggestions, clarified the contribution of our framework compared to the existing literature, and resolved ambiguities in the text.\n\n1. *The proposed method is not better on factual error prediction than the simple confidence baseline. It seems to me it does not bring any new information beyond the confidence score*\n\n **Do confidence and attention result in the same predictions? Combining Attention and Confidence**: In Appendix E Figure 15, we compare the predictions by Attention vs Confidence for all three models in the Llama family. Overall, while they do correlate, there are many cases where both predictors disagree. For instance, the model can be overconfident and attention can correctly identify factual errors (see top left). Because the errors are not identical, we can combine them to build a better system. \n\nWe extended our results with a *Combined* predictor, where we add confidence in addition to the attention features, and perform logistic regression. This model mostly performs the best overall, in terms of AUROC (Figure 6), and risk in either tail (bottom 20% and top 20% most confident predictions, Figures 9 and 10). \n\n **Why is the finding that model attention can be used to predict constraint satisfaction and is comparable to confidence important\\interesting?** We would like to highlight that the LLM is optimized using confidence as the objective (e.g. probability of the next token). We find it to be an extremely interesting finding that the process that the LLM uses to produce an output, without information about the objects it is acting on (predicted probability distribution), gives almost as much information as the output. We believe our findings could lead to several follow-up works in studying factual errors and the model\u2019s reliability. Why do we have more attention to popular entities? How can we manipulate attention to increase constraint satisfaction and steer the model behavior? Our framework and findings open interesting questions and can pave the way for future research in this area.\n\n**Important difference between attention and confidence:** As we emphasize in the paper, note that confidence is only associated with the generated tokens and cannot be traced or mapped back to individual constraints for queries with multiple constraints. This is a crucial functional advantage of SAT-Probe and becomes even more important as the query complexity increases.\n\n2. *Although the findings are interesting to know, I doubt the practicality of the proposed method*\n\n**Constraint availability:** We agree that our experiments focus on settings that assume access to constraints. Nevertheless, constraint extraction from user queries is an interesting language task to study by itself and can be considered as a pre-processing step prior to any factual verification method. Relevant methods may rely on entity recognition, previous work [a,b] on parsing declarative queries from natural text, or innovations in chain-of-thought methods that first extract main constraints from queries before generating factual answers. We added this note in our conclusion section.\n\n[a] Elgohary, Ahmed, Saghar Hosseini, and Ahmed Hassan Awadallah. \"Speak to your parser: Interactive text-to-SQL with natural language feedback.\" arXiv preprint arXiv:2005.02539 (2020).\n\n[b] Yaghmazadeh, N., Wang, Y., Dillig, I., & Dillig, T. (2017). SQLizer: query synthesis from natural language. Proceedings of the ACM on Programming Languages, 1(OOPSLA), 1-26."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013326135,
                "cdate": 1700013326135,
                "tmdate": 1700013326135,
                "mdate": 1700013326135,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7nWcog1DKM",
                "forum": "gfFVATffPd",
                "replyto": "cbWPcV04A9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MD6P 2/2"
                    },
                    "comment": {
                        "value": "3. *I think this paper focuses on and only studies a narrow notion of factuality, where constraint entities exist as in a world knowledge domain*\n\n**Generality of the framework:** We agree with the reviewer that the CSP framework does not cover all potential queries to LLMs. Our work lays out a framework for mechanistic interpretability to analyze a broader class of queries, thus contributing to the literature: **existing work** in factuality and mechanistic interpretability in LLMs (e.g. Meng et al. NeurIPS 2022, Geva et al. EMNLP 2023) only considered single-constraint queries with well-defined, yet restrictive (subject, relation, object) structure. While there are still limitations, it creates further opportunities regarding what we can understand with a mechanistic lens, in particular by directly mapping attention patterns of individual constraints to model performance. We would appreciate it if the reviewer could consider the context of the contribution here. \n\nFurther, we do not only look at CounterFact and we expand the set of datasets further to questions that are verifiable and that can also be reused in future research. We further agree with the reviewer that there is more work to do around mathematical knowledge and reasoning, and we believe these are interesting avenues for future work. We added this to the conclusion section, thank you for your suggestions.\n\n4. *From Figure 14-23, all the datasets look simple and toy.*\n\nAs further shown in Table 1 and Figure 6a, our datasets also contain queries with double constraints (which were previously not covered in related work). Most of the related work on mechanistic interpretability for factual queries has used CounterFact (which we also study) for single-constraint queries. Here, we expanded the set of datasets, and hope that this can serve as a resource to the community to study more complex query forms.\n\n5. *does that mean you need a training dataset with labels to learn the parameters w and b?*\n\nIn each of the datasets, we split the dataset into 50% train -50% test splits and train on the training set. Thus, the number of data points used to train a probe is half of the number of queries in Table 1 when testing on respective datasets.\n\n**Overall**, we do believe we are studying the problem of factual error prediction from a novel point of view, our findings around attention and factual error prediction, datasets, and the framework could prove useful to the literature. If you also find similar value in this research, the future work that can build on our insights,  and contextualization in the literature, we would appreciate it if you could consider increasing your score. Thank you very much once again, and we are happy to follow up with any additional questions you may have!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013386381,
                "cdate": 1700013386381,
                "tmdate": 1700013584851,
                "mdate": 1700013584851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bBJPHnJTOX",
                "forum": "gfFVATffPd",
                "replyto": "Y9mAcvUjTg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1031/Reviewer_MD6P"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1031/Reviewer_MD6P"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response, and I appreciate that the authors add the results of a combined baseline.\n\nTo clarify, I do acknowledge the strengths of this work and the contribution of probing the attention patterns for factuality, which was the main reason I gave a positive score of 6.\n\nHowever, my concerns on practicality remain -- the combined results mitigate this concern a bit, but the reliance on constraint availability and the toy setting/scope of factuality and evaluation datasets are limitations of this work. I understand that constraint extraction as a preprocessing step may be done well, but this argument is pretty weak without experimental evidence -- the experiments of this paper assume existence of \"ground-truth\" constraints, which is a rigorous condition that the confidence baseline does not require. Furthermore, I know that many factuality works are performing experiments on toy settings or datasets, especially before LLMs appear. However, as LLMs come out and release so much power, I really look forward to moves of this direction to more realistic settings, at least attempting to do so, for example in [1] (this work is concurrent and not particularly relevant, but they tried to evaluate on more realistic data). \n\nAgain, I lean towards acceptance of this paper, yet it is just not exciting enough for me to further increase the score due to these concerns.\n\n\n[1] Chern et al. FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. 2023"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664354213,
                "cdate": 1700664354213,
                "tmdate": 1700664354213,
                "mdate": 1700664354213,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LOnu36wBbh",
            "forum": "gfFVATffPd",
            "replyto": "gfFVATffPd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1031/Reviewer_m32R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1031/Reviewer_m32R"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on investigating characteristics of attention patterns when LLM makes factual errors or hallucinates. They show that entity tokens (defined as constraining tokens) in the query input exhibit strong correlation with the answer entity tokens. The authors had three key observations 1.) the more popular an entity-relation pair is (i.e., appears more frequently in corpus), LLM is less likely to be factually incorrect about it 2.) attention scores can serve as a way to measure LLM confidence and correctness and 3.) the larger the LLM the more the attention scores and attention scores correlate in a similar way across different size models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.) This work looks at mechanistic interpretability especially with respect to factor errors made by LLMs, which has not been studied before as per my knowledge.\n\n2.) They do extensive analysis and show the effectiveness of their probing technique across different datasets and models.\n\n3.) SAT-probe can also help predict failures by analyzing attention scores mid-training, which can be useful of debugging purposes."
                },
                "weaknesses": {
                    "value": "None I can think of"
                },
                "questions": {
                    "value": "1.) The observation that \"when the LLM is accurate, there is more attention to constraint token\" suggests that when LLM is confident it is often right. How does this generalize in cases shown in prior work when LLMs confidently hallucinate? It would seem that attention scores would be high even when hallucinating. In your experience, is there any way to differentiate between attention scores (or some other variable) for correct vs incorrect confident answers.\n\n2.) In Figure 5, it looks like attention is summed across layers, its unclear if the final score was normalized by number of layers? Is it possible that attention scores are more comparable between 7B and 13B as compared to 7B and 70B because of the number of layers scaling?\n\n3.) Were there any insights as to how attention patterns vary across different layers, especially for different sized models? for e.g., higher attention scores for hallucinating tokens towards the later layers perhaps?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699039465264,
            "cdate": 1699039465264,
            "tmdate": 1699636029262,
            "mdate": 1699636029262,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n69Gj5KyNR",
                "forum": "gfFVATffPd",
                "replyto": "LOnu36wBbh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer m32R"
                    },
                    "comment": {
                        "value": "Dear Reviewer m32R,\n\nThank you so much for your kind words and the time you took to review our paper. We are truly excited by your support, and we appreciate that you found no major weaknesses; and thanks for your insightful questions!\n\nAt a high level, we provide new results that\n- (i) show that the errors that attention and confidence make are different\n- (ii) this lets us come up with a method that combines confidence and attention, which works better than confidence and attention alone.\n\nBelow we respond to the individual points you raised.\n\n1. *...suggests that when LLM is confident it is often right. How does this generalize in cases shown in prior work when LLMs confidently hallucinate?*, *In your experience, is there any way to differentiate between attention scores (or some other variable) for correct vs incorrect confident answers*\n\n**Differences between confidence vs attention scores**: In Appendix E Figure 15, we compare the predictions by Attention vs Confidence for all three models in the Llama family. Overall, while they do correlate, there are many cases where both predictors disagree. For instance, the model can be overconfident and attention can correctly identify factual errors (see top left). Thus, there is value in considering both predictors.\n\n**Combining attention and confidence:** We extended our results with a *Combined* predictor, where we add confidence in addition to the attention features, and perform logistic regression. This model mostly performs the best overall, in terms of AUROC (Figure 6), and risk in either tail (bottom 20% and top 20% most confident predictions, Figures 9 and 10). \n\n2. *In Figure 5, it looks like attention is summed across layers, its unclear if the final score was normalized by number of layers?*\n\n**Normalization in Figure 5**: We sum the values across layers, and normalize all numbers by the maximum observed value as indicated in the caption of Figure 5. Indeed, for models with a larger number of layers, the unnormalized values are larger. We wanted the visualization to be more interpretable within the context of the model being investigated. We added further clarification to the caption (see in red), and thank you for raising this point.\n\n3. *Were there any insights as to how attention patterns vary across different layers, especially for different-sized models? for e.g., higher attention scores for hallucinating tokens towards the later layers perhaps?*\n\n**Attention Across Layers**: Very interesting question! We find that scaling matters here. In Appendix Figure 7, we discuss the impact of early stopping. Interestingly, we observe that we can make failure predictions in the earlier layers as well as the later layers for smaller models (left two plots). However, for larger models, we need to observe the attention in the later layers to make better predictions. We believe one can hypothesize that attention scores in later layers are more predictive for the larger models than smaller models, based on these findings.  Other than this, we did not observe stronger findings around larger attention values in later layers indicating factual errors, but we believe our tools (e.g. tracking the dynamics of attention to constraints) can lead to future research on this question.\n\n**Overall**, we do believe we are studying the problem of factual error prediction from a novel point of view, our findings around attention and factual error prediction, datasets, and the framework could prove useful to the literature. If you also find similar value in this research and future works that can build on these findings, we would really appreciate it if you could consider increasing your score. Thank you very much once again, and we are happy to follow up with any additional questions you may have!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013223317,
                "cdate": 1700013223317,
                "tmdate": 1700013575083,
                "mdate": 1700013575083,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6jOa1jt3Te",
            "forum": "gfFVATffPd",
            "replyto": "gfFVATffPd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1031/Reviewer_KSCt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1031/Reviewer_KSCt"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to use Transformers\u2019 internal attention mechanisms to understand the internal mechanism behind factual errors, in contrast to previous work focusing more on discovering internal mechanism behind generate *correct* factual answers. It poses the problem of answering factual queries as a constraint satisfaction problem whereby each factual queries presents a conjunction of constraints (e.g. \u201cis a basketball player\u201d and \u201cwas born in 1988\u201d) and the query is answered correctly if each constraint is satisfied. The paper begins with a series of analyses linking attention on the constraint tokens with factuality, then proposes the SAT probe, a linear classifier over the attention on the constraint tokens, trained to predict when all constraints will be satisfied (and thus when the LLM will make factually correct predictions)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Casting factual queries as a constraint satisfaction problem is an interesting approach, and the specifically the finding that attention on the constraint tokens is correlated/predictive of factual correctness is novel.\n\n2. Compared to confidence-based classification, the SAT probe is more efficient \u2014 allowing the model to not have to go through the entire inference procedure \u2014 and fine-grained \u2014 allowing us to isolate of which exact constraint was disregarded.\n\n3. The experiments covered many different datasets and a wide range of model sizes \u2014 and the findings generally carried over between the various model sizes and datasets. The wealth of empirical findings is insightful and convincing."
                },
                "weaknesses": {
                    "value": "1. My main point of contention is that the SAT probe actually underperforms confidence classification by a statistically significant margin in 17/27 of the settings studied in in Figure 6 (it would be good to note what type of error bars are being plotted and how many trials this was over/what the source of randomness was). This seems to undermine \u201cwe find that SAT PROBE mostly performs comparably to and sometimes better than the model\u2019s CONFIDENCE in the correctness prediction task\u201d (section 5.1, results).\n    1. Section 5.2 and appendix dive into results that demonstrate that analyzing attention at earlier layers can be useful \u2014 as it is generally more efficient and allows us to interpretably isolate the constraint that failed. This should be moved up and highlighted as one of the main advantages of this method. (The preliminary analyses on popularity and constrainedness predicting performance / attention predicting popularity and constrainedness can be compressed.)\n    2. Table 7 examines the ability of SAT probe to isolate *which* constraint was violated when there are multiple constraints. Some more detail is necessary here: in the dataset, when 1 constraint is violated, how often is it the case that both constraints are violated? Can we isolate the experiment to cases where one and only one of the constraints are violated, and have the SAT probe predict which one?\n\n2. I do wonder about the generality of thinking of factuality as a CSP \u2014 in particular simply treating it as a conjunction of constraints. There are many other types of composition, such as disjunction, nesting (multi-hop queries like \u201cparents of the President of the USA\u201d), etc., that cannot be dealt with in this framework. Furthermore, not are all factual queries can be dealt with just simply using constraints. For example, there may be additional reasoning operations on top of constraints (e.g. numerical counting). While I don\u2019t expect this one paper to comprehensively deal with all types of factual queries, perhaps some discussion of the coverage of this framework is warranted.\n    1. More generally, prior work has already examined factual queries from the lens of database (SQL) queries, which implicitly already carries constraints (i.e. the WHERE clauses) while also being more flexible to different types of constraint compositions, and many more operations. What is the comparative advantage to thinking of factual queries as conjunctions of constraints in this way?\n\n3. Furthermore, the name \u201cconstraint satisfaction problems\u201d generally suggests a different set of problems to me \u2014 where the key focus is not on evaluating the factuality of each atomic variables, but on how we may search through assignments to variables in order to create a permissible solution. (This is a relatively minor naming point, though does open up a potential question on the implication of thinking of queries as a constraint satisfaction problem, in the more generic sense, which could introduce much more novelty to this paper.)"
                },
                "questions": {
                    "value": "1. Are there any takeaways from this analysis and method for how we can actually fix factual errors? Or perhaps prevent models from generating factual errors?\n\n2. This method mainly relies on LM uncertainty to pick up when the LM is wrong, which is fine (I\u2019m not sure if there\u2019s any other way to internally discover when the LM is lying without external tools). However, broadly speaking, are there ever cases of the LM being confidently wrong? Or is it the case that, because of the distribution of the pre-training data, whenever there\u2019s factual errors it will always be much less certain than when there\u2019s correct answers?\n\n3. Section 5.2, Early Stopping: \u201cFor Llama-2 7B and 13B, we can stop the inference early without degradation in the average performance and save 50% of wall-clock time on failures for most datasets.\u201d Is there a breakdown of wall-clock time for each of the datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1031/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1031/Reviewer_KSCt",
                        "ICLR.cc/2024/Conference/Submission1031/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1031/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699391524302,
            "cdate": 1699391524302,
            "tmdate": 1700495045115,
            "mdate": 1700495045115,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DVhuKDsqwm",
                "forum": "gfFVATffPd",
                "replyto": "6jOa1jt3Te",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Review"
                    },
                    "comment": {
                        "value": "Hello Reviewer KSCt,\n\nFirst, thank you for your time reviewing our work! We believe the above review belongs to another paper, and the review for our paper is possibly elsewhere. Would you mind updating your review?\n\nThank you so much!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699638000311,
                "cdate": 1699638000311,
                "tmdate": 1699638000311,
                "mdate": 1699638000311,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HCT0FLVBWj",
                "forum": "gfFVATffPd",
                "replyto": "DVhuKDsqwm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1031/Reviewer_KSCt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1031/Reviewer_KSCt"
                ],
                "content": {
                    "title": {
                        "value": "Fixed review mixup"
                    },
                    "comment": {
                        "value": "Hi authors, thank you for pointing this out! That was my bad. I have updated my review with the one for your paper. Apologies for the mixup!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699939985105,
                "cdate": 1699939985105,
                "tmdate": 1699939985105,
                "mdate": 1699939985105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OxfxRiKTXN",
                "forum": "gfFVATffPd",
                "replyto": "6jOa1jt3Te",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KSCt 1/2"
                    },
                    "comment": {
                        "value": "Dear Reviewer KSCt,\n\nThank you so much for your detailed insights and the time you took to review our paper, and also for updating your review promptly, we really appreciate your help! We are extremely excited that you find our framework novel, and experiments insightful and convincing! \n\nAt a high level, we provide new results where\n- (i) we show that the errors made by attention and confidence are different\n- (ii) this lets us come up with a method that combines confidence and attention, which works better than confidence or attention alone.\n\nWe further incorporated the suggestions, clarified the generality of the framework compared to the analyses in the existing literature, and resolved ambiguities in the text.\n\nBelow we respond to the individual points you raised.\n\n1. *My main point of contention is that the SAT probe actually underperforms confidence classification by a statistically significant margin*, *However, broadly speaking, are there ever cases of the LM being confidently wrong?*\n\n**Confidence vs Attention:** We give two sets of new results to address this question:\n\n**Do confidence and attention result in the same predictions? Combining attention and confidence**: In Appendix E Figure 15, we compare the predictions by Attention vs Confidence for all three models in the Llama family. Overall, while they do correlate, there are many cases where both predictors disagree. For instance, the model can be overconfident and attention can correctly identify factual errors (see top left subfigure). Because the errors are not identical, we can combine them to build a better system. \n\nWe extended our results with a *Combined* predictor, where we add confidence in addition to the attention features, and perform logistic regression. This model mostly performs the best overall, in terms of AUROC (Figure 6), and risk in either tail (bottom 20% and top 20% most confident predictions, Figures 9 and 10). Thus, there is value in considering both predictors, in addition to providing fine-grained predictions per individual constraints and early stopping with attention.\n\n **Why is the finding that model attention can be used to predict constraint satisfaction and is comparable to confidence important\\interesting?** We would like to highlight that the LLM is optimized using confidence as the objective (e.g. probability of the next token). We find it to be an extremely interesting finding that the process that the LLM uses to produce an output, without information about the objects it is acting on (predicted probability distribution), gives almost as much information as the output. We believe our findings could lead to several follow-up works in studying factual errors and the model\u2019s reliability. Why do we have more attention to popular entities? How can we manipulate attention to increase constraint satisfaction and steer the model behavior? Our framework and findings open interesting questions and can pave the way for future research in this area.\n\n- *This should be moved up and highlighted as one of the main advantages of this method.*, *This seems to undermine \u201cwe find that SAT PROBE mostly performs comparably to and sometimes better*\n\nWe adjusted the tone of the claims about that table per se and highlighted the advantage of attention in being more efficient and allowing us to interpretably isolate the constraint that failed. Thank you for these suggestions.\n\n- *it would be good to note what type of error bars are being plotted and how many trials this was over/what the source of randomness was*\n\nError bars are discussed in the caption of Figure 6, i.e. \u201cError bars show the\nstandard error over 10 different random train/test splits.\u201d"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700012992370,
                "cdate": 1700012992370,
                "tmdate": 1700109267682,
                "mdate": 1700109267682,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zzyf8VBRij",
                "forum": "gfFVATffPd",
                "replyto": "6jOa1jt3Te",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1031/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KSCt 2/2"
                    },
                    "comment": {
                        "value": "2. *I do wonder about the generality of thinking of factuality as a CSP*\n\n**Generality of the Framework:** We agree with the reviewer that the CSP framework does not cover all potential queries to LLMs. Our work lays out a framework for mechanistic interpretability to analyze a broader class of queries, thus contributing to the literature: **existing work** in factuality and mechanistic interpretability in LLMs (e.g. Meng et al. NeurIPS 2022, Geva et al. EMNLP 2023) only considered single-constraint queries with a well-defined, yet restrictive (subject, relation, object) structure. We would appreciate it if the reviewer could consider that in the context of mechanistic understanding of LLMs, moving to multiple constraints is novel and allows broader investigations. While there are still limitations, it creates further opportunities regarding what we can understand with a mechanistic lens, e.g. in this case by directly mapping attention patterns of individual constraints to model performance.\n\nThanks for pointing out the analogy to how SQL and other declarative language model constraint-based search! Indeed, previous work (such as \u201cConjunctive-query containment and constraint satisfaction\u201d from Kolaitis & Vardi, 1998 but also others) have used the same framework to model similar queries, albeit not for machine learning models. The analogy is a great argument for leveraging the same framework but with an ML perspective and by using architectural information native to trained models (rather than stored information in database systems). If we missed any other works here, please let us know and we will be happy to discuss further and cite further work on this topic. \n\n\n3. *Is there a breakdown of wall-clock time for each of the datasets?*\n\n**Wall-clock time:** Our comments are based on the fact that the inference time scales linearly in the number of layers, and we can sometimes use less than 50% of the layers to predict failure as successfully as using the entire network. While the exact inference time will depend on the prompt length and the hardware, it will scale linearly in the number of layers.\n\n4. *Table 7 examines the ability of SAT probe to isolate which constraint was violated when there are multiple constraints. Some more detail is necessary here: in the dataset, when 1 constraint is violated, how often is it the case that both constraints are violated?*\n\nThe numbers vary across datasets \u2013 sometimes only one fails and sometimes both do. Below is a table summarizing the statistics for Llama-2 70B performance in the first train/test split as an example: \n\n| Dataset Name | Fraction of Samples | Which Constraint Failed |\n|--------------|---------------------|-------------------------|\n| Nobel Winner | 0.59                | Only One                |\n| Nobel Winner | 0.26                | Both                    |\n| Movies       | 0.04                | Only One                |\n| Movies       | 0.53                | Both                    |\n| Books        | 0.17                | Only One                |\n| Books        | 0.67                | Both                    |\n| Words        | 0.57                | Only One                |\n| Words        | 0.13                | Both                    |\n\nThus, indeed there are datasets where only one of the constraints fails, and attention can identify the ones that do (shown by Table 7). \n\n**Insights into how to fix errors:** This is an important question that warrants future research. While we do not have experiments regarding this point, our findings can lead to future investigations. E.g. It would be interesting to intervene in attention to constraints to modify the model behavior. However, our setting is akin to Selective Classification, where we focus on abstention as the action to take when reliability is expected to be low. We added further suggestions in the conclusion section on this point, thank you for the suggestions.\n\n**Overall**, we do believe we are studying the problem of factual error prediction from a novel point of view, our findings around attention and factual error prediction, datasets, and the framework could prove useful to the literature. If you also find similar value in this research and future works that can build on these findings, we would really appreciate it if you could consider increasing your score. Thank you very much once again, and we are happy to follow up with any additional questions you may have!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1031/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013082542,
                "cdate": 1700013082542,
                "tmdate": 1700109286074,
                "mdate": 1700109286074,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]