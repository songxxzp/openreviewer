[
    {
        "title": "Information-Theoretic World Model learning for Denoised Predictions"
    },
    {
        "review": {
            "id": "nb04UQo0aw",
            "forum": "MdHDUsP2lt",
            "replyto": "MdHDUsP2lt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7554/Reviewer_Dg3d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7554/Reviewer_Dg3d"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to maximize mutual information between the current state and future state while minimizing mutual information between the past state and current state.  This is then added as an auxiliary loss into the soft actor critic setup, and is used in continuous control tasks with known rewards.  One of my biggest issues with this paper is the empirical weakness of the results, which are inconclusive in the noisy setting, and much worse in the noise-free setting (in the appendix).  The other issue concerns the quality and novelty of the idea.  It feels very similar to the InfoPower paper and it's not obvious that the proposed objective is a good way of removing distractors, nor is it clearly justified.  \n\nNotes: \n  -Filtering background noise from videos.  \n  -Minimize past information, keep info about future.  \n  -SAC with information-based loss.  \n  -Uses \"CLUB\" mutual information method to increase MI between current state and future state."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-The idea of maximizing mutual information between current/future state while minimizing mutual information between past/current state is intriguing but a bit counterintuitive.  \n  -Doing noise removal in the online setting seems like an important problem."
                },
                "weaknesses": {
                    "value": "-The reconstructions aren't terribly compelling.  While background info is removed, it's not obvious if all the information about the agent is correctly retained.  \n  -Papers like ACRO (Islam 2022) and InfoGating (Tomar 2023, which I suppose is a bit recent) should be cited.  InfoPower also feels relevant, and while it is cited, the difference doesn't seem to be sufficiently discussed.  \n  -The integration with SAC feels overly specific.  \n  -The experimental results seem fairly weak.  There is a clear improvement in cheetah run and a small improvement in walker walk, but other tasks are more modest.  \n  -The results in the appendix on standard setup seem quite bad as the proposed method is badly losing to Dreamer.  \n  -The paper doesn't do much to analyze what is happening in the information bottleneck."
                },
                "questions": {
                    "value": "-The paper refers to the setup as a lagrangian formulation, but for this to work, it seems insufficient to treat beta as a hyperparameter.  So what exactly makes it a lagrangian setup, as opposed to just adding an extra penalty?  \n\n  -What is the reason to think that increasing mutual information between present and future states is compatible with removing distractors?  For example, the background video will have temporal correlation, so we could potentially maximize that by keeping the TV in the state.  See Efroni 2022 (PPE) as well as Lamb 2022 (AC-State) for some discussion of this issue."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7554/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766620896,
            "cdate": 1698766620896,
            "tmdate": 1699636914185,
            "mdate": 1699636914185,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pUmkQGrgzt",
                "forum": "MdHDUsP2lt",
                "replyto": "nb04UQo0aw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the authors"
                    },
                    "comment": {
                        "value": "We appreciate your time and feedback. We would also appreciate your concerns on the empirical weakness. We have worked on the issues as follow:\n\n**[W1] The reconstructions aren't terribly compelling. While background info is removed, it's not obvious if all the information about the agent is correctly retained.**\\\nAcknowledging the observations about the reconstructions, it is important to highlight from our results that the method retains the majority of the information about the agent. The process effectively darkens the background, predominantly isolating and preserving the agent's details. This indicates that while background information is removed, the critical aspects of the agent are largely maintained. Future iterations of our model will focus on refining this aspect to ensure even more precise retention of all critical information about the agent. For more analysis, we have also added the reconstruction of the Cartpole Swingup task in Figure 5 of the Supplementary Material.\n\n**[W2] Papers like ACRO (Islam 2022) and InfoGating (Tomar 2023, which I suppose is a bit recent) should be cited.**\\\n**[A2]** Thanks for bringing these papers to our attention. We have cited both the papers.\n\n**[W3]  InfoPower also feels relevant, and while it is cited, the difference doesn't seem to be sufficiently discussed.**\\\n**[A3]** Thank you for pointing out the relevance of InfoPower in our discussion. While InfoPower focuses on maximizing empowerment, specifically $I(a_k;z_{k+1}|z_k)$, our objective diverges significantly from this. We deconstruct $I(z_{k+1},a_k;z_k)$ into two components: $I(z_k;z_{k+1})$ and $I(z_k,a_k|z_{k+1})$. Although the second term might appear similar to InfoPower's approach, it fundamentally differs in both concept and implementation. It's true that InfoPower and other methods we cited [1-3] employ an inverse dynamic model $q(a_k|z_k,z_{k+1})$, but our approach advocates a more generalized strategy. We propose that focusing on $I(z_{k+1},a_k;z_k)$ implicitly encompasses learning inverse dynamics, diverging from InfoPower's explicit modeling of this aspect. We have cited InfoPOWER in Section 4.3 now.\n\n\n**[W4] The integration with SAC feels overly specific.**\\\n**[A4]** The detailed description of our SAC integration is crucial for both reproducibility and clear understanding, enabling other researchers to accurately replicate and build upon our work.\n\n**[W5] The experimental results seem fairly weak. There is a clear improvement in cheetah run and a small improvement in walker walk, but other tasks are more modest.**\\\n**[A5]** In our rebuttal, we present enhanced results in additional environments such as Walker Walk and Cartpole Swingup, achieved through weighing coefficient tuning conducted subsequent to the paper's initial submission. Our updated results now show a significant improvement over the baseline methods in the specified environments. We encourage reviewing the new graphs presented in Figure 4 for a detailed comparison of these enhancements. The details of these newly optimized weighing coefficient are available in Section 4 of the Supplementary Material. We have also added Cartpole swingup in the random settings (Figure 2 for results and Figure 5 for reconstruction in the Supplementary Material), which clearly aligns with our claims. Additionally, we deliberately highlight the limitations of all methods, including our own, in three challenging environments. This was done to underscore the current limitations of state-of-the-art methods in handling these complex environments, thereby emphasizing the need for further research in this area.\n\n**[W6] The results in the appendix on standard setup seem quite bad as the proposed method is badly losing to Dreamer.**\\\n**[A6]** DPI, although not outperforming Dreamer in static environments, is designed for effective generalization across natural settings. As illustrated in the graph, other methods in the same category as DPI also face challenges in these environments. Nevertheless, DPI maintains competitive performance compared to other methods (excluding Dreamer) in four out of the six tested environments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477112047,
                "cdate": 1700477112047,
                "tmdate": 1700477112047,
                "mdate": 1700477112047,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NbpSKmfF32",
                "forum": "MdHDUsP2lt",
                "replyto": "nb04UQo0aw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the authors-II"
                    },
                    "comment": {
                        "value": "**[Q1] The paper refers to the setup as a lagrangian formulation, but for this to work, it seems insufficient to treat beta as a hyperparameter. So what exactly makes it a lagrangian setup, as opposed to just adding an extra penalty?**\\\nThanks for this question. We use the similar terminology utilises in Information Bottleneck Principle (IB) [4] to be uniform with the theory. In IB formulation, this parameter is responsible for the tradeoff between the complexity of representation and  amount of preserved relevant information. Similarly, in our model, these parameters are instrumental in balancing the relationship between past and future variables, with the present variable acting as the bottleneck.\n\n**[Q2] What is the reason to think that increasing mutual information between present and future states is compatible with removing distractors? For example, the background video will have temporal correlation, so we could potentially maximize that by keeping the TV in the state. See Efroni 2022 (PPE) as well as Lamb 2022 (AC-State) for some discussion of this issue.**\\\n**[A2]** This is a great question and thanks for suggesting AC-State [5] especially. Our idea can also be viewed in this way: Forward models while adept at predicting future states, can falter in noisy environments as it can also model task-irrelevant details. Inverse dynamic models, on the other hand, are effective in determining actions based on state transitions but can suffer from a similar problem. Our approach, encompasses both the models, learning predictive models ($q(z_{k+1}|z_k,a_k,h_h)$ in Section 4.2) for the future states and learning actions from the inverse dynamic models implicitly with lower bound on $I(z_{k+1},a_k;z_k)$, which acts as a regularizer.\n\n**References**\\\n[1] Amy Zhang, Rowan Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning invariant representations for reinforcement learning without reconstruction. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=-2FCwDKRREu. \\\n[2] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 2778\u20132787. PMLR, 06\u201311 Aug 2017. URL https://proceedings.mlr.press/v70/pathak17a.html.  \\\n[3] Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip Thomas. Learning action representations for reinforcement learning. In International Conference on Machine Learning, pp. 941\u2013950. PMLR, 2019. \\\n[4] Tishby, Naftali, and Noga Zaslavsky. \"Deep learning and the information bottleneck principle.\" 2015 ieee information theory workshop (itw). IEEE, 2015."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700477173626,
                "cdate": 1700477173626,
                "tmdate": 1700506657819,
                "mdate": 1700506657819,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J8jIT9rF9K",
                "forum": "MdHDUsP2lt",
                "replyto": "nb04UQo0aw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you again for your valuable feedback . We have addressed all the questions raised in our rebuttal and are ready to respond to any further queries you have. We eagerly await your updated feedback."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639511181,
                "cdate": 1700639511181,
                "tmdate": 1700639511181,
                "mdate": 1700639511181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "egiyTEdIx3",
            "forum": "MdHDUsP2lt",
            "replyto": "MdHDUsP2lt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7554/Reviewer_cwoe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7554/Reviewer_cwoe"
            ],
            "content": {
                "summary": {
                    "value": "The paper purports to introduce a mutual-information based object for learning representations in RL that will be robust to noise. The loss is composed of 4 components that are designed to either maximize or minimize the mutual information of various quantities, with bound provided on each part.  The method performs favorably on some benchmark tasks that involve background noise."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Using mutual information as a \"grounding principle\" for representation learning has some strong precedent given prior works, the ease of theoretical interpretation, and existing tractable methodology. The problem is well grounded as \"avoiding nuisance variables\" in representation learning is important (e.g., for identifiability / generalization). The results are not bad and demonstrate some of the ideas here work."
                },
                "weaknesses": {
                    "value": "Unfortunately, the paper has several issues that cannot be overlooked. \n\nFirst and foremost, much of the story is a bit sloppy and even problematic at places: the introduction seems to claim without much support that RL only works well when the observations do not contain noise variables that aren't related to the task at hand. There's sort of a skimming over the real problem, particularly in POMDP settings, where reward-relevant signal might be particularly sparse in observations, which can hurt the sample complexity of algorithms in many problems. The problem is often multifaceted, as the SNR (wrt task relevant factors, eg are the task variables themselves noised), reward sparsity, the degree of which the environment is partially observed, etc all factor in towards the success of various algorithms. But overall I'm not sure why, given the story of the introduction, that this has anything to do with task relevance in the representations: learning all of the transition function, even the noisy part, still should be decodable to a policy that can maximize returns. The problem is that these representations might not generalize well (e.g., when the noise correlates with a subset of task-relevant variables).\n\nSecond, the components of the loss are a bit complex and not well motivated. Some of the components seem to be doing similar things, e.g., it's unclear whether the two bottleneck terms need to both be included since all the messaging needs to be passed through z_t anyways. Why are all of these terms needed and what do they do that is distinct from the others?\n\nSome of the bounds are fine and rather standard, but it's unclear whether L_CLUB is doing a good thing or not: since we've now introduced the conditional on the actions on the top, it seems like this will discourage the model from remembering which actions were taken in the past. Wouldn't this do the opposite as stated is a goal for this model: to encode controllable, task-relevant information? But then we have I_LTC which has actions as the conditioning variable on the top of the upper bound: it seems like this and the action conditioning part of I_CLUB are doing opposite things?\n\nThere is a missing baseline which I think is very important and should be tested agains: Self-Predictive Representations (SPR).\n\nBut related to baselines (including SPR), what is really missing is what precisely all of the loss components of this model has are either 1) present in existing baselines or 2) not present in existing baselines. The overall claim is this paper is contributing through MI, but this has been done many times before, just not in the precise form. So much more work needs to be done to ground the contributions wrt the loss components in prior works, as well as better motivating these as mentioned above. And more to the point it's unclear *why* having a MI based objective of this form(s) is necessary: there are other ways to implement MI maximizations / minimizations approximately (eg regression losses as in SPR) that may just work better.\n\nFinally, the results are a bit lackluster. On some experiments (e.g., cheetah run) things go very well, but others it's not so clear (most in fact). This makes me concerned that some bit of hyper parameter tuning was performed on half cheetah that wasn't done on other environments (or baselines)."
                },
                "questions": {
                    "value": "Most of my questions are as concerns / weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7554/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783295544,
            "cdate": 1698783295544,
            "tmdate": 1699636914056,
            "mdate": 1699636914056,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ow0UnMBQFy",
                "forum": "MdHDUsP2lt",
                "replyto": "egiyTEdIx3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the authors"
                    },
                    "comment": {
                        "value": "First and foremost, we would like to express our sincere appreciation for the insightful review and valuable feedback on our paper. Your detailed questions have prompted crucial clarifications, enhancing the overall depth and quality of our work. Additionally, your mention of a relevant method has significantly contributed to strengthening the empirical aspects of our paper.\n\n**[W1] First and foremost, much of the story is a bit sloppy ... task-relevant variables).**\\\n**[A1]** It is crucial to clarify the narrative presented in the introduction. The assertion that reinforcement learning (RL) faces challenges in noisy environments is not intended to imply that RL is only effective in the absence of noise. Rather, the point is to highlight the increased complexity and potential for sub-optimal performance when RL algorithms encounter observations cluttered with irrelevant information. Regarding POMDPs and reward sparsity, the reviewer's point is well-taken. Our focus on noise and irrelevant information in the introduction is meant to complement, not overlook, these challenges. The introduction's mention of task relevance in representations is to underline the importance of extracting meaningful information from noisy data, not to suggest that the other variables are insignificant fot the success of a task. By emphasizing task relevance in representations, we underline the importance of extracting meaningful information from noisy data, crucial for ensuring generalizable and robust model performance. The introduction thus sets the stage for presenting Denoised Predictive Imagination (DPI) as a method, designed to tackle these challenges in RL through an information-theoretic approach.\n\n**[W2] Second, the components of the loss are a bit complex and not well motivated. Some of the components seem to be doing similar things, e.g., it's unclear whether the two bottleneck terms need to both be included since all the messaging needs to be passed through z_t anyways. Why are all of these terms needed and what do they do that is distinct from the others?**\\\n**[A2]** Our core concept revolves around the bottleneck variable $z_t$, which emerges from two distinct processes outlined in Section 4. One is to learn representation from the observations and vice-versa i.e. $o_t \\rightarrow z_t$ and $z_t \\rightarrow o_t$ respectively. The other process occurs in the latent space itself between the representations i.e. from one representation to another ($z_{t-1} \\rightarrow z_t \\rightarrow z_{t+1}$). Thus, this creates a two-fold bottleneck. In equation 1, intuitively, the left part $I(o^-,z^-)$ refers that the variable z is being inferred from the observation in a compressed form, while the right part $I(o^+,z^+)$ aims at maximally retaining the information from $z^+$ to predict the subsequent observations $o^+$. This same logic applies in equation 2.\n\n**[W3] Some of the bounds are fine and rather standard, but it's unclear whether L_CLUB is doing a good thing or not: since we've now introduced the conditional on the actions on the top, it seems like this will discourage the model from remembering which actions were taken in the past. Wouldn't this do the opposite as stated is a goal for this model: to encode controllable, task-relevant information? But then we have I_LTC which has actions as the conditioning variable on the top of the upper bound: it seems like this and the action conditioning part of I_CLUB are doing opposite things?**\\\n**[A3]** The reviewer's concern about the conditional on actions in $L_{CLUB}$ potentially discouraging the model from remembering past actions is insightful. However, the intention behind $L_{CLUB}$ is to enhance the model's focus on the state transitions, rather than diminishing its recall of past actions. It utilises the variable $h_k$, which is a histocial variable, incorporating information from the past.  $I_{LTC}$, with its action conditioning, assures that inferring $z_{k+1}$ from $o_{k+1}$ should diverge minimally from the $z_{k+1}$ obtained with $z_k$.\n\n**[W4] There is a missing baseline which I think is very important and should be tested agains: Self-Predictive Representations (SPR).**\\\n**[A4]** Thank you for pointing out the omission of Self-Predictive Representations (SPR) [1] as a baseline in our study. Following your suggestion, we have now included SPR as a baseline in our experiments. In the Figure 4, we show that DPI significantly outerpforms SPR in Cheetah run, Walker walk and Cartpole swingup. In Figure 2 of the Supplementary Material, we have also shown that DPI outperforms SPR significantly in the cartpole swingup task in random settings. Additionally, we have also added Dreamer-V2 [2] and VSG [3] as a baseline, which significantly contributed to improving the quality of our research."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496432962,
                "cdate": 1700496432962,
                "tmdate": 1700496432962,
                "mdate": 1700496432962,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gE2BQZvO2E",
                "forum": "MdHDUsP2lt",
                "replyto": "egiyTEdIx3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the authors-II"
                    },
                    "comment": {
                        "value": "**[W5] But related to baselines (including SPR) ... work better.**\\\n**[A5]** Thank you for addressing this. We have consistently mentioned many different baselines in each section representing every component. For example, in Section 4.3, while addressing $I(z_{k+1},a_k;z_k)$, we have mentioned that our objective \"is closely related to the predictive coding objective [4,5]\". On the top of that, while splitting the term, we have mentioned that the inverse dynamic model resembles to [6-8]. Again, in the same section, while working on $I(z^+,o^+)$, we said that we optimise the lower bound on our MI objective similar to [9]. We respectfully disagree from the reviewer regarding not mentioning the prior work in our loss formulations. The primary focus of our paper is to introduce a novel and generalised framework based on Mutual Information, designed to minimize noise and extract task-relevant information from observations. It is indeed true that there are multiple ways to implement MI maximizations or minimizations, such as using regression losses as seen in Self-Predictive Representations (SPR). Our current approach is designed due to its tight bound and ease in implementation, but we are open to exploring other methodologies in future research.\n\n**[W6] Finally, the results are a bit lackluster. On some experiments (e.g., cheetah run) things go very well, but others it's not so clear (most in fact). This makes me concerned that some bit of hyper parameter tuning was performed on half cheetah that wasn't done on other environments (or baselines).**\\\n**[A6]** We appreciate your observation regarding the performance of DPI in natural background settings. In our rebuttal, we present enhanced results in additional environments such as Walker Walk and Cartpole Swingup, achieved through setting correct weighing coefficients, conducted subsequent to the paper's initial submission. The details of these newly optimized weights are available in Section 4 of the Supplementary Material. With these optimization, DPI now shows significantly better performance in environments such as Cheetah Run, Walker Walk, and Cartpole Swingup, aligning more closely with our initial claims (Figure 4). We also show a dominant performance of DPI w.r.t. SPR and Dreamer-V2 in cartpole swingup task in random environment (Figure 2 Supplementary Material). On the top of that, we also showed the reconstruction results in the Figure 5 of the Supplementary Material.\n\n**References**\\\n[1] Schwarzer, Max et al. \u201cData-Efficient Reinforcement Learning with Self-Predictive Representations.\u201d International Conference on Learning Representations (2020).\\\n[2] Hafner, D.; Lillicrap, T. P.; Norouzi, M.; Ba, J. Mastering Atari with Discrete World Models. International Conference on Learning Representations. 2021\\\n[3] Jain, Arnav Kumar, et al. \"Learning robust dynamics through variational sparse gating.\" Advances in Neural Information Processing Systems 35 (2022): 1612-1626. \\\n[4] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. \\\n[5] Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre C\u00f4t\u00e9, and R Devon Hjelm. Unsupervised state representation learning in atari. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019.\\\n[6] Amy Zhang, Harsh Satija, and Joelle Pineau. Decoupling dynamics and reward for transfer learning, 2018. URL https://openreview.net/forum?id=H1aoddyvM. \\\n[7] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 2778\u20132787. PMLR, 06\u201311 Aug 2017. URL https://proceedings.mlr.press/v70/pathak17a.html. \\\n[8] Homanga Bharadhwaj, Mohammad Babaeizadeh, Dumitru Erhan, and Sergey Levine. Information prioritization through empowerment in visual model-based RL. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=DfUjyyRW90. \\\n[9] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information bottleneck. In International Conference on Learning Representations, 2017. URL https://openreview.netforum?id=HyxQzBceg."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496561845,
                "cdate": 1700496561845,
                "tmdate": 1700496561845,
                "mdate": 1700496561845,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VBk84xZ3Gd",
                "forum": "MdHDUsP2lt",
                "replyto": "egiyTEdIx3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you again for your valuable feedback . We have addressed all the questions raised in our rebuttal and are ready to respond to any further queries you have. We eagerly await your updated feedback."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639498476,
                "cdate": 1700639498476,
                "tmdate": 1700639498476,
                "mdate": 1700639498476,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JiGCng7R7g",
            "forum": "MdHDUsP2lt",
            "replyto": "MdHDUsP2lt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7554/Reviewer_qnYC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7554/Reviewer_qnYC"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel information-theoretic world model through leveraging the principle of information bottleneck. It can distill task-relevant information for policy learning in noise-saturated environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper focused on unexplored noise-saturated scenarios and proposed a new methed, Denoised Predictive Imagination, to tackle such situations. \n2. A detailed derivation of Denoised Predictive Imagination was given in the paper.\n3. DPI outperformed five state-of-the-art baseline models on six modified DeepMind control tasks."
                },
                "weaknesses": {
                    "value": "1. The design of experiments can be improved. Switching background is not convincing enough to demonstrated the effectiveness of the proposed method.\n\n2. Other basline model like DreamerV2 and DreamerV3 can also be included for comparison."
                },
                "questions": {
                    "value": "I will increase my rating if all of my concerns are properly addressed.\n\nThank authors' for their detailed rebuttal, hovewer, not all of my concerns are well addressed. I will keep my score unchanged."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7554/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7554/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7554/Reviewer_qnYC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7554/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798983929,
            "cdate": 1698798983929,
            "tmdate": 1700929165663,
            "mdate": 1700929165663,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kmak4hsgKy",
                "forum": "MdHDUsP2lt",
                "replyto": "JiGCng7R7g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the authors"
                    },
                    "comment": {
                        "value": "First and foremost, we express our sincere appreciation for the review and valuable insights provided on our paper. Your feedback has played a significant role in shaping our research, and we are very grateful for it.\n\n**[W1] The design of experiments can be improved. Switching background is not convincing enough to demonstrated the effectiveness of the proposed method.**\\\n**[A1]** Thank you for your feedback. We appreciate your perspective on the experimental design. It's important to emphasize that changing backgrounds presents one of the most challenging problems in this field, and is an indicator of generalization and robustness. Our experiments, designed around this challenge, are intended to specifically address and demonstrate the effectiveness of our proposed method in tackling this complex issue. We also highlight how other models, despite their strong performance in static backgrounds, fail to maintain their effectiveness in these more challenging dynamic background settings. This contrast not only underscores the unique robustness of our proposed method but also reinforces the complexity and significance of the problem we are addressing. \n\n\n**[W2] Other basline model like DreamerV2 and DreamerV3 can also be included for comparison.**\\\n**[A2]** Thank you for suggesting these methods. We have included DreamerV2 in our comparative analysis, but due to time and resource constraints, we were unable to incorporate DreamerV3. This decision was made to ensure a thorough and focused evaluation within our available means. Additionally, we have incorporated other relevant methods as suggested by other reviewers, SPR [1] and VSG [2], broadening the scope of our analysis. Notably, our method outperforms all these included models, showcasing its efficacy and robustness in the evaluated scenarios.\n\n**Additional Comments:**\n- Following suggestions from other reviewers, we have incorporated these additional baseline methods, SPR [1] and VSG [2], into our analysis. \n- Our updated results now show a significant improvement over the baseline methods in the specified environments. We encourage reviewing the new graphs presented in Figure 4 for a detailed comparison of these enhancements.\n- Additional experiments have been conducted on the Cartpole Swingup task within random environments. These new results clearly demonstrate DPI's capability to filter out background noise and effectively extract relevant information (Results in Supplementary Material Figure 2 and Reconstruction in Supplementary Material Figure 5).\n\n**References**\\\n[1] Schwarzer, Max et al. \u201cData-Efficient Reinforcement Learning with Self-Predictive Representations.\u201d International Conference on Learning Representations (2020).\\\n[2] Jain, Arnav Kumar, et al. \"Learning robust dynamics through variational sparse gating.\" Advances in Neural Information Processing Systems 35 (2022): 1612-1626."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416021373,
                "cdate": 1700416021373,
                "tmdate": 1700417556361,
                "mdate": 1700417556361,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8xELYHeXaq",
                "forum": "MdHDUsP2lt",
                "replyto": "JiGCng7R7g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you again for your valuable feedback and for considering an increase in your rating upon the completion of the suggested changes. As the deadline for rebuttal is approaching, we would greatly appreciate receiving any further comments or responses from you. We would like to inform you that we have implemented all the necessary changes suggested by you, including some additional modifications that we believe further strengthen the paper. We appreciate your dedication to the review process and look forward to your prompt feedback on these latest revisions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574964817,
                "cdate": 1700574964817,
                "tmdate": 1700574964817,
                "mdate": 1700574964817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k4XXKXI1be",
            "forum": "MdHDUsP2lt",
            "replyto": "MdHDUsP2lt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7554/Reviewer_7DVA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7554/Reviewer_7DVA"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors present a novel approach to reinforcement learning problems in noisy environments through a world model that learns denoised predictions of the environment. The effect is essentially like an encoder or a dimensionality reducer, using a set of models to identify only relevant features from the environment. After compression, these features are then used to reconstruct the state in terms of only relevant features, which is then used to guide an agent. The paper presents results across three cases, finding that their approach meets or exceeds the performance of relevant baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In terms of clarity, the paper does a good job making an argument for the importance of this problem and the authors' approach to solving it. The technical details are somewhat dense, but exhaustive which helps clearly express the approach. The setup for the experiments is also very clear. \n\nIn terms of quality, the technical quality of the work is high. The detail in the paper is sufficient for reproducibility concerns, which is further strengthened with the supplementary material. There don't appear to be major issues with any equations or figures. \n\nIn terms of originality, while there are prior work in terms of denoising and reinforcement learning, the authors do a good job of overviewing it and differentiating their approach. \n\nIn terms of significance, the paper would certainly be of interest to researchers interested in RL for noisy environments."
                },
                "weaknesses": {
                    "value": "I identify one major and one minor weakness in the current paper draft. \n\nThe major weakness is in the experiment setups, results, and discussion around them. In the paper, the authors refer to the results of the first experiment with the static backgrounds as \"competitive with all other methods\". This is perhaps a bit generous, when DPI as below median performance in four of the six settings. For the natural background settings, the claim is that \"DPI achieves better rewards in nearly all environments\", which may be true in terms of the blue line spinning above the others, but the performance of DPI only outperforms the baselines in two of the six tasks (Cheetah Run and Walker Walk). Finally, for the random background setting, results are only given for DPI's best performing environment (Cheetah Run). Cheetah Run is also the only environment in the visualized examples, which further supports that it is DPI's best performing environment. Beyond the text perhaps overstating the performance of DPI, there's also no discussion of why the performance of DPI should improve so much in this one environment compared to the others or why it's appropriate to only showcase visualizations and random background performance in it. These issues currently lead to me, as a reader, wondering if the complexity of DPI is justified by the relatively small number of environments it improves in, compared to baselines. Clarification on this point would be greatly appreciated. \n\nIn terms of the minor weakness, the technical approach section is very dense, making it difficult to get a clear sense of the high level structure of the approach. Figure 2 exists, but is a model of the problem without a strong connection to the implemented solution. Figure 3 also exists but only covers two of the models in the approach. A complete pipeline diagram or text overview would be greatly appreciated."
                },
                "questions": {
                    "value": "1. Why does DPI perform so much better in Cheetah Run?\n2. Why does DPI's performance in the static environments perform below the median in so many cases?\n3. Why only include visualizations and random background results in Cheetah Run? What did the other environments look like?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7554/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7554/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7554/Reviewer_7DVA"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7554/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698985567692,
            "cdate": 1698985567692,
            "tmdate": 1700535473556,
            "mdate": 1700535473556,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0s8K2IO7f3",
                "forum": "MdHDUsP2lt",
                "replyto": "k4XXKXI1be",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the authors"
                    },
                    "comment": {
                        "value": "First and foremost, we express our sincere appreciation for the review and valuable insights provided on our paper. Your feedback has played a significant role in shaping our research, and we are very grateful for it.\n\n**[W1]  In the paper, the authors refer to the results of the first experiment with the static backgrounds as \"competitive with all other methods\". This is perhaps a bit generous, when DPI as below median performance in four of the six settings.**\\\n**[A1]** Thank you for highlighting this aspect. We have revised the statement to reflect a more nuanced understanding: \"DPI exhibits a degree of effectiveness in certain scenarios involving static backgrounds, although it does not consistently outperform all other methods.\"\n\n**[W2] For the natural background settings, the claim is that \"DPI achieves better rewards in nearly all environments\", which may be true in terms of the blue line spinning above the others, but the performance of DPI only outperforms the baselines in two of the six tasks (Cheetah Run and Walker Walk).**\\\n**[A2]** We appreciate your observation regarding the performance of DPI in natural background settings. In our rebuttal, we present enhanced results in additional environments such as Walker Walk and Cartpole Swingup, achieved through hyperparameter tuning conducted subsequent to the paper's initial submission. The details of these newly optimized hyperparameters are available in Section 4 of the Supplementary Material. With these optimizations, DPI now shows significantly better performance in environments such as Cheetah Run, Walker Walk, and Cartpole Swingup, aligning more closely with our initial claims (Figure 4).\n\n**[W3] These issues currently lead to me, as a reader, wondering if the complexity of DPI is justified by the relatively small number of environments it improves in, compared to baselines. Clarification on this point would be greatly appreciated.**\\\n**[A3]** As mentioned above, DPI now shows significantly better performance in environments such as Cheetah Run, Walker Walk, and Cartpole Swingup, aligning more closely with our initial claims (Figure 4).\n\n**[W4] Finally, for the random background setting, results are only given for DPI's best performing environment (Cheetah Run).**\\\n**[A4]** Thank you for pointing out the scope of our results in the random background setting, particularly focusing on DPI's performance in Cheetah Run. Our approach for experimental evaluation in random settings was modeled after the TPC's [1] method. The primary constraint for the limited range of experiments was computational cost and tight timeline. It was challenging to conduct comprehensive experiments across all tasks, including comparisons with baselines. However, we managed to extend our analysis to include the Cartpole Swingup environment within the random background settings, offering a broader view of DPI's performance. The results in Figure 2 in Supplementary Material shows that DPI outperforms the two selected baselines significantly.\n\n**[W5] These issues currently lead to me, as a reader, wondering if the complexity of DPI is justified by the relatively small number of environments it improves in, compared to baselines. Clarification on this point would be greatly appreciated.**\\\n**[A5]** The primary focus of our paper is to introduce a novel framework based on Mutual Information, designed to minimize noise and extract task-relevant information from observations. This approach, particularly the use of predictive information for noise reduction, is a novel contribution to the field. As we have highlighted, following further optimization, DPI now demonstrates significantly improved performance in environments such as Cheetah Run, Walker Walk, and Cartpole Swingup. This enhancement in performance aligns more closely with our initial claims and underscores the efficacy of our approach in these specific settings, thereby justifying the complexity of DPI in the context of our study's objectives."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415503562,
                "cdate": 1700415503562,
                "tmdate": 1700415503562,
                "mdate": 1700415503562,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MS8ae9v6ax",
                "forum": "MdHDUsP2lt",
                "replyto": "k4XXKXI1be",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from the authors-II"
                    },
                    "comment": {
                        "value": "**[Q1] Why does DPI perform so much better in Cheetah Run?**\\\n**[A1]** We have conducted hyperparameter tuning across all environments. Please refer to the updated results for further information (Figure 4, Figure 2 in Supplementary Material).\n\n**[Q2] Why does DPI's performance in the static environments perform below the median in so many cases?**\\\n**[A2]** DPI, although not outperforming Dreamer in static environments, is designed for effective generalization across natural settings. As illustrated in the graph, other methods in the same category as DPI also face challenges in these environments. Nevertheless, DPI maintains competitive performance compared to other methods (excluding Dreamer) in four out of the six tested environments.\n\n**[Q3] Why only include visualizations and random background results in Cheetah Run? What did the other environments look like?**\\\n**[A2]** We have added Cartpole environment in the Supplementary Material (Figure 5). However, due to time constraints for the rebuttal, we were unable to compare it with all the baselines. Thus, we have compared it with the two methods (Supplementary Material Figure 2) suggested by other reviewers: SPR [2] and Dreamer-V2 [3].\n\n\n[1] Nguyen, Tung D., et al. \"Temporal predictive coding for model-based planning in latent space.\" International Conference on Machine Learning. PMLR, 2021.\\\n[2] Schwarzer, Max et al. \u201cData-Efficient Reinforcement Learning with Self-Predictive Representations.\u201d International Conference on Learning Representations (2020).\\\n[3] Hafner, D.; Lillicrap, T. P.; Norouzi, M.; Ba, J. Mastering Atari with Discrete World Models. International Conference on Learning Representations. 2021"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415547990,
                "cdate": 1700415547990,
                "tmdate": 1700415547990,
                "mdate": 1700415547990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cFCKmwqTg8",
                "forum": "MdHDUsP2lt",
                "replyto": "0s8K2IO7f3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Reviewer_7DVA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Reviewer_7DVA"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response from the authors"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to address my concerns and questions! However, I am somewhat worried that the solution appears to have been to find new hyperparameters that supported the original claims. The hyperparameter tuning process also does not appear to be covered anywhere. This raises concerns similar to training on the test set, and related concerns around the generalizability of the framework. Can the authors clarify the nature of the hyperparameter tuning process?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419395981,
                "cdate": 1700419395981,
                "tmdate": 1700419395981,
                "mdate": 1700419395981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ssdyMEwV7",
                "forum": "MdHDUsP2lt",
                "replyto": "qeEU3ZRtPt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7554/Reviewer_7DVA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7554/Reviewer_7DVA"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response to the reviewer"
                    },
                    "comment": {
                        "value": "Thanks for the quick response and clarification! Yes, that fully addresses that concern. I will increase my rating to reflect this. However, I now share some of the concerns of my fellow reviewers and I would want to see whether they felt their concerns had been addressed before any further increases."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7554/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535451886,
                "cdate": 1700535451886,
                "tmdate": 1700535451886,
                "mdate": 1700535451886,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]