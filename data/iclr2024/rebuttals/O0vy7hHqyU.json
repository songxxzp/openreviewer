[
    {
        "title": "Fake News Detection via an Adaptive Feature Matching Optimization Framework"
    },
    {
        "review": {
            "id": "jWG8ZkYMEw",
            "forum": "O0vy7hHqyU",
            "replyto": "O0vy7hHqyU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7754/Reviewer_PUtn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7754/Reviewer_PUtn"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by the fake news detection task, the paper investigates the use of simulated annealing to perform selection of text and image features while training a multimodal neural network. The proposed framework, Adaptive Feature Matching Optimization (AFMO), consists of extracting text and image features using well-known pretrained models (BERT and VGG), removing sample outliers, and then training a feed-forward neural network while the set of active features in the input is modified by an algorithm whose proposals are accepted or rejected through simulated annealing (SA) based on the loss. The fraction of features which are \"flipped\" in the proposal decreases from 1.0 to 0.0 linearly as the temperature goes from t0 to tmin. The authors evaluate AFMO performance for text-only data on PolitiFact and for image+text data on Gossipcop and one of the Weibo datasets. The results indicate that AFMO outperforms all the baselines w.r.t. Accuracy, Recall and F1-score."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "S1. While simulated annealing for feature selection has been explored by other works, this idea doesn't seem to be too explored in the context of neural networks with noisy inputs (perhaps due to the challenge of stabilizing the optimization)."
                },
                "weaknesses": {
                    "value": "W1. The text does not provide enough details for implementing the proposed feature selection algorithm.\n\nW2. Some methodological issues, particularly regarding the use of datasets.\n\nW3. No ablation study to understand the impact of feature selection.\n\nW4. Novelty seems to be limited to decreasing the number of flipped features during annealing.\n\nW5. Text is imprecise at some points and needlessly elaborate overall.\n\nW6. Paper does not provide a way to generalize to datasets that contain environment features (e.g., comments, likes, etc)."
                },
                "questions": {
                    "value": "Q1. Many details have been left out.\n- For text features, do you take the embeddings corresponding to all positions (instead of the usual approach of taking just the first position) and, if so, why? \n- Why do you need segment embeddings if the task doesn't seem to involve multiple text segments in each observation?\n- Can you describe the \"array of fully connected layers\" at the end of the each feature extractor and that at the end of classifier?\n- Does the temperature change at the end of an iteration (minibatch) or at the end of an epoch?\n\nQ2. Please address the following concerns:\n- For KNN, you don't need to normalize the dimensions by variance?\n- Which multimodal Weibo dataset did you use? Jin et al., 2017 or Zhang et al., 2021a?\n- Why the statistics in Table 2 do not match those in other papers (see Shu et al. 2018)? Is it showing the reduced data? Could this also explain the discrepancies between the results in Tables 4-5 and those in Hu et al., Deep learning for fake news detection: A comprehensive survey, AI Open, 2022?\n- Does AFMO reduce to a BERT classifier when there is only textual data? If so, what could explain the fact that such simple model it is outperforming all the baselines on PolitiFact by a wide margin; don't you need to include a stronger baseline (e.g., dEFEND)? Or does AFMO still includes the feature selection step?\n\nQ3. What is the performance gain of the feature selection step? What are potential alternatives for feature selection?\n\nQ4. Please clarify what exactly are the novel contributions of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethical concerns."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Reviewer_PUtn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698283258816,
            "cdate": 1698283258816,
            "tmdate": 1699636946715,
            "mdate": 1699636946715,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "J27GKIAlYt",
                "forum": "O0vy7hHqyU",
                "replyto": "jWG8ZkYMEw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "In the new version, we have updated the literature review and experimental sections.\n\nComment 1.1: For text features, the mean of embeddings from all segments of the text is used to represent the overall embedding of that text.\n\nComment 1.2: \"Array of fully connected layers\" refers to using multiple layers of fully connected layers to perform the final classification on the processed features.\n\nComment 1.3: The temperature changes in every iteration, i.e., after the accuracy is optimized or with a certain probability each time.\n\nComment 2.1: KNN was used without normalization of dimensions because the same method was used to extract features.\n\nComment 2.2: The Weibo dataset used is from Jin et al., 2017.\n\nComment 2.3: The statistical data in Table 2, except for Gossipcop, was obtained from open-source datasets by other authors. Gossipcop was crawled based on FakeNewsNet mentioned in the paper. FakeNewsNet is a web crawler on websites such as Twitter. The crawled content may be missing text or images due to reasons such as the news being banned by Twitter or the original post being deleted by the user or the image being expired. Therefore, after crawling, news data with empty text or images were removed, which may cause some differences in statistics from the dataset.\n\nComment 2.4: AFMO also includes steps of abnormal feature elimination and retraining, as well as annealing feature fusion when there is only text data.\n\nComment 3: The performance gain of feature selection is around 2%.\n\nComment 4: Outlier detection algorithms are used to eliminate training samples that exhibit abnormal features, thereby improving the accuracy and reliability of the trained model. In addition, simulated annealing algorithms are employed to wisely fuse features extracted from different modalities, filtering out image features that may visually stand out and affect key textual information, thus optimizing the accuracy and reliability of the model."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701826437,
                "cdate": 1700701826437,
                "tmdate": 1700702185633,
                "mdate": 1700702185633,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B3wLPSU57M",
            "forum": "O0vy7hHqyU",
            "replyto": "O0vy7hHqyU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7754/Reviewer_eXUi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7754/Reviewer_eXUi"
            ],
            "content": {
                "summary": {
                    "value": "The author has introduced an optimization method, the core of which lies in the utilization of the simulated annealing algorithm. The objective is to filter out the most informative  features of text and image features and reduce potential interference between text and visual \ninformation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is generally structured clearly. \n2. Experimental results show that the proposed model has comparable or improved performance."
                },
                "weaknesses": {
                    "value": "1. The advantages and innovativeness of the proposed method appear to be somewhat limited.\n2. The placement of relevant figures in the appendix makes for slightly inconvenient reading, particularly with respect to Figure 1.\n3. In the experimental section, some of the recent models proposed in 2022 and 2023 have not been compared, especially the unimodal methods.\n4. The position of comparative analysis\uff084.4.3\uff09 seems unreasonable and should be mentioned in 4.4.1."
                },
                "questions": {
                    "value": "1. The proposed model seeks to minimize potential interference between textual and visual information, but whether the feature selection through simulated annealing algorithm actually achieves the above-mentioned purpose seems to lack a detailed mechanism explanation.\n2. What are the advantages compared to using neural networks like attention mechanisms to fuse features?\n3. Which dataset was the experiment in Figure 2 done on? It should be noted in the text that it would be better to perform the same ablation experiments on the other two datasets.\n4. The example in the 4.4.2 case study seems to be able to identify correctly using some methods based on capturing the similarity or ambiguity of images and texts(e.g. Cross-modal Ambiguity Learning for Multimodal Fake News Detection). I don\u2019t know if there is a similar model in the baseline selected by the authors. This example does not seem to explain the advantages of the simulated annealing algorithm very well.\n5. Are abnormal data removed during the running process of the comparison algorithms in Tables 4 and 5?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Reviewer_eXUi"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673164730,
            "cdate": 1698673164730,
            "tmdate": 1699636946497,
            "mdate": 1699636946497,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nVZi6Y6pGx",
                "forum": "O0vy7hHqyU",
                "replyto": "B3wLPSU57M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "In the new version, we have updated the literature review and experimental sections.\n\nComment 1: The use of annealing algorithms can eliminate the mutual interference generated during multimodal feature fusion.\n\nComment 2: Using accuracy as the objective function in simulated annealing is fair to other baselines, as it represents the main direction of optimization. AFMO has higher accuracy than almost all other baselines, and the comparison with other baselines in this article is mainly based on the value of accuracy.\n\nComment 3: The ablation experiment was conducted on the weibo dataset.\n\nComment 4: The baseline does not include a model for capturing image and text similarity.\n\nComment 5: Yes, we removed the abnormal features."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701691413,
                "cdate": 1700701691413,
                "tmdate": 1700702171890,
                "mdate": 1700702171890,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RbXoJ8EfMM",
            "forum": "O0vy7hHqyU",
            "replyto": "O0vy7hHqyU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7754/Reviewer_Snvc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7754/Reviewer_Snvc"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a comprehensive optimization methodology specifically designed for fake news detection, capable of handling both unimodal and multimodal data sources. The framework is structured in four sequential steps: feature extraction, outlier removal, feature fusion, and classification. The paper validates its approach by conducting experiments on three diverse datasets: PolitiFact, Weibo, and Gossipcop. The empirical results show that the proposed framework consistently outperforms existing baselines in key metrics such as accuracy, precision, recall, and F1 score. Overall, the paper makes a robust contribution to the area of fake news detection by introducing a multi-faceted, effective methodology."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper leverages a variety of text features, including word-level, sentence-level, and contextual features, which help to understand the text content.\n2. The paper employs a simulated annealing algorithm to optimize the feature fusion process, which is a novel and effective technique for this task.\n3. The paper conducts extensive experiments on three datasets with different languages and domains, and demonstrates the superiority of the proposed framework over existing methods."
                },
                "weaknesses": {
                    "value": "1. The paper does not provide enough details about the outlier removal step, such as how to choose the threshold for Mahalanobis distance and the number of neighbors for KNN.\n2. The baseline compared in this paper is relatively weak. As far as I know, there are many more advanced multi-modal fake news detection works.\n3. The paper does not provide any qualitative examples or visualizations to illustrate how the framework works and why it is effective."
                },
                "questions": {
                    "value": "1. What do you mean by \"aberrant instances\", is it deleting part of the dataset or part of the features?\n2. What are the shortcomings and advantages of your method compared with other methods for eliminating multi-modal feature interference?\n3. Why are the parameters of simulated annealing chosen in this way? Is there a better option? Will the cost be too high?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Reviewer_Snvc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721351049,
            "cdate": 1698721351049,
            "tmdate": 1699636946377,
            "mdate": 1699636946377,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4e6wBBDHyX",
                "forum": "O0vy7hHqyU",
                "replyto": "RbXoJ8EfMM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "In the new version, we have updated the literature review and experimental sections.\n\nComment 1: Provide the threshold value of Mahalanobis distance (0.08) and the number of domains for KNN (domain number k=3). Currently, the threshold value is set manually, such as 0.08 in the experiment. Different threshold values will result in different abnormal features being removed, which will affect the performance of the final retrained model. Bayesian parameter tuning can be used to find the optimal threshold ratio, but the time cost will increase exponentially.\n\nComment 2: Baseline selected the most advanced CARMN (2021) and TRIMOON (2023) in recent years. In the TRIMOON paper, it was mentioned that it was already the most advanced model, so no new baselines were added.\n\nComment 3: An exception instance refers to some characteristics.\n\nComment 4: Annealing has advantages and disadvantages in eliminating multimodal feature interference: it can accurately capture and eliminate interference, but it takes a long time.\n\nComment 5: Using Bayesian parameter tuning to find the optimal parameters can be very time-consuming."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701522608,
                "cdate": 1700701522608,
                "tmdate": 1700702155241,
                "mdate": 1700702155241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ou8XBXwEKF",
                "forum": "O0vy7hHqyU",
                "replyto": "4e6wBBDHyX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7754/Reviewer_Snvc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7754/Reviewer_Snvc"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response. I appreciate the time and efforts you put on rebuttal."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713386420,
                "cdate": 1700713386420,
                "tmdate": 1700713386420,
                "mdate": 1700713386420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XHghaV6ibd",
            "forum": "O0vy7hHqyU",
            "replyto": "O0vy7hHqyU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7754/Reviewer_Gycm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7754/Reviewer_Gycm"
            ],
            "content": {
                "summary": {
                    "value": "The authors in this paper focus on the fake news detection and propose an Adaptive Feature Matching Optimization framework (AFMO) for both unimodal and multi-modal scenarios. AFMO first extracts the feature representations from diverse modals with distinct neural networks, then eliminates training instances with unnatural features by using an outlier detection approach, and designs a feature-centric optimization technique based on the principles of simulated annealing to obtain the most optimal fusion of multi-modal features followed by a MLP classifier. The experimental results demonstrate the effectiveness of the proposed AFMO framework."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe paper focuses on a practical and challenging issue, multi-modal fake news detection.\n-\tThis paper is well-written and quite easy to follow.\n-\tThe experimental results and ablation study show the effectiveness of the proposed framework."
                },
                "weaknesses": {
                    "value": "-\tWhat are the strengths of feature selection compared with the co-attention technique? The authors of this paper attempt to apply the feature selection based on the simulated annealing principle to obtain the most optimal fusion of multi-modal features. The current widely-used feature fusion technique is the co-attention. Thus, what are the strengths of feature selection compared with the co-attention technique? A more detailed discussion about this is expected. Otherwise, in the simulated annealing procedure of AFMO, \u201cthe accuracy rate emerges as the pivotal objective function governing the simulated annealing algorithm\u201d, is it fair for other baselines?\n-\tTraditionally, the convergence speed of the simulated annealing algorithm is slow and it will cost more time. Thus, the time complexity analysis and real running time of AFMO are expected to be compared with baselines.\n-\tIn the outlier detection procedure of AFMO, a preset threshold is required. How to set the threshold in the experiments, and how does the threshold affect the performance? \n-\tThough CARMN leveraging the attention to fuse the multi-modal features is used as a baseline, the experimental results of another fake news detection method based on the co-attention MCAN are also expected.\n-\tWhy does Table 5 miss the results of MKN? MKN serves as one of the baselines on the Weibo dataset in Table 4, but misses on the Gossipcop Dataset in Table 5. The performance of MKN on the Gossipcop Dataset is also expected."
                },
                "questions": {
                    "value": "Please refer to the weakness for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7754/Reviewer_Gycm"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7754/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770672698,
            "cdate": 1698770672698,
            "tmdate": 1700714831808,
            "mdate": 1700714831808,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SQuoHs0qXz",
                "forum": "O0vy7hHqyU",
                "replyto": "XHghaV6ibd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7754/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7754/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "In the new version, we have updated the literature review and experimental sections.\n\nComment 1: Using accuracy as the objective function in simulated annealing is fair to other baselines, as it represents the main direction of optimization. AFMO has higher accuracy than almost all other baselines, and the comparisons with other baselines in this article are mainly based on the value of accuracy.\n\nComment 2: The convergence speed of simulated annealing is indeed slow, requiring the continuous generation of new 01 solutions. It takes approximately several days to optimize to a better target, which is usually not the case for the training of baseline models.\n\nComment 3: The threshold set by AFMO is usually the proportion of abnormal features to be removed. Currently, the threshold is set manually, such as 0.08 in the experiment. Different thresholds will result in different abnormal features being removed, which will affect the performance of the final retrained model. Bayesian parameter tuning can be used to find the optimal threshold proportion, but the time cost will increase exponentially.\n\nComment 4: MCAN's performance on Weibo is 0.899. In addition, it was only tested on mediaeval2016, which has a total of 14,480 data items, but only 512 of them have images. We believe that this dataset does not well reflect the multimodal task scenario.\n\nComment 5: MKN does not disclose its source code, so it does not provide results on the gossipcop dataset. Only the data from MKN's original paper on the weibo dataset is used."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701271488,
                "cdate": 1700701271488,
                "tmdate": 1700702142687,
                "mdate": 1700702142687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b4NQe20YTR",
                "forum": "O0vy7hHqyU",
                "replyto": "SQuoHs0qXz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7754/Reviewer_Gycm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7754/Reviewer_Gycm"
                ],
                "content": {
                    "comment": {
                        "value": "I am convinced by other reviewers' comments, so I change my score from 5 to 3."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7754/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714841615,
                "cdate": 1700714841615,
                "tmdate": 1700714841615,
                "mdate": 1700714841615,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]