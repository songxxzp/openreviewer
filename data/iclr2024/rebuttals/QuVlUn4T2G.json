[
    {
        "title": "Is Generalized Dynamic Novel View Synthesis from Monocular Videos Possible Today?"
    },
    {
        "review": {
            "id": "PNzSmz5vuj",
            "forum": "QuVlUn4T2G",
            "replyto": "QuVlUn4T2G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3109/Reviewer_bWXg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3109/Reviewer_bWXg"
            ],
            "content": {
                "summary": {
                    "value": "The submission aims to find minimally necessary requirements for a generalized dynamic NeRF method for monocular RGB video input. It finds that running off-the-shelf methods for consistent depth estimation of the RGB video is sufficient. Using the RGB input video and these depth estimates, dynamic novel view synthesis is possible without further per-scene optimization at test time (e.g. for appearance). As additional input annotations, semantic segmentation masks (to identify dynamic objects in the input images) and optical flow are obtained via pretrained, off-the-shelf methods. These inputs can then be combined with a pretrained generalizable static NeRF transformer and novel special handling for dynamic content to render novel views. Experiments on existing benchmark datasets show that the method outperforms or is on par with many prior scene-specific dynamic NeRFs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is an intriguing problem setting and while the paper does not find a method that is truly fast (due to the consistent depth estimation), it is a carefully executed study with good experiments. It is informative for researchers in the field to see where things currently stand.\n\nThe paper is extremely well written. The experiments of the method by itself are very thorough, only comparisons to other methods are a bit lacking (see below). \n\nThe appendix is thorough and covers all the questions I had about finer details of the method."
                },
                "weaknesses": {
                    "value": "The most obvious downside of the paper is the result quality, unfortunately. The supplemental videos show rather low-quality results. Given that this is the first method in its problem setting, this is not necessarily a reason for rejection, as long as the experimental evaluation is great. The remaining weaknesses all concern the evluation.\n\n(1) Qualitative video comparisons: I don't understand the results on the Nvidia Dynamic Scenes dataset. Quantitatively, Neural Scene Flow Fields seems to be about on par with the submission. However, qualitatively, the results of NSFF (on their website) are much better. Where does that large discrepancy come from? Qualitative video comparisons to other works would help.\n\n(2) Fast scene-specific dynamic NeRFs: I would like to see a comparison with fast scene-specific dynamic NeRF methods. In terms of utility for novel view synthesis, generalizable NeRFs have two main advantages over scene-specific NeRFs: speed and learned prior knowledge. The latter is not exploited by the submission, as the results show novel view synthesis that sticks closely to the input camera path (instead of revealing hidden areas that scene-specific methods could not handle). Which leaves speed and I'd hence like to a see a comparison with Fang et al. TiNeuVox '22 (code is available), which optimizes a dynamic NeRF in a few minutes, unless there is a reason why such a comparison is unnecessary.\n\n(3) Static generalizable per-frame NVS: Also, given the rather limited quality and temporal instability of the results, I'd like to see a comparison with static single-image generalizable novel view synthesis methods. For example, Sajjadi et al. Scene Representation Transformer '22. MIT-licensed code is here https://github.com/stelzner/srt and the authors say on their project page that this code is reliable.\n\n(4) All qualitative results of the proposed method: Why are the qualitative video results on the DyCheck iPhone dataset not included in the supplement? \n\n== Minor Comments ==\n\nWhat is \"hundreds of GPU hours per video\" in the introduction referring to? It sounds as if existing per-scene methods take hundreds of hours per video (incl. appearance optimization). But that's not the case with most monocular dynamic NeRFs, especially the recent fast methods that take minutes rather than hours.\n\nThe related work is covered very thoroughly. Only recent diffusion-based approaches for novel view synthesis could additionally be cited, e.g. Watson et al. Novel View Synthesis with Diffusion Models (ICLR '23).\n\nThe first citation in A.1 is for transformers in general, not for GNT.\n\nPlease add a sentence to the main text that splat-/point-/mesh-based rendering of the point cloud is used to get the dynamic image rendering. The section on dynamic rendering feels incomplete currently, with some context/framing missing. \n\nI would not call the results \"high-quality\" (e.g. caption of Figure 8). The videos are not high-quality."
                },
                "questions": {
                    "value": "I am confused by what's happening in Table 3. The final method is 3? And 5-1 and 5-2 differ in what way? Does the final method not use Sec. 3.3.2, while 5-1 and 5-2 do?\n\nOther than that, the weaknesses cover the four points I think need to be addressed in a rebuttal. I am open to arguments as to why these experiments might not be necessary.\n\n===\n\nThe rebuttal addressed my concerns very well and I am hence updating my score to Accept."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3109/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3109/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3109/Reviewer_bWXg"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697630738529,
            "cdate": 1697630738529,
            "tmdate": 1699955808642,
            "mdate": 1699955808642,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hzroPpOLLv",
                "forum": "QuVlUn4T2G",
                "replyto": "PNzSmz5vuj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3109/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## About supplementary videos\n\nThanks a lot for pointing this out. After double-checking, we found that we indeed uploaded the wrong videos. We corrected this in the updated supplementary material. Videos exhibit much fewer artifacts.\n\n## About quantitative vs. qualitative results\n\nNote that the videos in the supplementary material are for **spatio-temporal interpolation**, which does not perfectly reflect the quantitative results for **novel-view synthesis** in the main paper.\n\nTo be specific, the quantitative results on the multi-view NVIDIA Dynamic Scenes dataset reported in Tab. 1 don\u2019t need temporal interpolation (see the paragraph below Eq. (3)). Essentially, quantitative evaluations are for spatial interpolations. However, when synthesizing the videos in the supplementary material, we conduct both spatial and temporal interpolations to demonstrate the ability of our study-found approach to synthesize frames between observed times. As expected, results are not as good as some methods which use scene-specific optimization. Nonetheless, we think it is valuable to show these results and understand present day capabilities.\n\nTo provide illustrations for the quantitative results provided in the main paper, we included videos composed of evaluation frames in the updated supplementary. See folder section \u201c[NVIDIA Dynamic Scenes] Videos from Frames for Quantitative Evaluations\u201d in the updated html. \n\n## About updated quantitative results\n\nWe found an error in computing SSIM results for our approach on NVIDIA Dynamic Scenes data in the original submission. The DyCheck results are not affected.\n\nWe corrected it and updated the affected Tab. 1, 3, S1, S2, and S4. After correction, the gap between SSIM of our approach and scene-specific approaches is smaller.\n\nMoreover, we find that DynIBaR\u2019s official evaluations are not fair since they exclude pixels that DynIBaR is unable to render. We added a discussion in the new Sec. E and updated the affected Tab. 1.\n\n## About videos on NSFF website\n\nNote, the original qualitative results of NSFF and DVS cannot be directly compared here due to the different setup. Originally, NSFF and DVS only selected 24 frames from the full video for training and evaluation. In contrast, we follow DynIBaR and evaluate on the whole video. Arguably, NSFF\u2019s inability to handle long videos is one of the motivations for the development of DynIBaR (see DynIBaR\u2019s Sec. 1 and NSFF results on the DynIBaR website).\n\n## About fast scene-specific dynamic NeRF\n\nThanks a lot. Great suggestion. We add quantitative and qualitative results for TiNeuVox on both datasets to the paper and the supplementary html respectively. For qualitative videos, please refer to sections \u201c[NVIDIA Dynamic Scenes] Videos from Frames for Quantitative Evaluations\u201d and \u201c[DyCheck iPhone] Videos from Frames for Quantitative Evaluations\u201d on the updated html.\n\nSpecifically, we run TiNeuVox on each video for 40k steps, which is double the steps than the default 20k iterations. This gives TiNeuVox an additional advantage. We evaluate on its checkpoints at both 20k and 40k steps of optimization, which takes 45min and 1.5 hours on a V100.\n\nNote, TiNeuVox can only complete optimization \u201cin a few minutes\u201d on the D-NeRF dataset. According to DyCheck (Gao et al., 2022a), the D-NeRF dataset is arguably one of the easiest datasets for dynamic novel view synthesis as it is \u201ceffectively multiview\u201d (see Fig. 3 of DyCheck\u2019s paper). In contrast, the NVIDIA Dynamic Scenes and DyCheck iPhone data are much more challenging.\n\nMeanwhile, we want to clarify that TiNeuVox also requires consistent depth for its regularization loss on background 3D points (https://github.com/hustvl/TiNeuVox/blob/d1f3adb6749420d10ecc074806f06459f189acbd/lib/load_hyper.py#L79-L83). Therefore, our comparison to TiNeuVox is fair.\n\nWe observe that TiNeuVox results are inferior to our approach on both datasets. \nWe updated Tab. 1, 2 and Fig. 1 to include the updated results.\nNote that there are no big differences between results after 20k and 40k steps, i.e., the optimization converged.\nWe hypothesize the inferior results are due to TiNeuVox not disentangling static and dynamic content. Though a joint representation works well on \u201ceffective multiview\u201d datasets, e.g., D-NeRF data and Nerfies data, it is more challenging to attain good results with a joint representation on NVIDIA Dynamic Scenes or DyCheck iPhone data."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699886499126,
                "cdate": 1699886499126,
                "tmdate": 1699887236992,
                "mdate": 1699887236992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CoHJVwe6a0",
                "forum": "QuVlUn4T2G",
                "replyto": "0JWreT1mQo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3109/Reviewer_bWXg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3109/Reviewer_bWXg"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "I appreciate the rebuttal a lot. It addresses my concerns very well. I am also glad that the corrected result videos look much more temporally stable than before. I am also, at this point, satisfied with the rebuttal to the other reviewers' concerns. Unless a discussion there reveals that I've misunderstood something major, I will increase my rating to Accept.\n\nThe only remaining nitpick I have is that \"usually hundreds of hours\" really only refers to a few (but impactful) methods like Nerfies or DynIBaR. As far as I've seen, most methods don't take that long because they come from academic labs with fewer resources. It would be nice if this could be qualified in some manner."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699955696529,
                "cdate": 1699955696529,
                "tmdate": 1699955696529,
                "mdate": 1699955696529,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vt83zqUBNp",
            "forum": "QuVlUn4T2G",
            "replyto": "QuVlUn4T2G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3109/Reviewer_3K58"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3109/Reviewer_3K58"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a study on generalized dynamic novel view synthesis from monocular videos, a challenge yet to be addressed in the literature. The authors establish an analysis framework, developing a \"pseudo-generalized\" technique that doesn't require scene-specific appearance optimization. The study found that geometrically and temporally consistent depth estimates are crucial to achieve this approach. Interestingly, this pseudo-generalized method outperformed some scene-specific techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Originality in addressing the generalized dynamic novel view synthesis from monocular videos.\n- Introduction of the pseudo-generalized process without scene-specific appearance optimization.\n- A comprehensive set of experiments and detailed ablations to validate the approach."
                },
                "weaknesses": {
                    "value": "- Presentation and clarity can be enhanced.\n- A broader range of related works should be included in the comparisons.\n- Ambiguity about the role of consistent depth estimates in the final result.\n- Experimental validation seems limited to certain datasets, potentially affecting generalizability."
                },
                "questions": {
                    "value": "- Can the authors clarify the specific role and impact of consistent depth estimates in their method?\n- How does the proposed method compare to generalized techniques not mentioned in the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698064674880,
            "cdate": 1698064674880,
            "tmdate": 1699636257332,
            "mdate": 1699636257332,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LjhsvHcEXj",
                "forum": "QuVlUn4T2G",
                "replyto": "vt83zqUBNp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3109/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## About presentation\n\nThanks a lot for your suggestions. We are more than happy to further polish the paper given specific suggestions.\n\n## About related works\n\nThanks a lot for the suggestions. We are happy to add those missing references if specific pointers are given.\n\n## About role of consistent depth\n\nTo clarify, we want to understand whether generalized dynamic novel view synthesis is possible based on state-of-the-art data priors. Based on our study, we find that we are unable to achieve a completely generalized approach. However, with consistent depth, we are able to get rid of scene-specific appearance optimization.\n\n## About datasets\n\nWe evaluate on two challenging datasets: NVIDIA dynamic scenes and DyCheck iPhone. Both are commonly used datasets in the field of dynamic novel view synthesis. Moreover, according to DyCheck (Gao et al., 2022a), these two datasets are much more challenging than others in this field.\n\n## About other generalized methods\n\nThanks a lot for the suggestion. We are happy to add more discussions given specific suggestions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699886171470,
                "cdate": 1699886171470,
                "tmdate": 1699886171470,
                "mdate": 1699886171470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "goM0ARlH2c",
                "forum": "QuVlUn4T2G",
                "replyto": "dntYLR6Jbn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3109/Reviewer_3K58"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3109/Reviewer_3K58"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thank you so much for discussing these minor concerns. I have no further questions and will stick to my initial positive opinion."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661586900,
                "cdate": 1700661586900,
                "tmdate": 1700661586900,
                "mdate": 1700661586900,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zU6pCsGjgI",
            "forum": "QuVlUn4T2G",
            "replyto": "QuVlUn4T2G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3109/Reviewer_BQLw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3109/Reviewer_BQLw"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to solve and study the novel view synthesis problem for a general dynamic scene. It argues that we can have a generalized approach to dynamic novel view synthesis modeling from monocular videos by overcoming the dependence on the scene appearance. The proposed method uses scene depth, optical flow, and dynamic and static content masks, assuming that dynamic motion is linear and spatially consistent. Results on a few datasets are shown to back up the claims made in the paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper aims at solving a very challenging problem and studies existing bottlenecks."
                },
                "weaknesses": {
                    "value": "## Abstract\n- We find a pseudo-generalized \u2026 is possible -> We found that \u2026 is possible.\n\n## Introduction\n- Authors have given explanations justifying the keywords such as generalized, scene-specific optimization, scene consistent depth, etc., used in the paper. Yet, it is rather weak as the approach itself relies on consistent depth estimates of a dynamic scene, which, in fact, is a very open problem and acceptable solutions generally rely on appearance cues and scene flow.\n\n- Furthermore, with years of practice with physical depth sensors\u2014be it iPhone depth sensing modalities or recent LiDAR, it's very hard, if not impossible, to recover consistent depth estimates for outdoor and indoor cluttered scenes. Even for static scenes, it is highly dependent on the subject material type, lighting condition, and other physical phenomena to have acceptable depth from a physical sensor, and here we are dealing with dynamic scenes. This is precisely the reason for methods such as \"Stable View Synthesis\" CVPR 2021, \"Enhanced Stable View Synthesis\" CVPR 2023, and Enhancing photorealism enhancement, TPAMI 2022 papers to make use of multi-modal 3D data to train the model. For completion, TPAMI 2022 could also work for dynamic scenes. The paper should emphasize such intrinsic details, detailing the papers mentioned above and the role of 3D data in novel view synthesis.  \n\n- Authors should also clarify why MonoNeRF does not qualify the definition of pseudo-generalized approach, given that the paper mentions \u201cit is unclear whether MonoNeRF is entirely generalizable and can remove scene-specific appearance optimization\u201d. It is better to test and present clarity in the rebuttal phase.\n\n## Scene Content Rendering\n- \u201cwe think it is possible to avoid scene-specific appearance optimization even if the monocular video input contains dynamic objects.\u201d This argument is provided despite the paper relies on Varma et al. 2023 pretrained GNT which greatly benefits from appearance. Please clarify in the rebuttal as it is inconclusive as to how far the proposed methods benefit from Varma et al. 2023 work, given that the current method is aware of the dynamic subject mask. Hence, in my view, the contribution looks very little.\n\n## Using Depth and temporal priors\n- The assumptions about linear motion and use of optical flow is mentioned later in the paper. This must be highlighted in the introduction. Also, the assumption about linear dynamics of a scene is not convincing for a paper oriented towards a generalized or pseudo-generalized approach. \n\n## Experiments\n\n- Results are considerably lower in performance. This makes me conclude appearance is indeed an important cue for neural rendering. Of course, it could take more time, yet it helps gain realism. So, I am not sure whether the research presented in the paper is about time optimization or towards photorealistic rendering of dynamic scenes. Please clarify.\n\n\n- Missing experiments on outdoor dynamic scene dataset such as Cityscapes. Kindly evaluate results on this dataset and compare it with Enhancing photorealism enhancement, TPAMI 2022."
                },
                "questions": {
                    "value": "Kindly refer weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698468788674,
            "cdate": 1698468788674,
            "tmdate": 1699636257231,
            "mdate": 1699636257231,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "40UKZ1XjOI",
                "forum": "QuVlUn4T2G",
                "replyto": "zU6pCsGjgI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3109/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## About generalized approaches\n\nWe emphasize that we do not claim our approach is \u201cgeneralized\u201d. In contrast, we unambiguously highlight our findings: we were unable to achieve a completely generalized approach based on current state-of-the-art data priors (see our abstraction, introduction, and conclusion).\n\nMeanwhile, as correctly mentioned in the review, consistent depth needs scene-specific optimization. Hence we can only achieve a **pseudo-generalized** approach which doesn\u2019t need scene-specific **appearance optimization**. \n\nImportantly, we never state that we do not need appearance cues (note the difference between **appearance cues** and **appearance optimization**). In contrast, we think appearance cues are important and our analysis of GNT adaptation in Sec 3.2.1 is based on exploiting appearance cues.\n\n## About TPAMI 2022\n\nThanks a lot for suggesting \u201cEnhancing photorealism enhancement\u201d. As an image-to-image style-transfer work, it requires a synthetic image as input and aims to enhance the photorealism of the input image, which cannot be used for dynamic novel view synthesis.\n\n## About MonoNeRF\n\nSorry for the confusion. The word \u201cunclear\u201d reflects our belief that MonoNeRF does not fall in the \u201cgeneralized\u201d category, as it requires scene-specific optimization on test scenes. To clarify, we copy our explanation from Sec. 2:\n> Notably, MonoNeRF only reports results for a) temporal extrapolation on training scenes; or b) adaptation to new scenes after fine-tuning. Both setups require scene-specific appearance optimization on test scenes.\n\nWe point to Fig. 1 and Fig. 5 in the MonoNeRF paper, which verify the need for appearance optimization: without scene-specific appearance optimization, results are not competitive.\n\n## About the difference to GNT\n\nAs an approach proposed for static scenes, GNT cannot handle dynamic novel view synthesis (see row 0 in Tab. S4).\n\nOur adaptation clearly improves the performance of novel view synthesis (see Tab. S4 row 3 vs. row 0 and Fig. S2).\n\nIn hindsight, our proposed adaptation for GNT in Sec. 3.2.1 is simple. Yet, we think it is a finding that\u2019s beneficial to the community. As far as we know, there does not exist work that states how to adapt a pretrained generalized view synthesis approach designed for static scenes to dynamic scenes.\n\n## About mentioning depth and temporal priors in the introduction\n\nWe note our use of depth and temporal priors already (see quote below, copied from our submitted introduction). We revised our introduction to make it clearer.\n\n> For the dynamic part, we aggregate dynamic content with the help of two commonly-used data priors, i.e., depth and temporal priors, and study their effects.\n\n## About linear dynamics\n\nTo clarify, we only use the linear motion assumption for dynamics between observed frames. We think this is reasonable since there does not exist any additional information about the dynamics besides the observed frames. Such a technique is commonly used in dynamic novel view synthesis as stated in Sec. 3.3.1:\n\n>  To produce renderings for temporal interpolation, i.e., $t_\\text{tgt} \\notin \\\\{ t_1, \\dots , t_N \\\\}$, previous works assume that motion between adjacent observed times is simply linear (Li et al., 2021; 2023).\n\n## About our contribution\n\nWe want to emphasize that our contributions are to study and understand whether a generalized approach for dynamic novel view synthesis is possible nowadays. As a result, the following thought of the reviewer does not fully reflect the goal of our study:\n\n> I am not sure whether the research presented in the paper is about time optimization or towards photorealistic rendering of dynamic scenes.\n\nWithout any doubt, with scene-specific appearance optimization we can obtain better results. But this isn\u2019t our goal. As mentioned by reviewer YL5S: \u201cthe paper tries to solve a worthwhile problem that would be very impactful to many groups and companies (rerendering a dynamic video without per-video optimization)\u201d. Further note the opinion from reviewer bWXg: this work is a \u201ccarefully executed study with good experiments\u201d and \u201cis informative for researchers in the field to see where things currently stand\u201d.\n\n## About evaluating on Cityscapes\n\nThanks a lot for the suggestion. As our work focuses on dynamic novel view synthesis, we use the commonly used dataset in the field. Specifically, we evaluate on two datasets: NVIDIA Dynamic Scenes and DyCheck iPhone. According to DyCheck (Gao et al., 2022a), these two datasets are much more challenging than others in this field."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699886112438,
                "cdate": 1699886112438,
                "tmdate": 1700422808966,
                "mdate": 1700422808966,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZGiCtMDaqS",
            "forum": "QuVlUn4T2G",
            "replyto": "QuVlUn4T2G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3109/Reviewer_YL5S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3109/Reviewer_YL5S"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to render novel views of a dynamic scene with much less per-scene optimization than competing techniques such as NSFF [Li et al. 2021] and Dynlbar [Li et al. 2023] . The input to the method is a video of a scene and a set of new camera poses over time. The output is a rerendered video. \n\nThe method works by computing a mask of the dynamic parts of the scene using existing methods. It also computes depth and optical flow. The dynamic parts of the scene are then modified by turning the dynamic pixels into point clouds and rerendering according to the new camera poses. The static parts of the scene are rerendered using a modified version of the generalizable NeRF transformer [Varma et al. 2023]. The dynamic and static parts of the scene are then combined.\n\nAccording to the quantitative metrics, the proposed method seems to perform slightly worse than NSFF and better than some other baselines that work on dynamic video inputs. This would be acceptable as the proposed method is much faster than NSFF and Dynlbar. However, qualitatively, according to the supplemental videos, the proposed method seems much worse than all competing methods, with substantial flickering.\n\n\nZhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural Scene Flow Fields for SpaceTime View Synthesis of Dynamic Scenes. In CVPR, 2021.\n\nZhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. DynIBaR: Neural Dynamic Image-Based Rendering. In CVPR, 2023."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tries to solve a worthwhile problem that would be very impactful to many groups and companies (rerendering a dynamic video without per-video optimization). The paper reads a bit like a systems paper where there are a dozen components (dynamic mask generation, depth estimation, optical flow, static scene rendering, dynamic scene rendering) that contribute to the final solution.\n\nThe overall algorithm makes a lot of sense and seems like it should work. There are also a lot of comparisons to other methods and an ablation study."
                },
                "weaknesses": {
                    "value": "Given that Dynlbar exists, the answer to the title\u2019s question seems like a resounding yes? I suggest the authors change the title to not be a general question and to be something specific about how their method works. I know that the authors are not planning on using scene-specific optimizations, which is how they distinguish their work from Dynlbar, but the title does not make this clear.\n\n\nThe supplementary results seem much, much worse than Dynlbar or NSFF. In the presented results, the dynamic portion of the scene flickers in and out of existence. The LPIPS metrics reported in the paper (Figure 1, Table 1) are only slightly worse than NSFF, but the actual results seem much worse. Perhaps this is because LPIPS doesn\u2019t capture any notion of temporal consistency between the frames? The presented results are very inconsistent while those in NSFF and Dynlbar are not that inconsistent.\n\nI am not sure if the metrics evaluated make sense since they don\u2019t take into account temporal consistency.\n\nThere is a minor missing citation. Consider discussing Figure 5 of https://arxiv.org/pdf/1909.05483.pdf [Niklaus et al. 2019] when presenting the statistical outlier removal technique (Fig.~S1). Niklaus et al. 2019 solve a similar problem where inaccurate depth estimates at object boundaries cause a similar problem to the one presented in Figure S1.\n\nMy relatively negative rating is based on the seemingly low quality results presented in the supplemental. It seems like the proposed technique does not work that well? The overall algorithm makes sense to me, so I am very surprised at how low quality the results are."
                },
                "questions": {
                    "value": "The video results presented in the supplemental seem much worse than both NSFF and Dynlbar. Specifically, there is a lot of temporal flickering in the dynamic parts of the scene. Do the authors know why this is? I would be very interested in seeing the dynamic mask, the optical flow and the depth estimates to better understand why there is so much flickering. I wonder if the authors uploaded the wrong set of results?\n\nI would be interested in seeing video results on the DyCheck dataset.\n\nHow does the proposed method handle cases where the new camera poses peer behind an object to a location that was not seen in any of the input frames? There\u2019s no explicit inpainting step, but my guess is that GNT will not do a good job in these locations? You can kind of see this issue in the presented videos at the edge of the frames, where there are parts of the scene not seen in any of the input views."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3109/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3109/Reviewer_YL5S",
                        "ICLR.cc/2024/Conference/Submission3109/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3109/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698704807115,
            "cdate": 1698704807115,
            "tmdate": 1700662892106,
            "mdate": 1700662892106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lNivK4fNjJ",
                "forum": "QuVlUn4T2G",
                "replyto": "ZGiCtMDaqS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3109/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3109/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## About title\n\nThanks a lot for the suggestion. We plan to change the title to \u201cPseudo-generalized dynamic novel view synthesis without scene-specific appearance optimization\u201d.\n\n## About supplementary videos\n\nThanks a lot for pointing this out. After double-checking, we found that we indeed uploaded the wrong videos. We corrected this in the updated supplementary material. Videos exhibit much fewer artifacts.\n\n## About quantitative vs. qualitative results\n\nNote that the videos in the supplementary material are for **spatio-temporal interpolation**, which does not perfectly reflect the quantitative results for **novel-view synthesis** in the main paper.\n\nTo be specific, the quantitative results on the multi-view NVIDIA Dynamic Scenes dataset reported in Tab. 1 don\u2019t need temporal interpolation (see the paragraph below Eq. (3)). Essentially, quantitative evaluations are for spatial interpolations. However, when synthesizing the videos in the supplementary material, we conduct both spatial and temporal interpolations to demonstrate the ability of our study-found approach to synthesize frames between observed times. As expected, results are not as good as some methods which use scene-specific optimization. Nonetheless, we think it is valuable to show these results and understand present day capabilities.\n\nTo provide illustrations for the quantitative results provided in the main paper, we included videos composed of evaluation frames in the updated supplementary. See folder section \u201c[NVIDIA Dynamic Scenes] Videos from Frames for Quantitative Evaluations\u201d in the updated html. \n\nNote, the original qualitative results of NSFF and DVS cannot be directly compared here due to the different setup. Originally, NSFF and DVS only selected 24 frames from the full video for training and evaluation. In contrast, we follow DynIBaR and evaluate on the whole video. Arguably, NSFF\u2019s inability to handle long videos is one of the motivations for the development of DynIBaR (see DynIBaR\u2019s Sec. 1 and NSFF results on the DynIBaR website).\n\n## About updated quantitative results\n\nWe found an error in computing SSIM results for our approach on NVIDIA Dynamic Scenes data in the original submission. The DyCheck results are not affected.\n\nWe corrected it and updated the affected Tab. 1, 3, S1, S2, and S4. After correction, the gap between SSIM of our approach and scene-specific approaches is smaller.\n\nMoreover, we find that DynIBaR\u2019s official evaluations are not fair since they exclude pixels that DynIBaR is unable to render. We added a discussion in the new Sec. E and updated the affected Tab. 1.\n\n\n## About evaluation metrics for temporal consistency\n\nWe think better LPIPS indicates a better alignment with the ground truth, which implicitly reflects better consistency. Meanwhile, we agree that LPIPS cannot specifically assess consistency. As far as we know, there are no commonly-accepted metrics to assess the consistency of dynamic novel view synthesis. Any suggestions are very welcome.\n\n## About the consistency of DynIBaR\n\nWe emphasize that it is expected that our study-found approach performs worse than DynIBaR. As stated in the paper (Sec. 4.2), DynIBaR performs appearance optimization using over 300 GPU hours per video while we don\u2019t use any appearance optimization.\n\n## About understanding flickering\n\nWe added a new Sec. F in the updated pdf to analyze and understand the flickering. We find flickering essentially comes from occlusions.\n\n## About video results on DyCheck\n\nWe added videos composed of evaluation frames in the updated supplementary material. See section \u201c[DyCheck iPhone] Videos from Frames for Quantitative Evaluations\u201d in the updated html.\n\n## About view synthesis from cameras behind objects\n\nYou are right that our approach cannot handle such challenging scenarios. Context-aware generative priors are required to inpaint those missing regions as we state in Sec. 4.4.\n\n## About missing citations\n\nWe added a discussion in Sec. C2 of the updated pdf."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699885894930,
                "cdate": 1699885894930,
                "tmdate": 1699885894930,
                "mdate": 1699885894930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LJGZKpCSY5",
                "forum": "QuVlUn4T2G",
                "replyto": "zFuBp4FF7d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3109/Reviewer_YL5S"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3109/Reviewer_YL5S"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal. It has addressed my concerns about the paper and I see that the concerns brought up by other reviewers is also addressed."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3109/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662949617,
                "cdate": 1700662949617,
                "tmdate": 1700662949617,
                "mdate": 1700662949617,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]