[
    {
        "title": "Protecting Sensitive Data through Federated Co-Training"
    },
    {
        "review": {
            "id": "QQ5lW2BMGl",
            "forum": "uvq4Nh8eZB",
            "replyto": "uvq4Nh8eZB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5684/Reviewer_UQKJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5684/Reviewer_UQKJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors use federated co-training, in which local hard labels on the public unlabeled datasets are shared and aggregated into a consensus label. Then, the server forms a consensus, while clients use this consensus as pseudo-labels for the unlabeled dataset in their local training. For data protection, the idea is to integrate XOR-mechanism for achieving differential privacy over binary data. At last, the authors provide empirical experiments over multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The authors conducted many experiments."
                },
                "weaknesses": {
                    "value": "+ The idea seems to be just a combination of different modules.\n+ The explicit motivation is unclear.\n+ The writing logic should be improved."
                },
                "questions": {
                    "value": "What is the explicit motivation of this paper?\nWhy is this idea important?\n\n### Abstract\n- The first half introduces some general and basic knowledge of federated learning, data privacy, applications, and co-training. The logic is not smooth. Several sentences have no much relation here. What is the explicit problem that you target here? What are the explicit problems of prior research?\n- The proposed idea seems to have better model quality and privacy. However, the ambiguious language makes me confused. What is the definition of privacy or quality?\n\n### Introduction\n- The introduction mentions differential privacy, unlabeled datasets, and gradient-based methods? What are the problems of these research areas? Do the authors improve each of them?\n- The authors detailed what they have done. I feel confused that why the idea of combination (co-training, differential privacy, federated learning) is meaningful. What are your insights?\n\n### Construction\nSection 3 combines the prior research and the new idea.\nSection 4 starts with introduction of differential privacy.\nCould the authors elaborate what is new here? What research line do you follow?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Reviewer_UQKJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697964173305,
            "cdate": 1697964173305,
            "tmdate": 1699636593775,
            "mdate": 1699636593775,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "npUxmwxjLb",
                "forum": "uvq4Nh8eZB",
                "replyto": "QQ5lW2BMGl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer UQKJ,\n\nThank you for your evaluation. We will address your questions point-by-point.\n\n* The main motivation for our work is to improve privacy in federated learning. As stated in the abstract and introduction, federated learning promises to protect data privacy by only sharing model parameters, but in fact model parameters reveal a lot about training data. Differential privacy mechanisms can be used to improve privacy, but their privacy-utility trade-off is not ideal. Therefore, improving privacy is a very relevant problem. We approach this problem by first noting that in many application scenarios where privacy is paramount, e.g., healthcare, large public unlabeled datasets exist. Our approach uses such an unlabeled dataset to exchange information between clients. Distributed distillation and its variants have shown that by sharing soft labels for such an unlabeled datasets we can train neural networks collaboratively. We show that sharing soft labels indeed improves privacy over sharing model parameters, but does not fully protect private data. By sharing hard labels instead, we are able to protect privacy to the highest degree - at least in terms of existing membership inference attacks - while maintaining the model quality of sharing model parameters and soft labels. We hope that this clarifies our motivation. We will improve the description of our motivation in the manuscript.\n\n\n* Model quality is canonically measured in terms of test accuracy as a proxy for the generalization error. Privacy is measured empirically by the likelihood of an attacker being able to conduct a successful membership inference attack (as described in Sec. 5 \u201cExperimental Setup\u201d, see also the probabilistic interpretation of ROC AUC), and it is determined theoretically via differential privacy (as described in Sec. 4).\n\n\n* As discussed in the first point, our motivation is to improve privacy while maintaining model quality in federated learning. We do so by using an unlabelled dataset.\n\n\n* Our insights are that sharing hard labels instead of model parameters or soft labels indeed improves privacy - what was to be expected - and surprisingly does not decrease model quality. Moreover, we find that sharing hard labels has the additional benefit of allowing us to train interpretable models, like decision trees and rule ensembles, in a federated way. We can even train ensembles of various local models, as we show in the general comment. \n\n\n* Our Section 3 explains the scenario, introduces the novel method FedCT and analyzes its convergence and communication complexity. Since privacy is the main motivation for our work, we show in Section 4 how a privacy guarantee based on differential privacy can be derived for FedCT. For this, we provide a novel sensitivity analysis (Proposition 2), which is required to provide a meaningful privacy guarantee."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326642114,
                "cdate": 1700326642114,
                "tmdate": 1700326642114,
                "mdate": 1700326642114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D1UxaFOxiB",
                "forum": "uvq4Nh8eZB",
                "replyto": "QQ5lW2BMGl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_UQKJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_UQKJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552643999,
                "cdate": 1700552643999,
                "tmdate": 1700564761814,
                "mdate": 1700564761814,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qtOD3btf7U",
                "forum": "uvq4Nh8eZB",
                "replyto": "5Whce8np51",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_UQKJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_UQKJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' efforts, and I re-read the paper again. Sorry that I cannot improve my rating at this stage.\n\nWhat is the explicit definition of privacy in this paper? For example, differential privacy has its definition. What is the exact meaning of improving privacy (improve differential privacy?) in your paper? Using some attacks to verify is not strong for claiming improved privacy. Actually, the response is still ambiguous from my view."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700565243426,
                "cdate": 1700565243426,
                "tmdate": 1700565243426,
                "mdate": 1700565243426,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uaNHBnKMHO",
            "forum": "uvq4Nh8eZB",
            "replyto": "uvq4Nh8eZB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5684/Reviewer_W2md"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5684/Reviewer_W2md"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an algorithm to protect the privacy of shared soft predictions on a public dataset used to achieve consensus in the server in semi-supervised learning. In particular, the server owns a unlabeled dataset and clients own private labeled datasets. Client utilizes local model to infer on the public dataset and compute the soft predictions on the unlabeled data, then sends those information to the server in a differentially private manner. The server aggregates those local knowledge into consensus and matigate the over-fitting in local trainings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper attempts to solve an important problem: semi-supervised federated learning.\n2. The paper provides theoretical guarantee of the privacy confidence.\n3. utilizing a different differential privacy mechanism (XOR) instead of Gaussian mechanism, which is interesting."
                },
                "weaknesses": {
                    "value": "1. The novelty is limited. There are many prior studies have explored semi-supervised federated learning [1-3] and knowledge distillation [4-6]. The paper does not propose a new framework of FedSSL but just naively combining differential privacy techniques. The paper does not illustrate the difference between FedCT and those previous work.\n2. The motivation to protect the privacy of soft predictions is weak. Comparing to raw data/ features map, the soft predictions are not likely to leak privacy.\n3. The paper is not well-organized and using non-standard terminology. For example, the term 'semi-honst' in section 4. Usually, we use 'honest-but-curious' to descrtibe the server which can not modify the local updates but trys to infer private information of clients. \n4. The baselines are not comprehensive. DP-FedAvg is using DP to the updated gradients but the proposed FedCT is using DP in the soft  predictions, it is not comparable. FedCT should compare other FedSSL methods [1-3] and conduct attach on the method to verify the effect of FedCT.\n5. Setting for data heterogenity is not sufficient. With beta = 2 for non-i.i.d setting is not good. Most of papers set beta = 0.01 or 0.1 to create heterogeneous data partitions.\n\nReference:\n\n[1] Diao E, Ding J, Tarokh V. SemiFL: Semi-supervised federated learning for unlabeled clients with alternate training[J]. Advances in Neural Information Processing Systems, 2022, 35: 17871-17884.\n\n[2] Lin H, Lou J, Xiong L, et al. Semifed: Semi-supervised federated learning with consistency and pseudo-labeling[J]. arXiv preprint arXiv:2108.09412, 2021.\n\n[3] Itahara S, Nishio T, Koda Y, et al. Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data[J]. IEEE Transactions on Mobile Computing, 2021, 22(1): 191-205.\n\n[4] Li D, Wang J. Fedmd: Heterogenous federated learning via model distillation[J]. arXiv preprint arXiv:1910.03581, 2019.\n\n[5] Zhu Z, Hong J, Zhou J. Data-free knowledge distillation for heterogeneous federated learning[C]//International conference on machine learning. PMLR, 2021: 12878-12889.\n\n[6] Chen H, Vikalo H. The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation[J]. arXiv preprint arXiv:2301.08968, 2023."
                },
                "questions": {
                    "value": "1. What is the main difference between FedCT and the previous FedSSL methods?\n\n2. what's the motivation to use XOR mechanism instead of Gasussian mechanism? What's the benefit?\n\n3. I don't understand the last discussion of Interpretable Models? Why there is only FedCT and Centralized in the experimental results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Reviewer_W2md"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698523633988,
            "cdate": 1698523633988,
            "tmdate": 1699636593678,
            "mdate": 1699636593678,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QvoMQ76GLD",
                "forum": "uvq4Nh8eZB",
                "replyto": "uaNHBnKMHO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer W2md,\n\nWe appreciate your detailed assessment of our work and we are happy that you find the problem relevant and our theoretical privacy analysis and the employed XOR mechanism interesting. We address your questions point-by-point.\n\n**Weaknesses:**\n\n**W1:** We are aware of the wide range of semi-supervised federated learning methods, but to the best of our knowledge, FedCT is novel in that it shares hard labels on a public unlabeled dataset. This improves privacy substantially while maintaining model quality. Moreover, it allows us to train interpretable models. Since sharing hard labels hasn\u2019t been explored in the literature, achieving differential privacy is not straight-forward. We employed the XOR mechanism for binary data, which required us to derive a novel bound on the sensitivity of classification algorithms. Regarding the papers you have referenced, [4] is not applicable since it assumes a public labeled dataset, [3] shares soft labels similar to distributed distillation, and [1],[2],[5],[6] share model parameters and therefore do not improve privacy over  FedAvg at all. \n\n**W2:** As our experiments show, the vulnerability of sharing soft labels to known membership inference attacks is lower than sharing model parameters, as one would expect, but still considerable. Since sharing hard labels achieves a model quality similar to sharing soft labels, its additional privacy is a clear advantage. Together with the fact that sharing hard labels allows us to train interpretable models, or mixed models (see the general comment), this means that FedCT has clear advantages over both sharing model parameters and soft labels.\n\n**W3:** We used the term semi-honest as defined in [7] and used widely in cryptography and security, but we are happy to change the term to honest-but-curious in our manuscript.\n \n**W4:** We have compared FedCT to model parameters sharing and soft label sharing in terms of accuracy and privacy, as well as the common differential privacy mechanism applied to federated averaging. Our empirical privacy evaluation measures the success in terms of ROC AUC of an attacker to perform state-of-the-art membership inference attacks on the information that is shared - model parameters, soft labels, or hard labels. Therefore, the resulting vulnerability scores are comparable. We do not see how we can attach our method of sharing hard labels to other federated semi-supervised learning methods that share soft labels or model parameters. Could you please elaborate how you envision such experiments?\n\n**W5:** We did not extensively explore scenarios of too strong heterogeneity. Setting $\\alpha$ to 2 yields the class distribution shown in Figure 2. The impact of strong heterogeneity on consensus is intricate and depends on factors such as the local learning algorithm, model, number of clients, and consensus mechanism robustness. Investigating this is very interesting, but it falls beyond the scope of this work.\n\n[Figure 2:  Class distribution for $\\alpha = 2$ ](https://drive.google.com/file/d/1d_2E5NN3bXYUzOciYw0dKC66eORAN3wa/view?usp=sharing)\n\n**Questions:**\n\n\u064e**Q1:** Regarding the FedSSL methods you referenced, [1],[2],[5],[6] share model parameters and therefore do not improve privacy over FedAvg, [3] shares soft labels similar to distributed distillation, and [4] assumes a public labeled dataset and thus is not applicable to our scenario. \n\n**Q2:** Since the information shared by clients in FedCT are binary matrices, we cannot apply the Gaussian or Laplacian mechanism, but need a noise mechanism tailored to binary data. The XOR mechanism was recently developed for binary data and provides a differential privacy guarantee, given that we can quantify the sensitivity. Since we cannot use clipping, like in the Gaussian mechanism for federated averaging, determining the sensitivity is non-trivial. In our Proposition 2, be derive a novel bound of the sensitivity for on-average-leave-one-out-stable learning algorithms, which includes a wide range of machine learning methods. \n\n**Q3:** The reason why there are no baselines is simply that to the best of our knowledge, there are none. There exist several approaches that train decision trees in a semi-federated manner by computing the information gain in a distributed way (and protecting privacy via encryption)[8], but there is no method that trains local models collaboratively. The most informative baseline, thus, is centralized learning. \n\n**References:**\n\n[7] Goldreich, Oded. Foundations of cryptography: volume 2, basic applications. Cambridge university press, 2009.\n\n[8] Truex, Stacey, et al. \"A hybrid approach to privacy-preserving federated learning.\" Proceedings of the 12th ACM workshop on artificial intelligence and security. 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327409566,
                "cdate": 1700327409566,
                "tmdate": 1700327409566,
                "mdate": 1700327409566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "akrixbArMF",
                "forum": "uvq4Nh8eZB",
                "replyto": "QvoMQ76GLD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_W2md"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_W2md"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion"
                    },
                    "comment": {
                        "value": "Thank you for the response! Some of my questions have been clarified. Nevertheless, I still have one main concern:\n\nAs several reviewers question the motivation of using hard labels rather than soft labels, the authors claim that using hard label has higher privacy protection than soft labels while preserving similar performance on IID data. However, knowledge distillation (KD) techniques using soft labels in FL are aimming to mitigate the detrimental effect of data heterogeneity, such as FedMD [1]. This work did not only ran experiments on non-IID data so it is hard to tell whether FedCT still achieves similar performance without using soft labels.\n\nReference:\n\n[1] Li D, Wang J. Fedmd: Heterogenous federated learning via model distillation[J]. arXiv preprint arXiv:1910.03581, 2019."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434405706,
                "cdate": 1700434405706,
                "tmdate": 1700434405706,
                "mdate": 1700434405706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qdn8etdjS7",
                "forum": "uvq4Nh8eZB",
                "replyto": "uaNHBnKMHO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer W2md,\n\nThank you for your response. We are glad that most of your questions have been clarified and we are happy to address your remaining concern. We agree that data heterogeneity is an important problem in federated learning. We find in our experiments that for label heterogeneity, label sharing (both soft, as in distillation, and our hard label sharing) requires that client predictions are not too far off, so that we can still achieve a meaningful consensus. In our experiments, we therefore designed local datasets to have a part that is fairly homogeneous (Dirichlet with large $\\alpha$), to ensure that each client has at least a little information about each class. The other part is heterogeneous (Dirichlet with smaller $\\alpha$). The same could in expectation be achieved with an intermediate Dirichlet parameter $\\alpha$, but we wanted to make sure data is of the desired form. We have used $\\alpha=100$ for the fairly homogeneous part, and $\\alpha=2$ for the more heterogeneous part (see the experimental setup in Sec. 5 and Appendix C.3, and our visualization of the data distribution in Fig. 2 in our previous reply). Since in many standard FL papers, stronger heterogeneity is assumed, we performed an additional experiment for this rebuttal where we use $\\alpha=0.01$ for the heterogeneous part. The results in the following table show that FedCT still performs on par with both FedAvg and DD. \n\n| Dirichlet $\\alpha$ | FedCT  | FedAvg | DD     |\n|-----------------|--------|--------|--------|\n| $\\alpha = 0.01$    | $0.7981$ | $0.7907$ | $0.7903$ |\n\nThe experiments in FedMD [1] on heterogeneous data are very interesting. They use a private labeled dataset and a public labeled dataset with related classes - in this case, CIFAR100 which contains 100 classes from 20 superclasses (bicylce, bus, and car are all vehicles). The task of local models is to predict the superclass, but client datasets only contain one class per superclass. Thereby, the datasets are heterogeneous wrt. the class labels, but homogeneous wrt. the superclasses. This also ensures that a meaningful consensus can be achieved. While we do not assume a labeled dataset as FedMD does, we evaluate FedCT in this scenario as well. We achieve a test accuracy of $ACC=0.5106$ which is comparable to the test accuracy of roughly $ACC=0.5$ that FedMD reports - note that this comparison is favoring FedMD, since FedMD uses supervised pre-training (transfer learning) on the labeled public dataset, which we do not. We will include these new results in the updated version of our manuscript."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578396955,
                "cdate": 1700578396955,
                "tmdate": 1700579698206,
                "mdate": 1700579698206,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8BzGyIRzio",
                "forum": "uvq4Nh8eZB",
                "replyto": "Qdn8etdjS7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_W2md"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_W2md"
                ],
                "content": {
                    "title": {
                        "value": "Further Discussion"
                    },
                    "comment": {
                        "value": "I appreciate the authors' effort to implement additional experiments. More details need to be provided in the experimental results:\nwhat's the dataset? how many clients in the system? \n\nI understand the authors do not have much time to run experiments in the rebuttal period but the work still has large space to improve, including comprehensive experiments to evaluate the effect of data heterogeneity. I don't think just one single experiment can illustrate the effect. \n\nTherefore, I can not raise my score at this stage."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582258352,
                "cdate": 1700582258352,
                "tmdate": 1700582258352,
                "mdate": 1700582258352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u1JOmgloZP",
            "forum": "uvq4Nh8eZB",
            "replyto": "uvq4Nh8eZB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5684/Reviewer_e9mW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5684/Reviewer_e9mW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a federated learning framework namely FedCT to provide a privacy-preserving collaborative learning by sharing \u201chard labels\u201d for unlabeled dataset. The authors theoretically demonstrate the convergence of FedCT. Besides, they propose an XOR-Mechanism to protect the privacy of sharing labels. Experiments on 7 datasets showcase the superiority of FedCT compared to some baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The proposed FedCT is technically sound. \n\nS2. This paper provides a sufficient theoretical analysis of convergence and privacy guarantees. \n\nS3. The writing is generally good and easy to understand."
                },
                "weaknesses": {
                    "value": "W1. The contribution is trivial. There are a lot of federated learning frameworks, to name a few [1,2,3,4,5], based on sharing knowledge via a public unlabeled dataset. It seems that the only difference is that clients in FedCT upload \u201chard label\u201d while other works\u2019 clients share soft labels. \n\nW2. The motivation is not clear. According to the basic idea of knowledge distillation, soft labels should potentially contain more useful information than hard labels. The authors are encouraged to clarify the reason for replacing soft labels with hard labels.\n\nW3. More representative baselines are needed. As mentioned in W1, there are many similar works; it would be more convincing to conduct experiments to compare them.\n\nW4. The appendix mentioned in the paper cannot be found; maybe it is just for me.\n\n\n[1] FedMD- Heterogenous Federated Learning via Model Distillation.\n\n[2] Heterogeneous Ensemble Knowledge Transfer for Training Large Models in Federated Learning\n\n[3] Communication-Efficient and Model-Heterogeneous Personalized Federated Learning via Clustered Knowledge Transfer\n\n[4] Model-contrastive federated learning\n\n[5] Ensemble distillation for robust model fusion in federated learning"
                },
                "questions": {
                    "value": "See Weeknesses. Addressing these weaknesses (especially W1 and W2) will improve the convincing and quality of this paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663829394,
            "cdate": 1698663829394,
            "tmdate": 1699636593559,
            "mdate": 1699636593559,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "np7TMTrKLU",
                "forum": "uvq4Nh8eZB",
                "replyto": "u1JOmgloZP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer e9mW,\n\nWe appreciate your comprehensive assessment of our work and appreciate the favorable evaluation of the soundness and theoretical analysis of our approach. We will address your questions point-by-point.\n\n**W1.** Thank you for the references. In our related work section, we decided to discuss only those federated semi-supervised learning methods that directly fit our scenario, due to space limitations. That is, distributed clients hold a private labeled dataset, have access to a public unlabeled dataset and aim at training models collaboratively without sharing either data nor model parameters. Regarding your references, [1] assumes a public labeled dataset, [2, 4] require clients to share model parameters with the server, and [2] in addition assumes that the unlabeled dataset is only accessible by the server, [3] uses co-regularization for personalized federated learning, so we naturally compare to the non-personalized variant distributed distillation, and we discussed [5] in our related work section. We apologize for not having sufficiently clarified this. We will highlight the problem statement in our manuscript and add a discussion of more distantly related federated semi-supervised learning methods (including the ones you mentioned) in the appendix.  \n\n**W2.** The motivation for our paper is to maximize privacy in a federated learning scenario, where clients have access to an unlabeled public dataset. Indeed, sharing soft labels preserves privacy to a larger extent than sharing model parameters, as our experiments show for the first time, yet they still reveal a lot about private data (VUL >$ 0.6$). As discussed in the introduction, we wanted to go one step further by sharing hard labels. Indeed, this substantially improves privacy protection, with an empirically optimal privacy (VUL $\\approx 0.5$). Of course, this is only optimal wrt. known membership inference attacks, which is why we developed the differentially private variant of FedCT. What was surprising to us is that strong improvement in privacy comes at virtually no loss in accuracy. That is, we achieve a test accuracy comparable to FedAvg and Distributed Distillation, the latter sharing soft labels. We conjecture that in practice the advantage of sharing soft labels over hard labels in terms of model performance is negligible, yet the additional privacy risk is significant. Moreover, sharing hard labels allows us to train interpretable models as well, and even use different models at each client (please see our general comment for further details).\n\n**W3.** We selected the most prominent baselines for each type of methods, i.e., vanilla FedAvg as a representative for model sharing in homogeneous data scenarios, DP-FedAvg as its private variant, and distributed distillation for sharing of soft-labels (or co-regularization). We additionally compared FedCT to PATE for this rebuttal, a model distillation method that can be adapted to our scenario. The results in Table 1 indicate that PATE is outperformed by FedCT and all other baselines in terms of test accuracy. As discussed above, we cannot compare to the approaches you referenced, since they are not applicable in our scenario.\n\n| Dataset       | FedCT   | PATE    |\n|---------------|---------|---------|\n| FashionMINST  | 0.7658  | 0.6451  |\n| CIFAR10       | 0.7608  | 0.7039  |\n| Pneumonia     | 0.7478  | 0.7208  |\n| MRI           | 0.6274  | 0.6038  |\n| SVHN          | 0.8805  | 0.8721  |\n**Table 1: Average test accuracy ACC of FedCT and PATE for m=5**\n\n\n**W4.** The appendix is available as a separate document in the supplementary material. For convenience, we also included a full document (paper and appendix) in the supplementary material. If you have any trouble accessing the supplementary material, please let us know and we will upload it to our anonymized repository."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325617559,
                "cdate": 1700325617559,
                "tmdate": 1700326019404,
                "mdate": 1700326019404,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ooFLHk1yZQ",
                "forum": "uvq4Nh8eZB",
                "replyto": "np7TMTrKLU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_e9mW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Reviewer_e9mW"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' response. The clarification regarding the distinctions between this paper and the related works I mentioned in the review (W1) partially addresses my concerns. Concerning W2, it would be convincing if the authors provided a direct comparison between sharing soft labels and hard labels, illustrating factors such as performance and privacy ability (I could not find this comparison in the paper; please point it out if I missed it). As the \"main\" contribution of this paper appears to involve the conversion from soft labels to hard labels, which may be deemed trivial from a technical standpoint, the necessity of this conversion should be clearly articulated and supported by possible theoretical and empirical evidence."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544822521,
                "cdate": 1700544822521,
                "tmdate": 1700544822521,
                "mdate": 1700544822521,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dtU8gKxZ4c",
                "forum": "uvq4Nh8eZB",
                "replyto": "u1JOmgloZP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer e9mW"
                    },
                    "comment": {
                        "value": "Dear reviewer e9mW. \n\nWe are happy we could address most of your concerns. \nRegarding your concern W2, we will gladly clarify the distinction between hard and soft label sharing in the manuscript. To reiterate: As mentioned in the introduction, sharing hard labels instead of soft labels improves privacy and allows us to train interpretable models. Since hard labels carry less information than soft labels, this could in principle lead to a decrease in model quality. In our empirical evaluation (particularly Table 1), we show that we cannot observe any significant decrease in model quality - both soft and hard label sharing perform comparable to model parameters sharing (FedAvg). Note that we use distributed distillation as the most prominent representative of soft label sharing. At the same time, our experiments support the claim that sharing hard labels improves privacy: soft label sharing has an average vulnerability of 0.61, whereas sharing hard labels has a substantially reduced vulnerability of 0.52. We summarize these differences in the following table. We will include a polished version of the table together with a paragraph explaining these details in the appendix.\n\n| Shared Information | Model Quality IID | Model Quality Non-IID | Privacy | Interpretable Models | \n| --- | --- |--- | --- | --- | \n| Model Parameters | ++| ++| - | -| \n| Soft Labels | ++| +| + | - | \n| Hard Labels (FedCT) | ++| +| ++| ++|\n**Table 1: Comparison of parameter, soft label, and hard label sharing in federated learning.**\n\nNote that both soft and hard label sharing requires an unlabeled public dataset."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571082159,
                "cdate": 1700571082159,
                "tmdate": 1700571103898,
                "mdate": 1700571103898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uGNmEJkO1t",
            "forum": "uvq4Nh8eZB",
            "replyto": "uvq4Nh8eZB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5684/Reviewer_NCnb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5684/Reviewer_NCnb"
            ],
            "content": {
                "summary": {
                    "value": "In order to achieve privacy under a federated learning setting, this paper proposes the use of a form of federated co-training called FEDCT, where local hard labels on the public unlabeled datasets are shared and aggregated into a consensus label. This consensus label is then used in training each local model. The paper analyzes the convergence of the proposed FEDCT and further develops a privacy version of FEDCT based on the XOR-Mechanism. The paper compares the proposed method with two baseline methods on several datasets for both iid and non-iid settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper considers the privacy issue in training machine learning models, which is a very important problem.\n\n2. This paper employs the membership inference attack to practically demonstrate the model's ability to resist real-world attacks.\n\n3. This paper conducts the convergence analysis for the proposed algorithm FEDCT."
                },
                "weaknesses": {
                    "value": "1. This paper does not discuss the difference between the proposed method and PATE (Papernot et al., 2016). In PATE, each teacher model can be viewed as a local model, and a majority vote is also utilized to reach a consensus for learning from each other's knowledge. The key distinction is that PATE transfers this knowledge to a student model, while the method in this paper circulates the knowledge back to each teacher/local model. Without further discussion, it's challenging to ascertain whether the contribution of this paper is incremental when compared to PATE and DD (Bistritz et al., 2020).\n\n2. The experimental results are not promising. As depicted in Table 1, only in 1 out of 5 datasets does the proposed method outperform the baselines in terms of ACC, and on none of the datasets does the proposed method achieve the best privacy utility trade-off, that is the best ACC, while at the same time, having the best VUL compared to baselines.\n\n3. This paper lacks clarity on the algorithm's scalability in relation to the number of clients, as it only reports the impact of client numbers on one dataset, and default settings of m (client numbers) for other datasets are only 5."
                },
                "questions": {
                    "value": "1. How does FEDCT compare with PATE in terms of the underlying mechanism design?\n2. Besides differentially private distributed SGD (Xiao et al., 2022), are there any other related works that can be utilized as baselines for comparison with DP-FEDCT on the Privacy-Utility Trade-Off with Differential Privacy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5684/Reviewer_NCnb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698729887882,
            "cdate": 1698729887882,
            "tmdate": 1699636593431,
            "mdate": 1699636593431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cMnESycOpQ",
                "forum": "uvq4Nh8eZB",
                "replyto": "uGNmEJkO1t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 1"
                    },
                    "comment": {
                        "value": "Dear reviewer NCnb,\n\nThank you for your detailed evaluation of our work, in particular that you highlighted the importance of the problem we tackle, our theoretical analysis, and privacy evaluation. We will address your questions point by point.\n\n**Questions:**\n\n**(1)** PATE is essentially a distillation algorithm with the goal to produce a single student model from an ensemble of teachers. To protect the private dataset the teachers have been trained upon, a Laplace mechanism is applied to their predictions - more precisely, their counts - before determining the consensus label. Thereby, a curious student cannot infer upon the private training data. In both PATE and FedCT, hard labels are used to form a consensus. The fundamental differences are that (i) PATE is not a collaborative training algorithm and that (ii) PATE therefore is not concerned with protecting against an honest-but-curious server. Regarding point (i): In a nutshell, PATE trains all teachers until convergence once, uses their predictions to pseudo-label the public unlabeled data, and then train the student on this pseudo-labeled dataset. Thus, teachers never improve. This fundamentally differs from FedCT where clients train models collaboratively by iteratively sharing hard labels to improve the pseudo-labeled dataset, which in turn improves local models, which again improves the pseudo-labeling. This difference also explains point (ii): In PATE, the student is separated from the teachers, so that the main privacy concern is protecting against a curious student. In FedCT instead all clients can be honest-but-curious, but their data needs to be not only protected from other clients, but also from an honest-but-curious server. Therefore, we cannot apply the Laplace mechanism that PATE uses, but instead have to protect the hard labels, which we achieve via the XOR mechanism.\n\n**(2)** The privacy of distributed SGD is arguably lower than FedAvg, since sharing gradients allows not only for membership inference attacks, but even for data reconstruction attacks [2]. At the same time, FedAvg performs similar to distributed SGD in the literature [3,4]. Therefore, we have used DP-FedAvg as a baseline in our experiments. Differential privacy mechanisms are in principle compatible with most federated learning approaches, including those designed personalized federated learning [5,6]. It will be great to extensively evaluate FedCT in those scenarios, but goes beyond the scope of this work.\n\n**Weaknesses:**\n\n**(1)** We have discussed PATE in our related work section under semi-supervised learning, since it performs distillation via multiple teachers, instead of collaborative learning. Consequently, PATE is concerned with protecting the privacy of consensus labels, not of teachers\u2019 predictions. Distributed distillation, instead, collaboratively trains models and naturally we compared to it - both in distributed distillation and federated co-training, clients act both as teachers and students. Since PATE shares hard predictions of teachers, we can adapt it by letting each client be a teacher. Similar to the experiments reported in [1], each client trains a model to convergence and then makes predictions on the public unlabeled dataset. A consensus is formed from all predictions that are used to pseudo-label the unlabeled dataset. A single student model is then trained on the pseudo-labeled dataset. We report the results with $m=5$ clients in Table 1. \n\n| Dataset       | FedCT   | PATE    |\n|---------------|---------|---------|\n| FashionMINST  | 0.7658  | 0.6451  |\n| CIFAR10       | 0.7608  | 0.7039  |\n| Pneumonia     | 0.7478  | 0.7208  |\n| MRI           | 0.6274  | 0.6038  |\n| SVHN          | 0.8805  | 0.8721  |\n**Table 1: Average test accuracy ACC of FedCT and PATE for m=5**\n\nNote that in the original experiments, a much larger number of clients is required. Therefore, we also report results for $m=100$ clients for two datasets in Table 2.\n| Dataset       | FedCT   | PATE    |\n|---------------|---------|---------|\n| FashionMINST  | 0.7154  | 0.6318  |\n| Pneumonia     | 0.7269  | 0.6903  |\n**Table 2: Average test accuracy ACC of FedCT and PATE for m=100**\n\nThe results show that performing just a single round of training performs substantially worse than collaborative training (both distributed distillation and our proposed FedCT). Therefore, the privacy-utility trade-off of PATE is substantially worse: it achieves lower test accuracy while its privacy mechanism does not protect against a honest-but-curious server."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323903676,
                "cdate": 1700323903676,
                "tmdate": 1700323903676,
                "mdate": 1700323903676,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wbbBqo4kfI",
                "forum": "uvq4Nh8eZB",
                "replyto": "uGNmEJkO1t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part 2"
                    },
                    "comment": {
                        "value": "**(2)** We would like to clarify that the goal of FedCT is not to outperform federated learning in terms of test accuracy, but to improve privacy. Naturally, any gain in privacy will typically come at a loss in accuracy. Therefore, our results are actually quite astonishing. We can achieve a test accuracy comparable to FedAvg with greatly improved privacy. That is, the ability of an attacker to distinguish a private training example from an arbitrary one is as good as random guessing (VUL=$0.5$). An amazing side-effect of FedCT is that sharing hard labels not only improves privacy, but also allows us to train interpretable models, like decision trees and rule ensembles. This is not possible with FedAvg or sharing of soft labels, like in distributed distillation.\n\n**(3)** We performed experiments for m=5 clients, which is typical in cross-silo scenarios that are ubiquitous in healthcare. We empirically evaluated the scalability in the number of clients on FashionMNIST in Figure 5 of our manuscript. Following your suggestion, we performed the same evaluation for the Pneumonia dataset as well and report the results in Figure 1. Similar to FashionMNIST, FedCT scales very well with the number of clients.\n\n[Figure 1: Test accuracy (ACC) of FEDCT and FEDAVG (FL) Pneumonia with |U|= 200  for various numbers of clients m.](https://drive.google.com/file/d/1Txm7i6_sWjGqI85eYMcO5V2wNXKW7W2W/view?usp=sharing)\n\n\n**References:**\n\n[1] Papernot, Nicolas, et al. \"Semi-supervised knowledge transfer for deep learning from  private training data.\" arXiv preprint arXiv:1610.05755 (2016).\n\n[2] Zhu, Ligeng, Zhijian Liu, and Song Han. \"Deep leakage from gradients.\" Advances in  neural information processing systems 32 (2019).\n\n[3] McMahan, Brendan, et al. \"Communication-efficient learning of deep networks from decentralized data.\" Artificial intelligence and statistics. PMLR, 2017.\n\n[4] Mills, Jed, Jia Hu, and Geyong Min. \"Faster Federated Learning with Decaying Number of Local SGD Steps.\" IEEE Transactions on Parallel and Distributed Systems (2023).\n\n[5] Mansour, Yishay, et al. \"Three approaches for personalization with applications to federated learning.\" arXiv preprint arXiv:2002.10619 (2020).\n\n[6] T Dinh, Canh, Nguyen Tran, and Josh Nguyen. \"Personalized federated learning with moreau envelopes.\" Advances in Neural Information Processing Systems 33 (2020): 21394-21405."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700325318927,
                "cdate": 1700325318927,
                "tmdate": 1700325357007,
                "mdate": 1700325357007,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]