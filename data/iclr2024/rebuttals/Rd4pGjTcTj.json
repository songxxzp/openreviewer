[
    {
        "title": "Parrot: Enhancing Multi-Turn Chat Models by Learning to Ask Questions"
    },
    {
        "review": {
            "id": "G1Jpi7L8Zs",
            "forum": "Rd4pGjTcTj",
            "replyto": "Rd4pGjTcTj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9266/Reviewer_myqY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9266/Reviewer_myqY"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the multi-turn instruction-following capabilities of chat models. The authors propose a method called Parrot-Ask to generate high-quality instruction-tuning data with more human-style instructions. They also introduce a multi-turn evaluation benchmark called MT-Bench++ to assess the performance of chat models. The experiments show that the Parrot-Chat model, trained on the Parrot-Ask data, outperforms other open-source models on various instruction-following benchmarks. The main contributions of the paper include the systematic exploration of multi-turn instruction-following, the development of the Parrot-Ask method, the construction of the MT-Bench++ evaluation benchmark, and the demonstration of the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is well-written and clear to read.\n\n+ The proposed method achieves better performance than several strong baselines.\n\n+ The paper identifies the shortage of previous methods (self-chatting and iterative self-chatting) for SFT data generation."
                },
                "weaknesses": {
                    "value": "- For the first contribution the authors claimed, i.e., \"show that a high-quality instruction-tuning dataset plays a key role in empowering the multi-turn instruction-following capabilities of the chat models\", I think this is obvious enough and has been revealed by many previous works. Personally, I don't take it as a \"contribution\".\n\n- The paper identifies an important shortage of previous methods (self-chatting and iterative self-chatting), but the proposed method lacks intrinsic novelty. I like the idea that we probably need a better model to simulate real human questions; this is interesting, though, but more like an engineering trick, not a scientific research problem.\n\n- It's very strange to me that the authors \"extend dialogue turns based on the dialogues sampled in ShareGPT\". I think most of the dialogues in ShareGPT are already finished and it is unnatural to \"extend\" such dialogues. Extending such dialogues on purpose could make the generated data longer, but not real.\n\n- The generated dataset heavily relies on existing two datasets (ShareGPT and UltraChat), the success of Parrot may be largely owing to the high quality of existing datasets.\n\n- It happens that the performance improvement is because the GPT evaluator prefers the generation that has long content and has multiple rounds of dialogue. I mean the performance is better probably because the model is biased towards generating longer answers, not better answers. Some experiments are needed to further verify this point.\n\n- Minor: given the rapid development of SFT for open-source LLMs, the current SOTA has been leveled to 95%+, even higher than GPT-4. However, the paper only compares relatively weak baselines, I think the author could add several recent baselines to further demonstrate the quality of the proposed dataset."
                },
                "questions": {
                    "value": "Do you have plans to test on larger version of LLaMA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9266/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9266/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9266/Reviewer_myqY"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9266/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698387502183,
            "cdate": 1698387502183,
            "tmdate": 1699637167351,
            "mdate": 1699637167351,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t7gacrP0k4",
                "forum": "Rd4pGjTcTj",
                "replyto": "G1Jpi7L8Zs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer myqY Part 1"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the valuable comments. We hope our rebuttal will address the concerns of the reviewer.\n\n### **Q1: Contributions.**\nAlthough intuitive, previous works have not provided quantitative evidence to support or verify that chat models' ability to follow multi-turn instructions needs high-quality multi-turn data. Existing evaluation benchmarks like Alpaca Eval only cover single-turn instructions, and MT-Bench is limited to two turns. To the best of our knowledge, we are the first to quantitatively assess the multi-turn capabilities of current chat models and to reveal the gaps between them and GPT-4, ChatGPT. We think this should be considered a contribution.\n\n### **Q2: Values of asking model.**\nThe evolution of asking models and chat models should progress in parallel, akin to the interplay between the spear and the shield. An asking model that can provide diverse, in-depth, and extensive questions is indeed critical for effectively evaluating chat models, as it challenges them to prove their ability to handle a wide range of topics and ensures that issues of safety are thoroughly tested. A strong asking model can drive the progress of chat models by expanding their capability boundaries and revealing potential areas for improvement. Therefore, it is imperative for the academic community to explore building more powerful asking models that can drive the progression of chat models towards higher levels of reliability, safety, and versatility.\n\nIn this paper, we propose training an asking model and exploring its application in enhancing the multi-turn capabilities of chat models. Our experimental results reveal that an asking model is capable of generating human-like questions, which can help improve a chat model's multi-turn abilities. Furthermore, we believe that training better asking models will have even more applications in the future. \n\n\n### **Q3: Using ShareGPT.**\nWe would like to clarify that the purpose of extending dialogues from ShareGPT is not merely to increase their length, but to enhance their depth and topic diversity. Dialogues in real life do not have predetermined endpoints. In real-life communications, it is common for users to not obtain the information or answers they need in just one round of conversation. They may delve deeper into a topic or transition to related new topics.\n\nOur method, therefore, employs an asking model specifically trained to generate contextually appropriate and topically relevant questions, thereby ensuring a natural progression of the conversation. When provided with short dialogues from ShareGPT, our asking model is capable of introducing new questions that deepen the existing topic or smoothly transition to a new related one. This is in contrast to the approach of simply concatenating short dialogue sessions, which often results in incoherence and logical inconsistencies [R1]. Our data collection method, therefore, not only increases the length of the dialogues but also improves their depth, and the naturalness of topic transitions.\n\nFurthermore, the quality of the data collected through our method is tailored to meet the complex requirements of users in multi-turn dialogue scenarios. The resulting data is not just longer; it is richer and more representative of genuine human conversation patterns, enabling the trained model to better understand and respond to users' needs. To vividly demonstrate the effectiveness of our asking model, we have included additional examples from our dataset in the appendix (**Section G**).\n\n[R1] Re3Dial: Retrieve, Reorganize and Rescale Conversations for Long-Turn Open-Domain Dialogue Pre-training, EMNLP 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469053918,
                "cdate": 1700469053918,
                "tmdate": 1700469053918,
                "mdate": 1700469053918,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l8cG0sAPwO",
            "forum": "Rd4pGjTcTj",
            "replyto": "Rd4pGjTcTj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9266/Reviewer_sPEb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9266/Reviewer_sPEb"
            ],
            "content": {
                "summary": {
                    "value": "This paper highlights a gap in multi-turn conversation quality between open-source chat models and state-of-the-art closed source (e.g.,  ChatGPT), attributing it to the lack of high-quality instruction-tuning data. For instance most of existing open-source models are trained with single turn dialogues rather than complex multi-turn or topic switching examples. To address this, the authors introduce \"Parrot,\" a that generates high-quality instruction-tuning data from ChatGPT, leading to the development of \"Parrot-Chat,\" a model that significantly improves multi-turn conversation performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Quality/clarity\n- the paper is overall well written and clear. The figures and tables are easy to follow, and the main methodology is clearly explained.\n- the proposed models outperform existing baselines of the same (or higher) parameter size.\n\nSignificance\n- building an high quality multi-turn conversational datasets is definitely very important for building high-quality models."
                },
                "weaknesses": {
                    "value": "Originality\n- using ChatGPT generated to train/distill another model has been already widely explored by many other papers. Moreover, it is worth pointing out that using ChatGPT generated dataset has little or no values at this points because: 1) cannot be used for any commercial models, and 2) doesn't unveil how to actually collect high quality datasets."
                },
                "questions": {
                    "value": "check weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)",
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper released data from ChatGPT, which might break \"usage and terms\" if not properly licensed."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9266/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631589485,
            "cdate": 1698631589485,
            "tmdate": 1699637167243,
            "mdate": 1699637167243,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "APFVt9tQ2O",
                "forum": "Rd4pGjTcTj",
                "replyto": "l8cG0sAPwO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer sPEb"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the valuable comments. We hope our rebuttal will address the concerns of the reviewer.\n\n### **Q1: Novelty.**\nOur work is different from previous works that train models with ChatGPT-generated data. We clarify our novelty and significance to the community in the following aspects:\n1. we analyze the issues of existing multi-turn datasets such as non-human-like instructions, less detailed responses, short sessions, or limited topic transitions. \n2. to overcome these issues, we propose to train an asking model that can simulate user questions and then collect a new multi-turn dataset. The experimental results demonstrate that our dataset surpasses existing datasets in terms of topic diversity, number of turns, and human-likeness.\n3. we introduce the new MT-Bench++ benchmark, designed to quantitatively evaluate a model's multi-turn performance. By utilizing our Parrot dataset with more dialogue turns and higher quality, we have enhanced the multi-turn capability of a chat model based on LLaMA-2.\n\n\n### **Q2: Values of our work.**\nAs academic researchers, our primary focus is know-how, e.g., how to mitigate the gap between open source models and the state-of-the-art API models (e.g., GPT-4 and ChatGPT) in multi-turn capability. For this, we indicate that existing multi-turn datasets have some issues and propose to train an asking model to help collecting a multi-turn dataset with superior quality. Our model trained with our dataset show significant improvement on multi-turn capability compared with models trained on ShareGPT, Baize or UltraChat. Our dataset is fully available for faciliting future academic research, akin to UltraChat and Baize.\n\nMoreover, we believe our proposed method of training a human-like asking model can be applied to generate multi-turn data if using an alternative model that permit commercial usage when necessary. For companies, even in cases where manual annotation is involved in collecting multi-turn data, our asking model can be utilized to enrich topic diversity and prolong the duration of conversation. This ensures that annotators refrain from prematurely ending dialogues or encountering limited topic transitions.\n\n\n### **Q3: Revealing how to collect high-quality dataset.**\nWe have delved deeper into the issues present in current multi-turn datasets, such as non-human instructions, less detailed responses, short sessions, or limited topic transitions. Our approach effectively overcomes these issues by employing a specially trained 'asking model' that can promote richer dialogues. This model is designed to simulate user asking, which in turn prompts more human-like questions, longer sessions and broader topics. Through the improved performance of the chat model, manual evaluation and qualitative examples, it is proved that our method can collect higher-quality multi-turn data than existing methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467860551,
                "cdate": 1700467860551,
                "tmdate": 1700467860551,
                "mdate": 1700467860551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lXJeSISXbo",
            "forum": "Rd4pGjTcTj",
            "replyto": "Rd4pGjTcTj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9266/Reviewer_NNDS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9266/Reviewer_NNDS"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a solution to generate instruction-tuning data for multi-turn chat. They first train the Parrot-Ask model to generate questions, conditioning on answers and conversational history. Then, they employ Parrot-Ask to interact with GPT-3.5, collecting multi-turn instruction tuning data. The authors utilize the collected Parrot-40K dataset to train a chat model called Parrot-Chat, which outperforms existing datasets in terms of statistics and performs better on instruction-following benchmarks, including MT-Bench++, an extended version of MT-Bench."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A simple and effective method is proposed to collect multi-turn instruction-tuning data.\n- The collected Parrot-40k datasets show larger average number of turns, token length, topic shifts and transitions than other datasets.\n- A new benchmark MT-Bench++ is proposed which is an expansion of MT-Bench where additional six follow-up questions are added.\n- Experimental results show that Parrot-Chat achieves the best performance on multiple instruction-following benchmarks over open-source models."
                },
                "weaknesses": {
                    "value": "- The human evaluation part is unclear.\n- The authors do not reveal the structure of the proposed prompts.\n- The authors do not explain how does  the follow-up questions in MT-Bench++ are decided."
                },
                "questions": {
                    "value": "- It seems that the supplementary materials mentioned at the end of section 4.3 are missing.\n- For human evaluation, how is the criteria defined? And how many annotators are involved? And how about the sample size and inter-agreement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9266/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9266/Reviewer_NNDS",
                        "ICLR.cc/2024/Conference/Submission9266/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9266/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736324202,
            "cdate": 1698736324202,
            "tmdate": 1700632136049,
            "mdate": 1700632136049,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PzZNHWl1yF",
                "forum": "Rd4pGjTcTj",
                "replyto": "lXJeSISXbo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer NNDS"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the valuable comments.  We are encouraged by the recognition given to our work. We hope our rebuttal will address the concerns of the reviewer.\n\n### **Q1: Details of human evaluation.** \nWe utilize the first-turn questions from MT-Bench because it widely covers eight domains, then we employ Parrot-Ask and ChatGPT iterative self-chatting methods (e.g., UltraChat) to collect dialogues of ten-turns, each method generating 800 questions in total. Three annotators are asked to evaluate the quality of these generated questions following these criteria:\n- Repetitiveness: Questions should not be repetitive, ensuring each is unique and has some contributions to the conversation.\n- Conciseness: Questions should be free of unnecessary verbosity, maintaining clarity and brevity.\n- Politeness Patterns: Questions should avoid excessive politeness or expressions of gratitude that are not typically used in natural human questioning.\n- Relevance: Questions should be directly related to the previous context or responses, showing a clear understanding of the conversation flow.\n\nThe result indicates that 81.8% of the questions generated by our Parrot method are human-like and high-quality, while only 36.8% for the iterative self-chatting method. The Kappa among annotators is 0.72, which reaches a high significant level.\n\nFor ChatGPT self-chatting method (e.g., Baize), we observed that 82.5% of the dialogues would end in a single turn, and fail to generate new turns of dialogues. Therefore, we did not include this method for human evaluation.\nWe will add these important details in the final version of our paper.\n\n\n### **Q2: Proposed prompts.**\nWe use the following prompts for the Parrot-Ask model: \"The following is a conversation between a user and an AI assistant. User statements start with [USER] and AI assistant statements start with [ASSISTANT]. You need to tell me what the user is likely to ask in each round of the conversation.\" \nWe have updated an appendix to include this important information (**Section B**). Please refer to the supplementary material.\n\n### **Q3: Follow-up questions in MT-Bench++.**\nWe decide the follow-up questions in MT-Bench++ according to the following standards: \n1. The question should be challenging and require AI to perform complex reasoning or rely on wide knowledge to answer.\n2. The question should be related to the previous text. Try to reference or imply the content of the previous dialogue in the question. \n3. The questions in a session should include appropriate topic transitions.\n\nWe have updated an appendix to include this important information and provided more examples of MT-Bench++ (**Section A**), where we have underscored specific features of the questions such as references, topic transitions, and knowledge requirements. These features are highlighted in blue and accompanied by annotations for clarity. For further details, please refer to the supplementary material.\n\n\n### **Q4: Missing supplementary materials.**\nWe apologize for missing the supplementary materials. We have updated an appendix, which introduces more details of MT-Bench++ and evaluations, and more qualitative examples. Please refer to the supplementary material."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467538523,
                "cdate": 1700467538523,
                "tmdate": 1700535517809,
                "mdate": 1700535517809,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DzMhdKPhiq",
                "forum": "Rd4pGjTcTj",
                "replyto": "PzZNHWl1yF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9266/Reviewer_NNDS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9266/Reviewer_NNDS"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response! The information provided in the appendix is certainly valuable for understanding the paper. Taking into account the feedback from other reviewers, I will maintain the current score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643567220,
                "cdate": 1700643567220,
                "tmdate": 1700643567220,
                "mdate": 1700643567220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hQMj6klhFm",
            "forum": "Rd4pGjTcTj",
            "replyto": "Rd4pGjTcTj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9266/Reviewer_tuRa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9266/Reviewer_tuRa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Parrot, a model trained specifically for simulating a user, on ShareGPT data. The chat model trained with Parrot, Parrot-Chat, outperforms models trained with ChatGPT self-chat data and models trained with ShareGPT data alone."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is overall well-written and very clear.\n2. Different from Baize/UltraChat, which asks ChatGPT to act like a user in a zero-shot manner, the authors trained a user-simulating model with real user prompts. This model serves as a data augmentation tool, especially for very long dialogue. \n3. In contrast to ShareGPT, Baize and UltraChat data, the data generated by Parrot can be very long, which allows long-context alignment. This is promising as the model will suffer less out-of-distribution problems for long-context model, e.g., GPT-4 64k, Claude 100k etc. I recommend the authors to emphasize this strength in their paper."
                },
                "weaknesses": {
                    "value": "1. The technical novelty may be limited.\n2. I'd like to see experiments with long-context models, e.g., Long LLaMA."
                },
                "questions": {
                    "value": "1. How good is Parrot's out-of-domain performance? For example, how good is Parrot for specific domains? Also I'd like to see more examples. Consider adding an appendix for qualitative examples.\n2. I would like to see discussion/analysis for the hallucinations in the data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9266/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821498638,
            "cdate": 1698821498638,
            "tmdate": 1699637166966,
            "mdate": 1699637166966,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Mt9ovNWCHB",
                "forum": "Rd4pGjTcTj",
                "replyto": "hQMj6klhFm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer tuRa Part 1"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for the valuable comments.  We are encouraged by the recognition given to our work. We hope our rebuttal will address the concerns of the reviewer.\n\n### **Q1: Novelty.**\nWe clarify our novelty and significance to the community in the following aspects:\n1. we analyze the issues of existing multi-turn datasets such as non-human-like instructions, less detailed responses, short sessions, or limited topic transitions. \n2. to overcome these issues, we propose to train an asking model that can simulate user questions and then collect a new multi-turn dataset. The experimental results demonstrate that our dataset surpasses existing datasets in terms of topic diversity, number of turns, and human-likeness.\n3. we introduce the new MT-Bench++ benchmark, designed to quantitatively evaluate a model's multi-turn performance. By utilizing our Parrot dataset with more dialogue turns and higher quality, we have enhanced the multi-turn capability of a chat model based on LLaMA-2.\n\n\n### **Q2: Long-context models.**\nWe fully recognize that long-context models require long conversations for alignment. However, due to limitations of ChatGPT, such as the 16K token support, as well as the high costs associated with collecting very long dialogues, as an initial research exploration, we have constructed dialogues of up to ten turns, most of which are around 4K in length. We have demonstrated the effectiveness of our methodology for long conversations through sufficient experiments. Given the emergence of more powerful models like GPT-4-Turbo-128K, we believe our methods will be applicable for the alignment of long-context models, and we plan to explore this in the future.\n\n\n### **Q3: Out-of-domain performance.** \nMT-Bench has questions across 8 domains. In Table R1, we compare our proposed Parrot-Chat model with OpenAI GPTs and several open-source models across these domains. Although there remains a significant gap compared to GPT-4 overall and across individual domains, our Parrot-Chat-13B model outperforms GPT-3.5-Turbo in the roleplay and humanities domains. Our model also performs the best among all open-source models in 6 out of 8 domains and the second best in the other two domains. In addition, compared to GPT models, big gaps occur in math, coding, and extraction domains. To further enhance performance, we plan to enrich our dataset with more examples in these domains.\n\nTable R1. Evaluation results of our proposed and baseline chat models in different domains on MT-Bench. We show the average scores in Overall column. Among those open source models, we make the best performance in bold and the second one underlined.\n| Model           | Writing | Roleplay | Reasoning | Math | Coding | Extraction | Stem | Humanities | Overall |\n|-----------------|:-------:|:--------:|:---------:|:----:|:------:|:----------:|:----:|:----------:|:-------:|\n| GPT-3.5-Turbo   |   $9.20$  |   $8.40$   |    $5.65$   | $6.30$ |  $6.90$  |    $8.85$    | $8.70$ |    $9.55$    |   $7.94$  |\n| GPT-4           |   $9.65$  |   $8.90$   |    $9.00$   | $6.80$ |  $8.55$  |    $9.38$    | $9.70$ |    $9.95$    |   $8.99$  |\n| Baize-v2-13B    |   $7.65$  |   $6.80$   |    $5.40$   | $1.80$ |  $3.00$  |    $4.60$    | $7.73$ |    $9.03$    |   $5.75$  |\n| UltraLM-13B     |   $8.15$  |$\\underline{7.25}$|    $4.75$   | $2.60$ |$\\underline{3.45}$|$5.55$    | $8.00$ |$\\underline{9.55}$|   $6.16$  |\n| Vicuna-v1.3-13B | $\\bf{9.25}$|   $7.18$   |  $\\bf{5.85}$ | $2.60$ |  $3.25$  |    $5.55$    | $7.98$ |    $9.45$    |   $6.38$  |\n| Vicuna-v1.5-13B |   $8.10$  |   $8.05$   |   $4.15$ |$\\underline{3.45}$|  $3.00$  |$\\underline{6.35}$|$\\underline{8.50}$|9.45 |$\\underline{6.57}$|\n| Parrot-Chat-13B |$\\underline{8.50}$| $\\bf{8.40}$ |$\\underline{5.50}$|$\\bf{3.95}$|$\\bf{3.85}$|$\\bf{6.55}$  | $\\bf{8.55}$|$\\bf{9.65}$ | $\\bf{6.81}$|"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465829503,
                "cdate": 1700465829503,
                "tmdate": 1700467460840,
                "mdate": 1700467460840,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UYwiPuGbjl",
                "forum": "Rd4pGjTcTj",
                "replyto": "hQMj6klhFm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9266/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response To Reviewer tuRa Part 2"
                    },
                    "comment": {
                        "value": "### **Q4: Qualitative examples.**\nWe have added an appendix containing qualitive examples, including the model generated responses (**Section E**) and examples of our dataset (**Section G**). Please refer to the supplementary material.\n\n### **Q5: Hallucinations in data.**\nOur dataset occasionally exhibits instances of \"hallucinations,\" where ChatGPT generates responses that contain information not aligned with reality. This phenomenon is somehow  unavoidable for now, even in existing datasets such as ShareGPT and UltraChat.  \n\nMore importantly, we have observed that even when ChatGPT generates hallucinatory responses, our Parrot-Ask model can identify mistakes and guide ChatGPT to produce correct responses in subsequent interactions. For example, when ChatGPT initially incorrectly stated that automating the Cisco AnyConnect Secure Mobile Client via a script was not possible, our Parrot-Ask model pointed out that it could be accomplished using C#.  ChatGPT then revised its initial response and provided relevant examples, successfully correcting a hallucination. We have shown more such examples like this in **Appendix F**. \n\nWe find this discovery intriguing, as it suggests that our asking model may have the potential to correct model hallucinations through multi-turn interactions. This is a topic we intend to explore in further detail in the future."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465995376,
                "cdate": 1700465995376,
                "tmdate": 1700469201230,
                "mdate": 1700469201230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "klHZcqIgKR",
                "forum": "Rd4pGjTcTj",
                "replyto": "UYwiPuGbjl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9266/Reviewer_tuRa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9266/Reviewer_tuRa"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. I've read the author response and other reviews. I would like to keep my original recommendation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9266/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632113486,
                "cdate": 1700632113486,
                "tmdate": 1700632113486,
                "mdate": 1700632113486,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]