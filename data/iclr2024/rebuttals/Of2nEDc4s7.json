[
    {
        "title": "Anisotropy helps: improved statistical and computational complexity of the mean-field Langevin dynamics under structured data"
    },
    {
        "review": {
            "id": "BLHR2S5kXZ",
            "forum": "Of2nEDc4s7",
            "replyto": "Of2nEDc4s7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8085/Reviewer_Gjnf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8085/Reviewer_Gjnf"
            ],
            "content": {
                "summary": {
                    "value": "The authors address how gradient-based feature learning interacts with anistropic input data. They do this by studying the sparse parity problem where feature coordinates have varying magnitudes. To this end, they analyze the learning complexity of the mean-field Langevin dynamics, which describes the noisy gradient descent update on two-layer neural networks. They show one can use the anisotropy of the data to improve the statistical complexity (i.e. sample size) and computational complexity (i.e. network width) of the mean-field Langevin dynamics. This improvement is found when the main directions of the anisotropic input data aligns with the support of the target function. They also provide a method using coordinate transformations determined by the gradient covariance to show that the computational complexity can be improved by exploiting the anisotropy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The results seem novel and interesting. Namely they show how one can utilize the anisotropy of the input data to improve learning for a specific setting.\n- The result of coordinate transformations which leverages anisotropy could be of interest for practical neural network training.\n- The implication of a tradeoff between statistical and computational complexity also seems interesting. \n- The paper is clearly written and the results clearly stated."
                },
                "weaknesses": {
                    "value": "- In my opinion, the claim that \"anisotropy helps\" seems too strong, a bit specific to the problem in the paper, and not necessarily a general statement that can be made for neural networks based on the assumptions and results of the paper. It would be great if the authors could better motivate a connection between this problem and more general neural networks with real-world datasets.\n\n- The authors state on page 2, under \"Feature learning under structured data\", that \n\n  > in certain regression settings with low-dimensional target, structured data with a spiked covariance structure can improve the performance of both kernel methods and optimized NNs (Ghorbani et al., 2020; Ba et al., 2023; Mousavi-Hosseini et al., 2023; Suzuki et al., 2023b). However, these regression analyses do not directly translate to the binary classification setting which the k-parity problem belongs to.\n\n  This implies that the intuition \"anisotropy helps\" is perhaps already observed in the literature, which may weaken the impact of the paper. If the authors could expand on why the $k$-parity problem is worth studying separately, that would really help motivate things."
                },
                "questions": {
                    "value": "- One page 7, under Corollary 1, the authors state: \n  > On the other hand, if the input covariance is anisotropic so that ... then the value of $R$ becomes dimension-free: $R = O(k^2 log(k)^2 )$.\"\n  \n  Can similar results apply to real-world datasets?\n\n- How applicable is the coordinate transformation used in the paper, to conventional neural network training for real-world datasets?\n\n- For equation (2), we're using a smooth activation function (tanh). Can the results apply for nonsmooth activation functions, namely ReLU networks? i.e. can the intuition \"anisotropy helps\" also apply for ReLU networks?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8085/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8085/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8085/Reviewer_Gjnf"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8085/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737964134,
            "cdate": 1698737964134,
            "tmdate": 1699637001273,
            "mdate": 1699637001273,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ckJg2OcI1s",
                "forum": "Of2nEDc4s7",
                "replyto": "BLHR2S5kXZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8085/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive evaluation and helpful feedback. We address the technical comments below. \n\n**Connection to more general neural networks with real-world datasets**\n\nFirst, we note that our primary goal is to develop theory for representation learning \u2013 to understand when it is beneficial in terms of statistical and computational complexity, and how it interacts with different aspects of the learning problem / algorithm, such as structure of the data and target function, model architecture, and optimization algorithm. More specifically, the focus of this work is to rigorously examine the interplay between feature learning and the anisotropic structure of input data, which, as remarked in the introduction, is relatively underexplored especially in the context of classification problems. While our theoretical setting is idealized, it is motivated from practical observations that real-world data exhibits low-dimensional structure (see below), and we believe that our analysis serves as an important step towards a concrete understanding of feature learning in more complicated and realistic settings. \n\nOn the other hand, our problem setting is motivated by empirical observations on realistic data. Real-world data, including image and text, can be high-dimensional, yet neural networks often efficiently learn from data and avoid the \u201ccurse of dimensionality\u201d, This success can be attributed to the fact that real-world data exhibits some underlying structure, and intrinsic low dimensionality is considered as one of the prominent factors. Specifically, low intrinsic dimensionality has the following two aspects.  \n(1) *Low-dimensionality of ground truth (target function)*. This means that not all directions of the input features are important for predicting $y$.   \n(2) *Anisotropy of input data*. This means that the features already contain low-dimensional structures despite the large ambient dimension.   \nThe $k$-sparse parity classification problem is a classical example of low-dimensional function acting on high-dimensional data, and most prior theoretical studies for this problem setting considered isotropic input data, which accounts for (1) but not (2). By introducing this \u201cgeneralized\u201d version of the sparse parity problem on anisotropic input, we aim to theoretically study the interplay between structured (anisotropic) data and the efficiency of feature learning via gradient descent. We show that neural networks trained by gradient-based algorithms indeed exploit such low-dimensional structure, as evident in the improved statistical and computational complexity.  \nAs for application to real-world data, our Theorem 1 suggests that neural networks can leverage low-dimensional structure to improve generalization performance, whereas Theorem 2 suggests that the structure of target function can be revealed by the gradient covariance matrix. We believe that these insights can be transferred to practical settings at a high level. \n\n**Difference from prior results on anisotropic data** \n\nWe have revised the Introduction to highlight the difference from prior works and new technical challenges. We make the following remarks. \n1. Classification and regression problems have fundamentally different structures: it is possible that a predictor achieves vanishing classification error but has large regression loss. Concretely, the following aspects of our analysis do not translate from the regression setting: \n* For the neural network results, we exploited properties of the logistic loss, and more importantly, margin conditions on the optimal classifier. Such analysis does not follow from reducing the classification problem to regression (e.g., with the squared loss). \n* For the kernel lower bound, establishing a lower bound on the classification error is much more challenging than on the regression loss, because it is clear that a kernel model can perfectly predict the sign of the labels $y$ but do not achieve small squared error. \n2. Regarding the optimization dynamics, our analysis differs significantly from prior results (Ghorbani et al., 2020; Ba et al., 2023; Mousavi-Hosseini et al., 2023). (Ghorbani et al., 2020) did not provide any optimization guarantees for neural networks. (Ba et al., 2023; Mousavi-Hosseini et al., 2023) considered a narrow neural network in the so-called \u201crotation\u201d regime, where the statistical complexity is governed by the *information exponent* of the ground truth. Roughly speaking, to learn a degree-$k$ parity function in this regime, the sample complexity may scale with $d^{\\Theta(k)}$. In contrast, we focused on neural networks in the mean-field regime, which allows us to \u201cdecouple\u201d the degree from the exponent of the dimension dependence, albeit at a cost of higher computational complexity (Theorem 1). Finally, we realize that (Suzuki et al., 2023b) does not handle the anisotropic setting \u2013 we have removed this citation in the paragraph on structured data."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288604542,
                "cdate": 1700288604542,
                "tmdate": 1700288604542,
                "mdate": 1700288604542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3nWk1bJw1F",
                "forum": "Of2nEDc4s7",
                "replyto": "qyygDNtsXU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8085/Reviewer_Gjnf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8085/Reviewer_Gjnf"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their nicely written reply. I also see they have updated the paper, and I find these updates to be good revisions. I have also read the other reviews.\n\nThe authors have mostly addressed my concerns. In general, I remain reserved about whether findings from studying just the $k$-sparse parity problem can generalize to more settings, and specifically to real-world settings. As mentioned in my review, this can be alleviated if the authors had more experiments closer to real-world situations.\n\nFor now, I am inclined to keep my score, but recognize that the authors have made good clarifications in the discussion, and positive updates to their paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512704285,
                "cdate": 1700512704285,
                "tmdate": 1700512704285,
                "mdate": 1700512704285,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SHK052LKMn",
            "forum": "Of2nEDc4s7",
            "replyto": "Of2nEDc4s7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8085/Reviewer_e822"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8085/Reviewer_e822"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates mean-field Langevin dynamics on structured anisotropic input data with a k-sparse parity target function. It provides results for specific 2-layer neural networks around both statistical and computational complexity. In particular, it proves discrete-time and finite-width learning guarantees thus extending on the results of previous work. These results are verified empirically on synthetic data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Disclaimer: I have not checked the validity of the proofs in the appendix and, therefore, cannot comment on the correctness of the results.\n\n* This work is generally well written.\n* The related work seems to be well addressed (although I am not familiar with the literature). \n* The contributions are quite clearly laid out in the introduction. \n* Section 2 (and 3) does a reasonable job of laying out the problem setting."
                },
                "weaknesses": {
                    "value": "* I am not well-positioned to comment on the significance of the results within this research area. However, it is not clear to me why we should be interested in this formal setting beyond the fact that some previous works have considered it. The assumptions seem highly contrived and no attempt is made to link them to any practical task in a meaningful way. Could the authors explain why progress in this research direction is worth pursuing? I would argue that if this work is being submitted to a broad conference such as ICLR, more effort should be made to broaden its appeal by explaining why its setting is relevant."
                },
                "questions": {
                    "value": "* Given the very niche nature of this paper, I'm not sure that ICLR is the best venue for this work. Given that (I would imagine) this work would be of interest to quite a small subcommunity, would it not be more suitable to submit to a more specific venue rather than a generalist conference like ICLR? This is not to speak negatively about this work, but it seems that this topic is less generally relevant in its nature and may not be best suited to this large-scale style of conference. \n\n* Could the authors provide a definition of anisotropic input data? I don't think this was clearly defined. Does it exactly refer to the setting defined mathematically at the beginning of Sec 1.1?\n\n* Why can the existing results for the regression setting not be directly applied in the binary classification setting by converting it to a regression problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8085/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8085/Reviewer_e822",
                        "ICLR.cc/2024/Conference/Submission8085/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8085/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699361339640,
            "cdate": 1699361339640,
            "tmdate": 1700502776865,
            "mdate": 1700502776865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z98L0R9rTe",
                "forum": "Of2nEDc4s7",
                "replyto": "SHK052LKMn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8085/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the helpful feedback. \n\nThe reviewer\u2019s main concern is regarding the relevance of our theoretical findings to the ICLR community. We motivate our problem setting from the following three perspectives.  \n\n**1. Why do we care about the theory of representation (feature) learning?** \n\nIt is clear that representation learning is a central topic of ICLR, as indicated by the title of the conference. We argue that it is imperative to develop theory for representation learning \u2013 to understand when it is beneficial in terms of statistical and computational complexity, and how it interacts with different aspects of the learning problem / algorithm, such as structure of the data and target function, model architecture, and optimization algorithm. The focus of this work is to rigorously examine the interplay between feature learning and the anisotropic structure of input data, which, as remarked in the introduction, is relatively underexplored especially in the context of classification problems. While our theoretical setting is idealized, it is motivated from practical observations that real-world data exhibits low-dimensional structure (see point below), and we believe that our analysis serves as an important step towards a concrete understanding of feature learning in more complicated and realistic settings, and is certainly of interest to the ICLR community. In fact, there are at least 5 papers accepted to ICLR 2023 that theoretically analyzed the feature learning dynamics under similar idealized settings: [Mousavi et al.](https://openreview.net/pdf?id=6taykzqcPD), [Bordelon and Pehlevan](https://openreview.net/pdf?id=nZ2NtpolC5-), [Akiyama and Suzuki](https://openreview.net/pdf?id=6doXHqwMayf), [Telgarsky](https://openreview.net/pdf?id=swEskiem99), [Tian](https://openreview.net/pdf?id=s130rTE3U_X).\n\n**2. Why is the sparse parity problem worth studying?** \n\nThe $k$-sparse parity classification problem is a classical example of low-dimensional function acting on high-dimensional data. This function often serves as a testbed for theory of representation learning, and has been extensively studied in the isotropic setting  \u2013 see references in the Introduction section. The reason is twofold: (i) The target function only depends on a few coordinates of the input features, and hence we expect that the trained neural network representation can \u201czoom-in\u201d to the relevant k-dimensional subspace. (ii) For methods that do not exploit such low-dimensional structure, such as neural networks in the lazy (NTK) regime, we expect the sample complexity to be inferior to the feature learning alternative \u2013 this provides a simple setting to illustrate the benefit of representation learning. \n\n**3. What new insights do we gain from introducing anisotropy?** \n\nReal-world data, including image and text, can be high-dimensional, yet neural networks often efficiently learn from data and avoid the \u201ccurse of dimensionality\u201d, This success can be attributed to the fact that real-world data exhibits some underlying structure, and intrinsic low dimensionality is considered as one of the prominent factors. Specifically, low intrinsic dimensionality has the following two aspects (see [Pope et al.](https://arxiv.org/pdf/2104.08894.pdf) for example).  \n(1) *Low-dimensionality of ground truth (target function)*. This means that not all directions of the input features are important for predicting $y$.   \n(2) *Anisotropy of input data*. This means that the features already contain low-dimensional structures despite the large ambient dimension.    \n\nAs noted in the previous point, prior theoretical studies on the isotropic parity problem accounted for (1) but not (2). By introducing this \u201cgeneralized\u201d version of the sparse parity problem on anisotropic input, we aim to theoretically study the interplay between structured (anisotropic) data and the efficiency of feature learning via gradient descent. We show that neural networks trained by gradient-based algorithms indeed exploit such low-dimensional structure, as evident in the improved statistical and computational complexity. Similar problem setups and motivations have appeared in [Ghorbani et al. (2020)](https://proceedings.neurips.cc/paper/2020/hash/a9df2255ad642b923d95503b9a7958d8-Abstract.html) and [Ba et al. (2023)](https://openreview.net/forum?id=HlIAoCHDWW) but for the regression setting. \n\nWe have revised the Introduction to highlight the motivation of our problem setting and the relevance of our theoretical results."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287911918,
                "cdate": 1700287911918,
                "tmdate": 1700300261816,
                "mdate": 1700300261816,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KbFWapu75n",
                "forum": "Of2nEDc4s7",
                "replyto": "Z98L0R9rTe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8085/Reviewer_e822"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8085/Reviewer_e822"
                ],
                "content": {
                    "comment": {
                        "value": "I am grateful to the authors for their rebuttal. My primary issue with this work (and related works) is with respect to the claim that \"our analysis serves as an important step towards a concrete understanding of feature learning in more complicated and realistic settings\". It is not clear to me that this is true -- the path of understanding settings such as the one in this work and translating these findings into practical scenarios is something that appears less and less plausible to me over time. I hope future work will make a greater attempt to demonstrate this claimed value in some form. \n\nHowever, I appreciate that there may be some interest in the research community in this work which was certainly well presented. I do not have any strong reason to reject this paper based on technical complaints and therefore I raise my score to 6."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503394097,
                "cdate": 1700503394097,
                "tmdate": 1700503394097,
                "mdate": 1700503394097,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YxRqCQBHYK",
            "forum": "Of2nEDc4s7",
            "replyto": "Of2nEDc4s7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8085/Reviewer_obgQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8085/Reviewer_obgQ"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors study the statistical and computational complexity of mean-field Langevin dynamics (MFLD) with anisotropic input data. In particular, they show that both complexities can be improved when prominent directions of the anisotropic input data align with the support of the target function."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is technically solid and study an important problem. MFLD is a recent framework to help us understand the behavior of two-layer nonlinear NN, particularly relevant to the isotropic $k$-parity problem. Extending the isotropic setting to the anisotropic setting is definitely interesting to study, and the authors have established learning guarantees for two-layer nonlinear NN for this anisotropic setting."
                },
                "weaknesses": {
                    "value": "It appears that the whole work assumes that the matrix $A$ is known a priori or prespecified. To me, this assumption might be too strong and make the problem of more theoretical interest than very practically relevant, although it seems to me that the motivation of studying anisotropic data arises from the case of realistic datasets as mentioned in the abstract."
                },
                "questions": {
                    "value": "If we have real data while $A$ is generally unavailable or can only be estimated from data, what can we say about this case with the results of this paper? \n\nI also wonder if the problem can be tackled with a preconditioned version of MFLD, instead of performing coordinate transforms on the input. \n\nTypo:\n- page 6, after Proposition 2: otherwise, we \u201cstill\u201d\n- page 6, after Proposition 3: Under this \u201ccondition\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8085/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699419340767,
            "cdate": 1699419340767,
            "tmdate": 1699637001041,
            "mdate": 1699637001041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iBkverdoJF",
                "forum": "Of2nEDc4s7",
                "replyto": "YxRqCQBHYK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8085/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8085/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive evaluation and insightful comments. We have revised the manuscript and corrected the typos you pointed out. We address the technical points below.\n\n**\"The whole work assumes that the matrix $A$ is known a priori or prespecified\"** \n\nWe make the following remarks.  \n* In the data generating process, we assume a fixed transformation $A$ (which is not known to the learning algorithm). This problem setting is motivated by the empirical observation that real-world features exhibit low-dimensional structure, which is not captured by many prior works on feature learning assuming isotropic input. By introducing this \u201cgeneralized\u201d version of the sparse parity problem on anisotropic input, we aim to theoretically study the interplay between structured data and the efficiency of feature learning via gradient descent. To further illustrate the benefit of feature learning in the anisotropic setting, in the revised manuscript, we introduced a stronger kernel lower bound that applies to a wide range of (spiked) anisotropic data. \n* We emphasize that the transformation $A$ is not given to the learning algorithm; instead, the algorithm only has access to i.i.d. observations from the data generating process, and our goal is to characterize how anisotropy (controlled by the matrix $A$) enhances the performance of MFLD. Similar problem setups and motivations have appeared in [Ghorbani et al. (2020)](https://proceedings.neurips.cc/paper/2020/hash/a9df2255ad642b923d95503b9a7958d8-Abstract.html) and [Ba et al. (2023)](https://openreview.net/forum?id=HlIAoCHDWW) for the regression setting. \n* As for implications for real-world data, since the learning algorithm does not know $A$ a priori, intuitively speaking, what Theorem 1 suggests is that neural networks can leverage low-dimensional structure to improve generalization performance, whereas Theorem 2 suggests that the structure of target function can be revealed by the gradient covariance matrix. We believe that these insights can be transferred to real-world settings. \n\n**\"I also wonder if the problem can be tackled with a preconditioned version of MFLD\"**\n\nThank you for the interesting suggestion. At a high level, there is indeed a parallel between the coordinate transformation and preconditioning. However, we note that naively preconditioning the entire update yields the stationary distribution unchanged (though the optimization speed might improve), whereas our coordinate transform on the input feature yields a different stationary distribution on the parameter space, which enables us to establish a better LSI constant and generalization. It is an interesting question of whether we should apply the preconditioner to the gradient of the loss, the regularization, or the Gaussian noise separately, as here different design choices would result in different stationary distributions of MFLD. We intend to investigate this perspective in future work. \n\t\nWe would be happy to clarify any further concerns/questions in the discussion period."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287292327,
                "cdate": 1700287292327,
                "tmdate": 1700287292327,
                "mdate": 1700287292327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0DSXC4KoDm",
                "forum": "Of2nEDc4s7",
                "replyto": "iBkverdoJF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8085/Reviewer_obgQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8085/Reviewer_obgQ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks the authors for the response and some comments on my suggestion. This seems to be an interesting direction to pursue and I hope to see the future work which definitely make the problem concerned more practically relevant."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8085/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721368725,
                "cdate": 1700721368725,
                "tmdate": 1700721368725,
                "mdate": 1700721368725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]