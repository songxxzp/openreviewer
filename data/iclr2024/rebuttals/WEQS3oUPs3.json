[
    {
        "title": "Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations"
    },
    {
        "review": {
            "id": "5Sh79Q2TGk",
            "forum": "WEQS3oUPs3",
            "replyto": "WEQS3oUPs3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8055/Reviewer_yVGF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8055/Reviewer_yVGF"
            ],
            "content": {
                "summary": {
                    "value": "This paper argues that LLMs trained with SFT or single-step RL might struggle with tasks that require goal-directed behavior. The authors propose a method to use LLMs to generate useful data for solving such tasks by simulating human-like behaviors. The data are used to train a conversational agent for goal-directed tasks with offline RL, in order to improve over the trained conversations. Their results show that the approach achieves better performance than directly prompting LLMs or training the agents with behaviour cloning, in various goal-directed dialogue tasks. However, there are concerns about how the human annotator evaluates the conversations and why the authors did not choose widely-used task-oriented dialogue benchmark datasets like multiwoz and schema-guided-dialogues (SGD).\nIn addition, although the results show that the learned agents are better than LLMs in information-seeking and generating less overwhelming responses in some specific tasks or domains, I wonder if the smaller agents are able to handle other domains, where the small agents might not have knowledge. Is it still a good alternative to LLMs in this case? Is it still useful?"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper introduces an approach to generate goal-directed conversations with LLMs and then train a smaller agent to improve over these conversations. The human evaluation results show that the learning agents do generate responses that are more helpful in helping the users complete the tasks and generate less overwhelming responses."
                },
                "weaknesses": {
                    "value": "1. Why not utilize the widely used task-oriented dialogue benchmarks, MultiWOZ and SGD? There are works that leverage LLMs for task-oriented dialogue by training a small model to generate dialogue actions (plans) with RL, guiding LLMs for improved responses [1]. Have you considered comparing with them?\n2. From the examples comparing GPT-agent and IE+RL agents, GPT's responses didn't seem significantly inferior. How were the responses scored by the evaluators using the four criteria? Was there consensus in their annotations, and what was the level of agreement?\n3. LLMs often produce overwhelming responses, but their strength lies in their capability to converse on a wide range of topics due to their inherent knowledge. While training a smaller model for better goal-oriented conversations for some specific domains and topics might seem more beneficial than directly using LLMs, is such a model able to handle out-of-domain tasks and topics? How does it perform when discussing topics outside its training data? If it can not handle out-of-domain topics, is such a model still practical and useful for real-world applications?\n4. I feel that the paper complicates the data generation section by unnecessarily introducing numerous reinforcement learning concepts. Why introduce these concepts when it appear to be a simple data generation process, making it challenging to comprehend?\n\n[1] Li, Z., Peng, B., He, P., Galley, M., Gao, J., & Yan, X. (2023). Guiding Large Language Models via Directional Stimulus Prompting. arXiv preprint arXiv:2302.11520."
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Reviewer_yVGF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8055/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698545852144,
            "cdate": 1698545852144,
            "tmdate": 1699636996110,
            "mdate": 1699636996110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YLqUHn0bgO",
                "forum": "WEQS3oUPs3",
                "replyto": "5Sh79Q2TGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yVGF (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review. Your primary concern was a lack of comparison on more objective benchmarks, or to sophisticated prompting methods. We address both of them by providing a new evaluation, and a comparison to a version of the directional prompting approach to our problem setting. We will address each of your concerns in more detail below.\n\n**Task-oriented dialogue benchmarks**\n\nWe considered using task-oriented benchmarks such as MultiWOZ. However, upon closer inspection, we found that the tasks themselves did not involve any intelligent information-gathering, and were more similar to question-answering tasks commonly found in NLP benchmarks. Because of this, we did not feel there was a need for agents to exhibit long-term planning behavior, which would be the primary advantage of using multi-step RL.\nIn addition, evaluation of agents in these benchmarks would involve computing a ROUGE or BLEU score, which are not quite representative of solving the task itself but how well agents mimic the data. Because of this, we feel that such benchmarks are tailored for supervised-learning agents rather than RL ones.\n\nHowever, we agree that evaluation via a purely user study is subjective. To remedy this, we provide a larger-scale evaluation on synthetic agents in the preference elicitation task (in Appendix C of our updated paper), where synthetic agents are LLMs prompted with a specific preference (which is an activity they enjoy doing), and we evaluate an agent\u2019s capability to exactly recover this preference. We did find that because such synthetic humans were prompted with an underlying preference, they often prematurely revealed that preference; for example, if the agent proposes an activity that the synthetic human would not enjoy, the synthetic human would simply point out they would prefer their activity more. This means that it was easy to ultimately achieve success against such synthetic humans. Therefore, we additional measure how well each evaluated agent gets the underlying activity in their first try, as well as how verbose they were (in terms of # tokens per utterance). Because our proposed agents are much better at information-gathering, they get the exact activity in the first try in 44% of our evaluations, compared to 18% by a prompted GPT agent; furthermore, our agents do so in 1/3 as many tokens per utterance (43 compared to 118). All our new results are summarized in Table 4.\n\n\n**Examples comparing GPT agent and IE+RL agent**\n \nWe believe the IE+RL agent does much more intelligent long-term planning, specifically in how it gathers information about the user\u2019s underlying intent. We present an example comparing the IE+RL and GPT-agent below (more examples can be found in Appendix B of the paper). Here, the GPT-agent tries to explain behavior cloning:\n```\nH: Hi, I've been hearing about something called \"behavior cloning\" in AI, but I don't really know what it is. Can you explain it to me? \nAI: Certainly! Let's start by understanding your background in AI to provide an explanation that matches your level of knowledge. Please answer the following questions:\n1. Have you ever studied or worked with artificial intelligence or machine learning concepts before, even at a basic level? \n2. Are you familiar with terms like \"neural networks,\" \"training data,\" or \"supervised learning\"? \n3. Are you interested in a high-level overview or a more detailed explanation of behavior cloning?\n```\nThe GPT-agent attempts information-gathering because it is prompted to, but does so in a myopic manner by dumping a survey of questions for the human to answer. In contrast, our IE+RL agent employs a \u201cstep by step\u201d plan to investigate how much the human already knows about behavior cloning:\n```\nH: Hi there! I've heard about this thing called \"behavior cloning\" in AI, but I don't really understand what it is. Can you explain it to me?\nAI: Of course! I'd be happy to explain behavior cloning. To start, could you tell me if you've ever come across the terms \"artificial intelligence\" or \"machine learning\" before?\nH: Yes, I've heard those terms, but I'm not exactly sure what they mean.\nAI: No problem at all! Let's take it step by step. Have you ever used a computer or a smartphone?\nH: Yes, I use my computer and phone all the time. \nAI: Great! So, imagine that a computer or a smartphone could learn to do something by watching you do it. That's a bit like what behavior cloning is.\nH: Oh, interesting. Can you explain more about how that works?\nAI: Sure. Behavior cloning is a \u2026\n```\nWe would argue that this is greatly preferred, as users who are unfamiliar with ML concepts would not be able to answer most of the questions provided in the survey by the GPT-agent. This is reflected in the quantitative ratings in Table 1 that the users provide after interacting with both agents. We believe that the level of agreement among users is high, as we also report standard error which in general is much lower than the observed improvement."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248381209,
                "cdate": 1700248381209,
                "tmdate": 1700248381209,
                "mdate": 1700248381209,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KThCbNE5cF",
                "forum": "WEQS3oUPs3",
                "replyto": "5Sh79Q2TGk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Let us know if you have further questions"
                    },
                    "comment": {
                        "value": "Thank you again for your review. Let us know if you have any further concerns or clarifications, and we will do our best to address them!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591476487,
                "cdate": 1700591476487,
                "tmdate": 1700591476487,
                "mdate": 1700591476487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lbMPfm81Im",
            "forum": "WEQS3oUPs3",
            "replyto": "WEQS3oUPs3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8055/Reviewer_Ntkf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8055/Reviewer_Ntkf"
            ],
            "content": {
                "summary": {
                    "value": "The study presents a reinforcement learning (RL) approach for training goal-directed dialogue agents on synthetic dialogues produced by large language models (LLMs). Known as the \"imagination engine,\" this method generates training data from simulated talks instead of large-scale human-generated datasets. This technique yields agents who perform better on goal-oriented activities than typical LLMs, indicating a new direction for conversational AI development\u2014one that can comprehend and accomplish difficult tasks with little to no human oversight."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It shifts the use of LLMs from direct interaction to data generation for optimization by introducing a zero-shot RL algorithm with a \"imagination engine\" that creatively creates synthetic conversation datasets for training dialogue agents.\n\n2. Compared to traditional approaches, the method optimizes for goal-directed dialogues more effectively since it trains agents on a variety of human-like talks generated by LLMs that are customized for particular dialogue objectives.\n\n3. The usefulness and efficiency of this approach are demonstrated empirically, as agents trained with it outperform state-of-the-art LLMs in interactive tasks."
                },
                "weaknesses": {
                    "value": "A shortcoming of the work is its somewhat dependent use of human-generated prompts, suggesting opportunities for further development in automating zero-shot dialogue agents' training to work without task-specific human input."
                },
                "questions": {
                    "value": "What are the detailed version specifications and hyper-parameter configurations of GPT-3.5 used in the imagination engine, and how do these parameters affect the generated dialogue quality and diversity?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Reviewer_Ntkf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8055/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840651926,
            "cdate": 1698840651926,
            "tmdate": 1699636995973,
            "mdate": 1699636995973,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OCA8LoH3G9",
                "forum": "WEQS3oUPs3",
                "replyto": "lbMPfm81Im",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Ntkf"
                    },
                    "comment": {
                        "value": "Thank you for your review. We raised a couple concerns that we address below. If there is any additional concrete evidence or information that we can provide to you, we would be happy to do so. \n\n**Human-generated prompts** \n\nThat is indeed a limitation of the current approach. However, we would like to point out that prompt-engineering is a large part of many existing approaches that leverage LLMs. We also alleviate much of the burden of prompt-engineering in the reasoning step, by having the LLM provide the task-specific knowledge required to craft prompts. We show the specific prompts we used for each task in Appendix A of our paper, as one can observe that they are quite natural and not overly-engineered. Further, our approach does not seem to be sensitive to the specifics of the prompts used \u2013 we will add to Appendix A some qualitative evidence of dialogues generated from different paraphrases of prompts for comparison. This means that our approach takes out much of the heavy-lifting in prompt-engineering and is much more robust than many contemporary usages of LLMs.\n\n**Hyper-parameter configurations**\n\nWe use the gpt-3.5-turbo-instruct model that is the same as the widely used ChatGPT interface. We simply use a temperature of 1 and maximum generation length of 512 tokens. We agree that temperature can potentially have some effect on the dialogue diversity, but found that the default of 1 was enough to generate sufficiently diverse dialogues from the same prompt. We have added this information to implementation details in Appendix A of our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700248180070,
                "cdate": 1700248180070,
                "tmdate": 1700248180070,
                "mdate": 1700248180070,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KNS1STWwsc",
                "forum": "WEQS3oUPs3",
                "replyto": "lbMPfm81Im",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Let us know if you have further questions"
                    },
                    "comment": {
                        "value": "Let us know if you have any remaining questions/concerns, as we would be happy to address them!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591392625,
                "cdate": 1700591392625,
                "tmdate": 1700591392625,
                "mdate": 1700591392625,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gsvEjE2ESN",
            "forum": "WEQS3oUPs3",
            "replyto": "WEQS3oUPs3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8055/Reviewer_sDqR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8055/Reviewer_sDqR"
            ],
            "content": {
                "summary": {
                    "value": "This paper utilizes LLM to simulate sub-optimal but human-like behavior to produce examples of possible interactions. The algorithm uses the data and offline reinforcement learning to train an interactive conversational agent to learn to perform more optimal interactions. Experiments show that the method achieves the most advanced performance in a variety of goal-oriented conversation tasks.\nWhat contributions does it make: \n1.The paper propose a zero-shot RL algorithm that effectively optimizes for goal-directed dialogue tasks.\n2.The idea of imagination engine (IE) that generates a dataset of diverse, task-relevant, and instructive dialogues makes sense."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.The experimental analysis is detailed and methodical, and the case is clear and intuitive.\n2.The idea of using LLM to imitate human behavior is interesting."
                },
                "weaknesses": {
                    "value": "1.Even thought RL can combine parts of behavious seen form behavior policies in the data, it is not convincing that the RL can take all the long-term planing responsibility in the goal-oriented conversation tasks. \n2.The novelty of this paper is limited. The proposed method can be regarded as a pipeline of LLM generation and offline RL training.  \n3.All the evaluation methods are human evaluation, which are highly subjective. \n4.More relevant works should be compared in the experiments."
                },
                "questions": {
                    "value": "Need more details about the evaluators in the experiments, such as their education background."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8055/Reviewer_sDqR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8055/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698923816879,
            "cdate": 1698923816879,
            "tmdate": 1700644488464,
            "mdate": 1700644488464,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lKSpOLCig4",
                "forum": "WEQS3oUPs3",
                "replyto": "gsvEjE2ESN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sDqR (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review. The reviewer raised concerns regarding the results of the paper being unconvincing, particularly in whether our agents actually perform long-term planning. First, we will present why we believe our existing experiments suggest long-term planning capabilities by our agents, and then, we will show results for a new large-scale synthetic evaluation as further evidence. We agree with the reviewer that RL may not solve \u201call\u201d components of long-term planning, but we believe it is a significant improvement over existing approaches.\n\nFirst, we argue that the tasks we consider require long-term planning to be done effectively. This is because successful agents require taking information-gathering steps to better understand the user\u2019s intent before proposing a solution. \n\nQuantitatively, our results show that users greatly prefer interacting with our proposed agent than a baseline that prompts existing LLMs (even though these LLMs are much larger), because our agent shows more competence at information-gathering. Specifically, our agent was much better at maintaining back-and-forth dialogue and asking relevant questions, achieving almost 50% higher ratings in those metrics (see Table 1). Qualitatively, we believe that agents learned via our approach explicitly demonstrate long-term planning behavior. Specifically, the dialogue below (and others shown in Appendix B of our paper) shows that our learned agent employs a \u201cstep by step\u201d plan to investigate how much the human already knows about behavior cloning, before giving an explanation tailored to the human\u2019s past responses. This capability was not found in any dialogues by agents obtained from directly prompting GPT (examples can again be found in Appendix B).\n```\nH: Hi there! I've heard about this thing called \"behavior cloning\" in AI, but I don't really understand what it is. Can you explain it to me?\nAI: Of course! I'd be happy to explain behavior cloning. To start, could you tell me if you've ever come across the terms \"artificial intelligence\" or \"machine learning\" before?\nH: Yes, I've heard those terms, but I'm not exactly sure what they mean.\nAI: No problem at all! Let's take it step by step. Have you ever used a computer or a smartphone?\nH: Yes, I use my computer and phone all the time. \nAI: Great! So, imagine that a computer or a smartphone could learn to do something by watching you do it. That's a bit like what behavior cloning is.\nH: Oh, interesting. Can you explain more about how that works?\nAI: Sure. Behavior cloning is a \u2026\n```\n\nIn addition, we add a larger-scale evaluation on synthetic agents in the preference elicitation task (in Appendix C of our updated paper), where synthetic agents are LLMs prompted with a specific preference (which is an activity they enjoy doing), and we evaluate an agent\u2019s capability to exactly recover this preference. We did find that because such synthetic humans were prompted with an underlying preference, they often prematurely revealed that preference; for example, if the agent proposes an activity that the synthetic human would not enjoy, the synthetic human would simply point out they would prefer their activity more. This means that it was easy to ultimately achieve success against such synthetic humans. Therefore, we additionally measure how well each evaluated agent gets the underlying activity in their first try, as well as how verbose they were (in terms of # tokens per utterance). Because our proposed agents are much better at information-gathering, they get the exact activity in the first try in 44% of our evaluations, compared to 18% by a prompted GPT agent; furthermore, our agents do so in 1/3 as many tokens per utterance (43 compared to 118). All our new results are summarized in Table 4."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247993225,
                "cdate": 1700247993225,
                "tmdate": 1700247993225,
                "mdate": 1700247993225,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lw1ApyMIZ3",
                "forum": "WEQS3oUPs3",
                "replyto": "gsvEjE2ESN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8055/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Let us know if you have further questions"
                    },
                    "comment": {
                        "value": "Thanks for reviewing our work. Before discussion ends, we would appreciate it if you let us know what your remaining concerns are with our work. We will do our best to answer them!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8055/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591362276,
                "cdate": 1700591362276,
                "tmdate": 1700591362276,
                "mdate": 1700591362276,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]