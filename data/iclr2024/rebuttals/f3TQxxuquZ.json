[
    {
        "title": "One-stage Prompt-based Continual Learning"
    },
    {
        "review": {
            "id": "Lr3rO3jX0I",
            "forum": "f3TQxxuquZ",
            "replyto": "f3TQxxuquZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1437/Reviewer_2Phe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1437/Reviewer_2Phe"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a one-stage prompt-based continual learning strategy that simplifies the query design and improves computational efficiency. In particular, it directly uses the intermediate layer's token embedding as a prompt query and introduces a query-pool regularization strategy to enhance the representation power. The experimental evaluation shows that the proposed method achieves about 50\\% computation cost reduction and better performance on two benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow.\n\n- The proposed idea is well-motivated and the method is simple and effective."
                },
                "weaknesses": {
                    "value": "- Assumption on the pretrained network. One limitation of existing PCL methods is that they rely on supervised pretraining on ImageNet1k, which has a large impact on the model performance but can be infeasible for a general CL task. Does this method still work without using supervised pretraining? For instance, what if replacing the supervised pretraining model with an unsupervised pretraining model (e.g. DINO [1])?\n\n[1] Caron et al. Emerging Properties in Self-Supervised Vision Transformers. ICCV2021.\n\n- Lack of clarity in experimental results. In particular, Figure 2 is not very clear. Is the token embedding distance depicted in this Figure related to the current task? What are the distances between token embeddings for tasks 1 to t-1 when task t is learned? Such an illustration could provide more insights into how the method alleviates catastrophic forgetting."
                },
                "questions": {
                    "value": "See the comments in the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829030902,
            "cdate": 1698829030902,
            "tmdate": 1699636072393,
            "mdate": 1699636072393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PNaIepYu71",
                "forum": "f3TQxxuquZ",
                "replyto": "Lr3rO3jX0I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We appreciate your positive feedback on our work. Please check our response to your questions and concerns.\n\nQ1: Does this method still work without using supervised pretraining? For instance, what if replacing the supervised pretraining model with an unsupervised pretraining model (e.g. DINO [1])? \n\nA1: Thank you for the valuable feedback. We think the reviewer's suggestion is interesting and important for PCL works. In response to this suggestion, we use DINO pre-trained weights instead of ImageNet-1k pre-trained weights, and compare the performance across ER, LwF, and PCL works. We set the other experimental settings as identical.\n\n| Method (DINO pre-trained method) |    Acc (%)    | Forgetting Score |\n|----------------------------------|:-------------:|:----------------:|\n| ER                               |  60.43 \u00b1 1.16 |   13.30 \u00b1 0.12   |\n| LwF                              | 62.73 \u00b1 1.13  |    4.32 \u00b1 0.63   |\n| L2P                              |  60.32 \u00b1 0.56 |    2.30 \u00b1 0.11   |\n| DualPrompt                       |  61.77 \u00b1 0.61 |    2.31 \u00b1 0.23   |\n| CodaPrompt                       |  67.61 \u00b1 0.19 |    2.23 \u00b1 0.29   |\n| OS-Prompt                        |  67.52 \u00b1 0.34 |    2.32 \u00b1 0.13   |\n| OS-Prompt++                      |  67.92 \u00b1 0.42 |    2.19 \u00b1 0.26   |\n\nThe non-PCL methods (ER and LwF) demonstrate similar or even superior performance compared to DualPropmt and L2P. This contrasts with the results obtained using the ImageNet-1k pre-trained model, where PCL methods outperformed non-PCL methods. This observation suggests that the backbone model plays a crucial role in PCL. However, CodaPrompt and our OS-Prompt outperform the other methods by a significant margin. This indicates that our method continues to perform well even without utilizing supervised pertaining. \n\nWe have included the experiments and analysis related to the above observations in the Appendix.\n\nQ2: Lack of clarity in experimental results. In particular, Figure 2 is not very clear. Is the token embedding distance depicted in this Figure related to the current task? What are the distances between token embeddings for tasks 1 to t-1 when task t is learned? \n\nA2: Thanks for the insightful suggestion. In response to the reviewer's feedback, we have restructured Fig. 2 in our manuscript. This figure depicts the measurement of distances between token embeddings of previous tasks when a new task is learned. We conducted experiments on 9 pairs, assessing the distance between token embeddings of task 1 and those after tasks {3, 5, 7, 9}; the distance between token embeddings of tasks 3 and those after tasks {5, 7, 9}; and the distance between token embeddings of tasks 5 and those after tasks {7, 9}. The results reveal that token embeddings from early layers show minimal changes during training. Consequently, utilizing token embeddings from these earlier layers (layers  1~5) provides a stable and consistent representation throughout the continual learning stages."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107061771,
                "cdate": 1700107061771,
                "tmdate": 1700107061771,
                "mdate": 1700107061771,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mQYVIxSIAv",
                "forum": "f3TQxxuquZ",
                "replyto": "PNaIepYu71",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1437/Reviewer_2Phe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1437/Reviewer_2Phe"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for providing additional analysis and clarification. The results demonstrate the efficacy of the proposed method on the main benchmark. On the other hand, as the algorithms show different properties, it seems that a more comprehensive evaluation should be included for all the datasets.  Overall, I would maintain my original rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705905010,
                "cdate": 1700705905010,
                "tmdate": 1700705905010,
                "mdate": 1700705905010,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jr2zK0n7nV",
            "forum": "f3TQxxuquZ",
            "replyto": "f3TQxxuquZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1437/Reviewer_Wfb2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1437/Reviewer_Wfb2"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the OS-Prompt framework, a new one-stage Prompt-Based Continual Learning (PCL) approach for image recognition tasks. By restructuring the traditional two-step feedforward stages, the OS-Prompt aims to optimize computational costs in PCL. Experiments are conducted on datasets such as Split CIFAR-100 and Split ImageNet-R, comparing the efficacy of OS-Prompt with other continual learning methodologies."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tAddressing Computational Inefficiencies: The paper astutely identifies a significant limitation in current PCL methods, notably the high computational cost stemming from the two separate ViT feed-forward stages. The introduction of the OS-Prompt as a remedy is commendable, achieving an impressive reduction in computational cost by nearly 50% without compromising on performance.\n2.\tInnovative QR Loss Introduction: In response to the minor performance decline observed with the one-stage PCL framework, the paper introduces a QR loss. This strategic addition ensures that the prompt pool remains consistent with token embedding from both intermediate and final layers. Importantly, the implementation of the QR loss is efficient, adding no extra computational burden during the inference phase."
                },
                "weaknesses": {
                    "value": "1.\tPotential Scalability Concerns: While the OS-Prompt framework demonstrates efficiency improvements on benchmarks like CIFAR-100 and ImageNet-R, the paper does not provide insights into how this method scales with larger, more complex datasets. This leaves questions about its applicability in broader, real-world scenarios where data variability and volume might be significantly higher.\n2.\tLack of Exploration on QR Loss Limitations: The introduction of the QR loss is innovative, but the paper could benefit from a more in-depth discussion on its potential limitations or scenarios where it might not be as effective. A deeper dive into the trade-offs associated with the QR loss would provide a more balanced view of its utility.\n3.\tComparative Analysis Depth: While the paper highlights the superiority of the OS-Prompt over the CodaPrompt method, it might be beneficial to see how the proposed framework fares against a wider array of contemporary methods. A more extensive comparative analysis would offer readers a comprehensive understanding of where OS-Prompt stands in the broader landscape of PCL techniques."
                },
                "questions": {
                    "value": "Please the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698980501451,
            "cdate": 1698980501451,
            "tmdate": 1699636072016,
            "mdate": 1699636072016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2vN6TuW4Ny",
                "forum": "f3TQxxuquZ",
                "replyto": "Jr2zK0n7nV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your efforts in reviewing our article and providing constructive feedback. We\u2019d like to reply to your concerns in detail.\n\nQ1: Potential Scalability Concerns. The paper does not provide insights into how this method scales with larger, more complex datasets.\n\nA1: We appreciate the insightful suggestion from the reviewer. In response to their input, we conducted experiments on DomainNet [R1], a large-scale domain adaptation dataset consisting of around 0.6 million images distributed across 345 categories. Each domain involves 40,000 images for training and 8,000 images for testing. Our continual domain adaptation task was created using five diverse domains from DomainNet: Clipart \u2192 Real \u2192 Infograph \u2192 Sketch \u2192 Painting. The experimental methodology closely followed the protocol established in prior PCL research. The outcomes of these experiments are presented in the table below.\n\n| Method (Dataset: DomainNet) |    Acc (%)   | Forgetting score |\n|-----------------------------|:------------:|:----------------:|\n| UppderBound                 |     79.65    |         -        |\n| FT                          | 18.00 \u00b1 0.26 |   43.55 \u00b1 0.27   |\n| ER                          | 58.32 \u00b1 0.47 |   26.25 \u00b1 0.24   |\n| L2P                         | 69.58 \u00b1 0.39 |    2.25 \u00b1 0.08   |\n| Deep L2P                    | 70.54 \u00b1 0.51 |    2.05 \u00b1 0.07   |\n| DualPrompt                  | 70.73 \u00b1 0.49 |    2.03 \u00b1 0.22   |\n| CODA                        | 73.24 \u00b1 0.59 |    3.46 \u00b1 0.09   |\n| OS-Prompt                   | 72.24 \u00b1 0.13 |    2.94 \u00b1 0.02   |\n| OS-Prompt++                 | 73.32 \u00b1 0.32 |    2.07 \u00b1 0.06   |\n\nOur method (OS-prompt and OS-propmt++) achieves comparable performance compared to the previous PCL methods with ~50% inference computational cost. These results affirm the practical viability of our approach, indicating its applicability in real-world scenarios. We have added the results in the Appendix.\n\n[R1] Peng et al. Moment matching for multi-source domain adaptation. ICCV19\n\nQ2: Lack of Exploration on QR Loss Limitations. A deeper dive into the trade-offs associated with the QR loss would provide a more balanced view of its utility. \n\nA2: Thank you for your suggestion. During the training of OS-Prompt++, in comparison to OS-Prompt, the inclusion of QR loss does enhance accuracy. However, QR loss requires a reference ViT feedforward step (which is different ViT from the backbone ViT), resulting in additional training computational cost compared to OS-Prompt. It's worth highlighting that while OS-Prompt++ has a high training computational cost compared to OS-Prompt, it remains consistent with the training cost of prior PCL works (refer to Table 3).\n\nThis computational efficiency during training becomes important in the context of addressing the online continual learning problem [R2]. This problem assumes the model needs rapid training on the given streaming data. If the training cost requires a high computational cost, the stream does not wait for the model to complete training before revealing the next data for predictions, leading to performance degradation. Therefore, an analysis of the training efficiency of OS-Prompt++ is required.\n\nTo address this issue, in Fig. 6, we provide the trade-off between FLOPs and accuracy within a reference ViT used for QR Loss. In our OS-Prompt++ default setting, we extract the reference prompt query r from the final layer of the reference ViT. This requires additional training complexity compared to OS-Prompt. To improve efficiency, instead of using the final layer\u2019s [CLS] token embedding for prompt query r, we utilize the intermediate token embeddings from layers deeper than 5 within the reference ViT. The results show there is an accuracy and training efficiency trade-off in our QR Loss design. \n\nMore importantly, as per the reviewer's suggestion, we have added content in the Conclusion section to point out the potential limitations of the proposed method.\n\n[R2] Real-Time Evaluation in Online Continual Learning: A New Hope, Y. Ghunaim et al., CVPR2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107210264,
                "cdate": 1700107210264,
                "tmdate": 1700107210264,
                "mdate": 1700107210264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1anZ6vPXiR",
                "forum": "f3TQxxuquZ",
                "replyto": "wdoH9ChPWU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1437/Reviewer_Wfb2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1437/Reviewer_Wfb2"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your responses"
                    },
                    "comment": {
                        "value": "Thanks for your responses to my questions. Some of my concerns have been addressed. However, although the authors claimed DomainNet is a large-scale dataset, they only used five domains, which can not convince me about with the scalability of the proposed method."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699451472,
                "cdate": 1700699451472,
                "tmdate": 1700699451472,
                "mdate": 1700699451472,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Gk9Qr67QWB",
            "forum": "f3TQxxuquZ",
            "replyto": "f3TQxxuquZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1437/Reviewer_84n7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1437/Reviewer_84n7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a single stage PCL framework that directly using the intermediate layer\u2019s token embedding as a prompt query, so that the first query pre-training ViT could be avoid and half of the computational cost is avoid. The paper describe why using intermedia layer output to construct query embedding, and further propose QR loss to regulate the relationship between the prompt query and the prompt pool, and enhance the representation ability. Experimental result show that this approach could maintain performance under 50% less cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Reducing the computational cost by reducing query ViT seems well and could be regarded as new direction compared with prompt construction or weighting. I think the exploring on intermediate layer output as prompt is reasonable, organization of the paper is also good to follow.\n\n2. The designed QR loss for supplementing the absent of [CLS] tokens in the query is interesting, it could maintain the representation power.\n\n3. The result and discussion in the experimental part is convinced, the figure 4 shows the obvious improvement on consuming. Relative discussion and ablation study, parameter analysis also seems well."
                },
                "weaknesses": {
                    "value": "I think some specific description should be more clear:\n1. Although the training avoid the query ViT process, the author mentioned that QR loss need a reference ViT architecture. This process also need to forward the input, so for reducing the two-step forward, what's the difference between Query function forward and reference forward? Why the (training) computational cost still reduce 50% when QR loss with reference forward existing?\n2. The author mention that this approach focuses on improving efficiency through a new query selection. But from the description, seems that how to select query that different from previous PCL is not clear, both of them apply the [CLS] token embedding (no requirement of query ViT in this process is the major difference)."
                },
                "questions": {
                    "value": "Is QR without query ViT could perform better generalization on unseen task/domain?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1437/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1437/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1437/Reviewer_84n7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699418799623,
            "cdate": 1699418799623,
            "tmdate": 1699636071956,
            "mdate": 1699636071956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1gwPiOpchM",
                "forum": "f3TQxxuquZ",
                "replyto": "Gk9Qr67QWB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We appreciate your positive feedback on our work. Please check our response to your questions and concerns.\n\nQ1: What is the difference between Query function forward and reference forward? Why the (training) computational cost still reduce 50% when QR loss with reference forward existing? \n\nA1: Sorry for the confusion. We would like to clarify QR loss is only used in OS-prompt++, which is an advanced version of our OS-Prompt. The OS-Prompt does not employ QR loss and is more efficient than existing two-stage methods during both training and inference.  The users can choose to use OS-prompt if they have fewer computing resources in training, but we highlight that OS-prompt can achieve comparable performance with existing SOTA. We summarize the difference between OS-Prompt and OS-Prompt++ in our general response.\n\nThe Query function (used by previous PCL methods) is leveraged in training and inference, but the Reference function (proposed in this work) is only used in training, not inference. Therefore, with QR loss (i.e. OS-prompt++), the training computational cost does not reduce by 50%. We report the relative computational cost of both training and inference in Table 3. Please refer to the training cost for OS-prompt++ in Table 3.\n\nQ2: From the description, seems that how to select query that different from previous PCL is not clear, both of them apply the [CLS] token embedding.\n\nA2: We apologize for the unclear description. \nTo obtain a query in layer l, prior methods utilize the [CLS] token from the 'last layer' of the Query ViT (Fig. 1, Left). In contrast, our approach involves using the [CLS] token from the  'layer l,' as depicted in Fig. 3 (arrow \u2461). \nConsequently, traditional PCL methods necessitate two feedforward steps: one for obtaining the query prompt and the second for the backbone ViT. In our OS-Prompt, we directly leverage the intermediate [CLS] token feature, which requires only one feedforward step. \nThe difference in query selection strategy is also elaborated in Eq.(1) and Eq.(6) in our manuscript.\n\n\nQ3: Is QR without query ViT could perform better generalization on unseen task/domain?\n\nA3: We appreciate the insightful suggestion from the reviewer. This is an interesting and important concern because we use the intermediate layer\u2019s [CLS] feature as a query, which might have less generalization power compared to the [CLS] feature from the last layer of query ViT.\n\nTo this end, we conducted experiments on DomainNet [R1], a large-scale domain adaptation dataset consisting of around 0.6 million images distributed across 345 categories. Our continual domain adaptation task was created using five diverse domains from DomainNet: Clipart \u2192 Real \u2192 Infograph \u2192 Sketch \u2192 Painting. Thus every task has a different domain image, which is a proper experiment to check the generalization ability of our method. The experimental results are presented in the table below.\n\n| Method (Dataset: DomainNet) |    Acc (%)   | Forgetting score |\n|-----------------------------|:------------:|:----------------:|\n| UppderBound                 |     79.65    |         -        |\n| FT                          | 18.00 \u00b1 0.26 |   43.55 \u00b1 0.27   |\n| ER                          | 58.32 \u00b1 0.47 |   26.25 \u00b1 0.24   |\n| L2P                         | 69.58 \u00b1 0.39 |    2.25 \u00b1 0.08   |\n| Deep L2P                    | 70.54 \u00b1 0.51 |    2.05 \u00b1 0.07   |\n| DualPrompt                  | 70.73 \u00b1 0.49 |    2.03 \u00b1 0.22   |\n| CODA                        | 73.24 \u00b1 0.59 |    3.46 \u00b1 0.09   |\n| OS-Prompt                   | 72.24 \u00b1 0.13 |    2.94 \u00b1 0.02   |\n| OS-Prompt++                 | 73.32 \u00b1 0.32 |    2.07 \u00b1 0.06   |\n\nOur method (OS-prompt and OS-propmt++) achieves comparable performance compared to the previous PCL methods with ~50% inference computational cost. These findings suggest the adaptability of our method across various domains in different stages of continual learning. \n\nWe have added the results in the Appendix.\n\n[R1] Peng et al. Moment matching for multi-source domain adaptation. ICCV19"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107124438,
                "cdate": 1700107124438,
                "tmdate": 1700107124438,
                "mdate": 1700107124438,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IQbGirdhla",
            "forum": "f3TQxxuquZ",
            "replyto": "f3TQxxuquZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1437/Reviewer_PuDx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1437/Reviewer_PuDx"
            ],
            "content": {
                "summary": {
                    "value": "The submission proposes a method for one-stage prompt-based continual learning (PCL) where a separate forward pass of a ViT is not  required to extract the query tokens from the given image. Instead, they utilize intermediate [CLS] tokens from the previous layer as query tokens such that a single forward pass of the ViT is sufficient for PCL. To improve performance, they use a reference ViT during training to generate reference query tokens and use a query pool regularization loss to match the intermediate [CLS] tokens to the reference query tokens. Evaluation is done on CIFAR10 and ImageNet-R datasets, which show superiority of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem of having to do 2 forward passes of a ViT in PCL is an important task to tackle.\n- The proposed method is not specific to a model architecture, which makes it widely applicable.\n- The idea itself is well presented and easy to follow.\n- The empirical evaluation shows strong performance of the proposed method."
                },
                "weaknesses": {
                    "value": "- The query pool regularization introduces extra computation in the training phase, especially as more advanced reference ViTs are used. I do not think this is particularly fair in comparison to prior work as they used less resources during training. Specifically, as the idea of continual learning depends much more heavily on the computation cost of training and inference due to the training phase being run continuously, I do not think matching the computation cost during the inference phase only is fully fair. Recent works such as [A] tackle this specific part of the resource constraints in continual learning and I believe the proposed method needs to discuss on the computation cost of the training phase more thoroughly.\n\n[A] Real-Time Evaluation in Online Continual Learning: A New Hope, Y. Ghunaim et al., CVPR2023"
                },
                "questions": {
                    "value": "- Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1437/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699495266835,
            "cdate": 1699495266835,
            "tmdate": 1699636071867,
            "mdate": 1699636071867,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V5fpcmxNwH",
                "forum": "f3TQxxuquZ",
                "replyto": "IQbGirdhla",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1437/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1437/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We appreciate your constructive feedback on our work. Please check our response to your questions and concerns.\n\nQ1: The proposed method needs to discuss the computation cost of the training phase more thoroughly. \n\nA1: \nThank you for the valuable feedback. We want to clarify that our OS-Prompt++ (with query pool regularization) does not result in increased training costs compared to previous Prompt Continual Learning (PCL) methods, such as CodaPrompt, DualPrompt, and L2P (refer to Table 3). Instead, OS-Prompt++ maintains the same training cost as the earlier approaches. On the other hand, our efficient version of PCL, named OS-prompt, minimizes the forward computation of the PCL framework, improving computational efficiency for both training and inference.\n\nIn our work, we aim to solve 'Rehearsal-Free' continual learning, aiming to address both privacy and memory efficiency concerns, aligning with prior PCL research. Although our problem setting differs from online continual learning like [A], we agree with the reviewer that our proposed method needs a more thorough discussion of the training computation cost to provide a better understanding of the proposed approach. Following Table 1 provided in [A], we compute the relative training complexity with respect to ER or Experience Replay, which is the simple baseline with standard gradient-based training. \n\n\n| CL strategy       | Method             | Relative training complexity w.r.t ER |\n|-------------------|--------------------|---------------------------------------|\n| Experience Replay | ER                 | 1                                     |\n| Regularization    | LwF                | 4/3                                   |\n| Prompt-based      | L2P                | 1                                     |\n| Prompt-based      | DualPrompt         | 1                                     |\n| Prompt-based      | CodaPrompt         | 1                                     |\n| Prompt-based      | OS-Prompt (Ours)   | 2/3                                   |\n| Prompt-based      | OS-Prompt++ (Ours) | 1                                     |\n\nHere, we would like to clarify the training computational cost of prompt learning. In general neural network training,  the forward-backward computational cost ratio is approximately 1:2. This is due to gradient backpropagation ($\\frac{dL}{da_l}=W_{l+1}\\frac{dL}{da_{l+1}}$) and weight-updating ($\\frac{dL}{dW_l}=\\frac{dL}{da_l+1}a_l$), where $L$ represents the loss, $W$ denotes the weight parameter, $a$ is the activation, and $l$ is the layer index.  In prompt learning, the forward-backward computational cost ratio is approximately 1:1. This is in contrast to general neural network training, as only a small fraction of the weight parameters (less than 1\\%) are updated.\n\n\nBearing this in mind, we present the observations derived from the table, assuming that all methods employ the same architecture.\n* Previous PCL (L2P, DualPrompt, CodaPrompt) consists of two steps; (1) the query ViT requires only feedforward without backpropagation; (2) the backbone ViT feedforward-backward training with prompt tuning. This results in the relative training computational cost is 1 ($= \\frac{Query ViT forward (1) + Backbone ViT forward (1) + Backbone ViT backward (1)}{ER forward (1) + ER backward (2)}$)\n* Similarly, our OS-Prompt++ also consists of two steps; (1) the reference ViT with only a feedforward step to calculate QR loss; (2) the backbone ViT feedforward-backward training with prompt tuning. This results in a relative training computational cost 1 ($= \\frac{Reference ViT forward (1) + Backbone ViT forward (1) + Backbone ViT backward (1)}{ER forward (1) + ER backward (2)}$) with respect to ER. \n* On the other hand, our OS-Prompt requires one feedforward-backward step with prompt tuning. Therefore, relative training computaional cost becomes $\\frac{2}{3}$ ($= \\frac{Backbone ViT forward (1) + Backbone ViT backward (1)}{ER forward (1) + ER backward (2)}$). This shows the potential of our OS-Prompt on online continual learning.\n\nOverall, we propose two versions of the one-stage PCL method (i.e., OS-prompt and OS-prompt++), and these two options enable the users to select a suitable method depending on the problem setting. For example, for online continual learning, OS-prompt is a better option since it requires less training cost. On the other hand, for offline continual learning, one can use OS-prompt++ to maximize performance while spending more training energy. \n\nWe have added this discussion in the Appendix. \n\n[A] Real-Time Evaluation in Online Continual Learning: A New Hope, Y. Ghunaim et al., CVPR2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107174169,
                "cdate": 1700107174169,
                "tmdate": 1700107174169,
                "mdate": 1700107174169,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9xSw0Uy9Jz",
                "forum": "f3TQxxuquZ",
                "replyto": "V5fpcmxNwH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1437/Reviewer_PuDx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1437/Reviewer_PuDx"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "The reviewer appreciates the detailed response. However, the reference ViT used in OS-Prompt++ seems to consist of larger models, and authors even indicate that using larger models is beneficial. Thus, the presented argument about training costs do not fully convince me."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1437/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698691832,
                "cdate": 1700698691832,
                "tmdate": 1700698691832,
                "mdate": 1700698691832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]