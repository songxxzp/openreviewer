[
    {
        "title": "Referring Expression Matters: Multi-referring Feature Aggregation for Referring Video Object Segmentation"
    },
    {
        "review": {
            "id": "AqmuCUlqxs",
            "forum": "eaXMEb6fa4",
            "replyto": "eaXMEb6fa4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8803/Reviewer_QT6n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8803/Reviewer_QT6n"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses the task of Referring Video Object Segmentation and introduces to integrate multiple referring expressions to boost performance. A neural expression generation module is proposed to create complementary features from these expressions, which not only improves object identification accuracy but also accelerates training convergence. Experimental results on popular RVOS datasets are presented."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1)\tThe paper explores the effect of multiple referring expressions for RVOS, which is interesting.\n\n(2)\tThis paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "(1) Although the authors present an interesting motivation, suggesting that adjusting referring expressions could enhance segmentation performance, the method proposed does not fully align with this motivation. The reviewer, after going through the introduction, expect to find how the unclear parts within referring expressions are identified and improved. However, the authors merely concatenate multiple referring expressions.\n\n(2) The paper's contribution mainly involves adding an MLP to ReferFormer to merge multiple referring expressions. However, this incremental addition lacks further in-depth consideration, i.e., what kind of scenarios need multiple inputs, how the extent of overlap and divergence between referring expressions affects final performance. Consequently, the contribution of the paper is limited.\n\n(3) The experimental comparisons are unfair. While the proposed method uses multiple referring expressions as input, the compared methods utilize only one expression. To truly demonstrate the impact of the integration of MRE, a more comprehensive comparison should involve merging results from different expressions in other methods. This would effectively showcase the performance gains derived from exploring relationships within referring expressions."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8803/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8803/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8803/Reviewer_QT6n"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8803/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698639673962,
            "cdate": 1698639673962,
            "tmdate": 1699637107429,
            "mdate": 1699637107429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "hj7cJTnSn2",
            "forum": "eaXMEb6fa4",
            "replyto": "eaXMEb6fa4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8803/Reviewer_SAH4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8803/Reviewer_SAH4"
            ],
            "content": {
                "summary": {
                    "value": "A Referring Video Object Segmentation method is proposed. However, the motivation is not clear. The details of most of the methods are not explained."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The picture of the model architecture of the proposed method is clear."
                },
                "weaknesses": {
                    "value": "The writing is too bad. The details of the Multi-modal Fusion are not explained clearly. No referenced paper is mentioned in Deformable Transformer and Instance Sequence Segmentation. It's quite hard to understand the paper."
                },
                "questions": {
                    "value": "How to do Multi-modal Fusion? What's the structure of the Deformable Transformer? What is Cross-Modal Feature Pyramid Network (CM-FPN) ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8803/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647725649,
            "cdate": 1698647725649,
            "tmdate": 1699637107301,
            "mdate": 1699637107301,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "3Cdkq3pvHW",
            "forum": "eaXMEb6fa4",
            "replyto": "eaXMEb6fa4",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8803/Reviewer_s2MZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8803/Reviewer_s2MZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a referring video object segmentation method via a multi-referring feature aggregation mechanism. This mechanism can effectively obtain complementary features with less redundancy, which is not only helpful in identifying the referred object, but also speeds up the training convergence. Experimental results show the effectiveness and superiority of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The multiple referring expressions can generate a complete and concise linguistic feature, experimental results also show the effectiveness of the proposed strategy.\n+ The proposed method can achieve better training convergence. \n+ The proposed method achieves the new SOTA and outperforms the second-best by a large margin"
                },
                "weaknesses": {
                    "value": "- The novelty of the proposed method is somewhat limited. The main contribution is the neural expression generation via multiple-referring expressions. It seems that this aggregation strategy is simple and lacks insights. \n- The authors declare that they proposed different sampling strategies in cross-modal attention for pre-training and fine-tuning to boost the model performance. However, the illustration of this sampling strategy is unclear, and the differences with existing sampling strategies are also unclear. Also, there are no experimental results to support this assertion. \n- In Eq.3, the authors used the concat operation but in Table 2(c), the proposed NEG is different MRE Cocat, so the reason is unclear.\n- The authors do not show the training convergence in the pre-training strategy, So, it is hard to assert the proposed method achieves faster convergence only by verifying it in the fine-tuning stage. \n - I think the comparison is somewhat unfair. The batch size is different. It mainly influences the training convergence and even the performance."
                },
                "questions": {
                    "value": "Please seeing the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8803/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8803/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8803/Reviewer_s2MZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8803/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842439716,
            "cdate": 1698842439716,
            "tmdate": 1699637107161,
            "mdate": 1699637107161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]