[
    {
        "title": "Enhancing Kernel Flexibility via Learning Asymmetric Locally-Adaptive Kernels"
    },
    {
        "review": {
            "id": "pxlukMmsOr",
            "forum": "D6aGz0Zyvn",
            "replyto": "D6aGz0Zyvn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ"
            ],
            "content": {
                "summary": {
                    "value": "The algorithm introduces a variant of the centered-based kernel method, they call the centers \"support vectors\". The idea is that the model's size remains smaller than the entire dataset, similar to  FALKON and EigenPro3.0. In contrast to methods that utilize fixed model support vectors, their algorithm adaptively adjusts these support vectors throughout the training process. Moreover, utilizing concepts from the asymmetric kernel method, they adaptively fit the support vectors with varying bandwidths throughout the training process.They show that their algorithm outperform some existing methods on several data sets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. the most interesting part is the idea of adaptively change the support vectors in an iterative manner. This was something novel worth exploring more.\n2. The idea of adaptively adjust the bandwidth and mixing it with asymmetric kernel methods seems intriguing.(Not sure how useful)\n3. The paper is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "Main concern:\n1. The most important caveats is that the paper has only compared to vanilla KRR methods. It is not surprising that they got a slightly better performance compare for example to FALKON. I'm not at all convened this methods is better than well developed techniques such as:\n\ni. traditional Automatic Relevance Determination(ARD) known in GP community, it is implemented with Gpytorch see here:https://docs.gpytorch.ai/en/stable/kernels.html. or see section 5 of this https://gaussianprocess.org/gpml/chapters/RW.pdf\n\nii. EigenPro3.0, see https://arxiv.org/abs/2302.02605 \n\niii. Recursive Feature machines (RFMs), see: https://arxiv.org/abs/2212.13881\n\nScalability:\n\n2. the authors claim that this method is scalable and they provide table 3 to justify this. But those data sets are not at all large scale. The inverse problem can be done using direct calculation for those cases. The authors should try other data sets such as Taxi, CIFAR5m to justify consistency and scalability. (both in data and model size)\n3. It is mentioned in section 3 that the computation complexity is O(N_sv^3). This fundamentally shows this method on its own is not scalable. Eventually you need to scale the required support vectors(or model size) as it is discussed in https://arxiv.org/abs/2302.02605. \nHowever, I can see that this method combined by other methods like FALKON or EigenPro3.0  can potentially be scalable.\n4. How do you compute line 4 of the algorithm? Did you use FALKON or some other off the shelf algorithm or you did direct inverse? \n\n\nMinor issues:\n\n1. RBF kernel are known to be sensitive to bandwidth. While you have results for MKL, the performance of your method specifically for the Laplace kernel, which is relatively insensitive to bandwidth, remains ambiguous. Does outperforming MKL indicate superiority over merely using Laplace? The same concern applies to NTK kernels or other popular kernels.\n2. I suggest more explaining for asymmetric kernels methods. For example why the inverse even exist in equation 6. or you claimed \"this paper for the first time establishes an asymmetric KRR framework\", but how is it different from He et. al. paper? not clear.\n3. Please add what M means in the tables, helps with reading."
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2653/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ",
                        "ICLR.cc/2024/Conference/Submission2653/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2653/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701875200,
            "cdate": 1698701875200,
            "tmdate": 1700463037990,
            "mdate": 1700463037990,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uTS243MUX2",
                "forum": "D6aGz0Zyvn",
                "replyto": "pxlukMmsOr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PWHQ (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable suggestions and recognition of our dynamic strategy and locally adaptive bandwidths. \n\n## Scalability is not Our Contribution\n \nFirst of all, we wish to emphasize our central focus, as indicated by the title. Our paper aims to explore **the application of asymmetric kernel learning in regression tasks** and assess its advantages over symmetric kernels, rather than concentrating on large datasets or large kernel machines.\n\nOur main contributions lie in (i) introducing the first asymmetric KRR model and (ii) presenting a novel asymmetric kernel learning algorithm. These two techniques collectively offer **a fresh approach to learn flexible asymmetric kernels in regression tasks**. Current investigations into asymmetric kernels continue to face **a gap in terms of interpretable regression models and efficient kernel learning algorithms**, adding to the novelty of this work.\n\nWe  would like to clarify what we mean by scalability in the context of this paper. The introduction of locally-adaptive bandwidths largely enhance the flexibility of RBF kernels. This allows us to **significantly reduce the need for support data, and the decrease in support data consequently benefits the scalability of the model.** This is also what we want to emphasis in Table 3, with the reduction being more evident in the larger datasets. \n\nConsider that we can reduce the requirement for support data. When coupled with existing acceleration techniques, as you suggested, it becomes feasible for us to effectively manage millions of data points. The feasibility, however, hinges on the computing infrastructure and the number of support vectors, aspects beyond the scope of this paper.\n\nIt's noteworthy that a majority of current solutions for large kernel machines are founded on symmetric kernels. We believe extending their approach to accommodate asymmetric kernels would be particularly intriguing.\n\nNevertheless, we value your comments and will carefully revise the paper to eliminate any potential ambiguity."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169822892,
                "cdate": 1700169822892,
                "tmdate": 1700169822892,
                "mdate": 1700169822892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TZqdl42wl7",
                "forum": "D6aGz0Zyvn",
                "replyto": "pxlukMmsOr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PWHQ (Part 2/2)"
                    },
                    "comment": {
                        "value": "## Accuracy Comparison in Regression with EigenPro3.0 and RFMs\n\nIn response to your concerns regarding accuracy improvement, as acknowledged by the first reviewer, our algorithms **showcase state-of-the-art accuracy in regression tasks.** Notably, our results surpass even the performance of ResNet, a benchmark for high-level regression methods in small and medium-sized datasets.\n\nWe sincerely appreciate the advanced kernel methods you recommended. To address your main concern, we have conducted an additional comparative experiment with EigenPro 3.0 and Recursive Feature Machines (RFMs).\n\n**Table1: Performance of EigenPro3.0 and LAB RBF kernels on real datasets.**\n|               Dataset               |   Airfoil (N=1503)  |  Parkinson (N=5875) | Electrical (N=10000)  | KC House (N=21623) | TomsHardware (N=28179) |\n|:-----------------------------------:|:----------:|:----------:|:----------:|:--------:|:------------:|\n| Eigenpro3.0-Laplace  ($R^2$) | 0.9201 | 0.9251 | 0.9593          | 0.8636 |    0.9436    |\n|      Eigenpro3.0-#Centers       |     1202    |     4700    |     10000    |    20000   |      20000     |\n| RFMs  ($R^2$) | 0.9394 | 0.9988 |  0.9582       | 0.9008 |   0.9115    |\n|         LAB RBF kernels ($R^2$)        |   0.9608   |   0.9950   |   0.9642   |  0.8917  |    0.9809    |\n|         LAB RBF kernels (#S.V.)        |     200    |     110    |     300    |    400   |      500     |\n\n\nIn our experimental setup, we train EigenPro3.0 for 50 iterations, while RFM for 5 iterations, aligning with the parameter setting outlined in their papers. As shown in the public code, RFMs use all training data points to compute the final decision function.  That is, they **use all training data as the support data.**\n\nBoth EigenPro3.0 and RFM exhibit excellent capabilities in managing large-size kernel models; however, their dependence on  symmetric kernel formulations poses a high requirement on large number of support data. While by utilizing locally adaptive bandwidth, **our algorithm can significantly reduce the requirement of support data\u2014essentially, the model complexity\u2014while maintaining a high level of accuracy.**\n\nMoreover, we've observed that in RFMs, they learn a matrix $M$ in a generalization of the Laplace kernel $\\mathcal{K}(x_i, x_j) = \\exp(-\\gamma\\||x_i - x_j\\||_M)$, where $\\||x_i - x_j\\||_M = (x_i - x_j)^\\top M(x_i - x_j)$. That is, the matrix $M$ is designed to **capture relationships among features**.\n\nIn contrast, our proposed LAB RBF kernels $\\mathcal{K}(x_i, x_j) = \\exp(-\\||\\Theta_j\\odot(x_i - x_j)\\||_2^2)$  and corresponding learning algorithm aim at **learning locally-adaptive bandwidths for each support data**. Exploring a completely different mechanism, combining these two methods in the future could prove to be intriguing\n\nAgain, we appreciate your recommendation of these advanced methods,  and we are committed to incorporating this comparative experiment into the revised version of our work.\n\n## Minor issues\n1. The computation at line 4 of the algorithm, specifically evaluating $$f_{\\mathcal{Z},\\Theta}(t) = K_{\\Theta} (t,X_{sv})(K_\\Theta(X_{sv},X_{sv})+\\lambda I_N)^{-1}Y_{sv}$$\nis not  time-consuming, especially when the number of support data is limited. In our experiments, we directly compute this inverse due to the manageable size of the support data. We agree that existing scalability techniques, while primarily designed for symmetric kernels, can be applied here to further speed-up the computation, opening new possibilities.\n\n2. Distinguish from Prior Works: While previous works [[1](https://www.sciencedirect.com/science/article/pii/S1063520315001360)][[2](https://ieeexplore.ieee.org/document/10070836)] and our study use the same definition of asymmetric kernels, $\\mathcal{K}(x,t) = \\langle \\phi(x), \\psi(t)\\rangle$, our paper introduces **the first formulation of asymmetric kernel ridge regression**.  In contrast to previous studies focusing on PCA and least square SVM, this results in distinct stationary solutions and applications.\nFurthermore, prior works employ manually designed asymmetric kernels such as the Kullback-Leibler Kernel and SNE Kernels (refer to [2] for details). While these kernels exhibit good performance on asymmetric data (e.g., directed graphs), their advantage on general data is not apparent. To address this limitation, our paper proposes a novel asymmetric kernel learning algorithm, demonstrating **superior performance on general data**.\n\n3. In Eq. (6), since $(K(X,X)+\\lambda I)$ is a square matrix, its inverse exists as long as it is full-rank.\n---\nWe trust that our explanation, coupled with the additional experiment, sufficiently addresses your concerns regarding both accuracy and scalability in our experiments, and we would greatly appreciate it if you could consider raising the score accordingly. We are very willing to further discuss with you on equipping current techniques to our LAB RBF kernels or any other topics that might interest you."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700171074653,
                "cdate": 1700171074653,
                "tmdate": 1700171074653,
                "mdate": 1700171074653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BIpjTB9qZH",
                "forum": "D6aGz0Zyvn",
                "replyto": "TZqdl42wl7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ"
                ],
                "content": {
                    "comment": {
                        "value": "The performance metrics you've presented are intriguing and suggest that your algorithm is competitive with other state-of-the-art (SOTA) techniques. The potential synergy of combining these methods to enhance performance is particularly noteworthy.\n\nThis new information has significantly bolstered my confidence in your approach; previously, I had reservations about the practical utility of your algorithm. However, please make sure that,\n\n1. These new results are comprehensively included in the final version of your paper. \n\n2. While ARD was not tested, given its typically lesser performance compared to RFM, your results likely remain superior. Nonetheless, mentioning ARD could add value to the discussion.\n\n3. It is crucial to explicitly clarify in Table 3 that your contribution is not about scalability issues.\n\nI trust these aspects will be addressed in the final version. Considering these new results, I believe your work is of significant value to the ICLR community. I am inclined to increase my score for your submission accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426090935,
                "cdate": 1700426090935,
                "tmdate": 1700426090935,
                "mdate": 1700426090935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SciW9pisVS",
                "forum": "D6aGz0Zyvn",
                "replyto": "pxlukMmsOr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Notification of revision."
                    },
                    "comment": {
                        "value": "Dear reviewer PWHQ,\n\nWe sincerely appreciate your understanding and recognition of our work. We are delighted that these results have boosted your confidence in our approach.\n\nThe latest version of our paper is now accessible in the OpenReview system, and we hope that this updated version meets your standards. For your convenience, here we've outlined the key modifications:\n\n-   We have removed our ambiguous statement regarding scalability. Instead, in the revised version, we emphasize that learning flexible kernels can reduce the need for support data. This reduction in support data, in turn, significantly reduces model complexity, making it more efficient to deal with large-scale datasets. \nPlease refer to the contributions in the end of introduction on page 4, the statements at the beginning of Section 4 on page 7, and the discussion about experimental results on larger datasets in Table 3 on page 8.\n-   EigenPro3.0 and RFMs have been introduced as new comparative methods. Please refer to Table 1 on page 7, and Table 3 on page 8 for details.\n-   A discussion on incorporating current accelerating methods for kernel machines into our methods has been added. Please refer to the line 15 on page 6.\n-   The definitions of N and M have been incorporated. Please refer to Table 1 on page 7.\n-   Additional discussions on current studies of asymmetric kernels in related works. Besides, ARD and RFMs are also included in relted works. Please refer to Section 5 on page 9.\n\nOnce again, thank you for your recognition and insights. In the following days, we will continue to revise our paper according to your suggestions and those provided by other reviewers."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453727234,
                "cdate": 1700453727234,
                "tmdate": 1700453727234,
                "mdate": 1700453727234,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SlJ1Fn5mj1",
                "forum": "D6aGz0Zyvn",
                "replyto": "SciW9pisVS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Reviewer_PWHQ"
                ],
                "content": {
                    "comment": {
                        "value": "The updated version of your paper seems sophisticated. However, I would like to point out a potential inaccuracy on page 8, under the section \"Performance on large-scale datasets.\" The statement, \"... they utilize almost all training data as support data,\" does not accurately represent the methodologies of FALKON and EigenPro3.0. These methods are specifically designed to avoid using the entire dataset as centers. Your approach, on the other hand, introduces an innovative iterative method for selecting centers, which effectively reduces the number of required support-vectors/centers. This distinction is crucial and should be clarified in your paper.\n\nAside from this, I found your paper to be quite interesting, especially in the context of efficiently choosing centers. Consequently, I have increased my score for your paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462982585,
                "cdate": 1700462982585,
                "tmdate": 1700462982585,
                "mdate": 1700462982585,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CBGjaQ8OJi",
            "forum": "D6aGz0Zyvn",
            "replyto": "D6aGz0Zyvn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2653/Reviewer_aToc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2653/Reviewer_aToc"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new asymmetric kernel names Local-Adaptive-Bandwidth RBF kernel. To solve the asymmetry of the kernel, the paper establishes an asymmetric KRR framework. To learn the kernel parameter efficiently and accelerate computation. the paper devises a kernel learning algorithm. Experimental results show the algorithm\u2019s superiority."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper demonstrates a clear logical structure with a comprehensive framework. It tackles the complex relationship between bandwidth and data from the perspective of experimental results.\n2. The paper takes into account the impact of differences in implicit mappings on the results and proposes an interesting approach to non-symmetric kernel KRR framework.\n3. The paper introduces an algorithm based on dynamic strategies for parameter computation, which can effectively reduce the computational complexity associated with high-dimensional kernel matrices."
                },
                "weaknesses": {
                    "value": "1. Intuitively, the relation between the mapping function's distinctiveness and the loss function, which means the coefficient of the last term in the KRR optimization objective may vary with datasets. \n2. The initial data selection for support data in the kernel learning algorithm proposed in the article seems to be too random. Moreover, inappropriate data selection appears to have a significant impact on the model."
                },
                "questions": {
                    "value": "1. Is the final coefficient in the asymmetric KRR framework proposed in the article required to be 1/2? Can this be understood as simply for the convenience of computing stationary points? \n2. Is the small number of support vectors in the experimental results of the proposed method due to the algorithm's termination condition?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2653/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2653/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2653/Reviewer_aToc"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2653/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828024453,
            "cdate": 1698828024453,
            "tmdate": 1699636205488,
            "mdate": 1699636205488,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IwgAjaDlC9",
                "forum": "D6aGz0Zyvn",
                "replyto": "CBGjaQ8OJi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aToc (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your attentive reading and valuable suggestions.  \n\n## Coefficient of the Error Terms in Asymmetric KRR Model\nThanks for your recognization on the novelty of our asymmetric kernel ridge regression model. We are glad that you are interested in this model. It is one of our major technical contributions to deal with asymmetric kernels on regression tasks and also provids theoretical support for subsequent asymmetric kernel learning algorithm.\n\nTo address you concern on the coefficient of the error terms, it would be better to start at the LS-SVM form of our model, i.e. the model in Eq. (7).\n\nThe objective function in Eq. (7) primarily incorporates a **regularization term** $w^\\top v$ and a **error term** $\\sum_{i=1}^N e_i r_i$. As most of machine learning models, these two terms are adjusted by **a trade-off hyper-parameter $\\lambda$ varing with different datasets**.\n\nBy substituting constraints in Eq. (7) to the objective function, we obtian the model in Eq. (4),  whose corresponding objective function is represented as $$\\lambda w^\\top v + \\frac{1}{2}(\\phi^\\top(X)w-Y)^\\top(\\psi^\\top(X)v-Y). $$To enhance clarity, we delved deeper into the interpretation of $(\\phi^\\top(X)w-Y)^\\top(\\psi^\\top(X)v-Y)$, outlining it as two loss terms and one mapping function\u2019s distinctiveness term. That is,\n$$2(\\phi^\\top(X)w-Y)^\\top(\\psi^\\top(X)v-Y) = \\||\\psi^\\top(X)v-Y\\||_2^2 + \\||\\phi^\\top(X)w-Y\\||_2^2 - \\||\\psi^\\top(X) v - \\phi^\\top(X) w\\||_2^2.$$\nTherefore, **the three terms** $\\||\\psi^\\top(X) v - \\phi^\\top(X) w\\||_2^2$, $\\||\\psi^\\top(X)v-Y\\||_2^2$, and $\\||\\phi^\\top(X)w-Y\\||_2^2$ **are not independent; instead, they are integrated as a whole** and thus do not require a extra hyper-parameter tuning.\n\nThank you for this insightful question. We will make it clearer in the revised version.\n\n\n## Impact of the Initial Support Data Selection \nFirstly, you are correct in pointing out the significant influence of support data selection on the algorithm. In light of this, **we have introduced a dynamic strategy aimed at mitigating the impact of initial support data selection.**\n\nTo address your concern, we explore three different approaches to initial data selection: two rational methods (Y-based and X-based) and one irrational method (Extreme Y).\n -   Y-based (utilized in the manuscript): data is sorted based on their labels, and support data is uniformly selected.\n-   X-based: k-means is applied to the training data to identify cluster centers, followed by the selection of data points closest to these centers.\n-   Extreme Y: data is sorted based on their labels, and those with the largest Y values are selected.\n\n**Table1: Performance of Alg.1 with different selection of initial support data.**\n|Dataset| Selection Approach |  Mean of $R^2$   |   Std of $R^2$   |\n|:------------------:|:----------------:|:-------:|:------:|\nYacht|     Extreme Y    | 0.0012 | 0.4805 |\nYacht|     Y-based    | 0.9957 | 0.0025 |\nYacht|    X-based    | 0.9953 | 0.0032 |\nParkinson|     Extreme Y    |   0.8115 | 0.0126 |\nParkinson|     Y-based    |   0.9921 | 0.0015 |\nParkinson|    X-based    |   0.9928  |          0.0016|\n\nThe results indicate that the poor selection method does have a detrimental impact on our performance, particularly evident in the case of Yacht where we struggle to fit the data. In contrast, the other two sensible methods demonstrate good and comparable performance.\n\nIn order to further improve, we introduce a dynamic strategy at the end of Section 3. In this strategy, we dynamically incorporate hard samples into the support dataset. We then integrate these approaches with the proposed dynamic strategy to evaluate its effectiveness.\n\n**Table 2: Performance of Alg.1 with dynamic strategy**\n|Dataset| Selection Approach | Mean of $R^2$   |   Std of $R^2$   |\n|:------------------:|:----------------:|:-------:|:-------:|\nYacht|   Extreme Y     | 0.9961 | 0.0126 |\nYacht|     Y-based    | 0.9982 | 0.0015 |\nYacht|    X-based    |  0.9981 | 0.0016 |\nParkinson|   Extreme Y     | 0.9712 | 0.0049  |\nParkinson|     Y-based    | 0.9972 | 0.0007  |\nParkinson|    X-based    |  0.9966 | 0.0013  |\n\nBased on these results, it is evident that the proposed dynamic strategy has a significantly positive impact on performance. It not only enhances accuracy but also reduces variance, resulting in more stable solutions. Even with the bad selection selection, the final performance is improved to a satisfactory level."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169460432,
                "cdate": 1700169460432,
                "tmdate": 1700169460432,
                "mdate": 1700169460432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KXVN9scJK3",
                "forum": "D6aGz0Zyvn",
                "replyto": "CBGjaQ8OJi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aToc (Part 2/2)"
                    },
                    "comment": {
                        "value": "## Why is the Number of Support Data is so Small?\n\nIn fact, the limitation in support data is not attributed to the termination condition. Instead, the constrained support data stems from **the remarkable flexibility of LAB RBF kernels.** This flexibility empowers our algorithm to represent intricate data patterns with a reduced number of support data.\n\nIn determining the termination condition, we prefer achieving a modest number of support data, given its advantages in computational efficiency and memory conservation, all while upholding accuracy. \n\nTraditional symmetric kernels, lacking such flexibility, struggle to achieve the same efficiency as they apply a uniform kernel formula to all data points.\n  \nThe example in Figure 1 in the manuscript precisely illustrates this point. Attempting to fit unevenly distributed data with a uniform bandwidth necessitates more support data to approximate local distributions, as depicted in (b) and (c). By leveraging the high flexibility of LAB RBF kernels, as demonstrated in (d), our algorithm perfectly tackles this challenge, resulting in advantages in both accuracy and model complexity.\n\n---\nWe hope that our explanations and the additional experimental results adequately address your concerns regarding support data selection and the formulation of the asymmetric KRR model. We would greatly appreciate it if you could consider revising the score accordingly. Moreover, we are open to further discussions regarding the design of the asymmetric KRR model or any other topics that might interest you."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169549804,
                "cdate": 1700169549804,
                "tmdate": 1700169549804,
                "mdate": 1700169549804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vin1883BCr",
                "forum": "D6aGz0Zyvn",
                "replyto": "CBGjaQ8OJi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Notification of revision."
                    },
                    "comment": {
                        "value": "Dear reviewer aToc,\n\nJust now, we have uploaded the revised papar in OpenReview system and sincerely hope it meets your standards. Here is a quick look of the key updates:\n\n - We tried our best to better clarify the design of the asymmetric kernel ridge regression model in Eq.(4). Please review the reorgnized sentences in Section 2.2 on page 4.\n-   We added the discussion and the additional experiment on the selection of initial support data in Appendix F. We trust these additions will provide robust evidence for the effectiveness of our proposed dynamic strategy.\n\nWe're truly grateful for your valuable suggestions, which have played a pivotal role in elevating the quality of our paper.  Any lingering concerns on your end? We're all ears and more than ready to tackle them before the discussion period wraps up."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453607305,
                "cdate": 1700453607305,
                "tmdate": 1700453607305,
                "mdate": 1700453607305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XTnIg95S3t",
                "forum": "D6aGz0Zyvn",
                "replyto": "Vin1883BCr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Reviewer_aToc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Reviewer_aToc"
                ],
                "content": {
                    "comment": {
                        "value": "I truely appreciate your detailed response and additional experiments, and they are helpful to me to better understand your work. However, I still think there are some major concerns which may not be solved in a short time:\n1) The application prospect of this research is not clear, in what scenario the proposed asymmetric kernel can produce better performance?\n2) It seems a trivial study on kernel methods area, the paper only proposed an asymmetric kernel ridge regression model and a gradient based optimization method, the computation cost is still high. I think the research may not provide some helpful insights on kernel method area.\n3) The kernel ridge regression is a classical model, but not a good choice in most applications at present, while the deep models are widely applied in many areas. However, the proposed asymmetric kernel seems can not incorporate with deep models, which makes the study shows less significance."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723836050,
                "cdate": 1700723836050,
                "tmdate": 1700723836050,
                "mdate": 1700723836050,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cHrPCu2nnl",
            "forum": "D6aGz0Zyvn",
            "replyto": "D6aGz0Zyvn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2653/Reviewer_gJwQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2653/Reviewer_gJwQ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel approach to enhance the flexibility of kernel-based learning by introducing Locally-Adaptive-Bandwidth (LAB) kernels. Unlike traditional fixed kernels, LAB kernels incorporate data-dependent bandwidths, allowing for better adaptation to diverse data patterns. To address challenges related to asymmetry and learning efficiency, the paper introduces an asymmetric kernel ridge regression framework and an iterative kernel learning algorithm. Experimental results demonstrate the superior performance of the proposed algorithm compared to existing methods in handling large-scale datasets and achieving higher regression accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The introduction of LAB kernels with trainable bandwidths significantly improves the flexibility of kernel-based learning. By adapting bandwidths to individual data points, the model can better accommodate diverse data patterns, leading to more accurate representations.\n2. The paper establishes an asymmetric kernel ridge regression framework specifically designed for LAB kernels. Despite the asymmetry of the kernel matrix, the stationary points are elegantly represented as a linear combination of function evaluations at training data, enabling efficient learning and inference.\n3. The proposed algorithm allows for the estimation of bandwidths from the training data, reducing the demand for extensive support data. This data-driven approach enhances generalization ability by effectively tuning bandwidths based on the available training data.\n4. The proposed algorithm shows superior scalability in handling large-scale datasets compared to Nystr\u00f6m approximation-based algorithms. LAB kernels, with their adaptive bandwidths, offer a flexible and efficient solution for kernel-based learning tasks with extensive data."
                },
                "weaknesses": {
                    "value": "1. While the paper presents empirical evidence of the superior performance of the proposed algorithm, it may lack strong theoretical guarantees or formal analysis of its convergence properties. Further theoretical investigations may be needed to fully understand the behavior and limitations of LAB kernels\n2. The performance of LAB kernels heavily relies on the accurate estimation of bandwidths. Selecting appropriate bandwidths for different data patterns can be a challenging task, and suboptimal choices may result in reduced performance or overfitting."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2653/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698984121715,
            "cdate": 1698984121715,
            "tmdate": 1699636205418,
            "mdate": 1699636205418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XiHDDac6Ug",
                "forum": "D6aGz0Zyvn",
                "replyto": "cHrPCu2nnl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gJwQ (Part 1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your valuable comments and highly recognition of our work.\n\n## About Convergence Properties\nWe agree with you regarding the significance of convergence properties in algorithmic discussions. Given that our optimization problem, as outlined in equation (9), is a non-convex unconstrained challenge, we have opted for the utilization of stochastic gradient descent (SGD) methods to efficiently address it. Consequently, the convergence analysis of SGD in the realm of general non-convex optimization is pertinent to our algorithm, e.g.[[1]](https://dl.acm.org/doi/abs/10.5555/3455716.3455852). \nFor this reason, we chose not to delve into this topic in the manuscript. But we value your suggestions and intend to incorporate a discussion on it in the revised version.\n\n[1] Fehrman, Benjamin, Benjamin Gess, and Arnulf Jentzen. \"Convergence rates for the stochastic gradient descent method for non-convex objective functions.\" _The Journal of Machine Learning Research_ 21, no. 1 (2020): 5354-5401.\n\n## Learning Behavior of LAB RBF Kenrels\n\nThe learning behavior of LAB RBF kernels, as you point out, is indeed both intriguing and notably distinct from existing kernels due to their inherently asymmetric nature. The loss of symmetry renders the analysis tools in traditional RKHS, or even in more general case e.g., reproducing kernel Kre&iuml;n spaces (RKKS) and reproducing kernel Banach spaces (RKBS), not applicable. Our ongoing work specifically addresses and explores these unique characteristics. \n\nNotably, LAB RBF kernels, featuring trainable bandwidths, share a resemblance with multiple kernel learning whose corresponding functional space comprises a direct sum space of Hilbert spaces. Inspired by this correlation, our analysis of LAB RBF kernels aims to construct a sparse model within the direct integral space of Hilbert spaces. This work is not only interesting but also presents a notable challenge due to the complex nature of the space being explored."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700168965753,
                "cdate": 1700168965753,
                "tmdate": 1700168965753,
                "mdate": 1700168965753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vGUUDNx6sF",
                "forum": "D6aGz0Zyvn",
                "replyto": "cHrPCu2nnl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gJwQ (Part 2/2)"
                    },
                    "comment": {
                        "value": "## Effects of Inaccurate Estimation of Bandwidths\n\nWe appreciate your insights into the crucial role that bandwidths play in the performance of LAB RBF kernels. It is precisely for this purpose, we propose the kernel learning algorithm to determine proper bandwidths for different data patterns.\n\nIndeed there might be some challenges to determine good bandwidth in practice. For example, within our kernel learning algorithm, various factors during training, such as the choice of initial points, stopping criteria, learning rate, and batch size, may result in distinct bandwidth estimations.\n\nIn our experiments, these hyper-parameters are determined through cross-validation, detailed in Appendix C. And the influence of initialization is thoroughly addressed in Appendix E.\n\nHere, to address you concern, we present extra experimental results that demonstrate their impact on the performance of our algorithm. \n\n**Impact of Maximal Iteration Number**\n1. Yacht dataset: 246 training data, batch size=32.\n\n| Iterations | 0      | 100    | 200    | 300    | 400    | 500    | 1000   | 2000   | 3000   | 4000   | 5000   |\n|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| Mean of $R^2$ | 0.5633 | 0.9396 | 0.9789 | 0.9872 | 0.9902 | 0.9913 | 0.9940  | 0.9956 | 0.9966 | 0.9966 | 0.997  |\n| Std of $R^2$  | 0.0737 | 0.0316 | 0.0187 | 0.0105 | 0.0073 | 0.0065 | 0.0056 | 0.0033 | 0.0023 | 0.0024 | 0.0021 |\n\n2. Pakinson dataset: 4700 training data, batch size=128.\n\n| Iterations| 0      | 100    | 200    | 300    | 400    | 500    | 1000   | 2000   | 3000   | 4000   | 5000   |\n|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| Mean of $R^2$ | 0.5458 | 0.9101 | 0.9445 | 0.9622 | 0.9752 | 0.9814 | 0.9889 | 0.9895 | 0.9925 | 0.9931 | 0.9944 |\n| Std of $R^2$  | 0.0315 | 0.0065 | 0.0061 | 0.0052 | 0.0061 | 0.0055 | 0.0033 | 0.0023 | 0.0026 | 0.0019 | 0.0019 |\n\n**Impact of Learning Rate**\n1. Yacht dataset\n\n|Learning Rate| 1.00E+00 | 1.00E-01 | 1.00E-02 | 1.00E-03 | 5.00E-04 | 1.00E-04 |\n|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n| Mean of $R^2$ |  0.9723 |  0.9968 |  0.9970 |  0.9911 |  0.9804 |  0.8089  |\n|Std of $R^2$|  0.0164 |  0.0019 |  0.0017 |  0.0038 |  0.0104 |  0.0246  |\n\n2. Pakinson dataset:\n\n| Learning Rate | 5.00E-01 | 1.00E-01 | 5.00E-02 | 1.00E-02 | 5.00E-03 | 1.00E-03 |\n|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|\n|  Mean of $R^2$  | 0.98275  | 0.99411  | 0.9943   | 0.99066  | 0.9763   | 0.8947   |\n| Std of $R^2$  | 0.0031   | 0.0018   | 0.0018   | 0.0022   | 0.0058   | 0.0052   |\n\n**Impact of Batch Size**\n1. Yacht dataset\n\n|  Batch Size | 16       | 32       | 64        | 128      |\n|:----------:|----------|----------|-----------|----------|\n| Mean of $R^2$ | 0.9932 | 0.9944 | 0.9952 | 0.9965 |\n|  Std of $R^2$ | 0.0065 | 0.0044 | 0.0047 | 0.0019 |\n\n2. Pakinson Dataset:\n\n| Batch Size | 16      | 32      | 64      | 128    |\n|:----------:|----------|----------|-----------|----------|\n| Mean of $R^2$  | 0.9863 | 0.9907 | 0.9931 | 0.9943 |\n|  Std of $R^2$  | 0.0041 | 0.0027 | 0.0018 | 0.0018 |\n\nThese results underscore that the carefully selection of these hyperparameters enhances the final performance. Nevertheless, even under suboptimal hyperparameter settings, the performance remains commendable, albeit with varying bandwidth estimates. This highlights the robustness and insensitivity of our algorithm across a wide spectrum of hyperparameter choices.\n\n\n---\nWe hope that our explanations, along with the additional experiments, effectively address your concerns regarding algorithm convergence and robustness. Furthermore, we welcome the opportunity for further discussions on the learning behavior of LAB RBF kernels if you are interested."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169169183,
                "cdate": 1700169169183,
                "tmdate": 1700169169183,
                "mdate": 1700169169183,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w12ZAuK12e",
                "forum": "D6aGz0Zyvn",
                "replyto": "cHrPCu2nnl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2653/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Notification of revision."
                    },
                    "comment": {
                        "value": "Dear reviewer gJwQ,\nWe've just uploaded the revised paper onto the OpenReview system. Taking your insightful suggestions into account, we've made thorough revisions to the manuscript, where you can find that:\n\n - On page 7, we added the discussion of convergence analysis in the end of Section 3.\n - In the Appendix E, we added the extra experiments on the effects of hyper-parameters on final performance.\n\nOnce again, we appreciate these valuable suggestions from you, which have significantly contributed to enhancing the quality of our paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2653/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453517415,
                "cdate": 1700453517415,
                "tmdate": 1700453517415,
                "mdate": 1700453517415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]