[
    {
        "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement"
    },
    {
        "review": {
            "id": "ZMQAeW9VUN",
            "forum": "bNt7oajl2a",
            "replyto": "bNt7oajl2a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6886/Reviewer_MmPX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6886/Reviewer_MmPX"
            ],
            "content": {
                "summary": {
                    "value": "This work examines the inductive reasoning capabilities of LLMs, decomposing this process into two distinct stages: hypothesis proposal and rule application. The results suggest that LLMs are often able to propose reasonable hypotheses, but less reliable at applying those hypothesized rules. A neurosymbolic approach is proposed in which LLM-proposed hypotheses are symbolically implemented, improving performance on multiple inductive reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Diverse set of tasks\n- Thorough evaluation, evaluating range of hyperparameters, metrics, and base models\n- The proposed approach, hypothesis refinement, is interesting and yields improved performance across multiple tasks. The approach also has interesting connections to human reasoning.\n- The distinction between hypothesis proposal and rule application generates useful insights into the strengths and weaknesses of reasoning in LLMs."
                },
                "weaknesses": {
                    "value": "- The results would be more informative if compared directly with human performance. Do any of these benchmarks contain human performance measures (e.g. I believe that there is already human behavioral data for miniSCAN in the original paper)?\n- The results on noisy rule induction are especially difficult to interpret without a human baseline. The authors cite a paper indicating that humans are somewhat robust to noise when inducing rules, but the amount of noise and the specific tasks will matter a lot. \n- Throughout the paper, the authors appeal to intuition concerning putative human performance on the benchmarks they consider (e.g. in considering the potential for human reasoners to show a discrepancy between hypothesis generation and rule application), but intuition is not always a reliable guide regarding human performance. It would be good to qualify these statements a bit more (or, to the extent possible, to include a direct comparison with human performance).\n- The 'task accuracy' measure seems designed to emphasize the consistency of the symbolic rule application. When looking at raw accuracy, the differences between symbolic and LLM rule application don't look nearly as large. I think this somewhat undermines the claim that their rule application abilities are so much worse than their hypothesis generation abilities. \n- It should be emphasized more that miniARC, unlike the other tasks, is a distinctly visual task, in that it requires understanding visual concepts like 'objectness'. It is somewhat unsurprising that a text-only model would show special difficulty on such a task.\n- Do the authors have any explanation for why the hypothesis refinement approach achieves worse performance on miniARC relative to the baselines? Given the visual nature of the task, it's not surprising that it doesn't help much, but I was surprised that it actually seems to impair performance.\n- The familiarity analysis seems to confound two very different issues: 1) The presence of the same exact problems in the LLM pretraining data (a significant possibility, given the use of pre-existing datasets), and 2) the use of familiar vs. unfamiliar words (e.g. pseudowords). I think it's important to dissociate these concerns. This can be done by creating a new dataset with similar properties, e.g. by replacing the specific pseudowords and colors in miniSCAN (but maintaining the same general pseudoword -> color structure). \n- Is the interaction between iid vs. ood and IO vs. hypothesis refinement (figure 2) statistically significant? Also, the text describes the hypothesis refinement results as demonstrating superior robustness to this ood setting, but for miniARC the ood accuracy is actually higher for the IO baseline (even though the difference between iid and ood performance is larger for IO). This should be clarified in the text.\n- Were the language model and human hypotheses systematically compared in any way, or only qualitatively inspected? Did the strength of the language model hypotheses correspond to performance on the task? Were there cases where the model performed well on the task despite providing unhuman-like hypotheses?\n\n## Minor comments:\n- What setting was used for the 'top p' parameter in GPT-4?\n- It would be helpful to either include the variable names in figure 1, or to have a separate figure illustrating the overall flow of the model with the corresponding variable names.\n- It would be good to say a bit more about how this work differs from Wang et al (2023). This is concurrent work, so there is no concern about novelty, but it would still be useful to have more discussion of the relationship.\n- I found the description of the approach somewhat confusing. Based on the abstract and intro, I was expecting that the hypotheses would be articulated in natural language, and this would somehow be translated into code which is then symbolically executed. It is explained later on (in section 2.2) that the LM also carries out this translation step for list functions and miniARC, but it would be good to provide some hint that this is the case earlier in the section describing the approach. My understanding is that for the other tasks, the LM is prompted so as to ensure hypotheses in a particular format, which can then be automatically parsed, is that correct? It would be helpful to clarify this (in section 2 it says that the hypotheses are 'constrained', but it's not immediately clear what that means).\n- I found this sentence, 'However, they also behave as puzzling inductive reasoners, showing notable performance gaps in rule induction (i.e., identifying plausible rules)\u2026' to be confusing, because it seems to say that LLMs are bad at proposing rules, even though it was just stated that they are good at this. This also seems misaligned with the results. It seems like this sentence should instead emphasize the rule application specifically (though, as mentioned above, it's not clear how significant this discrepancy really is).\n- The authors might consider citing this related work on analogical reasoning (a special case of inductive reasoning) in LLMs: Webb, T., Holyoak, K. J., & Lu, H. (2023). Emergent analogical reasoning in large language models. Nature Human Behaviour."
                },
                "questions": {
                    "value": "I have listed some questions in the previous section. I would be happy to raise my score if some of these issues can be addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6886/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6886/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6886/Reviewer_MmPX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697934362064,
            "cdate": 1697934362064,
            "tmdate": 1700682387598,
            "mdate": 1700682387598,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ywLNr01LIf",
                "forum": "bNt7oajl2a",
                "replyto": "ZMQAeW9VUN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MmPX (1/n)"
                    },
                    "comment": {
                        "value": "Thank you so much for your insightful questions and constructive feedback!\n### Response to weakness\n\n>  The results would be more informative if compared directly with human performance. Do any of these benchmarks contain human performance measures (e.g. I believe that there is already human behavioral data for miniSCAN in the original paper)?\n\nThanks for the suggestion! All of our experiments are largely motivated by cognitive scene literature. Therefore, they do have existing human performance as a reference. Note the exact setups, data, and evaluations in these studies might differ from ours. We include all existing human studies results in Appendix C.1 and refer it when discussing main results in Section 3.\n\n>  The results on noisy rule induction are especially difficult to interpret without a human baseline. The authors cite a paper indicating that humans are somewhat robust to noise when inducing rules, but the amount of noise and the specific tasks will matter a lot.\n\nWe conducted human study using the same setup and added results in Appendix C.3. We observe that both the LM and humans perform worse on tasks with noisy examples. However, the relative performance drop of the LM is more significant. We clarified this in Section 4.2 footnote 9. \n\n>  Throughout the paper, the authors appeal to intuition concerning putative human performance on the benchmarks they consider (e.g. in considering the potential for human reasoners to show a discrepancy between hypothesis generation and rule application), but intuition is not always a reliable guide regarding human performance. It would be good to qualify these statements a bit more (or, to the extent possible, to include a direct comparison with human performance).\n\nThanks for the feedback! We conducted more human studies to provide a more comprehensive head-to-head comparison between the LM and humans. The additional human experiments include asking crowdworkers to (1) write rules for 50 randomly sampled tasks from List Functions and MiniARC (as a comparison to Table 1), (2) write rules for perturbed tasks with a hint indicating that examples may be incorrect (comparable to the dashed line in Figure 4a), and (3) write rules for perturbed tasks without hints (comparable to the bar Figure 4a). We included all details of these human studies in Appendix C.3. While these studies do not cover all the experiments conducted with LMs due to cost constraints, we hope they provide a better understanding of the behaviors of LMs and humans and motivate future research in this area.\n\n>  The 'task accuracy' measure seems designed to emphasize the consistency of the symbolic rule application. When looking at raw accuracy, the differences between symbolic and LLM rule application don't look nearly as large. I think this somewhat undermines the claim that their rule application abilities are so much worse than their hypothesis generation abilities.\n\nWe consider \u201ctask accuracy\u201d as an alternative metric as it captures a desirable property of an induction system: it should ideally consistently solve examples within a task. We use this metric to approximate the model\u2019s true understanding of a task because a model is less likely to solve all examples without using the expected computation. Although the differences in raw accuracy are not as large as those in task accuracy, they are still significant for certain tasks (e.g. relative performance drop is around 27% for MiniSCAN and around 42% for MiniARC). In addition, we observe that more advanced prompting techniques, such as self-consistency prompting and zero-shot CoT prompting, also do not bridge the gap (see Appendix B.2 for new experiments). As we argue in Section 4.1, we do not claim that we should expect LMs to behave as effectively as symbolic interpreters, but often the gaps are still large enough to support the argument."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406300274,
                "cdate": 1700406300274,
                "tmdate": 1700406300274,
                "mdate": 1700406300274,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pDftNoggyB",
                "forum": "bNt7oajl2a",
                "replyto": "k0aobkgzhl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6886/Reviewer_MmPX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6886/Reviewer_MmPX"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you to the authors for this extremely thorough rebuttal and revision. I am happy to raise my score to an 8 based on these updates.\n\nI do think that it would still be worth emphasizing that the intuition regarding the discrepancy between rule generation and rule application in human reasoners is, as far as I can tell, unsupported by human behavioral data (unless I missed it, the newly added behavioral results do not address rule application in human subjects). There is also no human behavioral data on the 'task accuracy' measure, which I believe may give a somewhat exaggerated view of performance differences between LM and symbolic rule application. So even though the difference between these approaches may seem quite large and at odds with human intuition, I don't think we can say for sure how human participants would perform on this metric. I don't think this in any way undermines the results of this study, but I think a more qualified statement about intuitions regarding human performance would be helpful.\n\nRegarding the sentence in the abstract that I found confusing, I think it would be clearer if it were rephrased to read: 'However, they also behave as puzzling inductive reasoners, showing notable performance gaps *between* rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances)...'"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682350376,
                "cdate": 1700682350376,
                "tmdate": 1700682350376,
                "mdate": 1700682350376,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YE2hJRcjXS",
            "forum": "bNt7oajl2a",
            "replyto": "bNt7oajl2a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6886/Reviewer_6Upx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6886/Reviewer_6Upx"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the inductive reasoning capabilities of large language models (LLMs) through iterative hypothesis refinement. The key ideas are:\n\n- Inductive reasoning involves proposing hypotheses to explain observations, selecting the best hypothesis, and refining it based on new examples. This process mirrors\nhuman inductive reasoning.\n- The authors test LLMs on this through:\n    1. Using the LLM to propose rule hypotheses based on examples\n    2. Testing the rules using symbolic interpreters or LLMs as rule appliers on new examples\n    3. Providing feedback to the LLM to further refine the rules\n- Experiments on 4 datasets show LLMs are phenomenal at proposing plausible hypotheses when combined with symbolic interpreters. Iterative refinement significantly improves\nperformance.\n- However, LLMs display counter-intuitive inductive behaviors compared to humans:\n    - They struggle to apply their own proposed rules\n    - They are brittle to minor perturbations in examples\n    - Their induced rules differ in content and form from human-proposed rules"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Well motivated, clear and flows well. I really enjoyed reading the paper.\n- The paper tackles an important problem in reasoning, reasoning inductively by proposing hypotheses.\n- The domains are well defined and the content is diverse.\n- The human experiments are insightful - comparing induced rules reveals qualitative gaps between LLMs and human reasoning.\n- The paper makes an important contribution in carefully evaluating both strengths and weaknesses of LLMs for inductive reasoning.\n- The analysis is thorough, spanning different models, datasets, and evaluations.\n- The limitations, scope and results are clearly defined and discussed.\n\nOverall, this is a clearly written, rigorous, and impactful study that advances our understanding of inductive reasoning in LLMs. The paradoxical findings are intriguing and point to promising future directions."
                },
                "weaknesses": {
                    "value": "- An analysis of the complexity of the rules used to generate the data would be interesting. Comparing the complexity of the hypothesis across tasks and domains might give some insight into the model performance.\n- Similarly, the complexity of the human induced and LLM induced rules might be interesting to analyze.\n- How were the number of examples seen by the model chosen across domains? What is the minimum number of examples needed to learn a rule?\n- An open source model would make the evaluations more comprehensive.\n- A separate evaluation for LLMs as symbolic interpreters of rules would help tease apart the rule-proposing / application componenets more. More on complexity: LMs might be bad appliers of complex rules.\n- Can LLMs apply rules induced by humans?\n- Is there a change in the types of rules induced if the prompt is changed to encourage communication (since this was what humans seemed to do)? Change prompt to emphasize communication?\n- MiniAC\u2192MiniARC: 4.3 para1 line 3"
                },
                "questions": {
                    "value": "I have specified the questions/ suggestions in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819451265,
            "cdate": 1698819451265,
            "tmdate": 1699636800884,
            "mdate": 1699636800884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ghWpnvwLyy",
                "forum": "bNt7oajl2a",
                "replyto": "YE2hJRcjXS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6Upx (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and insightful suggestions! We appreciate that you found our paper interesting and insightful. \n\n>  An analysis of the complexity of the rules used to generate the data would be interesting. Comparing the complexity of the hypothesis across tasks and domains might give some insight into the model performance.\n\nIt\u2019s definitely very interesting to analyze the relationship between the complexity of the rules and model performance. However, it is challenging to determine a good measure of complexity. As a preliminary experiment, we investigate how rule complexity affects model performance using List Functions, as its data was generated by programs. For each task, we collect its Python Lambda expression and analyze its complexity using abstract syntax tree (AST), similar to Bi et al. (2023). Note that the original List Function dataset uses a domain-specific language (DSL). This DSL defines symbols for basic list operations following a Lisp-like syntax. For instance, a list `[1, 2, 3]` is represented as `(cons 1 (cons 2 (singleton 3)))`. We did not consider this DSL for our analysis, as we hypothesize that Python is better represented in LMs' pre-training data than this DSL. Analyzing Python expressions, therefore, could provide a better estimate of rule complexity.\n\nWe consider both primitive and structural complexities and investigate their correlations with model performance. The former indicates the learnability of individual primitive operations, while the latter reflects the difficulty of the compositional structures of the rule, which we approximate using the number of nodes and tree depth of the AST. \n\nWe first examine how different primitive operations contribute to rule induction. We train a logistic regression model to predict LM's task accuracy on a new instance, using the counts of different types of nodes as features. The logistic regression models achieve an averaged test accuracy of 68.4% with a small number of features, suggesting that the model performance is somewhat predictable based on the primitives of the rule. We use the coefficient as an approximation of the learnability of the primitive operations.  These coefficients vary across different primitives, with some operations such as `In` being easier to learn than others like `Sub`. \n\nFor structure complexity, we measure the Spearman correlations between test accuracy and the structure complexity measurements. The number of nodes has a correlation of -0.31, and tree depth has a correlation of -0.37. Tasks with high accuracy generally have a relatively small number of nodes and tree depth. However, some programs with low structural complexity also have low accuracy, indicating that other factors, such as the aforementioned primitive complexity, should also be considered.\n\nZhen Bi, Ningyu Zhang, Yinuo Jiang, Shumin Deng, Guozhou Zheng, Huajun Chen. When Do Program-of-Thoughts Work for Reasoning?\n\n>  Similarly, the complexity of the human induced and LLM induced rules might be interesting to analyze.\n\nSince the human-induced and LM-induced rules are often in free-form natural language, it is even more challenging to measure their complexity. Measures such as minimum description length and the number of unique n-grams could be potential solutions. However, it is unclear whether these measures correlate with the complexity of rules in our setup. We agree that this is a very interesting direction, but it is beyond the scope of this paper. Therefore, we leave this as a direction for future work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406179100,
                "cdate": 1700406179100,
                "tmdate": 1700406179100,
                "mdate": 1700406179100,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IRi6S55XwL",
            "forum": "bNt7oajl2a",
            "replyto": "bNt7oajl2a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6886/Reviewer_bJW9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6886/Reviewer_bJW9"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the inductive reasoning capacities of language models on a set of tasks, in terms of hypothesis proposal, selection, and refinement and then analyzes how the hypotheses differ from human ones."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Although the tasks are somewhat toy, the paper demonstrates its claims, is well-written, and is relatively comprehensive. They perform a novel analysis of the kinds of hypotheses and the model's ability to apply them. This is (in my view) a clear contribution, and I have no substantial criticisms."
                },
                "weaknesses": {
                    "value": "A few of the experiment setups feel a bit contrived - for example, randomly perturbing a set of items in a small set of experiments, of course, makes the task harder for a language model since it also requires it to infer that the noise is noise and not itself a deterministic part of the rule. The section on familiarity of exemplars should also likely mention Dasgupta et al.'s \"Language models show human-like content effects\" there."
                },
                "questions": {
                    "value": "I'm curious about how this paper squares with some results like that in the after-submission-deadline \"Large Language Models Cannot Self-Correct Reasoning Yet\" from Huang et al. (2023). The point there was that language models, given the opportunity to revise their reasoning, will often make it worse. In practice, did you see this when refining hypotheses? It would be interesting to see how the number of revisions affects performance, similar to Table 2 in the concurrent \"Hypothesis Search\" paper from Wang et al. I see there is some version of this in this paper's Table 2, but given the emphasis that this paper places of hypothesis refinement, I'd expect a bit more detail. Especially given the self-consistency, it would be valuable to understand the tradeoff between more attempts and more revisions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821041233,
            "cdate": 1698821041233,
            "tmdate": 1699636800691,
            "mdate": 1699636800691,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZsNiO7KSeJ",
                "forum": "bNt7oajl2a",
                "replyto": "IRi6S55XwL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bJW9"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We appreciate your support for our paper.\n\n\n### Response to weakness\n\n>  A few of the experiment setups feel a bit contrived - for example, randomly perturbing a set of items in a small set of experiments, of course, makes the task harder for a language model since it also requires it to infer that the noise is noise and not itself a deterministic part of the rule.\n\nTo better calibrate model performance, we conducted human study using the same setup and added results in Appendix C.3. We observe that both the LM and humans perform worse on tasks with noisy examples. However, the relative performance drop of the LM is more significant. We clarified this in Section 4.2 footnote 9.\n\n>  The section on familiarity of exemplars should also likely mention Dasgupta et al.'s \"Language models show human-like content effects\" there.\n\nThanks for the pointer! We discussed this paper in Section 4.2 footnote 10. \n\n### Response to questions\n\n>   In practice, did you see this when refining hypotheses? It would be interesting to see how the number of revisions affects performance, similar to Table 2 in the concurrent \"Hypothesis Search\" paper from Wang et al. I see there is some version of this in this paper's Table 2, but given the emphasis that this paper places of hypothesis refinement, I'd expect a bit more detail. Especially given the self-consistency, it would be valuable to understand the tradeoff between more attempts and more revisions.\n\nHuang et al. (2023) evaluates *intrinsic* self-correction without external feedback, which is similar to Self-Refine. As shown in Table 1, we similarly found an iterative approach without external feedback is insufficient. However, we did observe performance improvement over iterations for all datasets when coupled with external interpreters. The results are shown below. See Figure 5 in Appendix B.3 for a better visualization. This is consistent with the findings in Huang et al. (2023), as they discussed in Section 5 \u201cLeveraging external feedback for correction\u201d. \n\nRaw Accuracy \n| Iteration |    1 |    2 |    3 |\n|-----------|-----|-----|-----|\n| ACRE      | 79.8 | 82.3 | 82.5 |\n| MiniSCAN  | 86.6 | 92.9 | 93.3 |\n| List Fns  | 62.4 | 68.3 | 71.2 |\n| MiniARC   | 12.8 | 15.4 | 18.7 |\n\nTask Accuracy \n| Iteration |    1 |    2 |    3 |\n|-----------|-----|-----|-----|\n| ACRE      | 48.0 | 56.0 | 59.0 |\n| MiniSCAN  | 70.0 | 84.0 | 85.0 |\n| List Fns  | 52.4 | 58.4 | 61.2 |\n| MiniARC   |  9.2 | 12.3 | 14.6 |\n\nHowever, as we mentioned at the end of Section 4.3, qualitatively, in some cases, LMs tend to make minor modifications rather than starting from entirely new hypotheses. Therefore, in cases where the initial hypothesis is completely off, iterative refinement tends to be less effective.\n\nWe agree that it is certainly interesting to investigate the tradeoff between the number of hypotheses (N) and the maximum number of iterations (T). We did preliminary exploration (in Table 1) to demonstrate the correlations between model performance and these two hyperparameters. For tasks where LMs achieve strong performance, such as ACRE and MiniSCAN, a limited number of iterations is already effective. For tasks like MiniARC, where LMs perform poorly, the trends remain positive after the maximum number of iterations.\n\nNote that the maximum number of iterations is bounded by the context lengths of LMs. Empirically, we find it is infeasible to further increase T. An alternative could be tracking only the latest K iterations. Additionally, increasing N leads to more API calls, which can be computationally expensive. A summarization strategy, as suggested in Wang et al. (2023) might offer a solution. Therefore, identifying the best tradeoff between more attempts and more revisions could require more method-wise exploration, which is beyond the scope of this paper. We hope our experiments motivate future work along this direction."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406093527,
                "cdate": 1700406093527,
                "tmdate": 1700406093527,
                "mdate": 1700406093527,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qRjHijqmXO",
                "forum": "bNt7oajl2a",
                "replyto": "ZsNiO7KSeJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6886/Reviewer_bJW9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6886/Reviewer_bJW9"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response! I will keep my score as is."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666095581,
                "cdate": 1700666095581,
                "tmdate": 1700666095581,
                "mdate": 1700666095581,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gdyANrkNL7",
            "forum": "bNt7oajl2a",
            "replyto": "bNt7oajl2a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6886/Reviewer_EAiR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6886/Reviewer_EAiR"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on evaluating the inductive reasoning capabilities of large language models. Given input examples, the authors propose a three-stage process that first asks LLMs to propose hypotheses about the task, and then use a domain-specific interpreter to evaluate hypotheses on input examples, finally, hypotheses that pass most input examples are used to apply to unseen examples for testing.  The authors also propose to leverage interpreter results of hypotheses as feedback to refine the hypotheses. Experiments show that this approach significantly boosts the performance of LLMs on 4 inductive reasoning datasets. The authors then show various differences between humans and LLMs through additional experiments such as asking LLMs to apply rules without interpreters and perturbing part of input examples. These experiments demonstrate the behavior difference between humans and LLMs on inductive reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The author proposes an effective approach that disentangles the inductive reasoning task into the process of proposing a hypothesis and interpreting the hypothesis that shows strong performance compared with recent approaches that use various types of prompting without external interpreters.\n- The proposed method is validated on multiple large language models by comprehensive experiments on 4 datasets of different domains, showing the generalizability of the method."
                },
                "weaknesses": {
                    "value": "My concern mainly lies in Section 4:\n\n- For the example perturbation experiment in section 4.2, there are no studies on how well humans can actually perform on perturbed tasks. It is hard to judge how big the performance drop of LMs is compared with humans.\n- Experiments in 4.1 and 4.2 are conducted with simple prompting which may not be the most effective method to elicit this type of reasoning from the model.\n- The generalizability of the findings in 4.3 is doubtful because only one type of prompt is used to generate rules from LM. According to the appendix, example rules on List Fn and MiniARC are generated from LM with a prompt that contains no format instruction. It is unknown whether LMs can generate human-like inductions with more guidance or provided with human-induced rules as few shot examples.\n\nOverall, I believe the main contribution of the paper is an effective inductive reasoning pipeline using LMs. So these are not significant flaws of the paper. So I still recommend acceptance. However, I strongly encourage authors to provide more rigorous evidence when making claims in Section 4."
                },
                "questions": {
                    "value": "- What are the prompts used to generate the Python program from rules for List Functions and MiniARC?\n- In Table 2, on MiniScan, T=3, N=1 yields higher raw accuracy and task accuracy than T=3, N=5, which means that fewer hypotheses considered lead to worse performance. Is there an explanation for this?\n- In Section 2, the authors claim to evaluate models on \u201cOOD\u201d examples by generating longer or larger examples than those in the original datasets. This is a bit confusing. Are authors actually fixing the seen examples and only changing the unseen examples for testing?\n- When asking LMs to write Python programs given the hypothesis, are the seen examples also provided in the prompt?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6886/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6886/Reviewer_EAiR",
                        "ICLR.cc/2024/Conference/Submission6886/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6886/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829878981,
            "cdate": 1698829878981,
            "tmdate": 1700731085638,
            "mdate": 1700731085638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "O2ycR9Ww1H",
                "forum": "bNt7oajl2a",
                "replyto": "gdyANrkNL7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6886/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6886/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EAiR (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback! We appreciate that you found our approach effective and our experiments comprehensive.\n\n\n### Response to weakness \n\n>  For the example perturbation experiment in section 4.2, there are no studies on how well humans can actually perform on perturbed tasks. It is hard to judge how big the performance drop of LMs is compared with humans.\n\nThanks for the feedback! We conducted human study using the same setup and added results in Appendix C.3. We observe that both the LM and humans perform worse on tasks with noisy examples. However, the relative performance drop of the LM is more significant. We have clarified this in Section 4.2 footnote 9.\n\n>  Experiments in 4.1 and 4.2 are conducted with simple prompting which may not be the most effective method to elicit this type of reasoning from the model.\n\nFor section 4.1, we consider two alternative prompting techniques for rule application: self-consistency prompting (SC) and zero-shot chain-of-thought (0-CoT). The results are shown below. We do not observe significant performance differences across these methods, except on ACRE, where 0-CoT underperforms other methods in task accuracy. We included detailed results and discussions in Appendix B.2 and clarified this in Section 4.1 Our results show the rule application performance is consistent across various prompting methods. It would be interesting to explore more advanced prompting techniques for our setting, but that is out of scope of this paper.\n\nRaw Accuracy\n| Method   | ACRE | MiniSCAN | List Fns | MiniARC |\n|----------|------|----------|----------|---------|\n| Standard | 77.8 | 67.6     | 65.8     | 10.8    |\n| 0-CoT    | 73.2 | 65.5     | 61.2     | 12.1    |\n| SC (N=5) | 77.0 | 67.5     | 66.3     | 9.7     |\n\nTask Accuracy \n| Method   | ACRE | MiniSCAN | List Fns | MiniARC |\n|----------|------|----------|----------|---------|\n| Standard | 47.0 | 0.0      | 50.0     | 5.4     |\n| 0-CoT    | 25.0 | 0.0      | 48.4     | 6.9     |\n| SC (N=5) | 46.0 | 0.0      | 50.8     | 4.6     |\n\nFor section 4.2, all perturbation experiments use iterative hypothesis refinement (T=3, N=5), which has the strongest performance in our main experiments. We clarified this in the main text and included results using other models and configurations in Appendix B.2. \n\n>  The generalizability of the findings in 4.3 is doubtful because only one type of prompt is used to generate rules from LM. According to the appendix, example rules on List Fn and MiniARC are generated from LM with a prompt that contains no format instruction. It is unknown whether LMs can generate human-like inductions with more guidance or provided with human-induced rules as few-shot examples.\n\nWe did not introduce task-specific format instructions for List Functions and MiniARC, as they cover a wide range of concepts, making it challenging to determine the most optimal constraints. To investigate whether other prompts would lead to different behaviors, we evaluate an alternative hypothesis generation prompt that includes task-specific heuristics (see Appendix B.2, Task-specific Heuristics). Specifically, we use MiniARC and add a detailed task description in the prompt (Table 10). This does help LMs to generate more human-readable rules (Table 11) for a fraction of examples, but these rules are still generally distinguishable from those induced by humans. Additionally, we did not observe performance improvement by imposing these constraints. It is certainly possible that other guidance or few-shot examples could encourage LMs to generate human-like rules. However, the former requires more prompt engineering, and the latter requires additional human annotations, which are beyond the scope of this paper. We mention these alternatives as potential directions for future work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405928090,
                "cdate": 1700405928090,
                "tmdate": 1700405928090,
                "mdate": 1700405928090,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8wnL2ZpEwa",
                "forum": "bNt7oajl2a",
                "replyto": "gB9wWUQwIW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6886/Reviewer_EAiR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6886/Reviewer_EAiR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for the detailed response. I raise my rating to 8."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6886/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731074412,
                "cdate": 1700731074412,
                "tmdate": 1700731074412,
                "mdate": 1700731074412,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]