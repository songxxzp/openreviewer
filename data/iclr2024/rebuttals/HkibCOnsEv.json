[
    {
        "title": "Structured Inverse-Free Natural Gradient: Memory-Efficient & Numerically-Stable KFAC for Large Neural Nets"
    },
    {
        "review": {
            "id": "AajsqQBCd9",
            "forum": "HkibCOnsEv",
            "replyto": "HkibCOnsEv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3942/Reviewer_GKer"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3942/Reviewer_GKer"
            ],
            "content": {
                "summary": {
                    "value": "This paper combines the strengths of two algorithms and leverages the advantages of each to improve upon their respective limitations. Specifically, the following results and contributions are observed:\n\n- The proposal of an inverse-free KFAC update scheme: \n\n Inspired by the inverse-free natural gradient descent\n(INGD) algorithm, the authors introduce an inverse-free approach to the KFAC algorithm. This modification allows the KFAC algorithm to be utilized in low-precision training scenarios, eliminating the need for computationally expensive matrix inverses.\nThe authors propose an inverse-free KFAC update scheme, inspired by the inverse-free Kronecker-factored natural gradient descent\n(INGD) algorithm. This scheme enables the KFAC algorithm to be used without requiring matrix inverses, making it suitable for low-precision training. \n\n- Imposition of a Kronecker-factored structure on INGD: \nThe authors impose a Kronecker-factored structure on the INGD algorithm and propose structured inverse-free natural gradient descent (SINGD). This structural modification allows for the utilization of sparse structures, effectively reducing memory costs. By leveraging the sparsity of specific components within the algorithm, the authors aim to optimize memory utilization while maintaining computational efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Overall, this paper is well-motivated and provides a clear and detailed description of the KFAC and INGD methods. It successfully bridges the gap between these two methods by addressing their respective limitations. Specifically, the paper achieves two key objectives:\n1. Making KFAC inverse-free for low-precision training: The authors propose an inverse-free KFAC update scheme, which eliminates the need for computationally expensive matrix inverses. This modification enables the KFAC algorithm to be effectively used in low-precision training scenarios, where precision is reduced to reduce memory and computational requirements.\n\n2. Reducing memory cost in INGD: By imposing a Kronecker-factored structure on the INGD algorithm, the authors achieve a lower memory cost. This structural modification leverages sparse structures, optimizing memory utilization while maintaining computational efficiency."
                },
                "weaknesses": {
                    "value": "This paper presents an incremental improvement by combining the strengths of existing algorithms and addressing their limitations. However, it fails to demonstrate completely new elements that offer significant advantages over previous approaches. Furthermore, upon reviewing the original INGD algorithm, it appears that the authors of the original work already discussed the sparse structures by considering sparse group structures in K and C. Consequently, the imposition of a Kronecker-factored structure on the INGD algorithm may be considered trivial, and not a substantial contribution compared to the original INGD algorithm. As a result, the overall strength of this work may not be sufficient for it to be accepted as a significant advancement in the field.\n\nAfter rebuttal, I agree that this paper provides fair improvements on the existing algorithms KFAC and INGD."
                },
                "questions": {
                    "value": "See the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Reviewer_GKer"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698586410002,
            "cdate": 1698586410002,
            "tmdate": 1700460916581,
            "mdate": 1700460916581,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kibgV0aa0L",
                "forum": "HkibCOnsEv",
                "replyto": "AajsqQBCd9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer GKer"
                    },
                    "comment": {
                        "value": "Thanks for your feedback.\n\n> incremental improvement by combining the strengths of existing algorithms and addressing their limitations.\n\nWe believe it is non-trivial to bridge the gap between INGD and KFAC. Both methods are quite different and it is left unclear by Lin et al. 2023 how they relate. One of our main contributions is to establish this previously unknown connection. Please see also the general response as we discuss the difficulty of imposing useful and sparse structures on INGD without compriosing downstream performance. Please also see general response (1).\n\nBy providing this theoretical argument, we allow users to replace KFAC with IKFAC or INGD without compromising performance and introducing major overhead, while expanding its scope to modern low precision training and applications beyond optimization, such as influence functions [1] and uncertainty estimation [2]. \n\nLast but not least, our work is the first work empirically validating the structured second-order updates in half precision settings for many different model classes including CNNs, GNNs, and transfomers.  Please also see general response (2).\n\n\n**References:**\n\n[1] Bae, Juhan, et al. \"If Influence Functions are the Answer, Then What is the Question?.\" Advances in Neural Information Processing Systems 35 (2022): 17953-17967.\n\n[2] Immer, Alexander, et al. \"Scalable marginal likelihood estimation for model selection in deep learning.\" International Conference on Machine Learning. PMLR, 2021.\n\n---\n\n\n\n> the authors of the original work already discussed the sparse structures \n\nLin et al. 2023 mention the possibility to construct sparse structures in the outro of their work (end of Section 4). However, to us, it seems like they left a substantial discussion and evaluation for future work. They do not provide any hints how one would realize such structures and efficiently preserve and compute them throughout the update. \n\nOur work offers such a detailed discussion and evaluation. Hence, we don't believe that our work was substantially addressed by Lin et al. 2023 and therefore represents a novel contribution. If you disagree, could you point us to the page in their paper where they provide such a discussion?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443553886,
                "cdate": 1700443553886,
                "tmdate": 1700444312042,
                "mdate": 1700444312042,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "x0GNHWSALQ",
                "forum": "HkibCOnsEv",
                "replyto": "kibgV0aa0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_GKer"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_GKer"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for the efforts to clarify my concerns. \nI agree with the contributions to bridge the connection between INGD and KFAC and make fair improvements on the two algorithms."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461092700,
                "cdate": 1700461092700,
                "tmdate": 1700461092700,
                "mdate": 1700461092700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KK0uyO1Kx8",
            "forum": "HkibCOnsEv",
            "replyto": "HkibCOnsEv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3942/Reviewer_zZxu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3942/Reviewer_zZxu"
            ],
            "content": {
                "summary": {
                    "value": "The authors aim to address the memory inefficiency and the numerical instability issue of KFAC. For the theory part, they close the gap between INGD and KFAC. For the experiment part, they show that the inverse-free methods have better numerical stability than KFAC. Furthermore, they show that by imposing structures on the Kronecker factors, they can achieve competitive performance while being more memory efficient."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors give a theoretical connection between an existing inverse-free method (INGD) and KFAC.\n\n2. The authors provide a plausible reason for the better empirical performance of INGD compared to KFAC and IKFAC."
                },
                "weaknesses": {
                    "value": "1. It feels like the contribution is mostly theoretical. The novelty of the inverse-free part is undermined by INGD. The discussion and analysis on the structured part also seems not to be very in-depth.\n2. The empirical evaluation is a bit weak. The visualization makes it hard to see how the proposed methods compared to Adam near the end. Also, I think the evaluation will be more reasonable under settings where KFAC outperforms Adam. Such examples can be found in some previous work like [1].\n3. The application of the block diagonal structure seems to be straightforward and existing. So it feels important that the proposed structure should be shown to solidly outperform block diagonal or some other simpler structure to have contribution on this part, but the discussion and the evaluation does not seem to be in-depth. The hierarchical structure seems to be interesting and outperforms the block diagonal structure, but the exact setting for the experiment is a bit unclear. For clarity, it is better to show how model weights with more than two dimensions are merged into two dimensions and the resulting dimensionality. Also, papers like [1] seem to suggest that for CNN, it is plausible for one Kronecker factor to be small and dense, while the other to be large but diagonal. This kind of structure is interesting to be considered/compared with, and it also suggests that setting the sparsity parameter to be the same for both Kronecker factors might not be the best for comparison.\n\n[1] Bahamou, A., Goldfarb, D., & Ren, Y. (2023, April). A Mini-Block Fisher Method for Deep Neural Networks. In International Conference on Artificial Intelligence and Statistics (pp. 9191-9220). PMLR."
                },
                "questions": {
                    "value": "The following is a list where a response from the authors could make my opinion of the paper more positive.\n\n1. I feel like evaluations on settings where KFAC outperforms Adam can convince me more on the effectiveness of the proposed method.\n2. More thorough discussion and comparison to show that the hierarchical structure performs better than the other simpler ones can also provide more contribution on the empirical part.\n3. It is also possible for authors to justify if the theoretical contribution is enough to cover up the weakness on the empirical evaluation part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Reviewer_zZxu",
                        "ICLR.cc/2024/Conference/Submission3942/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813225273,
            "cdate": 1698813225273,
            "tmdate": 1700688264806,
            "mdate": 1700688264806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SGIPW3AfBT",
                "forum": "HkibCOnsEv",
                "replyto": "KK0uyO1Kx8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer zZxu"
                    },
                    "comment": {
                        "value": "Thanks for your feedback.\n\n>More thorough discussion on sparse structures such as the hierarchical structure\n\nWe have updated Sec 3.2 to include more thorough discussion on sparse Kronecker factors including the hierarchical structure. Moreover, we further improve Figure 5 (now, Figure 6 in Appendix E) and readers can clearly see that the  hierarchical structure give a better approximation while keeping the computational cost low. \nPlease also see the general response (1). \n\n\n> The empirical evaluation is a bit weak. The visualization makes it hard to see how the proposed methods compared to Adam near the end.  The evaluation will be more reasonable under settings where KFAC outperforms Adam.\n\nOur method can indeed outperform AdamW in many models such as CNNs, GNNs and even transformers. We include additional results in Appendix F.\nPlease also see the general response (2)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443454465,
                "cdate": 1700443454465,
                "tmdate": 1700443484320,
                "mdate": 1700443484320,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rLDm7UldAv",
                "forum": "HkibCOnsEv",
                "replyto": "SGIPW3AfBT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_zZxu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_zZxu"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors for the added results. I am considering changing my score.\n\nI am curious why for three of the plots in Figure 9, SINGD uses the diagonal structure instead of the block-diagonal or the hierarchical structure. It seems important to me to have the hierarchical structure work well consistently as this is the main structural innovation. Alternatively, the authors can elaborate if the diagonal and block-diagonal structure are novel and non-trivial."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647443504,
                "cdate": 1700647443504,
                "tmdate": 1700647443504,
                "mdate": 1700647443504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V9Vv1AENkl",
                "forum": "HkibCOnsEv",
                "replyto": "KK0uyO1Kx8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer zZxu"
                    },
                    "comment": {
                        "value": ">   why for three of the plots in Figure 9, SINGD uses the diagonal structure instead of the block-diagonal or the hierarchical structure\n\nNote that both block-diagonal and the hierarchical structures include the diagonal structure as a special case.\nIn these three models, our experiments show that the diagonal structure is good enough to obtain competitive performance.\nThus, we can further reduce the peak memory by using the diagonal structure.\nWe will add the block-diagonal and the hierarchical structures in the next revision of the paper. \nWe will update the caption to clarify this point.\n\n \nIn summary, we show that the hierarchical structure can achieve similar performance as the dense structure (INGD) in all the eight models. In some models, the diagonal structure is good enough to match the performance of INGD.  Our approach allows users to choose a range of sparse structures in $\\mathbf{K}$ and $\\mathbf{C}$ at each layer.\n\nThank again for asking this follow-up question. Please let us know if you have any questions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659798984,
                "cdate": 1700659798984,
                "tmdate": 1700660174187,
                "mdate": 1700660174187,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6t9J1dON6v",
                "forum": "HkibCOnsEv",
                "replyto": "Ml0aOpLy59",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_zZxu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_zZxu"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the effort of the authors, and therefore raising my score to 6.\n\nI am still not fully convinced the contribution on the structure part is significant, and think the paper can be further improved with more endeavor on that part."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688233645,
                "cdate": 1700688233645,
                "tmdate": 1700688233645,
                "mdate": 1700688233645,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "p1yoV1IHb4",
            "forum": "HkibCOnsEv",
            "replyto": "HkibCOnsEv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3942/Reviewer_bReV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3942/Reviewer_bReV"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe a novel variant of INGD, sparsifying updates in the INGD step. Structured Kronecker factors are also used to obtain a matrix-free variant of KFAC. These approaches allow for low-precision training and memory savings over their respective matrix based variants which suffer from numerical issues due to the numerical instabilities of solving lin. systems in low precision. The approach is well-written but could use some more experimental results to demonstrate the effectiveness of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- well-written theory exposure that draws a bridge between INGD and KFAC\n- The method is well-motivated and and well-explained."
                },
                "weaknesses": {
                    "value": "- While Figure 1 provides memory footprints, these could be better-described and contextualised. Also timings would have been a welcome addition.\n- It would have been great to have some more details on the hierarchical approach. Even with the SM, I could not understand it well."
                },
                "questions": {
                    "value": "- How do overall training times compare to the non-matrix free variants and to SGD-based approaches such as ADAM?\n- From the figures alone, it seems that these methods do no outperform ADAM in terms of convergence to a certain test-loss. What would be the advantage of using this method over ADAM?\n- how exactly does the hierarchical approach work?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Reviewer_bReV"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825860565,
            "cdate": 1698825860565,
            "tmdate": 1699636354835,
            "mdate": 1699636354835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YxGbjGlpSS",
                "forum": "HkibCOnsEv",
                "replyto": "p1yoV1IHb4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to  Reviewer bReV"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to read and comment on this work.\n\n\n> how exactly does the hierarchical approach work?\n\nWe have updated Sec 3.2 to include more thorough discussion on sparse Kronecker factors including the hierarchical structure. Moreover, we further improve Figure 5 (now Figure 6 in Appendix E) and readers can clearly see that the  hierarchical structure give a better approximation while keeping the computational cost low. \nPlease also see the general response (1). \n\n---\n> How do overall training times compare to the non-matrix free variants and to SGD-based approaches such as ADAM?\n\nTable 2 in Appendix A shows the theoretical results of training time. We will include training time for all eight models in the next version due to the limited computational budget and time constraint.\n\n---\n> What would be the advantage of using this method over ADAM?\n\nOur method, using the right settings, can indeed outperform AdamW in many models such as CNNs, GNNs, and even transformers. We include additional results in appendix F. Please also see the general response (2)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443229855,
                "cdate": 1700443229855,
                "tmdate": 1700443586938,
                "mdate": 1700443586938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jTEiNDqRAZ",
                "forum": "HkibCOnsEv",
                "replyto": "YxGbjGlpSS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_bReV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_bReV"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. I have decided to keep the current score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637901347,
                "cdate": 1700637901347,
                "tmdate": 1700637901347,
                "mdate": 1700637901347,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g0IoNqdPG1",
            "forum": "HkibCOnsEv",
            "replyto": "HkibCOnsEv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3942/Reviewer_qqwe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3942/Reviewer_qqwe"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a method called Structured Inverse-Free Natural Gradient Descent (SINGD) to address the memory inefficiency and numerical instability issues of second-order methods like KFAC for training large neural networks. SINGD combines an inverse-free update and imposes sparse structures of each Kronecker factor. Experimental results on transformer-based and convolution-based models demonstrate that SINGD outperforms KFAC in terms of memory efficiency and numerical robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-written and organized.  This work presents SINGD method to improving the efficiency and stability of second-order optimization methods for deep learning.  Under certain conditions, they make a connection between the INGD method and the KFAC method. Furthermore, they extend the original INGD and develop memory-efficient SINGD method by imposing special structures in the Kronecker factors."
                },
                "weaknesses": {
                    "value": "This article provides a very detailed and specific introduction to KFAC and INGD method. However, it seems not highlight the contribution of this article itself. Some details on sparse kronecker factors are put in the appendix or skipped. Some derivation process and proofs of the algorithm could be included in the main text rather than in the appendix, which would make the algorithm more naturally presented.\n\n\nMore detailed comparisons with other state-of-the-art optimization methods such as Shampoo[1] and NGPlus[2] would strengthen the case for the superiority of SINGD. In Fig. 6, it is better to show the changes of the test error with respect to the training time. Besides, it is important to report the peak memory for these four neural networks.\n\n[1] Anil, Rohan, et al. \"Scalable second order optimization for deep learning.\" arXiv preprint arXiv:2002.09018 (2020). \n\n[2] Yang, Minghan, et al. \"An efficient fisher matrix approximation method for large-scale neural network optimization.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 45.5 (2022): 5391-5403."
                },
                "questions": {
                    "value": "From my experience, for experiment VGG16 on CIFAR100 dataset, the KFAC method should be tuned carefully. I want to ask if the search space of hyperparamters of KFAC method is large enough?\n\nThe difference between this paper and the INGD paper should be stated clearly and properly. As shown in Figure 4, the difference is that the use of projection $\\hat{\\Pi}_K$ and the structures $\\hat{\\mathcal{L}}$ in SINGD. Could you please give a specific example to show how to compute the projection and how to use the strcuture?\n\nWhy the NGD update for BLR can be simplified as the equations in the end of Page 4? Please add some explanation.\n\nDoes $O(\\beta_1^2)$ small enough in Theorem 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3942/Reviewer_qqwe"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3942/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698857891589,
            "cdate": 1698857891589,
            "tmdate": 1699636354757,
            "mdate": 1699636354757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Amrrq7VFbx",
                "forum": "HkibCOnsEv",
                "replyto": "g0IoNqdPG1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to  Reviewer qqwe"
                    },
                    "comment": {
                        "value": "Thanks for your feedback.\n\n---\n> However, it seems not highlight the contribution of this article itself. Some details on sparse kronecker factors are put in the appendix or skipped.\n\nIn the last paragraph of the introduction, we highlight the contribution of introducing sparse structures. \nWe do agree that focussing more on the sparse structures we introduce will make our contribution clearer. We have updated the manuscript according to the changes in our global response to emphasize the difficulty of constructing a structures and give concrete examples of sparse factors and their projection maps.\n\nPlease also see the general response (1). \n\n---\n\n> Comparisons with other state-of-the-art optimization \n\nOne of our contributions is to enable KFAC optimizers in lower precision settings and expand the scope of KFAC methods in modern low precision training. Thus, our main focus is on enabling KFAC in lower precision settings.\nMoreover, to our best knowledge, many existing second-order optimizers such as Shampoo are numerically unstable in half precision settings. \n\nThus, we believe that KFAC (in Float32) and AdamW (in BFloat16) are strong baselines in our experiments.\n\nPlease also see the general response (2). We also consider other models such as graph NNs and convolution NNs.\n\n---\n\n> Report Peak memory and training time\n\nTables 2-3 in Appendix A show the theoretical results of peak memory and training time. We will include them for all eight models in the next verison due to the limited computational budget and time constraint.\n\n---\n\n> I want to ask if the search space of hyperparamters of KFAC method is large enough?\n\nWe use a random search in the log scale to find the hyperparamters of KFAC include learning rate, weight decay, damping, the moving average term $\\beta_1$ to update $S_K$ and $S_C$. The search space of each hyperparameter contains multiple orders of magnitude.\n\n---\n\n>A specific example to show how to compute the projection and how to use the structure\n\nTable 4 show four examples of computing the projection and the usage of sparse structures.\nWe moved Table 4 in the appendix into the main text. This Table is Table 1 in the new revision. \n\n---\n> Why the NGD update for BLR can be simplified as the equations in the end of Page 4? Please add some explanation.\n\nAs shown in Appendix C of Khan et al. 2018 [1], the NGD update for BLR can be simplified as\n$\\mu = \\mu- \\beta S^{-1} \\nabla_\\mu (-\\mathcal{L}) =\\mu -\\beta S^{-1} \\nabla_\\mu \\{ E_{w\\sim q}[\\ell(w)] - H(q) \\}$ and $S = S + 2 \\beta \\nabla_{S^{-1}}  (-\\mathcal{L}) = S + 2 \\beta \\nabla_{S^{-1}} \\{ E_{w\\sim q}[\\ell(w)] - H(q) \\}$.\n\nWe further use gradient identities of a Gaussian distribution $q$ (see [2]) to obtain the update. These gradient identities are known as reparameterization gradients in variational inference. \n\n\n$\\nabla_\\mu \\{ E_{w\\sim q}[\\ell(w)] - H(q) \\} = \\nabla_\\mu  E_{w\\sim q}[\\ell(w)] = E_{w\\sim q}[ \\nabla_w \\ell(w)]$ (Bonnet\u2019s Theorem).\n\n$2 \\nabla_{S^{-1}} \\{ E_{w\\sim q}[\\ell(w)] - H(q) \\} = 2\\nabla_{S^{-1}}  E_{w\\sim q}[\\ell(w)] - S =  E_{w\\sim q}[\\nabla_w^2 \\ell(w)] - S$ (Price\u2019s Theorem).\n\nIn the first step, we use the fact that the entropy of a Gaussian is $H(q)=\\frac{1}{2} \\log \\mathrm{det}(S^{-1})$. We obtain the last step by using the gradient identities of the Gaussian.\n\n---\n> Does the approximation small enough in Theorem 1?\n\nIn all our experiments, we use values ranging from $\\beta_1 = 10^{-3}$ to $10^{-2}$. Therefore, we believe that the second-order term $O(\\beta_1^2)$ in Theorem 1 is small enough, and therefore KFAC should be close to IKFAC.\n\n**References:**\n\n[1] Khan, et al. \"Fast and scalable bayesian deep learning by weight-perturbation in adam.\" International conference on machine learning. PMLR, 2018.\n\n[2] Lin et al. \"Stein's Lemma for the Reparameterization Trick with Exponential Family Mixtures.\" arXiv preprint arXiv:1910.13398 (2019)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700442674242,
                "cdate": 1700442674242,
                "tmdate": 1700443637857,
                "mdate": 1700443637857,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HIWJoUCUKW",
                "forum": "HkibCOnsEv",
                "replyto": "Amrrq7VFbx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_qqwe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3942/Reviewer_qqwe"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' responses. I will keep my rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3942/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726972678,
                "cdate": 1700726972678,
                "tmdate": 1700726972678,
                "mdate": 1700726972678,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]