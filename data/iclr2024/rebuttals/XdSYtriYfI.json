[
    {
        "title": "Federated Ensemble-Directed Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "7mL5Vr4193",
            "forum": "XdSYtriYfI",
            "replyto": "XdSYtriYfI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5861/Reviewer_mieC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5861/Reviewer_mieC"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers federated reinforcement learning problems with offline datasets of different qualities.\nInstead of aggregating models in a uniform way, FEDORA adopts the principle of maximum entropy to aggregate local models accordingly. Specifically, the model from offline datasets with higher quality weights more in the central aggregation.\nIt is empirically shown that FEDORA outperforms FedAvg in both simulated environments and real-world experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The design of FEDORA is well-written and easy to follow.\n2. The paper considers a novel offline federated reinforcement learning where local datasets are generated by different behavioral policies.\n3. FEDORA has been evaluated in various environments and has achieved outstanding performance w.r.t. traditional FedAvg."
                },
                "weaknesses": {
                    "value": "1. What role does the entropy regularizer in Eq. (5) play? Collapsing to datasets with the best performance does not seem to worsen the convergent performance of FEDORA. Is there any ablation study on this term?\n2. The approach toward computing $Q_{t_i}$ and $\\pi_{t_i}$ in Section 5.1 is not clearly described. The evaluation of Q functions and policy may be inaccurate on datasets of low quality. How will it influence the performance of FEDORA?"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5861/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5861/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5861/Reviewer_mieC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5861/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674636272,
            "cdate": 1698674636272,
            "tmdate": 1699636620842,
            "mdate": 1699636620842,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1aryO8PEAM",
                "forum": "XdSYtriYfI",
                "replyto": "7mL5Vr4193",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer mieC"
                    },
                    "comment": {
                        "value": "Thank you very much for your comments and suggestions. We are delighted to know that you find our paper well written, our framework novel, and our experiments extensive.\n\n In response to your question regarding the entropy regularizer paramater $\\beta$, **we have now conducted experiments**, please see **Appendix E.3**.  We believe that we have addressed all your concerns, and we sincerely hope that the reviewer would consider increasing their score.\n\nBelow we the concerns raised in your review. \n\n**Q1.** *``What role does the entropy regularizer in Eq. (5) play''*\n\n**Response:** The role of the entropy regularizer is to modify the ensemble scheme of our algorithm.  When $\\beta=0$, it reduces to a uniform weighting scheme, where the quality of data present in each client is not considered during federation. As $\\beta \\rightarrow \\infty$ it tends to a max weighting scheme, where the federated policy is the same as an individual client's policy with the highest quality data. In response to your question, we have **now conducted additional experiments** with a hyperparameter sweep for different values of $\\beta$, see Appendix E.3.\n\n\n**Q2.** *``The approach toward computing $Q_t$ and $\\pi_t$ in Section 5.1 is not clearly described''*\n\n**Response:** $Q_i^t$ and $\\pi_i^t$ are the local critic and actor at client $i$ in round $t$. It is computed by minimizing the loss in Eq. 9 and Eq. 10 respectively. We follow an ensemble learning approach where in we weigh clients according to the quality of their datasets, and initialize the local critic and actor during each round of federation to the federated actor and critic. This ensures that each client starts off with the knowledge possessed by clients participating in federation. We then eventually down weigh clients who perform poorly  and decay the influence of local data. Thus, although a client individually has low quality data, using actor and critic trained over the ensemble to initialize the training process, and decaying the influence of local data, ensures the clients of datasets of low quality do not affect the performance of FEDORA. Please note that we have already explained these steps modularly in Section 5.1, 5.2, 5.3 and 5.4."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550413233,
                "cdate": 1700550413233,
                "tmdate": 1700606077977,
                "mdate": 1700606077977,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8oTV9Xw8Sz",
            "forum": "XdSYtriYfI",
            "replyto": "XdSYtriYfI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5861/Reviewer_oNBT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5861/Reviewer_oNBT"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of federated offline reinforcement learning, aiming to learn a high-performing policy by leveraging multiple distributed offline datasets without the need for centralizing the data. The authors highlight several challenges inherent in this setup and propose a framework called FEDORA (Federated Ensemble-Directed Offline Reinforcement Learning Algorithm) to tackle these challenges in the context of federated offline reinforcement learning. To evaluate the effectiveness of FEDORA, the authors conduct experiments on mujoco environments, demonstrating its capability to achieve desirable results. Furthermore, the proposed framework is deployed on real turtlebots, providing practical validation of its performance and applicability in real-world scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper addresses the intriguing problem of federated offline reinforcement learning, which has received limited attention in the existing literature. The authors shed light on various technical challenges that emerge from this unique setting. To tackle these challenges, they propose an innovative algorithm called FEDORA. The efficacy of FEDORA is demonstrated through its successful application to continuous control tasks in mujoco environments, showcasing its ability to learn effective policies. Moreover, the authors validate the practicality and real-world applicability of the proposed framework by deploying it on real turtlebots, further highlighting its performance under realistic scenarios."
                },
                "weaknesses": {
                    "value": "While the paper introduces a novel approach to an interesting problem, there are several areas that could benefit from further improvement:\n1. The assumption of identical MDPs and reward functions across all agents limits the generalizability and applicability of the proposed approach to real-world scenarios. Exploring techniques to handle heterogeneity among agents' environments could enhance the practicality of the framework.\n2. The requirement for the server to have access to the complete MDP raises concerns about privacy and may not be practical in many scenarios. Providing a practical example or discussing potential alternatives for this setup would strengthen the paper's applicability.\n3. The paper assumes **knowledge of dataset quality**, but it does not thoroughly address the issue of comparing and assessing the quality of different datasets. Specifically, in Sec. 4 , is the dataset generated by hopper-expert-v2 guaranteed to be better than that generated by hopper-medium-v2? Suppose that $D_m$ is generated by the medium policy which has converged while $D_e$ is generated by the expert policy during the initial random exploration stage. Is the quality of $D_e$ always better than $D_m$ ? It is important to explore methods to evaluate dataset quality more robustly and consider scenarios where the assumption of one dataset being consistently better than another may not hold.\n4. Experimental details for Figure 1 are missing, making it difficult to fully understand and interpret the results. Including the specific experimental setup, including hyperparameters, training procedures, and any other relevant details, would enhance the reproducibility and credibility of the findings.\n5. Eq. (7) implies that the server selects the action based on a weighted average of all agents' actions. Further clarification is needed to explain how the weights assigned to individual agents' actions influence the final decision. Does the weighting scheme prevent one agent from significantly overpowering the others? If not, what mechanisms or strategies are in place to address potential dominance issues and ensure a balanced contribution from all participating agents?\n6. The process of training the centralized policy to achieve the straight-line performance in Figure 2 requires further explanation.\n7. Can you compare the performance of FEDORA with just the expert policy running the offline RL algorithm in Fig. 2?\n8. Some important FedRL papers are missing from reference. Including them would strengthen the scholarly contribution of the paper."
                },
                "questions": {
                    "value": "1. Could you provide a compelling real-world example that demonstrates the practicality of the problem setup? By illustrating a specific scenario or application where the proposed approach can be effectively applied, readers can gain a better understanding of its relevance and potential impact.\n2. How do you determine the quality of a given dataset?\n3. How is the value of $\\beta$ determined, and what insights can you provide regarding its impact on the weights assigned to individual agents?\n4. Could you provide a brief discussion or analysis of the computational and communication costs associated with the proposed approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5861/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5861/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5861/Reviewer_oNBT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5861/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711534962,
            "cdate": 1698711534962,
            "tmdate": 1699636620742,
            "mdate": 1699636620742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "202eQac3Tf",
                "forum": "XdSYtriYfI",
                "replyto": "8oTV9Xw8Sz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oNBT (1/2)"
                    },
                    "comment": {
                        "value": "This is a **copy-paste review** of our earlier NeurIPS submission.  We are afraid that the review for our NeurIPS submission was full of misconceptions, which are repeated in this ICLR review.  The reviewer had not responded to our clarifications for NeurIPS, and we sincerely hope that the reviewer will find time to respond to our clarifications this time around, and reconsider and increase their score.  Thank you for your time.\n\n**Q1.** *``The requirement for the server to have access to the complete MDP raises concerns...''* \n\n**Response:** **We are afraid that the reviewer had a misunderstanding here.** We **do not assume** that the server has access to the MDP, simulator or any data present in the clients. Furthermore, the clients do not have access to the MDP or a simulator.  Indeed, this is the fundamental premise of offline RL. We would appreciate it if you could point out the lines in our paper that caused the misunderstanding, and we will be happy to fix them.  \n\n**Q2.** *``The paper assumes knowledge of dataset quality, but ..''* \n\n**Response:**  **We are afraid that the reviewer had a misunderstanding here.** In our setting, the clients or server are **not aware** of the quality of the data they possess. Indeed, the lack of this knowledge is why we develop the ensemble-directed learning approach, under which estimating the quality of local polices and critics is critical, and which is a fundamental contribution of our work.  We would appreciate it if you could point out the lines in our paper that caused the misunderstanding, and we will be happy to fix them. \n\n**Q3.**  *``Experimental details for Figure 1 are missing, making it difficult to ...''* \n\n**Response:** The basic description of the experimental setting for the results shown in Figure 1 is provided in Section 4 right above Fig. 1. More detailed description of all our experiments are given in Section 6 and in Appendix A. We have also provided the entire code base from where  you can run the experiments in the supplementary material. \n\n**Q4.** *``Eq. (7) implies that the server selects the action based on a weighted average of all agents' actions.''* \n\n**Response:**  **We are afraid that the reviewer had a misunderstanding here.**  **The server does not take any action at all.**  As with all federated learning approaches, the server merely computes the federated policy using the local policies of the clients, which  is then sent back to the clients (see algorithms 1 and 2). The weights are based on the estimated quality of the data present in each client, and the weighting scheme ensures that the well performing policies are given more importance. \n\n**Q5.** *``The process of training the centralized policy to achieve the straight-line performance in Figure 2 requires further explanation.''*\n\n**Response:** Please note that the straight line is not a training curve, rather the performance of the policy trained using TD3-BC on the combined data present in all clients.  This is mentioned in Section 6.\n\n**Q6.** *``Some important FedRL papers are missing from reference. Including them would strengthen the scholarly contribution of the paper.''*\n\n**Response:** We did a through literature survey and cited the most of relevant papers that we are aware of. We will be happy to include more papers in our literature survey if the reviewer could kindly give pointers to any relevant papers that are missing from our references. \n\n**Q7.**  *``Can you compare the performance of FEDORA with just the expert.''*\n\n**Response:** Please note that we have indeed done this.  In  Figure 1,  we compare the performance of FEDORA with the performance of the policy obtained by training on the expert data present in an individual client.\n\n **Q8.**  *``Could you provide a compelling real-world example that demonstrates the practicality $\\dots$*\n\n**Response:** We illustrated the value proposition of the approach using a mobile robotics application, wherein each robot collects data independently and privately via arbitrary policies, and our approach is successful at distilling their collective knowledge of navigation without sharing any data (video in supplementary material).  The scenario has several applications, such as that of cleaning robots, which must collaboratively learn to navigate within homes, without sharing data that might violate home owners' privacy.  Much the same scenario applies to factory floor or warehouse robots that must collaboratively learn to navigate, without sharing data that would stress the limited network bandwidth available.\n\n**Q9.** *``How do you determine the quality of a given dataset?''*\n\n**Response:** We use a variant of off-policy evaluation, where the clients uses its local data and the federated/local critic to evaluate the quality of its dataset. (See section 5.1 and 5.2)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549837276,
                "cdate": 1700549837276,
                "tmdate": 1700549856077,
                "mdate": 1700549856077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JpYQfQu5TO",
                "forum": "XdSYtriYfI",
                "replyto": "8oTV9Xw8Sz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oNBT (2/2)"
                    },
                    "comment": {
                        "value": "**Q10.** *``How is the value of $\\beta$ determined''* \n\n**Response:** In our setup, we treat $\\beta$ as a hyperparameter and use a fixed value of $0.1$ for all our experiments. When $\\beta=0$, it boils down to a uniform weighting scheme, where the quality of data present in each client is not considered during federation. As $\\beta \\rightarrow \\infty$ it tends to a max weighting scheme, where the federated policy is the same as an individual client's policy. \n\n\n**Q11.**  *``Could you provide a brief discussion  or analysis of the computational and communication $\\dots$\"*\n\n**Response:**  The computation and communication costs are similar to standard federated learning algorithms. That is why we omitted the details. In short: \n\n**Client side:** A subset of participating clients initializes its local policy to the federated policy and performs gradient updates using local data, and evaluates the performance of the learnt policy using the local data. **Server side:** Performs a weighted-averaging of the weights of the actor and critics of the participating clients. **Communication:** The subset of participating clients communicates back the weights of actor and critic to the server. The server broadcasts the federated actor and critic to all clients."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549996836,
                "cdate": 1700549996836,
                "tmdate": 1700549996836,
                "mdate": 1700549996836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gCTJj75ClX",
            "forum": "XdSYtriYfI",
            "replyto": "XdSYtriYfI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5861/Reviewer_RUBc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5861/Reviewer_RUBc"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel federated ensemble-directed offline reinforcement learning algorithm (FEDORA) that enables multiple agents with heterogeneous offline datasets to collaboratively learn a high-quality control policy without sharing data. The key contributions are:\n1. Identifying key challenges of federated offline RL including ensemble heterogeneity, pessimistic value computation, and data heterogeneity.\n2. Systematically addressing these challenges through an ensemble-directed federation approach that extracts collective wisdom of policies and critics and discourages over-reliance on possibly irrelevant local data.\n3. Demonstrating strong empirical performance of FEDORA over baselines on MuJoCo environments and a real-world robot navigation task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper clearly motivates the problem of federated offline RL and identifies key challenges that are not addressed by naively combining existing methods.\n2. FEDORA is systematically designed to address the identified challenges in an intuitive manner through techniques like ensemble-directed federation and federated optimism.\n3. Extensive experiments demonstrate the effectiveness of FEDORA over baselines on simulated and real-world tasks. The ablation studies provide useful insights.\n4. The paper is clearly written and provides sufficient details to understand the proposed techniques."
                },
                "weaknesses": {
                    "value": "1. How does FEDORA perform when some clients have near-random or adversarial datasets?\n2. Have the authors experimented with different values of \u03b2 and \u03b4? Is there a principle behind setting them?\n3. How does FEDORA compare to state-of-the-art offline RL algorithms like CQL or IQL in the federated setting?\n\nThe paper makes solid contributions in identifying and addressing key challenges in federated offline RL. The algorithm design is methodical and supported through extensive experiments. More theoretical and implementation details will further improve the paper."
                },
                "questions": {
                    "value": "see above\u3002"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5861/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769804971,
            "cdate": 1698769804971,
            "tmdate": 1699636620640,
            "mdate": 1699636620640,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GPtIwzhQrM",
                "forum": "XdSYtriYfI",
                "replyto": "gCTJj75ClX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RUBc"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We are encouraged to hear that our *\"paper makes solid contributions in identifying and addressing key challenges in federated offline RL\"* and our *\"algorithm design is methodical and supported through extensive experiments.\"* \n\nIn response to your question regarding the hyper-parameter $\\beta$ we have **conducted additional experiments** by varying it in the Hopper-v2 environment, please see Appendix E.3. \n\nBelow, we give the detailed response to your comments. We believe that we have addressed all your concerns, and we sincerely hope that the reviewer would consider increasing their score.\n\n**Q1.**  *``How does FEDORA perform when some clients have near-random or adversarial datasets?''*\n\n**Response:** Thank you for your question. In our paper, we have already shown the the performance of FEDORA when some clients have random data. Specifically,   Fig.3 shows the superior performance of FEDORA when $25$ clients have random data and $25$ clients have expert data. Further, Fig.11  in the appendix shows the  effects of clients having data from multiple behavior policies for varying proportions of clients participating in federation. Here, 12 clients have expert data, 12 clients have medium data, 14 clients have random data, and 12 clients have data from the replay buffer of a policy trained up to the performance of the medium agent. We observe that FEDORA performs well even this scenario with clients having data from multiple policies.      \n\n\n**Q2.**  *``Have the authors experimented with different values of $\\beta$ and $\\delta$? Is there a principle behind setting them?''*\n\n**Response:** Thank you for your question. In our setup, we treat $\\beta$ as a hyperparameter and use a fixed value of $0.1$ for all our experiments. In response to your question, we have now conducted additional experiments with different values of $\\beta$, see  Appendix E.3.  When $\\beta=0$, it reduces to a uniform weighting scheme, where the quality of data present in each client is not considered during federation. As $\\beta \\rightarrow \\infty$ it tends to a max weighting scheme, where the federated policy is the same as an individual client's policy with maximum value. We would like to point out that we do not have a $\\delta$ parameter in our work.\n\n\n**Q3.** *``How does FEDORA compare to state-of-the-art offline RL algorithms like CQL or IQL in the federated setting?''*\n\n**Response:**  We designed FEDORA as a framework where we can use use any actor-critic based offline RL algorithm in the clients. We chose TD3-BC due to its simplicity and superior performance over other baselines such as CQL, AWAC, Fisher-BRC, and BRAC-Q. We kept the local offline RL algorithm consistent throughout our simulations for fair comparisons. We believe that if we use a superior local offline RL algorithm, the performance of FEDORA might be better."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549035899,
                "cdate": 1700549035899,
                "tmdate": 1700605805334,
                "mdate": 1700605805334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SoSlNgCmtY",
            "forum": "XdSYtriYfI",
            "replyto": "XdSYtriYfI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5861/Reviewer_MJbF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5861/Reviewer_MJbF"
            ],
            "content": {
                "summary": {
                    "value": "This study examines the issue of training offline RL in a federated environment. Initially, the authors outline the difficulties associated with federated offline RL. Following this, a method named FEDORA is introduced, which aims to enhance the collective performance of federated learning. FEDORA accomplishes this by managing how models are aggregated and by reducing the influence of local training data on the global model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Clarity and comprehensibility: The manuscript is articulated clearly. The content is easy to understand.\n\n- Relevance and novelty: This paper focuses on a new problem for RL training (i.e. federated offline RL).\n\n- Insightful discussion: The presentation of challenges related to federated offline RL is clear and insightful."
                },
                "weaknesses": {
                    "value": "- The paper claims that it outperforms the centralized training offline RL, but the proposed method uses an additional data selection technique compared to the baselines, which is not quite fair. To make this claim, the baseline needs to also have data rebalancing or data selection technique (e.g. with [1], or just send the average reward of all episodes to the server).\n\n- The title is \"ensemble\" method, but it seems the solution is not really using an ensemble method. It's a modified federated aggregation method.\n\n- Even though the proposed method is about federated RL, related work and evaluation should be considered for supervised RL with heterogeneous data as well.\n\n[1] Yue, Yang, et al. \"Boosting offline reinforcement learning via data rebalancing.\" arXiv preprint arXiv:2210.09241 (2022)."
                },
                "questions": {
                    "value": "- The proposed technique seems to be doing a data selection based on the quality of the data, would you get a similar performance if you just select the clients with higher reward datasets? Like in [1] for example.\n \n- Would the proposed method increase the amount of communication required for federated training compared to simple FAvg? It would be nice to quantify that.\n\n\n\n[1] Yue, Yang, et al. \"Boosting offline reinforcement learning via data rebalancing.\" arXiv preprint arXiv:2210.09241 (2022)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5861/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780344061,
            "cdate": 1698780344061,
            "tmdate": 1699636620535,
            "mdate": 1699636620535,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EeRNEXOAZg",
                "forum": "XdSYtriYfI",
                "replyto": "SoSlNgCmtY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5861/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MJbF"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their comments and suggestions. We are delighted to know that the reviewer finds  our paper well articulated, and the contents easy to follow. We are happy to know that the reviewer considers our problem framework novel. **In response to the reviewer's questions, we have conducted additional experiments and have added them to the appendix of our paper.** Below, we give detailed responses to  the specific questions, and we hope they address all the concerns and comments by the reviewer. \n\n**Q1** *``The paper claims that it ...the baseline needs to also have data rebalancing or data selection technique (e.g. with [1]( [Yue et al, 2022]), or just send the average reward of all episodes to the server)''*\n\n**Response:** Based on the reviewer\u2019s comments, we have now added an additional baseline, TD3-BC with data rebalancing, which uses the the data rebalancing approach from [Yue et al, 2022], see Appendix E.1. We notice that data rebalancing does help, performing better than TD3-BC. However, its performance is still worse than our FEDORA algorithm. We believe this is due to the exponential weighting employed by FEDORA, and that data rebalancing methods do not completely solve the\ndistribution shift issue that arises from combining data from multiple behavior policies. We would like to emphasize that the centralized offline RL algorithms serve only as a hypothetical baseline, because our setting federated learning without sharing the data.\n\n**Q2.**  *``The title is \"ensemble\" method ...''*\n\n**Response:** We believe that our approach bears a strong resemblance to ensemble learning in that we have a set of experts of heterogeneous qualities, whose collective wisdom we must combine together to obtain a high-quality policy. This is similar to ensemble learning, where weights must be determined for the different experts. Furthermore, we are constrained to evaluation only using local data. Hence, we also have an ensemble of critics, which must also be jointly combined to obtain an overarching critic. Hence, we named the algorithm as ensemble-directed to reflect its dependence the ensemble of actors and critics.\n\n\n**Q3.**  *``Even though the proposed method is about federated RL ...''*\n\n**Response:** We have now added references to two works ([Yue et al, 2022], [Yue et al, 2023]), thanks to the pointer by the reviewer in Q1. We will be happy to include more references if the reviewer has additional suggestions.\n\n**Q4.** *``The proposed technique ... would you get a similar performance if you just select the clients with higher reward datasets? Like in [1] ([Yue et al, 2022]) for example..''* \n\n**Response:** To answer this question, we have now **conducted two additional experiments (See Appendix E.2)**: $(i)$ we select clients based only on the rewards in their datasets using the weighing method proposed in [Yue et al, 2022], $(ii)$ we keep all the components of FEDORA such as optimistic critic, decaying the influence of local data, and the proximal term, but only modify the weighing mechanism, and use a similar method proposed in [Yue et al, 2022].  \n\nThe experiment results show that  selecting clients based only on the quality of data (i.e., method $(i)$) performs poorly, as there are several other relevant components that are needed  to ensure reasonable performance of federated RL with offline data. In method $(ii)$, we included all these additional components (optimistic critic, adding appropriate regulations, and decaying the influence of local data) but used the weighing mechanism suggested in [Yue et al, 2022]. The experiment results show that method $(ii)$ performs better than method $(i)$, but FEDORA still outperforms  it. This can be attributed to FEDORA's superior weighing strategy, and the holistic approach we took in designing it by augmenting different algorithmic components that help mitigate the problems of federated offline RL described in section 4.1.\n\n**Q5.**  *``Would the proposed method increase the amount of communication ..  compared to simple FAvg?''*\n\n**Response:** The communication required for  FEDORA  is similar to FedAvg. In FedAvg, during each round of federation, the subset of participating clients communicate  the  actor and critic parameters to the server. The server broadcasts the federated actor and critic parameters to all clients. In FEDORA, in addition to this, we only need to communicate a scalar weighing factor  which adds a negligible overhead.\n\n [1] Yue, Yang, et al. \"Boosting offline reinforcement learning via data rebalancing.\" arXiv preprint arXiv:2210.09241 (2022)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5861/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548366740,
                "cdate": 1700548366740,
                "tmdate": 1700605468830,
                "mdate": 1700605468830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]