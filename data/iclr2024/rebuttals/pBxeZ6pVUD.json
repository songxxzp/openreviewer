[
    {
        "title": "Grounded Object-Centric Learning"
    },
    {
        "review": {
            "id": "6J9m8cW1ED",
            "forum": "pBxeZ6pVUD",
            "replyto": "pBxeZ6pVUD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6936/Reviewer_myKR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6936/Reviewer_myKR"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Conditional Slot Attention (CoSA), a novel variant of Slot Attention that incorporates the concept of grounded representations. Unlike the original Slot Attention, CoSA utilizes a dynamic binding scheme using canonical object-level property vectors and parametric Gaussian distributions. This approach enables specialized slots that remain invariant to identity-preserving changes in object appearance. The proposed method is evaluated on multiple downstream tasks, including scene generation, composition, and task adaptation, and achieves competitive performance compared to Slot Attention in object discovery benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Unsupervised object discovery remains a challenge and an open question in the research community.\n2. The concept of the Grounded Slot Dictionary (GSD) module is logical, particularly the construction of a dictionary as outlined in Definition 1.\n3. The visualization of GSD binding in Figure 3 is both interesting and insightful for the community, providing evidence of the effectiveness of the GSD approach."
                },
                "weaknesses": {
                    "value": "1. I would appreciate more ablation studies in the experiments section. The current version primarily presents the state-of-the-art (SOTA) performance for two case studies, but additional ablation studies would provide further insights into the specific contributions and the impact of different components or techniques employed in the proposed method.\n2. The author mentions that the method incorporates the object-level property vector, but there is a lack of evidence regarding how it functions. For instance, it is unclear whether the method can effectively discriminate between multiple instances with similar appearances. \n3. The visualization of the COCO results does not appear to be accurate, and it seems that the method may not be as applicable to real-world scenarios."
                },
                "questions": {
                    "value": "1. Besides performance gains, what evidence does the paper provide to show that the object-level property vectors are working effectively?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698325133736,
            "cdate": 1698325133736,
            "tmdate": 1699636808795,
            "mdate": 1699636808795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cvxj6Eyyuf",
                "forum": "pBxeZ6pVUD",
                "replyto": "6J9m8cW1ED",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6936/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments and constructive feedback. We very much appreciate the fact that our framework was received as logical, interesting, and insightful for the community. \n\n> I would appreciate more ablation studies in the experiments section. The current version primarily presents the state-of-the-art (SOTA) performance for two case studies, but additional ablation studies would provide further insights into the specific contributions and the impact of different components or techniques employed in the proposed method. \n\nWe agree that the ablation can provide more insights about the method; we have included ablations on sampling, types of spectral decomposition, and sample efficiency on multiple tasks and datasets. Additionally, we have included the ablations on codebook size and its implications on results in the appendix.  \nIn case we are missing any ablations that would add to the clarity in the context of CoSA, please let us know.\n \n\n> The author mentions that the method incorporates the object-level property vector, but there is a lack of evidence regarding how it functions. For instance, it is unclear whether the method can effectively discriminate between multiple instances with similar appearances \n\nThe evidence of selecting object-level property vectors can be observed in both object discovery and reasoning tasks. In the case of object discovery, FG-ARI measures this property, whereas HMC measures the same for reasoning tasks measures this property, whereas for reasoning tasks, HMC measures the same.  \n\nSimilarly, in the case of Object discovery, we have added new results to qualitatively demonstrate the binding across multiple instances; please refer to Figure 19 in the appendix section.  \n\n \n> The visualization of the COCO results does not appear to be accurate, and it seems that the method may not be as applicable to real-world scenarios. \n\n We do observe that the results on the COCO dataset are not as good as supervised segmentation models, but it is important to note that the proposed model is fully unsupervised, and based on the initial number of slots hyperparameter, the model does segregate between different types of objects in the image. We leave further exploration in improving the model performance to match closely to the segmentation ground truths to future work. \n\n \n> Besides performance gains, what evidence does the paper provide to show that the object-level property vectors are working effectively? \n\nWe demonstrate the notion of binding and qualitatively as well as quantitively. Qualitatively, this can be seen in the grounded dictionary visualization section in the main paper (Figure 3) and in appendix section K.1 and section H.5. Quantitively, the results are reflected in terms of FG-ARI and HMC metrics."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399787950,
                "cdate": 1700399787950,
                "tmdate": 1700399787950,
                "mdate": 1700399787950,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JR7sOQt6uW",
                "forum": "pBxeZ6pVUD",
                "replyto": "Cvxj6Eyyuf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6936/Reviewer_myKR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6936/Reviewer_myKR"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. It addresses most of my concerns. I believe this paper presents significant insights and contributions to slot attention and object discovery. Thus, I will maintain my current rating."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503133902,
                "cdate": 1700503133902,
                "tmdate": 1700503133902,
                "mdate": 1700503133902,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VPpM1tKl7F",
            "forum": "pBxeZ6pVUD",
            "replyto": "pBxeZ6pVUD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6936/Reviewer_yF2M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6936/Reviewer_yF2M"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on grounded object-centric learning which can bind to specific object types. To achieve this goal, the authors take inspiration from Slot Attention, and introduce a Grounded Slot Dictionary to encode object properties and bind to different object types. This dictionary enables the model to conditionally sample the slots from different distributions. And the reasoning module with property transformation module enhances the interpretability of object property binding. The experiments show improvements on various object discovery benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The motivation is clear and makes sense. It takes inspiration from the recent research in binding problem and concludes three major challenges in unsupervised object discovery. And it takes the binding to canonical object properties and types as the primary problem to help simultaneously solve all three challenges.\n2. The design of conditional slot attention is novel. It employs the spectral decomposition for discrete mapping and enables the model to sample from different distributions corresponding to different object properties.\n3. The visualization of the separated object properties are interesting and shows interpretable Ground Slot Dictionary."
                },
                "weaknesses": {
                    "value": "1. The experiments on more complex scenes are required. For example, multiple instances of the same object category, it would be interesting to show the property binding ability in this case.\n2. Does the model build on pre-trained backbones? Or a randomly initialized encoder $\\Phi$ is also sufficient to provide cues for spectral decomposition and discritization?\n3. I suggest authors to run the trained conditional slot attention with grounded slot dictionary on some video data, e.g., DAVIS-2017, to validate whether the slot dictionary can track objects across time and more vividly show the binding ability to object types."
                },
                "questions": {
                    "value": "The conventional slot attention based methods use reconstruction loss as the self-supervised objective to guide object decomposition. What is the relationship between the objectives used in this work and the recounstruction loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682551295,
            "cdate": 1698682551295,
            "tmdate": 1699636808656,
            "mdate": 1699636808656,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RrMU3eCxzA",
                "forum": "pBxeZ6pVUD",
                "replyto": "VPpM1tKl7F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6936/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments and constructive feedback. We very much appreciate the fact that our work was found to be well-motivated, novel, and the illustrations were found interesting and interpretable. \n\n> The experiments on more complex scenes are required. For example, multiple instances of the same object category, it would be interesting to show the property binding ability in this case. \n\nWe share the reviewers view on examining the binding ability of the framework on multiple object instances of the same object. Given that CLEVR has several of these examples in the dataset, we included a section in Appendix H.5, where we demonstrate the reusability of dictionary elements based on repeated object instances. As illustrated in Figure 19, similar looking object slot representations are sampled from the same distribution. \n\n \n> Does the model build on pre-trained backbones? Or a randomly initialized encoder is also sufficient to provide cues for spectral decomposition and discritization? \n\nFor CLEVR, ObjectsRoom, Bitmoji, FFHQ, and Tetrominoes datasets we start with a randomly initialized encoder. \nFor COCO, given the very high diversity of image content, we start with a pre-trained image encoder similar to DINOSAUR and apply CoSA on the extracted features. \n \n\n> I suggest authors to run the trained conditional slot attention with grounded slot dictionary on some video data, e.g., DAVIS-2017, to validate whether the slot dictionary can track objects across time and more vividly show the binding ability to object types. \n\nWe thank the reviewer for the great suggestion. We agree that extending the proposed method to a large-scale video dataset like DAVIS-2017 would be interesting and valuable. However, at this stage, we believe that this would be best served by a dedicated investigation in future work. \n\nNonetheless, to reassure the review of our method\u2019s ability to correctly identify and bind multiple object instances, we have added a discussion section and some illustrations using  CLEVR in the appendix (section H.5). \n\n  \n\n> The conventional slot attention based methods use reconstruction loss as the self-supervised objective to guide object decomposition. What is the relationship between the objectives used in this work and the reconstruction loss?  \n\nThe objective function in our work can be viewed as the sum of reconstruction loss (likelihood loss) + KL regularization. The additional KL term is used to train the strategies of selecting the elements from GSD for a given image.  \n\nOur work introduces a probabilistic perspective of slot attention by optimizing a principled variational lower bound of the log likelihood of the data. The image likelihood we maximize is related to the reconstruction loss in previous works in the sense that minimzing a MSE loss is analogous to maximizing a pixel-wise factored Gaussian likelihood."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399770288,
                "cdate": 1700399770288,
                "tmdate": 1700399770288,
                "mdate": 1700399770288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aWWwJ428k2",
                "forum": "pBxeZ6pVUD",
                "replyto": "RrMU3eCxzA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6936/Reviewer_yF2M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6936/Reviewer_yF2M"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. However, I am more interested in the model's ability to discriminate different instances of same semantics in realistic scenes, instead of the synthetic CLEVR dataset. From Fig.19, it is clear that it is feasible to discriminate objects solely by the color and shape or size, which is much simpler than the real world applications. Since DAVIS-2017 contains some videos that consist of multiple instances of same semantics, e.g., dogs-jump, goldfish, soapbox, I kindly request the authors to just run inference without retraining on such video sequences and show the visualization on consecutive frames."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700573025679,
                "cdate": 1700573025679,
                "tmdate": 1700573025679,
                "mdate": 1700573025679,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nq0L3GTBYz",
                "forum": "pBxeZ6pVUD",
                "replyto": "VPpM1tKl7F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6936/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and feedback. \n\nBased on your suggestion, we have included inference results on a video from the DAVIS dataset. Please refer to Figure 20 in the appendix, even in this case, we can observe that the CoSA consistently binds foreground and background slots to a particular GSD element."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700597651601,
                "cdate": 1700597651601,
                "tmdate": 1700636276069,
                "mdate": 1700636276069,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NaKdUXd9HU",
                "forum": "pBxeZ6pVUD",
                "replyto": "nq0L3GTBYz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6936/Reviewer_yF2M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6936/Reviewer_yF2M"
                ],
                "content": {
                    "comment": {
                        "value": "After reading the resutls in the appendix, I agree with the author's claim that CoSA consistenty binds foreground and background slots to a particular GSD element. However, the author only shows the results on simple one foregroud (a camel) vs background, this foreground background separation and binding attribute has been verified in [1] with very simple query slot attention. Simply replacing the slot initialization with learnable queries equips the model with this binding ability. I still insist that the authors should validate the binidng attribute of in more complex video sequences with multiple objects to show the advantage of the proposed framework over the simple query slot attention. It is important to show whether the learned model consistency bind specific semantics or instances in a video sequence to particular GSD elements.\n\n[1] Jia, B., Liu, Y., & Huang, S. Improving object-centric learning with query optimization. ICLR 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664873797,
                "cdate": 1700664873797,
                "tmdate": 1700664873797,
                "mdate": 1700664873797,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6WghMzWynD",
            "forum": "pBxeZ6pVUD",
            "replyto": "pBxeZ6pVUD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6936/Reviewer_dFhu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6936/Reviewer_dFhu"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Conditional Slot Attention (CoSA) that uses a Grounded Slot Dictionary (GSD) to sample initial slots from a shared library of canonical object-level property vectors. Spectral decomposition is used to estimate the number of initial slots $K$ and vector quantization is used to select initial slot distributions from the GSD. The authors run experiments on object discovery, scene composition and generation, and downstream task adaptation showing benefits over previous methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-motivated and the approach is novel, as far as I know. The authors show improvements over previous methods in terms of FG-ARI and downstream task performance and include additional experiments and analyses in the Appendix."
                },
                "weaknesses": {
                    "value": "- The paper is missing some ablations that I think would be important in evaluating the significance of the method:\n    - How much of the improved performance is from just predicting the number of slots instead of using a fixed $K$? What if we use the predicted number of slots without the GSD?\n    - Conversely, how much is attributed to the GSD? What if we used a fixed $K$ with GSD?\n- What is the distribution of the number of objects for the different datasets? This would be important to interpreting the MAE values.\n- While I can appreciate the probabilistic interpretation of the model, I feel it does not add to the clarity of the paper and may be more appropriate for the appendix. Specifically, if I understand correctly, if $q(\\tilde{z}|x)$ is deterministic and $p(\\tilde{z})$ is uniform as in VQ-VAE, then the KL term is just a constant and not actually used to optimize the model?\n- The discussion of the different sampling strategies (Euclidean, Cosine, Gumbel) does not seem necessary for the main text since (from my understanding) the experiments in the main text are only done with the Cosine version? I do see additional experiments on other sampling strategies in the appendix, but if that is the case, this discussion can be removed from the main text."
                },
                "questions": {
                    "value": "- The codebook size seems like a potentially important parameter. What size do you choose for the different experiments? How sensitive are the results to codebook size?\n- I want to confirm that the Abstraction Module is not differentiable and just uses the output of the encoder, which is trained through the Slot Attention path. Then, there are similarly no gradients flowing through the GSD and it is only updated with EMA (Appendix F). Is this understanding correct? If so, I think this could be stated more explicitly for clarity in the main text.\n- How are the dynamic number of slots $K$ actually implemented during training? From my understanding, different images in a batch may have different $K$, so this may need to be done with some masking of the softmax in Slot Attention, in which case a max number of slots still needs to be used. In that case, is the benefit in FLOPS only during inference for a single image?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6936/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6936/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6936/Reviewer_dFhu"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6936/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865078475,
            "cdate": 1698865078475,
            "tmdate": 1700684944637,
            "mdate": 1700684944637,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xuWObBe27X",
                "forum": "pBxeZ6pVUD",
                "replyto": "6WghMzWynD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6936/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6936/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed comments and constructive feedback. We very much appreciate the generally positive outlook and the fact that our work was found to be well-motivated and novel.  \n\n> How much of the improved performance is from just predicting the number of slots instead of using a fixed K? What if we use the predicted number of slots without the GSD? \n\nOur apologies for the confusion. Although it is technically possible to perform dynamic estimation of the number of slots K at training time using our method, it requires masking of surplus slots, which somewhat complicates the training pipeline without providing guaranteed benefits. Our method was used at inference time only; we have made this clear in the revised version of the paper. \n\nTherefore, comparing performance improvements from training with the dynamic estimation of K versus the GSD is not applicable, since the GSD was always trained with fixed K. If we remove the GSD, we end up with the standard Slot-Attention model, which we compare with in our ablation study (e.g. see the SA model in Table 1, Table 2 etc).  \n\n \n> What is the distribution of the number of objects for the different datasets? This would be important to interpreting the MAE values. \n\nThanks for highlighting this point, we have updated the dataset information in appendix section C, with the distribution of the number of objects for the different datasets.  \n\n\n> While I can appreciate the probabilistic interpretation of the model, I feel it does not add to the clarity of the paper and may be more appropriate for the appendix. Specifically, if I understand correctly, if q(z|x) is deterministic and p(z) is uniform as in VQ-VAE, then the KL term is just a constant and not actually used to optimize the model? \n\nWe thank the reviewer for the astute observation. It is true that under a uniform prior and deterministic sampling, the KL term drops to a constant. However, this is not the case for the Gumbel sampling scenario, as while the prior is still uniform, the sampling is stochastic. For this reason, the probabilistic exposition in our paper is designed to be generally consistent with any (discrete) representation learning setup.  \n\nWe have clarified this in the paper and have included additional background on quantization clarifying this in the appendix. \n\n \n\n> The discussion of the different sampling strategies (Euclidean, Cosine, Gumbel) does not seem necessary for the main text since (from my understanding) the experiments in the main text are only done with the Cosine version?  \n\nWe thank the reviewer for this suggestion; we have briefly introduced different types of sampling for the sake of completion, providing different options available for constructing the dictionary. We have included details of quantization and the sampling methods in the appendix.   \n\n\n> The codebook size seems like a potentially important parameter. What size do you choose for the different experiments? How sensitive are the results to codebook size? \n\nIn all our experiments, we initialize the codebook with a sufficiently large number of embeddings for the complexity of the datasets used. We\u2019ve included additional ablations and further discussion on codebook size in the appendix section H.4. \n\nHere are the ablation results on the CLEVR dataset. \n\n| M| MSE | FG-ARI | CBD | CBP |  \n| :----: |:----: |:----: |:----: |:----: |  \n| | | Cosine | | |  \n| M = 16 | 8.11  | 0.73 | 0.99 | 14.96 |  \n| M = 64 | 3.14 | 0.96 | 0.99 | 44.39 |  \n| M = 128 | 3.46  | 0.94 | 0.98 | 51.65 |  \n| M = 256 | 6.68  | 0.85 | 0.98 | 82.20 |  \n| | | Euclidian | | |  \n| M = 16 | 4.81 | 0.93 | 0.99 | 8.33 |  \n| M = 64 | 4.04 | 0.94 | 0.99 | 32.40 |  \n| M = 128 | 3.64 | 0.94 | 0.95 | 20.69 |  \n| M = 256 | 4.34 | 0.89 | 0.96 | 19.79 |  \n| | | Gumble | | |  \n| M = 16 | 6.64 | 0.88 | 1.0 | 11.06 |  \n| M = 64 | 3.82 | 0.93 | 1.0 | 29.62 |  \n| M = 128 | 3.89 | 0.93 | 0.99 | 27.16 |  \n| M = 256 | 5.46 | 0.92 | 0.99 | 30.28 | \n\n \n> I want to confirm that the Abstraction Module is not differentiable and just uses the output of the encoder, which is trained through the Slot Attention path. Then, there are similarly no gradients flowing through the GSD and it is only updated with EMA (Appendix F). Is this understanding correct?  \n\nYes, the reviewer is indeed correct; the abstraction module is nonparametric, but the gradients flow through it. \n\nIn the case of GSD, the key part of the dictionary $\\mathfrak{S}^1$ is updated via EMA, while the distribution parameters in $\\mathfrak{S}^2$ are updated with gradients from the SA module. \n\n\n> How are the dynamic number of slots actually implemented during training? \n\nDynamic slot selection during training requires masking of surplus slots, which complicates the training pipeline without providing guaranteed benefits. For this reason, we trained with a fixed number of slots K, and apply dynamic slot selection at inference time only."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700399744824,
                "cdate": 1700399744824,
                "tmdate": 1700399744824,
                "mdate": 1700399744824,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "INcIBiU72E",
                "forum": "pBxeZ6pVUD",
                "replyto": "xuWObBe27X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6936/Reviewer_dFhu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6936/Reviewer_dFhu"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to respond to my concerns and run additional ablation experiments. Given the response which addresses most of my questions and also reading the other reviews and responses, I've decided to increase my score to 6. However, I still do encourage the authors to simplify the main text by focusing on the Cosine variant which seems to be the one used in the main experiments. I feel like investigating different sampling strategies can almost form another paper and does not help in the understanding of the proposed method. \n\nI also could not find the updated text mentioning that estimating K is only done at inference time, although I may have missed it.\n\nMinor typo (that did not affect my decision): Gumble should be Gumbel"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6936/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684922917,
                "cdate": 1700684922917,
                "tmdate": 1700684922917,
                "mdate": 1700684922917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]