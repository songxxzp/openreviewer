[
    {
        "title": "Dissecting Zero-Shot Visual Reasoning Capabilities in Vision and Language Models"
    },
    {
        "review": {
            "id": "kghxn4NxE0",
            "forum": "UndmcWatBN",
            "replyto": "UndmcWatBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1914/Reviewer_PwJg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1914/Reviewer_PwJg"
            ],
            "content": {
                "summary": {
                    "value": "This paper systematically studies and compares the performance of LLMs and VLMs on visual reasoning problems using two synthetic datasets. The main findings are: 1. LLMs provided with ground-truth textual scene information perform better than those provided with visual embeddings. 2. CoT only helps when the scale of the LLMs is large enough. 3. They also analyze how the number of \u201creasoning steps\u201d, question types, and model scale affect the performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This is the first work to test VLMs and LLMs in visual reasoning tasks. It performs detailed experiments and analysis on both VLMs and LLMs models. \n2. The study is meaningful and provides some reference for both research in visual reasoning tasks and LLMs and LVLMs."
                },
                "weaknesses": {
                    "value": "1. The paper only studies one LVLMs (BLIP-2 FlanT5), the conclusions may not generalized to more recent LVLMs such as LLAVA and InstructBLIP.\n2. Some results and analysis are not very inspiring: E.g. 1. It is reasonable and not surprising that for synthetic visual reasoning tasks, providing ground truth metadata to LLMs is better than visual embedding processed by the model since ground truth metadata contains all the required information to reason the answer. 2. The observation that larger LLMs perform better using chain-of-thought has been discovered by prior work [1].\n3. The conclusion in the paragraph **Drawbacks of current VLM Architecture** is not well supported by the experiments. Instead, they are more like some hypothesis. \n\n[1] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
                },
                "questions": {
                    "value": "1. When providing explanations for the experiment result, the reviewer think it is better to provide some qualitative examples to support it. E.g. 'One possible explanation is that the objects in PTR are more complex, with multiple parts, hence the task for the VLM\u2019s visual frontend is more challenging, and more errors and uncertainty are introduced.'"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1914/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1914/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1914/Reviewer_PwJg"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697931621815,
            "cdate": 1697931621815,
            "tmdate": 1699636121939,
            "mdate": 1699636121939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ksy1drpM90",
            "forum": "UndmcWatBN",
            "replyto": "UndmcWatBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1914/Reviewer_13QZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1914/Reviewer_13QZ"
            ],
            "content": {
                "summary": {
                    "value": "This article systematically examine and benchmark the zero-shot visual reasoning capabilities of VLMs through synthetic datasets that require minimal world knowledge, and allow for analysis over a broad range of reasoning steps."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In my opinion, the most captivating aspect of this article is systematically examine and benchmark the zero-shot visual reasoning capabilities of VLMs through synthetic datasets that require minimal world knowledge. It can effectively separates actual visual reasoning capabilities from vast amounts of world knowledge obtained in VLM pre-training. Furthermore, the experimental strategy design is reasonable."
                },
                "weaknesses": {
                    "value": "While the initial insight is intriguing, as mentioned above, the experimental section falls short in effectively supporting the current viewpoints and uncovering more compelling conclusions. However, in the Chapter 5 LIMITATIONS AND FUTURE WORK, authors demonstrate a clear understanding of their weaknesses and identify the improvements of their research."
                },
                "questions": {
                    "value": "As mentioned above, I suggest that the authors further explore more varied visual reasoning tasks and evaluate the latest extensive VLM models on more datasets unrelated to world knowledge. While the initial insight is valid and the experimental strategy design is soundness, the execution and argumentation are lacking. If the authors address the points raised in the Chapter 5 and conduct a more comprehensive analysis to determine if VLM possesses genuine visual reasoning capabilities, I believe this work will generate significant interest within the VLM community. However, at its current stage, I find the work to be insufficient."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1914/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1914/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1914/Reviewer_13QZ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643099415,
            "cdate": 1698643099415,
            "tmdate": 1699636121868,
            "mdate": 1699636121868,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Rtyt0nRfZL",
            "forum": "UndmcWatBN",
            "replyto": "UndmcWatBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1914/Reviewer_An5H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1914/Reviewer_An5H"
            ],
            "content": {
                "summary": {
                    "value": "The paper is a straightforward investigation of two things. First, whether VLMs can outperform the corresponding LLMs they bootstrap from. Second, whether chain-of-thought prompting (\u201clet\u2019s think step by step\u2026\u201d) helps LLMs on structured visual understanding tasks (on CLEVR and PTR)."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Writing**:\n- The writing is easy to follow which is great.\n- The related work section is mostly thorough.\n\n**Results of interest**:\n1. That VLM performance could be lower than LLM performance on a visual task is somewhat surprising.\n2. In the CoT analysis: that Flan-T5 outperforms GPT-3.5 consistently is also surprising."
                },
                "weaknesses": {
                    "value": "**Criticisms/questions**:\n1. Are the VLMs and LLMs directly comparable? The VLMs only bootstrap from the pretrained LLMs. But after training, there\u2019s not much reason for them to be comparable.\n2. If they are directly comparable, why is VLM performance lower than LLM performance even when the VLMs have the scene metadata? They should be able to zero-out the contribution of the visual modality in each case. That should permit them to perform as well as language-only reasoning.\n3. Figure 2: to study scaling laws we need more than 2 datapoints. Given the BLIP family only offers two models, it could be useful to consider other model families. We'll also need error bars.\n\n**Writing**:\n- The paper could use some more sign-posting (e.g., there\u2019s nothing to introduce Section 4).\n- Some sections are really large (e.g., Section 4.1) and could be structured better.\n- Figure captions could be better/more informative (e.g., Figure 5 refers to a \u201ctop row\u201d and \u201cbottom row\u201d which are hard to find.)\n- With some compression (e.g., figures) the paper could fit 7 pages perhaps."
                },
                "questions": {
                    "value": "Please see weaknesses above, which I've tried to frame as questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828963546,
            "cdate": 1698828963546,
            "tmdate": 1699636121795,
            "mdate": 1699636121795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "9ucAnWPcF6",
            "forum": "UndmcWatBN",
            "replyto": "UndmcWatBN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1914/Reviewer_Jf6s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1914/Reviewer_Jf6s"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes Vision-language models (VLMs) and large language models (LLMs) on zero-shot visual reasoning. To minimize the impact of background knowledge on the inference result, the authors chose synthetic datasets including CLEVR and PTR. The main findings are: (1) LLMs are better than VLMs on average; (2) chain-of-thought is helpful only on 175B parameter GPT model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The choice of using synthetic data to focus on reasoning ability is well-motivated.\n- Performance analysis based on reasoning step, question family, model size, sheds some light on the behavior of VLMs and LLMs.\n- Performance was measured on GPT and FLAN-T5 across different parameter scale."
                },
                "weaknesses": {
                    "value": "- The paper has some interesting observations and hypothesis. However, majority of them are not backed up by experiments.\nFor example, for the claim \"visual information querying, where the model\u2019s visual frontend extracts scene\ndetails based on an initial text query,\" more evidence is appreciated. If this is indeed the case, the authors should try modifying initial text query to extract richer visual information. Another ways is leveraging a feedback loop as authors metioned as done in works like:\n(1) Haoxuan You et al., \"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models\" (2) Kaiwen Zhou et al., \"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models\"\n\nAuthors also should explore alternative possibilities such as scene information extracted by VLMs being incorrect.\n- Lacking insights. The authors classified the problems by # reasoning steps, question family (exist, count, etc). Why and how LLMs fail on these categories? Also, how LLMs compare on these categories compared to more traditional methods such as neuro-symbolic reasoners for CLEVR dataset? Are there any characteristic of visual reasoning problems that poses challenges to VLMs?\n- Only one kind of VLM is used. Hence, the conclusion from the current results may not generalize to other VLMs.\n- Writing and presentation could have been significantly improved. For example, figures could be condensed into numerical numbers in Tables."
                },
                "questions": {
                    "value": "- Please answer first two points on Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1914/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698914383913,
            "cdate": 1698914383913,
            "tmdate": 1699636121725,
            "mdate": 1699636121725,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]