[
    {
        "title": "Discovering Temporally-Aware Reinforcement Learning Algorithms"
    },
    {
        "review": {
            "id": "thJdGiWYB5",
            "forum": "MJJcs3zbmi",
            "replyto": "MJJcs3zbmi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5396/Reviewer_hAap"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5396/Reviewer_hAap"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the problem of discovering reinforcement learning algorithms via meta-learning. The paper reveals that incorporating temporal data regarding the agent's lifetime empowers the newly discovered algorithm to dynamically refine its objectives, such as the exploration-exploitation tradeoff, thereby fostering the development of a higher-performing agent. The proposed method can be readily combined with two existing algorithm, LPG and LPO, and demonstrates its effectiveness on a wide variety of benchmarks, including MiniGrid and MinAtar."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Novelty: Although the proposed method is a simple modification of previous algorithms, the idea of utilizing temporal information is well-motivated and novel.\n- Presentation: The authors meticulously analyze the evolution of the discovered objectives throughout the agent's lifetime, detailed in Section 5.3, with Figures 3 and 4 providing compelling visual representations of these dynamic shifts.\n- Experiment: The authors meticulously design experiments to rigorously assess the generalization ability of the proposed method to different environments or training hyperparameters (e.g. the number of environment interactions)."
                },
                "weaknesses": {
                    "value": "- Novelty: The application of antithetic sampling for gradient estimation, while prevalent in black-box optimization, appears to lack originality.\n- Ablation: The authors do not conduct a comprehensive ablation study comparing the proposed TA-LPG with the standard LPG. Specifically, they introduce two significant modifications, incorporating temporal information and employing antithetic sampling, without isolating the effects of each change."
                },
                "questions": {
                    "value": "- Ablation: Could you please provide me a clear understanding of the individual contributions of the two proposed component to the performance improvements?\n- Hyperparameter: Setting the entropy coefficient of PPO to 0.0 in the Brax environment appears to significantly limit exploration in my view and poses questions about the experimental design. To address this and strengthen the validity of your results, I recommend conducting a systematic hyperparameter search on the entropy coefficient of PPO within the Brax environment."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5396/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5396/Reviewer_hAap",
                        "ICLR.cc/2024/Conference/Submission5396/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5396/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697604661766,
            "cdate": 1697604661766,
            "tmdate": 1700531054255,
            "mdate": 1700531054255,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CmdiDBe52D",
                "forum": "MJJcs3zbmi",
                "replyto": "thJdGiWYB5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their thoughtful and insightful review. We are glad the reviewer finds our work \u201cwell-motivated and novel\u201d, with \u201ccompelling visual representations\u201d and \u201cmeticulous\u201d experiments. The reviewer mentions several good points that we would like to address.\n\n## Novelty of antithetic task sampling\n\nYou are correct, using antithetic sampling for ES is not a novelty (Salimans et al., 2017). However, the update we propose in (12), \u201cantithetic task sampling\u201d, is designed to reduce the variance from applying ES in the multi-task setting. Using the update from Salimans et al. (2017), each candidate is evaluated on a randomly sampled task, before a rank transformation is applied to their fitness. This can lead to instability in the update, when the fitness across tasks has varying scales. In (12), we propose evaluating each antithetic candidate pair *on the same task*, before applying a rank transformation over the pair (equivalent to selecting the higher-performing candidate). This allowed us to normalize fitness across tasks, which we found led to faster meta-training convergence and improved final performance.\n\n## Ablation of antithetic task sampling\n\nWe do not ablate antithetic task sampling in the paper since it is tangential from the focus of the work. However, we include it since it is a novel and impactful implementation detail. We note that ES with antithetic task sampling is applied to both LPG and TA-LPG in all experiments (other than the meta-gradient model), so the ablation of temporal information is independent of this detail and rigorously demonstrates the impact of temporal information.\n\n## Hyperparameters\nThis is a good discussion point. Our reasoning for setting entropy=$0.0$ is based on [OpenAI's original PPO implementation](https://github.com/openai/baselines/blob/master/baselines/ppo2/defaults.py), which uses that setting for continuous control. [Extremely thorough follow-up work](https://arxiv.org/abs/2006.05990) (See Decision C32, Figure 77) [1] has found no evidence that entropy significantly improves performance on continuous control tasks. While we can tune this coefficient per-environment, we believe that this is against the spirit of the benchmark and is not commonly done.\n\nFor a more in-depth discussion of this, we recommend [this blog post](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) [2].\n\nNonetheless, we have run the requested set of experiments and have done a hyperparameter sweep over the entropy coefficient in similar settings to [1] with 5 seeds over 5 values and have attached the results to the supplementary material in Figure 9. In general, our findings align with [1] and we find that an entropy coefficient of 0.0 performs comparably to the others. While an entropy coefficient of 0.005 performs significantly better in Walker2d, it performs significantly worse than 0.0 in Ant and Humanoid. If the reviewer believes this would significantly improve the analysis of our paper, we can provide updated figures that tune the coefficient per-environment and per-algorithm.\n\n[1] Andrychowicz, Marcin, et al. \"What matters for on-policy deep actor-critic methods? a large-scale study.\" International conference on learning representations. 2020.\n\n[2] Huang, et al., \"The 37 Implementation Details of Proximal Policy Optimization\", ICLR Blog Track, 2022.\n\n---\n\n*We hope that most of the reviewer\u2019s concerns have been addressed and, if so, they would consider updating their score. We\u2019d be happy to engage in further discussions.*"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222490542,
                "cdate": 1700222490542,
                "tmdate": 1700222490542,
                "mdate": 1700222490542,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d5VjtQLsuF",
                "forum": "MJJcs3zbmi",
                "replyto": "thJdGiWYB5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5396/Reviewer_hAap"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5396/Reviewer_hAap"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive response. I was concerned that antithetic sampling might only be applied to TA-LPG. I encourage the authors to make this more explicit in the paper. Since all of my concerns have been well addressed, I raise the score from 6 to 8."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531032329,
                "cdate": 1700531032329,
                "tmdate": 1700531091339,
                "mdate": 1700531091339,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IjlZvBCQXe",
            "forum": "MJJcs3zbmi",
            "replyto": "MJJcs3zbmi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5396/Reviewer_c7mC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5396/Reviewer_c7mC"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes to meta-learn a better objective function for reinforcement learning (RL) tasks by taking into account the information of the task horizon.\nSpecifically, with the help of the time-step information, the proposed algorithms could find a better balance between exploration and exploitation.\nFurthermore, this work shows that meta-gradient methods fail to adapt to different horizons, while evolution strategies can do better in this case."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea behind this work is simple and effective, supported by solid experiments and detailed analysis."
                },
                "weaknesses": {
                    "value": "- The idea of incorporating time-step information is not novel in RL, such as this [work](https://proceedings.mlr.press/v80/pardo18a.html), which should be included in the related work.\n\n- Lack of ablation study: In Section 4.1 Equation 9, $n/N$ and $\\log(N)$ are included as part of the input to the agent. It is not clear or argued how good this option is compared with other options, such as $n$ and $N$, $N-n$, or $n/N$."
                },
                "questions": {
                    "value": "- Section 4.1, after Equation 10, \"Since TA-LPO is only meta-trained on a single environment and horizon, we do not include the $\\log(N)$ term since it does not vary across updates.\" Would including the $\\log(N)$ term decrease the performance?\n- Section 5.2, Paragraph \"Lifetime conditioning improves performance on out-of-distribution environments.\" Figure Figure 2: typo."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5396/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698363291910,
            "cdate": 1698363291910,
            "tmdate": 1699636546456,
            "mdate": 1699636546456,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b9cYEK3uG9",
                "forum": "MJJcs3zbmi",
                "replyto": "IjlZvBCQXe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their clear and useful review. We are glad the reviewer finds our work to be \u201csimple and effective\u201d and \u201csupported by solid experiments and detailed analysis\u201d. The reviewer brings up many good suggestions and questions we would like to address.\n\n## Time Limits in RL\n\nWe would like to thank the reviewer for pointing out this line of related work. We have updated the manuscript to include the suggested paper. However,  there is an important difference in the use of the term \u201ctime-step\u201d between these works. In our work, \u201ctime-step\u201d refers to the total training budget (i.e. a \u201cglobal\u201d time-step). For the mentioned work, \u201ctime-step\u201d refers to the time-step *within* an episode. We\u2019ve further updated the manuscript to clarify this.\n\nEven with this distinction, we agree that incorporating time step information is not novel in RL. Any learning rate schedule incorporates global time-step information. However, incorporating time-step information in RL algorithm discovery methods  (e.g. LPG and LPO) is novel, and leads to the significant improvements in performance demonstrated in our work.\n\n## Ablation on the input\n\nThank you for raising this question, we have included a discussion of alternative representations in the revised paper and are running the ablations outlined below.\n\nFor LPG, our choice of representation was fairly arbitrary. We decided to include $\\log(N)$ as a measure of total training budget - applying a logarithmic transformation in order to handle large values - and $n/N$ as a measure of training progress that would be invariant to $N$. We did not condition on $N$ or $n$ directly, since these numbers can be very large (e.g. $1e7$ for Min-Atar) and lead to dramatically out-of-distribution values when transferring to much longer or shorter horizons. As long as we have a measure of both total and relative train step, and apply suitable transformations, we are confident that the representation of total and relative train step will have a negligible impact on performance.\n\nTo verify this, we are rerunning the experiments and will provide results with alternative parameterisations for training progress - $\\log(N-n)$ and $(\\log(N), \\log(n))$ - in addition to another experiment directly conditioning on the total train steps, $N$.\n\n> Would adding $\\log(N)$ to TA-LPO decrease performance?\n\nSince TA-LPO is trained on a single task with a constant $N$, $\\log(N)$ would simply be a constant bias term. Therefore, adding $\\log(N)$ to TA-LPO would be unlikely to impact in-distribution performance. However, including $\\log(N)$ could affect transfer to out-of-distribution tasks with different $N$, since the input would then be out of distribution.\n\n> Figure 2 typo\n\nThank you! We\u2019ve fixed this in our updated manuscript.\n\n---\n*We hope that most of the reviewer\u2019s concerns have been addressed. We\u2019d be happy to engage in further discussions.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222548189,
                "cdate": 1700222548189,
                "tmdate": 1700222548189,
                "mdate": 1700222548189,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d7axdPkVWW",
                "forum": "MJJcs3zbmi",
                "replyto": "b9cYEK3uG9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5396/Reviewer_c7mC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5396/Reviewer_c7mC"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thank you for your reply. Please let me know when you have the results of the ablations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516156317,
                "cdate": 1700516156317,
                "tmdate": 1700516156317,
                "mdate": 1700516156317,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9elcZFk6cs",
            "forum": "MJJcs3zbmi",
            "replyto": "MJJcs3zbmi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5396/Reviewer_47vA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5396/Reviewer_47vA"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a training horizon to meta-reinforcement learning algorithms to discover objective functions that depend on the learner's lifetime. The basic idea is to add the information about the lifetime to the input vector of the meta-learning algorithm. The authors propose two algorithms, Temporally-Adaptive Learned Policy Gradient (TA-LPG) and TA-Learned Policy Optimization (TA-LPO) by extending LPG and LPO, respectively. Then, the authors found that the evolutionary strategy is more appropriate than meta-gradient approaches to optimize the upper-level objective function."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Based on the proposed idea, the authors implement two meta-reinforcement learning algorithms: TA-LPG and TA-LPO. It implies that the idea can be applied to various algorithms, although input augmentation depends on the algorithms. Systematic experiments show that TA-LPG and TA-LPO outperform LPG and LPO, respectively."
                },
                "weaknesses": {
                    "value": "Although the experimental results support the authors' hypothesis that conditioning on the lifetime is helpful in meta-learning, there are no theoretical justifications. In addition, the way to incorporate the lifetime depends on the algorithm, and the experimenter carefully has to design augmentation."
                },
                "questions": {
                    "value": "1. The function $U_\\phi (x_t \\mid x_{t+1}, \\ldots, x_T)$ is defined in Section 3.2, but it is unclear because $T$ is not explained. Is it the total number of interactions, denoted by $N$ later?  \n2. Two variables $n/N$ and $\\log N$ are augmented in TA-LPG. Is $n/N = 0$ when $N$ is unbounded? In my understanding, $N$ is determined and fixed before training. Would you explain $N$ and provide an example task? \n3. In TA-LPO, $ \\frac{n}{N} x_{r, A}$ is augmented. It is not straightforward to me because it is proportional to $x_{r, A}$, which means linearly-dependent. Would you explain the problem if $n/N$ and $\\log N$ are added as the authors did in TA-LPG? \n4. In the final paragraph of Section 4.2, the authors mentioned, \"We found this stabilized training and led to higher performance...\" Would you explain \"this stabilized training\" in detail? If it means the rank transformation, it is used in Salimans et al. (2017). Discussing the relation between two equations would be better if the stabilized training means Eq. (12) rather than (11). \n5. I do not fully understand the final paragraph of Section 3.1. In the manuscript, LPG and LPO are selected as the base algorithms. However, the authors mentioned as follows: In our work, we focus on instances of meta-RL that parameterize surrogate loss functions with $\\phi$ and apply gradient-based updates to $\\pi_\\theta$ (Houthooft et al., 2018; Kirsch et al., 2019; Bechtle et al., 2021). Does it mean that three algorithms are implemented somewhere?\n6. Equation (10): Is $x_{r, A}$ a typo of $x_{p, A}$? \nPlease update the reference Kirsch et al. (2019) to Kirsch et al. (2020). Please see https://openreview.net/forum?id=S1evHerYPr"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5396/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569048998,
            "cdate": 1698569048998,
            "tmdate": 1699636546363,
            "mdate": 1699636546363,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fle1CqG85N",
                "forum": "MJJcs3zbmi",
                "replyto": "9elcZFk6cs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5396/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5396/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their very thorough and detailed review. We are glad that the reviewer finds that our experiments are systematic and show the strength of our method. \nThe reviewer brings up several good points and questions that we would like to address.\n\n## On theoretical justifications \n \nWe\u2019ve included a theoretical justification and proof in Appendix E. The high-level justification is that without temporal awareness, the algorithm cannot effectively trade off exploration and exploitation. For example, in the case of multi-armed bandits, the agent should converge to a single lever at the end of training, but perform exploration during training.\n\n\n##  On \u201ccareful\u201d augmentations\n\nFor LPG, our choice of augmentation was arbitrary. We decided to include $\\log(N)$ as a measure of total training budget - applying a logarithmic transformation in order to handle large values - and $n/N$ as a measure of training progress that would be invariant to $N$. As long as we have a measure of each of these quantities and are scaled to a reasonable range, we expect the representation of total and relative train step to have a negligible impact on performance.\n\nTo verify this, we are rerunning the experiments and will provide results with alternative parameterisations for training progress - $\\log(N-n)$ and $(\\log(n))$ - in addition to another experiment directly conditioning on the total train steps, $N$.\n\n> Would you explain the problem if $n/N$  and $\\log(N)$ are added [to TA-LPO]?\n\nFor TA-LPO it is *slightly* more involved. Given a quick understanding of the theoretical framework of LPO, TA-LPO is a simple augmentation.\n\nThe theoretical framework behind LPO is described in Section 3.3. The key takeaway from is that the network needs to output $0$ when the likelihood ratio $p$ is equal to $1$. The network used for LPO has no biases and uses a tanh activation, so if the input is $0$, then the output is $0$. Thus, the input to the LPO network, $x_{r,A}$, is designed to be $0$ when $p=1$ (Equation 7).\n\nIf we simply append $n/N$ as an input to TA-LPO, then the requirement doesn\u2019t hold since it is not $0$ when $p=1$. Thus, we multiply $n/N$ by $x_{r,A}$ (which is $0$ when $p=1$).\n\nWe do not include any variant of $\\log(N)$ because LPO is only trained with a single $N$, which would simply reduce that term to a bias term. \n\nWe believe this is an insignificant challenge for future work and does not reduce the strength of our contribution. \n\n## Questions:\n\n> $T$ is not explained\n\nThank you for catching this. $T$ is the trajectory/rollout length that the actor collects each update step, not the total number of environment interactions $N$. We have updated the manuscript to include this!\n\n> Is $n/N$ unbounded when $N$ is unbounded? Would you explain $N$ and provide an example task?\n\n$N$ is the total number of environment interactions we provide to the agent to learn. For example, in MinAtar-Breakout, the agent interacts with the environment for $1e7$ timesteps, so $N=1e7$. From this, we calculate $n/N$, which measures the proportion through training, and $\\log(N)$, which provides the agent with the total number of training steps.\n\nWhen $N$ is unbounded, $n/N$ is a constant $0$. In practice, the total training budget $N$ is usually known, as this allows RL algorithms to trade off exploration over training.\n\n> Would you explain \"this stabilized training\" in detail? \n\nThank you for pointing this out, we have revised the paper to make this clearer. The update we propose in (12), \u201cantithetic task sampling\u201d, is designed to reduce the variance from applying ES in the multi-task setting. Using the update from Salimans et al. (2017), each candidate is evaluated on a randomly sampled task, before a rank transformation is applied to their fitness. This can lead to instability in the update, when the fitness across tasks has varying scales. In (12), we propose evaluating each antithetic candidate pair *on the same task*, before applying a rank transformation over the pair (equivalent to selecting the higher-performing candidate). \n\nThis allowed us to normalize fitness across tasks, which led to faster meta-training convergence and improved final performance. We do not examine this rigorously in the paper since it is tangential from the focus of the work. However, we include it since it is a novel and impactful implementation detail, which we had to apply consistently throughout the LPG experiments.\n\n> Does it mean that three algorithms are implemented somewhere?\n\nThank you for pointing this out! We have updated the manuscript to clarify this. We implemented LPG and LPO, which are successors to those papers.\n\n## Misc:\n\nThank you for catching the typo in equation 10 and the incorrect year on the reference! We\u2019ve fixed these in the updated manuscript.\n\n---\n*We hope that most of the reviewer\u2019s concerns have been addressed and, if so, they would consider updating their score. We\u2019d be happy to engage in further discussions.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5396/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700223788499,
                "cdate": 1700223788499,
                "tmdate": 1700223788499,
                "mdate": 1700223788499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]