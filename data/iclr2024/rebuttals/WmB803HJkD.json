[
    {
        "title": "Denoising Low-Rank Data Under Distribution Shift: Double Descent and Data Augmentation"
    },
    {
        "review": {
            "id": "J9qyIfwOc4",
            "forum": "WmB803HJkD",
            "replyto": "WmB803HJkD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6422/Reviewer_sZRG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6422/Reviewer_sZRG"
            ],
            "content": {
                "summary": {
                    "value": "This work focuses on analyzing multi-output linear regression in the presence of noisy inputs.  An important part of their work is the discussion of the scenario in which the target multivariate linear regressor equals the identity, allowing them to explore the problem of linear denoising. Similar to a recent line of works, and relevant to contemporary machine learning practices, this work explores the asymptotic regime where data dimensions and the number of samples grow proportionally.\n\nThe primary distinctions from prior studies include the absence of an assumption that the training and test data are independently and identically distributed (i.i.d.) or even sourced from the same distribution. Instead, it is assumed that both the test and training data exist within the same low-dimensional subspace (exhibiting low-rankness). Similarly, this work employs relatively mild assumptions concerning the noise in both the test and training data\n\nDue to the specific assumptions made about the data, the test error is defined as the expected value taken over both the training and test noise of the mean squared error (MSE) on the test data. The authors subsequently derive precise formulas for the test error of the linear regressor that minimizes the training error and the one that minimizes the expected training error. Additionally, they discuss distribution shift bounds and the asymptotic behavior of the (relative) excess risk.\n\nThe authors conclude by comparing their theoretical predictions on real datasets, demonstrating good agreement."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper departs from the conventional assumptions that have been prevalent in previous high-dimensional regression studies. Notably, a particularly intriguing contribution lies in the elimination of the iid or \"Gaussian-like\" assumptions previously relied upon. This requires the application of novel and technical analyses, paving the way for promising avenues for future research.\n\nLastly, the strong agreement between the theoretical formulas and the experimental results on standard ML datasets suggests that the empirical observations hold a certain level of \"universality\" for approximately low-rank datasets."
                },
                "weaknesses": {
                    "value": "The paper operates in a regime where the noise is essentially negligible in comparison to the signal's magnitude. This is characterized by a signal-to-noise ratio that approaches infinity. However, the more intriguing scenario is when the noise magnitude is comparable to that of the signal.\n\nOn the same note, the authors assert that their noise assumption is \"strictly more general\" than that of previous works, like Cui and Zdeborova. It's important to note that Cui and Zdeborova consider Gaussian noise with a variance of O(1) for each entry, which contrasts with the o(1) variance examined here. For example, in Cui and Zdeborova, the squared Frobenius norm of Atr is of the order O(N).\n\nThe phenomenon of vanishing noise, combined with a fixed subspace dimension, leads to the peculiar situation where both training and test errors tend to zero, regardless of the under/over-parameterization ratio (c).\n\nThe theoretical results presented in the \"Out-of-Distribution and Out-of-Subspace\" section are not adequately discussed, leaving the reader to ponder their implications.\n\nThe predictions derived from empirical results apply specifically to linear denoisers on real datasets. However, it remains unclear whether these observations hold true or offer insights into the behavior of more general denoisers in contemporary practices, such as deep neural networks."
                },
                "questions": {
                    "value": "Could you please provide clarification regarding the regularization of W due to the noise discussed on page 8?\n\nFirstly, there is already significant regularization in place due to the selection of the minimum norm solution. Therefore, it's not entirely clear to me how this interacts with noise-induced regularization.\n\nSecondly, it's worth noting that the empirical results are based on noiseless data. If the objective is to showcase the effects of noise regularization, wouldn't it be beneficial to introduce a small amount of noise and compare it with the noise-free scenario?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617443525,
            "cdate": 1698617443525,
            "tmdate": 1699636716321,
            "mdate": 1699636716321,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Jnx9eWLBIO",
                "forum": "WmB803HJkD",
                "replyto": "J9qyIfwOc4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarifying amount of noise"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the in-depth comments about our work and for agreeing with us that removing the Gaussian-like assumptions is one of our major contributions. **If the reviewer feels that our series of responses adequately addresses their concerns, we hope that they will consider revising their score.**\n\nWe would first like to clarify a misconception about the relative size of the signal and noise. \n\nFrom assumption 1.2, we have that for $X_{trn} \\in \\mathbb{R}^{d \\times N}$, we have that $\\|\\|X_{trn}\\|\\|_F^2= O(N)$. This implies that each data point has norm norm $O(1)$. \n\nFrom assumption 2, we have that for $A_{trn} \\in \\mathbb{R}^{d \\times N}$, $\\mathbb{E}[A_{ij}^2] = \\eta^2/d$. However, this means that for the whole matrix $A_{trn}$ we have that \n\n$$\\mathbb{E}\\left[\\| \\|A\\_{trn}\\|\\|\\_F^2\\right] = \\sum\\_{i=1}^d \\sum\\_{j=1}^N \\mathbb{E}\\left[A\\_{ij}^2\\right] = d N \\eta^2/d = \\eta^2 N. $$\n\nFurther, from assumption 2, we assume that $\\eta^2 = \\Theta(1)$. \n\n**Hence we have that the norm of the data matrix and the norm of the noise matrix are on the same scale**. Hence we do not have vanishing noise. \n\nIn fact, based on our assumptions we have that $\\mathbb{E}\\left[ \\|\\|A_{trn}\\|\\|_F^2\\right] = \\Theta(N)$. However, we only have that $\\|\\|X\\_{trn}\\|\\|\\_F^2 = O(N)$. In particular, we allow for sublinear growth. **Hence are applicable to situations where we have diminishing signal.**\n\n----\n\nWe picked the scaling so that data points and noise vectors have constant norm as $d, N \\to \\infty$ rather than having both norms go to $\\infty$ as is the case in many prior works. Hence, due to this, the errors go to 0. However, if we undo the scaling, so multiply everything by $d$, we would have the asymptotic error be non-zero and would depend on $c$. This is not due to vanishing noise. It is due to the scaling we use for the problem, which is the same for both the signal and the noise."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699644679009,
                "cdate": 1699644679009,
                "tmdate": 1700148029570,
                "mdate": 1700148029570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vQuJStD8RL",
            "forum": "WmB803HJkD",
            "replyto": "WmB803HJkD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
            ],
            "content": {
                "summary": {
                    "value": "This paper derives generalization bounds for linear least square problems with noisy data. In contrast to prior literature, this work considers a more realistic and general setting where 1) the data matrix is low rank, 2) a non-classical proportional regime is used by taking both the data dimension $d$ and size of data $N$ to the limit $\\to\\infty$, and 3) the test data can be drawn from an arbitrary, non-iid distribution. The results of this paper yields some novel insights in the double descent phenomenon and out-of-distribution generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The problem setting in this paper is well-explained and well-motivated. Also, this paper offers a wide range of range theoretical and empirical results. These results lead to some interesting insights in a variety of topics of current interests. In particular, I like that Theorem 1 demonstrates the different generalization behaviors in the under-parameterized vs over-parameterized regimes. And the experiments reinforces this contribution by illustrating the double descent phenomenon in accordance to the predictions of Theorem1. Lastly, the out-of-distribution generalization bounds look promising, and I suggest that the authors mention Corollary 5 in the main body, as it is a considerably stronger statement than Corollary 1."
                },
                "weaknesses": {
                    "value": "However, my impression is that the paper tries to cover too much mileage at once and thus does not explain the results very clearly. Here are my criticisms:\n1. The authors did not clearly explain the terms in Theorem 1. In the second-to-last paragraph of page 6, what are \"bias term\" and \"variance term\" referring to? The authors also did not explain the difference between the second terms in the respective bounds for under/over-parameterized case. And finally, for the over-parameterized case, where did the extra third term come from?\n\n2. There is no proof sketch for any of the results. Given that the exact terms of the generalization bounds are difficult to digest, it is imperative for the authors to elucidate the key ideas and techniques behind the results. Also, since the proof of Theorem 1 is almost 20 pages long, without an outline of the proof's approach, there is no way for me to even superficially check its validity.\n\n3. The additional results in the Appendix are very poor organized, many experiments results are mixed into the \"Additional Theoretical Results\" section. And many theorem statements (e.g. Theorem 5) in Appendix C are not written out completely and rigorously despite that page limit is not a concern.\n\n4. In Section 4, I think the conclusion in \"Overfitting Paradigms\" is incorrect. In the limit as $N \\to \\infty$, the value of $c$ should be approaching zero, NOT to $+\\infty$. Then, we should not be achieving benign overfitting as suggested by the authors.\n\nLastly, as as suggestion, the author should consider briefly explain that Marchenko-Pastur measure is the \"natural\" limit distribution of the singular values of a random matrix. Otherwise, specifying the shape parameter $c$ may look weird to the reader."
                },
                "questions": {
                    "value": "1. In the first term of Theorem 2's bound, I feel that there should be a $N_{tst}$ in the denominator?\n\n2. For the figures, I personally do not like that $x$-axis is in $1/c$ instead of $c$. In the literature, the $x$-axis is usually for the amount of *over*-parameterization (e.g. Figure 1 in [1]). What are your motivations for doing it this way?\n\n3. If I understood correctly, the solid lines in Figure 1a are simply connected the empirical data points, right? This is quite confusing as the solid line denotes the theoretical prediction in Figure 1c.\n\n4. Why is that the theoretical prediction deviates slightly from the empirical observation in Figure 1c (real world setting) but is a near perfect match in Figures 9 and 10? Is that because the experiments for Figure 1c rely on a low-rank approximation? Or are there other reasons?\n\n[1] Schaeffer, Rylan, et al. \"Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle.\" arXiv preprint arXiv:2303.14151, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6422/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6422/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643971388,
            "cdate": 1698643971388,
            "tmdate": 1699636716179,
            "mdate": 1699636716179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NsMs4QOXVj",
                "forum": "WmB803HJkD",
                "replyto": "vQuJStD8RL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive comments. We agree with the reviewer that we offer a wide variety of theory results and that the difference in generalization between the under-parameterized and the over-parameterized regimes is fascinating. **If the reviewer feels that our series of responses adequately addresses their concerns, we hope that they will consider revising their score.**\n\nWe thank the reviewer for their interest in Corollary 5. We have updated the paper to mention Corollary 5 in the main paper. \n\nWe now address the reviewer's concerns. \n\n> Explanation of the terms in Theorem 1. \n\nWe thank the reviewer for pointing out this issue. We have added the following clarification. Our generalization error is given by \n\n$$\\mathbb{E}\\_{A\\_{trn}, A\\_{tst}}\\left[\\frac{\\|Y\\_{tst} - W\\_{opt}(X\\_{tst} + A\\_{tst})\\|^2}{N\\_{tst}}\\right]$$\n\nWe can expand this into \n\n$$\\frac{1}{N_{tst}}\\left( \\|Y_{tst} - W_{opt} X_{tst}\\|^2 + \\|W_{opt}A_{tst}\\|^2 - 2Tr((Y_{tst}-W_{opt}X_{tst})^TW_{opt}A_{tst})\\right). $$\n\nDue to the assumptions that entries of $A_{tst}$ have mean 0, when we compute the expectation, the cross term vanishes. Then, we are left with two terms.\nThe first is $\\frac{1}{N_{tst}} \\left( \\|Y_{tst} - W_{opt} X_{tst}\\|^2\\right)$, which we think of as the bias and the second is $\n\\frac{1}{N_{tst}}\\|W_{opt}A_{tst}\\|^2$, which we think of as the variance. \n\nRelating this to Theorem 1 (let us ignore the big-O and little-o terms), we have two terms for both the over and under-parameterized regimes. The first term (that depends on $L$ is the bias term. The second term (that is independent of the $L$) is the variance term. \n\nThe extra term in the overparameterized case appears due to a technical reason. In the under-parameterized case, certain products of matrices cancel nicely and either become 0 or the identity. Hence, we do not need to estimate them using random matrix theory. \nHowever, this is not the case for the overparameterized case. Unfortunately, we get a term for which the error between the true value and our estimate is $O(\\|\\Sigma\\|^/N^2)$. \n\n> Proof Sketch\n\n\nThe proof is broken down into the following steps. \n\n1. Obtain a formula for $W_{opt}$. In particular, we know that this $W_{opt} = X(X+A)^+$. However, this step expands $(X+A)^+$ and simplifies the expressions. (Lemma 1 for the overparameterized and Lemma 9 for the under-parameterized case)\n2. Substitute our expression into our expressions for the bias and the variance. Then, we can compute these norms as the trace of various matrices. (Lemma 2, 10 for the bias, the variance is done in the proof of the main theorem)\n3. This is the most challenging step - show that the variety of the products concentrate to diagonal matrices. This is quite difficult, and Lemma 3,4,5,6,7,11,12,13,14,15 are proving these concentration results. \n\n> Appendix organization. \n\nWe thank the reviewer for pointing this out. We have reorganized the appendix. \nWe have also added more details to Appendix C. \n\n> Overfitting incorrect\n\nWe thank the reviewer for pointing this out. However, we believe the error is a typo (not a fundamental one). The result has been updated. In the theorem for the underparameterized case, we were missing a $c$. So the relative error is $c/(1-c)$, which approaches 0 as $c \\to 0$. Also, we see in the highly overparameterized case where we have a relative error of $1/(c-1)$, which also approaches 0 as $c \\to \\infty$. This is the highly overparameterized case and the highly underparameterized case both benignly overfit in this case, and we see the more complex tempered overfitting in the proportional regime. \n\n## Questions\n\n> Theorem 2 bound\n\nYes, that is correct. We have fixed it. \n\n> c vs 1/c\n\nWe did it because we test on real data. Recall that $c = d/N$. However, for real data, $d$ is fixed (for CIFAR d is 3*3*32=3072), and is something that we **cannot change**. So, we vary $c$ by varying $N$ and plotted our graphs against $1/c = N/d$. This reads naturally once one notices that $N$ was the varied quantity. \n\n> Figure 1a\n\n**This is not correct.** Figure 1a, the solid line connects the theoretical values. If the reviewer looks closely (zooms in on the old pdf), the scatter points and the line do not exactly match. There are close, but it is not exact. \nHowever, to avoid this confusion. We have sampled more theory points and plotted a smoother version of the theory curves, making it more evident that the solid lines are always theory. \n\n> Figure 1c vs Figures 9 and 10\n\nFor Figures 9 and 10, we are doing this for distributions in our low-rank subspace. Hence, the curves exactly line up. Figure 1c is for an experiment without the exact low-rank assumptions needed by our results. Hence, there is a mismatch.\n\n\u2014-------------\n\nWe thank the reviewer again. With the above clarifications and the changes to the paper, the reviewer is willing to accept the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111963611,
                "cdate": 1700111963611,
                "tmdate": 1700147809504,
                "mdate": 1700147809504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n8JP37ODke",
                "forum": "WmB803HJkD",
                "replyto": "NsMs4QOXVj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and I appreciate the changes made to address my concerns.\n\nHowever, I find that not all changes to the manuscript were highlighted. Since it seems that I cannot access the original version anymore, it is rather difficult to locate the changes. So, please correct me if I missed anything.\n\nAll of my concerns except for points 1 and 2 are addressed to varying degrees. But there are two major issues that remain.\n\n1. The restructuring of the Appendices were very low-effort. Appendix B now contains no explanations of the figures at all. And the re-shuffling created some dangling pointers at the bottom of page 6.\n\n2. I am not entirely happy with the added discussions about Theorem 1 and the proof sketch. I don't think it is difficult to guess that the bounds would contain a variance and bias term. I am interested in seeing a careful dissection of the terms, e.g. what is the significance the matrix $L$ while $V_{trn}$ does not make it to the bound? In the second term, why the exponent of $c$ is different in the two settings? There are many possible questions the author could answer that could offer some technical insights beyond the equations. And speaking of the proof sketch, step 3 basically says it involves a bunch of complicated math without elaborating further. I feel I did not gain any useful intuition regarding the proof (the response to reviewer RbRx was a bit more insightful however).\n\nMy general impression is that authors did not make enough changes to address my concerns, especially regarding my points 1 and 2. Also, I feel the summary of my review in the top-level thread is potentially misleading (see my response above). Overall, I am comfortable with maintaining my original assessment of this paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283837111,
                "cdate": 1700283837111,
                "tmdate": 1700283837111,
                "mdate": 1700283837111,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FpMzdUvip9",
                "forum": "WmB803HJkD",
                "replyto": "vQuJStD8RL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Intuition"
                    },
                    "comment": {
                        "value": "Let us continue to build some more intuition. \n\n**Why do things concentrate**\n---\n\nLet $A$ be a $p \\times q$ matrix with IID entries that have mean 0 and variance 1 and bounded fourth moment. Then if we look at $\\frac{1}{q} AA^T$, let $\\lambda_1, \\ldots, \\lambda_k$ be the eigenvalues, and let $\\mu_{p,q}$ be the uniform distribution on these eigenvalues. \n\n**Note here the randomness in $\\mu_{p,q}$ is over which eigenvalue of $AA^T$ we are picking AND NOT over $A$**\n\nThen the Marchenko Pastur theorem tells us that $\\mu_{p,q}$ converges to marchenko pastur weakly. Hence this tells us that for EVERY A, this converges, so we have pathwise convergence instead of in probability or expectation. \n\nThen we saw above the expressions of the form $\\sum_{i=1}^N \\frac{1}{N} \\frac{1}{\\sigma_i^2}$ can be written as $\\mathbb{E}\\_{\\mu\\_{d,N}}\\left[\\frac{1}{\\sigma^2}\\right]$ where the expectation is taken not over $A$ but over this $\\mu_{d,N}$ measure. Hence, everything concentrates. \n\n**Difference between $c>1$ and $c < 1$**\n---\n\nContinuing with the notation from above. Suppose $p < q$, then we notice that $AA^T$ is $p \\times p$. It can be shown that $A$ is full rank. \n\nHowever, in our work, for one case we see $AA^T/q$ and in the other $A^TA/q$ (Recall $P$, $Q$ terms at the very beginning of the previous comment). \n\nHowever, the Marchenko Pastur theorem doesn't apply to $A^TA/q$, it doesn't have the correct normalization. So to correct we need to multiply by $q/p$. For our results $q = d, p = N$. Hence, we are multiplying by $c$. \n\n**This is the second reason that the powers of $c$ are different**\n\n**Understanding $L$**\n---\n\n$L$ is the cooridnates of $X_{tst}$ in the basis given by $U$. Then suppose $X_{tst} = U_{tst} \\Sigma_{tst} V_{tst}^T$. Then we see that $L = U^TU_{tst} \\Sigma_{tst} V_{tst}^T$. Then, using the unitary invariance of the norm, we have that \n\n$$ \\|\\|\\beta\\_U^T( \\Sigma\\_{trn}^2c + \\eta\\_{trn}^2 I)^{-1} L \\|\\|\\_F^2 = \\|\\|\\beta\\_U^T( \\Sigma_{trn}^2c + \\eta\\_{trn}^2 I)^{-1}  U^TU_{tst} \\Sigma_{tst} \\|\\|\\_F^2$$\n\nHence we see that the error depends on an alignment term between the training and test data $U^TU_{tst}$ and the singular values $\\Sigma_{tst}$. \n\n**Why no $V$ terms**\n---\n\nSince we are doing linear least squares problems, we see that everything depends on the Gram matrix of the data (this is also known as the kernel trick). Hence, we only get dependence on $U$ and not on $V$. In the classical regime, this would mean we depend on the eigenvectors and eigenvalus of the covariance matrix and not on anything else. \n\n**Different powers of $c$**\n---\n\nThis is due to two reasons \n\n1. The lack of symmetry the expressions for $W_{opt}$ (See previous reply)\n2. The need to multiply by $d/N$ to get convergence (see the section on concentration in this reply). \n\n-------\n\nWe hope that the above helps better understand the proof. If there are steps that are unclear or parts where the reviewer would like more detail, we are happy to clarify and provide more details."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700319513590,
                "cdate": 1700319513590,
                "tmdate": 1700319593767,
                "mdate": 1700319593767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v0oT5t2sBY",
                "forum": "WmB803HJkD",
                "replyto": "rD23DAVs9i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the quick updates. I know this may be difficult, but can you integrate at least some of the ideas you described above into the paper? I think it is absolutely crucial to have a discussion of proof ideas in the paper. I would be happy to raise my score if this is done."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700327509711,
                "cdate": 1700327509711,
                "tmdate": 1700327509711,
                "mdate": 1700327509711,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KlEIinip8U",
                "forum": "WmB803HJkD",
                "replyto": "7ZPqqTZTKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "content": {
                    "comment": {
                        "value": "> We kindly ask if the reviewer, due to the subjective nature of their concern and the discussion with us, would be willing to increase their score to 5. (Note we have limited the audience of this comment). \n\nApology if I am interrupting your conversion with reviewer RbRx. I think it is *extremely inappropriate* to directly ask the reviewer for a specific score. And trying to hide this request with a limited view permission goes against the spirit of peer review."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495162730,
                "cdate": 1700495162730,
                "tmdate": 1700495162730,
                "mdate": 1700495162730,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2Jt2hS9yZG",
                "forum": "WmB803HJkD",
                "replyto": "bhbVQO9K60",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "content": {
                    "comment": {
                        "value": "> We apologize did not realize this was inappropriate. We assumed it was similar to asking the reviewer to increase their score. The comment is deleted.\n\nMistakes happen, and I may be a little judgemental. But can you please not make this conversation even less transparent? Regardless of how people feel about directly asking the reviewer for a specific score, it is definitely wrong to make such request private, and now you are only making things worse by deleting it."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634537742,
                "cdate": 1700634537742,
                "tmdate": 1700634537742,
                "mdate": 1700634537742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Pq4PzPhCvw",
                "forum": "WmB803HJkD",
                "replyto": "PJElxkYmWg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_jx5r"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the responses and the revision.\n\nWhile the paper looks to be in better shape now, I still think it is a borderline case. Given that the authors made some unprofessional remarks during the discussion (see links below), I am hesitant to give them the benefit of doubts. So, I decided to maintain my original score.\n\n1. in a top-level coment:  https://openreview.net/forum?id=WmB803HJkD&noteId=uZ266DDUeY\n\n2. in a reply to reviewer RbRx: https://openreview.net/forum?id=WmB803HJkD&noteId=KlEIinip8U"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634551281,
                "cdate": 1700634551281,
                "tmdate": 1700634551281,
                "mdate": 1700634551281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7ZPqqTZTKD",
            "forum": "WmB803HJkD",
            "replyto": "WmB803HJkD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers a linear denoising problem in which the responses Y_{trn} are formed as Y_{trn} = \\beta^T X_{trn}.  These responses are observed noiselessly, but the data is observed with noise as Z_{trn} = X_{trn} + A_{trn}.  The authors study the performance of a least-squares denoiser W_{opt} = Y_{trn} \\cdot (Z_{trn})^{\\dagger} so that new noisy observations Z_{tst} are denoised as W_{opt} Z_{tst}.  Under a well conditioned, low rank assumption on X_{tst} and regularity assumptions (e.g. isotropic, existence of second moments, convergence of the empirical spectral distribution to the Marchenko-Pastur law) on the noise A_{trn}, the authors provide a characterization of the test error of this denoising procedure."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The non-asymptotic character of the results is appreciated and the validation on real datasets is very nice."
                },
                "weaknesses": {
                    "value": "- Assumption A2 seems quite similar to some classical random matrix theory assumptions, explicitly in part 4. In which convergence of the empirical spectral distribution to the Marchenko-Pastur law is assumed.  If I have understood correctly, this assumption (which would follow if independence of the coordinates of the noise matrix were assumed to be independent) allows to bypass the independence assumption.  \n\n- The main technical difficulty seems to come from analyzing the pseudoinverse (A + X)^{\\dagger} as opposed to the inverse (if it existed) (A + X)^{-1}, as the authors point out in the commentary following Theorem 1.  The analysis of this, while cumbersome, does not seem to me to involve much technical novelty.  In particular, the setting with additive structure A + X, where A is random and enjoys favorable structure and X is low rank is quite a bit easier to analyze than the setting in which samples are i.i.d. \\Sigma^{-\u00bd} x_i, and \\Sigma may be ill-conditioned (e.g., Cheng and Montanari, Assumption 3). \n\n- The experimental verification on real data-sets is nice, but it is not very surprising in the simple context considered here.  For instance (and to give an example not cited in the paper), in a similar situation (quadratic loss) which is amenable to random matrix theory, Paquette et. al (https://arxiv.org/abs/2205.07069) study the dynamics of SGD on quadratic models and give exact trajectories when the data is from MNIST (for example).  \n\n- Regarding the insights obtained by the characterizations: The insight over previous work regarding double descent and data augmentation seems limited.  Also, the discussion around benign overfitting is fairly unclear to me.  The setting here is fairly different from that of Bartlett, et. al (https://arxiv.org/abs/1906.11300) in which benign overfitting is characterized in terms of effective ranks.  The assumption on the spectrum and dimension (d/N = c + o(1), finite dimensional data) considered here seems to preclude this kind of decay. \n\n- (Minor) If my understanding of the previous points is correct, it would help to soften the language in the introduction.  As written, the claims made in the introduction are quite a bit stronger than what seems to be concretely shown. \n\n- (Minor) There are also several typos and misspellings in the paper that should be fixed before publication."
                },
                "questions": {
                    "value": "- Could the authors please provide an example of a distribution on the noise which satisfies the assumptions (Assumption A2) and does not have independent coordinates (e.g. is not a standard ensemble such as a Wigner matrix)? Why is this more realistic than the examples considered previously?\n\n- Could the authors please clarify what they feel the technical novelty is in analyzing the pseudoinverse (A + X)^{\\dagger}? In particular, it is not clear to me from looking at the proof what the new ideas involved in analyzing this are: It seems that the setting is technically more complicated than if A + X were invertible, but the complications seem to be purely technical and overcome using standard techniques.  To be clear, I do not think this is a bad thing, but clarifying this would help clarify the contribution overall.  \n\n- In the conclusion, the authors write \"Our work has opened the doors for a similar analysis of more sophisticated denoising\nproblems, involving linear networks or non-linearity...\".  Could you please elaborate why you feel this is the case? The analysis seems to me quite tailored to the linear setting with quadratic loss considered here.  In the appendix, the non-linear extension discussed also seems to require further independence assumptions, which seems counter to the viewpoint adopted here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6422/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6422/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755190965,
            "cdate": 1698755190965,
            "tmdate": 1699636716058,
            "mdate": 1699636716058,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GmQG82BapK",
                "forum": "WmB803HJkD",
                "replyto": "7ZPqqTZTKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Difficulty part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their detailed and in-depth review. We also agree with the reviewer that the non-asymptotic validation with real data is a strength of our paper. **If the reviewer feels that our series of responses adequately addresses their concerns, we hope that they will consider revising their score.**\n\nHere are some short responses to the weaknesses before we delve deeper:\n\n**Weakness 1:** As a punchline, using the spectrum of the noise allows us to work with possibly dependent data. However, as discussed in the paper, most results already make assumptions that encompass our assumptions on the noise. The fact that we only use assumptions about the noise is a strength of the work, not a weakness. Being able to handle such a broad setting comes with many complications, discussed in detail below.\n\n**Weakness 2**: It is already hard to analyze **the inverse of a sum** of random matrices (X+A)^{-1}. This is, in fact, a fundamental issue with analyzing input noise settings. Our idea is to use matrix inverse perturbation results, which comes with several complications already. The problem is _further_ complicated by having to deal with pseudo-inverses, __not only__ complicated by that. We provide more details below.\n\nWe will now explain why deriving our test error results is not easy. The difficulty in the proof comes from the placement of the noise and when the expectation is taken. Both of these are crucial. \n\n----\n\n### Placement of the noise\n\n\nIn our setup, we have noise on the input data instead of the output. However, for comparison, let us briefly consider the situation where we have noise on the output. \n\n**The case of output noise**\n\nWhen the noise is on the output, we have some training data $X$ and $y = \\beta^TX + \\epsilon$ where $\\epsilon$ is noise. In this situation, if we solve the least squares problem, we have our estimate:\n\n$$\\hat{\\beta}^T = yX^+ = \\beta^TXX^+ + \\epsilon X^+$$\n\nNow we have two terms. The first only depends on $X$ and not the noise. Hence, this is straightforward to analyze. The second term is *linear* in the noise. \n\nContinuing from here, suppose we care about $\\|\\|\\beta - \\hat{\\beta}\\|\\|^2 = \\|\\beta\\|^2 + \\|\\hat{\\beta}\\|^2 - 2\\beta^T\\hat{\\beta}$ (as is the case in prior work).  We see that $\\|\\beta\\|$ is a constant.\n\nNext $\\|\\hat{\\beta}\\| = \\|\\beta^TXX^+\\|^2 + \\|\\epsilon X^+\\|^2 + 0$ where the cross term is zero due to the independence of the noise. Here $XX^+$ is roughly constant, so it is easy to analyze, and if $\\epsilon$ is Gaussian, then $\\epsilon X^+$ is Gaussian, and the norm is easy to calculate. \n\nFor the cross term $\\beta^T\\beta = \\beta^TXX^+\\beta + \\beta^T(X^+)^T\\epsilon^T$. The second term is zero due to the independence of $\\epsilon$, and the first is a quadratic form dependent on $XX^+$. \n\n**Comparison to our setup (noisy inputs)**\n\nLet us now compare this to our setup. Since we have noise on the input, our estimate is \n\n$$ \\hat{\\beta} = \\beta^T X (X+A)^+$$\n\nTo start off, __we already cannot decouple the noise and the data.__ Hence, this is one challenge that we had to overcome.  In the output noise setup, since the two decouple, we only need to understand the spectrum of $X$. In our case, we need to understand the spectrum of $X+A$.\n\nIn the classical regime ($d$ fixed, $N$ to infinity), this would be easier since $(X+A)(X+A)^T$ would converge (with appropriate normalization) to the covariance matrix $XX^T$. However, there are __two issues__ in our problem that complicate this:\n\n1. First, we are not in the classical regime. In the proportional regime, as noted by Cheng and Montanari and many other prior works, $(X+A)(X+A)^T$ is not a consistent estimate for the covariance matrix.\n\n2. Second, we actually do not have terms of the form $(X+A)(X+A)^T$ (as was the case in the output noise situation). We have terms of the form $X(X+A)^T$. While this is subtle, it makes a significant difference. \n\nPoints 1 and 2 combined imply that we must understand the eigenstructure of $X+A$. Specifically, point two asks us to study $X(X+A)^T$, which implies that we need to understand the alignment between the eigenvectors of $X$ and the eigenvectors of $X+A$. Studying the eigenvectors of a perturbed matrix **is very difficult.** Further, we don't need to understand just the leading eigenspaces, but __all eigenvectors.__\n\n**We believe that the fact we got around this hurdle with accessible techniques is, in fact, a strength of our work.**\n\n**When the Expectation is Taken**\n\nAnother fact to highlight is that we compute the expectation over training data noise _after_ computing the test error for $W_{opt}$. Recall that $W_{opt}$ is trained to minimize the MSE error over a _single_ noise instance. If we computed the test error for a linear function minimizing the _expected_ MSE error (corresponding to $W^*$), the result is much simpler. This latter case is exactly the case of learning a regularized auto-encoder."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111378588,
                "cdate": 1700111378588,
                "tmdate": 1700147541148,
                "mdate": 1700147541148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8lqWpB1wTu",
                "forum": "WmB803HJkD",
                "replyto": "6yKTAGgNm0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ],
                "content": {
                    "title": {
                        "value": "Thanks and reply"
                    },
                    "comment": {
                        "value": "First, thank you to the authors for the response.  I am happy to leave my score as is: In my opinion, this will be\u2014upon further editing\u2014an interesting contribution, but I think it falls short (again in my opinion) of the bar for ICLR.  My main suggestion for improvement would be for the authors to more critically (and fairly) situate their work in the context of previous literature.  As an aside, I would like to echo Reviewer jx5r regarding the cherry picking of comments in the summary.\n\nI would like to clarify some points of my review and provide some concrete suggestions for improvement.  Also, to clarify, I agree with the authors that simple, accessible proofs are desirable and that technical difficulty for the sake of technical difficulty is not a useful exercise.  My apologies if my initial review signaled otherwise.\n\n- You\u2019ve provided a phenomenological study in which you propose a simple model which reflects some aspects of more realistic situations and analyze the simpler set-up to provide insight.  I have no issue with this broad set-up.  However, there are two axes on which to evaluate the study.  First regards the obtained qualitative insights and how well they reflect reality, and second regards the technical insights derived from the simplified model.  I am not sure that there is much novelty in either direction.  \nFor instance, regarding the qualitative insights, the authors mention avoiding c \\approx 1.  Why, aside from the difference in simple models considered, is that not an insight already evident from, e.g., [1]? \n\n- Thank you to the authors for their response regarding the difficulty in the analysis.  I read through the proof of Theorem 1 again in the appendix (as well as the response to reviewer jx5r), but am unable to articulate precisely what the author\u2019s key technical insights here were.  Could you please point me to the lemmas which attack this issue you have mentioned, and in particular where the difficulty arises as compared to Sonthalia and Nadakuditi (2023)? Many of the estimates used in the proof of Theorem 1 seem to come from that paper.  \n\n- Again, regarding technical insights and novelty, I want to emphasize that part of the confusion here since the authors have not clearly situated themselves with respect to prior work.  In general, the authors frequently cite previous work and situate themselves positively with respect to the previous work, without mention of any differences (and perhaps strengths that the previous work may have and weaknesses of their own).  \n\n- There seem to be two directly related works cited by the authors: Pretorius et. al (2018), and Sonthalia and Nadakuditi (2023).  I do agree that the results obtained here are improvements upon these.\n\n- However, there are misleading and superficial comparisons to other previous work.  For instance, the result of Cheng and Montanari (2022) considers kernel ridge regression (and not in the input noise set-up).  There is no additive \u201cMarchenko-Pastur\u2013like\u201d noise in that work; rather the random matrices involved are non-standard and require quite a bit of care to deal with.  In particular, my impression of the analysis done in this paper is that it is precisely the nice assumptions on the additive noise which enable the guarantees.  This is not emphasized at all in the manuscript.\n\n- Related to the previous point: It is likely that I have missed something crucial here, and it would help for the authors to clarify this.  In your paragraph on \u201cComparison on assumptions in prior work\u201d, you point out that most authors assume something along the lines of $x \\sim \\mathcal{N}(0, \\Sigma)$, and stress that you make less stringent assumptions on the independence conditions.  Perhaps I have mis-interpreted, but it seems to me that it might be a more fair comparison to be with respect to the centered data assumption? In which case, if we take the signal-less setting for both, the assumption from previous work is more general? Again, perhaps I\u2019ve mis-interpreted this point.  \n\n- To give one more example of potentially unfair comparisons, the authors provide a comparison to Cui and Zdeborova (2023), and simply say that the assumptions here are strictly more general than in Cui and Zdeborova (2023).  I am not sure this is true.  While your assumptions on the data are more general, they consider a seemingly more realistic, and more complicated, denoising model.  It seems to me that it is a bit misleading to compare your results to previous work in such a way that only favors your results and disregard other aspects of the work.  I bring up these points to give just a few examples, but the paper has many such, which I would suggest the authors to improve.  \n\n- One other big missing piece is comparisons and citations to results already in the random matrix theory literature.  \n\n[1] Hastie, et. al. \u201cSurprises in High-Dimensional Ridgeless Least Squares Interpolation\u201d"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483621663,
                "cdate": 1700483621663,
                "tmdate": 1700483621663,
                "mdate": 1700483621663,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zb5bkIREkJ",
                "forum": "WmB803HJkD",
                "replyto": "7ZPqqTZTKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comment and engagement and apologize for the summary.\n\n> My main suggestion for improvement would be for the authors to more critically (and fairly) situate their work in the context of previous literature. \n\nWe thank the reviewer for their comment. Our main difficulty is that in the denoising setting, there seems to be very few works. (Sonthalia and Nadakuditi 2023, Pretorius et al. 2018, and Cui and Zdeborova 2023). The reviewer agrees that we generalize the first two. We have modified the paper to highlight the strengths of all three papers. Primarily, Cui and Zdeborova 2023.\n\n> As an aside, I would like to echo Reviewer jx5r regarding the cherry picking of comments in the summary.\n\nWe apologize. We had hoped to identify the main concerns for each reviewer. We have updated the summary. \n\n> I would like to clarify some points of my review and provide some concrete suggestions for improvement. Also, to clarify, I agree with the authors that simple, accessible proofs are desirable and that technical difficulty for the sake of technical difficulty is not a useful exercise. My apologies if my initial review signaled otherwise.\n\nWe are thankful for the concrete suggestions. We hope that we have addressed most of these in the updated manuscript. \n\nWe are also thankful that the reviewer agrees that simple proofs are desirable. Hence, we hope that the simplicity is not held against us. \n\n> You\u2019ve provided a phenomenological study in which you propose a simple model which reflects some aspects of more realistic situations and analyze the simpler set-up to provide insight ... is that not an insight already evident from, e.g., [1]?\n\n[1] does tell us that avoiding $c = 1$ is necessary. However, that is not the only insight from our paper. Specifically, our insights are novel **when considering distribution shifts for denoising.** This is where we believe many of our insights appear. As we mentioned, there are very few works that study denoising. As far as we can tell, only Sonthalia and Nadukiti 2023 consider distribution shifts, but in a limited manner. \n\n**With the importance of denoising, we believe that having a simple model for which we can theoretically justify (known) phenomena is essential.** Specifically, we show\n\n1. Double descent occurs even when the test data is from a different distribution. \n2. We show that data augmentation hurts in the $c > 1$ regime. While this can be thought of as understood from prior work such as [1]. Due to the inherent dependence in the data when doing data augmentation. We believe having a simple model where such results are theoretically backed is important. \n3. As far as we can tell, The results on tempered and benign overfitting for denoising are also new. These do provide the interesting qualitative insight that when we have a small amount of training data, we can improve the denoiser by increasing the denoiser (**even with distribution shift**) by increasing over parameterization. \n\n> Thank you to the authors for their response regarding the difficulty in the analysis ... Many of the estimates used in the proof of Theorem 1 seem to come from that paper.\n\nWe agree with the reviewer that we do not have new, deep, technical mathematical results. However, the analysis here is not straightforward. We highlight some of the difficulties. \n\n1. Lemma 4, which considers the concentration of the inverse of matrices. This is not needed in Sonthalia and Nadakuditi 2023 as they don't have to invert matrices. \n2. Lemma 5, for bounding the variance of each entry, we consider the square of the matrix. We show that we can estimate this. Hence, by symmetry, we can bound the variance of the sums of the terms and, then, the individual terms. This argument is new and is not present in Sonthalia and Nadakuditi 2023. \n3. Lemma 7, we had to bound the variance of the product of random variables. Sonthalia and Nadakuditi 2023 did not need to do this. \n\n> Again, regarding technical insights and novelty, ... without mention of any differences (and perhaps strengths that the previous work may have and weaknesses of their own).\n\nWe tried to highlight the differences between prior work and ours \u2014 specifically, the difference in data setting. We have added a paragraph in the introduction concerning some of our method's limitations and highlighted the strength of some prior work. \n\n> There seem to be two directly related works cited by the authors: Pretorius et al. (2018) and Sonthalia and Nadakuditi (2023). I do agree that the results obtained here are improvements upon these.\n\nWe agree with the reviewer."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489739012,
                "cdate": 1700489739012,
                "tmdate": 1700492129742,
                "mdate": 1700492129742,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CvdYmk2dLE",
                "forum": "WmB803HJkD",
                "replyto": "7ZPqqTZTKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Continued."
                    },
                    "comment": {
                        "value": "> Cheng and Montanari (2022)\n\nWe want first to clarify that we are looking at the same paper. Is the reviewer referring to https://arxiv.org/pdf/2210.08571.pdf ?\n\nFurther, we are uncertain where we say Cheng and Montanari have input noise. Could the reviewer please point that out to us? We cite this paper in 4 places. \n\n1. To motivate the problem of having poorly conditioned covariances. This was done to help motivate the study for low rank data. \n2. As a reference to prior work that mentions the proportional regime. \n3. When we talk about their data assumption. How they have assumptions similar to $x\\_i = \\Sigma^{1/2}z\\_i$. Directly quoting from their paper \n\n``We allow the feature vector to be high-dimensional, or even infinite-dimensional, in which case it belongs to a separable Hilbert space, and assume either $z\\_i = \\Sigma^{-1/2}x\\_i$ to have i.i.d. entries, or to satisfy a certain convex concentration property.''\n\nFurther, their paper does have additive Marchenko Pastur noise and, unless we are mistaken, is not about kernel regression. Again, quoting directly from their paper. \n\n``we revisit ridge regression (l2-penalized least squares) on i.i.d. data $(x\\_i, y\\_i), i \\le n$, where $x\\_i$ is a feature vector and $y\\_i = \\langle \\beta, x\\_i\\rangle + \\xi\\_i \\in \\mathbb{R}$ is a response.''\n\nPage 4 details the assumptions on $\\xi\\_i$, which are mean 0, variance $\\tau^2$, and I.I.D. Theorems 1,2,3 on pages 9, 10, and 11 seem to be the main results. However, none of them are for kernel regression. Theorem 1 is for the ridge regression situation, and Theorems 2 and 3 are for the ridgeless case for the over and under-parameterized cases.\n\n**Due to these differences, we believe we and the reviewer are potentially referring to different papers.**\n\n> Related to the previous point: It is likely that I have missed something crucial here ... Again, perhaps I\u2019ve mis-interpreted this point.\n\nApologies, but we are not certain we follow. \n\nThe point here was to compare our data assumptions with prior work. Many prior works assume $x_i \\sim \\mathcal{N}(0,\\Sigma)$ and IID, whereas our paper studies $x_i \\in \\mathcal{V}$ in some low dimensional subspace. We do not need the data to be centered in our paper. Hence, we are unsure about the reviewer's point. \n\n> To give one more example of potentially unfair comparisons, the authors provide a comparison to Cui and Zdeborova (2023), \n\nWe have updated the manuscript to mention that Cui and Zdeborova 2023 have a more general model. \n\n> One other big missing piece is comparisons and citations to results already in the random matrix theory literature. \n\nWe do cite the original works showing convergence of the spectrum. However, since, as the reviewer points out and we agree with, we are not proving new random matrix theory results, we do not have an in-depth literature review. However, if the reviewer feels there are papers that we should mention, please let us know. \n\n-----\n\nWe thank the reviewer again for their comments. We hope that with these discussions, we clear doubts in relation to our contribution and relation to prior work. We hope that the updated manuscript reflects this."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489863185,
                "cdate": 1700489863185,
                "tmdate": 1700490398011,
                "mdate": 1700490398011,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oI8BOlzRiH",
                "forum": "WmB803HJkD",
                "replyto": "7ZPqqTZTKD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ],
                "content": {
                    "title": {
                        "value": "Re: \"Response continued\""
                    },
                    "comment": {
                        "value": "Thanks very much to the authors for pointing this out: you are entirely correct that the Cheng and Montanari (2022) paper does not study kernel ridge regression, my apologies for this (this was an unfortunate typo that I did not catch after erasing a sentence).  I have updated the comment to reflect this.  Thanks once more, and my apologies again for this typo.  I also did not mean to imply that you mentioned their work as studying input noise.  \n\nI will try to make clearer what I was trying to say while clarifying the centering issue I brought up:\n\n> The point here was to compare our data assumptions with prior work. Many prior works assume and IID, whereas our paper studies in some low dimensional subspace. We do not need the data to be centered in our paper. Hence, we are unsure about the reviewer's point. \n\n\nTo clarify, what I am wondering is whether it is more accurate to compare your results to a setting in which data is not centered, but the covariance of the noise is well-behaved? I.e., a more closely related model to yours seems to me to be the situation in which the observed data is X_{trn} + G, where the entries of G are standard Gaussian.  If I am correct about this, it is quite different from the $\\mathcal{N}(0, \\Sigma)$ setting you compare to, and my reference to Cheng and Montanari (2022) was to contrast these.  I hope this is clearer, and apologies for the confusion."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492579786,
                "cdate": 1700492579786,
                "tmdate": 1700492637399,
                "mdate": 1700492637399,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LKYyV5bAVf",
                "forum": "WmB803HJkD",
                "replyto": "zb5bkIREkJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ],
                "content": {
                    "title": {
                        "value": "Re: Response"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the quick response and for clarifying the points of difference between Sonthalia and Nadakuditi (2023).  I still feel that the contribution is rather incremental and stand by my suggestion in the initial review that it may be a more appropriate submission to, for instance, TMLR.  Of course, given the subjective nature of this, if the other reviewers and the area chairs have a conflicting opinion, I am happy with whatever they choose."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492911412,
                "cdate": 1700492911412,
                "tmdate": 1700492911412,
                "mdate": 1700492911412,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TsfegRRuyN",
                "forum": "WmB803HJkD",
                "replyto": "W12a4stycN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6422/Reviewer_RbRx"
                ],
                "content": {
                    "title": {
                        "value": "Re: Non centered data"
                    },
                    "comment": {
                        "value": "Thanks for clarifying.  I do stand by my point above then.  It would be far clearer to avoid comparisons to the $\\mathcal{N}(0, \\Sigma)$ setting entirely, or to explicitly mention that you weaken assumptions on the noise and consider distributions with different means.  As I mentioned before, almost all of the authors' references to previous work painted your results in a favorable light.  I think it would be far more beneficial to compare your results critically to past work.  I do appreciate what you have pointed out on the lack of references in your specific setting.  I suppose it would be more beneficial to spend more time motivating your specific setting (which as I mentioned in a previous comment, I do not find the motivation for the linear setting you consider to be quite satisfactory), rather than drawing potentially misleading comparisons to prior work in quite different settings.  \n\nI very much appreciate the effort the authors have put into their responses, but I will elect to keep my score as is."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495328493,
                "cdate": 1700495328493,
                "tmdate": 1700495328493,
                "mdate": 1700495328493,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]