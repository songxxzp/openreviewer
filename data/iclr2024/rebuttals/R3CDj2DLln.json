[
    {
        "title": "Disentangled Acoustic Fields For Multimodal Physical Scene Understanding"
    },
    {
        "review": {
            "id": "NxOeL6dOg7",
            "forum": "R3CDj2DLln",
            "replyto": "R3CDj2DLln",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2633/Reviewer_RUoW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2633/Reviewer_RUoW"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with the problem of predicting acoustic scenes such as types, materials, and positions of objects, and proposes a bi-directional encoder-decoder architecture for predicting acoustic scenes as well as synthesizing scene sounds from scene parameters. Since the proposed method assumes the existence of pairs of scene parameters and scene sounds as training examples, we can train both the encoder and the decoder in a supervised manner. This paper also presents a way of visualizing the prediction uncertainty of sound source locations and its application to agent navigation. Experimental evaluations with two public benchmarks demonstrate that the proposed method outperformed simple baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed architecture is interesting. If we can obtain lots of pairs of audio signals and object properties, the proposed architecture might be one of the best choices for simultaneously analyzing and synthesizing audio scenes.\n\n2. Based on the experimental evaluations, the proposed method reasonably work well for several tasks."
                },
                "weaknesses": {
                    "value": "1. The task of this paper is closely related to sound source localization. If the authors justify the effectiveness of the proposed method in terms of position errors of sound sources, experimental comparisons with several previous methods for this task would be mandatory. As presented in [Grumiaux+ JASA 2021 https://arxiv.org/abs/2109.03465], lots of previous methods have already been proposed for this purpose. I understand that many of those previous methods typically require multi-channel audio inputs and the proposed method employs a single-channel or binaural audio clip. However, you will find several techniques that work well only with single-channel audio, since single-channel sound source localization has been one of the typical tasks in audio signal processing before the deep learning era.\n\n2. If we want to simply predict acoustic scenes from audio signals, we do not have to introduce decoders that generate audio signals from object properties. In this sense, the ablation study that compares the full proposed method and the one without decoders should be presented.\n\n3. If the main objective of this paper is a proposal of a novel construction of neural acoustic fields, the proposed method should be compared with the original NAFs in the task of audio synthesis."
                },
                "questions": {
                    "value": "Please check the above Weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698135653726,
            "cdate": 1698135653726,
            "tmdate": 1699636203094,
            "mdate": 1699636203094,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ukQh34UUHV",
                "forum": "R3CDj2DLln",
                "replyto": "NxOeL6dOg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RUoW"
                    },
                    "comment": {
                        "value": "We are greatly encouraged by your assessment that our proposed architecture might be one of the best choices for simultaneously analyzing and synthesizing audio scenes. In line with your suggestions, we have conducted additional experiments to further demonstrate our method's superiority over baseline approaches. Additionally, we have refined our manuscript with more specific descriptions to clearly address each of your concerns. These updates, we believe, strongly validate the effectiveness of our approach. We address your concerns one by one as follows.\n\n> **Q1) Additional baselines in audio localization**\n> \nIn response, we choose two strong methods after reading the survey on sound source localization methods [4] and implement them with pytorch. One feeds the STFT of the audio into U-Net[5], and the other feeds the original waveform into CNN [6]. To evaluate the performance of our model in sound source localization, we employed the REALIMPACT dataset[3]. The experimental results, presented in the table below, clearly demonstrate that our proposed model outperforms the established baseline methods. **We appreciate the guidance to substantiate our claims through comparative analysis and believe these additional results strengthen the effectiveness of our proposed approach.**\n\n| Method   | Angle Acc. | Height Acc. | Distance Acc.|\n|---------|---------|----------|-------|\n| Chance  | 0.100    | 0.067    | 0.250|\n| Unet + STFT [5] |  0.825    |  0.902  | 0.972  |\n| CNN + Waveform [6]  |  0.671    |   0.755   |  0.802|\n| Resnet + STFT [3] | 0.758    | 0.881    | 0.983|\n| Ours  | **0.900**    | **0.960**    | **0.994**|\n\n> **Q2) Ablation study with encoder only**\n\nWe agree that the comparison of methods with and without the decoder component is crucial for evaluating the efficacy of the decoder in our model. But we would like to clarify that in Section 4.2 of our paper, **we have already conducted an analysis comparing the performance of our proposed method with a variant that does not include the decoder**. Experiments revealed that while the Success Rate (SR) remained comparable, there was a noticeable decrease in Shortest Path Length (SPL) and Sound Navigation Accuracy (SNA) metrics without the decoder.\n\nTo address your concerns, we will enhance the clarity of this comparison in our manuscript, better articulating the critical role the decoder plays in the overall performance of our method.\n\n> **Q3) Comparison on audio synthesis**\n\n\nIn response to reviewer RUoW's suggestion for a comparative evaluation with Neural Acoustic Fields (NAF) [1] in audio synthesis task, we collaborated with the NAF authors to create a baseline model equivalent to NAF. We then conducted a comparative analysis of our approach and NAF using 15,000 audio files from the REALIMPACT dataset[3]. This analysis focused on the accuracy of audio simulation, measured by comparing the Mean Squared Error (MSE) of the reconstructed Power Spectral Density (PSD) against the true PSD. The results, detailed in the accompanying table, clearly demonstrate our method's superior performance in audio simulation tasks compared to NAF, effectively addressing the reviewers' concerns.\n\n\n| Model   | NAF | Ours | \n|---------|---------|----------|\n| PSD MSE  | 0.0264    | 0.0052    | \n\n\n\n\n\nThank you for the suggestions. We will include these details and change our notation to improve the clarity of our paper. \n\n\n\n[1] Luo, Andrew, et al. \"Learning neural acoustic fields.\" NeurIPS (2022).\n\n[2] Gan, Chuang, et al. \"Finding fallen objects via asynchronous audio-visual integration.\" CVPR(2022).\n\n[3] Clarke, Samuel, et al. \"RealImpact: A Dataset of Impact Sound Fields for Real Objects.\" CVPR(2023).\n\n[4] Grumiaux, Pierre-Amaury, et al. \"A survey of sound source localization with deep learning methods.\" The Journal of the Acoustical Society of America 152.1 (2022).\n\n[5] Patel, S., Maciej Zawodniok, and Jacob Benesty. \"Dcase 2020 task 3: A single stage fully convolutional neural network for sound source localization and detection.\" DCASE2020 Challenge (2020).\n\n[6] He, Yuhang, Niki Trigoni, and Andrew Markham. \"SoundDet: Polyphonic moving sound event detection and localization from raw waveform.\" ICML(2021)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186223065,
                "cdate": 1700186223065,
                "tmdate": 1700186304262,
                "mdate": 1700186304262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fAVeq7RqOA",
                "forum": "R3CDj2DLln",
                "replyto": "NxOeL6dOg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions remaining?"
                    },
                    "comment": {
                        "value": "Dear Reviewer RUoW,\n\nTo streamline our responses:\n\n1. **Q1) Additional Baselines in Audio Localization:**\n   - Implemented two strong baseline methods for sound source localization based on survey recommendations [4].\n   - Utilized the REALIMPACT dataset[3] for evaluation, showcasing our model's superiority over established baselines.\n   - Results demonstrate the robustness and effectiveness of our proposed approach in comparison to alternative methods.\n\n\n2. **Q2) Ablation Study with Encoder Only:**\n   - Clarified that an ablation study comparing our method with and without the decoder component has already been conducted (Section 4.2).\n   - Acknowledged the critical role of the decoder in improving metrics such as Success Rate (SR), Shortest Path Length (SPL), and Sound Navigation Accuracy (SNA).\n   - Enhanced the manuscript to better articulate the significance of the decoder in our model's overall performance.\n\n3. **Q3) Comparison on Audio Synthesis:**\n   - Collaborated with NAF authors for a comparative evaluation in the audio synthesis task, addressing the concern raised by reviewer RUoW.\n   - Conducted a thorough analysis using 15,000 audio files from the REALIMPACT dataset[3], demonstrating superior performance in audio simulation tasks compared to NAF.\n   - Results, as presented in the table, illustrate the improved accuracy of our method in audio synthesis.\n\n| Model   | NAF | Ours | \n|---------|---------|----------|\n| PSD MSE  | 0.0264    | 0.0052    | \n\nThank you for your insightful feedback. If you have any remaining questions or concerns following our response, please let us know. We\u2019d be very happy to do anything we can that would be helpful in the time remaining!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528327481,
                "cdate": 1700528327481,
                "tmdate": 1700528327481,
                "mdate": 1700528327481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7r3mASjvag",
                "forum": "R3CDj2DLln",
                "replyto": "NxOeL6dOg7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Reviewer_RUoW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Reviewer_RUoW"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the effort"
                    },
                    "comment": {
                        "value": "Thanks for the effort in the rebuttal. I have read through all the author replies and I found that all the replies are reasonable. I think that this paper can be accepted if (1) all the author replies will be contained in the main paper (not in the supplementary material) and (2) there is a room for the acceptance."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729766490,
                "cdate": 1700729766490,
                "tmdate": 1700729766490,
                "mdate": 1700729766490,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HWoLCE7SeB",
            "forum": "R3CDj2DLln",
            "replyto": "R3CDj2DLln",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2633/Reviewer_reBa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2633/Reviewer_reBa"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a disentangled acoustic fields (DAF) as a complementary in acoustic modal for physical scene understanding. The DAF could potentially help embodied agent to construct a spatial uncertainty map over where the objects may have fallen."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe main contribution of this paper is to enhance audio perception by so-called analysis-by-synthesis framework. The major strength is to maintain the (generated/synthesis one) power spectral density (PSD) consistency with the input audio.   \n2.\tThe downstream multi-modal planning experiments demonstrates the effectiveness of proposed framework."
                },
                "weaknesses": {
                    "value": "1. The title \u201cphysical scene understanding\u201d could be overclaimed the contribution since the audio perception is limited to constrained scenarios (e.g., fallen objects).\n2. Though the DAFs or (the predict-generate) is novel in audio modality, it is not such innovative and is close to use the cycle-consistency to ensure robustness in vision modality. I would suggest this work more like an ICRA paper instead of ICLR."
                },
                "questions": {
                    "value": "1. The title \u201cphysical scene understanding\u201d could be overclaimed the contribution since the audio perception is limited to constrained scenarios (e.g., fallen objects).\n2. Though the DAFs or (the predict-generate) is novel in audio modality, it is not such innovative and is close to use the cycle-consistency to ensure robustness in vision modality. I would suggest this work more like an ICRA paper instead of ICLR."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698459488705,
            "cdate": 1698459488705,
            "tmdate": 1699636203012,
            "mdate": 1699636203012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "womEIHxRxn",
                "forum": "R3CDj2DLln",
                "replyto": "HWoLCE7SeB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer reBa"
                    },
                    "comment": {
                        "value": "Thank you for your constructive review. We carefully address your concerns as follow:\n\n\n\n> **Q1) A more informative title**\n\n\nWe appreciate your suggestion regarding the title \"physical scene understanding\", given that our work primarily focuses on using audio information from objects impacting surfaces (which may not always be falling) to infer physical properties like location, size, and material. To more accurately represent the scope of our research, we propose modifying the title to \"Disentangled Acoustic Field for **Physical Property Understanding\"** This change better aligns the title with the goal and contributions of our work.\n\n\n\n\n> **Q2) Cycle-consistency in audio**\n\n\nOur utilization of the analysis-by-synthesis framework is fundamentally geared towards enhancing the model's comprehension of physical properties through audio perception, rather than focusing on robustness in vision. This approach is distinct from cycle-consistency in vision, as it specifically targets the underconstrained task of inferring and understanding acoustic scenes and their physical characteristics.\n\n> **Q3) Topic relevance.**\n\n\nWe wish to emphasize that the core contribution of our work is the introduction of a Disentangled Acoustic Field (DAF), which significantly enhances the perception of an object's physical characteristics, including the material, type, and size. \n\n**Our research, while applicable to robotics, proposes an audio decomposition representation has broader applications in understanding the physical properties of objects that emit sounds**. It aligns with the broad themes in learned representations traditionally explored at ICLR. This assertion is substantiated by numerous precedents of similar research [3][4][5][6] being accepted and published at ICLR. Therefore, we firmly believe that our paper is aptly suited for ICLR, contributing to its diverse and innovative discourse.\n\n\n\n\nThank you again for your advice and feedback. We will incorporate the suggestions into the paper. \n\n[1] Luo, Andrew, et al. \"Learning neural acoustic fields.\" NeurIPS (2022).\n\n[2] Gan, Chuang, et al. \"Finding fallen objects via asynchronous audio-visual integration.\" CVPR(2022).\n\n[3] Abdullah, Hadi, et al. \"Demystifying limited adversarial transferability in automatic speech recognition systems.\" ICLR. 2021.\n\n[4] Ding, Shaojin, Tianlong Chen, and Zhangyang Wang. \"Audio lottery: Speech recognition made ultra-lightweight, noise-robust, and transferable.\" ICLR(2021).\n[5] Shim, Kyuhong, Jungwook Choi, and Wonyong Sung. \"Understanding the role of self attention for efficient speech recognition.\" ICLR(2021).\n\n[6] Lam, Max WY, et al. \"BDDM: Bilateral denoising diffusion models for fast and high-quality speech synthesis.\" ICLR (2022)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186149656,
                "cdate": 1700186149656,
                "tmdate": 1700186207714,
                "mdate": 1700186207714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ns67b7Vze2",
                "forum": "R3CDj2DLln",
                "replyto": "HWoLCE7SeB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions remaining?"
                    },
                    "comment": {
                        "value": "Dear Reviewer reBa,\n\nThank you for your constructive review. We appreciate your valuable feedback and have addressed your concerns as follows. To streamline our responses:\n\n1. **Q1) A More Informative Title:**\n   - Proposed a modified title: \"Disentangled Acoustic Field for Physical Property Understanding\" to better reflect the focus on inferring physical properties from impacting sounds.\n\n2. **Q2) Cycle-Consistency in Audio:**\n   - Clarified that our use of the analysis-by-synthesis framework aims to enhance audio perception for inferring physical properties, distinct from cycle-consistency in vision.\n\n3. **Q3) Topic Relevance:**\n   - Emphasized the core contribution of our work, the Disentangled Acoustic Field (DAF), and its broader applications in understanding the physical properties of sound-emitting objects.\n   - Asserted the relevance of our research to ICLR, aligning with the diverse and innovative discourse present in previous ICLR publications [1][2][3][4][5][6].\n\nIf you have any remaining questions or concerns following our response, please let us know. We\u2019d be very happy to do anything we can that would be helpful in the time remaining!\n\nThank you again for your time and thoughtful feedback."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527924267,
                "cdate": 1700527924267,
                "tmdate": 1700527939999,
                "mdate": 1700527939999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bnIOMD99dL",
                "forum": "R3CDj2DLln",
                "replyto": "HWoLCE7SeB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Reviewer_reBa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Reviewer_reBa"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors\u2019 efforts in rebuttal. Considering the contribution of this work and other reviewer's comments, I have decided to retain my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661672599,
                "cdate": 1700661672599,
                "tmdate": 1700661672599,
                "mdate": 1700661672599,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "86N5OKKfYa",
            "forum": "R3CDj2DLln",
            "replyto": "R3CDj2DLln",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2633/Reviewer_KbFD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2633/Reviewer_KbFD"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel approach to multimodal physical scene understanding that leverages a disentangled acoustic field model to capture the sound generation and propagation process. This approach enables the embodied agent to construct a spatial uncertainty map over where the objects may have fallen, which can be used to improve the success rate for the localization of fallen objects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ Transforming the sound from the waveform domain into power spectral density (PSD) representation rather than sound reconstruction is well-motivated.\n\n+ The proposed disentangled acoustic fields (DAFs) are an interesting and technically sound model that explicitly disentangles sounds into several different acoustic factors.\n\n+ DAFs can be used to infer the physical properties of a scene, represent uncertainty, and navigate and find fallen objects."
                },
                "weaknesses": {
                    "value": "+ A video demo would be very helpful for us to understand the model's performance on the localization of fallen objects in the real world.\n\n\n+ Currently, all of the experiments are conducted on synthetic datasets. It would be interesting to see how the model generalizes to real-world data.\n\n+ The proposed method requires full labels to train DAFs. It would be beneficial to develop a self-supervised learning approach to avoid using many labels during model training.\n\n+ Why not incorporate visual information into DAFs? Visual data can provide many acoustic cues."
                },
                "questions": {
                    "value": "See the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699078224004,
            "cdate": 1699078224004,
            "tmdate": 1699636202930,
            "mdate": 1699636202930,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BMkVDYL9L6",
                "forum": "R3CDj2DLln",
                "replyto": "86N5OKKfYa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KbFD"
                    },
                    "comment": {
                        "value": "We are strongly encouraged by your evaluation that our work is an interesting and well-motivated, and can technically perform well in multiple tasks. In response to the valuable feedback provided, we have taken several steps to address the concerns raised. First, we have produced demonstrative videos to visually showcase our model's effectiveness in localizing fallen objects, offering a clearer understanding of its real-world application. We have further revised the experimental sections of our manuscript to provide more clarity on our methods on real-world scenarios. We also acknowledge the importance of the suggestions regarding the potential of self-supervised learning to reduce dependency on labeled data, and the integration of visual information into our DAFs model. We discuss our approach to incorporating these elements in future work, aligning with the ongoing evolution of our research.\n\nWe believe these revisions address your suggestions and open exciting avenues for the advancement of our work.\n\n\n\n> **Q1) Suggestion for a video demo**\n\n\nWe strongly agree with the reviewer's suggestion for a video demonstration. We've created a video that showcases our model in action, with RGB images and semantic segmentation results during navigation. This website also includes a top-down view of the loss map and navigational trajectory, helping to illustrate how our model processes and interprets environmental data. We believe this video will effectively demonstrate the capabilities of our model in audio-visual navigation and object localization. For a detailed visual representation, we have provided the video link: [***https://sites.google.com/view/disentangled-acoustic-fields***](https://sites.google.com/view/disentangled-acoustic-fields). \n\n\n\n> **Q2) Experiments on real data**\n\nThanks for your kind suggestion. We want to clarify that our study **has already used real-world data (REALIMPACT dataset [3]) for the object property task**. As detailed in Section 4.1 of our paper, the REALIMPACT dataset [3] contains 150,000 sound recordings from 50 different object categories. This provided a wide range of **real-world acoustic scenarios**. The results, shown in Table 2, demonstrate our method's improved accuracy in predicting physical properties like angle, height, and distance over baseline methods. This shows that our method generalizes well in real-world scenarios.\n\nTo further clarify this point in our manuscript, we will augment this section with more explicit and detailed description, thereby addressing the concerns raised.\n\n> **Q3) Self-supervised learning**\n\nWe deeply appreciate the reviewer's insightful suggestion regarding the implementation of a self-supervised learning approach for training our DAF. We agree that reducing the dependency on fully labeled data is a valuable and important direction for advancing the field.\n\nWe would like to note that our system is supervised using the PSD, which is derived from the input. Our system is indeed self-supervised in this sense. \n\nHowever, we recognize the limitations this approach may have in terms of scalability in more diverse real-world scenarios. Moving forward, we are interested in exploring addition self-supervised learning paradigms. We believe that such an approach could significantly enhance the versatility and practicality of our method, enabling it to learn from a wider array of data with minimal labeling requirements. \n\n> **Q4) Visual information in DAFs**\n\nWe agree with the reviewer's suggestion about the potential synergies between visual and audio information in enhancing environmental perception. This concept has been well-demonstrated in several studies [4]. For example, NAF [1] integrates sparse visual views with acoustic data to improve model performance. \n\nWe would like to note our current setup uses visual information as a modular component of our DAF navigation task. We acknowledge that there remains additional opportunity in integrating visual information, and believe it is an important avenue for future research.\n\n\n\n\n[1] Luo, Andrew, et al. \"Learning neural acoustic fields.\" NeurIPS (2022).\n\n[2] Gan, Chuang, et al. \"Finding fallen objects via asynchronous audio-visual integration.\" CVPR(2022).\n\n[3] Clarke, Samuel, et al. \"RealImpact: A Dataset of Impact Sound Fields for Real Objects.\" CVPR(2023).\n\n[4] Zhu, Hao, et al. \"Deep audio-visual learning: A survey.\" International Journal of Automation and Computing 18 (2021)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700186055475,
                "cdate": 1700186055475,
                "tmdate": 1700186055475,
                "mdate": 1700186055475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UPDbYdFUDq",
                "forum": "R3CDj2DLln",
                "replyto": "86N5OKKfYa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions remaining?"
                    },
                    "comment": {
                        "value": "Dear Reviewer KbFD,\n\nIn light of your positive evaluation of our work as interesting, well-motivated, and technically proficient across multiple tasks, we've made concerted efforts to address your valuable feedback. Here's a succinct summary of our responses:\n\n1. **Demonstration Videos and Experimental Clarity:**\n   - Produced videos to showcase our model's effectiveness in localizing fallen objects.\n   - Revised experimental sections for enhanced clarity on real-world scenarios.\n\n2. **Specific Suggestions Addressed:**\n   - **Q1) Video Demo:**\n     - Created a [video](https://sites.google.com/view/disentangled-acoustic-fields) illustrating our model.\n\n   - **Q2) Experiments on Real Data:**\n     - Clarified the use of real-world data (REALIMPACT dataset [3]).\n     - Demonstrated improved accuracy in predicting physical properties.\n\n   - **Q3) Self-supervised Learning:**\n     - Acknowledged the importance and highlighted our system's inherent self-supervised nature.\n\n   - **Q4) Visual Information in DAFs:**\n     - Explained the integration of visual information and emphasized its importance in future research.\n\nThese revisions aim to address your suggestions and pave the way for further advancements in our work.\n\nIf you have any remaining questions or concerns following our response, please let us know. We\u2019d be very happy to do anything we can that would be helpful in the time remaining!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527703191,
                "cdate": 1700527703191,
                "tmdate": 1700527703191,
                "mdate": 1700527703191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1jHE9ggn7Y",
            "forum": "R3CDj2DLln",
            "replyto": "R3CDj2DLln",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2633/Reviewer_oZCW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2633/Reviewer_oZCW"
            ],
            "content": {
                "summary": {
                    "value": "The authors study the problem of multimodal physical scene understanding to infer the fallen objects' properties, direction and distance of an impact sound source. To deal with the limitation of current work NAFs which only captures the structure and material properties of a scene. They propose the disentangled acoustic fields to model acoustic properties across a multitude of different scenes. However, the design of DAF is incremntal to NAFs and the experiments are using only two different scenarios, kitchen and study room, which may not prove the generalization of DAF."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem is interesting."
                },
                "weaknesses": {
                    "value": "1. The proposed work lacks novelty. The main contribution of this work is the introduction of DAFs, which are applied to the generative frameworks and multitask learning to address this task. Although it shows promising performance, it lacks some degree of innovation.\n2. The paper mentions that NAFs lack generalization ability for new scenarios, while DAFs can effectively solve this problem. However, no relevant comparative experiments are observed in the experimental section.\n3. The paper mentions that DAFs can address the generalization issue for different new scenarios, but the experimental section only demonstrates generalization for two different scenarios, kitchen and study room."
                },
                "questions": {
                    "value": "The authors claim that their method is generalizable, why the experiments are using only two different scenarios, kitchen and study room?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699545711006,
            "cdate": 1699545711006,
            "tmdate": 1699636202841,
            "mdate": 1699636202841,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "foUiO9nnx9",
                "forum": "R3CDj2DLln",
                "replyto": "1jHE9ggn7Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviwer oZCW"
                    },
                    "comment": {
                        "value": "We thank Reviewer oZCW for the constructive review. We address specific comments below. \n\n> **Q1) Contribution and novelty**\n\n\nWe would like to clarify that the relationship between our proposed DAF (Disentangled Acoustic Field) and NAF (Neural Acoustic Field) [1], as they tackle fundamentally different tasks. While it's true that DAF builds upon the foundational aspects of NAF, they are distinct in their application and capabilities. NAF primarily focuses on predicting audio propagation characteristics within a scene. In contrast, DAF extends this premise to a more complex task of inferring physical properties such as the location, material, and size of objects across unseen novel scenes, solely from audio information. This advancement represents a substantial shift in the application of acoustic modeling, enabling a broader scope of physical scene understanding beyond what NAF offers. \n\nThus, while DAF and NAF share some conceptual lineage, their respective applications and the complexity of the tasks they address are fundamentally different, underscoring the innovative leap our work represents in the realm of audio-based property analysis.\n\n> **Q2) Comparative experiments against NAF.**\n\n\n\nWe appreciate the reviewer's observation regarding the generalization ability tests. In response, we conduct a comparative evaluation with NAF [1] in audio synthesis task. we collaborated with the NAF authors to create a baseline model equivalent to NAF. We then made a comparative analysis of our approach and NAF using 15,000 audio files from the REALIMPACT dataset[3]. This analysis focused on the accuracy of audio simulation, measured by comparing the Mean Squared Error (MSE) of the reconstructed Power Spectral Density (PSD) against the true PSD. The results, detailed in the accompanying table, clearly demonstrate our method's superior performance in audio simulation tasks compared to NAF, effectively addressing the reviewers' concerns.\n\n\n\n\n| Model   | NAF | Ours | \n|---------|---------|----------|\n| PSD MSE  | 0.0264    | 0.0052    | \n\n\n\n> **Q3) Generalization across scenes**\n\nWe would like to clarify that our method not only generalizes across two scenarios. As detailed in the experimental setup of Section 4.2 in our study, we employed Find Fallen Challenge [2], a comprehensive dataset encompassesing 30 distinct physical object types situated within **64 uniquely configured rooms**. Importantly, these rooms are categorized into two primary types: study rooms and kitchens. However, it is crucial to note that each category itself comprises 32 different room variations, offering a wide array of acoustic environments. Each scenario presents its unique set of acoustic challenges, thereby providing a robust testbed for evaluating the generalization capabilities of our DAF. The revised manuscript will include a more detailed explanation of these diverse experimental conditions.\n\nMoreover, our experiments follows [2] to utilize the same test split to assess the baseline algorithm's generalization performance. We believe that this extensive and varied dataset, encompassing a broad range of scenarios, adequately demonstrates the generalization prowess of DAF.\n  \n\n  \n\n\n\n\n\nWe sincerely appreciate your comments. Please feel free to let us know if you have further questions. \n\n[1] Luo, Andrew, et al. \"Learning neural acoustic fields.\" NeurIPS (2022).\n\n[2] Gan, Chuang, et al. \"Finding fallen objects via asynchronous audio-visual integration.\" CVPR(2022).\n\n[3] Clarke, Samuel, et al. \"RealImpact: A Dataset of Impact Sound Fields for Real Objects.\" CVPR(2023)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185659437,
                "cdate": 1700185659437,
                "tmdate": 1700185769849,
                "mdate": 1700185769849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2PPBlfKvcW",
                "forum": "R3CDj2DLln",
                "replyto": "1jHE9ggn7Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions remaining?"
                    },
                    "comment": {
                        "value": "Dear R oZCW,\n\nTo streamline our responses:\n\n**1. Differentiation from NAF:**\n- DAF and NAF serve different purposes; NAF focuses on audio propagation, while DAF extends to infer physical properties. This represents a significant advancement.\n\n**2. Comparative Experiments with NAF:**\n- Collaborated with NAF authors for a comparative audio synthesis task. Results show DAF's superior performance (MSE: 0.0052 vs. NAF's 0.0264).\n\n**3. Generalization Across Diverse Scenarios:**\n- DAF generalizes across 64 physically diverse rooms from the Find Fallen Challenge dataset.\n\nIf you have any remaining questions or concerns, please feel free to let us know. We are eager to assist in any way possible within the given timeframe. Thank you for your time and consideration."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700527317971,
                "cdate": 1700527317971,
                "tmdate": 1700528057662,
                "mdate": 1700528057662,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]