[
    {
        "title": "On the Implicit Bias of Adam"
    },
    {
        "review": {
            "id": "kgNWn8Q0Yg",
            "forum": "ZA9XUTseA9",
            "replyto": "ZA9XUTseA9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1518/Reviewer_nNMy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1518/Reviewer_nNMy"
            ],
            "content": {
                "summary": {
                    "value": "The authors apply the backward error analysis method to find ODEs that determine continuous trajectories which are close to the discrete trajectories of the popular Adam and RMSProp adaptive gradient-based algorithms.  They succeed in doing this up to a discrepancy which is second-order in the step size, for variants of the two algorithms depending on whether the numerical stability hyperparameter $\\varepsilon$ is inside or outside the square root in the step equation, and for both mini-batch and full-batch cases.  The main result for Adam uncovers three different regimes that penalise the positive one-norm of the gradient, or the negative one-norm of the gradient, or the squared two-norm of the gradient, depending on whether the squared gradient momentum hyperparameter $\\rho$ is greater than the gradient momentum hyperparameter $\\beta$ or not, and whether $\\varepsilon$ is small or large compared with the components of the gradient (the latter two cases correspond to early or late phases of the training, respectively).  Some of the results in the literature are derived as special cases of the theorems in this work.  The paper also reports some numerical experiments that seem to confirm the theoretical results as well as suggest that the one-norm of the gradient is inversely correlated with generalisation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The introduction conveys clearly the place of this work in relation to the literature.\n\nThe summary of the main result in the full-batch case is helpful, and so are the discussion, the illustration using the bilinear model, and the suggestions of future directions.\n\nThe proofs are provided in the supplementary appendix, together with details of the numerical experiments."
                },
                "weaknesses": {
                    "value": "The introduction is a little dry.\n\nThe statement of the main result that follows its summary in the full-batch case is difficult to parse, and its connection with the summary that precedes it is not obvious.  A minor point is that equation 10 should not end with a full stop."
                },
                "questions": {
                    "value": "Can you say more about the graphs in Figure 1?  Why are we plotting the integral, and what can we conclude from the shapes of the various curves?\n\nWhat can you say about the situation when the numerical stability hyperparameter $\\varepsilon$ (or rather its square root) is neither small nor large in relation to the components of the gradient?  Can that be the case for a long period of the training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1518/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1518/Reviewer_nNMy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697843360207,
            "cdate": 1697843360207,
            "tmdate": 1699636080069,
            "mdate": 1699636080069,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b3hJOaCLMo",
                "forum": "ZA9XUTseA9",
                "replyto": "kgNWn8Q0Yg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your comments. Each point has been addressed in our response below.\n\n## Weakness 1\n\nThank you for pointing this out. We will add more intuition in the introduction.\n\n## Weakness 2\n\nThank you for the great suggestions. We have incorporated a brief outline of the derivation and included a comment discussing the relationship between the informal summary and Theorem 3.1 (see Remark 3.2). Thank you for identifying the typo; it has been corrected in the revised version.\n\n## Question 1\n\nFigure 1 illustrates what kind of function of the gradient is getting \"penalized\" by taking dimension $p = 1$. The bias term in Eq. (4), ignoring the $h / 2$ coefficient, is the derivative with respect to (one-dimensional) $\\theta$ of the function $F(E'(\\theta))$, where $F(x) := \\int_0^x \\bigl[ \\frac{1 + \\beta}{1 - \\beta} - \\frac{1 + \\rho}{1 - \\rho} + \\frac{1 + \\rho}{1 - \\rho} \\cdot \\frac{\\varepsilon}{y^2 + \\varepsilon}\\bigr] d \\sqrt{\\varepsilon + y^2}$ and $E'(\\theta)$ is the derivative (= gradient) of the loss. Specifically, $ 2/h \\cdot \\mathrm{bias} = \\frac{d}{d \\theta} F(E'(\\theta)) $ represents what is being penalized, akin to how gradient descent penalizes the loss. For small  $ \\varepsilon $, the penalty is effectively plus or minus the 1-norm of the gradient, as shown in the left picture of Figure 1, adjusted by a positive coefficient. As $ \\varepsilon $ increases, this term gradually shifts to resemble more closely the 2-norm of the gradient, again modified by a positive coefficient, reflecting Adam's convergence towards gradient descent.\n\n## Question 2\n\nWe believe that in practice $\\varepsilon$ is small and, during most of training, the first extreme case describes the situation better. We do, however, also have an understanding of what happens if $\\varepsilon$ is not small. In such cases, where $\\rho > \\beta $, the anti-regularization effect is less pronounced compared to regular Adam. This leads to more stable training, potentially improving generalization, particularly in situations where SGD is superior. Of course, this is contingent on adjusting the effective learning rate and $ h $, as detailed in Example 2.3. Our hypothesis is that increasing $ \\varepsilon$ and modifying the learning rate can bring training dynamics closer to momentum-GD, in line with Choi et al. (2019).\n\nConversely, in language model training, where researchers sometimes fit the data almost exactly and continue training (a phase associated with \"grokking\"), the situation may hover between the two extremes, leaning towards the second.  It is an interesting regime to study in the future, and perhaps the qualitative behavior of our bias term can contribute some insights.\n\nReference: Choi, D., et al. (2019). \"On Empirical Comparisons of Optimizers for Deep Learning.\" ArXiv:1910.05446."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092489118,
                "cdate": 1700092489118,
                "tmdate": 1700092489118,
                "mdate": 1700092489118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KkFEC2Qbiw",
                "forum": "ZA9XUTseA9",
                "replyto": "b3hJOaCLMo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1518/Reviewer_nNMy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1518/Reviewer_nNMy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for these responses."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700132345541,
                "cdate": 1700132345541,
                "tmdate": 1700132345541,
                "mdate": 1700132345541,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2fvyVIOK0t",
            "forum": "ZA9XUTseA9",
            "replyto": "ZA9XUTseA9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1518/Reviewer_71dm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1518/Reviewer_71dm"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the implicit bias of Adaptive Gradient Methods (especially RMSProp and ADAM) based on the modified equation of those methods derived by backward error analysis (Sections 2 and 3). As pointed out by the authors (in Related Work), given that research has primarily been conducted on Batch GD, SGD, and SGD with momentum, it is timely to explore the Adaptive Gradient Method. The authors demonstrated that ADAM implicitly penalizes (perturbed) one-norm of gradient depending on $\\varepsilon$ scale (Section 2) and empirically verified it (Sections 5 and 6)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The overall contents are easily understandable.\n* A timely issue, ADAM's modified loss, is addressed in the paper.\n* Though I did not review all the content in the Supplementary material, it appears technically correct.\n* The authors validated their findings using a Toy problem (Section 5) and practical neural networks (Section 6)."
                },
                "weaknesses": {
                    "value": "* While the overall content is easily comprehensible, specific details are inaccessible unless one reviews the supplementary material. Specifically, many theoretical works provide a sketch of the proof in the main paper to explain the techniques newly considered/developed by the authors and clarify how they differ from existing techniques. This paper lacks such details.\n\n* In addition, the experimental setup in Section 6 is not self-contained and has elements that seem arbitrary, making cherry-picking possible.\n  * In Section 6, the authors refer to Ma et al. (2022) to conduct experiments for the stable oscillation regime when $\\rho$ and $\\beta$ are \"sufficiently close\". How do you define \"sufficiently close\"? Most ML practitioners are familiar with situations where $\\rho$ (0.99~0.999) is larger than $\\beta$ (0.9). As the authors suggest, is this a spike regime or a stable oscillation regime? There should be a discussion for such questions within Section 6, but the authors merely refer to Ma et al. (2022) without providing specific details.\n  * In Figures 4 and 5, the authors fixed $h$, $\\beta$, and $\\rho$ at certain values and ran experiments by varying other parameters. What is the basis for these fixed values? While $h$ is fixed at different values (7.5e-5 and 1e-4) without specific mention, proper empirical evidence should warrant experimentation across various values. Moreover, for $\\beta$ and $\\rho$, they shouldn't just experiment with a single value. Instead, they should test multiple values and demonstrate consistent backing of their theoretical results.\n\n* The concept of ADAM's modified loss is timely. However, it seems that the resulting modified loss doesn't explain the differences between traditional ADAM and SGD. For example, as the authors mentioned, ADAM often provides worse generalization performance and sharper solutions than SGD. Yet, in NLP tasks using Transformers, ADAM significantly outperforms SGD [1,2]. Such observations lead to two natural questions regarding the authors' study:\n  * If one tunes the hyperparameters of ADAM based on the discovered implicit bias, can they reproduce results where SGD performs better?\n  * Can the authors explain scenarios where ADAM outperforms SGD using their discovered implicit bias (e.g., can they argue that the proposed perturbed one-norm regularization is more suitable for Self-Attention in Transformers than for Convolutions in ResNet?)\n\n[1] Zhang, Jingzhao, et al. \"Why are adaptive methods good for attention models?.\" Advances in Neural Information Processing Systems 33 (2020): 15383-15393.\n\n[2] Kumar, Ananya, et al. \"How to fine-tune vision models with sgd.\" arXiv preprint arXiv:2211.09359 (2022)."
                },
                "questions": {
                    "value": "The questions needed to improve the paper are included in the Weakness section. \nIf the questions are addressed appropriately, I am willing to raise the score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1518/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1518/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1518/Reviewer_71dm"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698215335022,
            "cdate": 1698215335022,
            "tmdate": 1700661744962,
            "mdate": 1700661744962,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KQG0TKjCCL",
                "forum": "ZA9XUTseA9",
                "replyto": "2fvyVIOK0t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you so much for your comprehensive response. We address each of the points you raised below.\n\n## Weakness 1\n\nWe wholeheartedly agree that including more details of the proof would be beneficial to the reader. To address this, we have introduced two additional levels of detail. Apart from the full proof in the Appendix, we have incorporated a 3-page derivation in Section SA-9, also in the Appendix. This derivation essentially forms the basis of the full proof, with the key difference being the characterization of all big-O bounds in precise terms. Additionally, following Theorem 3.1 in the main paper, we have added a concise sketch of this derivation.\n\n## Weakness 2\n\nThis is a very valid concern that we thank you for raising. We have therefore launched additional hyperparameter sweeps, and pictures pertaining to more learning rates have been added in the latest revision.\n\nWhile excluding the spike regime might seem questionable, we believe it's justifiable in this case. Without explicit regularization and stochastic batching, the spike magnitudes are significant, often reverting the loss to values seen at the beginning of training. This can hardly go unrecognized by any researcher or engineer. We believe studying the spike regime itself is an important direction for research, but Adam in this regime is too unstable to be reasonably approximated by smooth ODE solutions.\n\nTo answer your question about the meaning of \"sufficiently close,\" the threshold for entering the spike regime varies with the task and, crucially, the learning rate. A sufficiently small learning rate prevents spikes altogether. Of course, we do not recommend making $ \\beta $ and $ \\rho $ equal. For instance, with Resnet-50 on CIFAR-10, using full-batch Adam without explicit regularization and a learning rate of $10^{-4}$, $(\\beta, \\rho) = (0.9, 0.999)$ enters the spike regime, whereas $ (0.94, 0.999) $ does not. We opt for higher learning rates to accelerate training and make the bias term (linear in $h$) more prounounced.\n\nFollowing your suggestion, we have added more discussion of this topic in the \"Numerical experiments\" section.\n\n## Weakness 3\n\nThank you very much for pointing out our overstatement regarding Adam's generalization compared to SGD. We acknowledge that Adam does not universally underperform and have revised such assertions. Additionally, we've included a brief mention in the \"Future directions\" section, noting that Adam often excels in training transformers, as supported by referenced studies.\n\nThe paper does not focus on whether the minima targeted by Adam's implicit bias term are advantageous or detrimental for specific tasks. For instance, Andriushchenko et al. (2023) observed that, in training transformers, sharper minima often correlate with lower test errors. As our bias term is novel, further research is necessary to understand its impact on transformer training dynamics.\n\nReference: Andriushchenko, M., Croce, F., M\u00fcller, M., Hein, M., \\& Flammarion, N. (2023). A modern look at the relationship between sharpness and generalization. Proceedings of the 40th International Conference on Machine Learning (ICML'23), 202, Article 36, 840-902. JMLR.org."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700092229767,
                "cdate": 1700092229767,
                "tmdate": 1700092229767,
                "mdate": 1700092229767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e4L1FCwXhY",
                "forum": "ZA9XUTseA9",
                "replyto": "KQG0TKjCCL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1518/Reviewer_71dm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1518/Reviewer_71dm"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response! I updated my score to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661774350,
                "cdate": 1700661774350,
                "tmdate": 1700661774350,
                "mdate": 1700661774350,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W0VRCH4Ys5",
            "forum": "ZA9XUTseA9",
            "replyto": "ZA9XUTseA9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1518/Reviewer_z9rC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1518/Reviewer_z9rC"
            ],
            "content": {
                "summary": {
                    "value": "The paper is about the implicit bias of Adam.\nIt does so by studying ODEs such as the gradient flow and the properties of their discretizations.\nThe approach is based on backward error analysis (Barret & Dherin 2021), which consists in considering a modified version of the ODE, where the modification is done so that the iterates of gradient descent lie closer to the curve traced by the continuous flow solution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The topic of the paper, implicit bias of first order algorithms, is an active field of research with many recent results. So far, characterizing the implicit bias of Adam and other preconditioned methods has not been easy.\n- The paper, to my understanding, seems to present a novel result corroborated by some empirical evidence in dimension 2."
                },
                "weaknesses": {
                    "value": "- The writing of the paper seems subpar to me, and would benefit from being thoroughly proofread. In some locations it sounded very informal/colloquial, eg \"which is ``eaten'' by the gradient\".\n- The analysis, though interesting, is also handwavy: see questions below."
                },
                "questions": {
                    "value": "In many places, statements are made informally that are to translate into rigorous mathematical terms:\n- how do the authors characterize/show/test if \"$\\epsilon$ is very large compared to all squared gradient components\"? What if it's smaller at the beginning, they becomes larger as the algorithm converges to an interpolating solution?\n- How does penalizing the norm of the gradient lead to flat minima (Sec 4 discussion)? Since the gradient is 0 at optimum, don't $f$ and $f  + ||\\nabla f||^2$ have the same set of minimizers? and doesn't this still hold when the 2 norm is replaced by any other norm?\n- similarly, in the experiment, why is the perturbed 1 norm close to 0 at convergence? It seems the authors are performing early stopping, but that precisely means that implicit regularization is not happening, and that the model overfits.\n- In the numerical illustrations, is it possible to display more than 3 curves/ values of $h$ and $\\beta$? In particular, for limiting values, why isn't the red cross attained if there is implicit bias?\n- The figure are not averaged across multiple runs to account for randomness\n\n- (contribution summary, third bullet point)?  Why do the authors consider that Adam does not have an implicit bias, despite having a bias term in the backward analysis ODE. It seems to me that the meaning of \"implicit regularization\", eg in eq 1.is the same as \"bias term\" mentioned page 2, but then the statement \"Adam has no implicit regularization\" is unclear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765986867,
            "cdate": 1698765986867,
            "tmdate": 1699636079929,
            "mdate": 1699636079929,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q9KlBo6JUw",
                "forum": "ZA9XUTseA9",
                "replyto": "W0VRCH4Ys5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We highly appreciate your feedback. We have carefully considered and responded to each specific point below.\n\n## Weakness 1\n\nThank you for your feedback on the style of our presentation. We have revised the sentence that ended with \"which is `eaten' by the gradient\" to ensure a more formal tone. We will also conduct a thorough proofreading of the entire document to identify instances where the writing is too informal.\n\n## Weakness 2\n\nThank you for your comment on the analysis. We included an \"Informal summary\" section at the beginning of our paper to make it more accessible and prevent readers from being overwhelmed by detailed mathematics. We would like to emphasize that the theorem and its proof are rigorous: the assumptions are precisely defined (as in Theorem 3.1 of the main paper), and the arguments are formal. Note that we do not use a common convention in pure mathematics to allow the value of constant $C$ to change from line to line (the statement and proof of Lemma SA-6.15 may illustrate this). In our latest revision, we have improved the connection between the theorem statement and the informal summary (see Remark 3.2).\n\n## Question 1\n\nYou make a great point. In reality, for the majority of training, the situation tends to align more closely with the first extreme case, which we have now clarified in the revised version of our document. The purpose of outlining these extreme cases is to provide an intuitive understanding of the qualitative behavior. Indeed, at the onset of training, the value of $\\varepsilon$ is likely to be smaller than the components. However, towards the end, particularly near an interpolating solution, $\\varepsilon$ may become comparable to or even exceed these components. It is in this phase that Adam can essentially become gradient descent, regardless of other hyperparameter settings.\n\n## Question 2\n\nYou are absolutely right in noting that the set of stationary points, where the gradient is zero, remains unchanged regardless of whether a norm is added or subtracted. However, this set can be broad, and varying parameter settings can lead different algorithms to converge to distinct points within this set. Figures 2 and 3 in our document illustrate this point, showing how Adam, with varying parameter settings, converges to different global minimizers. While adding a norm does not alter the location of the minima, it does impact the Hessian matrix. The Hessian plays a crucial role in determining the flatness of the landscape; changes to it affect this flatness. For instance, incorporating the 2-norm tends to penalize sharper regions in the optimization landscape, as highlighted in the works of Barrett \\& Dherin (2021) and others.\n\nReference: Barrett, D., and Dherin, B. (2021). \"Implicit Gradient Regularization.\" In International Conference on Learning Representations.\n\n## Question 3\n\nThank you for the great point on the unusual nature of implicit regularization in discrete steps, such as in gradient descent. Contrary to typical cases, the implicit bias we observe here acts more like anti-regularization. You also make a great point about the perturbed 1-norm not appearing to be near zero in the figures. This discrepancy arises because, as noted in our \"Notation\" section, the perturbed norm is not technically a norm: at or near a stationary point, the square root of $ \\varepsilon $ is multiplied by the dimension, which is in the tens of millions. In the final iterations, the gradient components themselves are indeed very small, making the perturbed norm close to its lower bound $ p \\sqrt{\\varepsilon} $, where $ p $ is the dimension. This might have given the impression of early stopping, but in reality, we are nearly fitting the training data exactly towards the end (with a training accuracy of 99.98%). We have made these points clearer in the latest revision. We also agree that one could interpret the model as overfitting due to the lack of both explicit and, as we argue, this particular form of implicit regularization, aligning with our analysis.\n\n(continued below)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091392257,
                "cdate": 1700091392257,
                "tmdate": 1700099035780,
                "mdate": 1700099035780,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "clFuVWxTRU",
                "forum": "ZA9XUTseA9",
                "replyto": "W0VRCH4Ys5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Question 4\n\nThank you for the great question. In the latest revision, we have included more curves in numerical illustrations. In the toy problem section, attempting to widen the gap further at this learning rate causes instability in Adam, leading to oscillatory behavior around the global minima curve, a pattern similar to RMSProp's periodic solution for minimizing $x^2 / 2$ in Ma et al. (2022). Increasing the learning rate in Figure 3 causes lack of convergence. Even substantial explicit regularization doesn't lead Adam to the red cross without disrupting its trajectory. In scenarios without explicit regularization, akin to Barrett \\& Dherin (2021) for GD and Ghosh et al. (2023) for momentum-GD (refer to Figure 1 in both), the red cross is not attained because of the relatively small magnitude of the implicit correction term.\n\nReference: Ghosh, A., Lyu, H., Zhang, X., and Wang, R. (2023). \"Implicit Regularization in Heavy-Ball Momentum Accelerated Stochastic Gradient Descent.\" In The Eleventh International Conference on Learning Representations.\n\n## Question 5\n\nWe haven't rerun the same experiments to smooth the curves, mainly because the sole source of randomness in our setup is the initialization seed. Our preliminary tests with various initialization seeds yielded visually indistinguishable results, suggesting that additional smoothing might not be effective. Moreover, adopting a Monte-Carlo approach for smoothing would significantly increase training time, likely by an order of magnitude. We have, however, added more experiments to demonstrate the variability in outcomes across different training sessions.\n\n## Question 6\n\nThank you very much for pointing out the need for clearer wording here. We've revised the relevant section to convey that the implicit bias we observe typically functions as an \"anti-regularization\" for standard hyperparameters and during most of training. This bias essentially \"penalizes\" the negative norm, rather than the norm itself, as shown in Table 1. This clarification has been included in the latest revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091499818,
                "cdate": 1700091499818,
                "tmdate": 1700693159494,
                "mdate": 1700693159494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cgdlx8h9g9",
            "forum": "ZA9XUTseA9",
            "replyto": "ZA9XUTseA9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1518/Reviewer_Mieg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1518/Reviewer_Mieg"
            ],
            "content": {
                "summary": {
                    "value": "Backward error analysis is used to find ODEs approximating convergence trajectories by optimization algorithms. Previous works have been done on Gradient Descent to show that it has implicit regularization properties since the terms in the ODEs penalize the Euclidean norm of the gradients. This paper studies a similar problem but for adaptive algorithms such as Adam and RMSProp. It shows that Adam and RMSProp also have similar implicit regularization properties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper provides detailed backward error analysis for both Adam and RMSProp. The author is able to show that Adam has bias terms that penalize $1-$ norm, $2-$ norm, or $-1-$ norm depending on the settings of $\\beta_1$ and $\\beta_2$ in Adam.\n\n- The paper's result in the implicit bias might help explain the difference in the generalization ability of Adaptive Algorithms and GD algorithms.\n\n- The numerical experiments confirm the theoretical results.\n\n- The paper is well-written overall."
                },
                "weaknesses": {
                    "value": "- Some of the graphs are a bit confusing since the $x$ and $y$ axes are not labeled carefully. More explanation and discussion on these graphs would be appreciated. \n\n- Some transformer tasks might be helpful to see if we can see consistent behaviors in the $1-norm$ across different domains. If I'm not mistaken, Adam generalizes better than SGD in transformer related tasks which slightly contradicts the first conclusion in the discussion section."
                },
                "questions": {
                    "value": "- Can the authors explain more about Figure 2 and Figure 3? I'm a bit confused about what these graphs are about and how we can see the change of the $1-norm$ from them.\n\n- Are the norms plotted in section 6 the norms in the final iterate of training?\n\n- Is full batch required to observe the same behaviors of the norm and $\\rho, \\beta$ as in section 6? Can we do mini-batches instead?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1518/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808041833,
            "cdate": 1698808041833,
            "tmdate": 1699636079813,
            "mdate": 1699636079813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ltI98YcDXZ",
                "forum": "ZA9XUTseA9",
                "replyto": "Cgdlx8h9g9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1518/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate your response. Below, we have addressed each point in detail.\n\n## Weakness 1 and question 1\n\nThank you very much for pointing out the absence of axis labels in Figures 2 and 3. We have included them in the latest revision. To clarify, the horizontal axis represents $ \\theta_1 $, while the vertical axis corresponds to $ \\theta_2 $. The loss function, optimized by Adam, is defined as $ E(\\theta_1, \\theta_2) = (1.5 - 2\\theta_1\\theta_2)^2/2 $. The blue line in the figures marks the hyperbola of global minima of this loss, where $ \\theta_1\\theta_2 = 3/4 $. The gradient is expressed as $ (4\\theta_1\\theta_2 - 3) \\times (\\theta_2, \\theta_1)^{T} $, which means its 1-norm is simply a factor of $\n 4\\theta_1\\theta_2 - 3 $ times the 1-norm of the two-dimensional vector $ (\\theta_2, \\theta_1)^T $.\n\nOn the level sets of the form $4 \\theta_1\\theta_2 - 3 = c$, \"lower 1-norm of the gradient\" is equivalent to \"lower 1-norm of $(\\theta_2, \\theta_1)^T$\". The red cross in our illustrations marks the global minimum that corresponds to the lowest 1-norm of the parameter. Of course, on the entire hyperbola, the 1-norm of the gradient is zero. This is why we describe this point as a limit point of minimizers on the level sets (we clarify this in the latest revision). Thus, the more Adam drifts towards the red cross, the more it penalizes the 1-norm of the gradient.\n\n## Question 2\n\nIn the \"Numerical experiments\" section, we stopped training at near-perfect train accuracy. This decision was based on the clarity of the images at that stage, rendering further training unnecessary. Consequently, at the final iteration, the norms are close to their lower bound $p \\sqrt{\\varepsilon}$, limiting their informativeness. Therefore, this section primarily features plots of norms and accuracies mid-training. A more complete picture is offered by the loss and norm curves in Figure 6. We intend to include additional examples of these curves in the Appendix.\n\n## Question 3\n\nAt present, our training utilizes full-batch Adam, due to our better understanding of its qualitative bias. Although our theorem introduces loss correction terms applicable to mini-batch Adam, we do not yet fully understand their implications. Gaining a deeper understanding of the qualitative behavior of these mini-batch correction terms represents a promising direction for future research.\n\n## Weakness 2\n\nThank you for highlighting our overly assertive statement regarding the generalization capabilities of adaptive gradient algorithms. A more accurate statement would be: \"In the absence of carefully calibrated regularization, it has been observed that adaptive gradient methods may exhibit poorer generalization compared to non-adaptive optimizers.\" This amended statement aligns with the findings reported by Cohen et al., 2021, who also reference three other studies supporting this observation. We have amended our text accordingly.\n\nWe have launched additional experiments and added more evidence in the latest revision. We also plan to explore the training dynamics of transformers as a future research direction. Efforts are underway to incorporate a language translation task, although time constraints may limit our ability to do so.\n\nReference: Cohen, J. M., Ghorbani, B., Krishnan, S., Agarwal, N., Medapati, S., Badura, M., Suo, D., Cardoze, D., Nado, Z., Dahl, G. E., et al. (2022). \"Adaptive Gradient Methods at the Edge of Stability.\" ArXiv:2207.14484."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091113497,
                "cdate": 1700091113497,
                "tmdate": 1700091113497,
                "mdate": 1700091113497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yRzPjoLcBe",
                "forum": "ZA9XUTseA9",
                "replyto": "ltI98YcDXZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1518/Reviewer_Mieg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1518/Reviewer_Mieg"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the author's responses! I'll keep my score and recommend the paper for acceptance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1518/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700338763212,
                "cdate": 1700338763212,
                "tmdate": 1700338763212,
                "mdate": 1700338763212,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]