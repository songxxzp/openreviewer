[
    {
        "title": "Improved Probabilistic Image-Text Representations"
    },
    {
        "review": {
            "id": "t2LbFiexRg",
            "forum": "ft1mr3WlGM",
            "replyto": "ft1mr3WlGM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3892/Reviewer_36mP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3892/Reviewer_36mP"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an improved probabilistic representation to address issues including the presence of false negatives and sparse annotations in image-text matching. \nSpecifically, authors introduce two optimization techniques to enhance many-to-many matching, including pseudo-positives to prevent the loss saturation and mixed sample data augmentation for probabilistic matching"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- well organized and easy to follow. \n- the paper focuses on two important issues in image text matching: many-to-many matching and sparse annotations."
                },
                "weaknesses": {
                    "value": "- Presented as Table 1 and 2, when ViT-B/32 was chosen as the backbone, the performance of PCME++ is inferior to VSE\\infnite. \nAdditionally, the performances of P2RM and DAA published in the original paper actually won PCME with a large margin, while their performances presented here are lower than PCME. \nThe authors should make necessary explanations on these issues. \nMost importantly, could the proposed PP and MSDA be applied to P2RM and DAA to enhance the many-to-many performance? If not, please make the necessary explanations; if yes, pls make comparisons. \n\n- The datasets used in this submission including CxC and ECCV both come from MSCOCO, and thus the proposed method needs further verification on larger datasets like vision-language pretraining methods (CC3M dataset). \nThe authors are also recommended to make comparisons against several recent works focusing on uncertainty modeling [A] and noisy correspondence[B]. \n\n\t\n[A] MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model, CVPR 2023.\n[B] Deep Evidential Learning with Noisy Correspondence for Cross-modal Retrieval, ACM MM 2022."
                },
                "questions": {
                    "value": "presented in weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3892/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3892/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3892/Reviewer_36mP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3892/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698565262495,
            "cdate": 1698565262495,
            "tmdate": 1700739839197,
            "mdate": 1700739839197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pwxEyKNHet",
                "forum": "ft1mr3WlGM",
                "replyto": "t2LbFiexRg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and constructive comments. We will address all the raised concerns by the reviewer, and will revise our paper before next Tuesday.\n\n**[W1] PCME++ performance**\n\nThanks for the question. First, we would like to emphasize that PCME++ is not inferior to VSE infty in all ranking-based metrics, such as mAP@R, R-Precision, and RSUM. It is because VSE infty is based on triplet loss using the hardest negative mining that is overfitted to recall@1 score as supported by Chun et al. 2022. Moreover, when the backbone size becomes larger, VSE infty shows inferior performances than PCME++ with a large margin in all metrics.\n\nWe presume that it is because when the backbone size is sufficiently complex to be overfitted to the false negatives, deterministic methods easily fall to the overfitted solutions. We remark that the training dataset suffers from severe false negatives (88.2% of caption-to-image positives and 72.1% of image-to-caption positives are labeled as \u201cnegative\u201d [Chun et al 2022]). On the other hand, the probabilistic methods are designed to handle many-to-many relationships, by less penalizing plausible matches (e.g., if an image-text pair is annotated as negative, but if the model predicts they are positive with high probability, the loss value becomes smoothed by its design \u2013 As discussed in Sec 2.3). It means that the effect of the FNs is saturated during training PCME++; hence it is profitable to be trained with large backbones.\n\nSecond, both P2RM and DAA are basically an additional regularization to the existing loss function (P2RM employs less panelized additional triplet loss, and DAA employs CIDER-based extra information), usually triplet loss with the hardest negative mining. Furthermore, when we trained the models, we found that their methods are very sensitive to the hyperparameter selection, as shown in below:\n\n| .                  | mAP  | RSUM  |\n|--------------------|------|-------|\n| P2RM (a=1)         | 8.3  | 158.8 |\n| P2RM (a=0.2)       | 39.0 | 530.2 |\n| DAA (b=1)    | 39.2 | 530.9 |\n| DAA (b=0)    | 40.0 | 536.5 |\n| PCME++               | 40.1 | 537.0 |\n\nPerhaps, if we more carefully tune the hyperparameters for the methods, we would get slightly better performances than the reported numbers. However, when we tried to apply DAA and P2RM, we frequently faced training instability by choosing larger hyperparameters than our choice. Note that we use the official implementation of DAA for the comparison, and P2RM is our own implementation because there is no official code.\n\nFinally, the backbones used for our experiments differ from those of DAA, P2RM, and even PCME and VSE infty. Our implementation is a new and very strong setting based on CLIP backbones, showing the state-of-the-art retrieval performances against the existing visual semantic embeddings (VSEs) as shown in Tab C.1. \n\n**[W1] Applying PP and MSDA to comparison methods**\n\nWe would like to emphasize that applying PP and MSDA to the triplet-based method is non-trivial. This is because the triplet loss is not designed for taking smooth labels. When we construct a triplet, we select an anchor sample (e.g., a visual embedding), its positive sample (e.g., its corresponding textual embedding), and its negative sample (e.g., one of the other textual embeddings). Here, for selecting the negative sample, triplet methods use the hardest negative mining, which uses the closest textual embedding as the negative. We argue that as there are abundant false negatives, it can harm the performances. The hardest negative mining is, therefore, very sensitive to the choice of batch size, as shown by VSE++ (https://github.com/fartashf/vsepp/issues/27).\n\nIf we use a smooth label for a given pair, it is impossible to construct a triplet of <anchor, positive and negative>. For example, assume we set a match annotation of <image_a, caption_b> to 0.6 from 0.0. How can we build triplets using this annotation? Moreover, if we introduce mixed samples and mixed labels, the problem becomes more complex. How can we handle <image_a_mixed_b, caption_a> and <image_b_mixed_a, caption_b> using a triplet relationship? Therefore, it is impossible to apply PP and MSDA for triplet-based methods.\n\nOne of our technical contributions is allowing smooth labels and mixed samples by designing a pairwise loss that is not affected by the other data samples. As shown in Fig 2., each loss computation of PCME++ is independent on the other pairs, while triplet loss or batch-wise contrastive loss is dependent on the relationships of other pairs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3892/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145718705,
                "cdate": 1700145718705,
                "tmdate": 1700145933929,
                "mdate": 1700145933929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UzVgLaTPiN",
                "forum": "ft1mr3WlGM",
                "replyto": "t2LbFiexRg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any follow-up question?"
                    },
                    "comment": {
                        "value": "Dear Reviewer 36mP,\n\nWe sincerely appreciate your efforts and time for the community. As we approach the close of the author-reviewer discussion period in 20 hours, we wonder whether the reviewer is satisfied with our response. It will be pleasurable if the reviewer can give us the reviewer's thoughts on the current revision to give us an extra valuable chance to improve our paper. We summarized our revision in the \"Revision summary\" comment and \"Revision has been uploaded\", where the latter is specially personalized to the reviewer's concerns. \n\nAgain, we thank the reviewer's valuable commitment and their help to strengthen our submission. We will address all the raised concerns by reviewers if there remain any."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3892/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668275766,
                "cdate": 1700668275766,
                "tmdate": 1700668628338,
                "mdate": 1700668628338,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2pRCPt4mGb",
                "forum": "ft1mr3WlGM",
                "replyto": "UzVgLaTPiN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3892/Reviewer_36mP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3892/Reviewer_36mP"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely thanks for your detailed response."
                    },
                    "comment": {
                        "value": "Thanks for your detailed response. \nMy concerns are addressed and I have raised my score.\n\nBest"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3892/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739916131,
                "cdate": 1700739916131,
                "tmdate": 1700739916131,
                "mdate": 1700739916131,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AfLza2zNec",
            "forum": "ft1mr3WlGM",
            "replyto": "ft1mr3WlGM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3892/Reviewer_RNqC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3892/Reviewer_RNqC"
            ],
            "content": {
                "summary": {
                    "value": "- This paper discusses some inherent issues with existing Image-Text Matching (ITM) methods and datasets, and provides analysis from the probabilistic perspective,\n\n - To solve the aforementioned issues, the authors propose Improved Probabilistic Image-Text Representations (PCME++) by introducing a closed-form probability distance to reduce the sampling costs of PCME and adopting two training techniques including pseudo-positive matches and mixed sample data augmentation. \n\n - Extensive experiments across abundant VL datasets show the effectiveness of PCME++. Moreover, primitive results on extending PCME++ to other scenarios are shown."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Serving as its major motivation, this paper analyzes the many-to-many nature and other inherent issues in the Image-Text Matching task. Focusing on this fundamental Vision Language (VL) downstream task, this work addresses shortcomings of previous uncertainty-based methods such as PCME. \n\n - This presentation of this work is clear and easy to follow. In particular, the figures, such as Figure 2, are informative in illustrating important contexts.\n\n - The authors provide solid experimental results to support the effectiveness of PCME++. In addition to the evaluations on MS-COCO, CxC, and ECCV Caption with sota ITM methods, extensions to noisy correspondence and uncertainty-based prompt-tuning are conducted, which further demonstrate the advantage of PCME++."
                },
                "weaknesses": {
                    "value": "- About the advantage of PCME++, the authors mentioned the advantage of PCME++ when scaling-up backbones, which might be lacking further discussions. (See Questions)"
                },
                "questions": {
                    "value": "- In Table 1, experimental results on different backbones are provided. While methods like VSE and InfoNCE show performance degradation, PCME and PCME++ show consistent improvements. Does this phenomenon relate to specific advantages of probabilistic modeling? (and How)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3892/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698643039953,
            "cdate": 1698643039953,
            "tmdate": 1699636347867,
            "mdate": 1699636347867,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iZiETr6DKN",
                "forum": "ft1mr3WlGM",
                "replyto": "AfLza2zNec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive evaluation and valuable comments. We will address all the raised concerns by the reviewer, and will revise our paper before next Tuesday.\n\n**[W1, Q1] Lack of discussions of the backbone scale-up**\n\nThanks for your comment. The discussion related to the question is in Sec 3.2. Main results. \u201cWe conjecture that it is because the non-probabilistic counterparts are easily overfitted toward FNs when the model has more complexity, particularly with the hardest negative mining strategy.\u201d Here, we provide more details beyond this explanation.\n\nAs shown in Fig 1, the training dataset already suffers from severe false negatives (88.2% of caption-to-image positives and 72.1% of image-to-caption positives are labeled as \u201cnegative\u201d [Chun et al 2022]). As the backbone becomes more complex and larger, we presume that the backbone complexity is sufficiently large to capture the noisy FNs in the dataset. Moreover, VSE infty uses a triplet loss with the hardest negative mining, which will make the effect of FNs more significant. On the other hand, the probabilistic methods are designed to handle many-to-many relationships by less penalizing plausible matches (e.g., if an image-text pair is annotated as negative, but if the model predicts they are positive with high probability, the loss value becomes smoothed by its design \u2013 As discussed in Sec 2.3). It means that the effect of the FNs is saturated during training PCME and PCME++. However, as we discussed in Sec 2.3., at the same time, it can cause a fast loss saturation for PCME. We address the problem by proposing two techniques: pseudo-positives and mixed data sample augmentation. Note that it is non-trivial to employ the techniques to triplet-based metric learning methods (VSE infty) and batch-wise contrastive learning methods (InfoNCE)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3892/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145378528,
                "cdate": 1700145378528,
                "tmdate": 1700145378528,
                "mdate": 1700145378528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OqST2bI447",
                "forum": "ft1mr3WlGM",
                "replyto": "AfLza2zNec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any follow-up questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer RNqC,\n\nWe deeply thank you for your dedication to the community. As we mentioned before, the positive feedback from the reviewer highly encouraged us. We again sincerely appreciate your valuable comments. As the author-reviewer discussion period is closed in 20 hours, we wonder whether the revised paper is still satisfactory to the reviewer. It will be pleasurable if the reviewer can give us the reviewer's thoughts on the current revision to give us an extra valuable chance to improve our paper. We summarized our revision in the \"Revision summary\" comment and \"Revision has been uploaded\", where the latter is specially personalized to the reviewer's concerns.\n\nAgain, we thank the reviewer's valuable commitment and their help to strengthen our submission. We will address all the raised concerns by reviewers if there remain any."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3892/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668596708,
                "cdate": 1700668596708,
                "tmdate": 1700668639933,
                "mdate": 1700668639933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SoKXLUGm4r",
            "forum": "ft1mr3WlGM",
            "replyto": "ft1mr3WlGM",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3892/Reviewer_XAP9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3892/Reviewer_XAP9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an improved Probabilistic Cross-Modal Embeddings (named PCME++) by introducing a new probabilistic distance, incorporates of pseudo-positives to prevent the loss saturation problem, and introduce the data augmentation technique. Experimental results demonstrate the effectiveness of the proposed model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The representation is good and easy to follow.\n\n2. The robustness of PCME++ is evaluated under noisy image-text correspondences.\n\n3. The most of the figures in this paper is informative, especially Figure1 and Figure5."
                },
                "weaknesses": {
                    "value": "1. This work is deeply coupled with PCME( Chun et al. (2021)), which might limiting the inspiration and extensibility for other work.\n\n2. As shown in Figure 5, the visualization could show the uncertainty of learned embeddings of visual features and textual features. However, the proposed closed-form sampled distance (CSD) could only simply measure the uncertainty but not the area of the overlap between two modality. I hope the authors could make more analysis.\n\n3. Some techniques such as Mixup (Zhang et al., 2018) or CutMix (Yun et al., 2019) are proposed in the previous works, and the loss functions are shared with PCME( Chun et al. (2021)). Please clarify your contribution and improvements.\n\nIf my concerns are solved, I would like to raise my score."
                },
                "questions": {
                    "value": "1. Figure 2 is not easy-understood. Please make interpretations about it and formulate the comparison losses.\n\n2. The motivation of this paper is \"PCME suffers from expensive computations due to Monte Carlo approximation and fast loss saturation\". However, the comparison or analysis about computation costs is not presented in the experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3892/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3892/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3892/Reviewer_XAP9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3892/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747780395,
            "cdate": 1698747780395,
            "tmdate": 1700473084764,
            "mdate": 1700473084764,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ehhpa64BQN",
                "forum": "ft1mr3WlGM",
                "replyto": "SoKXLUGm4r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3892/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and constructive comments. We will address all the raised concerns by the reviewer and will revise our paper before next Tuesday.\n\n**[W1] Inspiration and extensibility for other work**\n\nWe would like to emphasize that our work is an independent image-text matching work, not depending on specific methods. PCME++ is not strongly coupled with PCME. We will discuss it in more detail in the next bullet (**Similarity between PCME loss function and PCME++ loss function**). We presume that the reason why the reviewer feels that PCME++ limits the inspiration and extensibility for other work is that other approaches are usually based on deterministic methods. The technical details for learning a probabilistic method are somewhat difficult to be applied to deterministic methods. However, our work brings attention to the importance of the false negatives in the Image-Text matching training dataset (as pointed out by Reviewer RNqC). We explain that the FNs can be problematic for deterministic methods, especially for larger backbones, such as ViT-L/14. We do not believe that a probabilistic approach is the only method to tackle FNs. We believe that our work can give inspiration to other ITM methods to tackle the FN problem raised in our work.\n\n**[W1, W3] Similarity between PCME loss function and PCME++ loss function**\n\nWe would like to emphasize that the similarity between PCME and PCME++ is due to the nature of the probabilistic embeddings (probemb). The main goal of probemb is to map an input to a random variable (r.v.) rather than a deterministic vector. We would like to emphasize that the core ideas of other probembs are also similar to each other, for example, Oh et al. 2019, Sun et al., 2020, Shi and Jain, 2019, Chang et al., 2020, SilnovaPark et al., 2022a et al., 2020 and Neculai et al., 2022. PCME (Chun et al. 2021) and our method, therefore share the property of probemb: we train an encoder that maps an input to a mean vector and a variance vector and train the encoder by maximizing the negative loglikelihood (NLL) using the extracted distributions.\n\nPCME optimizes NLL by defining \u201cmatching probability\u201d: \n$p(m | x_v, x_t) = \\mathbb E_{Z_v, Z_t} \\text{sigmoid} (-a \\|\\| Z_v - Z_t \\|\\| + b) \\approx \\frac{1}{J^2} \\sum_j^J \\sum_{j^\\prime}^J \\text{sigmoid} ( - a \\|\\| z_v - z_t \\|\\| + b)$\n\nOn the other hand, PCME++ optimizes the NLL using binary cross-entropy loss:\n$p(m | x_v, x_t) = \\text{sigmoid} (-a \\mathbb E_{Z_v, Z_t} \\|\\|Z_v - Z_t\\|\\|^2 + b)$\n\nAlthough they look somewhat similar, PCME takes the expectation over the sigmoid of the pairwise distance of r.v.s, while PCME++ takes the expectation over the pairwise distance of r.v.s before taking the sigmoid. The latter has two benefits over the former: (1) our form has a closed-form solution, as shown in Eq (1), while PCME cannot. (2) our form can be naturally adopted into the binary cross entropy loss function, which is known to be stable and perform well in large-scale training [1].\n\nTherefore, we argue that PCME and PCME++ use different loss functions in terms of the NLL formulation and the probabilistic distance (PCME uses the Monte-Carlo approximation of \u201cmatching probability\u201d, and PCME++ uses the closed-form sampled distance, CSD).\n\n[1] ResNet strikes back: An improved training procedure in timm\n\n**[W3] Augmentation methods are already proposed in previous works**\n\nAlthough we do not propose a new MSDA, our contribution lies in applying MSDA to the relational datasets. For example, applying MSDA to classification is straightforward because the mixed sample does not affect the other samples in the mini-batch. However, in the relational training objectives, such as triplet loss or contrastive loss, a mixed sample affects the other samples in the batch as well. Especially, the triplet loss is impossible to handle MSDA, because the core concept of MSDA is the smooth label but the triplet loss cannot handle smooth label, because it has to construct a triplet of the selected sample, the positive sample, and the negative sample. It is non-trivial to define positive and negative samples when the label is smoothed (Fig 2a). Similarly, a batch-wise contrastive loss, such as InfoNCE, is also a little bit tricky to control the effect of smooth labels (Fig 2b) because the mixed samples are combined in the denominator term of the InfoNCE softmax. On the other hand, our pairwise contrastive loss can directly apply smooth labels because each loss computation is invariant to the other samples, but only the given pairs affect the loss computation (Fig 2d).\n\nOverall, we do not argue that we newly propose MSDA (CutMix and Mixup), but we argue that we first apply MSDA to image-text matching methods that are based on relational datasets, not instance-level datasets (e.g., image-only datasets)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3892/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700145049006,
                "cdate": 1700145049006,
                "tmdate": 1700145049006,
                "mdate": 1700145049006,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ea9hKVle2p",
                "forum": "ft1mr3WlGM",
                "replyto": "dhtrsB77xs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3892/Reviewer_XAP9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3892/Reviewer_XAP9"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the author's response."
                    },
                    "comment": {
                        "value": "Thanks for your reply. Some of my concerns have been solved. The discussion in the response should be added into the main paper (if accepted), such as the loss function, computation cost comparison, and the novelty. I'd like to raise my score to weak accept."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3892/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700472928952,
                "cdate": 1700472928952,
                "tmdate": 1700472928952,
                "mdate": 1700472928952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]