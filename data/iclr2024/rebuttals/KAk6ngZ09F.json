[
    {
        "title": "Data Filtering Networks"
    },
    {
        "review": {
            "id": "mYF66WIPs3",
            "forum": "KAk6ngZ09F",
            "replyto": "KAk6ngZ09F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2637/Reviewer_ZvvG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2637/Reviewer_ZvvG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a contrastive loss trained model for data filtering"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The finding is interesting that a model's data filtering ability is not correlated with its image task performance. Actually, which is also discussed by [1].\n2. The finding is interesting and useful that data quality determines the trained model's filtering performance.\n3. The proposed data filtering network achieves impressive results on data filtering.\n\n[1] Yu, Haichao, et al. \"The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering.\" arXiv preprint arXiv:2309.15954 (2023)."
                },
                "weaknesses": {
                    "value": "1. The HQITP-350M dataset is the key to the success of the DFN, so it is disappointing that it cannot be publicly accessible by the research community."
                },
                "questions": {
                    "value": "1. How many runs are conducted to get the quantitative experimental results in the paper (e.g., Table 1-6)? Are the standard deviations sufficiently smaller than the differences between different settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2637/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697935217711,
            "cdate": 1697935217711,
            "tmdate": 1699636203798,
            "mdate": 1699636203798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wPqtGSHsxF",
                "forum": "KAk6ngZ09F",
                "replyto": "mYF66WIPs3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ZvvG"
                    },
                    "comment": {
                        "value": "Thank you for your review. We are happy that you find our work interesting and the results impressive. We hope to address your remaining concerns.\n\nTo address your questions and concerns about HQITP we provide a copy of our general response from above.\nWe provide some clarity about the HQITP-350M dataset and what we omitted from the paper.\n\n* While we cannot release HQITP-350M for legal purposes, we have shown it is possible to build a DFN better than the existing OAI-CLIP models using only publicly available data. While we only did this up to the \u201clarge\u201d DataComp scale in the original submission, we extend this to the XL scale below and compare it to the previous state of the art filtering network (OAI-CLIP-B/32). Our DFN with none of the additional modifications (fine-tuning/open-ai-init/augmentation) from section 4 in the paper outperforms the OAI DFN.\n* Because HQITP-350M dataset would be similar in nature to licensed stock image datasets, so we don\u2019t have access to the exact labeling decision behind each image caption. We also note we will release the dataset induced by our best performing DFN which is sufficient for the community to train strong models.\n* In table 2 (above), We provide an additional ablation below where we control the number of examples from HQITP used to train the DFN and measure its performance on the datacomp benchmark (at the medium scale). We show that one can achieve most of the gains with a fraction of the of the full dataset. We note this experiment was done with an earlier version of HQITP that only contained 135M examples.\n\n\nEach result is a single run, as these experiments are quite expensive at the highest scales. However, we can provide error bars for the results on the validation tasks - below in table 5 are 95% Clopper-Pearson confidence intervals for the results of our best model."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259134685,
                "cdate": 1700259134685,
                "tmdate": 1700260951519,
                "mdate": 1700260951519,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HVNMVTLTzG",
                "forum": "KAk6ngZ09F",
                "replyto": "kXc52fFOyp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_ZvvG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_ZvvG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed comments and additional results."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631398431,
                "cdate": 1700631398431,
                "tmdate": 1700631398431,
                "mdate": 1700631398431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "agUwY6QfNL",
            "forum": "KAk6ngZ09F",
            "replyto": "KAk6ngZ09F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
            ],
            "content": {
                "summary": {
                    "value": "This paper's focus is on dataset filtering for contrastive language-image pre-training (CLIP). The discussion on data-filtering networks (DFN) is motivated by the use of pretrained CLIP models (referred to as the DFN) to filter datasets by discarding image-caption pairs whose embeddings (as produced by the DFN) are not similar to each other. The paper's main observation is that accuracy of the DFN on a given task doesn't predict how good a subset it selects (even for that task). The paper then uses this to motivate the creation of new DFNs that are trained on high quality image-caption data and then fine-tuned on \"important\" datasets. The paper confirms the effectiveness of the approach by demonstrating competitive performance on ImageNet Zero-Shot, ImageNet Distribution Shift, VTAB, Retrieval etc when filtering DataComp's medium, large and xlarge data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The problem of better understanding and improving DFNs is extremely interesting. \n\n2. The observation that a DFN with good accuracy on a given task doesn't necessarily induce a good dataset on that task is very interesting.\n\n3. Several large-scale experiments are conducted to explore the effectiveness of the approach."
                },
                "weaknesses": {
                    "value": "It seems like the improvement in performance over other DFNs is primary in ImageNet (and dist. shift) Zero-Shot as well as Retrieval. It seems like without \"finetuning\" on MS COCO training set, Flickr30k training set, and ImageNet 1K training set with ImageNet templates as captions, these performance improvements nearly disappear. While the main paper does ablate over fine-tuning, it doesn't highlight just how significant fine-tuning on these particular datasets is (primarily because these are the datasets we wish to evaluate on). \n\nPersonally, I don't find the conclusion that training a DFN on high-quality image-caption datasets allows the induced subsets to be higher quality very surprising or interesting. I do think a fine-grained study into specific properties of the pretraining data (e.g. similarity with downstream tasks' data, diversity of data, data imbalance) can make this work interesting and help further our understanding of DFNs. In it's current form, apart from the observation on ImageNet accuracy of DFN and that of the model trained on the induced dataset, I don't see what the contribution of this work is towards understanding DFNs. \n\nWhile I recognize, the improvement in accuracy on ImageNet, Retrieval etc., the importance of fine-tuning the DFN on the evaluation datasets leads to me believe that this approach may not generalize to other downstream tasks (as evidenced by the < 1% improvement on VTAB)."
                },
                "questions": {
                    "value": "1) Is there any explanation for why a DFN with higher ImageNet accuracy is not always better at selecting data for ImageNet?\n\n2) Could the authors go into greater depth about the impacts of fine-tuning i.e. is the key component of training a good DFN fine-tuning on the evaluation datasets. If so, how do the authors see these models as generalizing on unseen tasks? \n\n3) Is there a trade-off in accuracy on some datasets? For one of the models, could we see the per dataset accuracy for VTAB using different DFNs and see if there are trade-offs in which datasets improve and which don't?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2637/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2637/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2637/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698458459520,
            "cdate": 1698458459520,
            "tmdate": 1700485321223,
            "mdate": 1700485321223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hQNCXZUoXP",
                "forum": "KAk6ngZ09F",
                "replyto": "agUwY6QfNL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 2tx6"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review. We hope to address your concerns and are happy to answer any additional questions.\n\n\nWhile some may not find these results surprising, we believe this is the first work that trains a CLIP model specifically for the purpose of filtering data for large-scale dataset creation, whereas all prior work relies on using an OpenAI CLIP model to filter the data.\n\nIn the paper we provided analysis for understanding DFNs beyond showing that better ImageNet CLIP models do not result in better filtering models. We demonstrated that data quality is more important for training filtering models than for training standard models. Adding model filtered data from a raw pool is a noisy process that can still include bad training points, and these points negatively affect a model used for filtering much more than it affects standard models. For additional details, see Figure 4 and Section 3.3 in the paper.\n\n\n## Overfitting to ImageNet/COCO/FLICKR\nWe understand that the reviewer is concerned about potential overfitting to ImageNet because our filtering network trains on ImageNet + Coco + Flickr (while our data pool is de-duplicated against these datasets). To address this we first provide a full results table to provide an in depth analysis of results and provide ablations at a higher compute scale that show even less signs of overfitting to ImageNet/COCO/Flickr. We also add that while fine-tuning does help performance on a set of key benchmarks, our non-finetuned DFN consistently outperforms the best and most widely used DFN today (the OAI-CLIP-ViT-B//32).\nWe apologize for only providing averages of our evaluation which could cloud some of the takeaways. We provide a full results table (Table 3 and 4 above) for our best performing and comparable external models on the full evaluation suite. We will update our final manuscript to have links to these full results tables. We would like to highlight three points from this full results analysis\n1. Performance on several of the VTAB tasks (Cifar-10, Caltech-101, STL-10)  are close to saturated so making large improvements across all the tasks can be difficult, and even a 0.5% average improvement can be meaningful. We do agree that performance on a few high quality benchmarks may be more indicative of performance improvements.\n2. Our benchmark contains 2 additional high quality test sets specifically for highlighting biases in traditional imagenet style models DollarStreet and GeODE, this is accounted for in the average calculation but not VTAB calculation.\n3. We note the retrieval benchmark average is an average of 3 tasks (Flickr, Coco and WinoGAVIL). WinoGAVIL was a retrieval benchmark released in 2022 containing images from google images and de-duplicated from our training set.\n\nWith the in-depth results table in mind, we provide 3 additional models that provide evidence that overfitting to ImageNet/CoCO/Flickr is not a significant concern.\n1. First we scaled up our ViT-H/14 method further with 1 epoch of high resolution fine-tuning and compare our results across all categories to a state of the art CLIP model that performs inference at a higher resolution: SiGLIP (particularly the ViT-SO400M-14-SigLIP variant). We show that not only does our approach out-perform SiGLIP on ImageNet (84.4 vs 83.1) and average across the 38 benchmarks (71.0 vs 69.4), our performance gap on VTAB is greater than that on ImageNet or Retrieval (68.5 vs 64.6). We show strong performance on DollarStreet and GeODE, slightly under-performing SiGLIP on the former, and over-performing on the latter. We additionally note that this is all with the SigLIP model using a more efficient architecture [2] and being trained using the same compute budget.\n2. We agree our ViT-B/16 results seem to indicate we are \u201cweakening\u201d the model somehow by training the DFN on ImageNet, we show that at larger scale this phenomena reverses. We provide results of a ViT-L at XL datacomp-scale induced by the DFN without the fine-tuning on ImageNet/COCO/FLICKR. We see that the *average* performance of the model is higher for the finetuned-DFN model compared to the \u201cbase\u201d one (that does not finetune on IN). In addition we see improvements in both WinoGAVIL (+3pp) , DollarStreet(+2pp) and a slight decrease on GeoDE (-0.7pp). \n3. Finally we take our XL ViT-L model and continue training it for 3x the number of iterations (39B examples seen) and we see that while we get a slight performance win on ImageNet (+0.7pp), we get a larger gain on other aspects of our benchmark such as  +1.2pp on average, +0.9pp on VTAB, +1.5pp on Geode and a slight performance degradation on DollarStreet (-0.5pp)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259031643,
                "cdate": 1700259031643,
                "tmdate": 1700259031643,
                "mdate": 1700259031643,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wZTs4Xf2Dn",
                "forum": "KAk6ngZ09F",
                "replyto": "hQNCXZUoXP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for responding to my comments with specific experiments. \n\nAs mentioned in my original review, I find the problem of studying DFNs an extremely valuable direction of research and am glad to see the authors for shedding light on some empirical benefits of this. From my understanding of the review, the authors have clarified that the evaluation data is de-duplicated from the training data of the DFN. This was a major concern for me originally and I'm glad to see that this is not the case. \n\nI find that the conclusions of the paper i.e. \"better data -> better DFN\" to be an oversimplification of the underlying phenomena. From seeing the trends on (fine-tuning with ImageNet etc. and not) and the effects on the performance of different datasets, it seemed to be that if the DFN is trained / fine-tuned on data that is similar to the evaluation data, it would select better data for that task. While these numbers seem to change with the larger-scale experiments run by the authors, I'm not confident that \"better data -> better DFN\" accurately describes what is happening in these experiments. \n\nWith that, I stand by my original review and I encourage the authors to dive a little deeper into the relations between what kind of data the DFN is trained on and how that affects performance on specific downstream tasks."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700359279616,
                "cdate": 1700359279616,
                "tmdate": 1700359279616,
                "mdate": 1700359279616,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zCV1uQx6XX",
                "forum": "KAk6ngZ09F",
                "replyto": "agUwY6QfNL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2tx6\u2019s response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their quick response. It would be helpful for us to understand the reviewer\u2019s concerns better. For instance, the reviewer writes\n\n> I'm not confident that \"better data -> better DFN\" accurately describes what is happening in these experiments.\n\nFigure 4 shows how adding even a small amount of noisy data into the DFN training set leads to worse induced models - hence \u201cbetter data -> better DFN\u201d. What aspect of this experiment does the reviewer not agree with?\n\nFinally, we kindly ask that the reviewer re-consider their score as there now seems to be agreement on multiple positive aspects of our submission:\n* The reviewer calls DFNs \u201can extremely valuable direction of research\u201d\n* The reviewer agrees that we have addressed the concern regarding overfitting\n* Our experimental results are the best published CLIP-style models in the literature.\n\nWhat remains is that the reviewer encourages us \u201cto dive a little deeper into the relations between what kind of data the DFN is trained on and how that affects performance on specific downstream tasks.\u201d We agree that this is an interesting direction, and plan to investigate it in future work. But in one paper it is unfortunately not possible to follow all potential follow-up questions to their end. So taken together, we wonder if the overall assessment / rating is still \u201c3: reject, not good enough\u201d"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378005248,
                "cdate": 1700378005248,
                "tmdate": 1700378276574,
                "mdate": 1700378276574,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yls6qgDqoz",
                "forum": "KAk6ngZ09F",
                "replyto": "zCV1uQx6XX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_2tx6"
                ],
                "content": {
                    "title": {
                        "value": "Increased score from 3 -> 5"
                    },
                    "comment": {
                        "value": "I appreciate the authors' points and have increased the score from 3 -> 5 in light of the further experiments on the fine-tuning on ImageNet etc. \n\nThe reason I still do not recommend this paper for acceptance is that I don't feel in the current form the findings are significant / surprising / nuanced enough to be a completed paper (despite the problem statement being quite interesting). I understand this is a potentially subjective opinion, thus I've increased the score to allow the AC to make a better decision."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485687014,
                "cdate": 1700485687014,
                "tmdate": 1700485687014,
                "mdate": 1700485687014,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SrL5UMcjWl",
            "forum": "KAk6ngZ09F",
            "replyto": "KAk6ngZ09F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2637/Reviewer_Q5jf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2637/Reviewer_Q5jf"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of how to filter large-scale web-scrawled data into the training data for training CLIP models. The paper proposes a data filtering network (DFN) for this task. \n* Training DFN: DFN is a small CLIP model pretrained on a small but \u201chigh-quality\u201d dataset based on the authors\u2019 insight: (1) the performance of DFN itself is not correlated to the induced model trained on DFN\u2019s filtered data (Figure 3) and (2) Induced model\u2019s performance degrades as the quality of data drops (Figure 4).\n* Using DFN for filtering (inference): DFN filters out image-text pairs whose cosine similarities are lower than a threshold (Section 2).\n\nBased on the insight above, the paper curates the High-Quality Image-Text Pairs (HQITP-350M) dataset containing 357 million image-text samples with human-verified captions to train DFNs. In the experiments, the paper shows that CLIP models trained on DFN-induced datasets (1) achieve state-of-the-art performance, (2) improve model efficiency (i.e., training a smaller model that achieves similar performance with previous larger models), and (3) improve VQA performance. The paper will release the DFN-2B dataset filtered by DFN to facilitate future research."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper studies an important problem of filtering training data for CLIP.\n2. The paper provides insight into the problem\u2014high-quality pretraining data for DFN matters.\n3. The proposed method achieves state-of-the-art performance with better model efficiency.\n4. The paper will release the DFN-2B dataset, which is a good contribution to the research community.\n5. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "### Major Concern\n\n**[Details of Curating HQITP-350M]** The paper does not provide many details about how to create the high-quality HQITP-350M dataset for training DFNs. This is really important because it is the key for training a good DFN as the paper claims. For example, how does the human verification process work? For example, do human annotators make a binary decision of whether image and text are matched or give a score on the scale of 0 to 1? If it is the latter one, how to threshold the scores? How many annotators are requested? What is the source dataset for curating the HQITP-350M dataset? How many image-text pairs are filtered out by humans?\n\n### Minor Concern\n\n**[Definition of Dataset Quality]** The paper defines the \u201cquality\u201d of CLIP\u2019s training data as the image-text similarity. I wonder if the authors consider other aspects to define the data quality, such as the quality of images and the quality of captions [1].\n\n### Minor suggestions\n\nFor evaluating the robustness against distributional shifts, the paper can also discuss or include more evaluation on newer OOD ImageNet benchmarks: ImageNet-D, ImageNet-X, and ImageNet-W.\n\nMissing citations to OOD ImageNet datasets used in the paper: ImageNet-Sketch [5],  ImageNet-A [6], ImageNet-V2 [7], ObjectNet [8], ImageNet-R [9] (caption in Figure 5).\n\n### References\n\n[1] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt, \u201cImproving Multimodal Datasets with Image Captioning,\u201d in NeurIPS Dataset and Benchmark Track, 2023.\n\n[2] Evgenia Rusak, Steffen Schneider, George Pachitariu, Luisa Eck, Peter Vincent Gehler, Oliver Bringmann, Wieland Brendel, and Matthias Bethge, \u201cIf your data distribution shifts, use self-learning,\u201d TMLR, 2022.\n\n[3] Badr Youbi Idrissi, Diane Bouchacourt, Randall Balestriero, Ivan Evtimov, Caner Hazirbas, Nicolas Ballas, Pascal Vincent, Michal Drozdzal, et al., \u201cImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations,\u201d in ICLR, 2023.\n\n[4] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim, \u201cA Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others,\u201d in CVPR, 2023.\n\n[5] Haohan Wang, Songwei Ge, Eric P. Xing, and Zachary C. Lipton, \u201cLearning Robust Global Representations by Penalizing Local Predictive Power,\u201d in NeurIPS, 2019.\n\n[6] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song, \u201cNatural Adversarial Examples,\u201d in CVPR, 2021.\n\n[7] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar, \u201cDo ImageNet Classifiers Generalize to ImageNet?,\u201d in ICML, 2019.\n\n[8] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz, \u201cObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models,\u201d in NeurIPS, 2019.\n\n[9] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, et al., \u201cThe Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization,\u201d in ICCV, 2021."
                },
                "questions": {
                    "value": "In the rebuttal, I expect the authors to address my concerns regarding the following:\n1. Details of collecting the HQITP-350M dataset.\n2. More discussion on dataset quality"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2637/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731112733,
            "cdate": 1698731112733,
            "tmdate": 1699636203613,
            "mdate": 1699636203613,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TGpfOGryyB",
                "forum": "KAk6ngZ09F",
                "replyto": "SrL5UMcjWl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Q5jf"
                    },
                    "comment": {
                        "value": "Thank you for your kind review. We hope to address your remaining questions.\n\nTo address your questions and concerns about HQITP we provide a copy of our general response from above:\n\nWe provide some clarity about the HQITP-350M dataset and what we omitted from the paper.\n1. While we cannot release HQITP-350M for legal purposes, we have shown it is possible to build a DFN better than the existing OAI-CLIP models using only publicly available data. While we only did this up to the \u201clarge\u201d DataComp scale in the original submission, we extend this to the XL scale below and compare it to the previous state of the art filtering network (OAI-CLIP-B/32). Our DFN with none of the additional modifications (fine-tuning/open-ai-init/augmentation) from section 4 in the paper outperforms the OAI DFN.\n2. Because  HQITP-350M dataset would be similar in nature to licensed stock image datasets, so we don\u2019t have access to the exact labeling decision behind each image caption. We also note we will release the dataset induced by our best performing DFN which is sufficient for the community to train strong models.\n3. In table 2 (above), We provide an additional ablation below where we control the number of examples from HQITP used to train the DFN and measure its performance on the datacomp benchmark (at the medium scale). We show that one can achieve most of the gains with a fraction of the of the full dataset. We note this experiment was done with an earlier version of HQITP that only contained 135M examples.\n\n\nWe use image-text similarity as a proxy for quality, but it is flawed and is not truly measuring quality. We attempted to do something similar for images with a binary filter, as well as image and text separately with a M3AE model, but found that empirically CLIP similarity score works best. See Table 1 and Section 3.3 for additional details. Nguyen et al., changes the captions, and can be combined with the methods presented in our work; however, we did not do so because generating new captions can be quite expensive, especially when compared to calculating CLIP scores.\n\nThank you for your additional suggestions - we have added the citations and will evaluate the additional shifts in a future revision."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258798277,
                "cdate": 1700258798277,
                "tmdate": 1700258798277,
                "mdate": 1700258798277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DKwOILSfzl",
                "forum": "KAk6ngZ09F",
                "replyto": "TGpfOGryyB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_Q5jf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_Q5jf"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response. I have read other reviewers' comments. I keep my rating as \"8: accept, good paper.\""
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629548908,
                "cdate": 1700629548908,
                "tmdate": 1700629548908,
                "mdate": 1700629548908,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4bwpBfnKTi",
            "forum": "KAk6ngZ09F",
            "replyto": "KAk6ngZ09F",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2637/Reviewer_NYkb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2637/Reviewer_NYkb"
            ],
            "content": {
                "summary": {
                    "value": "The paper compares the performance of (primarily) various configurations of CLIP models to filter image-text datasets for subsequently pretraining vision transformers. The authors make interesting empirical observations regarding the same, and are able to achieve an impressive accuracy vs. compute tradeoff on image-net using their best configuration of data filtering networks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- SoTA compute vs. accuracy trade-off on image-net.\n- Thorough & insightful analyses / recipes to understand the effect of different kinds of confounders while training data-filtering networks on downstream performance.\n- Expected public release of the best-performing filtered datasets."
                },
                "weaknesses": {
                    "value": "- The proposed approach is still a heuristic for filtering data, and is limited to pointwise filtering for the sake of parallelizability.\n- The cost of filtering: (1) training the data filtering networks; and (2) linear inference over the entire dataset; might be too much to be amortized or ignored in the final compute vs. accuracy trade-off (more in questions)."
                },
                "questions": {
                    "value": "- Can you highlight the cost of filtering in e.g. Figure 1?\n- (Not relevant to the final rating) What do you think about applying these data filtering networks to other domains, e.g., text, or speech, or music?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2637/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2637/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2637/Reviewer_NYkb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2637/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698917920080,
            "cdate": 1698917920080,
            "tmdate": 1699636203526,
            "mdate": 1699636203526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TfAMUatJcM",
                "forum": "KAk6ngZ09F",
                "replyto": "4bwpBfnKTi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer NYkb"
                    },
                    "comment": {
                        "value": "Thank you for your kind review. We hope to address your remaining questions.\n\nWe agree that pointwise filtering is limited, but is computationally important due to parallelizability; however, this approach can be combined with other dataset filtering approaches, such as restricting keyword counts in the text (e.g. MetaCLIP).\n\nWhile the cost of filtering is not reflected in Figure 1, DFN, DC-1B, and LAION all rely on CLIP filtering to filter the dataset. DFN and LAION use a ViT B/32 model, which is cheaper (both training and inference) than the ViT-L/14 used in DC-1B. Additionally, the DFN we train for filtering is trained for less samples seen than the OpenAI models used to filter DC-1B and LAION (5B vs 12.8B). Finally, the cost of training a ViT-B/32 (4.4 GMAC at 224px) is much smaller than that of training the best models, such as a ViT-L/14 (81.1 GMAC at 224px) and a ViT-H/14 (167.4 GMAC at 224px). So the total cost of training our best filtering model is 22e9 GMACs, while the total cost of training our ViT-L and ViT-H models are 1e12 GMACs and  6.5e12 GMACS respectively. This corresponds to a filtering overhead of 2% and 0.3% respectively. Unfortunately, we are unable to directly compare with approaches used in proprietary datasets like OAI-WIT-400M and WebLI, as their creation processes are not public knowledge.\n\nWe do plan on applying similar methods to text, as well as additional multimodal datasets, and are excited to further explore this space in dataset creation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258599021,
                "cdate": 1700258599021,
                "tmdate": 1700258599021,
                "mdate": 1700258599021,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lgNtGxIOLM",
                "forum": "KAk6ngZ09F",
                "replyto": "TfAMUatJcM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_NYkb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2637/Reviewer_NYkb"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewers"
                    },
                    "comment": {
                        "value": "Thank you for your response. I would like to stick to my original rating."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2637/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733712199,
                "cdate": 1700733712199,
                "tmdate": 1700733712199,
                "mdate": 1700733712199,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]