[
    {
        "title": "Enhancing Instance-Level Image Classification with Set-Level Labels"
    },
    {
        "review": {
            "id": "6YFWeWAGpw",
            "forum": "AZW3qlCGTe",
            "replyto": "AZW3qlCGTe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8741/Reviewer_xN2u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8741/Reviewer_xN2u"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new setup for few-shot learning. The proposed model is pre-trained on coarse-grained set-level labels first and fine-tuned with fine-grained labels. Authors also provide theoretical analysis on the convergence rate for downstream tasks, which shows coarse-grained pre-training can enhance the learning process of fine-grained label tasks. The experiments are performed on both natural image datasets and medical histopathology datasets, where the baselines are mostly self-supervised learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think the idea of using coarse-grained label is reasonable. The conclusion of enhancing learning process of fine-trained labels is inspiring."
                },
                "weaknesses": {
                    "value": "- I have some questions about the method part, Sec. 2.1. In Fig.2 (a), are input samples all belongs to the same set-level labels? I am confused by this figure and Fig.1(a) CIFAR images. What I believe is correct is that each batch contains samples belong to different set-level labels, and the coarse label is assigned to each sample for pre-training. \n- How is SupCon trained? It is superised that supervised contrastive learning perform a lot worse than basic CE approach in most setups. There is not much information about training details."
                },
                "questions": {
                    "value": "- There should be more training details about the framework in Sec.2.1. \n- Fig. 3 is referenced in text before Fig. 2\n- Most of the refernece hyperlinks other than page 1 is not working."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8741/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8741/Reviewer_xN2u",
                        "ICLR.cc/2024/Conference/Submission8741/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698747959350,
            "cdate": 1698747959350,
            "tmdate": 1700651572344,
            "mdate": 1700651572344,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RLR0N8Epnq",
                "forum": "AZW3qlCGTe",
                "replyto": "6YFWeWAGpw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xN2u"
                    },
                    "comment": {
                        "value": "> Weakness (1): I have some questions about the method part, Sec. 2.1. In Fig.2 (a), are input samples all belongs to the same set-level labels? I am confused by this figure and Fig.1(a) CIFAR images. What I believe is correct is that each batch contains samples belong to different set-level labels, and the coarse label is assigned to each sample for pre-training.\n\n**A:** In the revision, we have added set-level label and fine-grained labels in Figure 2. This highlights that the input to the model is a set of images from CIFAR-100 and the corresponding set-level label is the most frequent superclass, i.e., fruit and vegetables. In Figure 2(a), we only show one input set (not a batch of input sets) due to limited space and the corresponding most frequent superclass of the input set, i.e., 'fruit and vegetables'. Figure 2(b) shows the fine-grained labels of the support set are apple, pear, orange, and mushroom.\n\nTo clarify, during pretraining in our FACILE framework, each batch indeed consists of multiple input sets. Each of these sets is associated with one set-level label. For the FSP (Fully Supervised Pretraining) baseline model used for comparison, we assign the corresponding coarse-grained label to each sample within an input set. \n\n> Weakness (2): How is SupCon trained? It is superised that supervised contrastive learning performs a lot worse than basic CE approach in most setups. There is not much information about training details.\n\n**A:** Please find the training details of SupCon in Appendix A and G.2. When we use set-input data with SupCon, the conventional way of augmenting each instance inside a set could create challenges for SupCon training. It is typically trained to maximize agreement between differently augmented views of the same data point using labeled data. In our application, when using set-input data, the conventional data augmentation methods applied to each instance within a set and the set-input models that extract permutation invariant representations for all instances inside a set may present unique challenges for the SupCon model especially when the model is trained from scratch. However, you can see from Tables 1, 2, 4, 5, 7, and 10 that FACILE-FSP and FACILE-SupCon performance similarly. It's important to note that our framework is flexible and allows for any supervised learning algorithm to be used during the pretraining stage. \n\n> Question (1): There should be more training details about the framework in Sec.2.1.\n\n**A:** We have documented these details to facilitate full reproducibility. The primary experimental setups, including configurations and parameters, are described in the Experiment section of the paper. For more in-depth information, including additional configurations and nuanced methodology, we direct readers to Appendix sections A, D, and G. \n\n> Question (2): Fig. 3 is referenced in text before Fig. 2\n\n**A:** Thanks for pointing this out! We have reorganized the figures in the revised version.  \n\n> Question (3): Most of the reference hyperlinks other than page 1 is not working.\n\n**A:** Thank you for bringing this up. We have fixed the reference issue in the revised version.\n\n---\nWe appreciate your feedback and recognize that the score may indicate some reservations about our work. We're committed to improvement and would greatly value any additional detailed feedback or specific suggestions you can provide!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241820077,
                "cdate": 1700241820077,
                "tmdate": 1700241820077,
                "mdate": 1700241820077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "e4hixu1iQe",
                "forum": "AZW3qlCGTe",
                "replyto": "6YFWeWAGpw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up with Reviewer xN2u"
                    },
                    "comment": {
                        "value": "We thank the reviewer very much for the questions and comments. We would like to kindly follow up with the reviewer, and respectfully ask the reviewer if they can reconsider their score if our response helps address their concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611036212,
                "cdate": 1700611036212,
                "tmdate": 1700611036212,
                "mdate": 1700611036212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lackT0BQZb",
                "forum": "AZW3qlCGTe",
                "replyto": "RLR0N8Epnq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8741/Reviewer_xN2u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8741/Reviewer_xN2u"
                ],
                "content": {
                    "title": {
                        "value": "Response to author"
                    },
                    "comment": {
                        "value": "- Thank you for your clarification on method part. I believe this aligns my understanding of the framework.\n- I think author made a point here. With coarse-labeled samples, SupCon will maximise agreement with different fine-grained samples. This is not the desire behaviour. Author should describe more clearly about \"unique challenges\" as it is vague. \n\nIn addition, author present new results with different backbones, datasets and tasks. This strength author's argument. I will consider increase my scores."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651546921,
                "cdate": 1700651546921,
                "tmdate": 1700651546921,
                "mdate": 1700651546921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wti4tVwN2V",
            "forum": "AZW3qlCGTe",
            "replyto": "AZW3qlCGTe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8741/Reviewer_jPxq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8741/Reviewer_jPxq"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new technique aimed at boosting instance-level image classification by utilizing set-level labels. Compared to conventional methods that rely on single-instance labels, the proposed approach achieves a 13% increase in classification accuracy when tested on histopathology image datasets. A theoretical examination of the method outlines conditions for rapid reduction of excess risk rate, adding credibility and robustness to the technique. This research serves to connect instance-level and set-level image classification, providing a noteworthy direction for enhancing image classification models that use coarse-grained set-level labels."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper presents a new technique for enhancing instance-level image classification by making use of set-level labels. This serves to fill the existing gap between instance-level and set-level image classification.\n\n- The robustness and reliability of the proposed method are underscored by a theoretical analysis, which outlines conditions for the rapid reduction of excess risk rate.\n\n- The paper clearly articulates the proposed method, shedding light on both its theoretical underpinnings and empirical results. These results are demonstrated on both natural and histopathology image datasets.\n\n- The method put forth in the paper holds promise for extending the capabilities of image classification models. By leveraging set-level coarse-grained labels, the approach achieves better classification performance compared to traditional methods reliant on single-instance labels. This is particularly relevant in real-world contexts where set-level labels may offer more comprehensive information."
                },
                "weaknesses": {
                    "value": "- The use of coarse-grained labels like TCGA or NCT is an interesting choice. These are indeed umbrella terms for various subcollections, and traditionally they may not provide a strong learning signal. It could be beneficial to delve into why these particular labels were chosen and what advantages they offer in this context.\n\n- Your team's approach to pretraining with coarse labels and then fine-tuning on a support set is a solid and proven method. However, it would enrich the work to articulate what sets this particular application or implementation apart in terms of novelty.\n\n- The comparison with SimCLR and simSIAM provides useful insights, but considering the advancements in the field, benchmarking against more recent self-supervised learning methods like DINO or DINOv2 might offer a more comprehensive evaluation.\n\n- To further validate the generalizability of the method, it could be insightful to include results against standardized few-shot learning benchmarks, such as Mini-Imagenet 5-way (1-shot) or SST-2 Binary classification.\n\n- Adding ablation studies that feature additional pretrained models\u2014or even models pretrained without the coarse labels\u2014could help underscore the specific benefits of using coarse-grained label-based pretraining in your approach.\n\n- Your methodology would be even more robust if additional training details are shared. Information on image augmentations, learning schedules, and optimizer settings could offer valuable insights and help in the reproducibility of your results."
                },
                "questions": {
                    "value": "- Do you think you could pictorially diagram the approach adding the relevant details? It is unclear to me if the method essentially pretrains using coarse-labels and then fine-tunes on the test set using the support set or is there more to the method\n\n- Why are the methods for pretraining SupCon and FSP chosen for pre-training? Adding rationale for this might help motivate the choice of pretraining method"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787880631,
            "cdate": 1698787880631,
            "tmdate": 1699637097090,
            "mdate": 1699637097090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PZXVDzsgBO",
                "forum": "AZW3qlCGTe",
                "replyto": "Wti4tVwN2V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jPxq (part 1)"
                    },
                    "comment": {
                        "value": "> Weakness (1) The use of coarse-grained labels like TCGA or NCT is an interesting choice. These are indeed umbrella terms for various subcollections, and traditionally they may not provide a strong learning signal. It could be beneficial to delve into why these particular labels were chosen and what advantages they offer in this context.\n\n**A:** Thank you very much for this suggestion. We have included these explanation and advantages in each experiment in the revision.\n\nOur theorem suggested that the relatedness, quantified by the relative Lipschitzness relative to $\\mathcal{E}$, is an important factor for excess risk bound. It is intuitive that tasks in different domains do not share the same embedding space (and thus a large Lipschitz constant). Therefore, we deliberately choose pretraining tasks in source domain that are closely related to the target downstream task. \n\nIn response to the use of coarse-grained labels like those in TCGA, please note that the TCGA consortium has categorized an extensive array of cancer types and subregions, covering a diverse range of tissues.  The strategic use of TCGA's whole slide level labels is rooted in their potential to enrich tissue-level classification. While these labels may appear broad, they encapsulate a wealth of underlying heterogeneity inherent to different cancer regions and tissue types and are routinely used in the cancer research community.\n\n\n> Weakness (2) Your team's approach to pretraining with coarse labels and then fine-tuning on a support set is a solid and proven method. However, it would enrich the work to articulate what sets this particular application or implementation apart in terms of novelty.\n\n**A:** Thank you for the opportunity to clarify the novelty of our work.  We have edited the introduction to highlight the novelty accordingly. We summarized novelty (and contribution) in the last three paragraphs of the introduction of our paper. Our work introduces FACILE, a novel generic learning framework that innovatively uses coarse-grained labels for pretraining, before fine-tuning on fine-grained tasks. This method diverges from standard fine-tuned supervised learning (FSL) and transfer learning (TL) by leveraging coarse labels from the source domain rather than heavily relying on detailed annotations. Our theoretical analysis underpins the empirical benefits of FACILE, providing evidence of how coarse-grained labels expedite the learning process with a significant increase in the convergence rate for fine-grained tasks. Furthermore, our experiments across different data domains---ranging from natural images to complex histopathology images---demonstrate FACILE's superior performance and versatility. Notably, we record a remarkable 13% performance improvement over existing baselines in a challenging histopathology benchmark. These elements collectively distinguish our work's novelty and significance in advancing the state of the art in representation learning (more specifically, in few-shot learning and model fine-tuning).\n\n> Weakness (3): The comparison with SimCLR and simSIAM provides useful insights, but considering the advancements in the field, benchmarking against more recent self-supervised learning methods like DINO or DINOv2 might offer a more comprehensive evaluation.\n\n**A:** Thank you for raising this important point. The primary aim of FACILE is not to engage in self-supervised learning that requires extensive data and computational resources. Instead, our focus is on leveraging coarse-grained labels to improve performance in downstream tasks. In this context, foundation models such as BYOL or DINO V2 serve as robust starting points. We can enhance these models by fine-tuning them with domain-specific data, effectively tailoring their capabilities to the nuances of the target domain.\n\nTo this end, we conducted two additional experiments. First, we sought to enhance model performance with coarse-grained labels of anomaly detection datasets (similar to Deep Set [5] and Set Transformer [6]). A total of 11,788 input sets of size 10 are constructed from the CUB200 training dataset by including one example that lacks an attribute common to the other examples in the input set. This setup creates a challenging scenario for models, as they must identify the outlier among otherwise similar instances. In downstream tasks, we evaluate the finetuned feature encoder composed of the CLIP [3] image encoder ViT-B/16 and appended fully-connected layer on the classification of species of the CUB200 test dataset. The batch normalization and ReLU are applied to the fully-connected layer."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241337943,
                "cdate": 1700241337943,
                "tmdate": 1700241337943,
                "mdate": 1700241337943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mnmPjL8Klb",
                "forum": "AZW3qlCGTe",
                "replyto": "Wti4tVwN2V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up with Reviewer jPxq"
                    },
                    "comment": {
                        "value": "We thank the reviewer very much for the questions and comments. We would like to kindly follow up with the reviewer, and respectfully ask the reviewer if they can reconsider their score if our response helps address their concerns."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610943494,
                "cdate": 1700610943494,
                "tmdate": 1700610943494,
                "mdate": 1700610943494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r2Il4TF34p",
            "forum": "AZW3qlCGTe",
            "replyto": "AZW3qlCGTe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8741/Reviewer_4JEd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8741/Reviewer_4JEd"
            ],
            "content": {
                "summary": {
                    "value": "The paper propose to utilize set-level coarse-grained labels to improve fine-grained image classification. Essentially the paper is proposing a new pretraining method, key to the method is selecting a dataset with coarse label, and use the set prediction on coarse label as pretraining task. The paper provides theoretical analysis for the proposed approach, showing that using coarse-grained labels speed up the learning on the fine-grained classification task. The paper also demonstrates the effectiveness on several datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of using set prediction on coarse label as pretraining task seems novel\n2. The performance seems strong compared to other baselines"
                },
                "weaknesses": {
                    "value": "1. More baselines for strong self-supervised pretraining methods (e.g., BYOL, DION  are needed to demonstrate the effectiveness. As proposed method is essentially a pretraining strategy, that bears a lot of similarity with exisiting self-supervised learning method\n\n2. More ablations and discussion on some key questions are needed (see below)"
                },
                "questions": {
                    "value": "1. To what extent does the similarity between the pretraining dataset and its coarse labels and the target dataset with its fine labels affect the effectiveness of the method? For instance, can the method perform well when the pretraining dataset is CIFAR-100 while the downstream task involves a medical dataset? In such a scenario, which pretraining method is preferable: supervised pretraining on ImageNet, self-supervised pretraining (ignoring labels entirely), or the proposed method?\n\n2.  Given the same 'related' dataset, if you have both the fine-grained label and coarse-grained label, which pretraining strategy is preferable?\n(Let say your downstream task is classification on a medical image dataset, with fine-grained label A,B,C. The pretraining dataset you have is another medical image dataset (thus more related than ImageNet). You have both coarse label D,E,F and fine-grained label G,H,I,J,K,L. In this case, is fully supervised pretraining on G,H,I,J,K,L more beneficial on set level coarse pretraining on D,E,F more beneficial?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8741/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8741/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8741/Reviewer_4JEd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8741/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797402568,
            "cdate": 1698797402568,
            "tmdate": 1699637096958,
            "mdate": 1699637096958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9EkKl7al6Y",
                "forum": "AZW3qlCGTe",
                "replyto": "r2Il4TF34p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4JEd (part 1)"
                    },
                    "comment": {
                        "value": "> *Weakness (1):* More baselines for strong self-supervised pretraining methods (e.g., BYOL, DION are needed to demonstrate the effectiveness. As proposed method is essentially a pretraining strategy, that bears a lot of similarity with exisiting self-supervised learning method\n\n**A:** Thank you for raising this important point. The primary aim of FACILE is not to engage in self-supervised learning or \"foundation\" model training that require extensive data and computational resources. Instead, our focus is on leveraging coarse-grained labels to improve performance in downstream tasks. In this context, foundation models such as BYOL or DINO V2 serve as robust starting points. We can enhance these models by finetuning them with domain-specific data, effectively tailoring their capabilities to the nuances of the target domain. \n\nTo this end, we have added two additonal experiments. \n\nFirst, we sought to enhance the model performance with coarse-grained labels of anomaly detection datasets (similar to Deep Set [5] and Set Transformer [6]). A total of 11,788 input sets of size 10 are constructed from the CUB200 training dataset by including one example that lacks an attribute common to the other examples in the input set. This setup creates a challenging scenario for models, as they must identify the outlier among otherwise similar instances. In downstream tasks, we evaluate the finetuned feature encoder composed of the CLIP [3] image encoder ViT-B/16 and appended fully-connected layer on the classification of species of the CUB200 test dataset. The batch normalization and ReLU are applied to the fully-connected layer. \n\nThe model training approach in this experiment centered around the CLIP image encoder, enhanced with an additional fully-connected layer. FACILE-FSP and FACILE-SupCon incorporate this setup, utilizing the CLIP-based feature encoder and focusing on finetuning the fully-connected layer through the FACILE pretraining step. In contrast, the SimSiam approach leverages the CLIP image encoder as a backbone while finetuning the projector and predictor components. Similarly, the SimCLR method also uses the CLIP encoder as its foundation but focuses solely on finetuning the projector. These varied strategies reflect our efforts to optimize the feature encoder for accurately identifying anomalies and improving classification performance in related tasks. The training details will be added to revised appendix.\n\n|pretrain method|NC              | LR             | RC             |\n|---------------|----------------|----------------|----------------|\n|CLIP (ViT-B/16)|$83.84 \\pm 1.10$|$81.01 \\pm 1.23$|$82.75 \\pm 1.17$|\n|SimCLR         |$84.03 \\pm 1.08$|$83.49 \\pm 1.14$|$86.30 \\pm 1.03$|\n|SimSiam        |$84.02 \\pm 1.10$|$83.90 \\pm 1.13$|$85.68 \\pm 1.07$|\n|FACILE-SupCon  |$87.49 \\pm 0.99$|$86.57 \\pm 1.07$|$88.01 \\pm 0.99$|\n|FACILE-FSP     |$\\mathbf{88.74 \\pm 0.94}$|$\\mathbf{ 88.45 \\pm 0.96 }$|$\\mathbf{ 88.36 \\pm 0.95 }$|\n\nPretraining on input sets from CUB200. Testing with 5-shot 20-way meta-test sets; average F1 and CI are reported.\n\nSecond, similar to first experiment, we finetune a fully-connected layer that is appended after DINO V2 [2] ViT-B/14. This methodology is applied across various models to assess their performance on histopathology image datasets. By adopting the DINO V2 architecture, known for its robustness and effectiveness in visual representation learning, we aim to harness its potential for the specialized domain of histopathology. We refer interested readers to the revised appendix for details of pretraining.\n\n1-shot 5-way test on LC dataset\n|pretraining method | NC | LR | RC | LR+LA | RC+LA |\n|-------------------|----|----|----|-------|-------|\n|DINO V2 (ViT-B/14) |$44.82 \\pm 1.41$|$47.51 \\pm 1.39$|$47.63\\pm 1.38$|$47.36 \\pm 1.39$|$48.88\\pm 1.44$|\n|SimSiam            |$48.79 \\pm 1.37$|$49.43 \\pm 1.35$|$48.43 \\pm 1.36$|$49.38 \\pm 1.34$|$49.50 \\pm 1.34$|\n|SimCLR             |$50.47 \\pm 1.31$|$50.52 \\pm 1.33$|$50.44 \\pm 1.32$|$51.66 \\pm 1.32$|$51.78 \\pm 1.38$|\n|FSP-Patch          |$49.73 \\pm 1.41$|$53.59 \\pm 1.38$|$53.07 \\pm 1.41$|$51.79 \\pm 1.40$|$51.27 \\pm 1.43$|\n|FACILE-SupCon      |$\\mathbf{56.24 \\pm 1.43}$|$\\mathbf{56.51 \\pm 1.41}$|$\\mathbf{55.95 \\pm 1.42}$|$\\mathbf{56.29 \\pm 1.43}$|$54.07 \\pm 1.44$ |\n|FACILE-FSP         |$55.67 \\pm 1.40$|$56.26 \\pm 1.36$|$55.83 \\pm 1.35$|$56.01 \\pm 1.38$|$\\mathbf{55.35 \\pm 1.40}$|"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700240654598,
                "cdate": 1700240654598,
                "tmdate": 1700240654598,
                "mdate": 1700240654598,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eFOD3LCiKu",
            "forum": "AZW3qlCGTe",
            "replyto": "AZW3qlCGTe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8741/Reviewer_jPxq"
            ],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8741/Reviewer_jPxq"
            ],
            "content": {
                "comment": {
                    "value": "Thank you to the authors for their detailed responses and for providing additional experimental results. While I appreciate these efforts, I believe that further clarification on the use of coarse grained labels, particularly in how they influence downstream performance, would significantly enhance the understanding of this method. The current rationale, especially regarding the choice of a coarse grained label like TCGA, seems to lack sufficient detail to fully grasp its potential impact. It reminds me of generalizing all images in the ImageNet dataset under a single, broad category. I am open to and would welcome a more compelling explanation, as it would provide a clearer insight into your approach."
                }
            },
            "number": 14,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8741/-/Official_Comment"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700631906657,
            "cdate": 1700631906657,
            "tmdate": 1700631906657,
            "mdate": 1700631906657,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QzubqYDgwT",
                "forum": "AZW3qlCGTe",
                "replyto": "eFOD3LCiKu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8741/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further response to reviewer jPxq"
                    },
                    "comment": {
                        "value": "> Thank you to the authors for their detailed responses and for providing additional experimental results. While I appreciate these efforts, I believe that further clarification on the use of coarse-grained labels, particularly in how they influence downstream performance, would significantly enhance the understanding of this method. The current rationale, especially regarding the choice of a coarse-grained label like TCGA, seems to lack sufficient detail to fully grasp its potential impact. It reminds me of generalizing all images in the ImageNet dataset under a single, broad category. I am open to and would welcome a more compelling explanation, as it would provide a clearer insight into your approach.\n\n**A:** Thank you for your feedback and for highlighting areas in our manuscript where further clarification is needed. We appreciate your interest in the nuances of our approach, especially concerning the use of coarse-grained labels. \n\nThe idea of generalizing all images in the ImageNet dataset under a single, broad category has been explored in prior lines of research [1][2]. Building upon this, our work extends this framework to a more general setting. In our setting, coarse-grained labels are associated with a set of instances, rather than being limited to individual instances (explained in section 2). This expansion allows for a more nuanced understanding and application of coarse-grained labels in complex learning environments.\n\nThe rationale for selecting coarse-grained labels in the TCGA is detailed in the third paragraph of the introduction and in Section 3.4. As previously discussed, both in this section and in our related work, this approach differs from \"the generalization of all images in the ImageNet dataset into a single, broad category\". First, unlike the methods used in [1] and in Experiment 4.2 of [2], we do not rely on a class taxonomy to differentiate between coarse-grained and fine-grained labels. Second, the inputs provided to the models for predicting these labels vary: we use set input for coarse-grained labels and instance input for fine-grained labels.\n\nIn response to your concerns, we would like to direct your attention to several key sections of our paper that address these issues:\n\n* Rigorous Definitions and Theoretical Analysis (Section 2): We provide detailed definitions of coarse-grained set-level labels, fine-grained labels, and related key conditions. The distinction between coarse-grained set-level labels and fine-grained labels is crucial for understanding the subsequent theoretical analysis. Our theoretical framework explores how learning with coarse-grained set-level labels can enhance downstream task performance. This is not just a speculative statement but is backed by rigorous analysis. The results presented in Figures 4 and 5 are not just empirical findings; they are in alignment with our theoretical predictions, demonstrating the practical effectiveness of our approach.\n* Intuitive Illustrations (Introduction, third Paragraph): To aid in understanding, we provide two illustrative examples that demonstrate why coarse-grained labels can be beneficial. These examples are designed to be intuitive and accessible, providing a clear rationale for our approach.\n* Experiment-Specific Explanations (Empirical Study Section and Appendix Section B): In each experiment, we do not merely apply coarse-grained labels. Instead, we provide specific reasons why certain coarse-grained labels are chosen and how they are expected to contribute to the learning of the particular downstream tasks. \n\nWe believe that these sections and figures collectively offer a comprehensive explanation of our methodology and its advantages. Our aim is to demonstrate not only the theoretical soundness of using coarse-grained labels but also their practical efficacy in enhancing downstream task performance. We trust that this clarification has addressed your concerns and offered a more transparent understanding of our approach. We are eager to answer any further questions you may have. Thank you for your time and effort.\n\nReferences:\n\n[1] Phoo, Cheng Perng, and Bharath Hariharan. \"Coarsely-labeled data for better few-shot transfer.\" In Proceedings of the IEEE/CVF international conference on computer vision, pp. 9052-9061. 2021.\n\n[2] Robinson, Joshua, Stefanie Jegelka, and Suvrit Sra. \"Strength from weakness: Fast learning using weak supervision.\" In International Conference on Machine Learning, pp. 8127-8136. PMLR, 2020."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8741/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665468313,
                "cdate": 1700665468313,
                "tmdate": 1700665468313,
                "mdate": 1700665468313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]