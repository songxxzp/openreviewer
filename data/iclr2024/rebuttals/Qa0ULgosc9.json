[
    {
        "title": "OpenTab: Advancing Large Language Models as Open-domain Table Reasoners"
    },
    {
        "review": {
            "id": "RlGbsaJElo",
            "forum": "Qa0ULgosc9",
            "replyto": "Qa0ULgosc9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4016/Reviewer_VJ1H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4016/Reviewer_VJ1H"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces OpenTab, an open-domain table reasoning framework powered by LLMs. Unlike most LLMs which rely on knowledge stored in their parameters or retrieved from documents, OpenTab is specifically designed to utilize the rich information contained in structured data. The framework consists of several key components: a retriever to collect relevant tables, a row selector to prune only the pertinent rows of records, a Coder to generate SQL queries, and finally, a Reader to verify the answers. Experiments conducted on Open-WikiTable, WikiTableQuestion, and Feverous demonstrate that OpenTab achieves superior performance compared to previous reader and parser-based methods on tables across both open and close domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly written and easy to follow.\n- Figures 1, 3, and 4 effectively illustrate the overall pipeline and modules.\n- The proposed pipeline is novel and generally reasonable for addressing the specific task at hand.\n- OpenTab outperforms the previously proposed table-reasoning methods."
                },
                "weaknesses": {
                    "value": "- The proposed pipeline might be applicable only to open-domain wikipedia-based tables and not to tables with a large number of rows as in Spider or BIRD. For instance, the row selector, although I agree with the motivation, can result in incorrect candidate tables as some questions may require a large number of rows. Determining the appropriate number of rows to retrieve using a heuristic seems to assume that the pipeline is not suited for dealing with complex tables. One of the main benefits of using tables is their ability to store a large number of records (mostly in close-domain scenarios). I would like to hear the authors\u2019 thoughts on this aspect.\n- Minor typos in the manuscript (e.g., by by)"
                },
                "questions": {
                    "value": "- In Figure 3, reader seems to appear after the SQL is selected, but in the explanation, the authors mention that the reader is used to select the SQL. Which is correct?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698828884857,
            "cdate": 1698828884857,
            "tmdate": 1699636363927,
            "mdate": 1699636363927,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rUBaKd7Fq7",
                "forum": "Qa0ULgosc9",
                "replyto": "RlGbsaJElo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VJ1H"
                    },
                    "comment": {
                        "value": "We thank the reviewer for highlighting the novelty of our proposed method, as well as the strong performance registered compared with other baseline models. We respond to reviewer\u2019s questions and comments below.\n\n1. Q: Row selection used, so the method cannot handle complex questions\n\nWhen humans are writing SQLs to query the large-scale database, one cannot memorize hundreds of thousands of row contents. Instead humans will focus on the table schema and several representative table rows as reference to write the SQL program. We argue that although only a few example rows are selected and fed into LLM for the SQL generation purpose in CODER, it can still mimic human behavior and generate solid SQLs that lead to correct question answering when the tables are large and the question is complex and based on a considerable number of rows. In our response to Reviewer 9EJC (the first reviewer, [link](https://openreview.net/forum?id=Qa0ULgosc9&noteId=nCYNTzpOo3)), we show the SQL generations of CODER and compare with human labeled SQLs. CODER only reads three selected table rows but still generate high-quality SQLs on the large-scale database of Spider dataset.\n\n2. Q: Minor typos in the manuscript (e.g., by by)\n\nWe thank the reviewer for carefully checking the draft. We updated the manuscript for correction.\n\n3. Q: In Figure 3, reader seems to appear after the SQL is selected, but in the explanation, the authors mention that the reader is used to select the SQL. Which is correct?\n\nTo clarify, READER is not responsible for the selection of SQL. READER will be input with SQL execution results, SQL program selected as well as context information.\n\nPlease let us know if you have further questions or comments."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379919173,
                "cdate": 1700379919173,
                "tmdate": 1700380884515,
                "mdate": 1700380884515,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VeeaMzFrlX",
            "forum": "Qa0ULgosc9",
            "replyto": "Qa0ULgosc9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4016/Reviewer_zsjW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4016/Reviewer_zsjW"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for open-domain table reasoning based on large language models. The method contains a table retriever, row selector, coder, and a reader. The table retriever is built on BM25. For the coder, the authors propose the simple-to-complex prompting strategy to generate SQL queries for three levels to resolve the complex input queries and infeasible queries by SQL itself. Then a reader module takes the context and the intermediate SQL execution results to produce the final answer to the query. \n\nTo resolve the hallucination problem of LLM when generating SQL queries, the authors also propose a generative reranking & sequential reasoning (GRSR) strategy by assessing the similarity of the generated SQL with the query so as to evade noisy tables. \n\nThe authors conduct comprehensive experiments to demonstrate the superiority of the proposed method as well as ablation studies to show the effectiveness of each proposed module."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is novel and effective, as demonstrated by experiments. Each proposed module is well-motivated and well-ablated."
                },
                "weaknesses": {
                    "value": "Some technique details are not clear. \nWhen using BM2.5 for the retriever, are you using the full table? \nFor the proposed simple-to-complex strategy for Coder, how is it working with GRSR? \nFor the GRSR, how to deal with SQL queries grounded on multiple gold tables?"
                },
                "questions": {
                    "value": "Are you going to release the code?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698987336604,
            "cdate": 1698987336604,
            "tmdate": 1699636363850,
            "mdate": 1699636363850,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "faQhstoti8",
                "forum": "Qa0ULgosc9",
                "replyto": "VeeaMzFrlX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer zsjW"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the superiority and novelty of our proposed method and the comprehensiveness of our experiments and ablation studies. Below we answer the reviewer's questions correspondingly.\n\n1. Q: When using BM2.5 for the retriever, are you using the full table?\n\nYes. Because BM25 is scalable to large tables even in full size, in our experiments BM25 works on full table size. Technically we linearize the table structure into text strings by join table title (if any), header, and cell strings row by row. The BM25 algorithm is built upon the rank_bm25 library [1].\n\n\n2. Q: For the proposed simple-to-complex strategy for Coder, how is it working with GRSR?\n\nIn our implementation, the STC strategy will generate three SQLs and select one for execution and input to READER. GRSR will do ranking based on the selected SQL. We have also tried to concatenate all three SQLs and did the ranking, but this approach didn\u2019t yield better results. \n\n3. Q: For the GRSR, how to deal with SQL queries grounded on multiple gold tables?\n\nDue to the traits of the table QA datasets, in our work we only consider a single-gold-table scenario, where grounded evidence for each question only exists in one single table instead of multiple. One proposal on this is to preprocess retrieved tables to join them into single tables before doing reasoning. We leave further extending to multiple-gold-table settings as future research direction.\n\n\n4. Q: Are you going to release the code?\n\nYes we will release our code upon publication of our work.\n\n\n[1] https://github.com/dorianbrown/rank_bm25\n\nPlease let us know if you have further questions or comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700379071554,
                "cdate": 1700379071554,
                "tmdate": 1700379071554,
                "mdate": 1700379071554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Va6jgjKaLQ",
            "forum": "Qa0ULgosc9",
            "replyto": "Qa0ULgosc9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4016/Reviewer_9EJC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4016/Reviewer_9EJC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes OpenTab, an open-domain table reasoning framework. The framework includes A) a \"retriever\" module to retrieve a subset of structured tables from a table corpus (using BM25), and B) a \"reasoner\" module that is composed of a \"coder\" module for generating SQL queries, a \"rowselector\" module to rerank rows, and a \"reader\" module to parse SQL execution and its accompanying context into a natural language response. The authors also propose a Generative Reranking & Sequential Reasoning (GRSR) strategy to rerank retrieved tables using SQL and query similarity, and a simple-to-complex (STC) prompting strategy for SQL generation. \n\nThe OpenTab framework contributes a method for open-domain table reasoning that has higher accuracy than existing baselines, and that is reliant only on few-shot prompting (no fine-tuning)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is largely written clearly, and easy to follow. The OpenTab framework is a notable contribution to the table reasoning and semantic parsing community for its approach to open-domain reasoning. The paper's combination of several approaches into the OpenTab framework can be considered as an original application to the problem of open-domain reasoning."
                },
                "weaknesses": {
                    "value": "- The paper claims to provide a new simple-to-complex prompting strategy for text-to-SQL generation using LLMs. However, the details of this prompting strategy are relegated to Appendix A.1, and it is not clear how the LLM is guided to increase the complexity of successive SQL generations. Moreover, for this claim of a new strategy to be sound, one would expect to see it applied to standard text-to-SQL benchmarks like Spider or the new BIRD-SQL.\n\n- The paper's framework for retrieval and generation for open-domain table reasoning is analogous to retrieval-augmented LLMs, but there is no mention of RAG (for e.g. [Lewis et al., 2020](https://arxiv.org/abs/2005.11401)). The paper is thus strangely not situated within the RAG literature (but the paper contains fair contributions to RAG, such as GRSR)."
                },
                "questions": {
                    "value": "**Questions**:\n\n1. The ICL prompt for SQL generation includes this line: \"Generate 3 SQLite programs with respect to the question separated by [SQLSEP], the complexities of the SQLite programs generated ascend (basic, intermediate, advanced).\" to \"sequentially generate three SQL programs with ascending complexities\", SQL-basic, SQL-intermediate and SQL-advanced.\n\n    - a) This prompt seems intuitively underspecified in guiding the model to generate SQL that focuses on column selection in the first SQL, and column+row selection in the second SQL. In other words, the wording of the prompt doesn't specify what kind of SQL should be generated in each stage. How did the authors verify the SQL was actually of that specific functionality?\n\n    - b) How did the authors verify that the SQL written was actually of ascending complexity?\n\n2. SQL generation performance: In claiming the new simple-to-complex prompting strategy for SQL generation, how did the authors verify the performance of this method on text-to-sql benchmarks like Spider or BIRD-SQL?\n\n3. Lost in the middle phenomenon: Was there any prompt engineering done for the ICL prompts to counter the \"lost in the middle\" phenomenon, particularly for the reader module which incorporates substantial context?\n\n4. From Section 4.2, given that the baseline methods \"require full tables to be fed into LLM for reasoning, which may exceed the upper bound of the input token limit, rendering an invalid prediction.\", do the results for baseline methods include or exclude these invalid predictions? \n\n5. From Table 4, what explains the large gap in difference for BM25 and BM25* for the \"w/o Fine-tuning\" row? This seems surprising given that BM25 is just supposed to be the authors' replication of BM25* presented in Kweon et al., 2023.\n\n6. On the effectiveness of GRSR: how does the effectiveness of GRSR vary with different choices of top-k (where k is number of tables retreived)?\n\n\n**Suggestions**:\n- In Section 3.1, \"DPR\" is first used. Suggest to include the full name of Dense Passage Retrieval when it is first used, and use the acronym subsequently.\n- In Section 5, explain why using all three SQL difficulties leads to the best performance, as was done in Section 3.2.1\n- It is likely relevant for the authors to include a section on retrieval augmented generation in LLMs under Section 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699568790202,
            "cdate": 1699568790202,
            "tmdate": 1699636363780,
            "mdate": 1699636363780,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nCYNTzpOo3",
                "forum": "Qa0ULgosc9",
                "replyto": "Va6jgjKaLQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9EJC #1"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for acknowledging the notable contributions by our work and the novelty of OpenTab to the important Open-domain table reasoning task. Below we address the reviewer's concerns accordingly. \n\n\n1. Q: The paper claims to provide a new simple-to-complex prompting strategy for text-to-SQL generation using LLMs, \u2026and it is not clear how the LLM is guided to increase the complexity of successive SQL generations\n\nWe guided LLM to generate three increasingly complex SQL programs by two means. \n\n- Firstly in the prompt we have such instruction, \u201cGenerate 3 SQLite programs with respect to the question separated by [SQLSEP], the complexities of the SQLite programs generated ascend (basic, intermediate, advanced).\u201d. Such instruction works effectively on the instruction-tuned LLMs we studied, which are gpt-3.5-turbo and falcon-180b.\n- What\u2019s more we included few-shot examples in the prompt, showing that generated SQLs are in three complexities reflecting different functionalities in order. LLMs learned from the demonstrations well because of their in-context learning ability.\n\n\n2. Q: \u2026prompt seems intuitively underspecified in guiding the model to generate SQL that focuses on column selection in the first SQL, and column+row selection in the second SQL. In other words, the wording of the prompt doesn't specify what kind of SQL should be generated in each stage. How did the authors verify the SQL was actually of that specific functionality?\n\nAs we have explained in the last question, the SQL generation is instructed and guided by the prompt instruction as well as the few-shot demonstrations. Although we didn\u2019t explicitly map [simple, intermediate, complex] to specific functionalities in the instruction, such correlation can be implicitly recovered in the given few-shot demonstrations. Moreover, we designed such demonstrations with the help of gpt-4. We asked gpt-4 to generate SQLs following the [simple, intermediate, complex] types and got back the initial SQL drafts reflecting the different functionalities (row selection, column selection, and beyond), intuitively showing that there could be some implicit alignment between the keywords and the functionalities.\n\n\n3. Q: Moreover, for this claim of a new strategy (STC) to be sound, one would expect to see it applied to standard text-to-SQL benchmarks like Spider or the new BIRD-SQL.\n\nSTC is proposed in the context of end-to-end table QA task, in which SQL programs work as an efficient and scalable interface to query the database and extract relevant information for READER to make inference on. SQL generation is an intermediate tool in the QA pipeline, instead of outcome. While on text-to-SQL benchmarks, ground truth is SQL program itself, which is different from our motivation. While text-to-SQL task is not the focus of this work, we carried out complementary experiments on the \u201cflight_4\u201d database of the Spider dataset to verify the effectiveness of our CODER module with the STC prompting strategy. Below we list the results in the order of question, ground truth, and SQL generated by CODER. We also show the stats of the large-scale database in the table.\n\n\n| Table name | Row number | \n| -------- | -------- | \n| airlines | 6162     | \n| airports | 7184 |\n| routes | 67240 |\n\n\n```\nHow many routes does American Airlines operate?\nSELECT count(*) FROM airlines AS T1 JOIN routes AS T2 ON T1.alid = T2.alid WHERE T1.name = 'American Airlines'\nSELECT COUNT(*) FROM routes r INNER JOIN airlines a ON r.alid = a.alid WHERE a.name = 'American Airlines';\n```\n\n```\nHow many routes end in a Canadian airport?\nSELECT count(*) FROM airports AS T1 JOIN routes AS T2 ON T1.apid = T2.dst_apid WHERE country = 'Canada'\nSELECT COUNT(*) FROM routes WHERE dst_apid IN (SELECT apid FROM airports WHERE country = 'canada');\n```\n\n\n```\nWhat are the names of all airports in Cuba or Argentina?\nSELECT name FROM airports WHERE country  =  'Cuba' OR country  =  'Argentina'\nSELECT name FROM airports WHERE country LIKE \"%cuba%\" OR country LIKE \"%argentina%\";\n```\n\n```\nWhat are the countries of all airlines whose names start with Orbit?\nSELECT country FROM airlines WHERE name LIKE 'Orbit%'\nSELECT DISTINCT country FROM airlines WHERE name LIKE \"Orbit%\";\n```\n\n```\nIn how many cities are there airports in the country of Greenland?\nSELECT count(DISTINCT city) FROM airports WHERE country = 'Greenland'\nSELECT COUNT(DISTINCT city) as airport_cities FROM airports WHERE country LIKE \"%greenland%\";\n```\n\nWe see that our CODER is able to generate solid SQL programs that achieve equivalent functionalities as human labeled, demonstrating the efficacy of our CODER module."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378717781,
                "cdate": 1700378717781,
                "tmdate": 1700378742014,
                "mdate": 1700378742014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6hKNQE88ok",
                "forum": "Qa0ULgosc9",
                "replyto": "Va6jgjKaLQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9EJC #2"
                    },
                    "comment": {
                        "value": "4. Q: Didn\u2019t do literature review on RAG papers\n\nThanks for the suggestion. We drafted a paragraph of discussion about the related works on RAG in Section 6 of the draft. We also put it here for review.\n\nRetrieval Augmented Generators. RAGs [6] leverage retrievers to fetch information from external knowledge database and augment the text input to the language models. REALM [5] and RETRO [1] pretrains the retrieval augmentation framework in the end-to-end manner so that the language model learns to better utilize the retrieved information, while ATLAS [2] jointly trains the retriever as well as the language model. We point out that these well-known RAG models are specialized in the text reasoning domain and cannot be directly applied to table reasoning tasks without fine-tuning, thus inapplicable to the experimental setup of this work.\n\n\n\n5. Q: Lost in the middle issue considered?\n\nAs in Liu et al. 2023 [7], the \u201clost in the middle\u201d problem is based on the severe long-context issue of LLM. For OpenTab in the open-domain setting, where the framework needs to inference on multiple retrieved tables, the JR (joint reasoning) strategy has struggling accuracy compared with SR (sequential reasoning) and GRSR (generative reranking & sequential reasoning). Considering the long-context trait of JR, the \u201clost in the middle\u201d issue could be the cause that is impacting the performance of JR. While for the default strategy GRSR, each time OpenTab does reasoning over one retrieved table and later collectively carries out generative reranking, so that we can effectively evade the \u201clong context\u201d issue and \u201clost in the middle\u201d problem.\n\n\n6. Q: From Section 4.2, given that the baseline methods \"require full tables to be fed into LLM for reasoning, which may exceed the upper bound of the input token limit, rendering an invalid prediction.\", do the results for baseline methods include or exclude these invalid predictions?\n\nFor the baseline methods, we include the invalid predictions and treat them as wrong predictions as scalability to large tables is an important property for advanced table reasoners. In experiment most tables in Open-WikiTables and FEVEROUS won\u2019t exceed the 4k token limit of gpt-3.5-turbo and falcon-180b with 2-shot demonstrations. The most severe token overflow issue happens on BINDER, which by default uses 14-shot examples. To fit the long sequence we deploy gpt-3.5-turbo-16k as backbone to run the BINDER.\n\n\n7. Q: From Table 4, what explains the large gap in difference for BM25 and BM25* for the \"w/o Fine-tuning\" row? This seems surprising given that BM25 is just supposed to be the authors' replication of BM25* presented in Kweon et al., 2023.\n\nAs in the official repo of Kweon et al, 2023 [3,4], there is no public implementation of the BM25 experiments. In the paper the authors did not disclose details of their implementation, either. Due to these reasons we cannot fully reproduce their results. We implemented BM25 on our side following the setting of Kweon et al, 2023 to the most extent and reported the numbers in Table 4. We will open source our code when our work gets published. \n\n\n8. Q: On the effectiveness of GRSR: how does the effectiveness of GRSR vary with different choices of top-k (where k is number of tables retrieved)?\n\nWe refer the reviewer to the third paragraph of Section 5, where we did ablation study to verify the effectiveness of GRSR with varying k. In Figure 5, GRSR improves the QA accuracy with more tables retrieved, while the plain JR and SR methods got more distracted and might get worse accuracy when k increases.\n\n\n9. Q: In Section 5, explain why using all three SQL difficulties leads to the best performance, as was done in Section 3.2.1\n\nFor our STC strategy, SQLs are incrementally generated, and the generation path can also function as reasoning steps, making the generations more likely valid. Moreover the solidness of the generated SQL programs will be verified on the fly, thus we can try to evade invalid SQLs that will either have syntax errors or empty execution results, which is common for a solo SQL generation. To sum up, the STC prompting strategy improves the performance through incremental reasoning as well as flexibility."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378820568,
                "cdate": 1700378820568,
                "tmdate": 1700378820568,
                "mdate": 1700378820568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gwtEgFYwFx",
                "forum": "Qa0ULgosc9",
                "replyto": "Va6jgjKaLQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9EJC #3"
                    },
                    "comment": {
                        "value": "[1] Borgeaud, Sebastian, et al. \"Improving language models by retrieving from trillions of tokens.\" International conference on machine learning. PMLR, 2022.\n\n[2] Izacard, Gautier, et al. \"Few-shot learning with retrieval augmented language models.\" arXiv preprint arXiv:2208.03299 (2022).\n\n[3] Kweon, Sunjun, et al. \"Open-WikiTable: Dataset for Open Domain Question Answering with Complex Reasoning over Table.\" arXiv preprint arXiv:2305.07288 (2023).\n\n[4] https://github.com/sean0042/Open_WikiTable\n\n[5] Guu, Kelvin, et al. \"Retrieval augmented language model pre-training.\" International conference on machine learning. PMLR, 2020.\n\n[6] Lewis, Patrick, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" Advances in Neural Information Processing Systems 33 (2020): 9459-9474.\n\n[7] Liu, Nelson F., et al. \"Lost in the middle: How language models use long contexts.\" arXiv preprint arXiv:2307.03172 (2023).\n\nPlease let us know if you have further questions or comments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378900592,
                "cdate": 1700378900592,
                "tmdate": 1700378900592,
                "mdate": 1700378900592,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]