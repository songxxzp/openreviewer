[
    {
        "title": "Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX"
    },
    {
        "review": {
            "id": "yeazBWAtMo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6204/Reviewer_d6hE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6204/Reviewer_d6hE"
            ],
            "forum": "C4CxQmp9wc",
            "replyto": "C4CxQmp9wc",
            "content": {
                "summary": {
                    "value": "Suite of Combinatorial Optimization benchmarks in JAX. Some experiments with an AC algorithm"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "New benchmarks are always good, especially CO, where fewer benchmarks are available."
                },
                "weaknesses": {
                    "value": "No comparisons to other benchmarks or implementations. It is a sympathetic and perhaps substantial effort, but lacks elements that would achieve wide adaptations. The software engineering is there, the science is unclear.\n\nThere are Gym-JAX environments, and there are CO-Gym implementations (OR-Gym).  \n\nIt appears that Jumanji does not follow the Gym interface. Stable Baselines algorithm are therefore not a drop in plugin.\n\nExplain clearly the difference with a Gym interface. Why this choice? \n\nCarrying around explicit state deviates from an RL principle, that the environment has the state, and not the agent. \n\nExperimental validation with limited algorithms. No comparison to other benchmarks."
                },
                "questions": {
                    "value": "What is the contribution of this paper?\n\nWouldn\u2019t it make more sense to remain Gym-compliant in providing a Gym-JAX-CO implementation? This remains implicit, and is not explained.\n\nWould a wrapper be possible for a Gym API? Could you use stable baselines unchanged?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697374906873,
            "cdate": 1697374906873,
            "tmdate": 1699636676110,
            "mdate": 1699636676110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K0KQmWiIKO",
                "forum": "C4CxQmp9wc",
                "replyto": "yeazBWAtMo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and feedback. We kindly provide answers to their questions and hope that the provided clarifications are sufficient for reassessing the recommendation for publication.\n\n> Carrying around explicit state deviates from an RL principle, that the environment has the state, and not the agent.\n\nWe agree with the reviewer that carrying the state explicitly could make some algorithmic implementations deviate from the MDP principle. This is because implementing simulators in JAX requires functions to be pure, this means their computation graph must link inputs to outputs without any state variables. This paradigm is in line with other JAX-based RL environments like Brax and Gymnax. The RL formulation differentiates states (used by the environment) from observations (given to the agent). In the Jumanji API, methods return a tuple `(state, timestep)`. Therefore, the RL principle is conserved if one implements an environment loop that is responsible for giving the state to the environment and the timestep (which contains the observation) to the agent. An example of such a training pipeline is provided in `jumanji.training.agents.a2c.A2CAgent` and was used to obtain the learning curves from Figure 1.\n\n> What is the contribution of this paper?\n\nAs opposed to demonstrating an algorithmic novelty, this work introduces a benchmark suite for RL research. Jumanji is an open-source and diverse suite of industry-inspired RL environments that are fast, flexible, and scalable. Within the COP community, recent works [Kwon et al., 2020; Hottung et al., 2022; Grinsztajn et al., 2023] have tackled problems such as generalization to bigger problem sizes and different problem distributions. Follow-up works could benefit from using Jumanji to train and evaluate their methods on a wide distribution of problems. \n\nAs such, Jumanji is a tool for RL research, for which we provide baseline actor-critic agents as a point of reference for researchers to build upon.\n\n> Wouldn\u2019t it make more sense to remain Gym-compliant in providing a Gym-JAX-CO implementation? This remains implicit, and is not explained.\n\nGym environments are stateful in the sense that their methods are impure and thus cannot be compiled with JAX. This is the approach other JAX RL libraries like Gymnax have chosen, diverting from the Gym API to make the environments\u2019 methods stateless. In Jumanji, we have chosen the `dm_env` API based on a timestep to make the function signatures lighter. This way, both reset and step methods return a state and a timestep. The former contains everything the environment needs to run the dynamics, theoretically, it is hidden from the agent. The timestep is a namedtuple that contains everything the agent sees, i.e. the observation, reward, discount, etc. This API avoids having 4 or 5 returned objects like one can find in Gym or Gymnax. It also prevents us from mixing environment data and agent observation like in Brax.\n\n> Would a wrapper be possible for a Gym API? Could you use stable baselines unchanged?\n\nA `JumanjiToGymWrapper` is already implemented via `from jumanji.wrappers import JumanjiToGymWrapper`. This wrapper converts a Jumanji environment to a `gym.Env` environment. The downside to this is that, although the step and reset functions can be jitted, the full environment loop (acting for multiple steps and training) cannot be jitted. By switching to a stateful paradigm (e.g. Gym), one loses the ability to fully train end-to-end on a device [Hessel et al., 2021]. We have amended appendix A.4 to precise the currently implemented wrappers in Jumanji, which include the gym wrapper.\n\n**References**:\n- Hessel et al. Podracer architectures for scalable reinforcement learning. 2021.\n- Kwon et al. POMO: Policy Optimization with Multiple Optima for Reinforcement Learning. NeurIPS (2020).\n- Hottung et al. Efficient Active Search for Combinatorial Optimization Problems. ICLR (2022).\n- Grinsztajn et al. Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization NeurIPS (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072482058,
                "cdate": 1700072482058,
                "tmdate": 1700152932074,
                "mdate": 1700152932074,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o9r7edZCbQ",
                "forum": "C4CxQmp9wc",
                "replyto": "K0KQmWiIKO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6204/Reviewer_d6hE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6204/Reviewer_d6hE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your detailed response. In light of your response and the other reviews, I feel that my rating is accurate and leave it unchanged."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634932174,
                "cdate": 1700634932174,
                "tmdate": 1700634932174,
                "mdate": 1700634932174,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Jd33b4rzRz",
            "forum": "C4CxQmp9wc",
            "replyto": "C4CxQmp9wc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6204/Reviewer_VTas"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6204/Reviewer_VTas"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose Jumanji, a diverse set of accelerated environments written in JAX focused on NP-hard combinatorial optimization problems (COPs). Jumanji is fully open-source, fast, flexible, and scalable, covering 18 environments such as TSP (Travelling Salesman Problem). The authors also present A2C learning curves in these 18 environments to demonstrate end-to-end learning. Interestingly, Jumanji can tune the difficulties of the environments, showing that these environments can get exponentially more difficult to solve."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* Open-source accelerated environments in COPs: most of the accelerated environments are in robotics (e.g., NVIDIA's isaacgym or Google's brax), but I like the authors specific focus on NP-hard optimization problems.\n* Optimal performance in some games: I like the authors added the reference optimal performance in some of the 18 environments."
                },
                "weaknesses": {
                    "value": "I do not see any major weakness. One issue is that Figure 3 does not seem like a fair comparison with GPU. In particular TPU-v4s should be compared with A100s instead of RTX 2080 Super."
                },
                "questions": {
                    "value": "I am curious why the authors chose A2C as the training algorithm instead of P"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6204/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6204/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6204/Reviewer_VTas"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698599429675,
            "cdate": 1698599429675,
            "tmdate": 1699636676000,
            "mdate": 1699636676000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KrG7oxeJp7",
                "forum": "C4CxQmp9wc",
                "replyto": "Jd33b4rzRz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback.\n\n> I do not see any major weakness. One issue is that Figure 3 does not seem like a fair comparison with GPU. In particular TPU-v4s should be compared with A100s instead of RTX 2080 Super.\n\nWe agree with the reviewer that the performance obtained with A100s should be closer to that of TPUs, but the formers were not available at the time of running the experiments. We would like to clarify that the experiment illustrated in Figure 3 is meant to highlight the increase in throughput as one scales the available hardware, and not to showcase differences between TPUs and GPUs.\n\n> I am curious why the authors chose A2C as the training algorithm instead of P\n\nIndeed, we chose the A2C algorithm for our benchmark instead of, say, PPO or other more advanced algorithms. We purposely wanted to show how simple algorithms, when combined with neural network architectures that account for the symmetries of the considered problem, can achieve reasonable performances. This benchmark can then be used by researchers to build better-performing algorithms and use our A2C performance as a reference."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072333879,
                "cdate": 1700072333879,
                "tmdate": 1700152629069,
                "mdate": 1700152629069,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NXGgKm93sx",
            "forum": "C4CxQmp9wc",
            "replyto": "C4CxQmp9wc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6204/Reviewer_S6or"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6204/Reviewer_S6or"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a Jax based RL environment suite called Jumanji. The 18 environments focus on combinatorial optimization problems, designed to be fast, flexible, and scalable. They also provide an A2C benchmark and examples to motivate these problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- RL for many years has struggled with good environment code maintenance and support, and it is good to see this problem continue to be addressed \n- The code base seems to be well designed and documented, the doc strings are generally informative and type hints are present."
                },
                "weaknesses": {
                    "value": "- NP Hard optimization style problems have seen some interest in RL, but are not as common in literature, it would be beneficial to have more citations justifying their uses or explain more how common RL problems can be rethought into the COP formalism\n- Having some sort of UML or diagram would be of great help to understanding the API.\n- I don\u2019t think random policy adds anything in Figure 2. It is expected that random does poorly and I\u2019m not sure it adds much (given the trends of the curves, the impression of learning comes across)\n- I\u2019m not sure how much the y-axis labels matter in Figure 2 given how much clutter they add. A lot of these environments are not super common (and even in common Atari environments human normalised performance is increasingly common as a metric since the actual scores don\u2019t mean much to most people). As long as they are all linear axes, and the optimal performance is there, all that matters is that the lines are going up (since this isn\u2019t an algorithm paper, this figure is just showing things can learn in your environments).\n- A plot like Fig 3(b) with number of TPUs vs. time to reach a certain performance could make a good figure (for the appendix at the very least)\n- If CPU is not visible on the plot, I would just leave it off the labels and keep the text remark\n- Although there are a lot of different environments implemented, it would be beneficial to have a point of comparison. As the authors note, there has been a fair amount of work in high performance environments already. Even if you can\u2019t make a 1 to 1 comparison (because the environments are not the same), finding something of comparable complexity and having a figure in the appendix would help to ground the speedups."
                },
                "questions": {
                    "value": "- How important is hardware flexibility? Are TPUs widely used outside google?\n- Gamma is put in the MDP formalism of Jumanji. Although this can be seen both in and outside of the tuple, is there any explicit representation of it in the software? I.e. in the Jumanji environments, clearly all the other elements of the tuple are required to be defined for a functioning environment, but is the gamma represented?\n- It would be beneficial to give more of an explanation of the state, just another sentence or so, explaining (perhaps with an example) what it is and contains. I assume it is a pytree (since the observation is), but is the key element required? Does step have to split the key necessarily if it doesn\u2019t use it (small details like this could go in the appendix)?\n- Environment version control is mentioned, but how often are changes made that increment this version? Version control is nice, but if there are hundreds of versions, it isn\u2019t a panacea.\n- Appendix C2 demonstrates weak (sometimes negative) scaling on CPU. Why is this the case? I would expect some speedup up to the 8 cores (assuming you are mapping across all cores, jax by default will just work with 1 (https://github.com/google/jax/issues/5022). \n- Why does figure 3a start at 2^7 environments? The on many of the environments doesn\u2019t seem as impressive as it could if this started at 2^0 perhaps\n- Why is it called Jumanji?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6204/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6204/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6204/Reviewer_S6or"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726955937,
            "cdate": 1698726955937,
            "tmdate": 1700377357097,
            "mdate": 1700377357097,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bp4OLPQoAQ",
                "forum": "C4CxQmp9wc",
                "replyto": "NXGgKm93sx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their helpful suggestions and we hope we have answered all their questions below.\n\n> NP Hard optimization style problems have seen some interest in RL, but are not as common in literature, it would be beneficial to have more citations justifying their uses or explain more how common RL problems can be rethought into the COP formalism\n\nWe have adjusted the text in the \"Combinatorial Optimization Problems\" paragraph of the related work section to better emphasize how COPs can be tackled with RL. We included the works of [Kool et al., 2018; Hottung et al., 2022] which proposed a transformer architecture to efficiently treat COPs as RL episodes.\n\n> Having some sort of UML or diagram would be of great help to understanding the API.\n\nWe thank the reviewer for the feedback and agree that a UML diagram would improve the clarity of the code base and provide useful insight for new developers working with Jumanji. We will provide this as part of the appendix and in the repository documentation.\n\n> I don\u2019t think random policy adds anything in Figure 2. It is expected that random does poorly and I\u2019m not sure it adds much (given the trends of the curves, the impression of learning comes across)\n\nAlthough a random policy is expected to perform poorly, we have decided to include this in our plots as we have repeatedly found that the random policy score is a useful lower bound for debugging new algorithms. An agent whose neural network parameterization and/or algorithm is incorrect could perform close to the random policy in expectation. Therefore, it helps to assess algorithm implementations and we think it will be useful to the community as well.\n\n> I\u2019m not sure how much the y-axis labels matter in Figure 2 given how much clutter they add.\n\nWe agree with the reviewer that the reward scales are generally arbitrary, and do not bring in itself more information than the relative improvements we observe during the training. However, in some environments (e.g. TSP or CVRP where the reward is the negative tour length, JSSP where it\u2019s the negative makespan), they directly correspond to the underlying problem objective and align with previous work, so having it could make the comparison easier in general.\n\n> A plot like Fig 3(b) with number of TPUs vs. time to reach a certain performance could make a good figure (for the appendix at the very least)\n\nWe thank the reviewer for their feedback and agree that this would be insightful to the reader. We have amended section C.2 of the appendix with such a table, and we hope it helps better understand the experiment.\n\n> Although there are a lot of different environments implemented, it would be beneficial to have a point of comparison. As the authors note, there has been a fair amount of work in high performance environments already. Even if you can\u2019t make a 1 to 1 comparison (because the environments are not the same), finding something of comparable complexity and having a figure in the appendix would help to ground the speedups.\n\nSince we can\u2019t compare the throughput of a Jumanji environment with a non-JAX version of it, we can at least demonstrate the difference in throughput between running the environment on a CPU (Figure 7b) and on a GPU (Figure 3a). We observe that running the environment on an accelerator brings orders of magnitude speed-ups compared to having the simulation on a CPU.\n\n> How important is hardware flexibility?\n\nAs we are focusing on providing tooling that will be used by the greater research community we believe that hardware flexibility is a valuable attribute of any modern research library. Many researchers will be in a position where they only have access to CPUs, while others will want to utilize GPUs. Although many researchers will not have access to TPUs, we believe it is important for the library to also have the flexibility to be used by these proprietary architectures. Additionally, the flexibility of XLA means that JAX-based libraries like Jumanji are future-proofed to new advances in hardware. \n\n> Are TPUs widely used outside google?\n\nWe acknowledge the reviewer's observation that many researchers do not have access to TPUs. However:\n\n- There is still a sizable portion of the RL community, in both academia and industry, using these devices outside of Google through the TRC program, as shown by this [publication list](https://sites.research.google/trc/publications).\n- Importantly, we highlight that Jumanji can be used with any XLA-compatible device (e.g. GPU). We emphasize that this library is not restricted to TPU users."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071885426,
                "cdate": 1700071885426,
                "tmdate": 1700152216759,
                "mdate": 1700152216759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J51Kf7TuEw",
                "forum": "C4CxQmp9wc",
                "replyto": "NXGgKm93sx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors (Part 2)"
                    },
                    "comment": {
                        "value": "> Gamma is put in the MDP formalism of Jumanji. Although this can be seen both in and outside of the tuple, is there any explicit representation of it in the software? I.e. in the Jumanji environments, clearly all the other elements of the tuple are required to be defined for a functioning environment, but is the gamma represented?\n\nGamma is usually specified as part of the problem (i.e. the MDP). However, most environment implementations (e.g. in Gym) do not include gamma. Therefore, we have adhered to the standard practice and have not included it. Moreover, all Jumanji environments have a limited horizon, which allows for undiscounted values (gamma = 1). Nonetheless, we mention gamma in Section 3.1 when properly defining the RL objective.\n\n> It would be beneficial to give more of an explanation of the state, just another sentence or so, explaining (perhaps with an example) what it is and contains. I assume it is a pytree (since the observation is), but is the key element required? Does step have to split the key necessarily if it doesn\u2019t use it (small details like this could go in the appendix)?\n\nWe thank the reviewer for their feedback and have accordingly updated the **State** paragraph of section 3.2 for better clarity. The reviewer is correct in that the state is a pytree. Additionally, the key is needed in order to handle stochasticity in wrappers like the `JumanjiToDMEnvWrapper`. If the environment is not stochastic, i.e. if it does not use the key, then it does not have to split it. On the contrary, it will split the key when any stochastic computation is needed within the step function.\n\n> Environment version control is mentioned, but how often are changes made that increment this version? Version control is nice, but if there are hundreds of versions, it isn\u2019t a panacea.\n\nWe agree and therefore aim to use the version control in the environments sparingly, only increasing the version after changes that affect the environment's behavior/performance. This policy is in line with that followed in Gymnasium (formerly, OpenAI Gym), where there are at most a handful of versions for each environment.\n\n> Appendix C2 demonstrates weak (sometimes negative) scaling on CPU. Why is this the case? I would expect some speedup up to the 8 cores (assuming you are mapping across all cores, jax by default will just work with 1 (https://github.com/google/jax/issues/5022).\n\nWe emphasize that Fig 7.b displays the scaling of the environment performance on a 2-core CPU and not a TPU.\n\n> Why does figure 3a start at 2^7 environments? The on many of the environments doesn\u2019t seem as impressive as it could if this started at 2^0 perhaps\n\nThe higher the number of parallel environments, the greater the parallelization on hardware accelerators. As a reference, Brax [Freeman et al., 2021] trained with a batch size of 2048 and experimented with a number of parallel environments ranging from $2^7$ to $2^{16}$.\n\n> Why is it called Jumanji?\n\nIn this context, 'Jumanji' is a metaphorical reference to the jungle, representing the complex and dynamic nature of scalable RL environments in JAX, much like the unpredictable jungle challenges in the Jumanji series.\n\n**References**:\n- Kool et al. Attention, learn to solve routing problems! ICLR (2018).\n- Freeman et al. Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation, 2021. URL: http://github.com/google/brax.\n- Hottung et al. Efficient Active Search for Combinatorial Optimization Problems. ICLR (2022)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072214461,
                "cdate": 1700072214461,
                "tmdate": 1700152531849,
                "mdate": 1700152531849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5OJ1DWHGkJ",
                "forum": "C4CxQmp9wc",
                "replyto": "NXGgKm93sx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6204/Reviewer_S6or"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6204/Reviewer_S6or"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors responses, and they have sufficiently addressed many of my listed concerns. As such, I am upgrading my recommendation (5->6)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377341914,
                "cdate": 1700377341914,
                "tmdate": 1700377368689,
                "mdate": 1700377368689,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DhIWRKrdHo",
            "forum": "C4CxQmp9wc",
            "replyto": "C4CxQmp9wc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6204/Reviewer_7YMa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6204/Reviewer_7YMa"
            ],
            "content": {
                "summary": {
                    "value": "Jumanji is a suite of scalable reinforcement learning environments designed for RL research with industrial applications. It provides a collection of environments that are fast, flexible, and scalable, focusing on combinatorial problems and decision-making tasks. Jumanji leverages JAX and hardware accelerators to facilitate rapid research iteration and large-scale experiments. It stands out from existing RL environments by offering customizable initial state distributions and problem complexities and includes actor-critic baselines for benchmarking. The paper demonstrates Jumanji's high scalability and flexibility through experiments, positioning it as a tool to advance RL research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Good paper and an important engineering contribution to an area of research in NP-hard combinatorial optimization problems (COPs). Solid design and software engineering work to make Jumanji modular, scalable, and fast and to fully unlock the power of hardware acceleration. The set of environments and tasks is complimentary in some sense to continuous control Jax-based training environments created by Google Brax team and will help to advance research in the area combinatorial problems and decision-making tasks."
                },
                "weaknesses": {
                    "value": "A lack of a new research results and novel approaches. But it\u2019s totally expected from such kind of more engineering oriented projects."
                },
                "questions": {
                    "value": "What are the most important research challenges do you expect Jumanji will help to address?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699338704084,
            "cdate": 1699338704084,
            "tmdate": 1699636675712,
            "mdate": 1699636675712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BHkKUPcoaF",
                "forum": "C4CxQmp9wc",
                "replyto": "DhIWRKrdHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6204/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and suggestions and hope that our answers clarify any concerns.\n\n> A lack of a new research results and novel approaches. But it\u2019s totally expected from such kind of more engineering oriented projects.\n\nWe agree with the reviewer that this work does not present new research results or approaches. Instead, our contribution focuses on utilizing advances in both hardware architecture and the high-performance computing library JAX to provide a test bed of hardware-accelerated environments. This contribution is impactful to the research community as it enables fast iteration of research and, due to the efficiency of our JAX environments, removes the need for large compute clusters to carry out meaningful RL research, as is the case with the majority of RL benchmarks.\n\n> What are the most important research challenges do you expect Jumanji will help to address?\n\nWithin the COP community, recent works [Kwon et al., 2020; Hottung et al., 2022; Grinsztajn et al., 2023; Chalumeau et al., 2023] have tackled problems such as generalization to bigger problem sizes and different problem distributions. Follow-up works could benefit from using Jumanji to train and evaluate their methods on a wide distribution of problems. Additionally, we would like to invite researchers and engineers working on real-world applications to use the flexibility of Jumanji to tackle the very problems that hinder RL applications.\n\n**References**:\n\n- Kwon et al. POMO: Policy Optimization with Multiple Optima for Reinforcement Learning. NeurIPS (2020).\n- Hottung et al. Efficient Active Search for Combinatorial Optimization Problems. ICLR (2022).\n- Grinsztajn et al. Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization NeurIPS (2023).\n- Chalumeau et al. Combinatorial Optimization with Policy Adaptation using Latent Space Search. NeurIPS (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071626788,
                "cdate": 1700071626788,
                "tmdate": 1700151716197,
                "mdate": 1700151716197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]