[
    {
        "title": "Efficient Instance-Optimal Finite-Sum Minimization"
    },
    {
        "review": {
            "id": "lwVk21rdtI",
            "forum": "RR70yWYenC",
            "replyto": "RR70yWYenC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7163/Reviewer_FDce"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7163/Reviewer_FDce"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the instance-optimal finite-sum minimization problem, where the setting is we have a sequence of functions $f_1, \\cdots, f_n$, and we need to output a sequence of points $x_1^*, \\cdots, x_n^*$, with $x_i^*$ being the minimizer of $\\sum_{j=1}^i f_j(x)/i$. This problem is inline with the modern machine learning setting as usually the training set is went through for only once, but we hope to make sure the current solution is the optimal for the current situation. \n\nFor this setting, the existing SGD or variance reduced methods are not optimal, because naively we need to call these methods $n$ times in order to find these $n$ minimizers. This paper proposed a new algorithm which finds the minimizer iteratively, which incurs $\\tilde{O}(n/\\epsilon^{1/3} + \\log n /\\sqrt{\\epsilon})$ FO complexity. The authors also show that the lower bound of this problem is $\\tilde{O}(n/\\epsilon^{1/4})$, which means the newly proposed algorithm is close to optimal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: the setting is interesting, and the proposed algorithm is novel as far as I know. In their algorithm, they propose to switch between compute full gradient with $i$ FOs and compute a simple modification based on the previous gradient with 1 FO for full gradient estimation, which is a special trick. \n\nQuality: The results are nice, because the authors not only provide the convergence guarantees, but also compare it with the existing methods, and provide a very close lower bound. \n\nClarity: The overall writing of this paper is excellent. \n\nSignificance: I think the results are interesting, because this setting is an important setting to explore, and they have presented a full story about this setting."
                },
                "weaknesses": {
                    "value": "I guess the main weakness of this paper is about the strong convexity assumption of f. This is kind of old-school assumption, because nowadays we always use deep neural networks to fit the data, and these models are almost never strongly convex.  In fact, as far as I know, variance reduced techniques are not so useful for neural networks, so I am afraid that the proposed method are not very practical for real world models."
                },
                "questions": {
                    "value": "I do not have additional questions. I have went through the proofs in the main paper, and they look correct to me. However, I did not get time to verify the long proofs in the appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698491443599,
            "cdate": 1698491443599,
            "tmdate": 1699636849248,
            "mdate": 1699636849248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9WGKWFVyX5",
                "forum": "RR70yWYenC",
                "replyto": "lwVk21rdtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for all the work and the positive feedback on our work.\n\nWe just want to mention that the primary scope of our work was to formally frame and investigate the problem of efficient updates of models upon the arrival of new data. We agree with the reviewer that going beyond the strong convexity assumption is a very important and interesting research direction. Investigating the strong convex case serves as an important first step towards this direction while it already tackles several challenges and reveals surprising properties (especially with respect to lower bounds).\n\nWe would also like to remark that we have added additional experimental evaluations for NNs (see Appendix K and our response to Reviewer jqUD and Reviewer s2LS) revealing interesting results for SIOPT-Grad. \n\nWe hope that our work will instantiate a research direction towards investigating efficient update methods with favorable empirical and theoretical performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978718601,
                "cdate": 1699978718601,
                "tmdate": 1699978718601,
                "mdate": 1699978718601,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4rfNbjpofx",
                "forum": "RR70yWYenC",
                "replyto": "9WGKWFVyX5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_FDce"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_FDce"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the rebuttal and will keep my score unchanged."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655237868,
                "cdate": 1700655237868,
                "tmdate": 1700655237868,
                "mdate": 1700655237868,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6ZqAg4d0yk",
            "forum": "RR70yWYenC",
            "replyto": "RR70yWYenC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7163/Reviewer_zvPG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7163/Reviewer_zvPG"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies a new problem of efficient instance-optimal finite-sum minimization, where given a sequence of functions, the goal is to find a sequence of points $x_1^\\star, \\ldots, x_n^\\star $ such that for each $i \\in [n]$, point $x_i^\\star$ approximately minimizes $\\sum_{j=1}^{i} f_j(x) / i $, with the number of first-order oracle(FO) calls as least as possible. Given the condition that each function $f_j(\\cdot)$ is strongly convex, the proposed method SIOPT-Grad produces an $\\epsilon$-optimal sequence within $\\tilde{O}(n/ \\epsilon^{1/3} + 1/\\sqrt{\\epsilon})$ FOs. In addition, they prove a lower bound that, there is no \\emph{natural} FO methods that completes this task within $O(n/\\epsilon^{1/4}) queries.\n\nIn the regime motivated by the empirical risk minimization in the strongly convex case where statistical error is $\\epsilon = O(1/n)$,  SIOPT-Grad requires only $O(n^{4/3})$ FOs compared to all previous FO solutions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The studied problem is well-motivated, both from the literature review on incremental learning, and from empirical estimation.\n- The theoretical analysis is complete, an upper bound and lower bound is provided for this problem, as well as compared to state of the art as in table 1.\n- The logic of this paper is easy to follow, and the assumptions/notations are presented in a clear way.\n- The paper has additional experiments on the ridge regression problem."
                },
                "weaknesses": {
                    "value": "- The upper bound provided by the algorithm is not tight compared to the lower bound. \n- The algorithm only work in the strongly convex case. \n\nMinor Issue:\n- there is no input in the algorithm 2.\n- the value of $\\alpha$ needs to be in line 2 of algorithm 1. \n- additional \"the\" in the second line of the first paragraph of section 3.1\n- the VR is never defined. suggestion: \"variance reduction(VR)\" and then use VR afterwards"
                },
                "questions": {
                    "value": "- The solution proposed in this paper, although is better in the number of FOs, may result in much added overall running time. Is there any practical regime where the number of FOs is expensive, compared to the efficiency of the algorithm measured by the running time?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7163/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7163/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7163/Reviewer_zvPG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698741822098,
            "cdate": 1698741822098,
            "tmdate": 1699636849127,
            "mdate": 1699636849127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "09KzLq6Lho",
                "forum": "RR70yWYenC",
                "replyto": "6ZqAg4d0yk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your work and the positive feedback on our paper. We will incorporate all the minor issues and thank you for spotting them.\n\nConcerning your question: We believe that considering *projection-free methods* that even with higher FO complexity may result in better running time due to the lack of a projection step, is a very interesting research direction."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978746300,
                "cdate": 1699978746300,
                "tmdate": 1699978746300,
                "mdate": 1699978746300,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "siUXg1q3hu",
                "forum": "RR70yWYenC",
                "replyto": "09KzLq6Lho",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_zvPG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_zvPG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply, I maintain my score for this paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700024455719,
                "cdate": 1700024455719,
                "tmdate": 1700024455719,
                "mdate": 1700024455719,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uiRMiCn04B",
            "forum": "RR70yWYenC",
            "replyto": "RR70yWYenC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the following natural problem: given a sequence of n convex functions. Find a sequence of inputs that minimizes the partial sum of the functions. Standard methods such as SGD require $O(n/\\epsilon)$ first order gradient information about the functions to get error up to additive $\\epsilon$. That paper introduces a novel algorithm which obtains the same accuracy using substantially fewer number of first order gradient information. In particular, the new algorithm only requires $O(n/\\epsilon^{1/3})$  gradient calls. For small $\\epsilon$, this is a big improvement. For instance, the authors note that $\\epsilon=1/n$ and is a very common setting, in which case their algorithm gives a polynomial improvement. The authors also introduce the notion of a 'natural algorithm' and prove a lower bound of $\\Omega(n/\\epsilon^{1/4})$ gradient calls under the assumption."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The introduction of the problem is a nice conceptual contribution. The new algorithm they proposed could also have other applications. In the notion of a \"natural algorithm\" is a nice contribution since it allows the analysis of algorithms and lower bounds."
                },
                "weaknesses": {
                    "value": "A weakness is the lack of intuition about their algorithm. I mostly follow the math, however conceptually I do not know why exactly they can get an improvement in the epsilon power. It seems like the high level idea is only to compute a gradient update if we have not had an update for a long time. \n\nEstablishing that the gradient is unbiased seems fairly straightforward: it just uses the fact that the gradient at the next step is a linear combination of the new function and the previous gradient. Analyzing the variance seems like some technical work. However I did not understand the conceptual improvement. \n\nCan the authors comment more about the intuition of their algorithm?\n\nAnother weakness is the experimental setting. In the experiments, the authors analyze a version of ridge regression. They divide by $i$ for every step which seems a bit unnatural to me. It would make more sense if the $i$ term was not there. But in that case, the problem becomes very easy. By using standard rank one update formulas, the *exact* optimum can be found in $O(nd^2)$ time. Perhaps the authors divided by $i$ to make the problem `artificially' harder? In any case, testing the algorithm on other natural problems which cannot be solved by other methods would be more convincing. The authors also do not list the parameters of the dataset and the $\\epsilon$ used in the algorithm."
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7163/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7163/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799750602,
            "cdate": 1698799750602,
            "tmdate": 1700698652953,
            "mdate": 1700698652953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IBjNY2rWy3",
                "forum": "RR70yWYenC",
                "replyto": "uiRMiCn04B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for their work and comments.\n\nUp next we provide the intuition behind our algorithm in $3$ main steps.\n\n- **Main Intuition of VR methods**\n\n  The main idea of VR methods is to compute a full-gradient at an anchor point $\\tilde{x}$, $\\tilde \\nabla := \\frac{1}{n}\\sum_{j=1}^n \\nabla f_j(\\tilde x)$ and use the following unbiased gradient estimator.  \n  $$\\nabla_t = \\nabla f_j(x_t)- \\nabla f_j(\\tilde x) + \\tilde \\nabla \\text{ with }j \\sim \\mathrm{Unif}(1,\\ldots,n)$$\n\n  The main idea behind this estimator is that *the closer $\\tilde{x}$ is to the optimal point $x^\\star$ the smaller the variance of $\\mathbb{E}\\left[||\\nabla_t - \\nabla g(x_t)||^2\\right]$* where $g(x) = \\sum_{j=1}^nf_j(x)/n$.\n\n  In order to understand the latter let $\\tilde{x} := x^\\star$. Initializing the above process for $x^0 = x^\\star$ results in $\\nabla_0= \\nabla g(x^\\star)$ meaning that the method stands still on the optimal point. On the contrary SGD even initialized at $x^\\star$ will move before converging back in general (e.g., without the strong growth setting).\n\n- **Straight-forward Approach in Instance-Optimal Finite-Sum Minimization**\n\n  In order to tackle Problem $2$, probably the most intuitive idea is the following. During stage $i \\in [n]$, set the anchor point $\\tilde x_i = \\hat x_{i-1}$ (the previous output of the method) and compute $\\tilde \\nabla_i = \\frac{1}{i}\\sum_{j=1}^i \\nabla f_j(\\tilde x_i)$. Then during stage $i \\in [n]$ perform SGD starting from the point $x^0_i := \\hat x_{i-1}$ and use the estimator \n  $$\\nabla_t^i = \\nabla f_j(x_t)- \\nabla f_j(\\tilde{x}) + \\tilde{\\nabla}_i \\text{ with }j \\sim \\mathrm{Unif}(1,\\ldots,n).$$\n\n  The intuition behind this approach is that for large values of $i$, $\\hat{x}_{i-1}$ is close to $x^\\star_i$ (optimal solution of stage $i$) which implies that the variance reduction mechanism that we described above is activated leading to accelerated rates.\n\n  To understand why $\\hat x_{i-1}$ is close to $x^\\star_i$, notice that $||x_{i-1}^\\star - x_{i}^\\star|| \\leq O(1/i)$ (see Lemma $6$) while we can inductively assume that $||\\hat x_{i-1} - x_{i-1}^\\star|| \\leq \\epsilon$.\n\n  The caveat of this approach is that computing $\\tilde \\nabla_i = \\frac{1}{i}\\sum_{j=1}^i \\nabla f_j(\\hat x_{i-1}) $ at each stage $i \\in [n]$ results in overall $O\\left(\\sum_{i=1}^n i\\right) = O(n^2)$ FOs."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978590614,
                "cdate": 1699978590614,
                "tmdate": 1699978590614,
                "mdate": 1699978590614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qrFnoayFjP",
                "forum": "RR70yWYenC",
                "replyto": "uiRMiCn04B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you to the others for their response. I would encourage them to add some of the intuition in any future version of the paper.\n\nThe new MNIST experiments are quite interesting. It is nice how the performance of SI-OPT is always an upper envelope to the performance of SGD in the ascending labels experiment.\n\nI read the review of reviewer Reviewer jqUd. It seems like they are quite knowledgeable about grading descent based algorithms. So I will momentarily hold off on any updates until they have a chance to respond about their proposed algorithm. I see how the exact stated algorithm does not work. But it is stated in quite informal terms, so maybe it can be easily modified. Let's give them a chance to respond."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700423805663,
                "cdate": 1700423805663,
                "tmdate": 1700423805663,
                "mdate": 1700423805663,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LugoUDpjba",
                "forum": "RR70yWYenC",
                "replyto": "o2CA1b60f5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_s2LS"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Thanks, I will update my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698629534,
                "cdate": 1700698629534,
                "tmdate": 1700698629534,
                "mdate": 1700698629534,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qtbBtNJgaf",
            "forum": "RR70yWYenC",
            "replyto": "RR70yWYenC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7163/Reviewer_jqUd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7163/Reviewer_jqUd"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the following problem: given $n$ functions, the goal is to find $n$ points such that the $i$-th point minimizes the average of the first $i$ functions. For this problem, the paper presents a lower bound on the number of iterations for a certain class of algorithms, called natural first-order methods. They present an algorithm that achieves first-order oracle complexity which is close to the lower bound."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "A clearly written paper with sound results."
                },
                "weaknesses": {
                    "value": "I have the following concerns about the paper:\n\n-- I can\u2019t connect Problem 2 with its motivation. For example, you say that \u201cit is important that a model is constantly updated so as to perform equally well both on the past and the new data\u201d, but:\n1) Problem 1 achieves precisely that;\n2) in Problem 2, you train *multiple* models, with later models performing well on all data, and with older models not taking into account new data at all.\n\nTo conclude, I don\u2019t see a motivation for the problem.\n\n-- You give lower bounds only for natural algorithms. If you don\u2019t restrict yourself to natural algorithms (and it\u2019s not clear to me why you consider only natural algorithms), I believe that $O(\\frac{\\log n}{\\epsilon})$ bound is possible, which outperforms all suggested methods. The idea is to run the following simple algorithm:\n\nfind $\\epsilon$-minimizer $x_n$ of $g_n$ using SGD\nfor i = n-1, n-2, \u2026, 1:\n\tstarting from $x_{i+1}$, find $\\epsilon$-minimizer $x_i$ of $g_i$ using SGD\n\nI didn\u2019t check the math in detail, but I believe that the proof goes like this:\n1) If $x_{i+1}$ is $\\epsilon$-minimizer of $g_{i+1}$, then $x_{i+1}$ is $(1 + 1/i) \\epsilon$-minimizer of $g_i$.\n2) The number of iterations required to find $x_i$ starting from $x_{i+1}$ is $O(\\frac{\\log (1 + 1/i)}{\\epsilon})$\n3) Summing this over all $i$, we get $O(\\frac{\\log n}{\\epsilon})$ iterations\n\n-- Experiments are only conducted in simple regression settings. Dataset statistics (e.g. the number of points) are not shown. It is unclear how exactly you execute the baselines."
                },
                "questions": {
                    "value": "Minor issues:\n\n-- Commas are missing in many places\n\n-- You mention $O(n \\log \\frac{1}{\\epsilon})$ complexity for VR methods, but isn\u2019t the same bound achievable by trivial deterministic gradient descent, by sampling all $n$ functions at every iteration?\n\n-- Page 2: \u201cfor Problem 1 using $O(1/\\epsilon)$.\u201d - \u201cFOs\u201d is missing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7163/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699315148655,
            "cdate": 1699315148655,
            "tmdate": 1699636848847,
            "mdate": 1699636848847,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m3xDyTV92g",
                "forum": "RR70yWYenC",
                "replyto": "qtbBtNJgaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the work and the valuable comments.\n\n- *\"I can\u2019t connect Problem 2 with its motivation. For example ... past and the new data\u201d*\n\n  We believe that some confusion might have been created due to the use of $n$ in both for Problem $1$ and Problem $2$. In many cases  the *overall $n$ data points are not available* from the beginning and are rather gathered in an online fashion as the system is deployed to the public.\n\n  We agree with the reviewer that if we had access to all $n$ data points from the beginning, we could directly solve Problem $1$. However our work is motivated from settings where only a small fraction of $n_1$ data points is available in the beginning and most $n-n_1$ data points are gradually revealed (for simplicity we set $n_1 =1$). To avoid confusion, we will use the notation $n_1$ to denote the initial data corresponding to Problem $1$.\n\n  To further illustrate the motivation of our work in the next item we provide a concrete example in the context of continual learning.\n\n\n- *Concerning the motivation of our work*\n\n  Efficiently updating a model upon the arrival of *\"fresh data''* is a fundamental problem in ML. For example in *continual learning* models are initially trained with an initial set of data and then they are constantly updated so as to incorporate new tasks/data that arrive online. To clarify things let us provide a continual learning point of view with respect to the MNIST data set.\n\n  Let us assume that we are initially given $n_1$ data points from MNIST with only $0$ and $1$ labels. The initial goal is to train a model with parameters $x$ to correctly classify $0/1$ digits. As a result, one needs to solve the following finite-sum problem (Problem $1$) \n\n  $$\\min_x \\sum_{j=1}^{n_1} f_j(x)/{n_1}~~\\text{where }f_j(\\cdot) \\text { captures the respective loss of data point j.} $$\n\n  After training the model (solving Problem $1$), an additional data point arrives with label $2$ (additional task). Our goal now is to update $x$ so that our model correctly classifies the $2$-example while preserving its classification ability for $0/1$'s. Thus we need to solve the following similar but different finite-sum problem \n  $$\\min_x \\sum_{j=1}^{n_1+1} f_j(x)/(n_1+1).$$\n\n  Now imagine the above process to be repeated for a stream of $n_2$ data points of label $2$. Namely, after each model-update a new $2$-data point arrives that then needs to be incorporated in the model. As a result, for each $i=1,\\ldots,n_2$, we need to compute $ \\hat x_i$ such that \n  $$ \\sum_{j=1}^{n_1 + i} f_j(\\hat x_i)/(n_1+i) -   \\min_x \\sum_{j=1}^{n_1+i} f_j(x)/(n_1+i) \\leq \\epsilon.$$\n  The latter can continue for streams of $n_3,n_4,\\ldots,n_9$ data points (corresponding to the respective labels). However resolving Problem $1$ from stratch (even with a state-of-the art VR method) every time a new data point arrives leads to a huge computational burden. That is the reason Problem $2$ asks for *sequence of approximate solutions* with mimimum number of overall FO calls.\n\n  We remark that for the clarity of presentation, in Problem $2$ we set $n_1 = 1$ however our framework is directly extendable for general $n_1$. Moreover for the clarity of presentation we also focus on the case of a new data point arriving at each stage $i$, however our setting can be directly extended to the case where several data points arrive at each stage.\n\n- *Concerning the analysis of the algorithm that you propose*\n\n   1. *If $x_{i+1}$ is $\\epsilon$-minimizer of $g_{i+1}$ then it is an $(1 + 1/i)\\epsilon$-minimizer of $g_i$.*\n       \n       We must be missing some hidden assumptions in your statement as the above statement is simply not true. Let $\\epsilon =0$ then the above statement says that a minimizer of $g_{i+1}(x)$ is also a minimizer of $g_i(x)$. That is clearly not the case: e.g., if you solve the very first data term to full accuracy, then you do not need to care about the incoming data in the future. \n\n   2. *The number of iterations required to find $x_i$ starting from $x_{i+1}$ is $O\\left(\\frac{ \\log (1 + 1/i)}{\\epsilon}\\right)$.*\n \n       We do not see how the above expression is derived. Please clarify if you make some assumptions in order to reach these conclusions.\n\n       We remark that even GD initialized at $x_0$ requires $O(\\log ( ||x_0 - x^\\star||/\\epsilon))$ iterations, with each iteration requiring $i$ FOs.\n\n       We additionally remark that SGD does not admit such *warm-start* guarantees (even outside log), $O( (f(x_0) - f(x^\\star))/\\epsilon)$ in general. The reason is that such guarantees would imply that SGD initialized at $x_0 := x^\\star$ would stand still with probability $1$ that is clearly not the case."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699978303992,
                "cdate": 1699978303992,
                "tmdate": 1699978303992,
                "mdate": 1699978303992,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FgvFQ0hJQw",
                "forum": "RR70yWYenC",
                "replyto": "qtbBtNJgaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion about Neural Network Experiments"
                    },
                    "comment": {
                        "value": "In order to facilitate further discussion we attach some new plots from Appendix K of the revised version of the manuscript that display the performance of SIOPT on training 2-layer Neural Networks.\n\nOn the first plot we display the performance of our method on data streams produced by stationary distributions. As we see in the ridge regression case our method achieves performance similar to SVRG with much fewer FOs.\n\n [Stationary Distributions](https://imgur.com/a/h9TZmFu)\n\nOn the rest of the plots we present the performance of our method on data streams with ascending labels, a problem setting motivated by continual learning. In this problem we initially wish to classify two labels (0s and 1s on MNIST) and then as time progress new classes are introduced to the dataset (digits 3, 4, 5, etc), this setting is similar to the one we discussed above as motivation for our work. SIOPT here achieves higher performance than SGD and adapts to the new tasks faster.\n\n[Ascending Labels](https://imgur.com/a/BWy8TVR)\n\n[Ascending Labels Train Accuracy](https://imgur.com/a/HEJKxlv)\n\n[Ascending Labels Test Accuracy](https://imgur.com/a/YbSVEno)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055416884,
                "cdate": 1700055416884,
                "tmdate": 1700063218626,
                "mdate": 1700063218626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v2XSPHvGkU",
                "forum": "RR70yWYenC",
                "replyto": "qtbBtNJgaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Authors"
                ],
                "content": {
                    "title": {
                        "value": "End of Discussion"
                    },
                    "comment": {
                        "value": "As the discussion period is concluding tomorrow, we would like to ask the reviewer if our replies have covered all of your concerns about our work. \n\nWe remain at your disposal for any questions."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587243892,
                "cdate": 1700587243892,
                "tmdate": 1700587243892,
                "mdate": 1700587243892,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0HnNiGgOeY",
                "forum": "RR70yWYenC",
                "replyto": "v2XSPHvGkU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_jqUd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7163/Reviewer_jqUd"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response, I appreciate the feedback and would like to keep the original score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7163/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693294213,
                "cdate": 1700693294213,
                "tmdate": 1700693294213,
                "mdate": 1700693294213,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]