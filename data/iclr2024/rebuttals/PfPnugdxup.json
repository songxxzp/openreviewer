[
    {
        "title": "From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction"
    },
    {
        "review": {
            "id": "IQbAyXa8J1",
            "forum": "PfPnugdxup",
            "replyto": "PfPnugdxup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8604/Reviewer_avot"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8604/Reviewer_avot"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a supervised pre-training strategy named JMP, which pretrains GemNet-OC on various small molecule data from multiple chemical domains. The pre-trained network acts as a foundational model, further finetuned for downstream atomic property prediction tasks. The authors\u2019 major contribution is demonstrating that by pretraining exclusively on small molecules, the network can be finetuned on large molecule datasets, achieving state-of-the-art results. The primary technical advancements concentrate on refining each component of the standard deep learning pipeline, including data preprocessing and hyperparameter tuning. Experiments across 40 fine-tuning benchmarks are conducted to showcase the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The paper is articulate and easily comprehensible. The authors provide an essential level of detail in describing their pipeline, facilitating a clear understanding of the processes. Although source code is not provided, the comprehensive details included in the paper should enable straightforward reimplementation.\n\n+ The experiments presented in the paper are exhaustive and meticulous. A diverse array of molecular property datasets, encompassing both large and small molecules, has been utilized for the experiments. The authors have conducted extensive ablation studies, offering valuable insights into the influence of various hyperparameters used in the pipeline."
                },
                "weaknesses": {
                    "value": "- My primary concern lies in the paper\u2019s technical contribution. The concept of building a foundational model for molecular property prediction tasks isn\u2019t novel. A significant challenge is bridging the substantial domain gaps across various chemistry domains. The authors seem to emphasize that the proposed pipeline can effectively bridge the molecule size gap, allowing it to work efficiently on larger molecules even when only pretrained on smaller ones. However, molecule size is just one of several apparent factors\u2014and likely among the simpler ones\u2014causing the domain gap. More complex factors, such as intrinsic differences in the distribution of graph structures and issues related to data availability, are not addressed in the paper. Consequently, it is challenging to be convinced that the proposed method significantly contributes to foundational models or represents \"an important step for universal ML potential,\" as claimed in the introduction.\n\n- The improvement brought by the proposed method appears to be mainly attributed to hyperparameter tuning. The network architecture itself isn\u2019t novel, and the loss function closely resembles commonly used ones, albeit with slight modifications to some coefficients. My overarching impression is that the authors engage extensively in manual hyperparameter tuning, which doesn\u2019t offer substantial insights to propel further research advancements. While I acknowledge the empirical enhancements demonstrated through comprehensive experiments, it is still challenging to bestow a favorable overall evaluation on the paper."
                },
                "questions": {
                    "value": "1. In Sect. 4.1, concerning Data Normalization, the authors have chosen to normalize the property values per dataset. A lingering question is how the output of the NN is transformed. Is the transformation still dependent on each specific dataset? If that is the case, it seems impractical for real world applications where a novel molecule is given, and it would be indeterminable as to which \"dataset\" it inherently belongs to and how to transform its output.\n\n2. Regarding Dataset Size Imbalance, I was wondering if the authors considered utilizing loss reweighting as opposed to data reweighting. By loss reweighting, I am referring to the approach of uniformly sampling the data but adjusting the coefficients of each sample to p_d (ensuring normalization across each batch).\n\n3. I devoted a significant amount of time attempting to digest whether each term in Eq.1 is a novel contribution or a previously introduced one. It would be beneficial if the authors could provide clearer definitions of each symbol used, elaborate more distinctly on the novel improvements introduced in this paper, and add a period to the end of the equation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8604/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8604/Reviewer_avot",
                        "ICLR.cc/2024/Conference/Submission8604/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698365510698,
            "cdate": 1698365510698,
            "tmdate": 1700689466582,
            "mdate": 1700689466582,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aS9L3Sc6Zy",
                "forum": "PfPnugdxup",
                "replyto": "IQbAyXa8J1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer avot: 1/4"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and effort in reviewing our paper. We have addressed the reviewer's comments below.\n\n> My primary concern lies in the paper's technical contribution. The concept of building a foundational model for molecular property prediction tasks isn't novel.  [...] Consequently, it is challenging to be convinced that the proposed method significantly contributes to foundational models or represents \"an important step for universal ML potential,\" as claimed in the introduction.\n\nWe refer the reviewer to our response to the general response for an in-depth discussion on novelty. Our model is pre-trained on diverse data from the small molecule and catalysis domains. This diversity presents major challenges in pre-training, and we address these challenges through our proposed pre-training pipeline. This formulation is what enables our model to generalize across a diverse set of downstream tasks and chemical domains. We believe that this is a significant contribution to the community.\n\n> A significant challenge is bridging the substantial domain gaps across various chemistry domains. The authors seem to emphasize that the proposed pipeline can effectively bridge the molecule size gap, allowing it to work efficiently on larger molecules even when only pretrained on smaller ones. However, molecule size is just one of several apparent factors\u2014and likely among the simpler ones\u2014causing the domain gap. More complex factors, such as intrinsic differences in the distribution of graph structures and issues related to data availability, are not addressed in the paper.\n\nWe are unsure what the reviewer means by \"intrinsic differences in the distribution of graph structures\". Our pre-training and fine-tuning datasets contain extremely diverse graph structures, including small molecules, large molecules, materials, and catalyst systems. This diversity covers much more than just the molecule size gap. Please see Table 1 for more detailed information on the datasets and Figure 1 (left and middle) for visualizations of sample systems from each dataset. As can be seen, these systems are extremely diverse in terms of atom count, atom types, graph structure, and chemical domain. If the reviewer believes that there are other factors that we have not considered, we would be happy to discuss further.\n\nRegarding the issue of data availability, our state-of-the-art results on low-resource datasets (e.g., rMD17, MatBench's JDFT2D, etc.) demonstrate that the pre-trained model is able to generalize to low-resource datasets, decreasing the need for large amounts of data. We believe that this is a significant contribution to the community."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238198753,
                "cdate": 1700238198753,
                "tmdate": 1700238198753,
                "mdate": 1700238198753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2OlNoQiqcb",
                "forum": "PfPnugdxup",
                "replyto": "cnApw0fawd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8604/Reviewer_avot"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8604/Reviewer_avot"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the authors' rebuttal. I appreciate the thorough clarification and the addition of new experimental results. My apologies for not responding sooner. I now recognize that the method is not as reliant on hyperparameter tuning as I initially thought, and I acknowledge its practical contribution in providing a model with decent performance. So I have revised my score from 3 to 5.\n\nHowever, I maintain a slightly negative stance due to my concerns about the novelty of the paper. Despite understanding the authors' general response and the discussions with other reviewers, the technical novelty seems somewhat limited. The use of reweighting and data normalization appears to be more aligned with engineering tricks rather than substantial technical innovations that can offer deep insights or potential inspiration for future research. While assessing novelty can be subjective, I believe that for a paper to qualify for ICLR, the novelty of the method should be a significant consideration, surpassing mere engineering or experimental performance."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689451480,
                "cdate": 1700689451480,
                "tmdate": 1700689451480,
                "mdate": 1700689451480,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cRynoQ0OEe",
            "forum": "PfPnugdxup",
            "replyto": "PfPnugdxup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8604/Reviewer_R7em"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8604/Reviewer_R7em"
            ],
            "content": {
                "summary": {
                    "value": "Authors explore the application of machine learning in predicting atomic properties across a wide array of applications, from healthcare to climate change. The authors introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training strategy that leverages a vast dataset comprising approximately 120 million examples from multiple chemical domains. The primary goal of JMP is to generate transferable atomic representations that can be fine-tuned for diverse downstream tasks, addressing the challenge of generalizing across the extensive and complex space of molecular interactions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Innovative Approach: The paper introduces Joint Multi-domain Pre-training (JMP), a novel supervised pre-training strategy that leverages a massive dataset from multiple chemical domains. This approach is innovative in its attempt to generate transferable atomic representations for a wide array of downstream tasks, addressing the challenge of generalizing across diverse molecular interactions.\n\nCreative Combination of Ideas: The authors draw inspiration from successful practices in Natural Language Processing (NLP) and Computer Vision (CV), creatively applying the concept of large-scale pre-training to the domain of atomic property prediction. This cross-disciplinary innovation enhances the originality of the work.\n\nBroad Applicability: The paper\u2019s contributions have broad applicability across various domains, ranging from drug discovery to material science. The ability of JMP to generalize across diverse chemical domains signifies its potential to drive advancements in multiple fields.\nAddressing a Critical Challenge: The paper tackles the critical challenge of generating transferable atomic representations in the vast and complex space of molecular interactions. By addressing this challenge, the paper makes a significant contribution to the field of machine learning for atomic modeling.\n\nComputational Efficiency: The computational efficiency achieved through JMP, with over 12x faster fine-tuning compared to training from scratch, is a notable strength. This efficiency is crucial for practical applications, making the paper\u2019s contributions highly significant."
                },
                "weaknesses": {
                    "value": "Need for Broader Ablation Studies:\nIssue: While the paper includes ablation studies to analyze the impact of different JMP components, these studies could be broadened to provide a more comprehensive understanding of the model\u2019s behavior and the contributions of individual components."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8604/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8604/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8604/Reviewer_R7em"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698783229215,
            "cdate": 1698783229215,
            "tmdate": 1699637076300,
            "mdate": 1699637076300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vOQQlU23qX",
                "forum": "PfPnugdxup",
                "replyto": "cRynoQ0OEe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R7em: 1/1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and effort in reviewing our paper. Specifically, we are glad that the reviewer found our work to be innovative, creative, and broadly applicable.\n\n> Need for Broader Ablation Studies: Issue: While the paper includes ablation studies to analyze the impact of different JMP components, these studies could be broadened to provide a more comprehensive understanding of the model\u2019s behavior and the contributions of individual components.\n\nIn our current manuscript (see section 5.1), we have conducted the necessary ablations to analyze the impacts of the core components of our pre-training technique. We also refer the reviewer to Appendix B, where we have conducted a number of additional ablations.\n\nWith that said, additional experiments, such as a \"scaling law\" analysis of model size and pre-training dataset size, would be very interesting and informative. However, due to the high computational cost of our experiments, we were unable to perform these additional experiments. In particular, our JMP-L model took 34400 GPU hours to train, and the fine-tuning experiments took 4600 GPU hours (for all 72 fine-tuning datasets). We have updated the conclusion in our manuscript to reflect this limitation and have added a note in the conclusion section that a more comprehensive ablation study is left for future work.\n\nRegarding model-architecture-level analysis, we heavily leverage the findings of the original GemNet and GemNet-OC works (see [Gasteiger et al., 2022](https://arxiv.org/abs/2106.08903) and [Gasteiger et al., 2022 (b)](https://arxiv.org/abs/2204.02782)), which do a very thorough job of exploring the impact of different components of the underlying model GemNet architecture across different datasets. See our response to reviewer `XFKt` for further discussion on our backbone architecture choice."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238161411,
                "cdate": 1700238161411,
                "tmdate": 1700238161411,
                "mdate": 1700238161411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jUQFp31fvi",
                "forum": "PfPnugdxup",
                "replyto": "vOQQlU23qX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8604/Reviewer_R7em"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8604/Reviewer_R7em"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the work. I have no further questions."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583290163,
                "cdate": 1700583290163,
                "tmdate": 1700583290163,
                "mdate": 1700583290163,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "87ZnUEQ6eL",
            "forum": "PfPnugdxup",
            "replyto": "PfPnugdxup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8604/Reviewer_KadD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8604/Reviewer_KadD"
            ],
            "content": {
                "summary": {
                    "value": "This paper pretrains a large generalizable model for atomic property prediction, which outperforms SOTA methods on many downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The model achieves exceptional performance.\n2. The experiments conducted in this study are comprehensive and thorough, covering various aspects such as hyper-parameter settings, ablation studies, and downstream tasks.\n3. The authors carefully study the balance between different datasets and pre-training tasks, ensuring a comprehensive analysis of their impact."
                },
                "weaknesses": {
                    "value": "1. The disscussion of the correlation between pre-training tasks and downstream tasks is missing. I am wondering what kinds of downstream tasks can be promoting by pre-training? \n2. The main contribution of this work is implemental and not suprising.\n3. This paper confuses the prediction of atomic properties with the prediction of molecular properties."
                },
                "questions": {
                    "value": "1. Does the use of JMP contribute to the prediction of molecular properties? If so, what specific types of molecular properties show improvements?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698910086797,
            "cdate": 1698910086797,
            "tmdate": 1699637076124,
            "mdate": 1699637076124,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nqtW6edSFG",
                "forum": "PfPnugdxup",
                "replyto": "87ZnUEQ6eL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KadD: 1/1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive comments, and we are glad of the recognition of our model's \"exceptional performance\" and that they found our work to be \"comprehensive and thorough\". We address each of the reviewer's comments below.\n\n> The disscussion of the correlation between pre-training tasks and downstream tasks is missing.\n\nIn Section 3 (Datasets), we discuss the differences between the pre-training and fine-tuning datasets in two different aspects:\n1. We identify the underlying chemical domain of the pre-training and fine-tuning datasets and show that our fine-tuning datasets span a wide range of chemical domains, including ones that are out-of-domain for the pre-training datasets.\n2. Our pre-training datasets all contain energy and force labels for force-field modeling. Our fine-tuning datasets, on the other hand, contain a diverse mix of labels, including atomization energy, polarizability, electron affinity, formation energy, and band gap, in addition to energy and force labels (for the DFT and MD datasets). We refer to these other labels as \"out-of-domain labels\" in the manuscript.\n\nWe have also updated the introduction of our manuscript to further clarify the differences amongst the chemical domains that we pre-train and fine-tune our model on.\n\n> I am wondering what kinds of downstream tasks can be promoting by pre-training?\n\nAs we show in Figure 2 (a), when compared to training from scratch, pre-training helps with all of the downstream tasks we consider, including all in-domain (ID) and out-of-domain (OOD) labels, as well as all ID and OOD chemical domains. However, if there are specific tasks you are interested in or if we have misunderstood your question, could you please provide more details or clarify? We would be happy to provide a more targeted response.\n\n\n> The main contribution of this work is implemental and not suprising.\n\nWe refer the reviewer to our response to the general response for further discussion on novelty. On the results being \"not surprising\": We believe that our works makes a significant contribution to the community, but we are happy to get more insights on additional experiments required to help demonstrate the efficacy of this approach.\n\n> This paper confuses the prediction of atomic properties with the prediction of molecular properties.\n\nIf the reviewer is referring to the difference between atom-level (node-level) and system-level\\*\\*\\* (graph-level) properties, we would like to point out that our set of downstream tasks cover atomic properties (e.g., atom-level forces) and molecular properties (e.g., dipole moment in QM9). Our pre-training objective also involves atomic properties (forces) and molecular properties (energy). Otherwise, we would like to ask the reviewer to further clarify their comment regarding the confusion between atomic and molecular properties.\n\n\\*\\*\\* We use the term \"system\" or \"graph\" as opposed to \"molecule\" because some of our datasets are not just molecules, but also materials (e.g., MatBench), catalyst systems (e.g., OC20), among others. We have clarified this in the manuscript.\n\n> Does the use of JMP contribute to the prediction of molecular properties? If so, what specific types of molecular properties show improvements?\n\nPlease see our response to the previous comment regarding molecular vs. atomic properties. Regarding observed improvements, as mentioned previously (and as shown in Figure 2 (a)), pre-training helps with all of the downstream tasks we consider compared to training from scratch, including atom-level and molecule-level properties."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700238105002,
                "cdate": 1700238105002,
                "tmdate": 1700238105002,
                "mdate": 1700238105002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zjfh4fFCFL",
            "forum": "PfPnugdxup",
            "replyto": "PfPnugdxup",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8604/Reviewer_XFKt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8604/Reviewer_XFKt"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the possibility of pre-training a foundation-style model over multiple chemical domains to generate transferable atomic representations for downstream fine-tuning tasks. The Joint Multi-domain Pre-training (JMP) strategy utilizes data from multiple chemical domains and achieves state-of-the-art results across many targets of various datasets. The paper establishes a comprehensive set of fine-tuning benchmarks across various chemical domains and tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Comprehensive set of fine-tuning benchmarks: The paper establishes a comprehensive set of fine-tuning benchmarks across various chemical domains and tasks, which enables researchers to evaluate the performance of their models against a standardized set of benchmarks.\n\n- State-of-the-art results: The Joint Multi-domain Pre-training (JMP) strategy achieves state-of-the-art results across many targets of various datasets, which demonstrates the effectiveness of the proposed approach.\n\n- Large and diverse molecular datasets: The paper highlights the importance of large and diverse molecular datasets in enabling the development of accurate and efficient models for atomic property prediction.\n\n- Different model sizes: The paper provides multiple model sizes with pretrained checkpoints that can benefit real-world deployment at different resource levels and potentially accelerate research progress in related fields."
                },
                "weaknesses": {
                    "value": "- The paper claims that the proposed pre-training method is model-agnostic. However, the only evaluated architecture backbone is GemNet-OC. It would be better to have a variant pre-trained using other types of model backbones to conduct further comparison and analysis.\n\n- The novelty is a bit limited. I admit that this paper contributes on providing the empirical evidence that pre-training cross-domain molecule data can benefit multiple downstream tasks. However, the techniques used in this paper are either introduced by other literature or very simple and straightforward. No novel methods/theories are introduced. I would recommend this paper submit to more domain-related or comprehensive journals. \n\n- This paper claims that many previous efforts on pretraining molecules focus on a specific domain which ignores the information provided by other domains and generalizability. It would be better to provide more empirical evidence that compares the proposed model with other pre-training methods [1].\n\n- More supervised SOTA should be compared. E.g., for the materials domain, there are two more recent papers [2, 3] that can be reported against the proposed method.\n\n- I understand the concerns of releasing the code and models before acceptance. However, in terms of reproducibility, it would be better to provide an anonymized repo including some demo models for testing purpose.\n\n[1] Xia, Jun, et al. \"A Systematic Survey of Chemical Pre-trained Models.\" IJCAI, 2023.\n\n[2] Yan, Keqiang, et al. \"Periodic graph transformers for crystal material property prediction.\" Advances in Neural Information Processing Systems 35 (2022): 15066-15080.\n\n[3] Lin, Yuchao, et al. \"Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction.\" arXiv preprint arXiv:2306.10045 (2023)."
                },
                "questions": {
                    "value": "- Can you provide more details about the scaling strategy of the model architecture? The message-passing paradigm suffers from over-smoothing a lot and it is notorious of hard to make the network deep. I would like to understand more about this and how this method overcome the issues.\n\n- Since for larger foundation models, we would like to include far more parameter. But the molecular graphs often just include a very small number of vocabularies. A better strategy could be adding full attention mechanism to make the parameter space larger. And the model would be a transformer-based architecture or so-called \"graph transformer\". Did the authors try this in their experiments? How did the two frameworks perform?\n\n- This paper claims that they can deal with OOD challenge and cross-domain adaption better and they can benefit drug discovery, etc. So I would like to see more results on how this method perform on other therapeutics data [4], e.g., the molecule property predictions (toxicity, solubility, lipophilicity, etc.) on MoleculeNet [5].\n\n[4] Huang, K., Fu, T., Gao, W. et al. Artificial intelligence foundation for therapeutic science. Nat Chem Biol 18, 1033\u20131036 (2022). https://doi.org/10.1038/s41589-022-01131-2\n\n[5] Wu, Zhenqin, et al. \"MoleculeNet: a benchmark for molecular machine learning.\" Chemical science 9.2 (2018): 513-530."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699127595710,
            "cdate": 1699127595710,
            "tmdate": 1699637075979,
            "mdate": 1699637075979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GSdEoB71zq",
                "forum": "PfPnugdxup",
                "replyto": "zjfh4fFCFL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8604/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful feedback and highlighting our comprehensive benchmarks with state-of-the-art results, our choice of large and diverse pre-training dataset, and our experiments with model sizes. We also appreciate the questions and concerns raised which we address below.\n\n> The paper claims that the proposed pre-training method is model-agnostic. However, the only evaluated architecture backbone is GemNet-OC. It would be better to have a variant pre-trained using other types of model backbones to conduct further comparison and analysis.\n\nThe primary goal of this work was to explore whether or not it is possible to learn robust and generalizable representations that work across many domains, which we were able to demonstrate for GemNet. When this work began we chose GemNet as the base architecture for a number of reasons including: (1) its application (or that of its predecessor, DimeNet) and performance across a diverse set of chemical domains and  downstream tasks, such as on OC20, OC22, QM9, MD17, MatBench, PDBBind (2) its performance on large diverse datasets such as OC20 and OC22 [[Gasteiger et al., 2022](https://arxiv.org/abs/2106.08903), [Tran et al., 2023](https://arxiv.org/abs/2206.08917)] (3) its ability to improve with a large number of parameters [[Sriram et al., 2022]](https://arxiv.org/abs/2203.09697), and (4) it is a reasonably efficient model in terms of training time. Even with (4) pre-training our largest model required 34400 GPU hours and 4600 GPU hours for fine-tuning models across all 72 fine-tuning datasets (which contains 40 unique tasks, and MatBench tasks having 5 folds). Given these compute numbers it was not feasible for us to include another model.\n\n> The novelty is a bit limited. I admit that this paper contributes on providing the empirical evidence that pre-training cross-domain molecule data can benefit multiple downstream tasks. However, the techniques used in this paper are either introduced by other literature or very simple and straightforward. No novel methods/theories are introduced.\n\nWe are the first to demonstrate that representations can indeed generalize when pre-trained across a wide range of atomic domains. These domains vary from molecules to materials, have a large variance in the number of atoms, and even use different levels of theory to perform their calculations. The novelty lies in framing a supervised pre-training framework for these diverse datasets and is bolstered by comprehensive benchmarks that leads a way to future work in this field. We refer the reviewer to our response to the general response for further discussion on novelty."
                    },
                    "title": {
                        "value": "Response to Reviewer XFKt: 1/5"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237528333,
                "cdate": 1700237528333,
                "tmdate": 1700237641235,
                "mdate": 1700237641235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]