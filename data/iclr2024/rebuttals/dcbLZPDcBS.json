[
    {
        "title": "Efficient Graph Representation Learning by Non-Local Information Exchange"
    },
    {
        "review": {
            "id": "LioLQ2mC7Y",
            "forum": "dcbLZPDcBS",
            "replyto": "dcbLZPDcBS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6116/Reviewer_TumZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6116/Reviewer_TumZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"express messenger\" a model-agnostic plug-in that re-wires the learning graph such that new connections between distant nodes are formed. The aim behind this approach is to achieve better non-local information exchange. The authors claim that their module reduces/eliminates the feature over-smoothing issue. They also integrate the plug-in with existing GNNs and demostrate its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Figure 1 clearly illustrates the idea discussed in this work and aids in the visualization of the proposed methodology\n\nThe experiments that are designed to show the effectiveness of the express messenger are thorough"
                },
                "weaknesses": {
                    "value": "(Table 2) Many of the numbers reported for baselines and the proposed method are within the margin of error of each other. This detracts from the effectiveness of the proposed methodology."
                },
                "questions": {
                    "value": "(Table 3) Going by the pattern of improvement in numbers when the ExM plug-in is included; would the authors be comfortable in agreeing to this?: If the delta of performance between GNN and GNN + ExM is plotted against h (x-axis), it would be an elbow curve with the delta approaching zero as h -> 1 and the elbow being somewhere around h = 0.2. Hope this is clear!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713845137,
            "cdate": 1698713845137,
            "tmdate": 1699636661612,
            "mdate": 1699636661612,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5dvRoEjLxC",
                "forum": "dcbLZPDcBS",
                "replyto": "LioLQ2mC7Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness:** We have performed two-sample t-test to examine whether the improvement is statistically significant, while compared to other graph rewiring methods DropEdge, GDC, and SDRF are not as significant as ours in Table 2. This can show the effectiveness of our proposed graph rewiring method.\n\n**Question:** Based on datasets we are using, we partially agree with this reviewer regarding the pattern of learning performance w.r.t. h. Since the number of heterophilous graph dataset (with small h ratio) is much less than homo graph (with big h ratio) in graph learning field, Such observation might be biased by the unbalanced datasets. \n\nAnother explanation for this pattern could be: Since smaller h ratio leads to far-away nodes bear the same class label, non-local information can promote message aggregations between non-local neighbors to the extent that far-away nodes are instantly connected via the express links. Please check the evidence on our new Fig 2, where the over-smoothness issue has been greatly mitigated by using non-local information."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013803380,
                "cdate": 1700013803380,
                "tmdate": 1700013803380,
                "mdate": 1700013803380,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "v0kVunKA5t",
            "forum": "dcbLZPDcBS",
            "replyto": "dcbLZPDcBS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6116/Reviewer_YKr6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6116/Reviewer_YKr6"
            ],
            "content": {
                "summary": {
                    "value": "In this submission, the authors propose a new method for GNN learning that focuses on addressing the challenge of over-smoothing in feature representation. Incorporating a novel mechanism called Non-Local Information Exchange (NLE), enhances the ability of GNNs to combine local and global information effectively. The main contribution is the ExM wrapper, which can be integrated with various GNN models to maintain state-of-the-art performance across different graph datasets, particularly improving performance on heterophilous graphs."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a new idea to tackle the over-smoothing issue.\n\n2. The experimental results seem interesting. Specifically, the ExM wrapper developed aids most baseline graph neural network (GNN) methods in retaining state-of-the-art (SOTA) performance across various graph datasets with diverse homophily ratios. It is highlighted that the C-ExMP variant of the ExM wrapper often outperforms its counterparts, securing top-3 rankings in multiple datasets\u200b."
                },
                "weaknesses": {
                    "value": "1. The presentation has a large space for improvement. The reviewer does not think the proof makes sense.\n\n2. The theoretical justification of the proposed method is weak. It is unclear why the proposed wrapper can be applied to general models.\n\n3. Some claims in this paper are too strong. For example, the paper mentioned the expressiveness of GNN has not been explored. However, there are many papers focusing this area including the papers cited in this submission."
                },
                "questions": {
                    "value": "Overall, the reviewer thinks the submission is not ready for publication. There are presentation issues and the methodology needs a theoretical justification.\n\nQ1. The proof of Proposition is so unclear, why the matrix operations can be linked with sets(these neighbors)?\n\nThere are grammar issues including:\n\n1. State-of-the-arts -> state-of-the-art\n2. far-reach neighborhoods -> far-reaching\n3. over-smoothing issue -> issues."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698771600181,
            "cdate": 1698771600181,
            "tmdate": 1699636661466,
            "mdate": 1699636661466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6CjspBNfWL",
                "forum": "dcbLZPDcBS",
                "replyto": "v0kVunKA5t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness 1:** The math and the proposition show why the convergence of our graph re-wiring method is 1-A. It\u2019s for edge modification, not for graph feature computation. \n\n**Weakness 2:** The proposed wrapper is a graph re-wiring method. It is not a part of graph computation but the data augmentation by modifying graph. Thus, it can be plugged into any GNN by just inputting the modified graph data.\n\n**Weakness 3:** We didn\u2019t mention that \u201cthe expressiveness of GNN has not been explored\u201d. Our claim is \u201cthe expressibility of graph data has not been explored\u201d. \n\n**Question 1:** The proposition shows why hop(A, k) is converging to 1-A. Since we treat A as a binary matrix, we used the set operation for those binary elements. It\u2019s indeed confusing reader that matrix cannot play with set operation. We revised the equation of the proposition in the manuscript by using logic operation: $1+1 = 0$, and $0+1 = 1$. Then\n$$\n1-A = \\mathcal N^K-\\mathcal N^{K-1}+\u2026+\\mathcal N^1-\\mathcal N^0 = \\sum hop(A, K) ,\twhere\\ hop(A,k)=(\\mathcal N^k-\\mathcal N^{k-1}) \\in\\{0,1\\}\n$$\n\nThank you for pointing out grammar issues, we have revised them."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013657786,
                "cdate": 1700013657786,
                "tmdate": 1700013657786,
                "mdate": 1700013657786,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Pxm9PzIPoh",
            "forum": "dcbLZPDcBS",
            "replyto": "dcbLZPDcBS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6116/Reviewer_JpoC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6116/Reviewer_JpoC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel framework to enhance the expressive power of GNNs and mitigate the over-smoothing in deep GNNs. The authors introduce an innovative non-local information exchange mechanism inspired by non-local mean techniques from image processing. This mechanism directly connects distant nodes, bypassing the traditional sequential propagation of information. Two express messenger wrapper are proposed to rewire the connection. The two wrapper allows to capture global representations thus free of over-smoothing. Extensive experiments are conducted to validate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The commitment to addressing the over-smoothing issue and improve expressiveness in Graph Neural Networks (GNNs) is highly commendable and worthy of research. \n\n2. Extensive experimental validation demonstrates the method's capability in enhancing performance."
                },
                "weaknesses": {
                    "value": "1.\tI disagree with the claim made in the abstract that \"However, little attention has been paid to improving the expressive power of underlying graph topology.\" In fact, there has been a significant amount of research in recent years on the expressive power of graph neural networks, which is a topic that this paper lacks discussion on. \n\n2.\tIn particular, the approach presented in this paper bears similarities to k-hop GNN. Therefore, it is important to provide a detailed discussion and conduct experimental comparisons between the two.\n\na)\tNikolentzos, Giannis, George Dasoulas, and Michalis Vazirgiannis. \"k-hop graph neural networks.\" Neural Networks 130 (2020): 195-205.\n\nb)\tFeng, Jiarui, et al. \"How powerful are k-hop message passing graph neural networks.\" Advances in Neural Information Processing Systems 35 (2022): 4776-4790.\n\n3.\tThe diagrams included in the article are difficult to comprehend.\n\n4.\tThe paper lacks experimental or theoretical evidence to support the claim that extracting global structural information can effectively resolve the issue of over-smoothing."
                },
                "questions": {
                    "value": "1.\tWhy we need to use proxy measurements instead of existing measurement methods\uff1f\n\n2.\tCould you please explain the design differences and suitable scenarios for the two types of messenger wrapper.?\n\n3.\tThe ablation study lacks a comparison that involves aggregation of global information exclusively. The overall experiments do not directly demonstrate the impact of different methods on over-smoothing. It is recommended that this be supplemented.\n\n4.\tHow does capturing global information overcome the problem of over-smoothing? Could an excessive focus on extracting global information potentially lead to even greater over-smoothing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6116/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6116/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6116/Reviewer_JpoC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6116/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848113642,
            "cdate": 1698848113642,
            "tmdate": 1699636661353,
            "mdate": 1699636661353,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XgosPkGqds",
                "forum": "dcbLZPDcBS",
                "replyto": "Pxm9PzIPoh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6116/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6116/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Weakness 1:** As we mentioned in the global reply, we didn\u2019t discussed the expressive power of graph neural networks, but the expressibility of graph data itself. We have replaced \u201cthe expressive power of underlying graph topology\u201d with \u201cthe potential of a graph to be expressed\u201d.\n\n**Weakness 2:** There are 2 main differences between k-hop GNN and our graph rewiring method:\n(1)\tEnhance GNN model (by k-hop) vs enhance the wiring topology (by our method). Our objective is to re-wire the graph to only has edges with k-hop nodes regardless of GNN architecture to learn graph representation more efficient by non-local information, while k-hop GNN is to find a combination for message from 0-hop to k-hop nodes to build a new GNN architecture. For instance, the proposed KP-GNN is a specific GNN framework by building peripheral subgraph.\n(2)\tMessage passing (by k-hops) vs rewiring (by our method).  Our idea is to progressively modify the graph to connect with k-hop neighbor as the input to the k-th layer of any GNN with any message aggregation, while k-hop GNN is to design a message aggregation for all neighbors from 0-hop to k-hop in each layer of GNN. Although its variant KP-GNN is building subgraph, such modification is more like the graph partition not the graph re-wiring.\nSince models of two papers reviewer mentioned are both designed for graph level representation, we need to adapt their models to node level representation in our experiments at this point and then report the comparison result in the final version.\n\n**Weakness 3:** We have revised the diagram of our method Fig 3 to mainly show cascaded wrapper (C-ExM) and aggregated wrapper (A-ExM), and removed adjacency matrix examples in the top of Fig 3. Please see the revised version of the manuscript.\n\n**Weakness 4:** Oversmoothing issue is present when GNN model becomes deeper and deeper. While deeper GNN can reach far-away nodes, but node features are often excessively smoothed before message passing reaches them. That is the motivation that we propose to capitalize on non-local (or you can call it global) information exchange by creating edges between far nodes and removing old edges, so they can do message passing before the feature is oversmoothed. We have run the same exp as [1, 2] to show Dirichlet energy trends along with layer number increasing on the revised version. Non-local (global) information is helpful to mitigate over-smoothing issue as shown in the new Fig 2 of the revised manuscript.\n\n**Question 1:** The existing measurement is for the expressive power of GNN. There is no measurement for the expressibility of graph data. So, we based on the WL algorithm proposed a proxy measurement for the expressibility.\n\n**Question 2:** Here are two short sentences to describe our wrappers: Cascade ExM wrapper is using non-local edges after the layer using the original edges of graph. The aggregated ExM wrapper uses non-local edges and the original edges in the same layer, and then add them together. \n\nFor different scenarios, results as we shown in Table 2 and 3 indicated non-local information exchange is more helpful for heterophilous graphs than homophilous graphs. Thus, we add a new ablation study in supplementary (Table 6) of the revised manuscript, the results of them on different scenarios (different average degree ratio of graph data) indicate the two wrappers are similar for graphs with lower average degree, while A-ExM performs better than C-ExM for graphs with higher average degree. This is a reasonable observation that cascaded and aggregated ExM provide similar information since it makes no big difference of non-local information by cascading with each other layer or aggregating with every layer from few neighbors, and for nodes with higher degree, it is more suitable to use the aggregated wrapper, which rewires graph to non-local neighbors on every layer of GNN to address the big number of neighboring nodes.\n\n**Question 3:** The new Fig 2 has the results of Dirichlet energy under the same exp setup as [1,2]. The trending of oversoomthness is much slower than counterpart methods. While GCN, and GraphSAGE is falling below $10^{-2}$ at 10th layer, they can hold the energy until 100th layer using our ExM. Even for the G2GNN that is not oversmoothing at all, using NLE leads to the energy occasionally increased in layers deeper than 100. For NAGphormer, which is a tokenized graph Transformer model, the energy is also increased using non-local information since the 1st layer.\n\n**Question 4:** We add experimental evidence to support the claim that extracting global structural information can effectively resolve the issue of over-smoothing. Please refer to the reply of Question 3 and  Weakness 4.\n\n[1] Rusch, T. Konstantin, et al. \"Gradient gating for deep multi-rate learning on graphs.\" ICLR 2023.\n\n[2] Rusch, T. Konstantin, et al. \"Graph-coupled oscillator networks.\" International Conference on Machine Learning. PMLR, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6116/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700013290355,
                "cdate": 1700013290355,
                "tmdate": 1700013290355,
                "mdate": 1700013290355,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]