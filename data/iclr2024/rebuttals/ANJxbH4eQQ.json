[
    {
        "title": "Beyond the training set: an intuitive method for detecting distribution shift in model-based optimization"
    },
    {
        "review": {
            "id": "2zVXEuGPRy",
            "forum": "ANJxbH4eQQ",
            "replyto": "ANJxbH4eQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses a common issue in design problems. Given a expensive-to-evaluate function $f(x)$ and an initial set of training points, the goal is to find points $x$ where $f(x)$ is maximised. In model-based optimization (MBO) approaches, the idea is to iteratively train a surrogate model to approximate $f(x)$ using the existing data and use it to generate new data. However, the common issue here is that the initial points may not be sampled i.i.d. from the input space, leading to a distribution shift problem.\n\nThe paper addresses the issue by training a binary classifier, which they call it Out-Of-Distribution (OOD) classifier, to predict whether the input comes from the training data or from the design data, and uses the classifier's logit to derive a notion called OOD score that can be used to weight the input towards the distribution of the design data. Using the OOD scores, they show how the distribution shift problem can be largely mitigated.\n\nExperiments were conducted in a 2D toy model, a simulated protein structure design, and the design of AAV capsid protein."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written. It's idea is clear and easy to understand. Experiments are reasonable and convincing for the design problems addressed in the paper."
                },
                "weaknesses": {
                    "value": "Forgive me for being straight to the point but I think the main contribution of the paper, the OOD classifier and its OOD score, is very well-known in the ML literature, under the name of propensity score (e.g. [1], [2]):\n\n[1] Agarwal et al. Linear-Time Estimators for Propensity Scores. In AISTATS, 2011.\n[2] P. Rosenbaum and D. Rubin. The central role of propensity score in observational studies for causal effects. Biometrica, 70:41\u201355, 1983.\n\nIt is the same idea: train a binary classifier using sampled points from two separate distributions $p$ and $q$ as negative and positive examples, then to make any point $x$ coming from distribution $p$ look like it comes from $q$, we assign a weight equal to the Radon-Nikodym derivative (RND) $\\frac{dq}{dp}(x)$ (see section 3 of [1]) to $x$. The RND is estimated by a function of the prediction scores of the classifier, named as the propensity score, which matches with the OOD score of the paper.\n\nIf we take OOD classifier/score out of consideration then unfortunately the remaining contributions are not sufficient for me to recommend acceptance."
                },
                "questions": {
                    "value": "I do not have any specific question. The paper should have been otherwise a good paper had the propensity score not been invented before."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698069245187,
            "cdate": 1698069245187,
            "tmdate": 1700689825865,
            "mdate": 1700689825865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fTTleXNq2S",
                "forum": "ANJxbH4eQQ",
                "replyto": "2zVXEuGPRy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the reviewer for directing us to the propensity score literature. To clarify, we do not consider the use of a binary classifier to estimate a density ratio as a contribution of our paper.  Notably, we have identified similar methods to the propensity score in the \u201cDensity ratio estimation for covariate shift and outlier detection\u201d subsection of our Related Work, and discussed how our approach differs from these methods; much of this discussion is also applicable to the propensity score literature. We will revise the Related Work section to include a discussion of the propensity score literature. \n\nIn our paper,  we adapt and apply a known concept (the density ratio estimator/propensity score) to a new domain in which it is not well-known and demonstrate its effectiveness in a real-world setting, where it outperforms widely adopted methods. Our work, which we agree is not inventing the density ratio estimator or propensity score, contributes in three significant ways: \n1. We adapt these concepts to tackle feedback covariate shift in sequence design, a previously unexplored application for this method.  \n2. We find the importance of using a design-induced test set, rather than a fixed test set (commonly used in the literature), as the q distribution.\n3. We design a real-world experiment demonstrating our method overcomes a major limitation commonly encountered in standard design methods.\n\nExpanding on each of the points above:\n1. Previously, the use of propensity scores and density ratio estimators in our context was unexplored. Our study is the first, to our knowledge, to prove their practical utility in sequence design. For adoption and further development of these approaches in our domain (where mistakes are expensive failed experiments), someone needs to show that they work, as we did here. Further, applying this method in our domain required at least one change in how it was implemented, which is described in the next paragraph.\n\n2. We find that the use of a design-induced test set, rather than a fixed test set, as the q distribution, is crucial to the performance of our method in sequence. To our knowledge, this choice of q distribution is not discussed in the density ratio estimation or propensity score literature, which typically assumes access to a fixed test set. An alternative approach could be to use a uniform distribution over the input space as the q distribution; however, we have found that this method is ineffective and is vastly outperformed by a design-induced q distribution. Therefore, the use of a design-induced q distribution is an innovation that is particular to our method. To illustrate this point, we plan on including a figure that shows how crucial the correct selection of the q distribution is for the effectiveness of our approach in real-world applications.\n\n3. Demonstrating feedback covariate shift in simulated environments like Design-Bench (Trabucco et. al. 2022) or FLEXS (Sinai et. al. 2020) is challenging. This difficulty is highlighted by the unexpectedly high performance of naive unconstrained search algorithms, such as the \u201cmax_x f(x)\u201d in Design-Bench and an unconstrained version of Adalead in FLEXS. Without tuning hyperparameters from their setting in simulations, these methods produce close to a 0% functional rate in real-world scenarios. This suggests that these simulation settings do not effectively mimic the feedback covariate shift issue found in real-world scenarios. To further support this observation, we plan on including an additional experiment, showing that common simulation settings fall short in replicating the feedback covariate shift problem. To bridge the gap between simulations and real-world conditions, we conducted a real-world experiment. Our experiment demonstrated two key findings: (a) the intensity of the distribution shift changes depending on the search algorithm\u2019s hyperparameters, specifically the number of optimization steps, and (b) our method detects these shifts. \n\nGiven the clarification above and the reviewer\u2019s perspective that it is otherwise a good paper, we wonder whether applications of methods in new areas and non-trivial observations that work better than the state of the art in that domain is not considered a worthy contribution. We are most happy to clarify the paper\u2019s language to make this clear.\n\nWe also want to clarify a misunderstanding in the reviewer\u2019s summary of the paper, which assumes the OOD score is \u201cused to weight the input towards the distribution of the design data\u201d. Our method does not modify the predictive model based on the OOD score, but instead uses the OOD score to infer optimal hyperparameter settings for offline MBO algorithms (e.g., the number of steps of optimization), thereby identifying designed sequences with reliable predictive scores. We will update the manuscript to minimize any potential confusion on this point."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700010791998,
                "cdate": 1700010791998,
                "tmdate": 1700012092538,
                "mdate": 1700012092538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1fv2b2cb97",
                "forum": "ANJxbH4eQQ",
                "replyto": "fTTleXNq2S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
                ],
                "content": {
                    "title": {
                        "value": "Sorry, I need further clarification"
                    },
                    "comment": {
                        "value": "I would like to understand your comment better. You seem to assume that density ratio estimator and propensity score are the same. Is that correct?\n\nIf so, I do not share the same view. The former refers to $\\frac{p_{de}(x)}{p_{tr}(x)}$ while the latter refers to the RND (which is technically not a ratio of two densities) $\\frac{dp_{de}}{dp_{tr}}(x)$. The \u201cDensity ratio estimation for covariate shift and outlier detection\u201d subsection of your Related Work explains the literature related to the former. But your OOD score matches with the latter."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700394441708,
                "cdate": 1700394441708,
                "tmdate": 1700394441708,
                "mdate": 1700394441708,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O9URKrKgF9",
                "forum": "ANJxbH4eQQ",
                "replyto": "fTTleXNq2S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback."
                    },
                    "comment": {
                        "value": "I have read the authors' feedback. Point 1 is received with appreciation. Regarding point 2 and 3, I would like to see the promised new results to be more convinced.\n\nRight now, my view is that if we take out OOD score from the list of claimed contributions then what's remaining:\n1. From a theoretical point of view, it is still about naturally applying propensity score to a new problem, with no new theoretical insight about propensity score.\n2. From the design problem's point of view, it is a great contribution to the design problem, where a design-induced test set is key, which has been overlooked by existing approaches in the literature.\n\nI have increased my rating. It can be increased further once I have seen the promised updates to the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700397251781,
                "cdate": 1700397251781,
                "tmdate": 1700397251781,
                "mdate": 1700397251781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8vXiKIeqgl",
                "forum": "ANJxbH4eQQ",
                "replyto": "XmTwrJu7Gg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_nQCc"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thank you for the updated results. With them I have raised further up my rating to 6.\n\nAlthough this point may no longer be valid, just for the record in my understanding the relationship between the Radon Nykodyn derivative and the ratio of probability measures is rather messy. As illustrated in this public page: https://math.stackexchange.com/questions/1293402/radon-nikodym-derivative-of-measures (not a good reference I agree)"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690003677,
                "cdate": 1700690003677,
                "tmdate": 1700690003677,
                "mdate": 1700690003677,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KAhxQUchT8",
            "forum": "ANJxbH4eQQ",
            "replyto": "ANJxbH4eQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_a8tv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_a8tv"
            ],
            "content": {
                "summary": {
                    "value": "Model-based optimization can be tricky in practice because, by definition, the goal is to move away from the training data to not parts of the search space. This means that models may extrapolate unpredictably. There are a variety of methods for reasoning about models' reliability in certain regions. The authors propose a simple approach: train a model that detects whether a point in the search space is similar to the training data. This can be done using a binary classifier. The outputs of this classifier can be combined with the output of the predictive model to form an acquisition function that penalizes reckless exploration."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper addresses a key challenge in model-based optimization: knowing when to trust the model's enthusiasm for certain new designs vs. where in the search space to not trust the model.\n\nThe paper presents a section of experiments based on real-world wet-lab experiments."
                },
                "weaknesses": {
                    "value": "The paper proposes a method (sec 2.4) for improving MBO using a surrogate OOD classifier model to penalize exploring regions of the search space where the model is likely to extrapolate poorly, since the regions are out of the distribution that the model was trained on. However, there are *no* experiments that directly test the impact of this approach. The fidelity between OOD metrics and model errors are presented, but no results demonstrate that incorporating the OOD classifier in the MBO search procedure improves the quality of proposed designs. As far as I understand, the analysis on the AAV data in section 4.3 does not actually incorporate the OOD classifier to change the library of selected AAV sequences. Instead, a library was generated without the paper's proposed MBO approach, and then some analysis was done retrospectively to argue that perhaps using the OOD classifier would have been helpful. Given that the paper provides a concrete proposal for an MBO algorithm, there needs to be a head-to-head comparison between this approach and a baseline approach. This could be done easily, for example, on the synthetic 2D data of Sec 4.1.\n\nOOD detection seems like an unreliable way to reason about where the model will reliably be able to extrapolate, since it only looks at the P(x) distribution of training data, not the distribution P(y|x) or labels or anything about the particular inductive biases and invariances of the model being used to make predictions. In particular, many modern predictive models use some sort of pretraining on natural protein sequences. How does this impact your assumptions?"
                },
                "questions": {
                    "value": "I find the overall flow of the proposed algorithm a bit confusing. If I understand correctly, the approach is this: fit a predictive model, run some sort of search algorithm to find points with high predictive model score, train an OOD classifier where the positive examples are the points from the previous step and the negative examples are the training data used to train the predictive model, re-run the search algorithm using a modified objective that combines the scores of the OOD model with the original model. Is this correct?\n\nAs far as I can tell, the data used for fig 3 came from a single round of MBO; only one additional wet-lab experiment was run, and it was run on sequences from the entire trajectory of the Adalead optimizer used for finding sequences with high model score. I understand that wet-lab experiments are expensive, and that multiple rounds of experiments would be infeasible. However, I don't understand the point of focusing on the 15 steps of the Adalead algorithm. This method should be treated as a black-box search algorithm used for finding high-scoring sequences. Why was experimental capacity spent on sequences from the early iterations of Adalead?\n\nI don't understand this:\n'' In contrast, the Deep Ensemble scores cannot effectively serve as a quantitative predictor of shift intensity.' Why not?\n\nSection 4.2 is extremely terse. What is the key take-away point from it that suggests it should appear in the paper?\n\nThere need to be far more details about the AAV setup. Is the data public? Will it be released with the paper? How big is it? How long is the sub-sequence of the protein that was mutated? \n\nPerhaps I'm misunderstanding things. Can you please address the 'weaknesses' above?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698166537211,
            "cdate": 1698166537211,
            "tmdate": 1699636140402,
            "mdate": 1699636140402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iH9Q87j4YY",
                "forum": "ANJxbH4eQQ",
                "replyto": "KAhxQUchT8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed review of our manuscript and the useful suggestions both in terms of clarity and possible extensions. Based on your feedback, we plan to make several improvements: (1) we will improve the explanation of our method\u2019s objectives, and (2) we will conduct an experiment to demonstrate how different model architectures impact prediction accuracy under distribution shift. We are excited to work with the reviewer to resolve these issues.\n\nWe would like to clarify an important nuance that failed to come through in the paper (which we plan on fixing). We are not devising a new MBO algorithm, instead, we are devising a complementary method that can be used alongside any MBO algorithm and only applies at the \u201cselection\u201d step after the initial sequences are designed. The selection problem can also be viewed as post-hoc hyperparameter tuning for an offline design algorithm (Fannjiang et al 2022). \n\nIt is a well-established practice in the field (in real-world applications) to distinguish between the design stage\u2014where millions of sequences are generated with high predicted scores using various models and search algorithms\u2014and the selection stage\u2014where a small subset of the most promising candidates are chosen for experimental testing. These batches can range from 10^1 to 10^5 in size. Our work enables a selection procedure that achieves low regret, better than ensemble uncertainties and greedy selection.  Our real-world experiment provides all of the data needed to validate the method (i.e. there would be no new information we\u2019d learn from another experiment, as we expand below).\n\nWe welcome additional questions or comments about our manuscript.\n\nResponse to specific comments:\n\n### Weaknesses\n\n> \u201cThe paper proposes\u2026. improves the quality of proposed designs.\u201d \n\nThe proposed method does not use the surrogate OOD classifier as part of an optimization objective, but instead, as a complementary and independent tool for selection after the in-silico design step. \n\n> \u201cAs far as I understand, the \u2026 there needs to be a head-to-head comparison of the synthetic 2D data of Sec 4.1.\u201d\n\nThe reviewer is correct that the experiment was not run with the OOD applied as a filter a priori. However, the procedure is identical to the case where it was, because as we mentioned above it\u2019s only relevant to the selection step. The fact that we didn't filter out the sequences for the experiment with the OOD classifier provided counterfactual samples (i.e. sequences that would have been filtered with OOD classifier and not measured). See below for the detailed procedure in response to your first question.\n\n> \u201cOOD detection seems like an unreliable way to reason about where the model will reliably be able to extrapolate, since it only looks at the P(x) distribution of training data, not the distribution P(y|x) or labels\u201d\n\nIn design problems, we only observe covariate shift, which means P(x) changes. P(y|x) stays constant because the underlying distribution relating sequences x to some property y is dictated by a fixed underlying biological process. This focus on covariate shift has been previously explored in the design literature (Fannjiang et. al. 2022). We will update Section 2.2 to clarify this. \n\n> \u201cIn particular, many modern predictive models use some sort of pretraining on natural protein sequences. How does this impact your assumptions?\u201d\n\nThank you for the suggestion. We will include an experiment showing how different model architectures\u2014in particular, a LLM pretrained on natural protein sequences\u2014impacts prediction error under distribution shift. It is worth emphasizing that because our method is applied at the selection stage, it can be combined with any design algorithm or predictive model."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700011665747,
                "cdate": 1700011665747,
                "tmdate": 1700104374392,
                "mdate": 1700104374392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xTdaya1pDX",
                "forum": "ANJxbH4eQQ",
                "replyto": "KAhxQUchT8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Questions\n\n> \u201cI find the overall flow of the proposed algorithm a bit confusing... Is this correct?\u201d\n\nThis is partly correct except there is no re-running of the search algorithm, since we focus on solving the selection problem. Here\u2019s a detailed breakdown of the method:\n\n(1) Fit a predictive model with (X_train, y_train) pairs.\n\n(2) Run search to produce millions of sequences X_design with high model scores. \n\n(3) Train the OOD classifier to separate X_train (1) from X_design (2).\n\n(4) Use the OOD classifier to choose a small subset of X_design for experimental testing.\n\nWe will clarify this in Section 2.4.\n\nRe-running the search algorithm with a modified objective is possible, but the main challenge isn\u2019t the search itself. Search algorithms like Adalead can generate many sequences with high scores. The key challenge is determining the reliability of model scores for designed sequences since developing an accurate predictive model for protein sequences is challenging. By identifying which sequences have model scores we can trust, we can select a small subset of sequences for lab testing.\n\nWhile re-running search with a modified objective is a valuable future research direction, it risks causing another distribution shift when searching under the joint objective. Therefore, it is cleaner to keep search and selection separate.\n\n> \"As far as I can tell, the data used for fig 3 came from a single round of MBO ... Why was experimental capacity spent on sequences from the early iterations of Adalead?\"\n\nYou are correct that the main objective of using a black-box search algorithm like Adalead is to identify sequences with high scores. Figure 3a shows that running Adalead for 15 steps results in 0% functional sequences, despite high predicted scores, indicating a high prediction, high error scenario. If the method were treated as a black-box and we simply took the sequences with the highest predicted scores (in this case, that would be sequences from the end of the trajectory at step 15), none of our sequences would have been functional. Therefore, the challenge lies in adjusting the number of Adalead steps to achieve both high predictions and low error. This problem can be cast as hyperparameter selection for search algorithms and has been discussed previously in Fannjiang et. al. 2022.\n\nTo demonstrate this balance, we measure the ground truth for sequences derived from each Adalead step, revealing the tradeoff that would otherwise remain unseen (and is assumed but not previously shown in sequence design literature). In fact, one reason to do this experiment was to fill this gap explicitly. Notably, in MBO simulation frameworks like Design-Bench (Trabucco et. al. 2022) or FLEXS (Sinai et. al. 2020) this effect is much weaker (we are happy to add a plot to the SI showing this) and this is why we ran the real-world experiment because we knew it happened in practice (through experience), but it was hard to capture in simulations. \n\nOur experiment suggests that the OOD classifier can detect subtle changes in the high prediction vs error continuum, which can be tracked as a function of the number of optimization steps. The OOD score increases as the fraction of designed sequences with a functional readout decreases (Figure 3a and 3b), suggesting our method assigns higher uncertainty to sequences with higher predictive error. This translates to design in the regret plot (Figure 3c) where we show that for a variety of values for the distribution shift detection score, our method produces lower regret than Deep Ensembles.\n\n> \"I don't understand this: '' In contrast, the Deep Ensemble scores cannot effectively serve as a quantitative predictor of shift intensity.' Why not?\"\n\nShift intensity can be tracked by the number of steps taken by an optimization algorithm. We show this relationship in the top panel of Figure 3a, where as the iteration increases, the proportion of designed variants that are functional decays to 0 despite all designed variants having a high predicted packaging score.\n\nThe Deep Ensemble score is a poor measure for shift intensity since this score remains constant as a function of iteration (bottom panel of Figure 3a). For it to be a useful predictor, we should expect higher uncertainty for data points at higher iterations since those variants do not package, despite high model scores.\n\nWe will rewrite this to say the Deep Ensemble score is a poor predictor of shift intensity relative to the OOD score."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700011825847,
                "cdate": 1700011825847,
                "tmdate": 1700057729207,
                "mdate": 1700057729207,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QyQBe5wpmC",
                "forum": "ANJxbH4eQQ",
                "replyto": "xTdaya1pDX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_a8tv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_a8tv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your thorough response.\n\nHere is my current understanding of the Adalead stuff. Can you confirm that this is correct?\n1) It has been suggested (but not backed up empirically) in the literature that running a local search optimizer such as Adalead in the selection step can 'hack' the model to find sequences with high model score, but low true fitness. \n2) Therefore, not running the optimizer to convergence, but instead using some sort of early stopping, provides a form of implicit regularization that favors sequences where the correspondence between model score and true experimental fitness are higher. As you explain in your response, the number of iterations can then be seen as a hyper-parameter for the optimization.\n3) When designing your AAV library, you ran Adalead for a significant number of iterations (15), but then experimentally characterized not just points from the final iteration, but from all iterations of the optimizer.\n4) This provides first-of-its-kind empirical confirmation of #1.\n5) The trajectory of data from (2) is used to provide multiple sets of sequences with varying degree of correlation between model score and true fitness. \n6) Analyzing the model's proposed OOD score method on different sets from (4) is helpful because we can see how the OOD method performs on data with different properties. This is why the experiments consider these different sets.\n\nMy thoughts:\nMethod (2) is orthogonal to the primary methodological contribution of the paper. I find it confusing and distracting that there is such a focus on it in the experiments. Point (4) is a nice contribution to the research conversation, but certainly not the focus of the paper. It required a lot of digging to discover the point you're trying to make with this. I understand that point (6) is attractive because it gives an interesting range of evaluation settings. However, it is unclear why this level of complexity was necessary to evaluate the OOD method."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576378316,
                "cdate": 1700576378316,
                "tmdate": 1700576378316,
                "mdate": 1700576378316,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DVwm6VuHNm",
                "forum": "ANJxbH4eQQ",
                "replyto": "QyQBe5wpmC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_a8tv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Reviewer_a8tv"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding the proposed algorithm for the paper, I have a few additional hesitations\n\nFirst, in the BayesOpt and offline model-based optimization literature, it is common to use an acquisition function account for explore/exploit, etc. You instead focus on a paradigm where the raw model score is the acquisition function, and then a separate 'selection' step is used to filter candidate designs with high score to a smaller final set. I understand that this is attractive software-wise, but doing so cuts out a number of standard baselines such as using a lower-confidence-bound acquistion function to penalize uncertainty, where uncertainty can be estimated, for example, using ensemble uncertainty. \n\nI also continue to be fundamentally confused by your proposed algorithm, and the fact that it requires so much clarification is concerning to me. Here's what you wrote:\n(1) Fit a predictive model with (X_train, y_train) pairs.\n(2) Run search to produce millions of sequences X_design with high model scores.\n(3) Train the OOD classifier to separate X_train (1) from X_design (2).\n(4) Use the OOD classifier to choose a small subset of X_design for experimental testing.\n\nWhat if step (3) uses a high-capacity OOD classifier that can achieve perfect accuracy when separating X_train and X_design? In step (4) the classifier is applied to the exact data used to train it, and a high-capacity model will assign a probability close to 0.0 or 1.0 for all such points. How are the OOD scores at all reliable in this case? In other words, it feels like there is a meta-problem of worrying about overfitting/extrapolation for the OOD classifier."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576907424,
                "cdate": 1700576907424,
                "tmdate": 1700576907424,
                "mdate": 1700576907424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sEXay5ZWT2",
                "forum": "ANJxbH4eQQ",
                "replyto": "KAhxQUchT8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> \"I understand that point (6) is attractive because it gives an interesting range of evaluation settings. However, it is unclear why this level of complexity was necessary to evaluate the OOD method.\"\n\nIn our global response above, we complete an experiment showing that simulation settings do not recapitulate the distribution shift we observe in real-world data. Therefore, we design a real-world experiment to show this shift intensity as a function of how far we explore (the optimization step). This is the problem we want to solve with the OOD method.\n\n> \"First, in the BayesOpt and offline model-based optimization literature...for example, using ensemble uncertainty.\"\n\nOur key baseline is ensemble uncertainty, which we refer to as \u201cDeep Ensemble Uncertainty\u201d. For the high-batch / low sequential round setting that we focus on, it is less common to use a single acquisition function to tradeoff explore/exploit. Instead, it is a well established practice to approach the problem in our formulation: generate a large number of sequences from one or multiple methods, and then score/rank/filter according to a particular desiderata. This is not a \u201csoftware\u201d shortcut, but a well-established approach to the problem considering the added complexities that come with reasoning about diversity in high-batch settings.\n\nWith that said, while it\u2019s possible for us to have focused on designing an acquisition function for selection, the OOD score does not admit an interpretable notion of uncertainty like other uncertainty quantification methods. For this reason, a fair comparison between the OOD score and ensemble uncertainty is done by looking at how they rank sequences using a percentile cutoff for filtering and selection (see Figure 3).\n\n> \"I also continue to be fundamentally confused by...about overfitting/extrapolation for the OOD classifier.\"\n\nThere are a few ways of addressing this. (1) Regularize the network, so it does not achieve perfect accuracy (e.g., by training the network for only a few iterations). (2) Train the network on a random sample of the designed variants and only select variants from the test set. The test scores will be less likely to overfit compared to the training predictions. (3)  Even if the model overfits to the data, that is okay as we are not expecting generalization to other distributions outside of the data the OOD classifier is trained on. After all that is the point of using the unlabeled design distribution as the test set. Even if the model overfits, the model correctly ranks the data points in the test set, which is what matters. (4) Select a q distribution that has overlap with p. If the test set is non-overlapping with the training data, this method is unlikely to work. We discuss this limitation in the Discussion section."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685594595,
                "cdate": 1700685594595,
                "tmdate": 1700694063822,
                "mdate": 1700694063822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gtimSeiXof",
            "forum": "ANJxbH4eQQ",
            "replyto": "ANJxbH4eQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_jVj2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_jVj2"
            ],
            "content": {
                "summary": {
                    "value": "In this study a new technique is proposed to filter out designed samples by offline MBO for which the surrogate model does not provide reliable predictions. In practical settings, this allows for prioritizing improved samples that have reliable predictions by the surrogate model and stay close to the initial set used for training. This is done by training a classifier to distinguish between the training and the designed samples. For each design sample, the logit score of the classifier determines its extent of deviation from the training distribution."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Generation of experimental data for the problem of AAV design to evaluate the performance of their technique"
                },
                "weaknesses": {
                    "value": "See Questions"
                },
                "questions": {
                    "value": "1)\tIt is not clear how the threshold on the OOD scores should be determined? Is there a systematic way to do this?\n2)\tThe binary classifier is trained to assign designed samples a different label from the training samples even if the designed samples are similar to the training samples. What are the downsides of this?\n3)\tIt is not discussed when the proposed technique could fail.\n4)\tMinor: Change Figure 4 to Figure 3 in the text"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Reviewer_jVj2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784589760,
            "cdate": 1698784589760,
            "tmdate": 1699636140262,
            "mdate": 1699636140262,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "occwEJb7wR",
                "forum": "ANJxbH4eQQ",
                "replyto": "gtimSeiXof",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer\u2019s insightful feedback. We would like to reiterate the reviewer\u2019s point that a strength of our paper is in applying an offline MBO algorithm in the real-world to (a) analyze the effect of distribution shift on predictive error and design quality and (b) assess the effectiveness of our method in addressing this issue. Distribution shift often remains unnoticed in offline MBO pipelines due to simulation benchmarks not fully reflecting the practical challenges we have observed. \n\nWe are hopeful we can resolve the reviewer\u2019s most pressing concerns. Our point-by-point response is below:\n\n### Questions\n> \u201c1. It is not clear how the threshold on the OOD scores should be determined? Is there a systematic way to do this?\u201d\n\nIn practice, we find the OOD scores to be robust across design algorithms (e.g., the scores are similar between the Adalead run in Figure 3 and the beam search run in Suppl. Figure 6). Therefore one could choose a threshold based on the results in the paper with confidence that the results will transfer. As we highlight in Section 2.4 \u201cSelection Using OOD Scores\u201d, a threshold choice will be application-specific and will depend on the user\u2019s needs and level of risk-tolerance (e.g., dependent on the batch size, goals of the experiment, etc). A user can choose to stratify their risk by choosing multiple cutoffs, or minimize risk by choosing a very low cutoff.\n\n> \u201c2. The binary classifier is trained to assign designed samples a different label from the training samples even if the designed samples are similar to the training samples. What are the downsides of this?\u201d\n\nFor designed samples that are similar (read: in-distribution) to the training samples, the OOD scores will be low and will likely fall within the distribution of OOD scores for the training data itself. This is not an issue as the model learns to assign a classification probability close to 0.5 (or lower) for these data points. The downside of this is when all designed samples are in-distribution, this method will likely have limited use as it requires there to be some degree of shift. For any practical sequence design application, however, it is inevitable that there will be a shift as the goal is to design a sample with an improved measurement relative to what is observed in the training data.\n\n> \u201c3. It is not discussed when the proposed technique could fail.\u201d\n\nThe OOD classifier is likely to fail when the distributions p and q are very different from each other. If the gap between these two distributions is especially large, training a classifier to distinguish points from p and q can result in achieving perfect accuracy, but it may provide a poor estimate of the density ratio between them. This emphasizes the importance of choosing a design distribution q that overlaps with p to enable effective density ratio estimation through classification. Recent work has introduced a modified density ratio method using a telescoping product [1] to tackle this specific situatation. We will update the discussion section to address this challenge.\n\n[1] Rhodes, Benjamin, Kai Xu, and Michael U. Gutmann. \"Telescoping density-ratio estimation.\" Advances in neural information processing systems 33 (2020): 4905-4916\n\nWe are eager to work with the reviewer to increase their confidence in our paper and answer any further questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179709046,
                "cdate": 1700179709046,
                "tmdate": 1700683821836,
                "mdate": 1700683821836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z4LEdXg9MI",
            "forum": "ANJxbH4eQQ",
            "replyto": "ANJxbH4eQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_6Sc3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_6Sc3"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an innovative solution to tackle the prevalent issue of distribution shift in model-based optimization (MBO) during design problems. Through the training of a binary classifier, the proposed method aims to differentiate between training and design data distributions, using classifier's logit scores as a distribution shift metric. The authors have also showcased the efficacy of their method in real-world scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The topic is extremely relevant in today's design scenarios where ML is frequently employed. Addressing the distribution shift problem is vital for the efficacy and reliability of models.\n\nThe paper's strength lies in its rigorous testing and validation. The authors did not stop at theoretical validation but extended their methodology to real-world applications, especially the experiment involving Adeno-Associated Virus (AAV) capsid sequences.\n\nThe straightforward nature of the proposed solution, its ability to be integrated with existing design methods, and its applicability across different black-box models make it versatile and broadly applicable.\n\nThe authors provided a clear understanding of the challenges tied to distribution shifts, especially feedback covariate shift, and visualized the problem effectively with Figure 1."
                },
                "weaknesses": {
                    "value": "While the simplicity of the method is a strength, there are concerns about its robustness when exposed to diverse and complex real-world scenarios. The ability to detect distribution shifts in more intricate and nuanced cases would be important.\n\nThe paper would have benefitted from a clearer comparative analysis of the proposed method against the existing methods to handle distribution shift. Such a comparison can elucidate the advantages of their approach over others.\n\n\nWhile the paper provides qualitative insights and findings from experiments, more quantitative metrics that measure the efficacy, false positives, and false negatives of the method would give a clearer picture.\n\nIt's not clear if the approach would be as effective across diverse domains outside of the ones presented in the paper."
                },
                "questions": {
                    "value": "Include a detailed comparison section with existing methods to highlight the novelty and advantages of the proposed technique.\nOffer more depth on the architecture and functioning of the binary classifier.\nProvide additional quantitative metrics for method evaluation.\nExplore the methodology's application in a wider array of domains and provide insights or findings from such applications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2081/Reviewer_6Sc3"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792360259,
            "cdate": 1698792360259,
            "tmdate": 1699636140186,
            "mdate": 1699636140186,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xUaw6k6mfZ",
                "forum": "ANJxbH4eQQ",
                "replyto": "z4LEdXg9MI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful comments and questions. We agree distribution shift is a critical problem faced by design practitioners today, and there\u2019s a strong need for a method that is easy to use. We also want to build on the reviewer\u2019s point about our rigorous validation by noting that real-world deployment of offline MBO is critical to understanding and addressing distribution shift as simulations alone are often inadequate for capturing the full scope of these challenges. We expand on this point about the challenges of simulation settings below.\n\nWe are hopeful that with additional experiments and improved writing we can resolve the reviewer\u2019s most pressing concerns. Our point-by-point response is below:\n\n> \u201cWhile the simplicity of the method is a strength\u2026 Such a comparison can elucidate the advantages of their approach over others.\u201d\n\nWe agree that adding additional baseline comparisons would strengthen the paper, although developing such baselines for our use case is not always straightforward. Existing methods to handle distribution shift that we could compare against can either be classified as scoring rules used for identifying OOD points or from improving model robustness to distribution shift. We discuss the possibilities for developing baselines from each of these categories below:\n* On scoring rules: Deep Ensemble scores are considered the state of the art for handling distribution shift and is largely considered the de facto standard, due to both its simplicity and robustness. For this reason, we focus our comparison against this baseline. With that said, conformal prediction has recently been applied to sequence design as a tool to correct covariate shift (Fannjiang et. al. 2022). This approach, however, in its most effective  form requires training (n + 1) x |y| models, where n is the number of test data points and |y| is the cardinality of y. Scalability improvements can enable training of n models, but for practical high-throughput sequence design problems, this remains a difficult computational challenge. We will update the text to explain this in more detail.\n\n* On improving model robustness: It has previously been shown that model pretraining can improve robustness to distribution shift [1]. In the protein domain, large language models have been pretrained on hundreds of millions of protein sequences [2]. These models have shown impressive performance on a variety of downstream prediction tasks. In response to the reviewer\u2019s comment, we have completed an additional experiment showcasing that improving model robustness (e.g., by using a pretrained LLM) does not address feedback covariate shift in the design setting. We used ProtBERT [2] (a pretrained protein language model) to predict our target property using linear probing [3], a method for adapting pretrained models to downstream regression problems by fixing the pretrained weights and fitting a linear head to predict the target property. We used an identical train/test split to the CNN described in the manuscript and computed predictions on the designed data. We find identical behavior to the CNN \u2013 model scores and model error go up as a function of the distribution shift. There is a massive drop in performance from a random holdout (Spearman rho = 0.75) to the design set (Spearman rho = -0.55). We plan on updating the manuscript with a figure showcasing this result. We reiterate that our method is broadly applicable since it can be integrated with any design algorithm or predictive model, so if future models prove to be robust to design-induced distribution shift, our method can be used to further improve this result. \n\n[1] Hendrycks, Dan, Kimin Lee, and Mantas Mazeika. \"Using pre-training can improve model robustness and uncertainty.\" International conference on machine learning. PMLR, 2019\n\n[2] Elnaggar, Ahmed, et al. \"Prottrans: Toward understanding the language of life through self-supervised learning.\" IEEE transactions on pattern analysis and machine intelligence 44.10 (2021): 7112-7127.\n\n[3] Kumar, Ananya, et al. \"Fine-tuning can distort pretrained features and underperform out-of-distribution.\" arXiv preprint arXiv:2202.10054 (2022)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179179545,
                "cdate": 1700179179545,
                "tmdate": 1700179179545,
                "mdate": 1700179179545,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fu7KuscxR3",
            "forum": "ANJxbH4eQQ",
            "replyto": "ANJxbH4eQQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_SeaM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2081/Reviewer_SeaM"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a method for detecting out-of-distribution samples drawn in model-based optimization ML-guided design. The main idea is that black-box MBO may when optimizing some target function result in designing (drawing from the input space) a sample that is OOD w.r.t the available training data. At the same time, these OOD samples are those for which the performance of the target function predictor is less reliable and therefore undesirable as design choices. Shift between training and design distributions are measured via a shift detection model that classifies samples coming from each distribution. The OOD detection is then used to limit the search space in the design process to those with low \u201cOOD scores,\u201d i.e. close to the distribution of the desired data."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Addresses an important problem in ML-guided design where under-represented regions of the input space are exploited by the surrogate prediction model.  \n\nBy using a distribution classifier, avoids estimating the distribution of the respective data sub-spaces directly. This is especially important to allow for high-dimensional data and avoids setting limiting assumptions on their distribution.   \n\nSynthetic and real experiments show the design shift problem and well motivate the use of OOD detection in MBO design pipelines"
                },
                "weaknesses": {
                    "value": "This approach overly constricts the search space of allowable design samples. The OOD scores rely on a classifier that considers only the data, and therefore is only effective to combat covariate shift. This means that design samples will be forced to be similar to training samples, even if distant samples in the input space could still have correct surrogate model predictions."
                },
                "questions": {
                    "value": "Q1] The statement that distribution shifts in supervised regression \u201ctypically takes the form of covariate shift\u201d is not well supported. Assuming covariate shifts is ones of the most restrictive assumptions on shift since the model predictions p(y|x) is unchanged between training and test/design distribution. Can you expand on this argument for focusing on this type of error?\n\nQ2] Several approaches to refining the search space in ML-guided design are discussed in Section 3 (first paragraph). Why were these not used as baseline comparisons in the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2081/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824783318,
            "cdate": 1698824783318,
            "tmdate": 1699636140101,
            "mdate": 1699636140101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0kn6cw5ojG",
                "forum": "ANJxbH4eQQ",
                "replyto": "fu7KuscxR3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2081/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thoughtful comments and questions. We agree distribution shift is an important problem in ML-guided design since under-represented regions are easily exploited by surrogate models. As the reviewer notes, estimating the density of high-dimensional distributions (common in sequence design applications) can be difficult in practice, while estimating the density ratio is easy, making this a particularly nice application to apply this concept. We also want to build on the reviewer\u2019s point about our method being well motivated in that experimental data from a real-world deployment of a design algorithm is critical to understanding and addressing distribution shift as simulation benchmarks do not capture the extent that this is a problem.\n \nWe are hopeful that with additional experiments and improved writing we can resolve the reviewer\u2019s most pressing concerns. Our point-by-point response is below:\n\n### Weaknesses\n> \u201cThis approach overly constricts the search space of allowable design samples\u2026 even if distant samples in the input space could still have correct surrogate model predictions.\u201d\n\nWe show evidence that the OOD score tracks shift intensity (Figure 3a), which translates to how large the search space of allowable designs is. Our method doesn\u2019t enforce any particular constraint but lets the user decide how restrictive they want the search space to be. Our OOD scores traverse the entire range from overly restrictive to relaxed. On the restrictive side, you can use an OOD score comparable to say the 75th or 90th percentile of the training data. In this case, we see close to a 100% functional rate (the 0-5 iteration regime in the top panel of Figure 3a). On the other end, we see at certain high OOD scores the sequence space contains mostly non-functional variants (the >10 iteration regime in the top panel of Figure 3a). Then there\u2019s a range in between which the user can select from depending on their goals.\n\nIn practice, we find the OOD scores to be robust across design algorithms (e.g., the scores are similar between the Adalead run in Figure 3 and the beam search run in Suppl. Figure 6), so risk can be controlled reliably (e.g., through stratification based on OOD score).\n\n### Questions\n> \u201cQ1] The statement that distribution shifts in supervised regression\u2026 Can you expand on this argument for focusing on this type of error?\u201d\n\nIn design problems, we only observe covariate shift, which means P(x) changes. P(y|x) stays constant because the underlying distribution relating sequences x to some property y is dictated by a fixed underlying biological process. This focus on covariate shift has been previously explored in the design literature (Fannjiang et. al. 2022). We will clarify Section 2.2 to clarify distribution shifts in *sequence design* problems typically take the form of covariate shift."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2081/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179485570,
                "cdate": 1700179485570,
                "tmdate": 1700179485570,
                "mdate": 1700179485570,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]