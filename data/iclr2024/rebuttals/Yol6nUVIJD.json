[
    {
        "title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs"
    },
    {
        "review": {
            "id": "FWhnALkOYd",
            "forum": "Yol6nUVIJD",
            "replyto": "Yol6nUVIJD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6257/Reviewer_saRD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6257/Reviewer_saRD"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a multi-model, multi-agent framework called ReConcile, which is inspired by roundtable conference discussions. In each round, every agent must produce an answer, an explanation, and the corresponding confidence level. Following multiple rounds of discussion, ReConcile generates a collective team answer. Experiments on various benchmarks (StrategyQA, ECQA, GSM8K, AQuA) demonstrate the effectiveness of ReConcile."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.It is an interesting idea to extend previous work from multi-agent debate to roundtable discussions.\n2.Experiments on various benchmarks show that the proposed method outperforms previous work."
                },
                "weaknesses": {
                    "value": "1.The experiments are conducted on relatively simple reasoning tasks, which seem inconsistent with the claim of \"solving complex reasoning problems\" made in the introduction. Does ReConcile enhance performance on more complex reasoning tasks, such as logical reasoning, the MATH dataset, CommonsenseQA?\n2.The proposed method consists of multi-model predictions. An ablation study (Table 5) shows that the performance significantly decreases without the multi-model approach. It is unclear whether the performance gain is caused by the multi-model ensemble. A comparison with an ensemble baseline should be considered.\n3.There are many hyperparameters in ReConcile (number of discussion rounds, voting weight). It would be interesting to explore if these hyperparameters can be dynamically generated during the roundtable discussions and if they are robust across tasks.\n4.It is unclear whether a weak model could negatively impact the performance. \n5.There is a line of work improving the reasoning ability of LLMs [1].   It would be interesting to investigate whether ReConcile outperforms these methods both theoretically and empirically."
                },
                "questions": {
                    "value": "1.How to calculate the performance of ChatGPT? The performance reported in Table 2 seems lower than previous work [2]\n\n[1] WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct\n[2] Making Large Language Models Better Reasoners with Step-Aware Verifier"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737019450,
            "cdate": 1698737019450,
            "tmdate": 1699636684881,
            "mdate": 1699636684881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vPIjoqczi0",
                "forum": "Yol6nUVIJD",
                "replyto": "FWhnALkOYd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer saRD (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments and for appreciating our idea of round-table discussions and the concerned experiments. Please find the answers to your questions below.\n\n> **The experiments are conducted on relatively simple reasoning tasks.**\n\nWe respectfully disagree with the reviewer here because the tasks considered in our study are not simple. StrategyQA, CommonsenseQA, GSM8k, and AQuA are some of the most commonly used benchmarks in almost all prior works on reasoning. That said, following your suggestions, we conducted experiments on a few other datasets to further demonstrate the capabilities of ReConcile. \n\n* **CommonsenseQA**: Please note that CommonsenseQA experiments were already part of our paper and were referred to as ECQA. This is because we leverage convincing human explanations in ReConcile, which come from the ECQA benchmark (short for Explanations for CommonsenseQA) [1]. ReConcile obtains a 6.4% improvement in ECQA, as per Table 2 in the original version of the paper.\n* **Logical Reasoning**: StrategyQA is listed as a logical reasoning task as per the Big-Bench categorization of tasks [5], for which we already have results in the paper. We also experimented with another logical reasoning task, namely the \u201cDate Understanding\u201d task [2]. See results in the common response.\n* **MATH**: Following your suggestion, we also tested ReConcile on MATH. See results in the common response.\n\n\n> **It is Unclear whether the performance gain is caused by the multi-model ensemble.**\n\nPlease refer to our analysis and discussion in the common response.\n\n> **A comparison with an ensemble baseline should be considered.**\n\nApart from the vanilla single-agent methods, all of our baselines in Table 2 are already indeed ensemble-based (either involving a single agent like self-consistency or involving multiple agents like the Multi-agent Debate). Moreover, even for ReConcile, we compare different mechanisms for obtaining the \"Team Answer\" in Table 3 (e.g., majority vote, weighted vote, maximum confidence), all of which should be seen as ensemble methods for aggregating the answers from different models. In summary, most of the pertinent baselines in our paper are ensemble-based. \n\n> **There are many hyperparameters in ReConcile (number of discussion rounds, voting weight).**\n\nThis is **not** true because the number of discussion rounds is not a hyperparameter. As Algorithm 1 of our paper shows, the number of discussion rounds is dynamically determined based on when all agents reach a consensus. R is just the \"maximum\" number of rounds, which can be any arbitrarily large value.\n\nAs for the voting weights, we use the exact same weights across all seven tasks in the paper and in this rebuttal. This is a testament to its robustness. Please refer to our common response for a more comprehensive discussion of our recalibration strategy and voting weights.\n\nIn summary, ReConcile does not have many hyperparameters, and our choice of voting weights is also adequately backed up by extensive robustness and ECE-style experiments."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270975701,
                "cdate": 1700270975701,
                "tmdate": 1700270975701,
                "mdate": 1700270975701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RPDJd2BBpL",
                "forum": "Yol6nUVIJD",
                "replyto": "vPIjoqczi0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_saRD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_saRD"
                ],
                "content": {
                    "comment": {
                        "value": "the tasks considered in our study are not simple.\n\n-I am not saying that the original datasets used are ***simple***, but ***relatively*** simple as the major point of your motivation is to solve ***complex*** reasoning tasks. I know these are popular benchmarks for many works demonstrating LLM\u2019s reasoning abilities. For example, why I mentioned MATH over GSM8K on math reasoning is that the performance on GSM8K can be ***relatively*** easy (through using CoT prompting)  [1,2] to get  around 85 on Claude-2 and even ~97 on GPT4 (Code Interpreter) alone. Therefore, I am not convinced that the reasoning complexity of GSM8K (as well as other benchmarks here) is able to demonstrate the usage of such a complex multi-round round-table prompting strategy. However, for MATH, I do not see significant performance improvement over GPT4 (around 40) by far even using various in-domain data fine-tuning methods. But unfortunately, as shown by your additional experiments here, the improvement on MATH is indeed very marginal (from 0.39 to 0.41).\n\nhyperparameters setting: number of discussion rounds and the voting weights\n\n-As stated in Sec5.1, \"all iterative methods go through 3 rounds of iteration\".  So why not set this number as you stated \"which can be any arbitrarily large value\"? Is it true that many prompts can reach a consensus within 3 rounds?\nAlso, from the additional results, the voting weights indeed affect the performance, so there should be a guideline about how to set these parameters."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475114347,
                "cdate": 1700475114347,
                "tmdate": 1700475114347,
                "mdate": 1700475114347,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1MNGZ2baRI",
                "forum": "Yol6nUVIJD",
                "replyto": "qs84jPCtEU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_saRD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_saRD"
                ],
                "content": {
                    "comment": {
                        "value": "a weak model could negatively impact the performance\n\n-I am not asking if a strong agent could dominate/improve, but that ***a weak one*** or even several ones would severely harm the debating, including the final performance and also the number of rounds to reach a consensus. \n\nHow to calculate the performance of ChatGPT? The performance reported in Table 2 seems lower than previous work.\n\n-I truly understand that you follow the most similar literature, which is reasonable. But I still suggest the authors follow some literature [1,2]  that shows much higher performance on Claude2/GPT4 with only CoT prompting, as the numbers seem not to ``vary a bit\u2019\u2019. I think this is very important to justify your performance gain (which is also achieved on prompting but more expensive).\n\n[1] MAMMOTH: BUILDING MATH GENERALIST MODELS THROUGH HYBRID INSTRUCTION TUNING\n[2] WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475230059,
                "cdate": 1700475230059,
                "tmdate": 1700475230059,
                "mdate": 1700475230059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5tCxJbTGfx",
            "forum": "Yol6nUVIJD",
            "replyto": "Yol6nUVIJD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6257/Reviewer_5C9y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6257/Reviewer_5C9y"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel algorithm (ReConcile) for leveraging multiple LLM calls (like self-refine) to improve results over single LLM calls. ReConcile is similar to Multi-Agent Debate, but with 3 additional innovations:  (1) using distinct model families (GPT, Bard, & Claude), (2) leveraging confidence estimates from the models, and (3) encouraging the models to attempt to convince each other. ReConcile is compared against several relevant baseline algorithms on four standard benchmarks (StrategyQA, ECQA, GSM8K, AQuA). ReConcile obtains clear improvements over baselines on most of these experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method is well motivated and clearly explained, which is a challenge for a framework of this complexity. The baselines and benchmarks are well chosen, and the experimental methodology appears sound. \n\nAblation studies, summarized in Table 5, quantify the contribution of each of the architectural components to accuracy. The lift is largest for the multi-model dimension, which is particularly interesting. \n\nInterestingly, it is noted (perhaps unsurprisingly) that when a stronger model (such as GPT-4) is included in multi-model, multi-agent debate, it tends to dominate the decision, which causes the results to converge on the accuracy of the stronger model acting alone."
                },
                "weaknesses": {
                    "value": "While GPT-4 itself is shown to benefit from ReConcile, GPT-4 is not included in most of the baseline algorithms, such as Multi-Agent Debate. So we are left to wonder whether GPT-4 would benefit as much or more from those other baseline. \n\nWhile relative efficiency is discussed (in terms of number of rounds for a given accuracy), token counts are not discussed. This makes it hard for readers to determine how much of the benefit of ReConcile might simply be due to increased tokens (and cost). \n\nThere are clearly many more important evaluations that could be performed. Fortunately the code for the framework is provided for review, so I assume it will be made available as OSS later. This will allow the community to run many more evaluations with new generations of models."
                },
                "questions": {
                    "value": "How often do the agents settle on a single answer? Maybe I missed this detail somewhere."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698972980042,
            "cdate": 1698972980042,
            "tmdate": 1699636684770,
            "mdate": 1699636684770,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6uMIIOMTtl",
                "forum": "Yol6nUVIJD",
                "replyto": "5tCxJbTGfx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5C9y"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the novelty and effectiveness of ReConcile and finding value in our experimental design and ablations. Please find the answers to your questions below.\n\n> **GPT-4 is not included in most of the baselines, such as Multi-Agent Debate.**\n\nPlease note that we do have the baseline numbers with GPT-4 as an agent. Specifically, we want to point you to the caption of Table 4, which shows that Multi-agent Debate with GPT-4 obtains an accuracy of 78.0% and Self-Refine with GPT-4 has an accuracy of 83.7%. Note that these are the most pertinent and strongest baselines. Compared to these, ReConcile (with GPT-4 as an agent) obtains an accuracy of 89%, a significant 5.3\\% improvement over the best baseline.\n\n> **While relative efficiency is discussed (in terms of number of rounds for a given accuracy), token counts are not discussed.**\n\nThis is a great point! Although we do not explicitly talk about token lengths, we have ablations in the paper that point to the fact that ReConcile\u2019s improvements are not because of increased token counts. \n\n- First, in Table 6, we compare the performance of ReConcile with random explanations and convincing explanations. Adding random explanations also increases the token count but significantly underperforms adding convincing examples (by 4%). \n\n- Second, following your suggestion, we also counted the average token length for ReConcile and Multi-agent Debate (w/ ChatGPT). They are 533 and 840 respectively. So, the strongest multi-agent debate baseline has a longer token length than ReConcile. \n\nIn summary, the two major sources of improvement in ReConcile (multi-model and convincing samples) should not be attributed to the increased token length because their respective baselines either have similar or longer token lengths.\n\n> **There are clearly many more important evaluations that could be performed. Fortunately the code for the framework is provided for review, so I assume it will be made available as OSS later.**\n\nYes, we will open-source our code. We are also looking forward to future work building on top of our implementation.\n\n> **How often do the agents settle on a single answer?**\n\nAgents settling on a single answer is basically what we refer to as \"reaching a consensus\". As per Figure 4(b) in the original version, after Round 3 for StrategyQA, all samples reach a consensus. The plot compares the consensus percentage after each round for all methods."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270683554,
                "cdate": 1700270683554,
                "tmdate": 1700270725127,
                "mdate": 1700270725127,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UxJJO0eqM7",
                "forum": "Yol6nUVIJD",
                "replyto": "6uMIIOMTtl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_5C9y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_5C9y"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications and additional information!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700341098358,
                "cdate": 1700341098358,
                "tmdate": 1700341098358,
                "mdate": 1700341098358,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rlrVJRll4C",
            "forum": "Yol6nUVIJD",
            "replyto": "Yol6nUVIJD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6257/Reviewer_b1xo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6257/Reviewer_b1xo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multi-agent framework to improve the reasoning capabilities of LLMs by leveraging multiple rounds of discussion, demonstrations of answer-rectifying human explanations used to convincing other agents, and a confidence-weighted voting mechanism. The discussion phase also includes uncertainties in predictions with recalibrated confidence values. The evaluations of the proposed approach with three diverse agents (Claude, GPT-3.5 turbo and Bard) show significant performance improvements in some of the reasoning benchmarks where majority of the gains come from diversity in agent outputs and the use of convincing samples."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The problem statement of leveraging multiple agents' diversity to improve reasoning is very timely.\n- The paper presents a clear concise definition of the proposed approach as well as ablations of the individual components of the proposed approach in providing gains for reasoning benchmarks. The detailed ablations provide useful insights for the reader in building atop the proposed work.\n- The gains on the StrategyQA benchmark are quite impressive from diversity of different model responses."
                },
                "weaknesses": {
                    "value": "- The recalibration scale used to determine uncertainties in predictions is not backed by experiments"
                },
                "questions": {
                    "value": "- It would be useful to provide individual ablations for demonstrations of answer-rectifying human explanations on each of the models and more qualitative examples of the same to disentangle its effect from diverse model responses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6257/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6257/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6257/Reviewer_b1xo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699386270037,
            "cdate": 1699386270037,
            "tmdate": 1699636684623,
            "mdate": 1699636684623,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fFfOBG9Nuk",
                "forum": "Yol6nUVIJD",
                "replyto": "rlrVJRll4C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer b1xo"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the contributions of our work and acknowledging our experiments and ablations. Please find the responses to your questions below.\n\n> **The recalibration scale used to determine uncertainties in predictions is not backed by experiments**\n\nThanks, please refer to our common response for a detailed discussion on this.\n\n> **It would be useful to provide individual ablations for demonstrations of answer-rectifying human explanations on each of the models.**\n\nThanks for the suggestion! We did not do this experiment specifically because \"convincing or answer-rectifying human explanations\" are used to prompt models to generate explanations that can convince _other_ agents. Hence, when there is only a single model, the role of convincing samples of other agents is unclear. That said, we experimented with including ChatGPT's _own_ convincing samples for its own predictions and observe moderate improvement in accuracy (from 67.3 to 69.0).\n\n> **More qualitative examples of the same to disentangle its effect from diverse model responses**\n\nThanks for the suggestion! Please check Table 11 in the updated version of our paper which shows some examples of generated model reasoning with and without convincing samples. Our analysis suggests that in the presence of convincing samples, agents tend to become more confident of their reasoning and generate explanations that indeed sound more \"_convincing_\" to other agents. For example, the model starts its reasoning with factual statements like \"_Mount Fuji is the highest mountain in Japan_\" and avoids phrases like \"_It is possible that \u2026_\", which may suggest uncertainty."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270617254,
                "cdate": 1700270617254,
                "tmdate": 1700270617254,
                "mdate": 1700270617254,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ud38VGoqUn",
            "forum": "Yol6nUVIJD",
            "replyto": "Yol6nUVIJD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6257/Reviewer_KtVY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6257/Reviewer_KtVY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a RECONCILE framework which leverages different LLM agents to collectively generates answers to reasoning tasks. Specifically, in multiple rounds, LLM agents generate responses and corresponding confidence using specific prompts, discuss in groups and generate explanations that may convince other agents. Experiments conducted on ChatGPT, Bard, and Claude2 achieve better performance on QA (StrategyQA and ECQA) and math problems (GSM8K and AQuA), compared to single-agent baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes an interesting multi-agent multi-round collection method to improve model performance on math and commonsense reasoning tasks. This indicates some complementary information from different trained LLMs and suggest how different LLMs can be applied to solve challenging tasks.\n2. Some ablation studies and experiments (such as the importance of using confidence estimation) are informative and motivating."
                },
                "weaknesses": {
                    "value": "1. There are many moving pieces in the propose method, especially the convincing samples and team answer generation heuristics with confidence estimation. In particular, the method requires choosing samples \"from the training set for which the agent's initial answer is wrong\" brings a very strong bias compared to the baseline methods. Although there are ablation studies, it is still not very convincing what \"complementary benefits\" are provided across different LLMs, and how exactly the multi-agent method is fundamentally better than previous debate or self-consistency. This is more concerning because in Table 3, it seems that although Weighted Vote saturates with 3 rounds, majority vote results keep improving. Furthermore, the proposed confidence rescaling heuristics seem brittle and not clear the impact of this from tasks to tasks.\n2. This paper is mostly evaluated on two major tasks, QA/reasoning (StrategyQA and ECQA) and math (GSM8K and AQuA). Previous reasoning papers (such as self-consistency) were evaluated on a much wider range of tasks to illustrate how generalizable the proposed approach is."
                },
                "questions": {
                    "value": "1. Why is the margin over baselines much larger for StrategyQA and ECQA (while being almost the same on GSM8K and AQuA)?\n2. Are the results in Table 2 and Table 3 consistent (on StrategyQA)? Why is the weighted vote score 79.0 in Table 2 and 78.7 in Table 3?\n3. Can you clarify how grouping works? What are \"distinct categories\"?\n4. From Table 4, why do you think GPT4 + Bard/Claude2 hurt the performance (of GPT-4 + GPT-4) significantly? i.e., why would the powerful model be dramatically impacted by smaller models?\n5. In Section 5.1, how is 3 rounds of Debate with GPT-4 different from RECONCILE with GPT-4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699601106540,
            "cdate": 1699601106540,
            "tmdate": 1699636684489,
            "mdate": 1699636684489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GVjP24AYdn",
                "forum": "Yol6nUVIJD",
                "replyto": "Ud38VGoqUn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KtVY (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for acknowledging the effectiveness of ReConcile and the concerned experiments and ablations. Please find the answers to your questions below.\n\n> **Choosing samples \"from the training set for which the agent's initial answer is wrong\" brings a very strong bias compared to the baseline methods.**\n\nWe want to highlight a few results here that will show that this is not the case.\n\n- First, note that ReConcile has many components to it and \"choosing convincing samples\" is only one of them. Even without these samples, ReConcile outperforms multi-agent baselines. In particular, Table 2 shows that even without access to these convincing samples, ReConcile achieves an accuracy of 74.2% on StrategyQA, substantially higher than single ChatGPT (67.3%) or multi-agent Debate with ChatGPT (66.7%) results. This demonstrates the utility of ReConcile beyond the usage of convincing samples.\n\n- Second, the utility of convincing samples goes beyond the specifics of the ReConcile framework and hence the concept of answer-rectifying explanations, in isolation, should be seen as an important contribution of our paper. Specifically, Table 6 demonstrates that convincing samples provide gains to baselines like multi-agent debate as well, improving the accuracy from 66.7% to 69.5%.\n\n- Third, this argument of bias also holds for papers like Chain-of-Thought prompting, which curate human-written demonstrations of step-by-step reasoning but then compare to \"no-explanation\" baselines. \n\n- Fourth, even if one were to consider usage of convincing samples as bias against the baselines, we only leverage 4 convincing samples in our experiments and hence the cost associated with obtaining those is negligible. \n\n>  **Although there are ablation studies, it is still not very convincing what \"complementary benefits\" are provided across different LLMs.**\n\nConceptually, employing agents belonging to diverse LLMs brings in diversity in their reasoning processes (and the corresponding answers). This, in turn, fosters a better discussion, because if all agents are mostly agreeing on the same answer, there wouldn\u2019t be any discussion. Better discussion leads to better consensus and improved accuracy. Please see our common response for more analysis of this complementarity and diversity of multiple models.\n\n> **How exactly the multi-agent method is fundamentally better than previous debate or self-consistency.**\n\nFigure 1 and Table 1 clearly explain the fundamental differences between ReConcile and other baselines. As also explained in Section 4, ReConcile differs from multi-agent debate in three distinct aspects: (1) usage of convincing samples (2) discussion between agents belonging to diverse models, and (3) usage of confidence estimation to quantify model uncertainty. Compared to self-consistency, ReConcile is a discussion framework while self-consistency employs a majority vote between different solutions from the same underlying model. Hence there is no notion of communication of explanations or iteratively improving/refining them in self-consistency. The ablation study presented in Table 4 methodically evaluates all key components by isolating one at a time, and the results show a notable drop in performance when any component is removed.\n\n> **In Table 3, it seems that although Weighted Vote saturates with 3 rounds, majority vote results keep improving.**\n\nMajority vote does not keep improving. We just chose to show results for 3 rounds in the paper but the results below should clarify that both voting mechanisms saturate in 3-4 rounds.\n\n|            | StrategyQA    |             | GSM8K        |             |\n|------------|--------------|--------------|--------------|--------------|\n| Round      | Majority     | Weighted     | Majority     | Weighted     |\n| Round 0    | 74.2         | 74.3         | 79.3         | 79.6         |\n| Round 1    | 76.3         | 77.0         | 81.0         | 82.7         |\n| Round 2    | 77.1         | 79.0         | 82.7         | 85.0         |\n| Round 3    | 78.0         | 78.7         | 83.3         | 84.3         |\n| Round 4    | 77.3         | 78.7         | 83.0         | 84.0         |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700270478507,
                "cdate": 1700270478507,
                "tmdate": 1700270478507,
                "mdate": 1700270478507,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZY7ZnXOFUg",
                "forum": "Yol6nUVIJD",
                "replyto": "GVjP24AYdn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_KtVY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6257/Reviewer_KtVY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses.\n\nI would like to point out that choosing samples \"from the training set for which the agent's initial answer is wrong\" is fundamentally different from bias in CoT, where annotating wrong examples as a fix would fix some of the errors and directly boost model performance in the same distribution. Given the benefits shown in CoT and various few-shot prompting work, 4 is not \"negligible\".\n\nRegarding the number of rounds, can you provide more information with more rounds? Why do you think different methods saturates at different rounds?\n\nCan you answer my comment above about the whether confidence rescaling is generalizable to different tasks?"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630080624,
                "cdate": 1700630080624,
                "tmdate": 1700630080624,
                "mdate": 1700630080624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]