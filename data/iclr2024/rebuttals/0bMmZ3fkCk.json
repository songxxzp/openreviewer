[
    {
        "title": "NEFTune: Noisy Embeddings Improve Instruction Finetuning"
    },
    {
        "review": {
            "id": "491duBvQEA",
            "forum": "0bMmZ3fkCk",
            "replyto": "0bMmZ3fkCk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8050/Reviewer_kT9q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8050/Reviewer_kT9q"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method for fine-tuning large language models. The authors propose an extremely simple modification to the standard procedure, adding noise to the embedding vectors during fine-tuning. Their method is called NEFTune. The authors experiment with several language models and benchmarks, showing strong results across the board."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper has many strengths.\n\n1. For starters, the simplicity of the method is hard to understate. Researchers and practitioners can try this with only a few lines of code. This is very valuable and a great strength of this paper.\n2. The experiments show strong results across the board. The authors study many models and datasets, and ubiquitously see large gains.\n3. Fine-tuning large language models is a vibrant research direction, and as a paper that advances our understanding and capabilities in such enterprise, I believe this would be of interest to many in the community.\n4. There are many ablations in this work that are informative to the readers.\n5. The paper is clear and well written"
                },
                "weaknesses": {
                    "value": "1. All experiments in this paper are done with autoregressive models. Studying other kinds of models (e.g. BERT, T5) would be a valuable addition to this paper, since these models are still used for many downstream applications today.\n2. The authors don't present error bars in the experiments, which . Fine-tuning language models can be notoriously noisy (e.g. [1]), and precisly understanding the magnitude of the noise in the presented experiments would be very valuable.\n3. There is little analysis on scaling trends. Despite some experiments with QLORA, almost all experiments in the paper are conducted with 7B parameter models. Showing that the gains from the proposed method do not diminish vanish with scale would greatly strengthen the paper, as it would demonstrate it's potential for larger models.\n4. There is little discussion on hyper-parameter tuning. It would be great if authors would show that the comparisons are fair but ensuring the computational budget spent for tuning hyper-parameters for the baseline and their method is equal. One concern is that the baseline is more poorly tuned that their method.\n\n[1] Dodge, Jesse, et al. \"Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.\" arXiv preprint arXiv:2002.06305 (2020)."
                },
                "questions": {
                    "value": "1. Do the authors think this method would also work for pre-training? \n2. In Table 3, do the numbers next to the method name correspond to the value of alpha? If so, I'm surprised to see such a big variance, and also surprised by the non-convex behavior in many cases. Perhaps this also ties with weakness #2.\n3. This is not a question, but I just wanted to thank the authors for starting their bar plots at zero. It is refreshing not to be visually misled."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698449678679,
            "cdate": 1698449678679,
            "tmdate": 1699636994674,
            "mdate": 1699636994674,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lD46vv3b70",
                "forum": "0bMmZ3fkCk",
                "replyto": "491duBvQEA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kT9q"
                    },
                    "comment": {
                        "value": "Thank you for your time, Reviewer kT9q.\n\n> Table 3, do the numbers next to the method name correspond to the value of alpha?\n \nYes, in Table 3 the number next to the method name is the corresponding alpha. Unlike the QLORA setting, we do not see the big variance for the Alpaca dataset under full parameter tunning from Table 6 different alphas have a similar increase in performance. Additionally, for AlpacaEval, we see a Std Err of less than 2% for all models studied.\n\n> There is little analysis on scaling trends.\n\nWe added a $70$B model to Table 2, finding NEFTune increases performance at this scale as well. Using the hyperparameters for supervised finetuning from the LLaMA-2 paper, we finetuned LLaMA-2 70B on Evolve-Instruct, finding that with NEFTune performance increased by about 10%. This illustrates that at larger scales this method can be applied.\n\n> Hyperparameter Tuning \n\nWe did not engage in extensive hyperparameter tuning for every model and dataset instead relying on fixed hyperparameters. However, we did do an initial hyperparameter tuning for the Alpaca model where we improved on the reported model as per the leaderboard from ~26% to ~32% (Adding NEFT puts the performance at ~62%).\n\n> BERT, T5 models\n\nWork like FreeLB, which was compared and inspired NEFTune, has been known to improve model performance on BERT classification tasks--like GLUE. We suggest users use this work and its subsequent works for improving classification tasks, which is well-studied.\n\n> Do the authors think this method would also work for pre-training?\n\nThis is a good question. This is difficult to tell as the tasks are different, and we prefer not to speculate on pretraining.\n\nPlease let us know if you have any more questions."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147100588,
                "cdate": 1700147100588,
                "tmdate": 1700580454948,
                "mdate": 1700580454948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3lk5QPeIVY",
                "forum": "0bMmZ3fkCk",
                "replyto": "lD46vv3b70",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_kT9q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_kT9q"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for responding to some of my comments. Since some of my concerns remain, I'm sticking to my original score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579668447,
                "cdate": 1700579668447,
                "tmdate": 1700579668447,
                "mdate": 1700579668447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vL1VXL25QU",
            "forum": "0bMmZ3fkCk",
            "replyto": "0bMmZ3fkCk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8050/Reviewer_nFQe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8050/Reviewer_nFQe"
            ],
            "content": {
                "summary": {
                    "value": "The authors demonstrate that adding noise to the embedding vectors during finetuning leads to significant performance increase in large language models. The results were demonstrated using two evaluation benchmarks, AlpacaEval and OpenLLM Leaderboard tasks. Only the former demonstrated significant increase where the NEFT models increased the output length while preserving diversity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors demonstrated empirically that a simple regularization of adding uniform noise to the embedding features lead to an average improvement of 15% across 5 datasets.\n- The performance gains were demonstrated using 3 different LLMS, which suggests the generalizability of the method to other architectures."
                },
                "weaknesses": {
                    "value": "- Overgeneralizing performance gains: While the authors demonstrated significant performance gains using the AlpacaEval dataset with 805 instructions, the NEFTune model did not show significant improvements on the OpenLLM Leaderboard tasks (Fig 3), which is more critical since it tests for reasoning and truthfulness. The authors' claim should consider this lack of improvement in their abstract and title so as to not increase the hype of this method without strong foundations. \n\n\n- Contribution of uniform noise is unclear: The authors clarified that longer outputs caused AlpaceEval scores to increase by 16%. To demonstrate that their method did not cause spurious outputs, they evaluated the model output in terms of 2-gram, log-diversity and whitespace lengths (Table 4). I am curious to know how the NEFT method increases output length without increasing 2-gram repetition and maintaining diversity. Is the model output riddled with sentence fillers, and results for 4-gram analysis were not included in the main text? The authors provided a qualitative example of the NEFT output. I am curious to know if this was a specifically chosen sample. The authors conducted human studies of which human annotators preferred the non NEFT in 30 instances (140-(80+22)). It will be helpful to show the output where the human annotators preferred the non-NEFT output over the NEFT ones. Additionally, I do not think Gaussian Noise 5 should be directly compared against Uniform Noise 5? Perhaps other forms of noise distributions should be considered to understand what the uniform noise is doing. \n\n- Need more clarity on computation:  As the authors mentioned in the Conclusion & Limitations, it is unclear why NEFT works on the AlpacaEval and not on the OpenLLM Leaderboard. More evaluation is needed on why there is a huge disparity in performance gains between metrics. Furthermore, a framework or theory on why the uniform noise improves AlpacaEval metric should be explored. Plotting the training and test loss distribution does not add much to the understanding of the computation. A more thorough analysis of why adding uniform noise seemingly improves performance needs to be explored."
                },
                "questions": {
                    "value": "- I am curious to know why are there no performance gains on the OpenLLM Leaderboard tasks?\n- Are there any other forms of regularization at the embedding level that replicates this result?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8050/Reviewer_nFQe",
                        "ICLR.cc/2024/Conference/Submission8050/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614027244,
            "cdate": 1698614027244,
            "tmdate": 1700533970875,
            "mdate": 1700533970875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b9jfMJKGCA",
                "forum": "0bMmZ3fkCk",
                "replyto": "vL1VXL25QU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nFQe Part 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your time, Reviewer nFQe!\n\n> Overgeneralization of Results -- No performance gains on the OpenLLM Leaderboard tasks\n\nWhile we do already discuss a lack of improvement on the leaderboard tasks, we have added an additional line in the abstract itself that states that NEFTune does not seem to improve performance on OpenLLM Leaderboard tasks like MMLU. However, it is important to note two things about the nature of the leaderboard test datasets and the LM-Eval-Harness setup.\n\nFirstly, the rankings suggest that OpenLLM Leaderboard performance is largely dependent on the base model and the relevance of the finetuning dataset. It is unclear how a regularization technique like NEFT (applied to the same base model on the same FT dataset) might impact this process, which is why we approached the question from an empirical standpoint and simply report our findings. \n\nA second important detail that might offer more of a mechanistic explanation, is that the OpenLLM Leaderboard tasks are measured by computing the loglikelihood on each of the candidate answers to the question. Then we check whether the model assigned the highest likelihood to the ground truth answer. This is a standard methodology, originally proposed by the GPT3 paper, and the one used to produce all the results on the leaderboard. However, since this set of evaluations is therefore not _generative_ the model's propensity to generate more thorough and detailed answers to conversational requests, as showcased in AlpacaEval, probably doesn't factor into the model's performance on these tasks. However, we report these results clearly in the main work because we felt it important to show that performance on these evaluations was not negatively impacted under our intervention.  \n\n> Are there any other forms of regularization at the embedding level that replicates this result?\n\nWe found that other techniques that manipuliate the embedding space (i.e FreeLB) improve AlpacaEval performance as well. See Table 9 in the Appendix."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147351812,
                "cdate": 1700147351812,
                "tmdate": 1700147351812,
                "mdate": 1700147351812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hDIPJLt2DQ",
                "forum": "0bMmZ3fkCk",
                "replyto": "vL1VXL25QU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Repsonse to Arjun Singh"
                    },
                    "comment": {
                        "value": "Hello, Arjun. The numbers can be found in the paper, and we will upload them after this work is de-anonymized."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415400564,
                "cdate": 1700415400564,
                "tmdate": 1700415457582,
                "mdate": 1700415457582,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Zhh4ydfJM",
                "forum": "0bMmZ3fkCk",
                "replyto": "vL1VXL25QU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_nFQe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_nFQe"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for addressing my comments. It is still unclear to me why NEFTune differs from any other regularization method. The current manuscript does not offer a theory, conjecture or analysis to give insights on why this specific regularization method  works. I have revised my score to 5 but not more than that as I feel the manuscript does not add to our understanding of why NEFTune works."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533926883,
                "cdate": 1700533926883,
                "tmdate": 1700533985843,
                "mdate": 1700533985843,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qUe8l2ybSJ",
            "forum": "0bMmZ3fkCk",
            "replyto": "0bMmZ3fkCk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8050/Reviewer_Agi9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8050/Reviewer_Agi9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes NEFTune, an augmentation strategy for instruction fine-tuning language models.  NEFTune augments samples by adding uniform random noise to the embeddings.  Fine-tuning language models with NEFTune helps models generate longer responses leading to better performance on OOD conversational and instruction eval tasks without harming knowledge evaluated through Q/A tasks. The authors further suggest that NEFTune performs well by reducing overfitting on fine-tuning datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The main strength of this paper are its strong results on improving conversational ability.  Authors show that across 5 different eval datasets, NEFTune improves performance by 8-35%, while retaining performance on knowledge and reasoning Q/A tasks. The proposed approach is simple to implement requiring only a simple change to add noise to the embeddings of the LLM.  This makes it so that the authors can easily test the method on large models (7B) and varied datasets.\n\n* The authors have also made efforts to demonstrate the effects of adding noise to the embeddings in particular leading to the model generating more text, and leading to higher loss (reduced overfitting).  \n\n* Augmentations in NLP for training LLMs are relatively unexplored.  The proposed method is similar to prior work in vision, but new to LLM training which typically only trains for a small number of epochs to avoid overfitting."
                },
                "weaknesses": {
                    "value": "* While performance looks strong with NEFTune, the authors do not evaluate with other augmentation strategies such as those in https://arxiv.org/abs/2106.07499.  These methods can help with limited datasets and overfitting.  NEFTune can appear stronger with comparison to other augmentation and regularization strategies.\n\n* Experiments are done only with 7B parameter models - primarily LLAMA-2 7B.  While results appear strong, and are applicable to large models, these models are trained on large amounts of data and have learned good embedding spaces.  It will be interesting to know if results with NEFTune scale with model size.  Does NEFTune work with small models and larger models? Particularly for smaller models, where the embedding space may not be as strong, or the model is not as susceptible to overfitting.\n\n* Figure 4 shows that Alpaca loss increases when training with NEFT as this adds noise to the embedding space.  Does this mean that performance will be worse on Alpaca eval?"
                },
                "questions": {
                    "value": "Q1: Given that models have access to the same information, but tend to generate more text, there are some concerns around hallucination the model might be doing.  Have the authors checked this?\n\nQ2: Does NEFTune improve smaller models such as GPT-2? \n\nQ3: Following on Figure 4, is it possible to measure Alpaca training loss and Alpaca validation loss difference? If the model is overfitting and NEFTune reduces overfitting, we should see this discrepancy decrease.  It appears not to be an overfitting argument but smoothing the embedding space where data from the OOD dataset like Evol-Instruct may be."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8050/Reviewer_Agi9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791457048,
            "cdate": 1698791457048,
            "tmdate": 1699636994361,
            "mdate": 1699636994361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OEJmo5WUaf",
                "forum": "0bMmZ3fkCk",
                "replyto": "qUe8l2ybSJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Agi9"
                    },
                    "comment": {
                        "value": "Thank you for your time, Reviewer Agi9.\n\n> Other augmentation techinques \n \nWe did try BPE-dropout which is a subword regularization (https://aclanthology.org/2020.acl-main.170.pdf) in addition to NEFTune and FreeLB (a robust optimization technique). We found that BPE-dropout did not improve the performance of the model with similar or slightly lower performance, but FreeLB did improve performance. Table 9 shows the FreeLB vs NEFTune differences on Evolv-Instruct. For BPE-dropout, we performed initial experiments on Alpaca dataset. Is there a particular one you would like us to run?\n\nSetting| Alpaca | Alpaca+BPE |  Alpaca+FreeLB | Alpaca+NEFTune|\n|-----| -------- | -------- | --------        |----------|\n|AlpacaEval (ChatGPT Evaluator)| 48.5     | 44.16     | 58.5     |    61.7    |\n\n> Hallucination\n\nAs the the model generates more text, the model has more chances to hallucinate. We find that this is true even for NEFTune models. NEFTune models does not help limit the hallucination problem of models.\n\n> More Model Sizes\n \nWe added a $70$B model to Table 2, finding NEFTune increases in perform at this scale as well. Using the hyperparameters for supervised finetuning from the LLaMA-2 paper, we finetuned LLaMA-2 70B on Evolve-Instruct, finding that with NEFTune performance increased by about 10%. This illustrates that at larger scales this method can be applied. We did try to train OPT-1.3B; however, the baseline model outputs degenerated often to the point where evaluating using ChatGPT would result in the flagging OpenAI's content filters too often (https://github.com/tatsu-lab/alpaca_eval/issues/162). Additionally, examining the AlpacaEval Leaderboard, there are almost no instruction models at this scale, which might indicate that 7B+ is really where instruction finetuning has been studied.\n\n> Figure 4 -- Loss/Embedding Smoothing\n\nThis is a good question. We did freeze the embedding layer (Table 8) and allowed the model to train; however, we found equivalent boosts even when freezing the word embedding layer. This would indicate there NEFTune is doing more than just smoothing the embedding. Additionally, we unfortunately used the entire training dataset as per the original Alpaca model, so we do not have a validation split from Alpaca for the models we trained. Additionally, we see that models like OPT-6.7B, which is much weaker model than LLaMA-1 and LLaMA-2, and thus may have a weaker embedding space, that performance on AlpacaEval still increases. Nevertheless the we will add an additional experiment showing the train/eval loss curves during training in a future version of the paper.\n\nPlease let us know if you have any more questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148917677,
                "cdate": 1700148917677,
                "tmdate": 1700148917677,
                "mdate": 1700148917677,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gYIbph1SyW",
                "forum": "0bMmZ3fkCk",
                "replyto": "OEJmo5WUaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_Agi9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_Agi9"
                ],
                "content": {
                    "title": {
                        "value": "Response to author comments"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for addressing many of my comments in the review.  The authors have addressed some concerns with comparison to existing regularization approaches, and addressed the issues with training smaller models.  However, there are still outstanding issues with why significant gains are achieved from NEFTune, and there are not thorough enough measures to fully claim there's no cost to the proposed approach.  \n\nIn particular, there is still outstanding concern that the model is primarily only generating more text, which does not increase knowledge capability, and downstream zero-shot performance on MCQ/retrieval, but may increase on longer-form generation tasks.  As the authors have acknowledged this can also lead to negative effects it the model generates more incorrect information/hallucinations."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637900327,
                "cdate": 1700637900327,
                "tmdate": 1700637900327,
                "mdate": 1700637900327,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "g2qFtDDx8N",
            "forum": "0bMmZ3fkCk",
            "replyto": "0bMmZ3fkCk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces NEFTune, a simple yet effective augmentation technique that improves the finetuning process of language models by adding noise to the embedding vectors during training. This is an empirical paper. The authors demonstrate that this method can substantially improve model performance, showcasing a huge increase from 29.79% to 64.69% on the AlpacaEval task when applied to the LLaMA-2-7B model fine tuned with Alpaca. Furthermore, the paper highlights NEFTune's ability to outperform strong baselines on more instruction datasets, reporting a 10% improvement for models trained with Evol-Instruct, an 8% improvement with ShareGPT, and an 8% improvement with OpenPlatypus. The authors also emphasize that even powerful models that are trained with RLHF, such as LLaMA-2-Chat, can still reap benefits from additional training with NEFTune. Overall, the paper establishes NEFTune as a valuable tool for enhancing the performance of language models across various tasks and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The research highlights a critical need to shift focus in the field of Large Language Models (LLMs) from predominantly concentrating on expanding datasets and model sizes, to also giving due attention to optimizing algorithms and regularization techniques. This is essential for enhancing model generalization and addressing overfitting, particularly when working with smaller instruction datasets. I completely agree with the authors on this. More emphasis must be laid on the regularization based methods for LLMs especially while finetuning on small datasets.\n2) The authors introduce a simple yet effective regularization method of introducing noise into the embedding vector. Their comprehensive experimentation across a variety of instruct finetuning datasets and LLMs comparisons substantiates the effectiveness of their approach.\n3) The authors deduce that an LLM trained with NEFTune yields longer and more comprehensive text generations, supported by qualitative examples where the NEFTune-generated text appears more detailed than its counterpart. However, they raise concerns about potential repetitiveness in these extended outputs. To address this, they employ n-gram and log diversity metrics, ultimately concluding that the text encompasses more information than mere repetition. They also conduct experiments to ascertain whether the generation of longer, comprehensive responses inherently leads to improved model performance. Their findings confirm that NEFTune outperforms this baseline, demonstrating its efficacy.\n4) The authors attribute the substantial performance of NEFTune, surpassing that of baseline methods, to its ability to mitigate overfitting and enhance generalization. They support this claim with experimental results presented in Figures 4 and 5, although I do have some questions regarding this aspect."
                },
                "weaknesses": {
                    "value": "I hope my comments will further strengthen the work.\n\n1) The authors assert that their method outperforms baseline approaches by reducing overfitting. In this context, I would appreciate the inclusion of standard deviation values for the results, calculated over five random seeds. A small standard deviation with NEFTune would indeed validate its efficacy. Conversely, a large standard deviation might indicate that the stochastic nature of the noise added to the embedding layer doesn\u2019t genuinely contribute to a regularization effect. Inclusion of standard deviation is insightful when finetuning an LLM on small datasets with a goal to mitigate overfitting [1, 2].\n\n2) I noticed that the BLEU and Rouge-L scores on the training data for models with NEFTune applied appear to be relatively low. This observation leads me to wonder if the model is able to effectively learn from the training dataset. To gain a more comprehensive understanding of the training process, I would kindly request the inclusion of the training and validation loss curve plotted as a function of iterations for the +NEFT and without NEFT method. This additional information would be greatly appreciated and beneficial for a more thorough evaluation of the model's learning dynamics. \n\n3) I've noticed that in the tables, such as Table 3, terms like \"+NEFT 5\" and similar are mentioned. It would be helpful if these terms could be explained or described directly in the captions of the tables to provide immediate context and clarity for readers. \n\n4) Though adversarial ML literature (Zhu et al., 2019; Kong et al., 2022) has been cited for the choice of the noise \u03b1/\u221aLd. However, since it is the backbone for the entire work, I would suggest the authors to have a detailed discussion about it in the work explaining the reason behind this choice. \n\n[1] Zhang, Haojie, et al. \"Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively.\" Advances in Neural Information Processing Systems 35 (2022): 21442-21454.\n\n[2] Somayajula, Sai Ashish, et al. \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023."
                },
                "questions": {
                    "value": "1) \u201cWhile longer generations do score better, we see that no generation-time strategy comes close to the performance of NEFTune models.\u201d The authors mention this in the paper, however it would be great if they can clarify further about this experiment and display those results in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "details_of_ethics_concerns": {
                    "value": "."
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8050/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8050/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8050/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698915384597,
            "cdate": 1698915384597,
            "tmdate": 1700593560187,
            "mdate": 1700593560187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bEtqZJYk2f",
                "forum": "0bMmZ3fkCk",
                "replyto": "g2qFtDDx8N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h2qY"
                    },
                    "comment": {
                        "value": "Thank you for your time, Reviewer h2qY.\n\n> While longer generations do score better, we see that no generation-time strategy comes close to the performance of NEFTune models.\u201d The authors mention this in the paper, however it would be great if they can clarify further about this experiment and display those results in the paper.\n\nThe sentence refers to the augmentation of the system prompt or changing the generation stragety to force longer outputs. These results can be found in Table 5. \n\n> Std Value of the Results\n\nAlthough we did not run different seeds for the finetuning, we can see from Table 6 that performance of difference noise values for the Alpaca dataset were equivalent. We are running different seeds and will add the results to the discussion if the experiments finish in time. For AlpacaEval, the reported Std Error is less than 2%. \n\n> Clarify \"+NEFT 5\"\n \nThis refers to the alpha hyperparameter of NEFTune. We will make this clear in the updated verison of the paper.\n\n> Learning the Training Dataset\n\nFrom the training curves, we believe the model learns from training dataset. The training curves with noise is very similar to the training loss curve without noise. This suggests that the model is learning the task where the NEFTune models learns the original task plus the added noise, essentially learning a slightly different problem. Futhermore, we see that we recover some the training data when adding noise to the embedding when measuring with ROUGE-L, which can be found in Figure 6 in the updated Appendix.\n\nPlease let us know if you have any more questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700146452626,
                "cdate": 1700146452626,
                "tmdate": 1700146452626,
                "mdate": 1700146452626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vmTbtNCVEl",
                "forum": "0bMmZ3fkCk",
                "replyto": "g2qFtDDx8N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
                ],
                "content": {
                    "comment": {
                        "value": "1) \" generation-time strategy \" This term is usually used for strategy such as beam search decoding, top-k sampling etc. So the result in table-5 does not support the claim.\n\n2) \"Std values\" I am not really convinced by the result in table 6 because of the following reason. By changing the noise statistics the results still hold, which is counter intuitive since there has to be an inflection point beyond which adding noise should hurt the performance. There has to be a detailed study of how and why this algorithm is working. I understand the experiments are running for standard deviation however for this settings of the paper reporting the standard deviation is important. The setting assumed is low resource setting for LLMs and standard deviation defines the reliability of the algorithm proposed. I strongly encourage the authors to report them. \"For AlpacaEval, the reported Std Error is less than 2%.\" this is a vague statement and I would want to see all the numbers.\n\n3) As I mentioned above in the weakness, the authors did not clearly explain why their algorithm works. For instance, in the work stated above [2], they propose to represent a word embedding vector as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix. The authors compare the proposed method with a baseline, BFTSS top-k random S, where they add random noise to the embedding representation of a word that improves robustness. This baseline outperforms the vanilla method however lags behind their method. I see NEFTune to be on the same lines as this work. However in their work, adding random noise outperforms vanilla in only some settings. So as I mentioned above, I urge the authors to inspect what is the noise statistics that improves the performance, why is it contributing to the better performance and also when it will deteriorate the performance. Another suggestion is, in the current form of the work, the authors are just adding noise to the embedding representation. This is uncontrolled noise and the interpretability of the method is less. I suggest the authors to add the baseline where the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix [2]. They do not need to learn for the task-dependent similarity matrix. I believe from table 6 if the method works with a range of noise initialization then the suggested baseline should perform at par or even better as observed in [2]. It also brings some interpretability to the table.\n\n[2] \"Bi-level Finetuning with Task-dependent Similarity Structure for Low-resource Training.\" Findings of the Association for Computational Linguistics: ACL 2023. 2023.\n\nI appreciate the authors for their research and especially recognizing the need to regularize to boost the performance. However, the current form of the work, as I mentioned in my previous comments, does not provide any understanding of why NEFTune works. The approach seems to be inspired from the previous methods in the literature, though a detailed comparison with those methods is not offered.  The standard deviation is necessary for methods dealing with low-resource. However, the results look promising and the paper is more on the empirical end. \n\nNevertheless, since all my comments are not adequately answered, I would like to reduce my score from 8->6. That said, I recommend the authors to revise the work thoroughly since I believe it has a good potential."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593529195,
                "cdate": 1700593529195,
                "tmdate": 1700593837309,
                "mdate": 1700593837309,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SYFOJLiZ5q",
                "forum": "0bMmZ3fkCk",
                "replyto": "WdBBAJYlV5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8050/Reviewer_h2qY"
                ],
                "content": {
                    "comment": {
                        "value": "Firstly, thank you for your response. It does clarify some of my questions. \n\n1) I agree that running the experiments to get the standard deviation would take some time. I request the authors to include those numbers in the final revision. The provided empirical evidence for a model trained on Alpaca dataset under a noise statistics does offer a flavor however I believe we will have to observe the results on all the settings to comment on the robustness.\n\n2) \"We have now included this table with the full range of noise values as Figure 11 in the Appendix.\" It is insightful and does offer  more understanding of the method. Thank you!\n\n3) I request the authors to add a paragraph in the methods section (sec 2) offering more intuitive explanation of what inspires them to choose the noise \u03b1/\u221aLd. Since it is the backbone for the entire work, I would suggest the authors to have a detailed discussion about it in the work explaining the reason behind this choice theoretically. I think including this will make it more understanding and interpretable.  \n\n\nHowever I have a few follow-up questions, \n\n1) I agree with the provided explanation for the impact of noise in the method \"In Figure 7, we see that for the optimal level of noise, when the noisy embeddings are projected back to their nearest neighbor in the embedding matrix, no flips to neighboring tokens occur. In the range of alphas where we observe a significant fraction of flips to neighboring tokens, the performance of NEFTune is reduced.\" \n\nHowever it makes me more convinced to see the performance of the method against the baseline in [2] atleast on one dataset and a model. No need to learn for a task specific similarity matrix. Just initialize the similarity matrix using the cosine similarity of a pretrained model embedding matrix. Because the embedding vector of a word is represented as a linear combination of all the related word embedding vectors weighted by the entries of a similarity matrix initialized using cosine similarity of some pre-trained model embedding matrix and going by the explanation provided there is an euclidean sphere of radius r (determined by the noise) around a particular word that when added to the embedding vector improves the robustness. I believe that such a euclidean sphere is better captured by the entries of a similarity matrix (in other words all related words) than a heuristically chosen noise.\n\n2) \"Aside from this, baseline regularization strategies such as weight decay and dropout are standard in language model finetuning, but while these approaches are targets for hyperparameter tuning, they are known to have no drastic effects on chat model performance.\" I think this statement is over generalizing, would be recommended to back it with references from literature. Also ** these approaches are targets for hyperparameter tuning**, I think the proposed method also has hyper parameters to tune such as $\\alpha$.\n\nThat said, I think this is a strong work but contingent upon inclusion of all the suggestions provided here and also by other reviewers. The major reason is that the interpretability of the proposed method is not clear. What are the reasons behind the choice (\u03b1/\u221aLd) and how it is achieving such huge gains. On the other hand, I am equally excited because the method is very simple to implement and requires no computational overhead and also provides good gains."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8050/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699007963,
                "cdate": 1700699007963,
                "tmdate": 1700699007963,
                "mdate": 1700699007963,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]