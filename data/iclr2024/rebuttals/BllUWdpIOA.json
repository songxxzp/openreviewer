[
    {
        "title": "Continual Momentum Filtering on Parameter Space for Online Test-time Adaptation"
    },
    {
        "review": {
            "id": "SBymNhwyGf",
            "forum": "BllUWdpIOA",
            "replyto": "BllUWdpIOA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1712/Reviewer_aPnP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1712/Reviewer_aPnP"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on online test time adaptation and introduces a Kalman Filter (KF) based approach on momentum filtering of DNNs.  The proposed method, referred to as CMF,  integrates a SGD-based optimization process with a KF-based inference (filtering) process. Some simplification techniques, such as momentum smoothing, have been employed to reduce time complexity. The experimental results demonstrate the competitiveness of the proposed method across multiple datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper studies a practical and important problem: online test-time adaptation, focusing on mitigating catastrophic forgetting during adaptation.\n\nThe proposed method, which employs a KF-based approach to filter the momentum parameter of DNNs, provides a sound solution to mitigate catastrophic forgetting in online test-time adaptation.\n\nOverall, the paper is well-structured and effectively communicates the details of the proposed method.\n\nFurthermore, in the experimental evaluation, the proposed method demonstrates superior performance compared to previous approaches, as reported in the paper."
                },
                "weaknesses": {
                    "value": "1) This paper could benefit from a comparison with related works that utilize the Kalman filter for online adaptation tasks, as seen in [1,2].  Such a comparison or discussion regarding the difference between the proposed method and [1,2] would be beneficial.\n\n2) It is very common to use replay-based approaches to overcome catastrophic forgetting, such as ER [3] and A-GEM [4].  An analysis of the advantages and disadvantages of the proposed approach in contrast to replay-based methods would provide a more comprehensive understanding of how it addresses the issue of catastrophic forgetting.\n\n3) In the alation study on \u201cEffectiveness of the source-conjugated transition model\u201d, do the hyperparameters alpha and gamma remain the same? It is better to explore the effects of hyperparameters alpha and gamma individually. Since the roles of alpha and gamma are distinct and independent, isolating their impact would provide a more detailed understanding.\n\n4) The paper simplifies the inference process by using scalar parameters to replace matrix parameters in the KF process. It would be helpful to conduct an experiment on a smaller model to compare the performance of the simplified version with the original matrix version of the method. \n\n[1] A. Abuduweili, et al, \u201cRobust online model adaptation by extended kalman filter with exponential moving average and dynamic multi-epoch strategy\u201d, L4DC 2020.  \n[2] Y. Cheng, et al. \"Human motion prediction using semi-adaptable neural networks.\" ACC, 2019.  \n[3] A. Chaudhry, et al \u201cOn tiny episodic memories in continual learning\u201d, arxiv 2019.  \n[4] A. Chaudhry, et al \u201cEfficient lifelong learning with a-gem\u201d, ICLR 2018."
                },
                "questions": {
                    "value": "1) How does the proposed method compare to related Kalman filter-based techniques for adaptation, as discussed in [1,2]?\n\n2) What are the advantages and disadvantages of the proposed approach in contrast to replay-based methods [3,4] for mitigating catastrophic forgetting?\n\n3) In the alation study on \u201cEffectiveness of the source-conjugated transition model\u201d, do the hyperparameters alpha and gamma remain the same? It is better to explore the effects of hyperparameters alpha and gamma individually. \n\n4) What is the performance and computational cost comparison between the simplified scalar version of the KF process and the original matrix version of the KF process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1712/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1712/Reviewer_aPnP",
                        "ICLR.cc/2024/Conference/Submission1712/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1712/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681807477,
            "cdate": 1698681807477,
            "tmdate": 1700667666655,
            "mdate": 1700667666655,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EcP2u3RrjS",
                "forum": "BllUWdpIOA",
                "replyto": "SBymNhwyGf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aPnP [1/2]"
                    },
                    "comment": {
                        "value": "### **Overall**\n\nWe have thoroughly reviewed and considered the insightful questions posed by reviewer **aPnP**. Our efforts have been directed towards enhancing the quality and clarity of our paper through this review process.\n\n**By adding a section on related works in Appendix D.1 and D.2,** we have addressed the papers mentioned in your questions and clarified the distinctions between CMF and other studies that have applied Bayesian filters to different adaptation tasks.\n\nOur focus is on the methods of Online Test-Time Adaptation (OTTA), as discussed in Section 2.1, which are characterized as unsupervised source-free domain adaptations. These methods require immediate adaptation to samples of real-time streaming data, where there are no labels for the target data, and the use of source data is not feasible. Additionally, multiple epochs are not allowed, necessitating immediate adaptation to each sample and subsequent prediction.\n\nIn this context, we have endeavored to comprehensively answer the questions raised by the reviewer.\n\n### **Response**\n\n**Q1. How does the proposed method compare to related Kalman filter-based techniques for adaptation, as discussed in [1,2]?**\n\n**A1 (other Kalman filter).** We have not found studies applying Bayesian filters like KF or EKF to OTTA. **The studies [1, 2]** focus on multi-epoch adaptation problems with **observations being natural data (e.g., images) or labels (e.g., labels for human motion)**. However, **CMF** focuses on OTTA problems with **DNN\u2019s parameter $\\theta_t$ as the observation.**\n\n\u2192 [1] designs the emission model $\\mathcal{N}(Y|f(\\theta_{t-1}, X_{t-1}), I)$ with a non-linear function $f$ since natural data or labels as input $X$ have a non-linear relationship. In contrast, CMF\u2019s emission model $p(\\theta^{(t)}|\\phi^{(t)})=\\mathcal{N} (\\theta^{(t)}|H\\phi^{(t)}, R)$ targets DNN\u2019s parameters, and **the relationship is often set as a linear system in previous studies [a, b, c].**\n\n\u2192 EKF requires linear approximation of non-linear functions, often leading to instability and reduced accuracy.\n\n\u2192 CMF, on the other hand, **does not require linear approximation with EKF**, ensuring stability. Furthermore, the inverse matrix operation of KF is removed in the simplified inference process (Section 3.4), allowing for stable operations. **Thus, CMF is numerically stable, simple, and has a relatively low computation cost. Additionally, unlike [1, 2], CMF introduces KF to refine noisy parameters rather than noisy labels.**\n\n[a] Izmailov, Pavel et al. \u201cAveraging Weights Leads to Wider Optima and Better Generalization.\u201d\u00a0*Conference on Uncertainty in Artificial Intelligence*\u00a0(2018).\n\n[b] Garipov, Timur, et al. \"Loss surfaces, mode connectivity, and fast ensembling of dnns.\"\u00a0*Advances in neural information processing systems*\u00a031 (2018).\n\n[c] Guo, Hao, Jiyong Jin, and Bin Liu. \"Stochastic weight averaging revisited.\"\u00a0*Applied Sciences*\u00a013.5 (2023): 2935.\n\n**Q2. What are the advantages and disadvantages of the proposed approach in contrast to replay-based methods [3,4] for mitigating catastrophic forgetting?**\n\n**A2 (overcoming catastrophic forgetting).**  We have not identified studies actively using methods like A-GEM or ER in OTTA. Fundamentally, OTTA cannot use source domain data, creating ambiguity in defining 'past tasks' for ER or A-GEM approaches.\n\n\u2192 In OTTA, information about past tasks is typically derived from gradients or data from the source domain, which is generally not permissible in OTTA scenarios. An alternative approach would be to identify past tasks within streaming data for applying A-GEM or ER, but this requires additional domain-aware methods, complicating the application.\n\n\u2192 However, there are OTTA methods like [d] that use memory similar to ER, storing part of the streaming data during adaptation and balancing classes among samples to mitigate catastrophic forgetting. Yet, this approach has downsides in terms of privacy, as mentioned in Section 1.\n\n\u2192 **For these reasons, as mentioned in Section 2.2, most OTTA methods continuously inject source model parameters into target model parameters during the adaptation process. In contrast, our proposed CMF method computes a new source model $\\phi_t$ (i.e., the hidden model), inferred from both the target and source models, to mitigate the issue of static source models overly relying on past tasks.**\n\n\u2192 Nonetheless, actively introducing ER or A-GEM into OTTA could be a valuable follow-up study, as it allows explicit control over past information.\n\n[d] Gong, Taesik, et al. \"NOTE: Robust continual test-time adaptation against temporal correlation.\"\u00a0*Advances in Neural Information Processing Systems*\u00a035 (2022): 27253-27266."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148938095,
                "cdate": 1700148938095,
                "tmdate": 1700536556018,
                "mdate": 1700536556018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iR22gwXKvu",
                "forum": "BllUWdpIOA",
                "replyto": "SBymNhwyGf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer aPnP [2/2]"
                    },
                    "comment": {
                        "value": "**Q3. In the ablation study on 'Effectiveness of the source-conjugated transition model', do the hyperparameters alpha and gamma remain the same? It is better to explore the effects of hyperparameters alpha and gamma individually.**\n\n**A3.** In the ablation study, as indicated in the caption of Figure 2, we initially fixed $\\gamma=0.99$ and varied $\\alpha$ to assess its impact. Following your suggestion, we recognized the value in exploring the effects of varying $\\gamma$ while keeping $\\alpha$ constant. \n\n\u2192 Consequently, we conducted additional experiments with $\\alpha=0.99$ fixed and varying $\\gamma$. The results are as follows:\n\n| Model | Source | TENT | 0.999 | 0.99 | 0.95 | 0.9 | 0.8 |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| ViT | 60.2 | 54.5\u00b10.04 | 45.2\u00b10.11 | 44.8\u00b10.12 | 45.5\u00b10.09 | 46.7\u00b10.07 | 49.18\u00b10.04 |\n| Swin | 64.0 | 64.0\u00b10.14 | 47.1\u00b10.21 | 46.6\u00b10.12 | 47.2\u00b10.14 | 48.5\u00b10.20 | 50.68\u00b10.28 |\n| D2V | 51.8 | 51.9\u00b10.09 | 43.2\u00b10.22 | 43.5\u00b10.04 | 45.3\u00b10.04 | 46.8\u00b10.06 | 48.64\u00b10.11 |\n\n\u2192 The results of these experiments, which provide insights into the individual effects of these hyperparameters, have been incorporated into Appendix C.5.\n\n**Q4. What is the performance and computational cost comparison between the simplified scalar version of the KF process and the original matrix version of the KF process?**\n\n**A4.** Implementing the matrix version of CMF requires handling a matrix of size $d^2$, where $d$ represents the dimension of the target parameter, which in our experiments ranged from $128$ to $1024$. The KF process necessitates matrix inversion for all target DNN's parameters to calculate the Kalman gain $K$. For example, for the ViT model, the calculation is required for $50$ matrices. However, due to the substantial size of this matrix, computations using the PyTorch toolkit are not only exceedingly slow but also prone to errors, leading to numerical instability.\n\n\u2192 Unfortunately, these limitations within the toolkit render conducting experiments with the original matrix version of KF currently unfeasible. To circumvent this, CMF introduces a simplified inference process in Section 3.4. Our extensive experiments have demonstrated that this scalar setting significantly enhances performance with only a minimal increase in computational cost. The results of these experiments are detailed in Table 1 - 5 and Appendix C.1. \n\n-> To illustrate the computational efficiency of the scalar version, we also provide actual execution times (in seconds) for each model and OTTA method in the TC-CS scenario on the ImageNet-C dataset On average, CMF consumed a very small 0.55% additional computation compared to the highest performing recorded ROID.\n\n| Method | Model |  |  |\n| --- | --- | --- | --- |\n|  | ViT | Swin | D2V |\n| CMF | 159.4 | 255.2 | 162.0 |\n| ROID | 158.6 | 254.4 | 160.6 |\n| SAR | 181.2 | 294.1 | 180.8 |\n| EATA | 112.6 | 176.8 | 64.0 |\n| TENT | 102.5 | 153.6 | 101.3 |\n\n\n\u2192 Furthermore, it is important to note that even in its scalar form, CMF continues to perform exact Bayesian filtering for the linear Gaussian model, ensuring the accuracy and reliability of the inference processes."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148978966,
                "cdate": 1700148978966,
                "tmdate": 1700613364849,
                "mdate": 1700613364849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QI2eUAIJAc",
                "forum": "BllUWdpIOA",
                "replyto": "SBymNhwyGf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer aPnP,\n\nThank you again for your insightful review of our submission. We have consistently endeavored to provide detailed responses to address the concerns you have raised along your commnets. Your valuable comments have also helped us to make our study more robust and clear.\n\n\nThere is now one day left in the author-reviewer discussion period, during which we hope to have the reviewer review our response and revised submission. If you have any additional comments, we will do our best to answer any questions.\n\n\nWe sincerely appreciate your efforts and time in reviewing our submission.\n\n\nAuthor(s)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627353247,
                "cdate": 1700627353247,
                "tmdate": 1700627438248,
                "mdate": 1700627438248,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wOFfTaqPSB",
                "forum": "BllUWdpIOA",
                "replyto": "QI2eUAIJAc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Reviewer_aPnP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Reviewer_aPnP"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your comprehensive response to my inquiries.\nYou have addressed most of my questions/concerns. After reading the rebuttal and other reviews, I am inclined to adjust my initial score from 6 to 8."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667681326,
                "cdate": 1700667681326,
                "tmdate": 1700667681326,
                "mdate": 1700667681326,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RwEvPYNfo2",
                "forum": "BllUWdpIOA",
                "replyto": "SBymNhwyGf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your positive feedback with score updating"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback and positive support. \n\nWe're glad we were able to address your concerns."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701394308,
                "cdate": 1700701394308,
                "tmdate": 1700701394308,
                "mdate": 1700701394308,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7SsKRx5rBg",
            "forum": "BllUWdpIOA",
            "replyto": "BllUWdpIOA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1712/Reviewer_gT9R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1712/Reviewer_gT9R"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel framework called Continual Momentum Filtering (CMF) that leverages the Kalman Filter to strike a balance between model adaptability and information retention. The CMF framework alleviates the catastrophic forgetting issue and provides high adaptability to shifting data distributions. The paper provides examples of real-world situations where the CMF framework has been validated, including scenarios involving covariate and label shifts in speech recognition tasks and ImageNet-C. The results show that the CMF consistently outperforms state-of-the-art methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The investigated problem, namely adaptability and anti-forgetting tradeoff, is practical for the real-world deployment of TTA methods. The resulting CMF framework is simple yet effective. \n\nExperimental evaluations on various models, datasets and scenarios are thorough and demonstrate the effectiveness of the proposed framework."
                },
                "weaknesses": {
                    "value": "The performance gains compared with ROID are a bit marginal."
                },
                "questions": {
                    "value": "Are there any sensitivity analyses regarding the parameter $I$ in algorithm 1?\n\nI am curious about the performance of replacing the \u201cweight ensemble\u201d in ROID with CMF\uff1f (namely ROID with CMF).\n\nHow about the performance of \u201cDW-SLR + SCE\u201d without CMF?\n\nHow about the in-distribution performance of compared methods after the adaptation of out-of-distribution data? Please refer to the comparison manner proposed in EATA.\n\nCould the authors provide a computational complexity (wall-clock GPU time) comparison regarding the proposed CMF? \n\nCould CMF help MEMO work stably in the online setting? \n\nMore motivation/explanations from the high level about why CMF could achieve a better tradeoff between adaptability and information retention are preferred.\n\nCould the authors provide implementation details (hyperparameters) of baselines on different models and datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1712/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1712/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1712/Reviewer_gT9R"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1712/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764054061,
            "cdate": 1698764054061,
            "tmdate": 1699636099896,
            "mdate": 1699636099896,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Tb3EWPGeF8",
                "forum": "BllUWdpIOA",
                "replyto": "7SsKRx5rBg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gT9R [1/2]"
                    },
                    "comment": {
                        "value": "### **Overall**\n\nWe have carefully reviewed the detailed questions the reviewer **gT9R** provided regarding the robustness of our experiments and have made every effort to respond to these questions. We have added several experimental results to our paper to enhance its robustness, as the reviewer suggested.\n\n### **Response**\n\n**Q1. Are there any sensitivity analyses regarding the parameter in algorithm 1?**\n\n\u2192 Thank you for suggesting sensitivity analyses. We expanded the experiments in Section 4's ablation study and added the details in Appendix C.5. These experiments in the TC-CS scenario on ImageNet-C involved varying one hyperparameter at a time for parameters in algorithm 1 $(q, \\alpha, \\gamma)$. \n\nSensitivity Analysis for $q$:\n| Model | Source | TENT | CMF ($q$) |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  | 0.01 | 0.0075 | 0.005 | 0.0025 | 0.0001 |\n| ViT | 60.2 | 54.5\u00b10.04 | 44.9\u00b10.09 | 44.9\u00b10.09 | 44.8\u00b10.12 | 44.8\u00b10.14 | 44.9\u00b10.05 |\n| Swin | 64.0 | 64.0\u00b10.14 | 46.7\u00b10.25 | 46.5\u00b10.05 | 46.6\u00b10.12 | 46.5\u00b10.18 | 46.9\u00b10.18 |\n| D2V | 51.8 | 51.9\u00b10.09 | 43.4\u00b10.08 | 43.4\u00b10.09 | 43.5\u00b10.04 | 43.7\u00b10.15 | 44.5\u00b10.02 |\n\nSensitivity Analysis for $\\alpha$:\n| Model | Source | TENT | CMF ($\\alpha$) |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  | 0.999 | 0.99 | 0.95 | 0.9 | 0.8 |\n| ViT | 60.2 | 54.5\u00b10.04 | 45.2\u00b10.03 | 44.8\u00b10.12 | 44.7\u00b10.06 | 44.9\u00b10.09 | 44.9\u00b10.06 |\n| Swin | 64.0 | 64.0\u00b10.14 | 46.9\u00b10.12 | 46.6\u00b10.12 | 46.7\u00b10.13 | 46.8\u00b10.24 | 47.1\u00b10.20 |\n| D2V | 51.8 | 51.9\u00b10.09 | 43.6\u00b10.17 | 43.5\u00b10.04 | 44.2\u00b10.11 | 44.6\u00b10.04 | 44.8\u00b10.04 |\n\nSensitivity Analysis for $\\gamma$:\n| Model | Source | TENT | CMF ($\\gamma$) |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  | 0.999 | 0.99 | 0.95 | 0.9 | 0.8 |\n| ViT | 60.2 | 54.5\u00b10.04 | 45.2\u00b10.11 | 44.8\u00b10.12 | 45.5\u00b10.09 | 46.7\u00b10.07 | 49.18\u00b10.04 |\n| Swin | 64.0 | 64.0\u00b10.14 | 47.1\u00b10.21 | 46.6\u00b10.12 | 47.2\u00b10.14 | 48.5\u00b10.20 | 50.68\u00b10.28 |\n| D2V | 51.8 | 51.9\u00b10.09 | 43.2\u00b10.22 | 43.5\u00b10.04 | 45.3\u00b10.04 | 46.8\u00b10.06 | 48.64\u00b10.11 |\n\n\u2192 The results showed that $\\gamma$ had the most significant impact. These results are added in Appendix C.5.\n\n**Q2. I am curious about the performance of replacing the \u201cweight ensemble\u201d in ROID with CMF (namely ROID with CMF).**\n\n\u2192 As mentioned in Section 4, CMF currently uses the same loss as ROID. Thus, the performance of CMF is equivalent to that of ROID with the weight ensemble replaced by CMF. This is listed in Table 1 - 4.\n\n**Q3. How about the performance of \u201cDW-SLR + SCE\u201d without CMF?**\n\n\u2192 The experiment you mentioned was conducted in the TC-CS scenario on the ImageNet-C dataset. The results are as follows:\n\n|  |  | ImageNet-C | D109 | Rendition | Sketch |\n| --- | --- | --- | --- | --- | --- |\n| DW-SLR + SCE (w/o CMF) | ViT | 47.6\u00b10.06 | 48.3\u00b10.92 | 44.6\u00b10.25 | 58.4\u00b10.05 |\n|  | Swin | 51.1\u00b10.30 | 50.0\u00b10.10 | 47.0\u00b10.14 | 59.1\u00b10.18 |\n|  | D2V | 46.3\u00b10.15 | 47.4\u00b10.41 | 42.2\u00b10.13 | 55.6\u00b10.07 |\n| DW-SLR + SCE + WE | ViT | 45.0\u00b10.09 | 45.0\u00b10.04 | 44.2\u00b10.13 | 58.6\u00b10.04 |\n|  | Swin | 47.0\u00b10.26 | 45.1\u00b10.10 | 46.0\u00b10.10 | 58.9\u00b10.11 |\n|  | D2V | 44.8\u00b10.01 | 44.2\u00b10.06 | 41.8\u00b10.11 | 56.2\u00b10.05 |\n| DW-SLR + SCE + CMF | ViT | 44.8\u00b10.12 | 43.4\u00b10.07 | 42.7\u00b10.20 | 57.0\u00b10.08 |\n|  | Swin | 46.6\u00b10.12 | 43.6\u00b10.12 | 44.1\u00b10.24 | 56.7\u00b10.13 |\n|  | D2V | 43.5\u00b10.04 | 42.3\u00b10.11 | 40.0\u00b10.06 | 53.9\u00b10.03 |\n\n\u2192 **The weight ensemble (WE) method, using a fixed source model, did not show significant performance improvements on the Rendition and Sketch datasets**. This empirically demonstrates the problem we raised in Section 2.2 about dependency on past tasks due to the fixed source model.\n\n\u2192 **Replacing WE with CMF** allows for a **balance between preserving past information and adapting to the current task**, thus securing superior performance across all datasets.\n\n\u2192 **These experimental details are added in Appendix C.3.**\n\n**Q4. How about the in-distribution performance of compared methods after the adaptation of out-of-distribution data? Please refer to the comparison manner proposed in EATA.**\n\n\u2192 We conducted experiments on both in-distribution (Clean) and out-of-distribution (Corrupt) data. In the TC-CS scenario, we used the ImageNet test dataset for Clean and the entire ImageNet-C for Corrupt. After completing a specific domain in the OTTA process, we measured performance on both Clean and Corrupt datasets.\n\n\u2192 **The results are added in Appendix C.4, Figure 5**. For Clean data, EATA, ROID, and CMF all showed a **tendency to converge to a similar average error rate**. However, for Corrupt data, both EATA and ROID lost past information and converged to a certain average error rate, failing to effectively utilize past information. In contrast, **CMF continuously reduced the average error rate**.\n\n\u2192 In conclusion, CMF demonstrated the ability to continuously accumulate knowledge through balancing past and current information."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149144977,
                "cdate": 1700149144977,
                "tmdate": 1700454254139,
                "mdate": 1700454254139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ib2kjyFl53",
                "forum": "BllUWdpIOA",
                "replyto": "7SsKRx5rBg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gT9R [2/2]"
                    },
                    "comment": {
                        "value": "**Q5. Could the authors provide a computational complexity (wall-clock GPU time) comparison regarding the proposed CMF?**\n\n\u2192 This is the computational complexity (in second) for each model and OTTA method in the TC-CS scenario on the ImageNet-C dataset. \n\n| Method | Model |  |  |\n| --- | --- | --- | --- |\n|  | ViT | Swin | D2V |\n| CMF | 159.4 | 255.2 | 162.0 |\n| ROID | 158.6 | 254.4 | 160.6 |\n| SAR | 181.2 | 294.1 | 180.8 |\n| EATA | 112.6 | 176.8 | 64.0 |\n| TENT | 102.5 | 153.6 | 101.3 |\n\n\u2192 Despite using the same code as the official GitHub, SAR reported high computational complexity. On average, CMF consumed a very small 0.55% additional computation compared to the highest performing recorded ROID.\n\n**Q6. Could CMF help MEMO work stably in the online setting?**\n\n**A6.** Thank you for raising this interesting question regarding the application of CMF in an online setting for MEMO. In response to your query, we conducted experiments applying CMF to MEMO in an online (i.e., non-episodic) setting. These experiments were carried out using the ViT model on the ImageNet-C dataset under the TC-CS scenario.\n\n\u2192 For these experiments, we adhered to the learning rate of $0.00001$ as originally used in MEMO, with a batch size of $1$. We employed AugMix for augmentation, increasing the batch size to $64$. Other experimental conditions followed the basic hyperparameters mentioned in Section 4. The results of these experiments are as follows:\n\n| Method | gaussian | shot | impulse | defocus | glass | motion | zoom | snow | frost | fog | bright | contrast | elastic | pixelate | jpeg | AVG. |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| MEMO | 71.1 | 96.7 | 99.0 | 91.5 | 99.5 | 99.3 | 99.7 | 99.8 | 99.9 | 99.9 | 99.6 | 99.9 | 99.9 | 99.9 | 99.9 | 97.0\u00b10.11 |\n| MEMO + CMF | 65.0 | 64.8 | 62.1 | 68.5 | 73.0 | 62.8 | 65.4 | 57.3 | 46.5 | 50.4 | 29.0 | 73.4 | 57.0 | 57.1 | 48.3 | 58.7\u00b10.01 |\n\n\u2192 The results indicate that integrating CMF with MEMO in an online setting effectively mitigates catastrophic forgetting. This finding is significant as it demonstrates the potential of CMF to enhance the stability and performance of MEMO in continuous learning environments. We believe these results contribute valuable insights into the adaptability and robustness of CMF when applied to different learning scenarios.\n\n\n**Q7. More motivation/explanations from the high level about why CMF could achieve a better tradeoff between adaptability and information retention are preferred.**\n\n\u2192 For a high-level explanation, we added **comparison figures and related works in Appendix D.1** to enhance the clarity of the tradeoff we claim. Thank you for the excellent advice.\n\n**Q8. Could the authors provide implementation details (hyperparameters) of baselines on different models and datasets?**\n\n\u2192 For fairness, we followed the hyperparameters of the existing benchmarks mentioned in Section 4, and all hyperparameters are disclosed in the supplemental materials `cfg/*.yaml` and `conf.py`. These values are fixed for datasets and vary only in learning rate for models. As disclosed in Appendix B.1 and B.2, \u201c\u2026 the learning rate was set to $0.00025$ for ViT and Swin, $0.0002$ for ResNet-50, and $0.0001$ for D2V.\u201d"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149166999,
                "cdate": 1700149166999,
                "tmdate": 1700570209674,
                "mdate": 1700570209674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TEsVYcMdqq",
                "forum": "BllUWdpIOA",
                "replyto": "7SsKRx5rBg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer gT9R ,\n\nThank you again for your insightful review of our submission. We have consistently endeavored to provide detailed responses to address the concerns you have raised along your commnets. Your valuable comments have also helped us to make our study more robust and clear.\n\n\nThere is now one day left in the author-reviewer discussion period, during which we hope to have the reviewer review our response and revised submission. If you have any additional comments, we will do our best to answer any questions.\n\n\nWe sincerely appreciate your efforts and time in reviewing our submission.\n\n\nAuthor(s)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627385651,
                "cdate": 1700627385651,
                "tmdate": 1700627461526,
                "mdate": 1700627461526,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hn96yqs321",
            "forum": "BllUWdpIOA",
            "replyto": "BllUWdpIOA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1712/Reviewer_iWLp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1712/Reviewer_iWLp"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the  Online Test-time Adaptation (OTTA) by applying the Kalman filter to infer the test-time \"model parameters\". The proposed method is evaluated over various datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Casting the OTTA challenge as a Bayesian filtering issue presents an interesting approach. The incorporation of the Kalman filter (KF) to facilitate this is noteworthy, as it offers a closed-form solution for posterior inference. However, the inherent linear assumption of the KF may not align well with many real-world scenarios.\n\n- The research commendably evaluates the proposed method across a spectrum of image classification and speech recognition tasks, and the results indicate reasonable enhancements."
                },
                "weaknesses": {
                    "value": "- Utilizing the Kalman filter for sequential model parameter inference might not be optimal given its inherent assumptions. The Kalman filter operates under the presumption of linear system dynamics and posits a Gaussian distribution for the posterior. This is often misaligned with real-world scenarios where state posteriors frequently exhibit multimodal distributions.\n\n- While the exploration of the Kalman filter and other Bayesian filtering techniques for model adaptation/TTA is not entirely novel, it is crucial to delineate this work from previous contributions. It's recommended to rigorously compare, both theoretically and empirically, with established works like EKF[1] and PFDE[2]. Such a comparative analysis can better spotlight this paper's unique technical contributions.\n\n- In Section 3.4, the paper discusses simplifying Bayesian filtering computations. An analysis evaluating the accuracy of these approximations is warranted.\n\n[1] Abuduweili, A., & Liu, C. (2020, July). Robust online model adaptation by extended kalman filter with exponential moving average and dynamic multi-epoch strategy. In Learning for Dynamics and Control (pp. 65-74). PMLR.\n[2] Huang, H., Gu, X., Wang, H., Xiao, C., Liu, H., & Wang, Y. (2022). Extrapolative continuous-time bayesian neural network for fast training-free test-time adaptation. Advances in Neural Information Processing Systems, 35, 36000-36013."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1712/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1712/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1712/Reviewer_iWLp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1712/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765464269,
            "cdate": 1698765464269,
            "tmdate": 1700243602453,
            "mdate": 1700243602453,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7ApeWtIwun",
                "forum": "BllUWdpIOA",
                "replyto": "hn96yqs321",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iWLp [1/2]"
                    },
                    "comment": {
                        "value": "### **Overall**\n\nWe have meticulously reviewed the questions raised by the reviewers **iWLp** and have endeavored to enhance our paper through this process. **By incorporating a section on related works in Appendix D.2 and Limitations of Appendix E,** we have addressed the papers mentioned in your queries. This addition helps clarify the distinctions between CMF and other studies that have applied Bayesian filters to different adaptation tasks.\n\n### **Response**\n\n**Q1. Utilizing the Kalman filter for sequential model parameter inference might not be optimal given its inherent assumptions. The Kalman filter operates under the presumption of linear system dynamics and posits a Gaussian distribution for the posterior. This is often misaligned with real-world scenarios where state posteriors frequently exhibit multimodal distributions.**\n\n**A1.** We are grateful for your insightful observation regarding the use of the Kalman filter for sequential model parameter inference in our work. Your point about the limitations of the Kalman filter, particularly its reliance on linear system dynamics and Gaussian distribution assumptions, is well-taken and pertinent to our discussion.\n\nIn response, we acknowledge that real-world scenarios often present complexities that linear systems and Gaussian models may not adequately capture. As you rightly pointed out, state posteriors in practical applications can exhibit non-linear characteristics and multimodal distributions. This discrepancy is a well-known challenge in the application of Bayesian filters to real-world data, as highlighted in studies such as [1], which deal with natural data or labels exhibiting such complex forms.\n\n\u2192 **However, in the context of our CMF approach, we treat DNN\u2019s parameters as observations, where the linear Gaussian model assumption remains effective. This treatment is underpinned by theoretical and empirical evidence from studies [a, b, c],** which suggest that applying a linear system, such as a moving average, to DNN\u2019s parameters over time can enhance model performance and generalizability. For instance, study [a] demonstrates **the application of a linear system to parameters obtained via an SGD-based optimizer, employing a methodology similar to CMF's linear Gaussian model and scalar parameter settings (Section 3.4)**.\n\nMoreover, **we have rigorously validated CMF across various datasets and in real-world applications, such as speech recognition tasks, where complex distribution shifts are prevalent**. In these scenarios, CMF has demonstrated superior performance compared to existing OTTA methods. This empirical evidence supports the efficacy of our approach even in the presence of non-linear and multimodal distribution characteristics in real-world data.\n\n\u2192 We also recognize the limitations of our approach, particularly when the hyperparameter $\\gamma$ in CMF is set to excessively low values.  As shown in Appendix C.5, Table 12 of our paper, CMF's performance can be disproportionately affected by changes in other hyperparameters under such conditions. This phenomenon, as you astutely noted, could be attributed to the non-linearities inherent in the data. **The sensitivity of CMF to non-linearities or multimodality, especially through the direct influence of the hyperparameter $\\gamma$ on the target model, underscores the importance of hyperparameter tuning in maintaining the linearity of parameters. This aspect indeed represents a limitation of CMF.** In our experiments, as detailed in the paper, we fixed $\\gamma$ at 0.99, which proved effective in most scenarios.\n\n**In conclusion, while we agree with your assessment of the limitations inherent in linear Gaussian models, our findings suggest that in the specific context of CMF and DNN parameter treatment, this approach remains robust and effective. We plan future work to identify applications where such linear CMF systems are challenging and apply simple non-linear functions (for example, confidence-based parameter selection) because it is challenging to guarantee that all applications will depend on this linear system.**\n\n[a] Izmailov, Pavel et al. \u201cAveraging Weights Leads to Wider Optima and Better Generalization.\u201d\u00a0*Conference on Uncertainty in Artificial Intelligence*\u00a0(2018).\n\n[b] Garipov, Timur, et al. \"Loss surfaces, mode connectivity, and fast ensembling of dnns.\"\u00a0*Advances in neural information processing systems*\u00a031 (2018).\n\n[c] Guo, Hao, Jiyong Jin, and Bin Liu. \"Stochastic weight averaging revisited.\"\u00a0*Applied Sciences*\u00a013.5 (2023): 2935."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149333187,
                "cdate": 1700149333187,
                "tmdate": 1700535855732,
                "mdate": 1700535855732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0ZVCpqUew4",
                "forum": "BllUWdpIOA",
                "replyto": "hn96yqs321",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iWLp [2/2]"
                    },
                    "comment": {
                        "value": "**Q2. While the exploration of the Kalman filter and other Bayesian filtering techniques for model adaptation/TTA is not entirely novel, it is crucial to delineate this work from previous contributions. It's recommended to rigorously compare, both theoretically and empirically, with established works like EKF[1] and PFDE[2]. Such a comparative analysis can better spotlight this paper's unique technical contributions.**\n\n**A2.** We have not found papers applying KF or EKF to OTTA. **[1] focuses on multi-epoch adaptation problems with observations being natural data or labels**. However, CMF focuses on **OTTA problems with DNN\u2019s parameter** $\\theta_t$ as the observation.\n\n\u2192 [1] designs the emission model $\\mathcal{N}(Y|f(\\theta_{t-1}, X_{t-1}), I)$ with a non-linear function $f$.\n\n\u2192 **EKF requires linear approximation of non-linear models**, which can lead to instability and reduced accuracy, especially in OTTA scenarios where computation cost is a critical factor.\n\n\u2192 In contrast, CMF\u2019s emission model $p(\\theta^{(t)}|\\phi^{(t)})=\\mathcal{N} (\\theta^{(t)}|H\\phi^{(t)}, R)$ targets DNN\u2019s parameter $\\theta^{(t)}$. Based on the rationale mentioned in A1, a linear system is applicable, **avoiding the need for linear approximation in Gaussian models** and **enabling exact Bayesian inference.**\n\n\u2192 **Thus, CMF is numerically stable, simple, and has a relatively low computation cost, making it suitable for various applications.**\n\n**A2 (cont.).** To our knowledge, there are no instances of particle filters being applied to OTTA methods. This is primarily due to the significant computational demands associated with particle filters, which require a large number of particles to function effectively.\n\n\u2192 This characteristic poses a substantial challenge for their use in computation-efficient OTTA tasks, where minimizing computational overhead is crucial.\n\n\u2192 [2] attempts to address this by redefining the problem using PFDE, which reduces the required number of particles. However, even with this reduction, PFDE still necessitates the use of 14 particles for relatively simple applications like MNIST and RNN. This requirement becomes even more prohibitive when considering large models such as ViT, Swin, and D2V, which are used in complex real-world scenarios proposed by SAR, or datasets with a high number of classes like ImageNet-C. **Our CMF employs the KF with scalar parameter settings, akin to using only one particle, which significantly reduces computational use and memory consumption compared to particle filters.**\n\n\u2192 Despite these challenges, the integration of particle filters with OTTA methods remains a promising avenue for future research, offering potential advancements in the field.\n\n**Q3. In Section 3.4, the paper discusses simplifying Bayesian filtering computations. An analysis evaluating the accuracy of these approximations is warranted.**\n\n**A3.** In the context of EKF, the non-linear function serves as the mean of the Gaussian model and is linearly approximated. This approximation is crucial for maintaining accuracy, as the non-linear function itself represents the ground truth.\n\n\u2192 However, the linear Gaussian model employed by CMF does not necessitate additional linear approximation. Furthermore, the parameters of KF in this model are optional, which complicates the definition of accuracy since there is no explicit ground truth against which to measure.\n\n\u2192 Moreover, **Section 3.4 of CMF primarily focuses on the settings for KF parameters rather than an approximation of the matrix version.**\n\n\u2192 A potential comparison could be made between the performance of CMF in matrix settings versus scalar settings. However, the dimension $d$ of the adaptation target parameter in our experiments ranges from $128$ to $1024$, leading to matrices as large as $1024\\times1024$.\n\n\u2192 Both EKF and KF require matrix inversion for all target DNN's parameters to calculate the Kalman gain $K$. For example, for the ViT model, the calculation is required for $50$ matrices. Thus, this process lead to computational errors or extremely slow processing speeds in the PyTorch toolkit.\n\n\u2192 **Due to these limitations, CMF adopts scalar settings and proposes a simplified inference process in Section 3.4.** This approach is a key contribution of our work, as it allows for exact Bayesian inference within the linear Gaussian model of the scalar setting, while also addressing computational efficiency concerns."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700149370349,
                "cdate": 1700149370349,
                "tmdate": 1700613560668,
                "mdate": 1700613560668,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eRhKJmgij3",
                "forum": "BllUWdpIOA",
                "replyto": "hn96yqs321",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer iWLp ,\n\nThank you again for your insightful review of our submission. We have consistently endeavored to provide detailed responses to address the concerns you have raised along your commnets. Your valuable comments have also helped us to make our study more robust and clear.\n\nThere is now one day left in the author-reviewer discussion period, during which we hope to have the reviewer review our response and revised submission. If you have any additional comments, we will do our best to answer any questions.\n\nWe sincerely appreciate your efforts and time in reviewing our submission.\n\nAuthor(s)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627503938,
                "cdate": 1700627503938,
                "tmdate": 1700627503938,
                "mdate": 1700627503938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ko6zWsoJoN",
                "forum": "BllUWdpIOA",
                "replyto": "7ApeWtIwun",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Reviewer_iWLp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Reviewer_iWLp"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for carefully handling my concerns.  I still have concerns about the oversimplification of the assumption of dynamics of DNN's parameters with respect to domain shifts. Besides, the oversimplification of the update related to the moment function is indeed remembering the moving average. I hope the author can put the details of such a moment function in the major paper to avoid confusing readers.  My doubt is that updating the final model parameter by balancing between the source model with the test-time tuned model with linear/nonlinear combinations seems equivalent and exhibits a much simpler form. Then the motivation for introducing the Kalman filter seems vague, considering such a model assumes linear dynamics for almost all components in a dynamical system (e.g. transition and emission probabilities ).  Since my major concern remains, I decided to keep my original ratings."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740175846,
                "cdate": 1700740175846,
                "tmdate": 1700740175846,
                "mdate": 1700740175846,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GdM5xmrhOp",
            "forum": "BllUWdpIOA",
            "replyto": "BllUWdpIOA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1712/Reviewer_4Nt5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1712/Reviewer_4Nt5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes parameter averaging/filtering for online test-time adaptation. The idea is to first find the task minimum, and then use the previous task to form a \u201cprior\u201d and average the parameters. The actual algorithm can be thought of as a simplified version of the Kalman filtering algorithm but they do not consider the full covariance because of high dimensionality. The paper shows the algorithm\u2019s effectiveness on online test-time adaptation, where a full network has been pretrained and only normalization parameters are \u201cadapted\u201d to each task, where different tasks exhibit distribution shift on image styles and textures or speech environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The proposed methodology is properly derived from a Kalman filter algorithm and has probabilistic interpretations.\n- The proposed methodology can be adapted and simplified to a deep network where only normalization parameters are changing.\n- Empirically, the proposed method can be applied on various backbone networks and achieve strong results on online test-time adaptation."
                },
                "weaknesses": {
                    "value": "- The proposed method seems to work for online test-time adaptation which requires a well trained network. It would be good to investigate whether this could be a limitation. It would be good to understand whether it relies on a pretrained network or the continual style of training can also extend to a network from scratch. It would also be good to understand the limitation on the number of adaptation parameters and the number of tasks it can continually learn without forgetting. When does the method break down? Since the methodology is a general one, it would be good to understand its general characteristics. To this end, I would appreciate to see some toy experiments on parameter averaging that answers these questions.\n\n- It would be good to study on the sensitivity of each hyperparameter."
                },
                "questions": {
                    "value": "See above comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1712/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699196252363,
            "cdate": 1699196252363,
            "tmdate": 1699636099738,
            "mdate": 1699636099738,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UASMvNMOSl",
                "forum": "BllUWdpIOA",
                "replyto": "GdM5xmrhOp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4Nt5"
                    },
                    "comment": {
                        "value": "### **Response**\n\n**Q1. The proposed method seems to work for online test-time adaptation which requires a well trained network. It would be good to investigate whether this could be a limitation. It would be good to understand whether it relies on a pretrained network or the continual style of training can also extend to a network from scratch. It would also be good to understand the limitation on the number of adaptation parameters and the number of tasks it can continually learn without forgetting. When does the method break down? Since the methodology is a general one, it would be good to understand its general characteristics. To this end, I would appreciate to see some toy experiments on parameter averaging that answers these questions.**\n\n**A1.** Thank you for your insightful comments. Our proposed framework is indeed primarily designed for OTTA scenarios. The methodology is built upon the premise of adapting a well-trained network to new, unseen data distributions encountered during the test phase. This approach is particularly relevant in real-world applications where data distributions can shift unpredictably.\n\nRegarding the reliance on a pretrained network, our method does indeed assume the availability of a well-trained source model as a starting point. This is a common practice in OTTA methodologies, as it leverages the extensive knowledge already encapsulated in the pretrained model. However, the adaptability of our CMF framework to a network trained from scratch is an interesting avenue for future research. \n\nRegarding the limitations concerning the number of adaptation parameters and tasks, our current experiments have not explicitly quantified the upper bounds of these aspects. The CMF framework has demonstrated robustness in various OTTA scenarios, but there is indeed a theoretical limit to the number of tasks and parameters it can handle before performance degradation occurs.\n\n\u2192 However, one point we would like to emphasize is **the new experimental results added in Appendix C.4, Figure 5**. These results suggest that **CMF has the potential to scale to a larger number of tasks**. In this experiment, we measured performance on both the source and target datasets at the end of each adaptation for each task.\n\n\u2192 As observed in the experiments, CMF appears to retain consistent information across the entire test dataset of the source dataset, ImageNet (Clean), while simultaneously improving performance on the entire ImageNet-C (Corrupt) dataset. **This characteristic is not observed in other recent methodologies** like EATA and ROID, indicating a unique advantage of CMF in **handling multiple tasks**.\n\n\u2192 We appreciate the suggestion to conduct toy experiments on parameter averaging to explore these boundaries and will consider incorporating such experiments in our future research.\n\n**Q2. It would be good to study on the sensitivity of each hyperparameter.**\n\n**A2.** Thank you for suggesting sensitivity analyses. In response to your valuable feedback, we have expanded our experiments to include a comprehensive analysis of hyperparameter sensitivity. These additional experiments are detailed in Section 4's ablation study and further elaborated in Appendix C.5.\n\n\u2192 In these experiments, conducted in the TC-CS scenario on ImageNet-C, we varied one hyperparameter at a time for the parameters in Algorithm 1 $(q, \\alpha, \\gamma)$. \n\n**Sensitivity Analysis for $q$:**\n\n| Model | Source | TENT | CMF ($q$) |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  | 0.01 | 0.0075 | 0.005 | 0.0025 | 0.0001 |\n| ViT | 60.2 | 54.5\u00b10.04 | 44.9\u00b10.09 | 44.9\u00b10.09 | 44.8\u00b10.12 | 44.8\u00b10.14 | 44.9\u00b10.05 |\n| Swin | 64.0 | 64.0\u00b10.14 | 46.7\u00b10.25 | 46.5\u00b10.05 | 46.6\u00b10.12 | 46.5\u00b10.18 | 46.9\u00b10.18 |\n| D2V | 51.8 | 51.9\u00b10.09 | 43.4\u00b10.08 | 43.4\u00b10.09 | 43.5\u00b10.04 | 43.7\u00b10.15 | 44.5\u00b10.02 |\n\n**Sensitivity Analysis for $\\alpha$:**\n\n| Model | Source | TENT | CMF ($\\alpha$) |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  | 0.999 | 0.99 | 0.95 | 0.9 | 0.8 |\n| ViT | 60.2 | 54.5\u00b10.04 | 45.2\u00b10.03 | 44.8\u00b10.12 | 44.7\u00b10.06 | 44.9\u00b10.09 | 44.9\u00b10.06 |\n| Swin | 64.0 | 64.0\u00b10.14 | 46.9\u00b10.12 | 46.6\u00b10.12 | 46.7\u00b10.13 | 46.8\u00b10.24 | 47.1\u00b10.20 |\n| D2V | 51.8 | 51.9\u00b10.09 | 43.6\u00b10.17 | 43.5\u00b10.04 | 44.2\u00b10.11 | 44.6\u00b10.04 | 44.8\u00b10.04 |\n\n**Sensitivity Analysis for $\\gamma$:**\n\n| Model | Source | TENT | CMF ($\\gamma$) |  |  |  |  |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n|  |  |  | 0.999 | 0.99 | 0.95 | 0.9 | 0.8 |\n| ViT | 60.2 | 54.5\u00b10.04 | 45.2\u00b10.11 | 44.8\u00b10.12 | 45.5\u00b10.09 | 46.7\u00b10.07 | 49.18\u00b10.04 |\n| Swin | 64.0 | 64.0\u00b10.14 | 47.1\u00b10.21 | 46.6\u00b10.12 | 47.2\u00b10.14 | 48.5\u00b10.20 | 50.68\u00b10.28 |\n| D2V | 51.8 | 51.9\u00b10.09 | 43.2\u00b10.22 | 43.5\u00b10.04 | 45.3\u00b10.04 | 46.8\u00b10.06 | 48.64\u00b10.11 |\n\n\u2192 The results from these sensitivity analyses revealed that $\\gamma$ had the most significant impact on the performance of our CMF framework. These findings have been incorporated into Appendix C.5 for a more detailed examination."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700154557418,
                "cdate": 1700154557418,
                "tmdate": 1700277316262,
                "mdate": 1700277316262,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9b8PhOUQUn",
                "forum": "BllUWdpIOA",
                "replyto": "GdM5xmrhOp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1712/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A gentle reminder for discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer 4Nt5 ,\n\nThank you again for your insightful review of our submission. We have consistently endeavored to provide detailed responses to address the concerns you have raised along your commnets. Your valuable comments have also helped us to make our study more robust and clear.\n\nThere is now one day left in the author-reviewer discussion period, during which we hope to have the reviewer review our response and revised submission. If you have any additional comments, we will do our best to answer any questions.\n\nWe sincerely appreciate your efforts and time in reviewing our submission.\n\nAuthor(s)"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1712/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627534672,
                "cdate": 1700627534672,
                "tmdate": 1700627534672,
                "mdate": 1700627534672,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]