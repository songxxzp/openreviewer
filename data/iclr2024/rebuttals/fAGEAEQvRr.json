[
    {
        "title": "Gradient descent for matrix factorization: Understanding large initialization"
    },
    {
        "review": {
            "id": "nPGEQnsVgM",
            "forum": "fAGEAEQvRr",
            "replyto": "fAGEAEQvRr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8450/Reviewer_MbHg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8450/Reviewer_MbHg"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the incremental learning behavior of gradient descent for matrix factorization problems. Different from existing works that only consider small initialization, this paper considers initializations that have constant scale. Under certain regularity assumptions on the initialization and singular values in the training process, the authors show that gradient descent still exhibits incremental learning. In their theoretical analysis, the auhtors use a  novel signal-to-noise ratio (SNR) argument to show the exponential separation between a low-rank signal and a noise term, which allows that to show that gradient descent trajectory is nearly low-rank."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The introduction part of this paper is written in a clear and succint manner. The main theoretical results are often followed by necessary explanations and proof sketch, making it easier for the readers to understand them.\n2) The effect of large initialization on implicit bias is an interesting topic and the theoretical results in this paper are novel to the best of my knowledge. The authors also provide comprehensive review of the related literature."
                },
                "weaknesses": {
                    "value": "1) The theoretical results in this paper are presented in a somewhat isolated manner, and it seems that then result for rank-$2$ is not even stated as an independent theorem. I suggest that the authors can briefly summarize the main results (for rank-$2$ and general ranks) in the introduction, before going into technical details.\n\n2) The organization of Sec. 3 can probably be improved: although the title of this section is \"challenges in examine general rank solutions\", Sec. 3.1 is about local convergence and the remaining two subsections seem to discuss the challenges for large initialization, rather than for general ranks.\n\n3) The \"signal-noise-ratio\" argument in this work looks different from existing works that also decompose the GD trajectory in the signal and noise term (e.g. [1,2] ), but the decomposition seems to be the same. I suggest that the authors can add more discussions about this difference to highlight the contribution of this paper.\n\n[1] Stoger, D. and Soltanolkotabi, M. Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction.\n[2] Jin, J., Li, Z., Lyu, K., Du, S. S., & Lee, J. D. Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing."
                },
                "questions": {
                    "value": "1) I am confused about Assumption 8 and cannot see how it is related to the arguments at the beginning of Sec. 4.2.3. According to my understanding, it says that the norm of the $2$-to-$d$ components is larger than its inner product with the first component. Can you explain this assumption in more details?\n\n2) The authors seem to employ a successive argument for general rank matrix. However, the definition of the benign set $R$ in Sec. 3.1 would change for higher ranks. Is there any arguments in your proof verifying that GD remains in the high-rank $R$? Probably you did it in Theorem 13, but I cannot understand how they are related.\n\n3) In existing works it is commonly the case that convergence/incremental learning hold with high probability (e.g.[1] Theorem 3.3), since the initialization has to be aligned with each component, otherwise it cannot make progess in some direction. Does this paper need to impose similar requirements for initialization?\n\n[1] Stoger, D. and Soltanolkotabi, M. Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8450/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8450/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8450/Reviewer_MbHg"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698432996582,
            "cdate": 1698432996582,
            "tmdate": 1700177572994,
            "mdate": 1700177572994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fuiM7Aoy8H",
                "forum": "fAGEAEQvRr",
                "replyto": "nPGEQnsVgM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer MbHg - Part one"
                    },
                    "comment": {
                        "value": "Thanks the reviewer for the valuable suggestions and questions. In this rebuttal, we will address them point by point. Due to the limitation of characters, we separate this rebuttal into two parts. The first one addresses the weakness section. The second one addresses the remaining questions.\n\n**Weakness**\n\n- The theoretical results in this paper are presented in a somewhat isolated manner, and it seems that then result for rank-2\u00a0is not even stated as an independent theorem. I suggest that the authors can briefly summarize the main results (for rank-2\u00a0and general ranks) in the introduction, before going into technical details.\n    - Thanks for the reviewer\u2019s suggestion. In our updated version, we summarize our results in a main theorem, Theorem 6, and an intuitive theorem, Theorem 1. We put the intuitive theorem in the introduction. We also move the visualization results and the intuitive ideas to the introduction. We move the analysis of rank-2 results to a later section called sketch of proof.\n- The organization of Sec. 3 can probably be improved: although the title of this section is \"challenges in examine general rank solutions\", Sec. 3.1 is about local convergence and the remaining two subsections seem to discuss the challenges for large initialization, rather than for general ranks\n    - Thanks for the reviewer\u2019s suggestion. We have changed the structure for that section. We have made the local linear convergence a single section, emphasizing the usage of SNR in this application. Then we discuss the random initialization. We both review small random initialization and present the main theorem for large initialization. Explanations and comparisons are given in corresponding places. After this, we give a detailed sketch of proof.\n- The \"signal-noise-ratio\" argument in this work looks different from existing works that also decompose the GD trajectory in the signal and noise term (e.g. [1,2] ), but the decomposition seems to be the same. I suggest that the authors can add more discussions about this difference to highlight the contribution of this paper.\n    - Thanks for the reviewer\u2019s suggestion. We have added more discussions in our updated version. Specifically, to employ the SNR analysis, we need a lower bound of the signal and an upper bound on the noise, both in terms of their previous iterations. These two bounds need to be related so that the ratio of SNR$_{t+1}$  by SNR$_t$ can be analyzed. The challenging part is to obtain two related upper and lower bounds. Previous literature does not provide SNR considerations so their upper and lower bounds are not applicable for our purpose. In addition, for the large initialization setting, our SNR analysis is even more novel, because we introduce new signal and noise terms. \n\nHope this addresses the reviewer's concerns regarding the weaknesses."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700160057217,
                "cdate": 1700160057217,
                "tmdate": 1700160692265,
                "mdate": 1700160692265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zXxixHbd07",
                "forum": "fAGEAEQvRr",
                "replyto": "FyQ2N2S0FP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Reviewer_MbHg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Reviewer_MbHg"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for clarification, and I think I can now understand the idea of your proof.\n\nNow it seems to me that your proof of convergence rely on the assumption that $r(1-\\eta \\Delta / 6)^{t_1^*} \\leq \\sqrt{\\frac{\\Delta}{8}} \\min \\sigma_1\\left(u_{2, t_1+t_1^*}\\right), \\sqrt{\\frac{\\Delta}{2}}$, so that you would have a \"good starting point\" for the second round. Do you think it is possible to remove this algorithm-dependent assumption, or verifying it with simple experiments? Recent works with smaller init should be able to address this issue."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167291073,
                "cdate": 1700167291073,
                "tmdate": 1700167291073,
                "mdate": 1700167291073,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Yal3SrNlnt",
                "forum": "fAGEAEQvRr",
                "replyto": "st4CO7Vwmo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Reviewer_MbHg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Reviewer_MbHg"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for clarifications. \n\nNow I think I can fully understand the contributions of this paper, though it might be better to state that \"assumption 4 almost surely holds\" as a separate proposition and include a formal proof. I will increase my rating to 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177540971,
                "cdate": 1700177540971,
                "tmdate": 1700177540971,
                "mdate": 1700177540971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sD7XqWGjuz",
            "forum": "fAGEAEQvRr",
            "replyto": "fAGEAEQvRr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8450/Reviewer_BGNj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8450/Reviewer_BGNj"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the influence of initialization on the performance of gradient descent for the matrix factorization problems.\nExisting work focusses on small initialization (see missing ref in References).\nThe analysis is based on a signal-to-noise (SNR) analysis, which generalizes the one of CHen et al (2019) for the rank 1 matrix factorization problem. Here the SNR is defined as the ratio of the norm of the components of X aligned with the desired directions (first k eigenspaces of target matrix $\\Sigma$), divided by the norm of the remaining components.\n\nThe paper has two main contributions:\n- Provided GD is initialized in a high SNR region (Eq 10), the authors prove linear convergence towards the global minimizer.\n- When no such initialization is available, the paper provides additional results, but (see first point below) it still seems to require that $X_0$ be in certain region which is unknown."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Potential important progress in the field of implicit bias for matrix factorization, if the results are true. All previous results I know of are for small initialization."
                },
                "weaknesses": {
                    "value": "See questions below:\n- only one experiment, on rank 2 matrix approximation\n- unclear dependency on initialization: $\\alpha$ needs to be small \n- unclear it it is required in the proof that $X_0 \\in S$ or not (contradictory statements in the paper)"
                },
                "questions": {
                    "value": "### Mathematics\nBy order of importance:\n- The paper claims to study large initialization, and that the first result with initialization in the region of Eq 10 is not satisfying. However, the proof of the main result states \"This is achievable if we use random initialization X0 = \u03b1N0 with a reasonably small constant \u03b1\", which is not \"large initialization\". $\\alpha$ must be small enough such that $\\sigma^2_1(K_r) \\leq \\lambda_r - 3 \\Delta/4$, so $\\alpha$ depends on $r$, and it's not so wild to think that $r$ is a fraction of $d$. Remark 4 seems to contradict that, highlighting that the theorem does not require $X_0 \\in S$, but the exposition is very confusing here (eg, Lemma 5 assumes that $X_0 \\in S$)\n- It is clear that Sigma can be assumed diagonal in problem 1 up to an orthogonal change of variable in $X$ (because GD is equivariant to such a change of variable), but this deserves to be stated explicitly the first time this assumption is made (P2).\n- $\\sigma_1(u_{k,t})$ is just its norm, right? $u_{k,t}$ is a vector. Same for $u_{k,t} K_{k, t}^\\top$\n\n\n## Experiments\n- In the experimental example, how do we know if 0.5 N(0, 1/d) is a large initialization? Since $d$ does not vary, what can we say? It would be better to have the experiment for several values of $d$ and fixed $\\bar \\omega$\n- The experimental example considers rank 2 matrix factorization which, though by nature different as the authors have explained, does not seem to far from rank 1 factorization (especially compared to the dimension 2000). It would highlight the goal of the paper better to use something like rank 10 matrix factorization here.\n- Please provide more than one experiment in dimension 2. The analysis is incomplete without more proof that the results hold for varying r and d, fixed $\\bar \\omega$.\n\n\n\n### Formulation\n- Incremental learning has many possible meanings, it would be nice to clarify it here. Same for \"spectral method\", I think the amount of details given in section 3.2 should be slightly increased.\n- P3 \"because at the global minima\"/\"the set R contains all the global minima\": it seems to me that there is a single global minima, the rank-$r$ truncated SVD of $\\Sigma$. can the authors clarify?\n- First sentence of Section 3.3 is a repetition from above.\n- Paragraph 4.1 consider replacing rank-$r$ by rank-2 for clarity (and all instances of $r$ in that paragraph)\n- Legend of Figure 1: \"top three rows\" are the first three rows?\n\n### References:\n- I believe the paper is missing the seminal reference \"S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro, \u201cImplicit regularization\nin matrix factorization,\u201d in Advances in Neural Information Processing Systems, 2017\" which conjectured global convergence of GD to the minimal nuclear norm solution in the case of small initialization.\n\n\nTypos:\n- takes infinity at 0\n- the rest elements: the remaining elements/the rest of the elements (several occurrences)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756402371,
            "cdate": 1698756402371,
            "tmdate": 1699637054120,
            "mdate": 1699637054120,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DfP2T9kdSK",
                "forum": "fAGEAEQvRr",
                "replyto": "sD7XqWGjuz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer BGNj - Part one"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable suggestions and questions. In this rebuttal, we will address them point by point. Due to the limitation of characters, we separate this rebuttal into two parts. This part responds to the mathematics.\n\n- **Mathematics**\n- The paper claims to study large initialization, and that the first result with initialization in the region of Eq 10 is not satisfying\u2026. However, the proof of the main result states \"This is achievable if we use random initialization X0 = \u03b1N0 with a reasonably small constant \u03b1\", which is not \"large initialization\".\u00a0\u00a0must be small enough such that\u00a0, so\u00a0\u00a0depends on\u00a0, and it's not so wild to think that\u00a0\u00a0is a fraction of\u00a0. Remark 4 seems to contradict that, highlighting that the theorem does not require\u00a0, but the exposition is very confusing here (eg, Lemma 5 assumes that\u00a0)\n    - Sorry for the confusion here. In our updated version, we have removed all the confusion part. Specifically, for every results we display, we only require $\\sigma_1(X_0)\\leq1/\\sqrt{3\\eta}$ and correspondingly $\\varpi\\lesssim 1/\\sqrt{\\eta}$. All the subsequent results are then proved, rather than assumed. In addition, we emphasize that this requirement is rate-optimal because if $\\sigma_1(X_0)$ is too large, then the GD sequence can simply diverge. A simple counterexample is when $\\Sigma=0$ and $\\sigma_1^2(X_0)\\geq 3/\\eta$. An inductive argument implies that $\\sigma_1(X_{t+1})\\geq 2\\sigma_1(X_t)$  for all t, and thus $X_t$ diverges. We have added this example and explanation below Theorem 6 in our updated version.\n    - We would also like to emphasize the difference between large initialization and small initialization. Large initialization means the norm of $X_0$ converges to a positive constant while small initialization means the norm of $X_0$ converges to zero. By concentration results, the norm of $X_0=\\varpi N_0$ is O($\\varpi$). Therefore, taking a positive constant $\\varpi$ means the large initialization. In contrast, in previous literature that assumes small initialization, they require $\\varpi$ to be as small as $\\min(d^{-1/2},d^{-3\\kappa^2})$, where $\\kappa>1$ is the condition number. Therefore, there should be a significant difference between two settings.\n- It is clear that Sigma can be assumed diagonal in problem 1 up to an orthogonal change of variable in\u00a0\u00a0(because GD is equivariant to such a change of variable), but this deserves to be stated explicitly the first time this assumption is made (P2).\n    - Yes. In our revised version, we have mentioned that such assumption is made without loss of generality, and explained the reason.\n- $\\sigma_1(u_{1,t})$ is just its norm, right? Same for $u_{k,t}K_{k,t}^\\top$.\n    - Yes. Both the norm notation and the singular value notation can be used. We chose the singular value notation for its consistency with other results in the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159638751,
                "cdate": 1700159638751,
                "tmdate": 1700160706485,
                "mdate": 1700160706485,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wfr0OaHYEp",
                "forum": "fAGEAEQvRr",
                "replyto": "sD7XqWGjuz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer BGNj - Part two"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for the valuable suggestions and questions again. In this part, we will address the remaining issues, including experiments, formulations, reference and typos.\n\n- **Experiment**\n- In the experimental example, how do we know if 0.5 N(0, 1/d) is a large initialization? Since d does not vary, what can we say? It would be better to have the experiment for several values of d and $\\varpi$\n    - We appreciate your attention to the details of our matrix factorization experiment. Theoretically, by the concentration results, the norm of $X_t$ is around 0.5. This is comparable to the largest eigenvalue 1 of $\\Sigma$. Consequently, the higher order term $X_tX_t^\\top X_t$ cannot be disregarded in the GD iterations. In this context, the theory of small initialization fails to account for the convergence behavior observed in GD.\n    - Additionally, we add new experiments with varying d and $\\varpi$ in the appendix. In particular, we choose d from 1000, 2000, 4000, and we take $\\varpi$ from 0.001, 0.5, 2, and 10. Our findings show that if $\\varpi$ is as large as 10 in this setting, the GD sequence will diverge. For the remaining three choices of $\\varpi$, we can observe similar convergence behaviors of GD.\n- The experimental example considers rank 2 matrix factorization which, though by nature different as the authors have explained, does not seem to far from rank 1 factorization (especially compared to the dimension 2000). It would highlight the goal of the paper better to use something like rank 10 matrix factorization here.\n    - We appreciate your attention to the details of our matrix factorization experiment. Our experiment is designed to demonstrate our core ideas in an intuitive and visually interpretable manner. By using rank-two matrix factorization setting, selecting five noteworthy points, and comparing their heat plots, we could succinctly illustrate the intuitions of our analysis.\n    - Additionally, we add new experiments to address the reviewer\u2019s question. In the second experiment in our appendix, we examine the setting of fixed dimension d with varying rank r and $\\varpi$. We let d=1000 and choose r from {2,6,10} and $\\varpi$ from {0.001, 0.5, 2}. The error curves of GD are plotted for these different settings. We find that these results are consistent with the results we have presented before.\n- Please provide more than one experiment in dimension 2. The analysis is incomplete without more proof that the results hold for varying r and d, fixed\u00a0$\\varpi$.\n    - Thanks for the reviewer\u2019s suggestion. In our updated version, we have provided additional experiments in the appendix. For rank-two matrix approximation, we vary both dimension d and the initial magnitude $\\varpi$. Our results are consistent with the results we have presented.\n\n- **Formulation**\n- Incremental learning has many possible meanings, it would be nice to clarify it here.\n    - Thank you for the suggestions. We have explained that in our context, incremental learning refers to the process where eigenvectors associated with larger eigenvalues are learned first.\n- Same for \"spectral method\", I think the amount of details given in section 3.2 should be slightly increased.\n    - We have explained the spectral method. It also means the power method. For these kinds of methods, the eigenvectors related to larger eigenvalues are learned first. Notably, $\\sigma_r(U)$ increases faster than $\\sigma_1(J)$.\n    - We have extended the content of that section. We add more details on two aspects: the theoretical intuition and their assumptions on small initialization.\n- P3 \"because at the global minima\"/\"the set R contains all the global minima\": it seems to me that there is a single global minima, the rank-truncated SVD of\u00a0$\\Sigma$. can the authors clarify?\n    - The global minima refer to all $X$ such that $XX^\\top=\\Sigma_r.$  While  $\\Sigma_r$ is unique, there are multiple $X$ satisfying the equality. Since R is a set of X, it is more accurate to say that the set R contains all the global minima.\n- First sentence of Section 3.3 is a repetition from above. Paragraph 4.1 consider replacing rank-\u00a0by rank-2 for clarity (and all instances of\u00a0\u00a0in that paragraph)\n    - Yes, we have made all the modifications.\n- Legend of Figure 1: \"top three rows\" are the first three rows?\n    - Yes.\n- **Reference and Typos**\n    - We added the mentioned reference and revised the typos in our updated version.\n\nIf the reviewer has additional concerns, please feel free to let us know."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159863994,
                "cdate": 1700159863994,
                "tmdate": 1700172410787,
                "mdate": 1700172410787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZFMi8uVjhY",
                "forum": "fAGEAEQvRr",
                "replyto": "sD7XqWGjuz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal discussion"
                    },
                    "comment": {
                        "value": "Hi Reviewer BGNj, We are writing again in hopes that you will respond to our rebuttal and let us know your current concerns so that we could address them. For now, we believe we have addressed all the weaknesses you has mentioned. Specially, \n\n- unclear dependency on initialization: $\\varpi$ needs to be small;\n   - we have demonstrated that our initialization condition is dimension-independent and rate-optimal; in particular, we only require $\\varpi\\lesssim 1/\\sqrt{\\eta}$, which is in sharp contrast to the previous 'small' requirements $\\varpi\\lesssim \\min(d^{-1/2},d^{-3\\kappa^2})$. \n- unclear it it is required in the proof that $X_0\\in S$\n   - we have made it clear that we do not need this condition; if this is the point that you are concerned, then we have demonstrated that such condition can be completely removed; hope this could address the reviewer's concern.\n- only one experiment, on rank 2 matrix approximation\n   - we have added more experiments in the appendix; due to the limitation of pages, we remain a single experiment in the main text for illustrating our ideas.\n\nPlease let us know if you have any concerns so that we could further address them. We are looking forward to your reply!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599855407,
                "cdate": 1700599855407,
                "tmdate": 1700603416563,
                "mdate": 1700603416563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M23bDOJ4nJ",
            "forum": "fAGEAEQvRr",
            "replyto": "fAGEAEQvRr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8450/Reviewer_yiZg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8450/Reviewer_yiZg"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on a simplified matrix factorization problem, aiming to understand the convergence of gradient descent when using large random initialization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper focuses on a simplified matrix factorization problem, aiming at understanding the convergence of GD when using large random initialization."
                },
                "weaknesses": {
                    "value": "The presentation could be further improved.   The authors claim that they aim to understand the convergence of GD with large random initialization on simple matrix factorization problems, but as a reader, I could not find a main theorem or corollary that clearly states that with large random initialization, GD converges with certain rates under appropriate parameter settings and assumptions.  Also, the presentation of the theorem involves many notations, which on the other hand, looks could not be uninvolved. \n\nThe  'large initialization' is in fact the `large random initialization'.  The authors consider GD with large random initialization, but the variance still depends on the dimension of the problem under consideration. Therefore, when the dimension d is relatively large, it reduces to the ``small\" random initialization setting. \n\nThe authors consider a simple matrix factorization problem, but as claimed in the main text, the motivation of this paper is to better understand  GD with large random initialization in training neural networks.\n\nEven in the simple matrix factorization problems, the comparisons with state-of-the-art results in this exact setting are not clear to me."
                },
                "questions": {
                    "value": "Line-5 on Page 1 and the other places, ''problem 1'' should be \"Problem (1)\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796083038,
            "cdate": 1698796083038,
            "tmdate": 1699637053997,
            "mdate": 1699637053997,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G6noySaBav",
                "forum": "fAGEAEQvRr",
                "replyto": "M23bDOJ4nJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer yiZg"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable suggestions and questions. In this rebuttal, we will address them point by point. \n\n- The presentation could be further improved\n    - In our updated version, we have significantly improved our presentation by taking all reviewers\u2019 suggestions into account. Specifically, we summarize our results into a main theorem and an intuitive one. We move visualization and intuitive ideas to the introduction. Also, we delay detailed analysis to a section called sketch of proof. Finally, we add more comparisons to existing literature to emphasize our contributions.\n- The authors claim that they aim to understand the convergence of GD with large random initialization on simple matrix factorization problems, but as a reader, I could not find a main theorem or corollary that clearly states that with large random initialization, GD converges with certain rates under appropriate parameter settings and assumptions.\n    - In our revised version, we present an improved version of the main theorem, Theorem 6, in Section 4.2. We also provide an intuitive version of the theorem in the introduction. Our theorem is deterministic and applicable to large random initialization. In our theorem, we prove that all the quantities beyond $t^*_k$ are bounded by either a constant or a logarithmic term. Hence, we claim that we provide a detailed trajectory analysis of GD. If we assume $t_k^*=O(\\log(d))$, then GD achieves $\\epsilon$-accuracy in $O(\\log(d)+\\log(1/\\epsilon))$ iterations. This is probably the desired result the reviewer want to see. However, due to the challenges posed by large initialization, we are currently unable to verify such assumption. This is one of the limitation of the paper. In some sense, this also reflects the difficulty of the problem we are studying. On the other hand, even without this assumption, our results still reveal the incremental learning behavior of GD and many other characteristics of GD. We hope the reviewer could appreciate our attempts to tackle the challenging and meaningful problem.\n- The 'large initialization' is in fact the `large random initialization'. The authors consider GD with large random initialization, but the variance still depends on the dimension of the problem under consideration. Therefore, when the dimension d is relatively large, it reduces to the ``small\" random initialization setting.\n    - To clarify for the reviewer, our perspective on large initialization refers to the scenario where the norm of $X_0$ remains a positive constant, even as the dimension approaches infinity. By statistical concentration results, the norm of $N_0$ approaches to a finite positive constant when dimension d tends to infinity. Therefore, if $X_0=\\varpi N_0$ with a positive constant $\\varpi$ independent of d, then it belongs to the large initialization regime. On the other hand, if $\\varpi$ tends to zero as d increases, then it belongs to the small initialization regime.\n    - In previous literature, $\\varpi$ is as small as $\\min(d^{-1/2},d^{-3\\kappa^2})$, where $\\kappa>1$ is the conditional number. In our case, $\\varpi$ is a constant independent of d. As we discussed on page 6 below Theorem 6, we only require $\\varpi=O(1/\\sqrt{\\eta})$ where $\\eta$  is the step size. We also demonstrate that this is rate optimal. If we further increase it, the GD algorithm will simply diverge.\n- Even in the simple matrix factorization problems, the comparisons with state-of-the-art results in this exact setting are not clear to me.\n    - As we explained, there is a significant difference between large initialization and small initialization. Except the rank-one case, our work is the first one to study large initialization setting. This already makes the comparison clear. In addition, small initialization is in fact a special case of large initialization. All of our results hold for small initialization, too. Finally, large initialization is the one people are using in practice. Therefore, we hope the reviewer could appreciate our efforts to narrow the gap between theory and practice.\n- The authors consider a simple matrix factorization problem, but as claimed in the main text, the motivation of this paper is to better understand GD with large random initialization in training neural networks.\n    - Yes. Even for matrix factorization, the understanding of large initialization is rather limited. We hope our paper could bring people to notice this gap between theory and practice. We also hope our technique and results can inspire future research in related fields, including deep learning theory.\n\nIf the reviewer has additional questions, please feel free to discuss with us."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159407753,
                "cdate": 1700159407753,
                "tmdate": 1700516941844,
                "mdate": 1700516941844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zmme2XwSMi",
                "forum": "fAGEAEQvRr",
                "replyto": "M23bDOJ4nJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal discussion"
                    },
                    "comment": {
                        "value": "Hi Reviewer yiZg, we still hope to hear from you! In our understanding, you have two major concerns, and we have addressed them as follows.\n\n- First, the reviewer holds the point that we are considering small random initialization setting because the variance of each entry tends to zero as dimension $d$ increases. \n   - We would like to kindly point out that this argument is not correct. Suppose $N_0\\in\\mathbb{R}^{d\\times r}$ with i.i.d. $N(0,1/d)$ entries. Then by standard concentration results, the spectral norm of $N_0$ will converge to a finite positive constant as dimension $d$ increases. If we do not require the variance of each entry tends to zero, such as taking $N_0$ as a matrix of $N(0,1)$ entries, then the norm of $N_0$ will explode and the GD algorithm will diverge when initialized with $N_0$. For the above reason, it is suitable to fix $N_0$ and define large or small initialization based only on the initial scale $\\varpi$. \n   - There is a significant difference between our work and previous works. Prior works require $\\varpi\\lesssim\\min(d^{-1/2},d^{-3\\kappa^2})$ while we only need $\\varpi\\lesssim 1/\\sqrt{\\eta}$, which is dimension-independent. In addition, our condition is rate-optimal, meaning that a further increase of $\\varpi$ would lead to divergence of the GD algorithm. Furthermore, one may review small initialization settings as special cases of our settings. As a result, the setting in our paper is much more challenging than previous results, and we provide a nontrivial extension of previous results.\n- Second, the reviewer suggests us to present a more concise and clear theorem. \n   - Thanks for the suggestion! We have carefully addressed this point. We have presented a clearer theorem that emphasizes our assumptions and results. Specifically, when using random initialization, our trajectory analysis holds almost surely. Moreover, if an additional assumption is made, we may obtain the $O(\\log(d)+\\log(1/\\epsilon))$ convergence rate of GD. We have also included detailed discussions in the paper. Hope this revision can address the reviewer's concern.\n\nPlease let us know if you have additional concerns. We are looking forward to your reply!"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602284706,
                "cdate": 1700602284706,
                "tmdate": 1700603169101,
                "mdate": 1700603169101,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Bgk3fIKrGz",
            "forum": "fAGEAEQvRr",
            "replyto": "fAGEAEQvRr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8450/Reviewer_dfta"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8450/Reviewer_dfta"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a stage-by-stage analysis of the dynamics of gradient descent on a low-rank symmetric matrix factorization problem, showing that the empirically observed stages of fast decrease followed by crawling progress before another fast decrease can be captured theoretically."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Fig. 1 and the first paragraph of 4.2 form a really nice motivation! I'd recommend moving them to the introduction. This forms a well-defined problem that is of relevance to understand the dynamics of gradient descent on more complex systems, which is relevant to the community. The approach appears novel, although I am not too familiar with the related work.\n\nAltough the theory might be described as \"incomplete\", as it relies on an Assumption 8 to transition from learning the first component to the second, I appreciate that the submission is clear in that it is an assumption and gives a justification."
                },
                "weaknesses": {
                    "value": "Up to minor clarity points below, the paper is understandable, but dense. My main concern regarding the submission its intended audience might be limited to the people looking to build upon those results to get towards a better understanding of symmetric matrix factorization. But my perspective is likely limited and there might be a wider applicability to the presented results."
                },
                "questions": {
                    "value": "**Questions**\n- The introduction states that the focus of the submission is on the implicit bias of gradient descent, but I do not see how studying the dynamics of Fig. 1 connects to the implicit bias (which, in my understanding, refers to the limit point the optimizer converges to)?\n- Remark 2 states the the results hold for PSD $\\Sigma$, but the results seem to also assume that $\\Sigma$ is diagonal. Is this assumption necessary, or is it presented for the diagonal case wlog? What is the key difficulty in generalizing it to non-diagonal matrices?\n\n**Minor points**\n- Please define incremental to avoid confusion with the alternative use of incremental learning as a synonym for seeing-one-example-at-a-time learning. I realise in post that this is what the second-to-last paragraph in \u00a71 is doing, but I didn't read it as such. A more explicit phrasing the first time the term appears might help, eg in the 4th paragraph \"Jin et al. demonstrate an increamental learning phenomenon with small initialization; Eigenvectors associated with large eigenvalues are learned first\". A sentence as to how this differs qualitatively from a similar observation on linear regression might help contextualize too.\n- The introduction describes matrix factorization as \"mirroring the training of a two-layer linear network\", but this doesn't hold for the symmetric matrix factorization studied here, which seems more similar to quadratic regression\n- The term \"period\" on page 7 might be replaced by \"time\", as the behavior is not periodic\n- \"ascend to infinity\" is more commonly referred to as \"diverge\" or \"diverge to infinity\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8450/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699577802403,
            "cdate": 1699577802403,
            "tmdate": 1699637053845,
            "mdate": 1699637053845,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ub184HeD8y",
                "forum": "fAGEAEQvRr",
                "replyto": "Bgk3fIKrGz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal for Reviewer dfta"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable suggestions and questions. In this rebuttal, we will address them point by point. \n\n- Fig. 1 and the first paragraph of 4.2 form a really nice motivation! I'd recommend moving them to the introduction.\n    - Thanks for the suggestion. We have moved them to the introduction in our updated version.\n- My main concern regarding the submission its intended audience might be limited to the people looking to build upon those results to get towards a better understanding of symmetric matrix factorization. But my perspective is likely limited and there might be a wider applicability to the presented results\n    - Symmetric matrix factorization intersects with a diverse array of problems such as compressed sensing, phase retrieval, matrix completion, PCA, quadratic regression, and neural networks, among others. It serves as a fundamental model that facilitates theoretical analysis. In addition, our work is the first one (except for the rank one case) to study large initialization. Such initialization is commonly used in practice, including neural networks or simply phase retrieval and PCA. Therefore, we expect our theoretical investigation will spark further research across these related fields.\n- The introduction states that the focus of the submission is on the implicit bias of gradient descent, but I do not see how studying the dynamics of Fig. 1 connects to the implicit bias (which, in my understanding, refers to the limit point the optimizer converges to)?\n    - We would like to offer two perspectives on the implicit bias of the gradient descent algorithm. Firstly, from the viewpoint of the limit point, the algorithm almost surely  converges to the global minimum. Although this is an optimization result, one may also view it as certain implicit bias, because the optimization problem is non-convex. Secondly, our trajectory analysis reveals incremental learning behaviors of GD during the training process. This second viewpoint, focusing on the training process, is typically useful when early stopping is utilized.\n- Remark 2 states the the results hold for PSD\u00a0, but the results seem to also assume that\u00a0\u00a0is diagonal. Is this assumption necessary, or is it presented for the diagonal case wlog? What is the key difficulty in generalizing it to non-diagonal matrices?\n    - There is no loss of generality to assume that $\\Sigma$ is diagonal, because the analysis of GD is invariant to the orthogonal transformation. In our updated version, we have mentioned this point when we make the assumption. Moreover, our research can be easily extended to general symmetric (not necessarily positive semi-definite) cases.\n- Explanation of incremental learning.\n    - Thank you for the suggestions. In our revised version, we have explained that in our context, incremental learning refers to the process where eigenvectors associated with larger eigenvalues are learned first.\n- The introduction describes matrix factorization as \"mirroring the training of a two-layer linear network\", but this doesn't hold for the symmetric matrix factorization studied here, which seems more similar to quadratic regression\n    - The training of a two-layer linear network can be interpreted as asymmetric matrix factorization (MF). See Section 2.2 in Sun et al. (2019). While there are distinctions between asymmetric and symmetric MF, the study of symmetric MF can be seen as a vital preliminary step towards analyzing asymmetric MF. Moreover, we would like to mention that asymmetric MF with a suitable regularizer\n     $$\\min_{X,Y}|\\Sigma-XY^T|_F^2+\\frac{1}{4}|X^T X-Y^T Y|_F^2$$\n    can be directly analyzed using the symmetric results.\n- Other minor points.\n    - We have updated our draft incorporating your valuable suggestions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700159261597,
                "cdate": 1700159261597,
                "tmdate": 1700159261597,
                "mdate": 1700159261597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h7vh3OUFPg",
                "forum": "fAGEAEQvRr",
                "replyto": "ub184HeD8y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8450/Reviewer_dfta"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8450/Reviewer_dfta"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Thanks for your answer.\n\nSome of the points raised in the responses would help readers if included in the main text, detailed below. \n\nI do not expect a response or a new version of the submission, those are only provided here as feedback.\n\n---\n\n**Connection between the studied setting and NNets, and motivation for a broader audience**  \nAs the disconnect between symmetric MF and neural networks has also been raised [by reviewer yiZg](https://openreview.net/forum?id=fAGEAEQvRr&noteId=M23bDOJ4nJ), an edit to this paragraph to make the motivation for the study of the asymmetric and symmetric MF (AMF, SMF) more explicit might help. \n\nThe current introduction states that neural networks and AMF are similar, and the reader is supposed to fill in the gaps as to how the study of SMF connects to the behavior of neural networks. What was missing from my first read, and the reason I found Figure 1 compelling, is an argument along the lines of \"incremental learning [short definition] has been reported in multiple models, including neural networks [citation]. This also happens in SMF, so let's try to describe this setting.\"\n\n\n**Implicit bias**\n> Firstly, [...] the algorithm almost surely converges to the global minimum[, which could be viewed as] implicit bias, because the optimization problem is non-convex.  \n    \nThe limiting point does not qualify my working definition of implicit bias if the algorithm converges to _the_ (implicitly unique) global minimum, as the choice of optimizer does not impact the solution selected, as long as the algorithm converges. I would follow if the argument was referring to a specific choice of $X$ and the generalization property depended on $X$ rather than on $XX^T$, which is unique and doesn't depend on which solution is selected by the optimizer, but this doesn't seem to apply here.\n  \n> Secondly, [...] incremental learning behaviors of GD during the training process [...] is typically useful when early stopping is utilized.\n  \nThe early stopping argument and its connection with the incremental learning idea is clearer. As the first point that comes to mind when the introduction talks about implicit bias, making this point explicit would help clear the confusion.\n\n\n**Incremental learning**  \nThe edit makes it clearer. However, the current description that \"Eigenvectors associated with larger eigenvalues are learned first\" seems incomplete. For example it also applies to a linear regression/convex quadratic functions (the error associated with the $i$th eigenvector evolves as $(1-\\alpha\\lambda_i)^t e^0_i$ where $\\alpha$: step-size, $\\lambda_i$: $i$th eigenvalue, $e^0_i$: initial error associated with the $i$th eigenvector). The penomenon studied here is, presumably, qualitatively different? For example, Gissin et. al (2020) describe it as \"the singular values of the model are learned separately, one at a time\", a component that seems missing from the current description."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8450/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601392108,
                "cdate": 1700601392108,
                "tmdate": 1700601392108,
                "mdate": 1700601392108,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]