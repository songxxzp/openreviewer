[
    {
        "title": "DebateGPT: Fine-tuning Large Language Models with Multi-agent Debate Supervision"
    },
    {
        "review": {
            "id": "27tBUjBYsR",
            "forum": "ChNy95ovpF",
            "replyto": "ChNy95ovpF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4835/Reviewer_9fkA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4835/Reviewer_9fkA"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called DebateGPT, which uses data generated through multi-agent debate to  fine-tune GPT-3.5. The authors show that DebateGPT achieves comparable performance to GPT-4 on various tasks, including instruction following, commonsense reasoning, and mathematics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method achieves impressive results, with DebateGPT showing comparable performance to GPT-4.\n2. The paper provides detailed explanations and analysis of the proposed method."
                },
                "weaknesses": {
                    "value": "1. If I'm understanding correctly, the Multi-Agent debate method used to generate data seems to following the approach in [1]. However, it's important to note that when comparing it to the baseline models, we're only looking at GPT-3.5 and GPT-4, and there's no direct mention of [1]. \n2. In the main contributions of the paper, it claims to \"offers a more economical alternative to leveraging models like GPT-4\". However, when we look at Figure 6, we see that both options are similarly priced. Furthermore, taking into account their divergent performance makes it difficult for me to fully support this claim.\n\n[1] Improving Factuality and Reasoning in Language Models through Multiagent Debate. https://arxiv.org/pdf/2305.14325.pdf"
                },
                "questions": {
                    "value": "1. Which LLMs are included in the Multi-Agent used for data generation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4835/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4835/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4835/Reviewer_9fkA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4835/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698480652193,
            "cdate": 1698480652193,
            "tmdate": 1699636466993,
            "mdate": 1699636466993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hLMSa4ptM1",
                "forum": "ChNy95ovpF",
                "replyto": "27tBUjBYsR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 9fkA (1/1)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review! We respond to the points below:\n\n**Question:If I'm understanding correctly, the Multi-Agent debate method used to generate data seems to following the approach in [1]. However, it's important to note that when comparing it to the baseline models, we're only looking at GPT-3.5 and GPT-4, and there's no direct mention of [1].**\n\n**Answer**: We thank the reviewer for the valuable comment. We have added the results of fine-tuning GPT3.5 using the data generated by the method proposed by [1] on three evaluation benchmarks. Our method consistently outperforms [1].\n| Model | AlpacaEval | Arithmetic | Winogrande |\n| -------- | --------------- | -------------- | ---------------- | \n| DebateGPT-3.5 Original Debate[1] | 85.64 | 50.30 | 84.19 |\n| DebateGPT-3.5 (Ours) | 91.91 | 55.30 | 86.97 | \n\n**Question:In the main contributions of the paper, it claims to \"offers a more economical alternative to leveraging models like GPT-4\". However, when we look at Figure 6, we see that both options are similarly priced. Furthermore, taking into account their divergent performance makes it difficult for me to fully support this claim.**\n\n**Answer**: Figure 6 shows that on 500 examples, generating responses to 500 examples with multi-agent debate costs 30 dollars less than GPT-4. As the number of examples increases, this cost will add up considerably. In our case with generating a dataset of 5K examples, we saved approximately 300 dollars using our approach with multi-agent debate. \n\nFurthermore, our performance is comparable with GPT-4. Table 1 shows that our method has a win rate accuracy of 88.6 and GPT-4 is 89.0. Table 6 in Appendix F (pasted below) shows that our method is consistently better than other methods at generating high quality data.\n\n| Model                            \t| Win Rate (higher is better) |\n|--------------------------------------|----------|\n| GPT-3.5                          \t| 74.2 \t|\n| Zero-Shot Chain of Thought       \t| 78.4 \t|\n| Multi-Agent Debate (Du et. al. 2023) | 77.2 \t|\n| Multi-Agent Debate (Ours)        \t| 81.2 \t|\n\n\n**Question: Which LLMs are included in the Multi-Agent used for data generation?**\n\n**Answer**: As stated in Section 3.2, we use GPT-3.5 in the multi-agent debate for data generation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714374133,
                "cdate": 1700714374133,
                "tmdate": 1700714374133,
                "mdate": 1700714374133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nacazTrmc0",
            "forum": "ChNy95ovpF",
            "replyto": "ChNy95ovpF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4835/Reviewer_8M4v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4835/Reviewer_8M4v"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces DebateGPT, which fine-tuned GPT-3.5 using a dataset generated through multi-agent debate, reducing the necessity for resource-intensive human intervention. By augmenting this method with techniques such as summarization, confidence scoring, and data refinement, the dataset's quality sees significant enhancement. DebateGPT demonstrates performance levels on par with GPT-4 across a diverse range of tasks, including domains like commonsense reasoning and mathematics. Notably, DebateGPT's distinguishing feature lies in its relatively compact size compared to GPT-4, all while relying on a modest dataset, thereby challenging the prevailing notion that larger models are heavily reliant on extensive human feedback."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "They employ a multi-agent debate approach to generate the dataset, and through the incorporation of elements like confidence scoring, summarization, and data cleaning, they demonstrate a noticeable enhancement in dataset quality when compared to the GPT-3.5 baseline."
                },
                "weaknesses": {
                    "value": "The primary contribution of this paper lies in its application of multi-agent debate for dataset creation. The experiments on dataset quality reveal that the introduction of multi-agent debate, along with summarization, confidence scoring, and data cleaning, results in an improved dataset quality. However, it's worth noting that the utilization of confidence scores and filtering techniques is a standard practice in the field of dataset augmentation, lacking any groundbreaking innovation apart from the incorporation of multi-agent debate. To make a meaningful assessment of its effectiveness, more comprehensive experiments should be conducted to compare these approaches with existing methods in data augmentation.\n\nFurthermore, the paper exclusively reports fine-tuned results on the dataset it generated. It remains unclear how fine-tuning the model on baseline datasets would perform in comparison. This omission prevents a thorough understanding of the extent to which a dataset with better quality can impact downstream tasks.\n\nAnother important consideration is the potential impact of fine-tuning on the model's generalization abilities for tasks beyond those presented in the paper. This aspect warrants further investigation to ascertain the overall implications of the fine-tuning process."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4835/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698514564830,
            "cdate": 1698514564830,
            "tmdate": 1699636466911,
            "mdate": 1699636466911,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rKmqBBs2Q0",
                "forum": "ChNy95ovpF",
                "replyto": "nacazTrmc0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 8M4v (1/1)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review! We respond to the points below:\n\n**Question: it's worth noting that the utilization of confidence scores and filtering techniques is a standard practice in the field of dataset augmentation, lacking any groundbreaking innovation apart from the incorporation of multi-agent debate.**\n\n**Answer**: Based on our knowledge, confidence scores and filtering techniques are the first time used for data generation under the multi-agent debate framework. We haven\u2019t seen any work use confidence scores or filtering specifically in designing text datasets. We would appreciate it if the reviewer could point to papers that use confidence scores and filtering techniques in the field of dataset augmentation. \n\n**Question: To make a meaningful assessment of its effectiveness, more comprehensive experiments should be conducted to compare these approaches with existing methods in data augmentation.**\n\n**Answer**: We thank the reviewer for the valuable comment. We include more baseline methods for generating data and measure the data quality of each method in comparison with our design with multi-agent debate on 1000 examples randomly sampled from Alpaca. We report this in Appendix F. We see that our method consistently generates higher quality data than every other prompting method. We also include this table here:\n\n| Model                            \t| Win Rate (higher is better) |\n|--------------------------------------|----------|\n| GPT-3.5                          \t| 74.2 \t|\n| Zero-Shot Chain of Thought       \t| 78.4 \t|\n| Multi-Agent Debate (Du et. al. 2023) | 77.2 \t|\n| Multi-Agent Debate (Ours)        \t| 81.2 \t|"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714207009,
                "cdate": 1700714207009,
                "tmdate": 1700714207009,
                "mdate": 1700714207009,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dYrZBvbqPA",
            "forum": "ChNy95ovpF",
            "replyto": "ChNy95ovpF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4835/Reviewer_x4r2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4835/Reviewer_x4r2"
            ],
            "content": {
                "summary": {
                    "value": "Pros: \n1. The author proposes a novel framework that utilizes multi-agent debate for data generation and supervised training. Multiple interesting features could be seen such as the confidence score, summarization, and cleaning.\n2. The results presented in the paper show that the model has increased performance on various benchmarks including commonsense reasoning, mathematics, factuality, etc.\n\nCons: \n1. The supervised data seems composed of questions and answers given by the multi-debate framework, rather than multi-agent debate demonstrations. It does not make so much sense to me as there are incorrect answers in the answers as well, right? (Plz correct me if any falsehood) Have the authors tried using answers given by human annotators?\n2. The author claims that the DebateGPT-3.5 is an economical alternative for GPT4, which could easily regarded as over-hyping (I am NOT doubting the results). However, the author should present more enriched results on more benchmarks.\n\nQuestions: \n1. Human-in-loop is so expensive,  utilizing other methods including debate might be a panacea, but need to carefully investigate. Does the multi-agent debate framework apply to the evaluation in the experiments?\n2. The confidence score sounds interesting, however, the reliability of the scalar given by LLM is still worth doubting (an empirical concern from the reviewer), has the author done any faithfulness investigation on that?\n3. The summarization makes sense to me, however, the answers from other agents will be compressed into a short answer, which inevitably brings a loss of information, has the author tried to handle that problem?\n4. agents only debate one time before issuing the final answer. How could they possibly converge to an answer with only ONE iteration? It sounds a little bit brutal to simply generalize the results before multi-iteration convergence (Du et al., 2023)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "See summary"
                },
                "weaknesses": {
                    "value": "See summary"
                },
                "questions": {
                    "value": "See summary"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4835/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698930447706,
            "cdate": 1698930447706,
            "tmdate": 1699636466837,
            "mdate": 1699636466837,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "w2lRvbwlsN",
                "forum": "ChNy95ovpF",
                "replyto": "dYrZBvbqPA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4835/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4835/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer x4r2 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review! We respond to your points below. \n\n**Question: The supervised data seems composed of questions and answers given by the multi-debate framework, rather than multi-agent debate demonstrations. It does not make so much sense to me as there are incorrect answers in the answers as well, right? (Plz correct me if any falsehood) Have the authors tried using answers given by human annotators?**\n\n**Answer**: This is correct. We construct our dataset by extracting the question and the final round response since our findings show that responses in later rounds are more accurate than responses in earlier rounds of multi-agent debate. \nWe show the results comparing DebateGPT-3.5 with a GPT-3.5 fine-tuned on the entire debate demonstration. We randomly select 100 examples from two evaluation datasets, GSM and MMLU, respectively. In the table below, we see that that fine-tuning on the final response is superior to fine-tuning on the entire debate demonstration. This is because when we fine-tune using the entire debate demonstration, the fine-tuned model is confused by the incorrect information in the early rounds of debate. Another reason is that the length of some demonstrations exceeds the context window length of GPT-3.5 and the conversations in later rounds of debate are discarded. Thus the method we proposed in the paper that utilizes the most accurate final debate results achieves better performance.  \n\nThe results have been added to Appendix H of the paper and are included below. \n\n| Model | MMLU | \n| -------- | ------- | \n| GPT-3.5 | 62.00 |\n| DebateGPT-3.5 (Full Debate) | 68.00 |\n| DebateGPT-3.5 (Ours) | 73.00 |\n\nMulti-agent debate can generate incorrect answers like any automatic generation method. However, our goal in this work is to avoid human annotation, which is expensive, time-consuming, and hard to scale, while constructing datasets (See paragraph 1 of our introduction in our paper) . Based on this, our paper shows that multi-agent debate can generate high quality data in an efficient manner and this high quality data can lead to stronger fine-tuning results as shown in the paper and the results above.  \n\n**Question: Human-in-loop is so expensive, utilizing other methods including debate might be a panacea, but need to carefully investigate. Does the multi-agent debate framework apply to the evaluation in the experiments?**\n\n**Answer**: We assume the reviewer is asking whether multi-agent debate can be added as an evaluation baseline. To be clear, multi-agent debate is only used for data generation in our paper. Our evaluation only uses a single agent the same as baseline methods for fair comparisons. However, multi-agent debate can also be used for more accurate evaluation which is not the focus of this paper.\n\n\n**Question: The confidence score sounds interesting, however, the reliability of the scalar given by LLM is still worth doubting (an empirical concern from the reviewer), has the author done any faithfulness investigation on that?**\n\n**Answer**: This is a good point! We have conducted a preliminary investigation into how reliable the confidence score given by the LLM is. Based on our results, LLMs usually have a confidence score > 90 and are even confident in incorrect answers. However, in multi-agent debate, we find that the agents will adjust their response based on the response from other agents, and responses with higher confidence scores are more likely to be accepted by the other agents. \n\nTo better understand the reliability of our confidence score, we design a small human evaluation. We sample 100 examples from our dataset. For each example, we had the evaluator analyze the quality of the response. The evaluator then analyzed the confidence score in the final round. If the confidence score was low (< 95) and the quality of the response was low, or if the confidence score was high (>= 95) and the quality of the response was high, then we say that the confidence score reliably reflects the quality of the response. We calculated the percentage of examples where the confidence score reliably reflects the response quality according to our definition in the 100 examples. We find 82\\% of confidence scores in the last round of debate accurately reflected the correctness.  Our human evaluation shows that the confidence scoring method in our paper can reflect the relative accuracy of the generated response, which is reliable enough as a reference when agents participate in multiagent debate."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4835/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713382855,
                "cdate": 1700713382855,
                "tmdate": 1700713382855,
                "mdate": 1700713382855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]