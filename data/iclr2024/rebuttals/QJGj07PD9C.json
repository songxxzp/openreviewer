[
    {
        "title": "Guaranteed Approximation Bounds for Mixed-Precision Neural Operators"
    },
    {
        "review": {
            "id": "NMygitKrb0",
            "forum": "QJGj07PD9C",
            "replyto": "QJGj07PD9C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission187/Reviewer_5SPZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission187/Reviewer_5SPZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper intends to reduce the cost of training and inference of FNO. The method used here include training by mixed-precision to reduce memory and time cost. The mixed-precision training technique proposed here is specialized for FNO, where bounds for precision error and approximation error are theoretically analyzed and guaranteed. The experiment results show that the proposed method greatly improves the speed and throughput of FNO while not sacrificing accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "**Originality:** The novelty of this paper lies in leveraging the analysis of precision error in Fourier transform, which successfully reduced the computational cost and maintained the accuracy of FNO.\n\n**Quality:** The quality of the paper is high in both theoretical analysis and experiment conduction.\n\n**Clarity:** The paper delivers its idea in a quite clear and detailed way.\n\n**Significance:** For people who intend to have a large scale of implementations of FNO, this work can be very interesting to them."
                },
                "weaknesses": {
                    "value": "The major doubt for this paper is regarding the significance. While it is dedicated to improving FNO, an important architecture of neural operators, FNO is not the only one and arguably the best one. If FNO is yet to be a dominant model for large scale applications of neural operators, the significance of this work is limited."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission187/Reviewer_5SPZ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698467051110,
            "cdate": 1698467051110,
            "tmdate": 1699635944510,
            "mdate": 1699635944510,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5r9Gu422Eu",
                "forum": "QJGj07PD9C",
                "replyto": "NMygitKrb0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 5SPZ"
                    },
                    "comment": {
                        "value": "We thank you for your thoughtful review. We appreciate that you find our work has high quality theoretical and empirical results, and that our work is novel. Furthermore, we appreciate that you found our work to be clear and detailed. We reply to your questions below.\n\n**Significance.**\nThank you for motivating this discussion! We give our answer in two parts; first, by motivating the large class of high-performing models that use FNO as a base, and second, by presenting new theoretical results that generalize our original results beyond FNO.\n\nFirst, our method and findings hold true for any model that is based on FNO, including a large class of state-of-the-art models. Although initially solving classical PDEs, the versatility of FNOs for real-world applications is [gaining recognition](https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/). Our experiments include architectures such as GINO, SFNO that are different from regular FNO and apply to [climate modeling](https://arxiv.org/abs/2306.03838) and [3D car geometries](https://arxiv.org/abs/2309.00583). Recently, new research such as [FourCastNet](https://arxiv.org/abs/2202.11214) have been unveiled that leverages FNO variants and targets high-impact applications, including weather and climate modeling. \n\nSecond, we still agree that our work could be even more impactful if we generalize our results. Therefore, we have now added **new theoretical results**, that generalize our original bounds using the Fourier basis, to arbitrary basis functions (Theorems A.1 and A.2 in Appendix A). This extends our results beyond Fourier neural operators to a larger class of integral operators, that can include e.g. Wavelet bases or graph neural operators. We also have initial simulation results which confirm our new theoretical results, in Appendix A.3.\n\nFinally, we would also like to highlight a few other updates we made to our paper\n- We give new experiments that directly compare our theoretical resolution and precision upper and lower bounds, to the empirical discretization and precision error (Appendix A.2). We find that the errors behave as expected by our theory, which is additional confirmation of the correctness of our theory and the applicability of our results.\n- We added another experiment setting, the Ahmed body CFD dataset. On this high-resolution 3D task, we achieve 38.4% memory reduction while the error stays within 0.05% of the full-precision model.\n\nThank you once again, for your excellent point. Please let us know if you have any follow-up questions or further suggestions. We would be happy to continue the discussion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700188493728,
                "cdate": 1700188493728,
                "tmdate": 1700188493728,
                "mdate": 1700188493728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HWfKmZ8j06",
                "forum": "QJGj07PD9C",
                "replyto": "5r9Gu422Eu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission187/Reviewer_5SPZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission187/Reviewer_5SPZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. I have read other reviews and would like to keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630761279,
                "cdate": 1700630761279,
                "tmdate": 1700630761279,
                "mdate": 1700630761279,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9nXn9Ng4z0",
            "forum": "QJGj07PD9C",
            "replyto": "QJGj07PD9C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission187/Reviewer_NM9S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission187/Reviewer_NM9S"
            ],
            "content": {
                "summary": {
                    "value": "The authors demonstrated mixed-precision training of FNOs on GPUs.  They derived theoretic bounds on the effect of rounding error, improved AMP to support complex arithmetic, addressed an instability by introducing a pre-activation thah, and conducted experiments on practical datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Application is of significance.\n- Well written. \n- Code released."
                },
                "weaknesses": {
                    "value": "> It would be helpful to include an illustration of empirical characterization of rounding error in direct comparison with the theoretical bound scaling law.   \n> Mixed-precision with FP8 has been proposed, how does the method fare with FP8?"
                },
                "questions": {
                    "value": "See minor requests above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791373290,
            "cdate": 1698791373290,
            "tmdate": 1699635944427,
            "mdate": 1699635944427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HtOjaXmIkQ",
                "forum": "QJGj07PD9C",
                "replyto": "9nXn9Ng4z0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer NM9S"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback. We appreciate that you find our work is well-written and covers an application that is significant. We reply to your points below.\n\n**W1: Illustration of empirical characterization.**\nThank you for the suggestion; this is a great idea! Following your suggestion, we added new experiments to Appendix A.2 that directly compare our theoretical resolution and precision upper and lower bounds, to the empirical resolution and precision error. We find that the discretization error is higher than the precision error, as expected by our theory, which is additional confirmation of the correctness of our theory and the applicability of our results. These experiments also make it easy to see qualitative features of our theoretical results, such as how the discretization error decays as the grid size increases, while the precision error stays constant.\n\n**W2: Mixed-precision with FP8.**\nThank you, this is another interesting suggestion. Despite there being no native support for FP8 in pytorch, we are able to simulate FP8 training through two approaches: \n1. Calibrating network activations from FP16 to FP8 using two format options, E5M2 and E4M3. Unfortunately, for both formats, the training instantly fails in the first iteration due to FNO producing inf values. While prior work has used FP8 for GEMM and convolution operations (as in [this paper](https://proceedings.neurips.cc/paper_files/paper/2018/file/335d3d1cd7ef05ec77714a215134914c-Paper.pdf), to the best of our knowledge, we are not aware of any work that computes Fourier transforms in FP8. \n2. Clipping out-of-range activations to the upper and lower limits of FP8. To mitigate training divergence, we constantly check for out-of-range values and clip them to the extrema of FP8 formats. Although training could proceed as normal, the FP8-FNO is unable to converge as shown in the Darcy Flow training curve comparing FP8 and FP16 training. Training curves are added in Appendix B.11.\n\nFurthermore, the worse result of FP8 is actually predicted by our theory. In our paper, Theorem 3.2 showed that for FP16, precision error is comparable to discretization error when plugging in $\\epsilon=10^{-4}$, the multiplicative precision of FP16. However, for FP8 $\\epsilon>10^{-2}$, and we can no longer prove that the precision error is lower than the discretization error. We have added this discussion to Appendix B.11.\n\nFinally, we would also like to highlight a few other updates we made to our paper\n- Improved theoretical bounds: while we originally proved bounds using the Fourier basis, we have now proven matching results for arbitrary basis functions (Appendix A.2).\n- We added another experiment setting, the Ahmed body CFD dataset. On this high-resolution 3D task, we achieve 38.4% memory reduction while the error stays within 0.05% of the full-precision model.\n\nThank you once again, for your favorable assessment of our work and your excellent suggestions. We would be very happy to answer follow-up questions or follow any additional suggestions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700188285303,
                "cdate": 1700188285303,
                "tmdate": 1700188285303,
                "mdate": 1700188285303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XPXJGHz94b",
            "forum": "QJGj07PD9C",
            "replyto": "QJGj07PD9C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission187/Reviewer_dRFB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission187/Reviewer_dRFB"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors provide a way to apply mixed-precision training to Fourier Neural Operators. The authors point out that while some previous approaches have used mixed-precision training with FNO, they have done so only on the real-valued parameters (i.e, the linear weights, and biases) and not on the FFT/DFT that is applied to the input in each layer. \n\nThey first show that mixed-precision training should not cause extra error in the training due to the Fourier transform, since the discretization error from FFT\u2192 DFT is orders of magnitude bigger than the DFT\u2192 half-precision DFT. \n\nFor the application of mixed-precision on complex tensors (the output after an FFT/DFT application) the authors apply mixed-precision by converting each \u201ctensors to real\u201d (this is something that the authors don\u2019t elaborate upon enough). \n\nThe authors show through their experiments, that they are indeed able to train various FNO based architectures with low-memory and hence are also able to increase the throughput of their runs while only taking a less than 1 percent hit in the performance as compared to non-mixed-precision baselines.\n\nThe authors also point to the use of a tanh based pre-activation that helps in mitigating the mixed-precision based overflow that usually occurs when trained FNO based architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow.\n\nThe experimental results of the paper are impressive. The authors are able to increase the training throughput for navier stokes $1.41$ times, by achieving an almost 50 percent memory reduction. \n\nThe use of tanh activation as a pre-activation to the neural operator block is a good technique to reduce the overflow issue, and an important finding. \n\nThe authors provide a theoretical proof for why the error for the mixed-precision training would be negligible when compared to the discretization error for DFT, while the results there are pretty standard its a good addition to have."
                },
                "weaknesses": {
                    "value": "I think that the primary methodology as to how the mixed-precision is applied to the Fourier Kernel is not clear. From the looks of it the mixed-precision approximation is applied to the weights in the complex domain (that are used in the kernal operator).\n\nHowever, this is something that seems to be different from what they say in the introduction, where they mention that they enable some mixed precision in the entire FNO block (which I assumed will try to also enabled mixed precision in the DFT algorithm). \n\nFrom the looks of it the primary contribution seems to be the addition of tanh, and the application of mixed precision in the complex domain by treating the real and the imaginary components as two distinct real tensors. Together, the overall contribution does not seem very novel in of itself."
                },
                "questions": {
                    "value": "The authors mention that neural operators are discretization convergent, however have not cited any relevant work around it. While empirically we know that for some FNO based architecture we can get zero shot super-resolution, are there any works that prove it? In general, adding relevant citation to that claim would be useful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792365260,
            "cdate": 1698792365260,
            "tmdate": 1699635944334,
            "mdate": 1699635944334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zgSvtcmwhX",
                "forum": "QJGj07PD9C",
                "replyto": "XPXJGHz94b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer dRFB"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review the manuscript and provide valuable feedback. We are glad to see that you found our experimental results to be impressive. We address your questions below.\n\n**W1: Unclear methodology.**\nThank you for pointing this out. We are happy to clarify that our methodology does run the entire FNO block in half-precision, with the forward FFT, the tensor contraction, and the inverse FFT all in half precision. We have now updated Section 4.2 and Figure 2 to clarify this further.\nBased on your question, we now also added a more extensive ablation study on which FNO block operations to keep in half-precision, which we have added to Appendix B.7. Forward FFT, tensor contraction, and inverse FFT can each be set to each full or half precision, creating a total of 8 settings. Our study shows the half-precision FNO block (our method) achieving better runtime, memory usage, and training error than all 7 other settings (this is consistent with our original Darcy flow results, in which half-precision outperforms full-precision).\n\nThe full results are as follows: \nPerformance of each setting on Darcy Flow. The fully half-precision setting (our method) is advantageous across all the metrics.\n\n| Forward FFT (F/H) | Contraction (F/H) | Inverse FFT (F/H) | Runtime per epoch (sec) | Memory (MB) | Train Error (L2 loss) |\n|-------------------|-------------------|-------------------|------------------------|-------------|-----------------------|\n| F             \t| F             \t| F             \t| 17.06              \t| 8870    \t| 9.00              \t|\n| F             \t| F             \t| H             \t| 16.55              \t| 8908    \t| 8.71              \t|\n| F             \t| H             \t| F             \t| 17.11              \t| 8614    \t| 8.76              \t|\n| F             \t| H             \t| H             \t| 16.96              \t| 8658    \t| 8.15              \t|\n| H             \t| F             \t| F             \t| 17.64              \t| 7824    \t| 9.13              \t|\n| H             \t| F             \t| H             \t| 16.81              \t| 8004    \t| **7.51**          \t|\n| H             \t| H             \t| F             \t| 16.57              \t| **7558**\t| 8.75              \t|\n| H             \t| H             \t| H             \t| **15.63**          \t| **7550**\t| **7.49**          \t|\n\n\n**W2: On novelty.**\nWe respectfully note that our novelty \u201clies in leveraging the analysis of precision error in Fourier transform, which successfully reduced the computational cost and maintained the accuracy of FNO\u201d (as pointed out by reviewers XXPr, 5SPZ). Second, the goal of our paper is to establish a method that improves the throughput and memory footprint within the domain of scientific machine learning and particularly neural operators. In the literature prior to our work, it was not clear if mixed-precision techniques could be used in this domain, especially since classical numerical solvers traditionally suffer from stability issues and sometimes need **higher** than standard precision. However, our paper shows that numerical stability is quite promising (with up to 50% reduction in memory and throughput) and our work generally lays a theoretical and empirical foundation in this area.\n\n**Q1: discretization convergent.**\nThank you for this question. Yes, [this paper](https://www.jmlr.org/papers/v24/21-1524.html) gives a formal proof of discretization convergence for neural operators, including FNO (Section 9.2, Theorem 8). We have now added this reference in the paper. This shows that the approximation error is approximately constant as the resolution increases. As you mentioned, we also empirically verify this property via our zero-shot super-resolution experiments (Table 1).\n\nWe would also like to highlight a few other updates we made to our paper\n- Improved theoretical bounds: while we originally proved bounds using the Fourier basis, we have now proven matching results for arbitrary basis functions (Appendix A.2).\n- We added another experiment setting, the Ahmed body CFD dataset. On this high-resolution 3D task, we achieve 38.4% memory reduction while the error stays within 0.05% of the full-precision model.\n\nThank you very much, once again, for your generally positive view of our work and your excellent suggestions. If you find our responses satisfying, we respectfully ask that you consider increasing your score. On the other hand, we would be very happy to answer any follow-up or additional questions you have."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700188160809,
                "cdate": 1700188160809,
                "tmdate": 1700188160809,
                "mdate": 1700188160809,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "16hqpwuhZg",
            "forum": "QJGj07PD9C",
            "replyto": "QJGj07PD9C",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission187/Reviewer_XXPr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission187/Reviewer_XXPr"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a mixed-precision training method for neural operators, focusing on Fourier Neural Operators (FNO) used in solving partial differential equations and other mappings between function spaces. The paper discusses the challenges of high-resolution training data, limited GPU memory, and long training times in the context of neural operators. It emphasizes the need for mixed-precision training to mitigate these issues. The paper demonstrates that, contrary to our expectations, mixed-precision training in FNO does not lead to significant accuracy degradation. It presents rigorous theoretical characterization of approximation and precision errors in FNO, highlighting that the precision error is comparable to the approximation error. The paper introduces a method for optimizing memory-intensive tensor contractions using mixed precision, reducing GPU memory usage by up to 50% and improving throughput by 58% while maintaining accuracy.\n\n These findings have the potential to advance the efficiency and scalability of neural operators in various downstream applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The strengths of the paper are as follows:\n\n1. Mixed-Precision Training for Neural Operators: The paper introduces the first mixed-precision training routine tailored specifically for neural operators. This novel approach optimizes the memory-intensive tensor contraction operations in the spectral domain and incorporates the use of tanh pre-activations to address numerical instability. \n\n2. Theoretical Results; The paper provides a strong theoretical foundation for its work by characterizing the precision and discretization errors of the FNO block. It demonstrates that these errors are comparable and proves that, when executed correctly, mixed-precision training of neural operators does not lead to significant performance degradation. \n\n3. Experimental Evaluation: The paper conducts thorough empirical validation of its mixed-precision training approach on three state-of-the-art neural operators (TFNO, GINO, and SFNO) across four different datasets and GPUs. The results indicate that the method significantly reduces memory usage (using half the memory) and increases training throughput by up to 58% across various GPUs, all while maintaining high accuracy with less than 0.1% reduction. \n\n4. Open-Source Code: The paper provides an efficient implementation of its approach in PyTorch, making it open-source and providing all the necessary data to reproduce the results."
                },
                "weaknesses": {
                    "value": "While the method of using tanh pre-activation before each FFT seems to avoid numerical instability, it would have been better if some theoretical justification was given (even for simplistic cases). I believe similar theory should have been provided for the learning rate schedule. However, the paper indeed contributes significantly in theoretical aspects (in Section 3) by characterizing the precision and discretization errors of the FNO block and showing that these errors are comparable. Given the strong theoretical contributions, the above is not significant. Moreover, a strong ablation study for tanh is provided in Appendix B.5"
                },
                "questions": {
                    "value": "The work is indeed very interested and provides a strong contribution to the research community. My questions are:\n\n1. Why is it that the standard mixed precision training used for training ConvNets and ViTs is ineffective for training Fourier Neural Operators?\n2. Similarly, why is is that the common solutions (loss scaling, gradient clipping, normalization, delaying updates) fail to address the numerical instability of mixed-precision FNO?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887404094,
            "cdate": 1698887404094,
            "tmdate": 1699635944255,
            "mdate": 1699635944255,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XwLA6Pz8aL",
                "forum": "QJGj07PD9C",
                "replyto": "16hqpwuhZg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer XXPr (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your favorable and detailed review. We appreciate that you find our work very interesting and provide a strong contribution to the research community. Further, we are glad that you found that our work has the potential to advance the efficiency and scalability of neural operators in downstream applications. We address each of your questions below:\n\n**W1: Justification for pre-activation before each FFT.** \nThis is a great question. In fact, pre-activation is predicted by our theory, and we have now updated our paper to point this out. Given a function $v(x)$ defined on $[0,1]$, as long as $|\\max_x v(x)-\\min_x v(x) |>1$, then $\\text{tanh}(v(x))$ has both a lower $L_\\infty$ norm and a lower Lipschitz constant compared to $v(x)$, both of which are terms that show up in the upper bounds of Theorem 3.1 and Theorem A.1. In other words, tanh decreases the magnitude and gradient of the functions, which decrease the theoretically-predicted discretization and precision errors. Finally, as you say, we also have an ablation study that empirically motivates tanh in Appendix B.8.\n\n**W2: Justification for learning rate schedule.**\nFirst, we would like to clarify that you meant the precision schedule (our contribution), instead of the learning rate schedule (well-known in the literature)? The precision schedule follows the following intuition: in the early stages of training, it is okay for gradient updates to be more \u201ccoarse\u201d, since the gradient updates are larger overall, so full precision is less important. However, in the later stages of training, the average gradient updates are much smaller, so full precision is important. In fact, we find that at the end stages of training, the magnitudes of some gradients in the FNO block fall below $10^{-4}$, which is below the precision constant of float16. This aligns with the empirical results of our precision schedule experiments: full precision performs well at the last stages of training.  We have now updated our manuscript with this explanation.\n\n**Q1: Standard techniques ineffective for FNO.**\nThis is a great question; we have now updated the paper to clarify this. The standard techniques are designed for common operations such as convolution and transformers, however, the most memory-intensive operations of FNO are in the spectral domain, which are not supported by AMP and require a different procedure that takes into account both casting to half precision as well as several vectorization operations (moving to and from complex-valued representation). Since complex-valued operations are not supported in AMP, it simply skips the most important operations inside the FNO block, and therefore only results in a small improvement for FNO. We introduce the first mixed-precision training method for the FNO block by devising a simple and lightweight greedy strategy to find the optimal contraction order, taking into account the vectorization operations for each intermediate tensor. Furthermore, numerical stability methods also must be handled differently for neural operators, since the typical methods fail, which we discuss below in Q2.\n\n(1/2)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187949371,
                "cdate": 1700187949371,
                "tmdate": 1700187949371,
                "mdate": 1700187949371,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r8HkPp5AzZ",
                "forum": "QJGj07PD9C",
                "replyto": "16hqpwuhZg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission187/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer XXPr (2/2)"
                    },
                    "comment": {
                        "value": "**Q2. Common numerical stability techniques.**\nThank you for the question! While the main source of instability in mixed-precision neural nets is the gradient updates, we find that for neural operators, there is even more numerical instability within the forward FFT operations. Therefore, the standard techniques such as loss scaling, gradient clipping, and delayed updates, are focused on reducing the dynamic range of the gradients, but for neural operators, we are interested in addressing the numerical instability of the FFT. We now further verify these claims by significantly extending our original ablation study, which we have also added to Appendix B.5.\n- Loss scaling: we now confirm in Appendix B.5 that the typical loss scaling (GradScaling) is ineffective, due to the reason above. Instead, we tried scaling just before each FFT. However, this fails due to the wide range of values. For example, if we typically have a few outliers of $10^6$, while most of the numbers are within the range $[-1,1]$, then we would expect scaling to perform much worse than tanh, since tanh negates the outliers while keeping the same range in the rest of the values. We confirm this in a new ablation study in Appendix B.5.\n- Gradient clipping: this also does not work due to the above reason. In our new ablation study, we clip gradients to a default value of 5 during training, yet it does not resolve the NaN issue. Instead, we tried two different forms of clipping the activations just before the FFT (from our original submission), in Appendix B.5.\n- Normalization: note that we already do have normalization in our models. In our new ablation study, we added another normalization layer before the FFT, which delays the NaN occurrence only for 2 epochs of training. Additionally, this incurs a 30% GPU memory overhead due to intermediate values stored for the backward pass. Our tanh approach is more lightweight and effective. \n- Delaying updates: for gradient accumulation, we tried updating weights every 3 batches, which did not resolve the NaN problem. \n\nFinally, we would like to highlight a few other updates we made to our paper\n- Improved theoretical bounds: while we originally proved bounds using the Fourier basis, we have now proven matching results for arbitrary basis functions (Appendix A.2).\n- We added another experiment setting, the Ahmed body CFD dataset. On this high-resolution 3D task, we achieve 38.4% memory reduction while the error stays within 0.05% of the full-precision model.\n\nThank you very much, once again, for your positive view of our work and your excellent suggestions. We would be happy to answer any additional questions.\n(2/2)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700187991302,
                "cdate": 1700187991302,
                "tmdate": 1700187991302,
                "mdate": 1700187991302,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]