[
    {
        "title": "To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets"
    },
    {
        "review": {
            "id": "qW9xBqp5YJ",
            "forum": "UHjE5v5MB7",
            "replyto": "UHjE5v5MB7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8168/Reviewer_4cwR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8168/Reviewer_4cwR"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the interplay between memorization and generalization in a controlled setting of synthetic task, where analytical solution exists and can be found by neural network optimizers. It demonstrates interesting behavior where generalization and memorization co-exists, and identified subnetworks that are responsible for each behavior. It further studies how popular regularization techniques impact the interplay between memorization and generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper presented a simple synthetic machine learning setting where memorization and generalization can be studied in a very controlled manner. It further demonstrate that memorization and generalization could co-exist, and when it happens, there are clear distinction of subnetworks (at neuron level) that are responsible for each aspect. Moreover, by using the inverse participation ration (IPR), those neurons can be easily identified. This allows the authors to conduct further analysis on how different regularization techniques impact generalization and memorization from the perspective of memorization neurons. For example, it is shown that batch normalization operates in a very different manner from dropout and weight decay. Although the task of study is completely synthetic, it is still valuable to have such a controlled task that demonstrate interesting neural network behaviors such as grokking and memorization/generalization."
                },
                "weaknesses": {
                    "value": "While some of the results are interesting, this paper is not strong in novelty and depth.\n\n1. The main synthetic task studied in this paper is from previous papers, including the construction of the analytical solution and the matching neural network architecture that could realize such solutions. The main technique to distinguish memorization and generalization neurons, the inverse participation ration (IPR), is also from previous literature. While this paper does has some interesting observations when analyzing the memorization-generalization behaviors, I think it could benefit significantly from more novel analytical or algorithmic contributions.\n\n2. The title and motivation put a lot of emphasis on grokking, yet there are no in-depth analysis of the grokking behaviors presented in the paper. I think the paper could be improved if it could show that the analytical tools used in this paper could bring new insights to our understanding of the grokking behaviors.\n\n3. The experiments could be improved by more in-depth analysis. See below for a few examples.\n\n    1. The paper shows regularizers help improve the generalization and demonstrated the \"inversion\" behaviors. However, given the emphasis on \"grok\" from the motivations, I would expect more in-depth analysis and insights to how those regularizers impact the *learning dynamics* that lead to the end result of \"inversion\" or \"coexistence\".\n\n    2. In the case of co-existence, are there any consistent patterns on which neurons would become memorization and which would become generalization? Are they mostly in the lower layer or the upper layer? Do they have different learning dynamics? \n\n    3. The paper studies the two-layer neural networks with quadratic activation function because analytical solution could be admitted with this architecture. However, most of the empirical studies (e.g. the phase diagrams) does not rely on this property. I think it is very helpful to see experiments with more realistic deep neural networks and diverse neural network architectures to see if they demonstrate consistent behaviors, and if not, to analyze what contribute to the different behaviors. There are some experiments in the appendix but only with small variations of the activation function or the width, and without in-depth analysis."
                },
                "questions": {
                    "value": "1. The paper uses p=97. Why such a value is chosen? Does the behavior critically depend on the actual value of p (odd, even, prime, very small, very large, etc.)?\n\n2. In Fig.6, the test accuracy remains almost perfect even after 60% of the neurons being pruned. How does the ratio (60%) depend on various aspects of the underlying task? Does it remain constant when the network size changes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697669379562,
            "cdate": 1697669379562,
            "tmdate": 1699637012755,
            "mdate": 1699637012755,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2ICxwzq50m",
                "forum": "UHjE5v5MB7",
                "replyto": "qW9xBqp5YJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8168/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8168/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comprehensive feedback and detailed suggestions.\n\n## Novelty\nPlease refer to the corresponding section in the Global Response.\n\n## Experiments with diverse architectures\nPlease refer to the corresponding section in the Global Response.\n\n## Impact of regularization on learning dynamics\nPlease refer to the corresponding section in the Global Response.\n\n## Choice of p\n\nNeural networks can generalize on modular addition tasks for all natural numbers $p$, given a large enough width. Furthermore, we find this to be true even on more general modular arithmetic tasks (such as $(mn)\\\\%p$ and $(m^2+n^2)\\\\%p$, etc.). \nHowever, formally, general modular arithmetic tasks are only well-defined over finite fields -- which require the modular base to be a prime number. We find that network training circumvents this subtlety in many cases.\n\nThe size of the dataset also depends on this choice -- the number of total examples is $p^2$. Hence, a very small $p$ (such as $2,3,5$) would prevent learning. On the other hand, a very large $p$, while learnable, would demand excessive computing resources. The prime $p=97$ is the choice from the original paper on Grokking by Power et al. (2022). This choice strikes a reasonable balance between the aforementioned factors, but is in no way unique.\n\n## How many neurons can be pruned without losing performance?\n\nWe would first like to clarify that perfect generalization in the pruning experiment (Figure 5) remains **even beyond** pruning $60\\\\%$ of the low IPR neurons. Figure 6(left) showed only up to $70\\\\%$ pruning to explain the (sole) effect of pruning out low IPR neurons. We have added the extended version of this pruning experiment in Appendix I.1 (Figure 17(a)), where one can see that it is possible to prune $\\\\sim 88\\\\%$ of the neurons without losing test performance. We find that for $N=500$, the number of remaining neurons after maximal pruning is $\\\\sim 60$ ($N$ is the network width).\n\nAccording to the analytical solution (Equation 2), $\\\\lceil p/2 \\\\rceil$ neurons with different frequencies are required for the network to generalize correctly. Empirically, in real networks, the periodicity in neurons is approximate and the neuronal frequencies are not always unique. Consequently, real networks require more than $\\\\lceil p/2 \\\\rceil$ neurons for proper generalization. This is reflected in the aforementioned number $60$ being higher than $\\\\lceil 97/2 \\\\rceil=49$. \n\nWe also empirically find that wider networks are more prone to such effects, and hence require more neurons for generalization. We have added this experiment in Appendix Q (Figure 32). As such, we see a monotonic, albeit sub-linear, increase in the number of neurons required with increasing network width. Note that the ratio of prune-able neurons *increases* substantially due to the sub-linearity with width. For N=10000, we find that $\\\\sim 99\\\\%$ of the neurons can be pruned while retaining generalization. In this case, the network requires $\\sim 100$ neurons to maintain generalization.\n\nMoreover, the number of prune-able neurons also weakly depends on the phase in which the network lies (coexistence/partial-inversion/full-inversion.) Intuitively, due to the scarcity of periodic neurons in the coexistence phase, the network seems more economical, compared to the inversion phases. These comparisons are explained in Figure 17 of Appendix I.1 of the revised manuscript.\n\n## New insights on grokking\n\nGrokking on modular arithmetic datasets has been observed in multitudes of setups with different architectures, optimizers, regularization schemes (or lack thereof), loss functions, etc. These setups, despite having very different dynamics, feature similar grokking behaviours. Consequently, we focus our attention on the common denominator, namely the dataset. We study how changing/corrupting the data affects these behaviors and also present new behaviors emerging out of this examination. As we mention in the Novelty section, on modular arithmetic datasets, label corruption doesn't just introduce conflicting examples -- it leads to a new modular arithmetic task. Thus, robust generalization on the target task with significant label corruption points to a bias towards a certain class of modular tasks, possibly ones that can be learnt with an algorithm similar to Hypotheses 2.1, 2.2. We believe that these insights are valuable for future investigations on grokking.\n\nAnother important aspect of grokking is the tussle between memorization and generalization. With that in mind, we demonstrate that one can tune the generalization/memorization performance by tuning label corruption and regularization. We also characterize these behaviors using phase diagrams and interpret them using detailed IPR results. Moreover, our analysis also generalizes to various network architectures. All in all, we think that our experiments and tools will significantly advance the understanding of the subject."
                    },
                    "title": {
                        "value": "Response (1/2)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8168/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522617488,
                "cdate": 1700522617488,
                "tmdate": 1700523185072,
                "mdate": 1700523185072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pnFGjrLCdB",
            "forum": "UHjE5v5MB7",
            "replyto": "UHjE5v5MB7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8168/Reviewer_j4fN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8168/Reviewer_j4fN"
            ],
            "content": {
                "summary": {
                    "value": "Grokking is a recent empirical phenomenon where training achieves zero error long before neural network models generalize. People have identified this phenomenon to explore memorization vs. generalization and acquire mechanistic understanding of training dynamics of neural networks.\n\nIn this paper, the authors present randomization experiments where a fraction of labels are corrupted, i.e., replaced by random labels. A simple arithmetic task, namely modular addition, is used to train a small 2-MLP network. According to levels of memorization and generalization, the trained neural networks are categorized into several regimes: (1) memorization (2) coexistence (3) partial inversion (4) full inversion (5) forgetting. Each regime corresponds to an explanation of the inner mechanism."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, I find this paper very clear and interesting. The authors focus on a recent phenomenon, and by experimenting under idealized data and networks, phase transitions are identified, each with meaningful interpretations.\n\nSome of the key contributions include:\n1. Randomization experiments reminiscent of [1], which helps understand generalization of neural nets in the presence of label noise.\n2. A clear phase transition phenomenon and separates different data generating regimes into interpretable regions. This may be helpful for understanding the inner workings of neural networks and training dynamics.\n3. Clear measurements that quantify the behavior of the trained network.\n4. Some insights into regularization techniques such as batchNorm and weight decay.\n\n[1] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning Rep- resentations, 2017."
                },
                "weaknesses": {
                    "value": "This paper follows a line of papers that study grokking, and provides a refined analysis of the phenomenon instead of proposing more general principles. \n\n1. I feel that compared with the initial phenomenon identified in earlier papers, this paper is in a way less innovative.\n2. The scope of the analysis is limited, as toy datasets and toy neural nets are studied. It is unclear whether these analyses can generalize to practical settings.\n3. This paper does not contain a detailed study of the training dynamics, how the weights behave, or some theoretical analyses. It is unclear whether this paper has impact on the theory community."
                },
                "questions": {
                    "value": "See the above sections.\n\nIn addition, is there any scenario where we believe Partial Inversion or Full Inversion may be observed in practice?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698361703638,
            "cdate": 1698361703638,
            "tmdate": 1699637012617,
            "mdate": 1699637012617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t9aMDbdjkD",
                "forum": "UHjE5v5MB7",
                "replyto": "pnFGjrLCdB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8168/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8168/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the encouraging feedback.\n\n## Novelty\nPlease refer to the corresponding section in the Global Response.\n\n## General settings\nPlease refer to the corresponding section in the Global Response.\n\n## Training dynamics\nPlease refer to the corresponding section in the Global Response.\n\n## Answers to Questions\n\n- Yes, we do observe partial as well as full inversion on Modular Arithmetic tasks learned with the Transformer architecture as well as deeper MLP networks (Section 2.2  and Appendix G). Even in these general settings, we can relate periodic weights and IPR to generalization.\nWe also observe inversion in corrupted image tasks such as MNIST (Appendix H). For further details please refer to the Global Response."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8168/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522188815,
                "cdate": 1700522188815,
                "tmdate": 1700522188815,
                "mdate": 1700522188815,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NkjLqOUj38",
            "forum": "UHjE5v5MB7",
            "replyto": "UHjE5v5MB7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8168/Reviewer_kDnz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8168/Reviewer_kDnz"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the grokking phenomenon on algorithmic datasets with label corruptions.  In particular, a two-layer MLP is fitted to a modular arithmetic dataset with varying degrees of label corruptions, weight decay, and dropout rates. As a measure of generalization, the authors propose to use the inverse participation ratio, which quantifies the periodicity in the parameters. Experiments show that (i) the model can perfectly memorize the mislabeled examples while achieving near-perfect test accuracy, (ii) the model learns to \"correct\" some or all mislabeled examples depending on the level of regularization, (iii) the memorizing/generalizing neurons are identifiable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is written clearly and the results are discussed well. The specific idea of training under label corruption is interesting, which seems to allow for studying memorization on the neuron level. I find some results important (e.g. simultaneous memorization and generalization and sub-network identification) while some are not as significant (e.g. the impact of weight decay)."
                },
                "weaknesses": {
                    "value": "Recently, there have been a series of papers on grokking, most of which rely on empirical analysis of training dynamics or final networks. Although such findings could be thought-provoking, they often come with the risk that findings/questions may apply to the particular setting considered in the work. I'm not sure whether the analysis and findings in this work can be extrapolated to other setups, e.g., different architectures or datasets. This is why I'm closer to a rejection but would be happy to discuss it with the authors and other reviewers.\n\n- The paper analyzes the learning under label corruptions in the same vein as previous works. As shown in Nanda et al. (2023), the algorithm implemented by the algorithm after the grokking phase, roughly speaking, boils down to a handful of periodic functions, which is further confirmed by Gromov (2023) in terms of an analytical solution. Based on this observation, the authors propose to use IPR as a measure of \"how periodic a neural representation is\", which serves as a proxy for whether a neuron generalizes or memorizes. Then, the effect of pruning individual neurons is investigated in the same spirit as Nanda et al. (2023). All in all, on the procedural side, the only novelties seem to be the use of IPR and label corruption.\n\n- I find the finding that there are separate generalizing/memorizing sub-networks (or rather collections of neurons) interesting. Yet, the identification of these sub-networks is demonstrated on this toy algorithmic problem whose solution is known to be periodic. Consequently, I'm not sure if this finding translates into more general problems with non-periodic subnetworks that generalize.\n\n- I do not understand the main takeaway of sections 3.1 and 3.2. Is there any new finding or just a confirmation of well-known results?"
                },
                "questions": {
                    "value": "- Interestingly, even in the case of full inversion without batch normalization, low IPR neurons do not disappear. Why does this occur? \n- Likewise, batch normalization seems to downweight the low IPR activations but does not zero them out, meaning that memorization still occurs. How do the authors interpret this?\n- Multi-modality in Figure 4 alone does not imply low/high IPR neurons memorizing/generalizing as there is no evidence that low IPR neurons are indeed connected to memorization. I think this becomes clear only in the second to the last paragraph of section 2.\n- _In trained networks, a larger population of high-IPR neurons leads to better generalization_ requires citation or explanation.\n- _Choosing quadratic activation function ... makes the problem analytically solvable._ requires citation.\n- _It\u2019s role ..._ <--- typo\n- _In the previous Section_ <--- S should be small"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8168/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761000801,
            "cdate": 1698761000801,
            "tmdate": 1699637012462,
            "mdate": 1699637012462,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ANKw7zAME1",
                "forum": "UHjE5v5MB7",
                "replyto": "NkjLqOUj38",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8168/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8168/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed feedback and suggestions.\n\n## General architectures and datasets\nPlease refer to the corresponding section in the Global Response.\n\n## Novelty \nPlease refer to the corresponding section in the Global Response.\n\n## Section 3\n\nIn the previous version of the manuscript, we dedicated Section 3 to summarize the effect of various regularization techniques and compare these effects with existing literature.\nIn the revised version, considering the reviewers' suggestions, we have shortened this discussion in favor of the results for general architectures -- Transformers and deeper MLPs.\n\n## Low IPR neurons in full inversion phase\n\nWe encode labels as $p$-dimensional one-hot vectors, so the accuracies depend only on the largest output logit. On the other hand, the optimization objective depends on the MSE loss over \\emph{all} the logits. In the presence of label corruption, the MSE loss naturally includes the corrupted training data. For these corrupted training examples, we find that the output logit-vectors contain two competing peaks -- corresponding to the corrupted and the \"correct\" labels (Appendix K, Figure 23). This underlies the tussle between generalizing and memorizing network predictions. Regularization techniques such as weight decay suppress the corrupted-logits compared to correct-logits. This results in the *full inversion* phase we observe. The few remaining low-IPR neurons still contribute to a weak peak and lower the training loss, but they do not affect the network predictions or accuracy. The effect of these low IPR neurons on loss can be seen in the pruning curves in Appendix I.2 (Figure 18(c)).\n\n## Effect of BatchNorm\n\nIndeed, BatchNorm does not zero-out the low IPR neurons, but merely downweights them. As a result, the regularization effect of BatchNorm is less substantial compared to weight decay and dropout. This can be seen by comparing the phase diagrams in the Figures 4, 8. We emphasize that the regularizing effect of BatchNorm, albeit weaker, is noteworthy due to its interpretability.\n\nSimilar to the logits in Figure 23, we also see multiple peaks in the presence of BatchNorm. The suppression of low IPR neurons with BatchNorm suppresses the peak corresponding to the corrupted label in favor of the correct one, which explains the regularizing effect of BatchNorm.\n\n## Corrections\n\nWe thank the reviewer for pointing out the typos and additional citations. We have made these corrections in the revised manuscript."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8168/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522047046,
                "cdate": 1700522047046,
                "tmdate": 1700522047046,
                "mdate": 1700522047046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w0PoYwG1r7",
                "forum": "UHjE5v5MB7",
                "replyto": "ANKw7zAME1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8168/Reviewer_kDnz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8168/Reviewer_kDnz"
                ],
                "content": {
                    "title": {
                        "value": "my response to author response"
                    },
                    "comment": {
                        "value": "Thanks for the author's response. I'm sorry if I'm being \"the annoying reviewer 2\" but I really am having difficulty seeing the main gist of the paper. I understand that this work introduces a way of analyzing neurons to identify where generalization/memorization happens. Although these findings are interesting as such, I don't see how they would translate into virtually any other machine learning problem. The first grokking paper presented a shocking finding, which was then (attempted to be) replicated in various setups. Yet, this work examines an even more constrained setup (with label corruptions). As such, it does not take \"generalization vs memorization\" in a broad sense. More concretely, I don't see how \"these ideas allow for a quantitative distinction between generalizing and memorizing sub-networks, deepening our understanding thereof in trained neural networks.\" What knowledge can I transfer from this paper to \"memorization in ResNets trained on ImageNet\"?\n\nSecond, I disagree with the claim that \"the resilience of modular arithmetic datasets to label corruption is also a non-trivial and significant finding\". What makes this finding important? How does it guide future grokking research? Further, the \"bias towards \"simpler\" polynomials introduced by regularization techniques\" was already shown in the \"mechanistic interpretability\" paper. Even more importantly, Figure 6c of \"Towards Understanding Grokking: An Effective Theory of Representation Learning\" paper already shows grokking happens without regularization.\n\nThese being said, I promise the discuss the significance of the contributions with the other reviewers and revise my score if I'm convinced."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8168/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670649428,
                "cdate": 1700670649428,
                "tmdate": 1700670649428,
                "mdate": 1700670649428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]