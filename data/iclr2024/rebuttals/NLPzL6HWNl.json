[
    {
        "title": "Improving LoRA in Privacy-preserving Federated Learning"
    },
    {
        "review": {
            "id": "cTrPmbyHUA",
            "forum": "NLPzL6HWNl",
            "replyto": "NLPzL6HWNl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6253/Reviewer_8DEX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6253/Reviewer_8DEX"
            ],
            "content": {
                "summary": {
                    "value": "The Federated Generative Learning (FGL) framework offers a novel approach to federated learning, leveraging foundational generative models like Stable Diffusion to generate training data from prompts shared by clients. Clients contribute class-level or instance-level prompts, encapsulating key features of their local data. The server, in turn, amalgamates these prompts and synthesizes corresponding training data for global model training. This approach trims down communication costs since only concise prompts, and not bulky gradients or models, are transferred. This system also boasts robustness to data diversity and has demonstrated superior performance \u2013 with just one communication round, it outdid FedAvg's 200 rounds in accuracy. When trialed on skewed ImageNet100 distributions, FGL exceeded FedAvg's performance by 30% in just five communication rounds. Apart from being efficient, FGL also enhances privacy, as prompts reveal lesser private data than traditional methods. Evaluations confirmed no private data memorization in the synthetic images and an enhanced resilience against membership inference attacks. However, challenges persist with non-IID data, intricate domains, and the potential risks associated with prompts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tClearly identifies limitations of vanilla LoRA in federated learning settings and provides theoretical analysis on the causes.\n2.\tProvides extensive experiments that demonstrate consistent improvements of FFA-LoRA over LoRA on multiple models, datasets, and conditions.\n3.\tReduces communication costs and removes reliance on scaling hyperparameters compared to LoRA."
                },
                "weaknesses": {
                    "value": "1.\tUnclear how the approach performs under other challenges like adversarial attacks, concept drift, and personalization. \n2.\tThe paper only evaluates NLP tasks with text data. Unclear if the benefits of FFA-LoRA generalize to other data types like image, speech, etc.\n3.\tThe theoretical analysis and intuitions provided are informal. No formal convergence or privacy proofs given."
                },
                "questions": {
                    "value": "please refer to the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698544084613,
            "cdate": 1698544084613,
            "tmdate": 1699636684229,
            "mdate": 1699636684229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h3B4h3wicM",
                "forum": "NLPzL6HWNl",
                "replyto": "cTrPmbyHUA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8DEX"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and support of our paper as well as the valuable suggestions. Please see our response below with respect to the specific comments. \n\n> Unclear how the approach performs under adversarial attacks, concept drift, and personalization.\n\nThe main objective of our paper is on the challenges when we consider the task of fine-tuning of LLMs, and impact of . Although these questions are important in the context of Fed Fine tuning LLMs. They are beyond the scope of this paper.\n\n> The paper only evaluates NLP tasks with text data. Unclear if the benefits of FFA-LoRA generalize to other data types like image, speech, etc.\n\nIt would be interesting to see the performance of such fine-tuning in the context of computer vision and speech. We running experiments on these tasks and will update the results here once they are ready. We note that fine-tuning LLMs with DP is computationally intensive, and we need more time to finish the experiments.\n\n> The theoretical analysis and intuitions provided are informal. No formal convergence or privacy proofs given.\n\nRegarding the **scaling factor** $\\alpha$ for FFA-LoRA and LoRA, we have updated our draft to include more formal statements, and provide an additional theorem to formally show that FFA-LoRA is in fact equivalent to LoRA with $\\alpha \\rightarrow \\infty$. \n>> **Theorem 1 (Scaling Factor):** For local updates with the same initial condition on $\\mathbf{W}$, vanilla LoRA update with scaling factor $\\alpha\\_{LoRA}$ produces trajectory $ \\\\{ W\\_{\\alpha\\_{LoRA}}^k \\\\}\\_{k\\in [K]}$, and FFA-LoRA with scaling $\\alpha\\_{FFA}$ produces trajectory $\\\\{W\\_{\\alpha\\_{FFA}}^k\\\\}\\_{k\\in [K]}$. Then we have\n>> $$\\lim_{\\alpha_{LoRA} \\rightarrow \\infty} W_{\\alpha_{LoRA}}^k = W_{\\alpha_{FFA}}^k, \\text{ for all } k, \\alpha_{FFA}.$$\n\nRegarding the **convergence study** of our proposed method. We first provide the following statements for some additional context on the theoretical aspects behind PEFT on LLMS. \n>> **Theorem 2 (Smoothness conditions):** Assume that the loss function given weight and dataset is denoted $F(\\mathbf{W}, D)$. For a low-rank decomposition on model parameter $\\mathbf{W}$ such that $\\mathbf{W}(\\mathbf{A}, \\mathbf{B}) = \\mathbf{W}\\_0 + \\mathbf{B} \\mathbf{A}$ satisfying Equation (1). We have the following properties.\n>> 1. If $\\mathbf{B} $ is trainable, $\\mathbf{A}$ is fixed with $||\\mathbf{A}|| \\leq C$ and $F(\\mathbf{W}, D)$ is Lipschitz smooth with respect to $\\mathbf{W}$ with factor $L$. The loss function $F(\\mathbf{W}(\\mathbf{A}, \\mathbf{B}))$ is Lipschitz smooth with respect to $\\mathbf{B}$ with factor $LC^2$.\n>> 2. If both $\\mathbf{A}$ and $\\mathbf{B}$ are trainable and $F(\\mathbf{W}, D)$ is Lipschitz smooth with respect to $\\mathbf{W}$ with factor $L$, the loss function $F(\\mathbf{W}(\\mathbf{A}, \\mathbf{B}))$ has no Lipschitz smoothness guarantees with respect to $\\[\\mathbf{A}, \\mathbf{B}\\]$.\n>>\n>> All the smoothness notions are defined with respect to the matrix Frobenius norm, which is denoted as $||\\cdot||$.\n\nThese results show that if the objective function loss defined on full fine-tuning with $\\mathbf{W}$ satisfies technical conditions such as Lipschitz smoothness, the same assumptions would also hold for FFA-LoRA, but not for Vanilla LoRA.\n\nSuppose that the objective function $F$ satisfies the conditions presented in existing works to ensure convergence of full federated fine-tuning [1] or DP tuning [2] on $\\mathbf{W}$, then we can get similar **convergence results for FFA-LoRA using the theorem above**.\n\nRegarding **differential privacy**, we formally state our corollary for privacy guarantee. Our analysis below is based on Theorem 1 of [3] and the parallel composition and resistance to post-processing of DP.\n>>**Corollary 3 (Privacy Guarantee):** Given Theorem 1 with moments accountant in [3],  the parallel composition and resistance to post-processing of DP, the mechanism updating FFS-LoRA with locally ran DP-SGD and FedAvg can satisfy $(\\epsilon, \\delta)$-DP given $\\forall i, q=\\frac{|B_i|}{|N_i|}$, the number of total local updates $T$ of each client and  $\\sigma = O(\\frac{q\\sqrt{T\\log(1/\\delta)}}{\\epsilon})$. \n\n(The exact $\\sigma$ is computed by the Pytorch's Opacus package [4] numerically given $q, T, \\epsilon, \\delta$).\n\nWe refer to our \"**Response to All**\" for detailed proof.\n\n---\n[1] *Lian, Xiangru, Yijun Huang, Yuncheng Li, and Ji Liu.* \"Asynchronous parallel stochastic gradient for nonconvex optimization.\" *Advances in neural information processing systems 28 (2015).*\n\n[2] *Wang, Di, Changyou Chen, and Jinhui Xu.* \"Differentially private empirical risk minimization with non-convex loss functions.\" *In International Conference on Machine Learning, 2019.*\n\n[3] *Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang.* Deep learning with differential privacy. *2016 ACM SIGSAC\nconference on computer and communications security*\n\n[4] <https://github.com/pytorch/opacus>"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419508701,
                "cdate": 1700419508701,
                "tmdate": 1700419508701,
                "mdate": 1700419508701,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6RGz6wk54x",
                "forum": "NLPzL6HWNl",
                "replyto": "h3B4h3wicM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Reviewer_8DEX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Reviewer_8DEX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks"
                    },
                    "comment": {
                        "value": "Thanks for the authors' detailed explanations, most of my concerns have been addressed. Do you have to updated results on other datasets?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617412631,
                "cdate": 1700617412631,
                "tmdate": 1700617412631,
                "mdate": 1700617412631,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PDc9B7ZKe2",
                "forum": "NLPzL6HWNl",
                "replyto": "cTrPmbyHUA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Comment"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their kind acknowledgment of our response and that our response addresses most of their concerns.\n\nWe here provide an additional dataset on the computer vision task.\nWe use the pre-trained vision transformer (https://huggingface.co/google/vit-base-patch16-224-in21k), and consider the task of fine-tuning on the Food-101 dataset (as shown on https://huggingface.co/datasets/food101) for image classification.\n\nFor context, we provide performance reported on huggingface as baseline. A centralized, fine-tuned model has an accuracy of 0.8539.\n\nWe first report the results in our centralized experimental setting in the table below.In this case there is no significant performance discrepancy between the two methods, implying that FFA-LoRA and vanilla has similar performance without consideration of DP and FL. This also aligns with Theorem 1 in Author's response.\n\nIn terms of the federated case, we first report the iid setting. In this setting, under similar hyper-parameters, we provide the learning curve of both FFA-LoRA and LoRA (https://imgur.com/5xnXWSu). as well as a last k-iterated (k=50) averaged accuracy. \n\nIt can be seen that compared to LoRA, FFA-LoRA has both (a) better convergence and (b) less fluctuations in training. The findings align with our findings in language-related tasks, showing that the properties of LoRA being discussed in our paper are not limited to language tasks only.\n\nThe non-iid setting as well as the DP setting are more time-consuming, and we are still running more experiments. Due to the time limit, we will add them to the final version of our paper if the paper is accepted.\n\n\n| Method          | Accuracy |\n|-----------------|----------|\n| Baseline        | 0.8539   |\n| Centralized LoRA | 0.8618   |\n| Centralized FFA-LoRA | 0.8583   |\n| FL iid LoRA | 0.8133   |\n| FL iid FFA-LoRA | 0.8210   |"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702702162,
                "cdate": 1700702702162,
                "tmdate": 1700702985561,
                "mdate": 1700702985561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0LzInab5vI",
            "forum": "NLPzL6HWNl",
            "replyto": "NLPzL6HWNl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6253/Reviewer_E2bX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6253/Reviewer_E2bX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presented an approach called Federated Freeze A LoRA (FFA-LoRA) to address the limitations of the low-rank adaptation method in federated learning setting. The limitations of the vanila low-rank adaptation include: 1) data heterogeneity, 2) amplication of difficiential privacy noise and 3) sensitivity to hyper-parameters. Authors provide empirical results, showing that the FFA-LoRA outperforms vanilla LoRA in federated learning settings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The study on federated LoRA is timely.\n2. The proposed approach is simple to implement.\n3. The authors provide case studies to highlight the limitations of the vanilla LoRA and motivate their approach."
                },
                "weaknesses": {
                    "value": "1. The benefit of FFA-LoRA on differential privacy (DP) is not very well backed by empirical evaluation. The performance gap between the vanilla LoRA and the proposed FFA-LoRA remains the same across various privacy budgets $\\epsilon$, including $\\epsilon = 0$. Such an empirical result suggests that the impact of DP noise is the same on both the vanilla LoRA and the proposed FFA-LoRA.\n\n2. I do not see why the proposed FFA-LoRA is free from tuning the hyper-parameter $\\alpha$. In Section 4, the authors claim that \"FFA-LoRA does not rely on $\\alpha$, and is equivalent to LoRA with $\\alpha = \\infty$\". Such a claim, in fact, suggests that the $\\alpha$ is fixed in FFA-LoRA. Then, in Theorem 1, the theoretical result suggests that tuning $\\alpha$ is equivalent to tuning the learning rate $\\eta$. I'm not able to fully follow the discussion here."
                },
                "questions": {
                    "value": "1. Is the FFA-LoRA approach more sensitive to random initialization? Suppose a bad initialization sets $\\mathbf{A} = \\mathbf{0}$; is the model still trainable?\n\n2. What's the variance in the experiment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788765005,
            "cdate": 1698788765005,
            "tmdate": 1699636684104,
            "mdate": 1699636684104,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Rd37Mf0fmw",
                "forum": "NLPzL6HWNl",
                "replyto": "0LzInab5vI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E2bX (Part 1)"
                    },
                    "comment": {
                        "value": "> The benefit of FFA-LoRA on differential privacy (DP) is not very well backed by empirical evaluation. The performance gap between the vanilla LoRA and the proposed FFA-LoRA remains the same across various privacy budgets \n, including \n. Such an empirical result suggests that the impact of DP noise is the same on both the vanilla LoRA and the proposed FFA-LoRA.\n\nWe note that the exact performance of the algorithms are In Table 1, although there are counter-examples, the performance gap between FFA and vanilla is still evident, with the largest gap occurring $\\epsilon = 1$ when taking average over the 4 tasks. Therefore we can say that FFA is more suitable for higher noise. \nThe individual performance of one task, $SST-2$ in this case, depends on many factors such as the difficulty of the task. \n\nWe further point out that our claim on noise amplification is more apparent in Table 4. Where for rank as low as 2 or 4, Lora can not converge to any meaningful results when $\\epsilon = 1$.\n\nAdditionally, when noise is small (no significant noise amplifying), the impact of noise is not as big, as shown in Figure 1 of our paper. The simulation in Figure 1 also aligns with our observation in Table 1 and 4.\n\nAdditionally, existing literature [1] (Figure 4) has shown that the relationship between performance gap and $\\epsilon$ is not always significant, and varies between tasks. This observed behavior could be due to robustness of DPSGD-style algorithms. Currently, the dynamics of DPSGD optimization in deep neural networks is still not fully understood.\n\n> I do not see why the proposed FFA-LoRA is free from tuning the hyper-parameter \n. In Section 4, the authors claim that \"FFA-LoRA does not rely on \n, and is equivalent to LoRA with \n\". Such a claim, in fact, suggests that the \n is fixed in FFA-LoRA. Then, in Theorem 1, the theoretical result suggests that tuning \n is equivalent to tuning the learning rate \n. I'm not able to fully follow the discussion here.\n\nWe would like to first reiterate that for the proposed FFA-LoRA algorithm, as confirmed by Theorem 1 in our paper, $\\alpha_{FFA}$ can be seen as fixed. However, this does not make FFA-LoRA equivalent to vanilla LoRA with some given $\\alpha_{LoRA}$. As we have stated, FFA-LoRA with any $\\alpha$ in fact has the same dynamic as LoRA with $\\alpha_{LoRA} \\rightarrow \\infty$, we provide the details below:\n\nIn order to reduce confusion and provide additional theoretical robustness to our paper, we provide an additional theorem as a more formal statement on the relationship between FFA-LoRA and vanilla LoRA. We refer to our \"**Response to All**\" for a more detailed explanation.\n\n> Is the FFA-LoRA approach more sensitive to random initialization? Suppose a bad initialization sets is the model still trainable?\n\nThis is a really good question. In our experiments, all the $A_0$ matrices were randomly initialized under random Kaiming distribution ( a default initialization method is deep learning and is consistent with previous works in LoRA) and according to Theorem 1, the scaling on the $A_0$ is not important for FFA-LoRA. \nAnd we briefly discuss the impact of other types of initialization below.\n\nWe know that for a zero-initialized A matrix, neither LoRA nor FFA-LoRA are able to train any meaningful results. However, suppose that we have A to be full rank (which is also satisfied for any random initialization in general), there are a number of different initializations that we could utilize, such as orthogonal random initialization and using the top $r$ singular vectors of $W_0$ as matrix A. \n\nWe provide some initial results here (without DP), it can be seen from the plot that matrix $A$ with orthogonal initialization seems to perform slightly better than the existing approach. \nHowever, the performance gap is not significant enough for a definitive answer.\n\n| Method   | QNLI mean | QNLI variance |\n|---|---|----|\n| Kaiming Init.    | 91.84\\%   | 0.38\\% |\n| Orthogonal Init. | 92.16\\%   | 0.83\\%  |\n| SVD Init.    | 91.50\\%   | 0.59\\%   |\n\nHowever, although this is a great question with many interesting potential directions, we mainly consider the result of freezing A after initialization, and the choices of initialization is beyond our initial scope of this work. Although all these methods use SGD as the back-bone optimization algorithm, the key ideas and procedures for training are very different from DP. Thus, each of these challenges may deserve an independent project to be investigated. We will mentions these problems as future directions in our revision."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418444362,
                "cdate": 1700418444362,
                "tmdate": 1700418444362,
                "mdate": 1700418444362,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wCOJJUSxfW",
                "forum": "NLPzL6HWNl",
                "replyto": "0LzInab5vI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer E2bX (Part 2)"
                    },
                    "comment": {
                        "value": "> What's the variance in the experiment?\n\nIn many of the experiments (especially for RoBERTa), we find that the variance is typically low, and the performance is consistent, therefore we mainly consider the performance. We conducted additional experiments (20 reruns with different seeds) the QNLI experiment, under the same setting as Table 1, and provide the mean and variance of the results here. We note that some rare cases where the algorithm failed to converge has been removed from the statistics.\n\n| Privacy Budget + Method   | QNLI mean | QNLI variance |\n|----|---|---|\n| Non Private + LoRA   | 91.56\\%   | 0.43\\%  |\n| Non Private + FFA-LoRA    | 91.84\\%   | 0.38\\%  |\n| $\\epsilon = 6$ + LoRA   | 86.55\\%   | 1.02\\% |\n| $\\epsilon = 6$ + FFA-LoRA | 87.42\\%   | 0.80\\%  |\n| $\\epsilon = 3$ + LoRA     | 85.66\\%   | 1.20\\%  |\n| $\\epsilon = 3$+ FFA-LoRA  | 86.18\\%   | 1.01\\%  |\n| $\\epsilon = 1$ + LoRA   | 80.86\\%   | 0.28\\%     |\n| $\\epsilon = 1$+ FFA-LoRA  | 83.63\\%   | 1.32\\%      |\n\n\n[1] *Yu, Da, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu.* Large scale private learning via low-rank reparametrization. *In International Conference on Machine Learning,PMLR, 2021.*"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700418469851,
                "cdate": 1700418469851,
                "tmdate": 1700418469851,
                "mdate": 1700418469851,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dcx1ItvCMd",
                "forum": "NLPzL6HWNl",
                "replyto": "0LzInab5vI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Reviewer_E2bX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Reviewer_E2bX"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thanks for replying.\n\n1. There still lacks evidence to support the claim \"FFA is more suitable for higher noise\". I expected the performance gap between FFA-LoRA and vanilla LoRA to increase as the noise magnitude grows, but I did not see such behavior in Tables 1 and 4. Could the authors provide additional numbers or explanations?\n\n2. Could the authors summarize the goal of the theoretical analysis before diving into the detailed technical claims on $\\alpha$ and the full rank property? Also, is there any definition of the equivalent relationship between LoRA and its variance?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700518904655,
                "cdate": 1700518904655,
                "tmdate": 1700518904655,
                "mdate": 1700518904655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m7T5VgQQnQ",
                "forum": "NLPzL6HWNl",
                "replyto": "0LzInab5vI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Comment"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their additional comments.\n\n> There still lacks evidence to support the claim \"FFA is more suitable for higher noise\". I expected the performance gap between FFA-LoRA and vanilla LoRA to increase as the noise magnitude grows, but I did not see such behavior in Tables 1 and 4. Could the authors provide additional numbers or explanations?\n\nAs we have stated in the paper, the performance gap between FFA-LoRA and LoRA widens when we are faced with a smaller privacy budget, which was verified in Table 1 and 4. To better illustrate this observation, we provide the following figure on the relationship between injected noise and averaged algorithm accuracy across the 4 tasks. As shown in the figure (<https://imgur.com/a/7FmlPdQ>), as more noise is added, the performance gap widens.\n\n> Could the authors summarize the goal of the theoretical analysis before diving into the detailed technical claims on \n and the full rank property?\n\nWe plan to update the draft such that the theoretical analysis is coherent with our existing statements in the paper. We provide a framework here on how the introduced theorems benefit our arguments made in the paper.\n\n- Our goal is to show that\n    - $\\alpha$ does not matter for FFA-LoRA, which is one less hyper parameter to worry about. This claim is backed by the theorem in our paper.\n    - For LoRA, generally big $\\alpha$ is good for performance but bad for stability. FFA-LoRA is equivalent to $\\alpha = \\infty$ for LoRA without being unstable. The equivalence is backed by *theorem 1 in our response* we provide in our response.\n- Many reviewers have mentioned the need for theoretical analysis. We state that the existing literature on the convergence of federated learning can apply to FFA-LoRA, but not LoRA. This is backed by our Theorem on smoothness.\n- The full rank property of $A$ was mentioned in our response to comment on how to construct different methods of initialization. By definition of linear algebra, a full rank $A$ is more expressive that a rank-deficient one, which is why we consider full rank initialization. It is by no means the only approach.\n\n> Also, is there any definition of the equivalent relationship between LoRA and its variance?\n\nThe equivalence between LoRA and FFA-LoRA happens when $\\alpha \\rightarrow \\infty$ for LoRA, we provide the formal definition in our theorem on equivalence. Or, informally speaking, we have\n\n$$\\lim_{\\alpha_{LoRA} \\rightarrow \\infty} W_{\\alpha_{LoRA}}^k = W_{\\alpha_{FFA}}^k,$$\n\nwhere the matrices $W_{\\alpha_{LoRA}}^k, W_{\\alpha_{FFA}}^k$ are produced by $k$ local iterations of LoRA and FFA-LoRA. \nWe want to show that as $\\alpha$ increases, LoRA's update on A and B gets closer to the updates made by FFA-LoRA. Which is good since in many cases we want LoRA to have big $\\alpha$."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544308213,
                "cdate": 1700544308213,
                "tmdate": 1700544326511,
                "mdate": 1700544326511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MGQgg8Udxy",
                "forum": "NLPzL6HWNl",
                "replyto": "0LzInab5vI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Reviewer_E2bX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Reviewer_E2bX"
                ],
                "content": {
                    "title": {
                        "value": "Comments to Authors"
                    },
                    "comment": {
                        "value": "Thanks for replying. Two major issues remain, so I have decided to keep my rating.\n\n1. There are several noticeable counter-examples in Tables 1&4, and the new figure does not address them. For example, we reach the largest gap between LoRA and FFA-LoRA with $\\epsilon=1$ on the QQP and QNLI datasets instead of $\\epsilon \\in \\\\{3, 6\\\\}$. LoRA also outperforms FFA-LoRA by a significant margin on the SST-2 datasets with $\\epsilon=1$. These results might be outliers, but we need more investigation before concluding that FFA-LoRA is more robust to DP noise.\n\n2. The equivalence relationship between FFA-LoRA and LoRA never holds in practice and the benefit of large $\\alpha$ is questionable. Theorem 1, in the revision, suggests that a necessary condition for such an equivalence relationship is $\\alpha_{LoRA} = \\infty$. In practice, $\\alpha_{LoRA} \\neq \\infty$. Indeed, in the provided reference (Kuang et al., 2023), the maximum $\\alpha=50.0$ or $128$ if I read the tables in the appendix correctly. Also, tuning $\\alpha$ does not seem to impact the accuracy much, as Figure 5(a) in Kuang et al., 2023 shows. Experiments with $\\alpha = 0.5$ and $\\alpha=50.0$ have almost the same average accuracy."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620744807,
                "cdate": 1700620744807,
                "tmdate": 1700620744807,
                "mdate": 1700620744807,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aFjyb8a3wJ",
            "forum": "NLPzL6HWNl",
            "replyto": "NLPzL6HWNl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6253/Reviewer_WAsF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6253/Reviewer_WAsF"
            ],
            "content": {
                "summary": {
                    "value": "The author proposes FFA-LoRA, a LoRA variant in FL by freezing one of the LoRA weight and training only the other LoRA weight so that it's easy to do model averaging in FL. Empirical results show that FFA-LoRA achieves comparable performance compared with LoRA under different differential privacy guarantees."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The motivation is sound and the paper writing is easy to follow.\n+ Empirical results show competitive performance under different differential privacy and parameter budget.\n+ Empirical results are comprehensive, considering multiple tasks and ablation study."
                },
                "weaknesses": {
                    "value": "+ The motivation is straightforward and intuitive, without theoretical insights."
                },
                "questions": {
                    "value": "+ Why rank 16 for MNLI is worse than rank 8 in Table 2?\n+ Another intuitive variant is to alternative optimize the two LoRA weights. How would this perform compare with the proposed method?\n+ Why non-iid performance is similar to iid performance in Table 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814654022,
            "cdate": 1698814654022,
            "tmdate": 1699636683968,
            "mdate": 1699636683968,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nB6w6GS7o8",
                "forum": "NLPzL6HWNl",
                "replyto": "aFjyb8a3wJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WAsF"
                    },
                    "comment": {
                        "value": "We are grateful for the reviewer's careful review and constructive comments. Based on your comments, we would like to make the following clarification.\n\n> The motivation is straightforward and intuitive, without theoretical insights.\n\nWe appreciate the reviewer's comment on our motivation being straightforward and intuitive. We provide some additional theoretical results regarding the relationship between FFA-LoRA and LoRA, a theorem regarding the convergence properties of FFA-LoRA, and lastly a guarantee on differential privacy. All of which will be added to our revised draft.\nWe refer to our \"**Response to All**\" for a more detailed discussion.\n\n> Why rank 16 for MNLI is worse than rank 8 in Table 2?\n\nWe report all our experiments truthfully. It has been shown that even for vanilla LoRA, there is an optimal rank for a specific fine-tuning task, and the performance of LoRA could decrease if rank is too large, the exact optimal rank is task-dependent [1]. This observation can be potentially explained by a similar reason.\n\nWe thank the reviewer for the careful observation, and will add statements to address this in the revised version of the paper.\n\n> Another intuitive variant is to alternative optimize the two LoRA weights. How would this perform compare with the proposed method?\n\nThis is a great question. We have conducted experiments regarding alternate updates on the weights. The results show slower convergence and no significant performance gains compared to vanilla LoRA. \nEmpirically, this means that alternating update is hard to tune: with a smaller learning rate similar to LoRA, it converges slower; with a larger learning rate similar to FFA-LoRA, it is not robust and often unable to converge. For the case of non-iid agents, this algorithm is unable to converge under 5 different random seeds and multiple learning rates $\\eta \\in \\\\{0.05, 0.1, 0.2\\\\}$. (For this experiment, the best LR for LoRA and FFA-LoRA is $\\eta=0.05$ and $\\eta = 0.5$, respectively.)\nThis can be confirmed by both accuracy scores as well as observations of the learning curve during training.\nAdditionally, with the consideration of DP, the number of gradient calculations is doubled for alternative update methods, which could result in a higher noise added to ensure privacy. In this case, the drawbacks overshadow the benefits. We will provide a brief discussion on this potential solution in our revision. \n\nWe also include one result from the alternate update case here demonstrating its performance in the Table below.\n\n> Why non-iid performance is similar to iid performance in Table 3?\n\nAs we stated in Section 5.1, for the heterogeneous setting, we split data based on their labels. Due to the heterogeneity presented to these methods is not strong enough, there is not a strong separation between iid and non-iid experiments for some tasks. Therefore, a performance gap exists between FFA-LoRA and vanilla LoRA, but it is not very significant. To demonstrate the impact of heterogeneity, we included an additional experiment with even stronger heterogeneity.\n\nIn the previous MNLI results we ran in the original paper, the non-iid case was formulated using a data label split of $[0.6, 0.2, 0.2], [0.2, 0.6, 0.2], [0.2, 0.2, 0.6]$, which we denote as mild non-iid here. We consider a stronger hetergeneity here, with data label split ratio of $[0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]$. The performance degradation is clearly shown, with FFA-LoRA significantly better than LoRA.\n\n\n| Privacy Budget + Method  | MNLI matched | MNLI mismatched |\n|----------------------------|--------------|-----------------|\n| I.I.D. + LoRA              | 86.90\\%      | 87.15\\%         |\n| I.I.D. + FFA-LoRA          | 87.13\\%      | 87.21\\%         |\n| I.I.D. + Alt. update       | 86.15\\%      | 86.98\\%         |\n| Mild Het. + LoRA           | 87.01\\%      | 87.33\\%         |\n| Mild Het. + FFA-LoRA       | 87.04\\%      | 87.36\\%         |\n| Severe Het. +  LoRA        | 83.95\\%      | 84.51\\%         |\n| Severe Het. +  FFA-LoRA    | 86.28\\%      | 86.71\\%         |\n| Severe Het. +  Alt. update | 35.45\\%      | 35.22\\%         |\n\n---\n\n[1] *Zhang, Qingru, Chen, Minshuo, Bukharin, Alexander, He, Pengcheng, Cheng, Yu, Chen, Weizhu, Zhao, Tuo.* Adaptive budget allocation for parameter-efficient fine-tuning. *The Eleventh International Conference on Learning Representation*"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417484333,
                "cdate": 1700417484333,
                "tmdate": 1700417484333,
                "mdate": 1700417484333,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QGaOujVM9q",
            "forum": "NLPzL6HWNl",
            "replyto": "NLPzL6HWNl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6253/Reviewer_1Jzn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6253/Reviewer_1Jzn"
            ],
            "content": {
                "summary": {
                    "value": "This paper discuss the potential discordances of applying LoRA in differentially private federated learning: (1) decompose `\\DeltaW` to `BA` moves LoRA into a nonlinear regime that potentially cause trouble for aggregation/averaging in model updates (2) the nonlinearity of `BA` cause trouble for DP noise (3) LoRA introduce an extra parameter \\alpha. A new algorithm FFA-LoRA is proposed, where instead of updating both `B` and `A` matrices in LoRA, FFA-LoRA only updates the matrix B and keeps A fixed at random initialization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I like the motivation of the FFA-LoRA algorithm, and appreciate the attempt to provide some analysis on the caveats of LoRA. The experiments on two models (RoBERTa and LLaMA) fine-tuning on a subset of GLUE tasks and a GSM-8K language generation task in both non-DP and DP settings show good empirical performance of FFA-LoRA."
                },
                "weaknesses": {
                    "value": "I thank the authors for providing details of the experimental setup. However, the federated learning setting in experiments seems a bit unconventional with a very small number of clients (only 3 clients). This might be categorized as a cross-silo setting, but it would be good to clearly discuss the targeted application (https://arxiv.org/abs/1912.04977 table 1, https://arxiv.org/abs/2107.06917 section 3.1). \n\nWhile I appreciate the motivation of analyzing LoRA in section 3, none of the explanations seems to be particularly convincing. The discussion of Discordance (1) and (2) heavily focus on the nonlinear nature of LoRA, but deep neural networks suffer from more severe nonlinearity, it is a bit unclear for me why LoRA `BA` suffers more than multi-layer network `W_1 W_2`. For example, for (1), I believe it not only applies to averaging models from clients, but also to averaging gradients from examples. \n\nI also fail to understand why \\alpha becomes an issue for LoRA in  Discordance (3) as it is only a scalar and might potentially be absorbed in learning rate. As shown in table  5, tuning the learning rate helps. \n\nMinor:\nThe empirical results on local DP seem to be very good with very little accuracy drop compared to non-DP results. It is possible that DP fine-tuning is not a particularly hard task compared to training from scratch, but could the authors share more details about the privacy accounting and important privacy parameters? \n\nFedBert seems to mainly focus on pre-training instead of fine-tuning. \n\nPlease cite \u201cCommunication-Efficient Learning of Deep Networks from Decentralized Data\u201d for federated learning and the FedAvg algorithm."
                },
                "questions": {
                    "value": "See weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6253/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899166842,
            "cdate": 1698899166842,
            "tmdate": 1699636683842,
            "mdate": 1699636683842,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Hb0POgSDkc",
                "forum": "NLPzL6HWNl",
                "replyto": "QGaOujVM9q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1Jzn (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and effort reviewing our paper. Please see our response below with respect to the specific comments. \n\n> However, the federated learning setting in experiments seems a bit unconventional with a very small number of clients (only 3 clients). This might be categorized as a cross-silo setting, but it would be good to clearly discuss the targeted application\n\nYes, our focus is indeed the cross-silo setting (similar to existing work in FL LLM, cite some). The major reason is the immense computational burden of LLM training. Also, the difficulties we discussed in this paper are well suited in the context of cross-silo setting, since cross-silo setting is interested in problems such as data-heterogeneity, communication efficiency and privacy, which we were able to address. In our revision, we will clarify with the exact setting.\n\n> While I appreciate the motivation of analyzing LoRA in section 3, none of the explanations seems to be particularly convincing.\n\nWe thank the reviewer for acknowledging the motivations of our analysis. \n\nFirstly, to further consolidate our analysis, we have added an additional theorems as well as a corollary. We refer to our \"**Response to All**\" for these theoretical results. We hope the reviewer find our statements more convincing with the addition of the theoretical results.\n\nSecondly, the disparity we pointed out in the paper is more significant when the weight difference is not-negligible, i.e. $A_1\\not\\approx A_2$. In terms of averaging gradients from examples, this problem is less of a concern since the learning rate $\\eta$ is generally small, and the disparity is not severe. However, for federated learning with averaging models after $K>1$ local update steps, the disparity becomes more problematic. \n\n> fail to understand why $\\alpha$ becomes an issue for LoRA in Discordance (3) as it is only a scalar and might potentially be absorbed in learning rate. As shown in table 5, tuning the learning rate helps.\n\nYes, as we have stated in Section 5.2 of the paper, by jointly tuning learning rate $\\eta$ and scaling factor $\\alpha$, the fine-tuning algorithm for LoRA is able to converge. However, in vanilla LoRA, as reported by previous works, *tuning learning rate $\\eta$ and scaling factor $\\alpha$ have very different effects* on the dynamic of the training process as well as the performance, and the exact relationship between them is unknown. In a way, changing both the learning rate and scaling factor is equivalent to tuning the initialization of the random matrices $A$ in the network.\nThere are existing works showing that for many tasks (FS-LLM), a bigger scaling factor (as high as 256) is able to converge to higher accuracy, however the algorithm becomes more unstable as $\\alpha$ increases, and a re-exploration on the optimal learning rate is needed for a different $\\alpha$.\n\nAdditionally, *in the context of Differential Privacy*, a higher scaling factor for vanilla LoRA would also change the clipping and noises added to the trainable parameters, which introduces additional difficulties.\n\nOn the other hand, for FFA-LoRA, the effect of $\\eta$ and $\\alpha$ is the same, which means we can set-and-forget one and focus on the other, meaning that we have one less hyper-parameter to tune in experiments.\n\n> Minor: The empirical results on local DP seem to be very good with very little accuracy drop compared to non-DP results. It is possible that DP fine-tuning is not a particularly hard task compared to training from scratch, but could the authors share more details about the privacy accounting and important privacy parameters?\n\nWe would like to clarify that no privacy was considered in pre-training, and we only consider the notion of differential privacy in finetuning.\n\nAdditionally, the difficulty in DP fine-tuning is an entirely different question. In general, the difficulty of DP training is associated with the dimension of the problem, with higher dimensional data requiring a lower signal-to-noise ratio. Fine-tuning LLMs with privacy was considered to be very difficult due to the high-dimensional trainable parameters.\nHowever, as pointed out by some recent works on DP fine-tuning, it has been shown that DP fine-tuning empirically exhibits much better performance [1, 2]. This was explained because of the lower intrinsic dimensions within LLMs [3], but the definitive reason for this behavior is largely an open problem. \n\nWe have added more theory as well as calculations behind our approach to ensure differential privacy in our \"**Response to All**\" (Corollary 3), which we reiterate below based on Theorem 1 of [4] and the parallel composition and resistance to post-processing of DP."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416647864,
                "cdate": 1700416647864,
                "tmdate": 1700416647864,
                "mdate": 1700416647864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t8c1ix36jY",
                "forum": "NLPzL6HWNl",
                "replyto": "QGaOujVM9q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6253/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1Jzn (Part 2)"
                    },
                    "comment": {
                        "value": "> FedBert seems to mainly focus on pre-training instead of fine-tuning.\n\nThe reviewer is correct. In our paper, citation of FedBERT had been referred to as a method for fine-tuning, it is instead intended for pre-training, we will change our statements accordingly.\n\n> Please cite \u201cCommunication-Efficient Learning of Deep Networks from Decentralized Data\u201d for federated learning and the FedAvg algorithm.\n\nWe will cite this paper in our revision, and we thank the reviewer for the useful suggestion.\n\n\n[1] *Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto.* Large language models can be\nstrong differentially private learners. *arXiv preprint arXiv:2110.05679, 2021*\n\n[2] *Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu.* Large scale private learning via low-rank reparametrization. *In International Conference on Machine Learning, 2021*\n\n[3] *Xuechen Li, Daogao Liu, Tatsunori B Hashimoto, Huseyin A Inan, Janardhan Kulkarni, Yin-Tat Lee, and Abhradeep Guha Thakurta.* When does differentially private learning not suffer in high dimensions? *Advances in Neural Information Processing Systems, 2022.*\n\n[4] *Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang.* Deep learning with differential privacy. *In Proceedings of the 2016 ACM SIGSAC\nconference on computer and communications security*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6253/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416671561,
                "cdate": 1700416671561,
                "tmdate": 1700416671561,
                "mdate": 1700416671561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]