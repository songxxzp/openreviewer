[
    {
        "title": "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers"
    },
    {
        "review": {
            "id": "IZhwiZT5UV",
            "forum": "uJVHygNeSZ",
            "replyto": "uJVHygNeSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission257/Reviewer_nfN9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission257/Reviewer_nfN9"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on enhancing the attention mechanism by incorporating geometric information. The motivation behind this is the observation that existing positional encoding techniques are not optimally suited for vision tasks. In particular, the authors encode the geometric structure of tokens by representing them as relative transformations and introduce a novel geometric-aware attention mechanism. Experiments are conducted on novel view synthesis tasks, where the proposed method achieves state-of-the-art performance among transformer-based neural video synthesis (NVS) methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper introduces a novel attention mechanism grounded in geometric information, and the motivation is reasonable.\n2. The paper is well-written and the experiments yield compelling qualitative results compared to baseline models."
                },
                "weaknesses": {
                    "value": "- Lack of comparisons with the latest NeRF-based models, except Pixel-NeRF which is proposed in 2021."
                },
                "questions": {
                    "value": "The resolution of training images used in the experiment seems to be not high. How does this method perform on high-resolution data, and what is the impact of different numbers of training views?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission257/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission257/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission257/Reviewer_nfN9"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697534109418,
            "cdate": 1697534109418,
            "tmdate": 1699635951370,
            "mdate": 1699635951370,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "89QMBatuFs",
                "forum": "uJVHygNeSZ",
                "replyto": "IZhwiZT5UV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on 'different numbers of training views' in your question"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedback.  In response to your concerns, we are planning to conduct additional experiments and would like to clarify one thing in your question: Regarding 'different numbers of training views', does this mean different numbers of training *scenes*, or does it refer to different numbers of *input context views* for each individual scene?"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699793085991,
                "cdate": 1699793085991,
                "tmdate": 1699793085991,
                "mdate": 1699793085991,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SYKcN5ZL0W",
                "forum": "uJVHygNeSZ",
                "replyto": "89QMBatuFs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Reviewer_nfN9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Reviewer_nfN9"
                ],
                "content": {
                    "comment": {
                        "value": "I mean the different numbers of input views for each scene."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699844596884,
                "cdate": 1699844596884,
                "tmdate": 1699844596884,
                "mdate": 1699844596884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5HnEfeFihT",
                "forum": "uJVHygNeSZ",
                "replyto": "IZhwiZT5UV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer nfN9"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedback. To address your concerns, we conducted additional experiments on RealEstate10K. \n\n>Lack of comparisons with the latest NeRF-based models, except Pixel-NeRF which is proposed in 2021. \n\nWe added additional NeRF-based models from the Appendix of Du et al. 2023 into Table 3.\nAdditionally, we trained and evaluated the latest concurrent NeRF-based approach, MatchNeRF, from arXiv on RealEstate10k. We included these numbers in Table 3 in the revised manuscript. \n\n| Model                              \t| PSNR  | LPIPS | SSIM   |\n|----------------------------------------|:-------:|:-------:|:-------:|\n| PixelNeRF (Yu et al. 2021)         \t| 13.91 | 0.591 | 0.460 |\n| StereoNeRF (Chibane et al. 2021)   \t| 15.40 | 0.604 | 0.486 |\n| IBRNet (Wang et al. 2021)          \t| 15.99 | 0.532 | 0.484 |\n| GeoNeRF (Johari et al. 2022)       \t| 16.65 | 0.541 | 0.511 |\n| MatchNeRF (Chen et al. 2023)       \t| **23.06** | 0.258 | 0.830 |\n| -------------------------------------- |-------|-------|-------|\n| GPNR (Suhail et al. 2022)          \t| 18.55 | 0.459 | 0.748 |\n| (Du et al. 2023)                   \t| 21.65 | 0.285 | 0.822 |\n| (Du et al. 2023) + GTA (Ours)     | 22.85 | **0.255** | **0.850** |\n\nWe grouped the works into methods using NeRF-based rendering and methods using Transformer-based rendering. We find that MatchNeRF outperforms both NeRF-based and Transformer-based prior work by large margins. By combining our GTA mechanism with Du et al. 2023, the transformer-based method also achieves state-of-the-art results.\nWe would like to mention that the transformer-based NVS with GTA, which introduces a relatively weaker prior into the model, is competitive with the model equipped with a strong physical prior (volumetric rendering).  Our attention mechanism effectively steers the transformer to learn the underlying 3D information behind images.\n\n\n\n>How does this method perform on high-resolution data, \u2026\n\nWe conducted experiments on RealEstate10k with 384x384 resolutions (1.5 times larger in height and width than in Table 3). We see that GTA also improves over the baseline model at higher resolution. We added this table as Table 7.\n\n|Method| PSNR  | LPIPS \t| SSIM |\n|----|:-----------:|:-----------:|:-----------:|\n| Du et al. 2023 | 21.29 | 0.341| 0.838 |\n| Du et al. 2023 + GTA (ours)| **21.95** | **0.322** | **0.849** |\n\nDue to limited computational resources and discussion period, we only trained for 100K iterations, which is one-third of the iterations in Table 3. We will include results with fully trained models in the final version.\n\n>...what is the impact of different numbers of training views?\n\nWe trained models with 3-context views and show the results below: We see GTA improves reconstruction quality over Du et al. 2023 with 3 views setting. We added this table as Table 8.\n\n| Method|  2-views PSNR | 3-views PSNR      |\n|----------------|:-----------------:|:--------------:|\n| (Du et al. 2023) |  \t21.43  |            21.84      |\n| (Du et al. 2023) + GTA (ours)     | \t**22.17** \t|         **22.50**    |\n\nAlso in this table, numbers are computed with models trained for 100K iterations, and we will include results with fully trained models in the final version.\n\n\nReferences\n* Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. PixelNeRF: Neural radiance fields from one or few images. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021\n* Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard Pons-Moll. Stereo radiance fields (SRF): learning view synthesis for sparse views of novel scenes. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021.\n* Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. IBRNet: Learning multi-view image-based rendering. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2021\n* Mohammad Mahdi Johari, Yann Lepoittevin, and Franc \u0327ois Fleuret. Geonerf: Generalizing nerf with geometry priors. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022.\n* Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Explicit correspondence matching for generalizable neural radiance fields. arXiv.org, 2023.\n* Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline stereo pairs. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2023"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690122646,
                "cdate": 1700690122646,
                "tmdate": 1700690122646,
                "mdate": 1700690122646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SfF2zadF9t",
            "forum": "uJVHygNeSZ",
            "replyto": "uJVHygNeSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission257/Reviewer_LyNT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission257/Reviewer_LyNT"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the limitation of existing positional encoding designs in 3D vision tasks, which do not consider task-relevant 3D geometric structures. The authors propose a geometry-aware attention mechanism that encodes the geometric structure of tokens for the novel view synthesis task. Extensive experiments demonstrate the effectiveness of the proposed method, and qualitative comparisons with previous state-of-the-art techniques highlight significant improvements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed GTA consistently outperforms existing state-of-the-art methods on multiple datasets, including RealEstate10k, ACID, CLEVR-TR, and MSN-Hard.\n\n2. The authors provide extensive qualitative comparisons and analyses, effectively supporting the superiority of the proposed method over previous state-of-the-art techniques."
                },
                "weaknesses": {
                    "value": "1. This paper's organization could be improved. For instance, placing the related work (Section 5) after the introduction would help readers understand the difference between conventional positional encoding schemes and the proposed relative transformation encoding scheme. The authors should summarize existing geometric-related positional encoding techniques (not only in novel view synthesis, but also in many 3D vision tasks), and conduct a comprehensive comparison. Since the background information of transformer (the first half of section 2 Background) is already well-known, it can be moved to the appendix. \n\n2. The paper's contribution appears to be incremental, offering limited insights from the proposed relative transformation positional encoding alone.\n\n3. While the paper introduces GTA as a novel positional encoding scheme, it seems to function more like a transformer block (consisting of positional encoding and the attention block) than just a positional encoding scheme. A more detailed ablation study of each component within GTA is needed.\n\nIn general, improving the paper's organization and conducting a more comprehensive comparison with existing techniques would strengthen its contributions. Additionally, more detailed analysis of the components within GTA would enhance the paper's overall quality."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission257/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission257/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission257/Reviewer_LyNT"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698551320434,
            "cdate": 1698551320434,
            "tmdate": 1699635951283,
            "mdate": 1699635951283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "avajdPkRSk",
                "forum": "uJVHygNeSZ",
                "replyto": "SfF2zadF9t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification on ablation studies"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedback. We plan to conduct additional experiments in response to your requests.\n\nRegarding Weakness 3, where it is mentioned:\n>A more detailed ablation study of each component within GTA is needed.\n\nCould you please clarify what specific ablation studies you are envisioning? In Table 5, we have already demonstrated the results when removing the representation matrix applied to the value vector in Equation (4). Additionally, Table 4 shows the results using different representation matrices. Could you suggest what other types of ablation experiments you think would be beneficial?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699781617064,
                "cdate": 1699781617064,
                "tmdate": 1699823404629,
                "mdate": 1699823404629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VmdTpHtFq4",
                "forum": "uJVHygNeSZ",
                "replyto": "avajdPkRSk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Reviewer_LyNT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Reviewer_LyNT"
                ],
                "content": {
                    "comment": {
                        "value": "I mean with or without relative transformation positional encoding. While Table 2 presents the performance comparison of your approach with PRE/APE, these two are the most basic encoding approaches. As I mentioned, it would be better to summarize existing geometric-related positional encoding techniques (not only in novel view synthesis, but also in many 3D vision tasks), and conduct a comprehensive comparison with these approaches."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108774472,
                "cdate": 1700108774472,
                "tmdate": 1700108774472,
                "mdate": 1700108774472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "H0LZjtryAf",
                "forum": "uJVHygNeSZ",
                "replyto": "SfF2zadF9t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer LyNT"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedback. In the revised version, we add a number of ablation studies and numerical comparisons to existing positional encoding schemes in 3D visions. We hope our new version of the manuscript and the response below will address your concerns.\n\n>This paper's organization could be improved. For instance, placing the related work (Section 5) after the introduction would help readers understand the difference between conventional positional encoding schemes and the proposed relative transformation encoding scheme.\n\nThank you for your feedback and suggestions. As suggested, in the revised manuscript we put discussions of 3D position encodings in Section 2 and renamed it to Related work. \n\n> The authors should summarize existing geometric-related positional encoding techniques (not only in novel view synthesis, but also in many 3D vision tasks), and conduct a comprehensive comparison\n\nIn Related work, we added discussions of existing positional encoding (PE) schemes from other 3D vision communities besides NVS. Specifically, we include PEs used in state-of-the-art 3D object detection and segmentation tasks (Frustum embedding, Motion Layer normalization), PEs when depth supervision is available (3DPPE), and also we also include a recent PE method used in geometry-biased transformer (GBT), which biases the attention matrix of each layer by ray distance between each query and key pair. Also we added 3D positional embedding methods for point clouds. \n\n>Background info on TransFormer can be moved to the appendix.\n\nThank you for the suggestion. We make the transformer\u2019s background more concise. We keep the introductions of APE and RPE in Section 2, since it should be helpful for readers to easily understand the difference to our proposed method.\n\n> A more detailed ablation study of each component within GTA is needed.\n\n(Response to our question)\n> I mean with or without relative transformation positional encoding. While Table 2 presents the performance comparison of your approach with PRE/APE, these two are the most basic encoding approaches. As I mentioned, it would be better to summarize existing geometric-related positional encoding techniques (not only in novel view synthesis, but also in many 3D vision tasks), and conduct a comprehensive comparison with these approaches\n\nThank you for your suggestions. We conducted additional experiments with various PEs. The below table shows test PSNRs on CLEVR-TR with Frustum Embedding (Liu et al. 2022), Motion layer norm (MLN) (Liu et al. 2023), Elementwise Mul., RoPE (Su et al. 2021) + FTL (Worrall et al. 2017) and GBT (Venkat et al. 2023). We observe that GTA achieves the best PSNR. This table was added to the revised manuscript as Table 5.\n\n| Method                                    \t| PSNR  \t|\n|-----------------------------------------------|:-----------:|\n| MLN (Liu et al. 2023)                     \t| 31.97    \t|\n| Element-wise Mul.                         \t| 34.33   \t|\n| GBT (Venkat et al. 2023)                  \t| 35.63  \t|\n| Frustum Embedding (Liu et al. 2022)   | 37.23 |\n| RoPE+FTL (Su et al. 2021, Worrall et al. 2017) | 38.18   \t|\n| GTA                                       \t| **38.99** |\n\nExperimental settings for this comparison are described in the Appendix E.4."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689370224,
                "cdate": 1700689370224,
                "tmdate": 1700689370224,
                "mdate": 1700689370224,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l1xZ16edeK",
            "forum": "uJVHygNeSZ",
            "replyto": "uJVHygNeSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission257/Reviewer_ZuJ5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission257/Reviewer_ZuJ5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new methodology for encoding positional information and camera transformations in 3D vision tasks. It employs a special Euclidean group to encode the geometric relationship between queries and key-value pairs within the attention mechanism. Experimental results demonstrate the effectiveness and efficiency of the proposed method, achieving top performance on the novel view synthesis dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The geometric relationship is crucial for multi-view geometry tasks. The proposed method straightforwardly and reasonably encodes transformation and position information into the attention mechanism. The method has demonstrated superior results on several novel view synthesis datasets.\n2. The involvement of the special Euclidean group in encoding transformations is interesting and useful for capturing geometric relationships.\n3. The proposed method requires minimal computation, making it easy to implement and integrate."
                },
                "weaknesses": {
                    "value": "1. Geometry-aware attention for transformers has been extensively explored, and the idea of encoding position and transformation information to improve 3D vision tasks has been discussed previously. While the detailed implementation of the encoding process (such as applying transformations to value vectors) differs from existing works, they share a similar underlying concept, thereby diminishing the novelty of this paper. \n2. The evaluation and ablation study primarily focus on the novel view synthesis task. However, since the proposed method is easily integrable into existing attention-based 3D vision tasks, I highly recommend validating the approach on other 3D vision tasks, such as multi-view 3D detection. \n3. Unlike the feature warping approach, the encoded transformation information relies on matrix multiplication. To be honest, I don't fully grasp the physical meaning behind this."
                },
                "questions": {
                    "value": "1. Table 4 validates that the method proposed in the paper effectively improves accuracy.  I am curious about the performance of other similar methods when evaluated against the same baseline and the same settings. Are they still inferior to the approach presented in the paper?\n2. Additionally, since geometry-aware attention is a general method, can it also enhance accuracy in other perceptual tasks apart from novel-view synthesis?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753763576,
            "cdate": 1698753763576,
            "tmdate": 1699635951189,
            "mdate": 1699635951189,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YBl2qN858Q",
                "forum": "uJVHygNeSZ",
                "replyto": "l1xZ16edeK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Regarding Question 1"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedback. There is one thing we would like to clarify regarding Question 1.\n> I am curious about the performance of other similar methods when evaluated against the same baseline and the same settings. Are they still inferior to the approach presented in the paper? \n\nCould you clarify what is meant by 'other similar methods' in the response? Table 4 focuses on comparisons *within* the GTA mechanism. Specifically, Table 4(a) investigates the effects of removing each component of the representations (SE(3), SO(2), and SO(3)) on performance, while Table 4(b) assesses the impact of different representation choices for image pixel positional encoding on performance. Also, as a comparison to other positional encoding schemes, Table 2 on the left presents performance with absolute positional encoding (APE) and relative positional encoding (RPE). In both encoding strategies, flattened SE(3) and SO(2) matrices are added to the input of each attention layer. Here, we demonstrate the superior performance of our GTA-based model over models based on APE and RPE. Thus, we presume that 'other similar methods' might refer to comparable architectural approaches such as the architecture employed by Du. et al. 2023 and used for our experiments on RealEstate10k and ACID, and that your suggestion is to conduct the same experiments in Table 4 but with different architectures. However, we are not entirely confident in this interpretation. Could you please confirm if this understanding matches what you intended in your response?"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699787694173,
                "cdate": 1699787694173,
                "tmdate": 1699787694173,
                "mdate": 1699787694173,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4n1t5tUcZw",
                "forum": "uJVHygNeSZ",
                "replyto": "YBl2qN858Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Reviewer_ZuJ5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Reviewer_ZuJ5"
                ],
                "content": {
                    "comment": {
                        "value": "I mean other similar methods in image position encoding and camera encoding. For e.g., the rotary PE for image and FTL encoding for camera. Hear FTL~(https://openaccess.thecvf.com/content_ICCV_2017/papers/Worrall_Interpretable_Transformations_With_ICCV_2017_paper.pdf) and their following methods  also aims at encoding camera Intrinsics and extrinsics in transformers. \nIf we combine other  image PE and camera encoding methods, how much worse will the performance be compared to the method proposed in the paper?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700015137740,
                "cdate": 1700015137740,
                "tmdate": 1700015137740,
                "mdate": 1700015137740,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aBee5hyhvP",
                "forum": "uJVHygNeSZ",
                "replyto": "l1xZ16edeK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ZuJ5"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedback. We respond to your concerns one by one below.\n\n>While the detailed implementation of the encoding process (such as applying transformations to value vectors) differs from existing works, they share a similar underlying concept\n\nIn our manuscript, we define our GTA in the language of group theory, which enables us to broaden the scope of the transformations from just 1D sequence in existing work (RoPE) to geometric structures such as SE(3) (camera transformations in our experiments) and can cover even more general geometric structures as long as they can be defined in terms of a group.  \nAlso, the transformation on V, which is missing in RoPE, is not a heuristic in GTA, but naturally arises in our formulation with an intuitive explanation. Value vectors, which will be added to other token features from different coordinate systems, should also be transformed with the relative transformation so that the addition is performed in an aligned coordinate space. Table 4(c) shows removing transformations on V degrades performance significantly. \n\n>Table 4 validates that the method proposed in the paper effectively improves accuracy. I am curious about the performance of other similar methods when evaluated against the same baseline and the same settings. Are they still inferior to the approach presented in the paper?\n\n(From the response to our question) \n>  I mean other similar methods in image position encoding and camera encoding. For e.g., the rotary PE for image and FTL encoding for camera.\n\nThank you for your questions.  To address your concern, we tested with a RoPE  (Su et al. 2021) + FTL (Worrall et al. 2017) model on CLEVR-TR. RoPE is similar to GTA but does not use the SE(3) part (extrinsic matrices) as well as transformations on value vectors. FTL is applying SE(3) matrices to the encoder output to get transformed features to render novel views with the decoder. Note that this is a similar method to the second method in Table 4(a) where we remove SE(3) representation matrices from the encoder. The results are below, and we see the performance of RoPE + FTL is inferior to GTA and also found to be similar to the performance of GTA without SE(3) in the encoder, as expected. We included the number of RoPE+FTL in Table 5.\n| Method                                        \t|\tPSNR \t|\n|---------------------------------------------------|:-----------:|\n| RoPE (Su et al. 2021) + FTL (Worrall et al. 2017) | \t38.18  \t|\n| GTA (ours)                                    \t|  **38.99**  |\n\n>Since the proposed method is easily integrated into existing attention-based 3D vision tasks, I highly recommend validating the approach on other 3D vision tasks, such as multi-view 3D detection.\n>Additionally, since geometry-aware attention is a general method, can it also enhance accuracy in other perceptual tasks apart from novel-view synthesis?\n\nIndeed, the general idea of GTA is applicable to other 3D vision tasks. \nIn 3D detection and segmentation, the target coordinate system is typically an orthographic birds eye view (BEV) grid which requires a different $\\rho$ for the decoder. Additionally, methods in these communities work with high-resolution feature grids, requiring sparse attention mechanisms like deformable attention to work efficiently. We think extending GTA to sparse attention and designing a novel representation $\\rho$ for orthographic target cameras deserves a more thorough investigation than is possible in the rebuttal period and is a promising direction for future work.\n\n> Unlike the feature warping approach, the encoded transformation information relies on matrix multiplication. To be honest, I don't fully grasp the physical meaning behind this.\n\nWe group the channels into multiple 3D points, which encourages the network to learn geometric features.  The attention layers are optimized to match the relevant query and key vectors, therefore each 3-dimensional feature is expected to behave as abstract point clouds. This type of composition of vector spaces is often used in the literature on equivariant neural networks (See Section3 of Deng et al. (2021) or Section 2.6 of Cohen and Welling (2017)) and in the literature on transforming autoencoders (Hinton et al. (2011), Worrall et al . (2017))."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689115556,
                "cdate": 1700689115556,
                "tmdate": 1700689115556,
                "mdate": 1700689115556,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dsls0hI19W",
                "forum": "uJVHygNeSZ",
                "replyto": "aBee5hyhvP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Reviewer_ZuJ5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Reviewer_ZuJ5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the\u00a0thorough response. Most of my questions have been addressed.  I will maintain the existing score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700706435248,
                "cdate": 1700706435248,
                "tmdate": 1700706435248,
                "mdate": 1700706435248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1xaIU3Gml4",
            "forum": "uJVHygNeSZ",
            "replyto": "uJVHygNeSZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission257/Reviewer_myD9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission257/Reviewer_myD9"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes an attention mechanism for multi-view transformers which factors in the geometry information, such as camera pose and image position, of the inputs. This differs from most previous work that often condition Transformers using absolute positional encoding of that information. A key intuition given by the authors is that attention from one token to another should happen in a canonical coordinate space. \nSparse-view novel-view synethesis results on a number of datasets show that the proposed method is state-of-the-art compared against other recent methods as well as a number of baselines. These datasets include synthetic yet challenging scenes (like MSN-Hard) as well as real world indoor and outdoor scenes (ACID and RealEstate10k)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a very well written and for the most part clear paper, with a clear motivation and intuition behind their method, as well as a proper literature review of relevant prior works.\n\nIt aims to tackle an important and challenging problem of novel view synthesis in the sparse (few input images) and wide-baseline (long-range camera variation) case; as such, strong results in challenging benchmarks imply a significant contribution to the community. \n\nThe method and claims are sound. The method itself does not involve a big departure from the standard Transformer architecture, which means it's easy to implement and widely applicably potentially to other problems.\n\nThe experimental set-up is generally thorough. A number of SoTA method are included for comparison and different synthetic as well as real-world datasets are used for evaluation."
                },
                "weaknesses": {
                    "value": "While I agree with the intuition behind this method, what the actual implementation does, and how it affects attention, is somewhat unclear to me. It makes sense to transform features of different views to canonical reference space during attention, and that's obvious only when those are actual coordinates (e.g. points in 3D space); it's not so obvious when features are linear projections of other features. \n\n\nIt's also not obvious that block concatenation of multiple group representations, each with a certain multiplicity, is the best way to achieve it, and I believe this work could do with a bit more discussion on these points, and potentially further ablations/baselines of other ways one could implement the same intuition."
                },
                "questions": {
                    "value": "A number of questions/suggestions:\n\n- As suggested above, I would like to see a variant of GTA that does not involve block concatenation of the geometry representations matrices along with their multiplicites (to mnatch the transformation to the input dimensionality `d`). Have the authors explored a variant such as: concatenate the flattened representation of all groups, followed by a linear projection up to `d`, element-wise multplication with input features (with the rest of the GTA method as proposed)?\n\n- The definition of $\\rho_{g_i g_j}$ in Eq. 4 I believe is missing, which makes it difficult to understand how the authors go from Eq 4. to Eq. 5, as well as their claims regarding the quadratic complexity in Eq 4.\n\n- While some results are provided on real world datasets, these are fairly limited in both size and quality (from looking at the videos). It's therefore difficult to conclude that this method is SoTA more generally in real-world cases. For instance, results would be strengthened if a study of the sensitivity of the model's predictions are to camera errors during training and/or test time. This is relevant given that other methods in the literature have carried out these types of experiments (see e.g. SRT paper, Fig 6)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission257/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780466166,
            "cdate": 1698780466166,
            "tmdate": 1699635951072,
            "mdate": 1699635951072,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y15JkrpxV4",
                "forum": "uJVHygNeSZ",
                "replyto": "1xaIU3Gml4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer myD9"
                    },
                    "comment": {
                        "value": "Thank you very much for your feedback and suggestions. We respond to each of your concerns one by one below:\n\n>While I agree with the intuition behind this method, what the actual implementation does, and how it affects attention, is somewhat unclear to me. It makes sense to transform features of different views to canonical reference space during attention, and that's obvious only when those are actual coordinates (e.g. points in 3D space); it's not so obvious when features are linear projections of other features. \n\nWe treated the channels into multiple 3D points, which encourages the network to learn geometric features. This composition of vector spaces is often used in the literature on equivariant neural networks (See Section3 of Deng et al. (2021) or Section 2.6 of Cohen and Welling (2017)) and in the literature on transforming autoencoders (Hinton et al. (2011), Worrall et al. (2017)). \n\nWe would like to mention that one potential advantage of integrating geometric information as group representations lies in the structured nature of these representations, i.e. the homomorphism property of the representations. In our approach, each QKV vector is transformed by \u03c1(g) in the same way irrespective of its value, ensuring consistency of the transformed vectors. This makes it easier for the subsequent layers to understand the effect of the transformations. Conversely, with absolute positional encoding (APE) and relative positional encoding (RPE), where the flattened camera transformations are added directly to the input, challenges arise. Not only does the flattened matrix lose its geometric structure when merged with each $Q$, $K$, and $V$, but subsequent layers are also burdened with learning the effect of complicated 3D transformations. \n\n\n>It's also not obvious that block concatenation of multiple group representations, each with a certain multiplicity, is the best way to achieve it, and I believe this work could do with a bit more discussion on these points, and potentially further ablations/baselines of other ways one could implement the same intuition.\n\nThank you for your suggestions. To address your concern, we additionally conducted experiments with Kronecker product-based GTA we describe below.  There are two conventional ways to construct representation matrices in representation theory: 1. the direct sum and 2. the Kronecker product of irreducible representations. The former is one we described in sections 3.1 and 3.2. \n\nThe Kronecker product of two square matrices $A, B \\in \\mathbb{R}^{m\\times m}, \\mathbb{R}^{n\\times n}$ is defined as \n\n$\nA\\otimes B = \\begin{bmatrix}\n    a_{11} B & \\cdots & a_{1m} B \\\\\\  \\vdots  & \\ddots  &\\vdots \\\\\\  a_{m1}B & \\cdots  & a_{mm} B\n\\end{bmatrix} \\in \\mathbb{R}^{mn\\times mn}\n$\n\nThe important property of the Kronecker product is that the Kronecker product of two representations is also a representation (homomorphism):  $(\\rho_1 \\otimes \\rho_2)(gg') = (\\rho_1 \\otimes \\rho_2)(g) (\\rho_1 \\otimes \\rho_2)(g') $ where $(\\rho_1\\otimes \\rho_2)(g) := \\rho_1(g) \\otimes \\rho_2(g)$. \nWe additionally experimented with this Kronecker version of GTA on CLEVR-TR and MSN-Hard, where we use the Kronecker product of the $SE(3)$ representation and $SO(2)$ representations for $\\rho_g$ in GTA:\n\n\\begin{align}\n    \\rho_g = \\rho_{\\rm cam} (c) \\otimes (\\rho_{h} (\\theta_h) \\oplus \\rho_{w} (\\theta_w)), \\text{where $g = (c, \\theta_h, \\theta_w)$}.\n\\end{align}\n\nThe multiplicity of $\\rho_{\\rm cam}$, $\\rho_{h}$, $\\rho_{g}$ are set to 1, and the number of frequencies for both $\\rho_{h}$ and $\\rho_{w}$ is set to 4 on CLEVR-TR and 6 on MSN-Hard.\nBelow is the comparison between the Kronecker version of GTA (GTA-Kronecker) with the GTA described. Because both GTA-Kronecker and the original GTA encode the geometric structure in a homomorphic way, the performance is relatively close to each other, especially on MSN-Hard.  We added these results to Table 6 in the Appendix. \n| Method           \t|  CLEVR-TR PSNR  | MSN-Hard PSNR |\n|----------------------|:---------------:|:-------------:|\n| GTA-kronekcer (ours) |   \t38.32    \t| \t24.52 \t|\n| GTA (ours)       \t|\t**38.99**\t|   **24.58**   |"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688146221,
                "cdate": 1700688146221,
                "tmdate": 1700698635034,
                "mdate": 1700698635034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w2dlyn2Df8",
                "forum": "uJVHygNeSZ",
                "replyto": "1xaIU3Gml4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission257/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">I would like to see a variant of GTA that does not involve block concatenation of the geometry representations matrices along with their multiplicites (to mnatch the transformation to the input dimensionality d). Have the authors explored a variant such as: concatenate the flattened representation of all groups, followed by a linear projection up to d, element-wise multplication with input features (with the rest of the GTA method as proposed)?\n\nThank you for your question and suggestion of an interesting variant of GTA. We had not tested element-wise multiplication of linearly transformed flattened representations and additionally ran an experiment with that approach. Note that when we transform the representations into vector form multiplied by QKV vectors, we lose the homomorphism property: suppose $f(g) :=W vec(\\rho(g))$ is the vector embedding of the representation $\\rho(g)$ where $vec()$ is the flattening operation, then $f(gg\u2019) \\neq f(g)f(g\u2019)$ in general. Thus we could say f is not faithful in terms of structure preservation of $g$. The table below shows that the performance significantly degrades with element-wise multiplication to QKV features (and outputs of QKV attn.). \n\n| Method|  CLEVR-TR PSNR  |\n|-----------------------------|:---------------:|\n| Element-wise multiplication |  \t34.33   \t|\n| GTA (ours) |\t**38.99**\t|\n\nWe included this experimental result in Table. 5.\n\n\n>For instance, results would be strengthened if a study of the sensitivity of the model's predictions are to camera errors during training and/or test time.\n\nThank you for your suggestion. We additionally train models with camera noise added to input views, which is the same setting as in the SRT paper. All modes are trained with \u03c3 = 0.1 and tested with different noise strengths on the CLEVR-TR dataset. We describe the experimental settings in detail in Appendix B.4.\nThe results are shown below. GTA shows better performance than RePAST regardless of the noise level.  The table below was added as Table 9. in the revised manuscript.\n| \u03c3                      \t\t\t|  0.01   |\t0.1 \t|\n|----------------------------- |:-------:|:----------:|\n| (RePAST, wo/noise during training) |  33.80  |  30.57  |\n| RePAST (Safin et al. 2023)  \t|  35.26  | 35.14 |\n| GTA (ours) |  **36.66**  |  **36.57**  |\n\nBecause of the limited time of the discussion period, we could not train on MSN-Hard for this experiment,\nbut will add results on that dataset in the final version.\nNote that, RealEstate10k and ACID datasets include camera noise because each camera parameter is estimated with a structure from motion algorithm. We see even in those settings, the model outperforms Du et al. 2023, which uses APE-based positional encoding.\n\n>The definition of \u03c1_{g_ig_j} in Eq. 4 I believe is missing, which makes it difficult to understand how the authors go from Eq 4. to Eq. 5, as well as their claims regarding the quadratic complexity in Eq 4.\n\nThank you for pointing this out. $\\rho_{g_i g_j^{-1}} = \\rho(g_i g_j^{-1}) \\in \\mathbb{R}^{d\\times d}$ is a representation of the relative group element $g_i g_j^{-1}$ between $i$ and $j$-th tokens. From the homomorphism property of \u03c1, this can be decomposed into the product of the representations of each group element:  $\\rho(g_i g_j^{-1}) =  \\rho(g_i) \\rho(g_j^{-1})$, with which we can derive the equation (5). (An example of the homomorphism: suppose $g$ is a degree of 2D rotation and $\\rho(g): R(g)$ is a 2D rotation matrix given a degree $g$. Then for example, we have $R(40) = R(20+20) = R(20)R(20))$.\nIn the revised manuscript, we explicitly mention the homomorphism property after Eq. 4 and define the homomorphism property when we introduce group and representation in Section 3.\n\n\nReferences: \n* Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In Proc. of the International Conf. on Artificial Neural Networks (ICANN), pp. 44\u201351, 2011\n* Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow. Interpretable transformations with encoder-decoder networks. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2017.\n* Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas Guibas. Vector neurons: A general framework for so (3)-equivariant networks. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021.\n* Taco S. Cohen, and Max Welling. Steerable CNNs.  In Proc. of the International Conf. on Learning Representations (ICLR), 2017"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission257/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688603179,
                "cdate": 1700688603179,
                "tmdate": 1700698607967,
                "mdate": 1700698607967,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]