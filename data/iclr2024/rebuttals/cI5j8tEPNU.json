[
    {
        "title": "PAIR Diffusion: A Comprehensive Multimodal Object-Level Image Editor"
    },
    {
        "review": {
            "id": "KFiZxnocKc",
            "forum": "cI5j8tEPNU",
            "replyto": "cI5j8tEPNU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission746/Reviewer_JFgB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission746/Reviewer_JFgB"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the PAIR Diffusion framework, which enables fine-grained control over the properties (appearance and structure) of individual objects in an image, allowing for comprehensive object-level image editing. The framework enables editing operations such as appearance editing, shape editing, adding objects, and variations, without the need for an inversion step. The authors propose multimodal classifier-free guidance, allowing for editing images using both reference images and text. The paper underlines its contributions through qualitative results, demonstrating the framework's effectiveness in enhancing the appearance of objects, whether they are simple or complex."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "_The paper is well-written and easy to follow.  \n_The proposed method is captivating and demonstrates its potential by effectively leveraging multiple features per object. This opens up new horizons for fine-grained object editing within this field.  \n_The authors have also conducted their proof and experiments with diligence and thoroughness, providing robust support for their proposed method."
                },
                "weaknesses": {
                    "value": "_Equation 2 raises a concern regarding the element-wise multiplication between the feature map and the object's shape, given the different sizes of the spatial map (h x w) and the shape (H x W). Moreover, I find the usage of notations j and k in the summation to be ambiguous and would appreciate a more precise definition.  \n_The paper lacks reported inference times for image encoders, considering the use of CNN and transformer-based methods. This is a notable omission in assessing practicality.  \n_Addressing issues with object illumination in appearance editing and adding object to scene is necessary, as some results appear less realistic."
                },
                "questions": {
                    "value": "Have you explored the utilization of alternative backbones such as ResNet or EfficientNet for capturing the low-level characteristics of objects?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission746/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756225768,
            "cdate": 1698756225768,
            "tmdate": 1699636001833,
            "mdate": 1699636001833,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "zkt1f1lGOb",
            "forum": "cI5j8tEPNU",
            "replyto": "cI5j8tEPNU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission746/Reviewer_63QG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission746/Reviewer_63QG"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to object-level image editing, positing images as composites of distinct objects. The aim is to control the property of each object in a fine-grained manner. Two main attributes are defined for each object structure and appearance. Under the formulation of object-level image editing, the proposed method can handle various image editing tasks. These include localized free-form shape editing, appearance editing, simultaneous shape and appearance editing, addition of objects in a precise manner, and varying the image at the object level.  The authors show promising results on each task. They also propose a classifier-free guidance based inference system to integrate all different modalities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. **Presentation**: The paper is laudable for its clarity in showcasing its contributions. It provides ample evidence for the method's efficacy through comprehensive evaluations.\n2. **User Guidance**: The introduced guidance control setting is intuitive. Users have the flexibility to adjust three parameters, offering them control over the reliance on each aspect of the editing process."
                },
                "weaknesses": {
                    "value": "1. **Discussion on Limitations**: The paper misses out on discussing some potential limitations. For instance, in Figure 3 (first row), even though there's a successful transfer of the general color, the resultant car's identity (structure) deviates from both the source and the reference image. Labeling this as \"appearance editing\" might be a misrepresentation."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission746/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission746/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission746/Reviewer_63QG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission746/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788218053,
            "cdate": 1698788218053,
            "tmdate": 1699636001763,
            "mdate": 1699636001763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "G1S9Al1TYW",
            "forum": "cI5j8tEPNU",
            "replyto": "cI5j8tEPNU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission746/Reviewer_GKjx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission746/Reviewer_GKjx"
            ],
            "content": {
                "summary": {
                    "value": "PAIR Diffusion framework introduces a novel approach to object-level image editing. By utilizing diffusion models, the authors enable precise control over the structure and appearance of individual objects within images. Their method facilitates a wide range of editing operations, including appearance and shape editing, object addition, and introducing variations. The approach relies on extracting structure information from panoptic segmentation maps and appearance information from VGG and DINOv2 features. These representations are then used to condition a diffusion model, which is trained to generate images. The reviewer would like to highlight the framework's capacity to define various image editing tasks in terms of altering the structure and appearance of individual objects and introduces multimodal, classifier-free guidance, incorporating reference images and text for controlling the output in conjunction with foundational diffusion models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The framework's editing operations are intuitively designed, making it user-friendly and accessible for image editing tasks, ensuring that users can readily manipulate an image's structure and appearance to achieve desired outcomes.\n\nAn intriguing and noteworthy aspect of the framework is its ability to perform appearance-level editing at the object level. This unique capability pleasantly surprised the reviewer, highlighting the framework's potential to address specific use cases that were not previously explored in depth."
                },
                "weaknesses": {
                    "value": "It would be valuable to see a detailed comparison of the PAIR Diffusion framework's performance with that of Epstein et al.'s \"Diffusion self-guidance for controllable image generation.\" Such a comparison could shed light on the unique strengths and weaknesses of each approach in the context of image editing and generation.\n\nThe use of 2D segmentation maps may introduce geometric distortions in objects, as they lack 3D awareness. Understanding how these limitations impact the framework's ability to accurately represent and edit objects in images is crucial for a comprehensive evaluation of its capabilities and potential improvements."
                },
                "questions": {
                    "value": "Is it possible to achieve identity-preserving edits using the PAIR Diffusion framework, or does it tend to make identities malleable and subject to changes during the editing process? Exploring the framework's capabilities in this regard can offer insights into the preservation of key object or subject attributes.\n\nUnderstanding how a \"dream booth\"-like approach functions within the framework would be intriguing. This raises questions about the mechanisms and control it provides in generating personalized content and the degree to which users can influence the creative and imaginative aspects of the editing process."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission746/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798304757,
            "cdate": 1698798304757,
            "tmdate": 1699636001696,
            "mdate": 1699636001696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "tGrSbavjEn",
            "forum": "cI5j8tEPNU",
            "replyto": "cI5j8tEPNU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission746/Reviewer_H2Ab"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission746/Reviewer_H2Ab"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes PAIR-Diffusion, a general framework to enable object-level editing in diffusion models. It allows editing the structure and appearance of each object in the image independently. The proposed design inherently supports various editing tasks using a single model: localized free-form shape editing, appearance editing, editing shape and appearance simultaneously, adding objects in a controlled manner, and object-level image variation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is meaningful to focus on local editing and integrate multiple tasks within a single framework. \n2. The proposed method is somewhat reasonable and easy to understand.\n3. The paper is overall well-structured."
                },
                "weaknesses": {
                    "value": "1. The designed method performs average pooling to get the appearance feature of local region. How to maintain the local detail features after average pooling? As shown the visualization results, although the overall color and texture are roughly correct, the method does not have the ability to maintain details. For example, in Figure 3 in the main paper, the details of yellow car are changed a lot. In Figure 4 in the main paper, the details of hamburger are changed a lot, although it is still a hamburger. In Figure 12 in the supplementary, the details of car and bagel are also changed a lot. Besides, the designed method is trivial and lacks novelty. \n2. Given an input image, without making any change, based on its semantic map and local appearance features, could this input image be exactly constructed? If so, how is this goal achieved? Based on my understanding, compared with the original image, semantic map and local appearance features have lost much detail information. If not, how can the unedited regions remain the same?\n3. The main paper does not mention the training details. How many training images are used? How about the generalization ability of the model? Could the model generalize well to the test images which are quite different from the training images?\n4. The evaluation details are not very clear. In Section 4.2, is the FID calculated between two images or two image sets? Is L1 distance only calculated within the unedited region? When calculating SSIM between edited region and the target image, is there any guarantee that the edited region and the target image have strict spatial correspondence? Besides, the evaluation metrics are not very comprehensive. For each subtask, only a few baselines are compared, which is not very convincing. The authors should adopt more evaluation metrics and conduct more sufficient comparison."
                },
                "questions": {
                    "value": "1. Provide more explanations for the proposed method and the visualization results. \n2. Provide more details of training process and explanations for the model generalization ability. \n3. Provide more sufficient experimental justification."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission746/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807876292,
            "cdate": 1698807876292,
            "tmdate": 1699636001623,
            "mdate": 1699636001623,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "ODR88DTU3P",
            "forum": "cI5j8tEPNU",
            "replyto": "cI5j8tEPNU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission746/Reviewer_BnZE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission746/Reviewer_BnZE"
            ],
            "content": {
                "summary": {
                    "value": "Problem\n* Text prompts and low-level conditioning cannot control multiple objects individually.\n\nGoal\n* To provide separate control over structure and appearance of individual objects\n\nMethod\n* The proposed method make the diffusion model to be conditioned on segmentation mask and appearance embedding (pretrained VGG and DINO) following ControlNet.\n* Multi-modal (image and text) classifier-free guidance controls strength of each component.\n\nApplications\n* Reference-based appearance editing\n* Random appearance editing\n* Free-form shape editing\n* Adding objects"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The proposed method does not require inversion steps.\n* Instead, it uses pretrained networks.\n2. The proposed method is simple and intuitive.\n3. This paper tackles an interesting and useful editing task."
                },
                "weaknesses": {
                    "value": "1. The proposed method is a naive variation of ControlNet: feature maps instead of edge / keypoints / segmentation masks.\n2. \"We do not require specialized datasets for training\" is an overstatement. It requires pretrained segmentation network and pretrained VGG & DINO.\n3. The proposed method loses style if the reference image is artistic. e.g., Van Gogh in Figure 1 top center and Figure 17 in supp.\n4. Definition of shape editing is vague or it accompanies a wrong experiment.\n* Appearance editing edits the appearance of an object *in the input image* (Figure 3).\n* Object appearance variation edits the appearance  of an object *in the input image* (Figure 13 in supp).\n* On the other hand, shape editing edits the shape of an object *in the reference image* (Figure 4).\n6. Quant. evaluation for semantic image synthesis covers only CelebA-HQ. It would be more informative to add CityScapes / ADE20K / Facades as in SEAN.\n\n(minor)\n\ntypos\n* off the shelf -> off-the-shelf\n* we use them to realize the Eq. 1.  -> remove \"the\"\n* ... many more. Running a grammar checker may help.\n\nPlease do not use \"can\" where it has nothing to do with capability. E.g., \"We can use the method described in ...\" -> We use ..."
                },
                "questions": {
                    "value": "1. What happens if the prompt and the reference do not agree?\n\nPlease refer to Weaknesses for my concerns toward rejection, especially #1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission746/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822162528,
            "cdate": 1698822162528,
            "tmdate": 1699636001537,
            "mdate": 1699636001537,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]