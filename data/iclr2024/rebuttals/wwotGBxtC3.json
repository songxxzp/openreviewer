[
    {
        "title": "Data-Efficient Molecular Generation with Hierarchical Textual Inversion"
    },
    {
        "review": {
            "id": "ilBAHcHyVO",
            "forum": "wwotGBxtC3",
            "replyto": "wwotGBxtC3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4824/Reviewer_9Hsy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4824/Reviewer_9Hsy"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by recent textual inversion technique in the visual domain, the authors proposed Hierarchical textual Inversion for Molecular generation (HI-Mol), a novel data-efficient molecular generation method. Extensive experiments demonstrate the superiority of HI-Mol with notable data-efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Adapting textual inversion to the molecule domain is novel.\n2. The method is well-introduced and convincing. \n3. The authors validated the effectiveness of HI-Mol on several downstream tasks including the molecular optimization for PLogP and the low-shot molecular property prediction on MoleculeNet."
                },
                "weaknesses": {
                    "value": "1. It is worth mentioning some recent works on molecule generation in related works such as:\n\n[1] Hoogeboom E, Satorras V G, Vignac C, et al. Equivariant diffusion for molecule generation in 3d, ICML 2022  \n[2] Zhang Z, Liu Q, Lee C K, et al. An equivariant generative framework for molecular graph-structure Co-design. Chemical Science 2023  \n[3] Flam-Shepherd D, Zhu K, Aspuru-Guzik A. Language models can learn complex molecular distributions. Nature Communications, 2022  \n\n2. Could HI-Mol leverage the structural information of molecules? Could it be adapted for 3D molecule generation?\n3. How to effectively interpret the learned tokens? Do they have chemical meanings?"
                },
                "questions": {
                    "value": "N.A."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4824/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4824/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4824/Reviewer_9Hsy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698453857920,
            "cdate": 1698453857920,
            "tmdate": 1699636465985,
            "mdate": 1699636465985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kPYIgIKlts",
                "forum": "wwotGBxtC3",
                "replyto": "ilBAHcHyVO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9Hsy"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9Hsy,\n\nWe sincerely appreciate your efforts in reviewing our manuscript. We respond to each comment in the following content. We carefully incorporated the discussions into the revised manuscript. We highlighted the revised contents in \"$\\text{\\color{blue}blue}$\" for your convenience to check.\n\n---\n\n**[W1] It is worth mentioning some recent works on molecule generation in related works [1, 2, 3].**\n\nWe appreciate your comment to improve our manuscript. Following your suggestion, we mentioned the related works [1, 2, 3] in Section 2 of our revised manuscript.\n\n---\n**[W2] Could HI-Mol be adapted for 3D molecule generation by leveraging the structural information?**\n\nWe strongly believe that our framework can be adapted for 3D molecular generation by leveraging the 3D structural information of molecules. Although we use the state-of-the-art text-to-molecule model (MolT5), which utilizes the SMILES string as the molecule representation, it is certainly possible to model the 'molecule' part of a text-to-molecule model as 3D molecule (with a recently proposed 3D molecular generative model [1] as a decoder architecture) without any methodological modifications of our method. \n\n---\n**[W3] How to effectively interpret the learned tokens? Do they have chemical meanings?**\n\nLearned tokens have a chemical meaning (i.e., hierarchical features of molecules). For example, detail tokens represent features of a single molecule in the dataset. These learned tokens are effectively interpreted through visualization. For example, in Figure 2 of the manuscript, one can see that certain intermediate tokens represent molecules with chemical substructures such as long carbon chains or sulfonyl groups.\n\n---\n[1] Hoogeboom et al., Equivariant Diffusion for Molecule Generation in 3D, ICML 2022.\\\n[2] Zhang et al., An Equivariant Generative Framework for Molecular Graph-structure Co-design, Chemical Science 2023.\\\n[3] Flam-Shephered et al., Language Models Can Learn Complex Molecular Distribution, Nature Communications 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115592925,
                "cdate": 1700115592925,
                "tmdate": 1700124378930,
                "mdate": 1700124378930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EqsSCuWQxa",
                "forum": "wwotGBxtC3",
                "replyto": "ilBAHcHyVO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9Hsy,\n\nThank you very much again for your time and efforts in reviewing our paper.\n\nWe kindly remind that we have only three days or so in the discussion period.\n\nWe just wonder whether there is any further concern and hope to have a chance to respond before the discussion phase ends.\n\nMany thanks, \\\nAuthors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475119670,
                "cdate": 1700475119670,
                "tmdate": 1700475119670,
                "mdate": 1700475119670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qiesFdCMja",
                "forum": "wwotGBxtC3",
                "replyto": "EqsSCuWQxa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Reviewer_9Hsy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Reviewer_9Hsy"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for the detailed response! Most of my concerns are addressed. \n\nBests,"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707109409,
                "cdate": 1700707109409,
                "tmdate": 1700707109409,
                "mdate": 1700707109409,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JIH2JPSymi",
            "forum": "wwotGBxtC3",
            "replyto": "wwotGBxtC3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4824/Reviewer_Mfay"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4824/Reviewer_Mfay"
            ],
            "content": {
                "summary": {
                    "value": "Recently proposed molecular generation methods are mainly trained on task-related data, which are computationally expensive. The authors propose a hierarchical textual inversion method for molecular generation to overcome this issue."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tIntroducing the successful textural inversion methods from the computer vision area into the molecular generation area is a good idea. \n2.\tThe experimental results presented in the paper demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1.\tThe authors should have a clearer motivation figure in the introduction, which could be specific examples of molecules, to demonstrate that the highly complicated and structured nature of molecules makes it difficult to apply textual inversion directly.\n2.\tThe Molecular language model part in the Section 3.2 Preliminaries should be moved to the Related Work section.\n3.\tTable 2 should also show the results of HI-Mol without grammar.\n4.\tIn Table 6, Valid decreases as the token hierarchical levels increases. The authors should provide some explanations and solutions for this issue.\n5.\tI suggest that the authors consider rearranging the positions of the figures and tables as some of them are too far from the references in the paper."
                },
                "questions": {
                    "value": "1.\tWhat is the \u2018simple resampling strategy\u2019 mentioned in Section 4.2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4824/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4824/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4824/Reviewer_Mfay"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754083817,
            "cdate": 1698754083817,
            "tmdate": 1699636465891,
            "mdate": 1699636465891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I7uhJhkisk",
                "forum": "wwotGBxtC3",
                "replyto": "JIH2JPSymi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Mfay"
                    },
                    "comment": {
                        "value": "Dear Reviewer Mfay,\n\nWe sincerely appreciate your efforts in reviewing our manuscript. We respond to each comment in the following content. We carefully incorporated the discussions into the revised manuscript. We highlighted the revised contents in \"$\\text{\\color{blue}blue}$\" for your convenience to check.\n\n\n---\n**[W1] Clear motivation figure to demonstrate highly complicated and structured nature of molecules.**\n\nThank you for the comment to improve the clarity of our manuscript. Following your suggestion, we added Figure 2 as a motivation figure in the introduction section of our revised manuscript.\n\n---\n**[W2] The molecular language model part in the Section 3.2 should be moved to the Related Work section.**\n\nWe appreciate your suggestion. Following your suggestion, we moved the Molecular language model part to the Related Work section in our revised manuscript.\n\n---\n**[W3] Results of HI-Mol without grammar.**\n\nThank you for the suggestion. We moved the QM9 results of HI-Mol without grammar to Table 3 (which is originally placed in Table 15 of Appendix G), in our revised manuscript.\n\n---\n**[W4] In Table 6, why Valid decreases as the token hierarchical level increases? How to resolve it?**\n\nAs the token hierarchy increases, Validity decreases because the model interpolates more levels of embeddings to generate more diverse molecules (measured by Unique. and Novelty), i.e., there is a trade-off between the diversity of generated molecules and their validity. However, we emphasize that an invalid molecule can be easily converted to a valid molecule via a simple modification algorithm (in Appendix H), which actually follows our desired distribution (see improvement in FCD score in the table below).\n\n\\begin{array}{l|c|ccccc}\n\\hline\n\\text{QM9} &\\text{Grammar} & \\text{FCD $\\downarrow$} & \\text{NSPDK $\\downarrow$} & \\text{Valid. $\\uparrow$} & \\text{Unique. $\\uparrow$} & \\text{Novelty $\\uparrow$} \\newline\n\\hline\n\\text{HI-Mol (Ours)} & \\text{X} &  0.434 & 0.001 & 90.7 & 75.8 & 73.5 \\newline\n\\text{HI-Mol (Ours)} & \\text{O} &  \\textbf{0.430} & \\textbf{0.001} & \\textbf{100} & \\textbf{76.1} & \\textbf{75.6} \\newline\n\\hline\n\\end{array}\n\n---\n**[W5] Rearrangement of the positions of the figures and tables.**\n\nThank you for the comment to improve our manuscript. Following your suggestion, we carefully rearranged the positions of the figures and tables in our revised manuscript.\n\n---\n**[Q1] What is \u2018simple resampling strategy\u2019 in Section 4.2?**\n\nThank you for the opportunity to clarify this point. The \u2018simple resampling strategy\u2019 means that we ignore the generated sample when it is (1) invalid, (2) identical to an already generated sample, or (3) identical to a sample in the training dataset. By applying this strategy, we obtain a set of generated samples whose Valid., Unique., and Novelty scores are all 100 (note that sampling costs only 1.8 sec per molecule). We consider this scenario to align the Valid., Unique., and Novelty scores with some methods (e.g., GDSS) for a fair comparison in the FCD score. Even in this scenario, HI-Mol significantly outperforms those methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700115404656,
                "cdate": 1700115404656,
                "tmdate": 1700124363214,
                "mdate": 1700124363214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WEsTG8V4H8",
                "forum": "wwotGBxtC3",
                "replyto": "JIH2JPSymi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer Mfay,\n\nThank you very much again for your time and efforts in reviewing our paper.\n\nWe kindly remind that we have only three days or so in the discussion period.\n\nWe just wonder whether there is any further concern and hope to have a chance to respond before the discussion phase ends.\n\nMany thanks, \\\nAuthors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475089548,
                "cdate": 1700475089548,
                "tmdate": 1700475089548,
                "mdate": 1700475089548,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LB0mTfc1T7",
                "forum": "wwotGBxtC3",
                "replyto": "WEsTG8V4H8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Reviewer_Mfay"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Reviewer_Mfay"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I have no further questions, and I will keep my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733400072,
                "cdate": 1700733400072,
                "tmdate": 1700733400072,
                "mdate": 1700733400072,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9pgGDUzxOj",
            "forum": "wwotGBxtC3",
            "replyto": "wwotGBxtC3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4824/Reviewer_5Up6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4824/Reviewer_5Up6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Hierarchical Textual Inversion for Molecular generation (HI-Mol), a novel approach to generate molecular structures efficiently with limited datasets. It leverages a new scheme of textual inversion tailored for molecules, using multi-level tokens to capture hierarchical information. This method enables the generation of molecules with significantly less data, showing superiority in various benchmarks such as the MoleculeNet and QM9 datasets, particularly improving data efficiency and performance in molecular property prediction and optimization tasks"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The problem they are trying to tackle is important and interesting.\n- The idea looks relatively novel and justified since molecules are constructed of similar smaller components.\n- The empirical results are promising."
                },
                "weaknesses": {
                    "value": "- The method is not described clearly and in detail. For instance, in the following paragraph of Eq. 1, it is mentioned that the intermediate tokens are \"selected\" during training. This is unclear and should be discussed in more detail.\n\n-  Figure 1 is not expressive enough to outline the method."
                },
                "questions": {
                    "value": "- The authors have mentioned that they are using the Caption2Smiles frozen model as the backbone. Can they please share just the frozen model performance on the tasks?\n\n- Sensitivity of the model's performance to the number of k sounds very important. According to Appendix E, table 10, there is no benefit in increasing k to more than 10. Do the authors have a hypothesis for that? Aren't the molecules supposed to have a lot more \"clusters\", sub-components?\n\n- I'm curious how making this approach multi-modal can be helpful. Could graph embeddings or vision embeddings of the molecules provide any benefit? I'm not a molecular properties expert, but I tried a couple of the figures (table 2 and figure 2) with GPT-4 vision, and it gave meaningful explanations. Have the authors investigated this?\n\n- Can the authors please provide the complete Qm9 results in table 15 of Appendix G? Specifically, what are the results at the 50% and 100% ratios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796622267,
            "cdate": 1698796622267,
            "tmdate": 1699636465752,
            "mdate": 1699636465752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gjq5eqDvIz",
                "forum": "wwotGBxtC3",
                "replyto": "9pgGDUzxOj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5Up6 (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5Up6,\n\nWe sincerely appreciate your efforts in reviewing our manuscript. We respond to each comment in the following content. We carefully incorporated the discussions into the revised manuscript. We highlighted the revised contents in \"$\\text{\\color{blue}blue}$\" for your convenience to check.\n\n---\n**[W1] Clear description of method, e.g., \u201cselected\u201d in Eq. (1).**\n\nThank you for the comment to improve the quality of our manuscript. For clarification, to learn the cluster-wise features of a molecular dataset, we propose a strategy to assign an appropriate cluster index to each molecule. Specifically, given a molecule, we assign (i.e., \u201cselect\u2019\u2019) the intermediate token $[I_k]$ where $k$ minimizes the reconstruction loss among the cluster indices $[1, K]$. This strategy allows us to cluster similar molecules (see Figure 2) in an unsupervised manner. We carefully revised the manuscript to provide a clear description of our method.\n\n---\n**[W2] Figure 1 is not expressive enough to outline the method.**\n\nThank you for the suggestion. Following your comment, we revised Figure 1 to better outline our method.\n\n---\n**[Q1] Performance of the frozen Caption2Smiles model?**\n\nFollowing your suggestion, we provide the generation results of the frozen Caption2Smiles model [1] on the HIV dataset, where we utilize the task description of the HIV dataset as the text prompt. The table below shows that our method indeed significantly outperforms na\u00efvely prompting frozen Caption2Smiles model, which implies that our proposed framework is essential in achieving the superior performance. We added these respects of discussion and the experimental details in Appendix F of our revised manuscript.\n\n\\begin{array}{l|cccccc}\n\\hline\n\\text{HIV} & \\text{Active. $\\uparrow$} & \\text{FCD $\\downarrow$} & \\text{NSPDK $\\downarrow$} & \\text{Valid. $\\uparrow$} & \\text{Unique. $\\uparrow$} & \\text{Novelty $\\uparrow$} \\newline\n\\hline\n\\text{MolT5-Large-Caption2Smiles [1]} & 0.0 & 60.6 & 0.196 & \\textbf{100} & 14.6 & \\textbf{100} \\newline\n\\hline\n\\textbf{HI-Mol (Ours)} & \\textbf{11.4} & \\textbf{16.6} & \\textbf{0.019} & \\textbf{100} & \\textbf{95.6} & \\textbf{100} \\newline\n\\hline\n\\end{array}\n\n---\n**[Q2] In Table 10, there is no benefit in increasing $K$ more than 10. Do the authors have a hypothesis?**\n\nThank you for the insightful comment. We kindly remark that some previous works on molecular clustering have shown that a thousand of molecules can fall into $\\sim$10 clusters [2], which could be a supporting hypothesis on our finding $K=$ 10. While the number of \u201cclusters\u2019\u2019 of molecules can be defined in various ways, we also hypothesize that choosing a relatively small number of clusters (e.g., $K=$ 10) is sufficient in our framework. This is because the sharing of the coarse-grained common features (i.e., intermediate tokens) serves to regularize the fine-grained features (i.e., detail tokens) which are biased toward a single molecule in the embedding interpolation-based sampling. To verify our hypothesis, we conduct an additional experiment, where we replace the intermediate tokens with additional detail tokens for each molecule (i.e., $K=$ 2,113). The reported result below show that the overall performance is rather degraded with $K=$ 2,113 compared to $K=$ 10, 20, and 30. This verifies our hypothesis and justifies our choice of the number of clusters $K$ in our experiments. We added these respects of discussion in Appendix E of our revised manuscript.\n\n\\begin{array}{l|ccccc}\n\\hline\n\\text{$K$} & \\text{FCD $\\downarrow$} & \\text{NSPDK $\\downarrow$} & \\text{Valid. $\\uparrow$} & \\text{Unique. $\\uparrow$} & \\text{Novelty $\\uparrow$} \\newline\n\\hline\n10 & 0.434 & \\textbf{0.001} & \\textbf{90.7} & 75.8 & 73.5 \\newline\n20 & \\textbf{0.430} & \\textbf{0.001} & 87.9 &\\textbf{77.3} & 73.8 \\newline\n30 & 0.436 & \\textbf{0.001} & 88.9 & 77.2 & \\textbf{73.9} \\newline\n\\text{2,113} & 0.443 & \\textbf{0.001} & 86.2 & 75.4 & 72.6 \\newline\n\\hline\n\\end{array}\n\n---\n**[Q3] Could graph embeddings or vision embeddings of the molecules provide any benefit?**\n\nThank you for your insightful comment for our future research direction. We strongly believe that graph embeddings or vision embeddings (such as the suggested GPT-4 vision examples) can be incorporated to improve the controllability of molecular generation of our framework, given some observations in the text-to-image domain that explore such a multi-modal approach to improve the controllability of image generation [3]."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700114588258,
                "cdate": 1700114588258,
                "tmdate": 1700124346103,
                "mdate": 1700124346103,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jCTuNuEw5N",
                "forum": "wwotGBxtC3",
                "replyto": "9pgGDUzxOj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5Up6 (2/2)"
                    },
                    "comment": {
                        "value": "---\n**[Q4] In Table 15, what are the results at the 50% and 100% ratio?**\n\nThank you for the suggestion to improve our manuscript. First, we would like to emphasize that our target problem is molecular generation in the low-data regime, and our method already outperforms state-of-the-art methods with only 2% of training data of the QM9 dataset. For this reason, we thought that the results using 50% and 100% training data are not that meaningful to show the superiority of our method. Nevertheless, for your information, we are currently running the experiments with 50% and 100% QM9 dataset. We will definitely include these results in our final draft (we hope your understanding that it is costly to train with such a large number of molecules in our local machine).\n\n---\n[1] Edwards et al., Translation between Molecules and Natural Language, EMNLP 2022\\\n[2] Hariharan et al., MultiMCS: A Fast Algorithm for the Maximum Common Substructure Problem on Multiple Molecules, JCIM 2011\\\n[3] Rombach et al., High-Resolution Image Synthesis with Latent Diffusion Models, CVPR 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700114656836,
                "cdate": 1700114656836,
                "tmdate": 1700114687922,
                "mdate": 1700114687922,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i8xP43pEhP",
                "forum": "wwotGBxtC3",
                "replyto": "9pgGDUzxOj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5Up6,\n\nThank you very much again for your time and efforts in reviewing our paper.\n\nWe kindly remind that we have only three days or so in the discussion period.\n\nWe just wonder whether there is any further concern and hope to have a chance to respond before the discussion phase ends.\n\nMany thanks, \\\nAuthors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475048510,
                "cdate": 1700475048510,
                "tmdate": 1700475048510,
                "mdate": 1700475048510,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "coT0Vx9niP",
            "forum": "wwotGBxtC3",
            "replyto": "wwotGBxtC3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4824/Reviewer_89Qo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4824/Reviewer_89Qo"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a hierarchical textual inversion strategy, which uniquely selects low-level tokens for each molecule. Subsequently, molecules are generated through a pre-trained text-to-molecule model by interpolating these tokens. Comprehensive testing showcases the marked data-efficiency and superiority of HI-Mol."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The tackled problem is both intriguing and holds practical significance.\n\n2. The paper is articulate and systematically presented.\n\n3. The introduction of multi-level token embeddings enhances the textual inversion model.\n\n4. Very strong experiments, which clearly show the superiority of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The main concern I have with this paper is its novelty. While the ideas of multi-level molecule representation and embedding interpolation are well-established in the field, the authors merely integrate them into the newly introduced textual inversion framework. This casts doubts over the paper's genuine novelty and the depth of its technical contribution.\n\n2. The rationale for adopting the textual inversion model appears somewhat nebulous. In my understanding, compared to SMILES, graph representations are generally more adept at modeling molecular structures. Notably, many graph-centric molecular generation methods have already incorporated hierarchical concepts. In their experiments, while the authors argue that HI-Mol surpasses several graph-based models, the underlying reasons remain unelucidated. An elucidation on how the textual inversion model specifically augments the molecular generation task, relative to its graph-based counterparts, would be greatly beneficial.\n\n3. There's an absence of crucial baselines. The authors have chosen to compare their work with two SMILES-based baselines, one based on RNN and the other on spanning tree. However, they have not included any methods based on large-scale text-to-molecule models, many of which are mentioned in the related works section. Therefore, it is unclear whether the improved performance of HI-Mol is attributable to the utilization of large-scale text-to-molecule models."
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836326886,
            "cdate": 1698836326886,
            "tmdate": 1699636465672,
            "mdate": 1699636465672,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PeThEWCQVG",
                "forum": "wwotGBxtC3",
                "replyto": "coT0Vx9niP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 89Qo"
                    },
                    "comment": {
                        "value": "Dear Reviewer 89Qo,\n\nWe sincerely appreciate your efforts in reviewing our manuscript. We respond to each comment in the following content. We carefully incorporated the discussions into the revised manuscript. We highlighted the revised contents in \"$\\text{\\color{blue}blue}$\" for your convenience to check.\n\n---\n\n**[W1] Novelty. Multi-level molecule representation and embedding interpolation are well-established.**\n\n\nWe politely disagree with the reviewer\u2019s concern; although the high-level concepts of multi-level representation and embedding interpolation have been used for various purposes and domains in the literature, their usage for our own purpose could be novel and not straightforward. Specifically, we strongly believe that our novelty lies in (1) introducing the textual inversion for molecular generation and (2) advancing the existing textual inversion technique (which originally uses a single shared token in the image domain) by introducing multi-level tokens. Here, (2) is motivated by our new finding that a na\u00efve adoption of textual inversion using a single shared token does not perform well in molecular generation. To alleviate this issue, we introduce a new concept, namely multi-level tokens, into the textual inversion framework to capture the complex distribution of molecules. Furthermore, we carefully design the sampling strategy based on the interpolation, since it is not straightforward to sample molecules from our newly proposed multi-level token embeddings. Therefore, our method is not a straightforward integration of previous techniques, but a novel framework for successfully adapting the textual inversion to molecular generation by introducing multi-level tokens into the textual inversion framework. \n\n---\n\n**[W2] Rationale for adopting textual inversion appears somewhat nebulous. Graph representations are generally more adept at modeling molecular structures.**\n\nFor clarification, we use SMILES strings as the representation of molecules because our method is built upon the state-of-the-art text-to-molecule model, MolT5 [1], that utilizes SMILES strings, not graphs. On the other hand, if there exist graph-based text-to-molecule models, our method is also applicable to them (then, we would use graphs as the representation of molecules). Namely, our method is agnostic to the underlying molecule representation of the foundation models, and we do not argue for the superiority of the molecule representation between SMILES strings and molecular graphs.\n\n---\n\n**[W3] Absence of baselines based on large-scale text-to-molecule model.**\n\n\nAlthough our framework is built upon large-scale text-to-molecule models, they are not meaningful baselines. This is because it is not feasible to prompt all the training molecules to the mentioned large-scale text-to-molecule models due to the maximum token length of the input prompt. Specifically, while the maximum token length of the text-to-molecule model is only 1024 and 512 for [1] and [2], respectively, we have thousands of molecules and each SMILES representation of a molecule can sometimes consist of more than 100 tokens. Nevertheless, one can utilize the mentioned text-to-molecule models [1, 2] by providing the task description only (without providing the training molecules). For your information, the table below shows that our method indeed significantly outperforms such a na\u00efve prompting of the text-to-molecule models on the HIV dataset. This confirms that our improved performance is not just due to the utilization of the large-scale text-to-molecule model, but to our carefully designed textual inversion framework. We added these respects of discussion and the experimental details in Appendix F of our revised manuscript.\n\n\\begin{array}{l|cccccc}\n\\hline\n\\text{HIV} & \\text{Active. $\\uparrow$} & \\text{FCD $\\downarrow$} & \\text{NSPDK $\\downarrow$} & \\text{Valid. $\\uparrow$} & \\text{Unique. $\\uparrow$} & \\text{Novelty $\\uparrow$} \\newline\n\\hline\n\\text{MolT5-Large-Caption2Smiles [1]} & 0.0 & 60.6 & 0.196 & \\textbf{100} & 14.6 & \\textbf{100} \\newline\n\\text{Text+Chem T5-augm-base [2]} & 0.0 & 62.2 & 0.188 & \\textbf{100} & 90.2 & \\textbf{100} \\newline\n\\hline\n\\textbf{HI-Mol (Ours)} & \\textbf{11.4} & \\textbf{16.6} & \\textbf{0.019} & \\textbf{100} & \\textbf{95.6} & \\textbf{100} \\newline\n\\hline\n\\end{array}\n\n\n---\n[1] Edwards et al., Translation between Molecules and Natural Language, EMNLP 2022\\\n[2] Christofidellis et al., Unifying Molecular and Textual Representations via Multi-task Language Modeling, ICML 2023"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700113824545,
                "cdate": 1700113824545,
                "tmdate": 1700124319653,
                "mdate": 1700124319653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iQrQ1gPSq2",
                "forum": "wwotGBxtC3",
                "replyto": "coT0Vx9niP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer 89Qo,\n\nThank you very much again for your time and efforts in reviewing our paper.\n\nWe kindly remind that we have only three days or so in the discussion period.\n\nWe just wonder whether there is any further concern and hope to have a chance to respond before the discussion phase ends.\n\nMany thanks, \\\nAuthors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700475000881,
                "cdate": 1700475000881,
                "tmdate": 1700475000881,
                "mdate": 1700475000881,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s1b9bucmcw",
                "forum": "wwotGBxtC3",
                "replyto": "coT0Vx9niP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4824/Reviewer_89Qo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4824/Reviewer_89Qo"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\nI express my gratitude to the authors for their work. However, I still have some questions about certain aspects.\n\n1. In the generation of the Hi-Mol method, are active molecules used as references? If so, have other generative models, including large text-to-molecule models, been fine-tuned based on active molecules?\n\n2. I still think SMILES alone is not enough. Many graph-based generative models can also perform molecular optimization tasks. Examples of such models include Graph-GA, DST, RetMol, etc."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644432684,
                "cdate": 1700644432684,
                "tmdate": 1700644432684,
                "mdate": 1700644432684,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]