[
    {
        "title": "Graph Transformers for Large Graphs"
    },
    {
        "review": {
            "id": "m5TAWWYVyd",
            "forum": "tGOOP7DGxs",
            "replyto": "tGOOP7DGxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3578/Reviewer_yzrb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3578/Reviewer_yzrb"
            ],
            "content": {
                "summary": {
                    "value": "The author highlights that while transformers have demonstrated remarkable performance in tasks related to predicting graph properties, their application has been restricted to small-scale graphs due to computational limitations. Additionally, the author contends that the existing neighbor sampling method constrains the model's ability to consider more global information. Consequently, this paper introduces a comprehensive GT framework, with a focus on enhancing model capacity and scalability. The proposed framework, known as LargeGT, combines a rapid neighborhood sampling technique with a local attention mechanism and an approximate global codebook. Extensive experiments illustrate that by integrating local and global attention mechanisms, LargeGT achieves improved performance in node classification tasks. Notably, LargeGT demonstrates a 3\u00d7 speedup and a 16.8% performance enhancement in specific node classification benchmarks when compared to their closest baseline models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is excellently composed, offering a straightforward narrative that's easy to follow. Notably, key terms and important experimental findings have been highlighted using various colors, resulting in an effective visual presentation.\n\n2. The experimental results presented in this paper indicate that the proposed framework can achieve superior performance within a shorter training time.\n\n3. The author introduces two significant challenges associated with handling large-scale graphs: scalability and constraints related to local information aggregation. These issues are prevalent and indeed worth discussing. As the author pointed out, computational resource requirements increase quadratically with the growing number of nodes. To address this, the author has proposed both a local and a global aggregation module. The former employs conventional sampling techniques to learn local representations, while the latter focuses on deriving insights from global node vector projections. Downstream predictions are then made based on both sets of representations. The problems raised, and the respective solutions are meaningful and coherent with each other."
                },
                "weaknesses": {
                    "value": "Despite the fluent presentation, some concerns arise in this paper. Firstly, the level of novelty in this framework appears limited. It is apparent that this paper heavily relies on the previous work, GOAT, particularly the global module, which encodes mini-batch nodes using global graph nodes. This component was introduced in a prior paper. The other aspects are mainly focused on aligning local and global features. The framework appears more like an updated version of GOAT than a fundamentally new invention.\n\nMoreover, in the experimental section, the comparison between the LG transformer and other baselines reveals that the proposed framework doesn't consistently outperform GOAT-local, especially in the ogbn-products dataset. Furthermore, in the ogbn-papers100M dataset, the framework is only compared to a single baseline. It's possible that other methods struggle with extremely large graphs, but there are likely additional viable solutions that should be explored.\n\nAdditionally, the fusion of transformers and Graph Neural Networks (GNNs) is a dynamic research area with various ongoing studies, such as TransGNN and Graphformers. It would be valuable to understand how these methods perform when confronted with similar tasks.\n\nLastly, the author emphasizes the significance of combining local and global representations. However, apart from GOAT, there are other techniques that can address this challenge, such as randomly selecting both nearby neighbors and global features. The author should offer further clarification on this matter."
                },
                "questions": {
                    "value": "This paper commences with two important challenges that have attracted the attention of numerous researchers. Specific comments were provided in the previous section, and it is hoped that the author will consider improvements from the following viewpoints.\n\nThe framework appears to inherit many key components from previous papers, with limited significant modifications. It would be beneficial to include more in-depth discussions and comparisons with transformer-based Graph Neural Networks (GNNs). Additionally, it is important to address how other approaches perform in terms of extracting global information from the graph.\n\nExpanding on these aspects would enhance the paper's contribution and provide a more comprehensive understanding of the research landscape in this domain."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676916556,
            "cdate": 1698676916556,
            "tmdate": 1699636312762,
            "mdate": 1699636312762,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8qNuSd50U0",
                "forum": "tGOOP7DGxs",
                "replyto": "m5TAWWYVyd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yzrb (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your time reading and evaluating our work, as well as highlighting the strengths of our paper. We would like to take this opportunity to justify some elements of our proposed method and explain the results as asked in your questions.\n\n>**Reviewer:** Despite the fluent presentation, some concerns arise in this paper. Firstly, the level of novelty in this framework appears limited. It is apparent that this paper heavily relies on the previous work, GOAT, particularly the global module, which encodes mini-batch nodes using global graph nodes. This component was introduced in a prior paper. The other aspects are mainly focused on aligning local and global features. The framework appears more like an updated version of GOAT than a fundamentally new invention.\n\n**Authors:** We thank you for your question on comparison with GOAT. We would like to clarify here that GOAT\u2019s global module is the only thing we adapt in LargeGT, while our local module is entirely different from GOAT, and addresses the bottleneck that was present in GOAT (we refer to page 7 of GOAT paper https://proceedings.mlr.press/v202/kong23a/kong23a.pdf) which explicitly claims \u201cThe neighbor sampling bottleneck is the major limitation of our method.\u201d To contextualize this, if one were to use GOAT, the model would be severely limited when operating on large graphs. In contrast to this, our contributed model LargeGT would not be limited and would work efficiently on large graphs. \nTo sum up, the main differences between LargeGT and GOAT are as follows: (i) LargeGT consists of a completely different local module compared to GOAT, which processes a fixed size of input tokens for local self-attention. (ii) LargeGT achieves a 4-hop receptive field through just 2-hop operations, whereas GOAT requires 4-hop operations to attain the same, often resulting in prohibitive computational costs in large graphs. (iii) The computational complexity of LargeGT is independent of the number of nodes in the graphs, unlike that of GOAT. (iv) Unlike GOAT, LargeGT implements sampling prior to training, addressing a significant computational bottleneck in GOAT\u2019s local module.\n\n___\n>**Reviewer:** Moreover, in the experimental section, the comparison between the LG transformer and other baselines reveals that the proposed framework doesn't consistently outperform GOAT-local, especially in the ogbn-products dataset. Furthermore, in the ogbn-papers100M dataset, the framework is only compared to a single baseline. It's possible that other methods struggle with extremely large graphs, but there are likely additional viable solutions that should be explored.\n\n**Authors:** On ogbn-products, GOAT-local-\u03b4 shows better performance due to the dataset's homophilic nature, where local-only information aggregation is often sufficient. Conversely, for snap-patents, a non-homophilic dataset, LargeGT excels by incorporating both local and global neighbor information. This adaptability of LargeGT to different dataset characteristics explains the observed performances. We have included our observation to this regard in Section 4.2 under the heading \u201cOn Performance\u201d. For ogbn-papers100M dataset, due to computational requirements, we compare our proposed architecture with the closest baseline which our work significantly improves, since we have provided a comparison of LargeGT with other baselines for the remaining two datasets. In other words, we use ogbn-products and snap-patents to verify our proposed model against a wide range of baselines, while for ogbn-papers100M, the objective translates to showing that it scales to such a dataset and also provides improved performance.\n\n___\n>**Reviewer:** Additionally, the fusion of transformers and Graph Neural Networks (GNNs) is a dynamic research area with various ongoing studies, such as TransGNN and Graphformers. It would be valuable to understand how these methods perform when confronted with similar tasks.\n\n**Authors:** We thank you for highlighting this connection, and refer to Section 2 in our paper where we have reviewed the classes of works on MPNNs scaling and Graph Transformers while sharing their respective limitations which build the motivations of our architecture. To summarize the key points here again: Fusions of GNNs with Transformers have produced several powerful Graph Transformer (GT) models, Graphormer being one of this class. However, an obvious barrier for GTs to scale to large graphs is the quadratic complexity brought by full-graph attention, i.e., O(N^2), with N being the number of nodes in a graph. There are other solutions which inject an extent of sparsity in the GTs to bring the quadratic complexity down, which work well on medium scale graphs. Yet, those remain unscalable on single large graphs due to the entire graph structure being operated upon."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503897567,
                "cdate": 1700503897567,
                "tmdate": 1700503897567,
                "mdate": 1700503897567,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GtPCNBGfhD",
                "forum": "tGOOP7DGxs",
                "replyto": "8qNuSd50U0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3578/Reviewer_yzrb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3578/Reviewer_yzrb"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the responses from the authors. I maintain my rating. The responses only addressed some of my concerns. For example, I am still concerned about the novelty, compared with GOAT.\n\nAlso, I agree with Reviewer xiXc. More baselines are needed. There are so many scalable algorithms for large graphs. At least several state-of-the-art scalable graph neural networks should be included. The final goal of this paper should be a scalable and effective graph learning framework. Adding transformers is not the goal, but the means."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675897861,
                "cdate": 1700675897861,
                "tmdate": 1700675897861,
                "mdate": 1700675897861,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V6nC0Qj2Gi",
            "forum": "tGOOP7DGxs",
            "replyto": "tGOOP7DGxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3578/Reviewer_S1aR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3578/Reviewer_S1aR"
            ],
            "content": {
                "summary": {
                    "value": "To scale graph models to large-scale graphs, MPNNs are often reduced to restricted receptive fields making them myopic, while Graph Transformers (GTs) fail because of their quadratic cost. This paper proposes a new framework for sampling sub-graphs to train a large GT that uses local and global modules to improve model performance and compute complexity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors propose a framework that leverage recent advances in graph transformer models, and address a critical challenge that limits the scalability of existing approaches, both MPNNs and GTs.  \n- The introduction provides a great overview of the current challenges for large-scale graph learning, and does a great job at comparing MPNNs and GTs, while setting stage for key concepts like neighborhood sampling."
                },
                "weaknesses": {
                    "value": "1. Baselines: LargeGT is compared to \"constrained versions\" of various baselines, notably all models are constrained to 2 hops only, while LargeGT has access to 4-hops worth of neighbors (in the local module). Including the non-constrained versions of these same baselines is critical for evaluation, even if they are more computationally demanding. Currently it is unclear whether adopting LargeGT leads to lower performance compared to state-of-the-art methods, at the expense of computational efficiency.\n2. Additionally, no auxiliary label propagation or augmentations are used for the baseline methods, when they are used in methods reported in the OGB leaderboard. These enhancements are not altering the receptive field of the baselines, and thus shouldn't impact computational performance, but might improve classification performance. This should be taken into account when comparing with approaches that might still outperform the proposed method, even under constrained training (2-hop). \n3. The main innovation can seemingly be credited to the use of the global codebook, so it is hard to define the main contribution of this work. If the focus of this work is combining all these different building blocks into a compute efficient framework, I would expect to see a more expansive breakdown of the computational costs of different components, memory usage and requirements. Notably, how is \"Epoch time\" defined in Figure 2? All models might be processing different amounts of data and thus might have different definitions of an \"epoch\" due to differences in sampling strategies. How many nodes does each model process in an epoch? Different models might require different numbers of epochs to converge, shouldn't total training time be more important?\n4. [Minor] A lot of the content in the first 4 pages is repetitive."
                },
                "questions": {
                    "value": "1. What are the memory constraints of using LargeGT compared to other baselines? How is the choice of batch size impacted by the choice of hyperparameter K? \n2. How important is the choice of a 4-hop neighborhood for the local module. Can the model still perform competitively given that it still has access to global information through the global module?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808686594,
            "cdate": 1698808686594,
            "tmdate": 1699636312658,
            "mdate": 1699636312658,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VZNWZSoXd0",
                "forum": "tGOOP7DGxs",
                "replyto": "V6nC0Qj2Gi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer S1aR (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your time reading and evaluating our work, as well as pointing out our strengths. We would like to take this opportunity to justify some elements of our proposed method and answer your questions on baselines, experiments and the contribution with respect to the literature.\n\n_____\n>**Reviewer:** Baselines: LargeGT is compared to \"constrained versions\" of various baselines, notably all models are constrained to 2 hops only, while LargeGT has access to 4-hops worth of neighbors (in the local module). Including the non-constrained versions of these same baselines is critical for evaluation, even if they are more computationally demanding. Currently it is unclear whether adopting LargeGT leads to lower performance compared to state-of-the-art methods, at the expense of computational efficiency.\n\n**Authors:** We appreciate your feedback on the selection of baselines for comparison. The decision to compare LargeGT with 2-hop constrained versions of various models was initially to maintain a consistent scope of expensive computation incurred across all models. For instance, the cost of fetching l-hop neighbors for a node in a very large graph would be the same and agnostic of any selected model. Nevertheless, below is a comparison of the baselines along with LargeGT scores on ogbn-products without restriction to 2-hop. For instance, GOAT models are run with original hyperparameters which samples 20, 10, 5 neighbors at the 3 hops.\n\n\n| Model | \t\tTest Acc (original) | \tEpoch Time (original) |Test Acc (2-hop ) | Epoch Time (2 hop)|\n| - | - | - | - | - | \n|GraphSAGE | \t\t78.17 |\t\t22s|\t\t76.62|\t\t\t7s|\n|GAT | \t\t\t79.21 |\t\t86s|\t\t77.38|\t\t\t31s|\n|GT-sparse | \t\t67.87 |\t\t40s|\t\t60.76|\t\t\t12s|\n|NAGphormer |\t77.71 |\t\t22s|\t\t75.28|\t\t\t8s|\n|GOAT-local | \t\t81.29 |\t\t490s|\t\t81.17\t|\t\t120s|\n|GOAT-global | \t70.28 |\t\t3.5s|\t\t70.28\t|\t\t3.5s|\n|GOAT-full | \t\t81.21 |\t\t500s|\t\t79.88\t|\t\t205s|\n|LargeGT-local |\t78.95 |\t\t45s|\t\t78.95\t|\t\t45s|\n|LargeGT-full | \t\t79.81 |\t\t68s|\t\t79.81\t|\t\t68s|\n\nWe can observe that the best performing model remains the same while LargeGT (in both cases uses up to 2 hop operations) remains better and competitive compared to the best performing model. In the revised manuscript, we have included a section Appendix A.4 which details these comparisons to provide a clearer perspective on LargeGT's performance relative to state-of-the-art methods.\n\n_____\n>**Reviewer:** Additionally, no auxiliary label propagation or augmentations are used for the baseline methods, when they are used in methods reported in the OGB leaderboard. These enhancements are not altering the receptive field of the baselines, and thus shouldn't impact computational performance, but might improve classification performance. This should be taken into account when comparing with approaches that might still outperform the proposed method, even under constrained training (2-hop).\n\n**Authors:** Thank you for pointing out the absence of auxiliary label propagation or augmentations for baseline methods. We acknowledge that including these enhancements could have a chance to provide improved performance for specific models. However, it has been observed in the leaderboards that the composition of several enhancement techniques blurs the dissection on which model components are critical and contributes highly to the performance of a model. For example, a label propagation technique known as Correct & Smooth (C&S) [1] is employed in various models on the OGB leaderboard for ogbn-products [2]. It demonstrates that MLP-based models with C&S perform both better and worse than TransformerConv [3] in different settings. This provides a somewhat blurred perspective on whether MLP or Transformer models are superior for ogbn-products from a neural network perspective. We  mention our reasoning in Section 4.1 while GOAT, which we closely follow, also follow a similar experimental setting on not using enhancement tricks.\n\n[1] Huang, Q., He, H., Singh, A., Lim, S.N. and Benson, A.R., 2020. Combining label propagation and simple models out-performs graph neural networks.     \n[2] https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-products.    \n[3] Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W. and Sun, Y., 2020. Masked label prediction: Unified message passing model for semi-supervised classification."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503550899,
                "cdate": 1700503550899,
                "tmdate": 1700503550899,
                "mdate": 1700503550899,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0YD6wKInMx",
            "forum": "tGOOP7DGxs",
            "replyto": "tGOOP7DGxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3578/Reviewer_xiXc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3578/Reviewer_xiXc"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes LargeGT, a scalable graph transformer for large-scale graphs. It uses fast neighborhood sampling and a local attention mechanism to learn local representations. These are integrated with global representations from an approximate global codebook. This framework overcomes previous computational bottlenecks, achieving 3x speedup and 16.8% better performance on benchmarks compared to baselines. LargeGT also scales to 100M nodes, advancing representation learning for single large graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The model's performance is thoroughly validated on large-scale graphs, demonstrating sufficient workload.\n* Exploring base model architectures on graphs is a very valuable endeavor."
                },
                "weaknesses": {
                    "value": "* The efficiency analysis is incorrect. In Algorithm 1, it is required to gather 1/2-degree neighbors for each node, and then select k nodes. The process of selecting nodes is O(K), but if the graph is relatively dense, the complexity of gathering second-degree neighbors is O(N^2).\n* In Algorithm 1, some nodes are sampled with replacement, while some are sampled without replacement. It is uncertain whether this will introduce bias in the sampling.\n* It lacks some key baselines such as SGC[1], SIGN[2]. \n\nReference:\n\n[1] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. \"Simplifying graph convolutional networks.\" In International conference on machine learning, pp. 6861-6871. PMLR, 2019.\n\n[2] Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Ben Chamberlain, Michael Bronstein, and Federico Monti. \"Sign: Scalable inception graph neural networks.\" arXiv preprint arXiv:2004.11198 (2020)."
                },
                "questions": {
                    "value": "See. Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3578/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3578/Reviewer_xiXc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840085358,
            "cdate": 1698840085358,
            "tmdate": 1699636312578,
            "mdate": 1699636312578,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sjsTbMrZrY",
                "forum": "tGOOP7DGxs",
                "replyto": "0YD6wKInMx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xiXc"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your time reading and evaluating our work, as well as pointing out the strengths of our paper. We would like to take this opportunity to clarify some elements of our proposed method and provide answers to your questions.\n\n>**Reviewer:** The efficiency analysis is incorrect. In Algorithm 1, it is required to gather 1/2-degree neighbors for each node, and then select k nodes. The process of selecting nodes is O(K), but if the graph is relatively dense, the complexity of gathering second-degree neighbors is O(N^2).\n\n**Authors:** We appreciate your critical assessment of the efficiency analysis. The concern regarding the complexity of gathering second-degree neighbors in dense graphs is valid. However, Algorithm 1 is an offline step and does not affect the runtime during training and/or inference. We have written more on this under the heading \u201cOffline Step Prior to Training\u201d in Section 3.2. Our computational complexity analysis in Section 3.2 is based on the neural network module \u2013 local module and global module (Fig 1) \u2014 runtimes as usually done in the literature when reporting the computational complexity of a neural network layer. We have included a clarification on the complexity analysis of the offline step and of the LargeGT modules in our revised manuscript.\n\n___\n>**Reviewer:** In Algorithm 1, some nodes are sampled with replacement, while some are sampled without replacement. It is uncertain whether this will introduce bias in the sampling.\n\n**Authors:** Thank you for highlighting this aspect of our sampling method in Algorithm 1. We take this opportunity to clarify that the \u201csampling with replacement\u201d only applies to those nodes whose 1 and 2 hop neighbors are less than K-1. This becomes a corner case and applies to only a small fraction of the nodes in general. As such it does not bias how we perform our overall sampling, except that we apply special handling of the nodes with lesser neighbors than K-1 . It is true that such corner cases can be handled differently as well, such as using padding nodes, instead of sampling with replacements. \n\n___\n>**Reviewer:** It lacks some key baselines such as SGC[1], SIGN[2].\n\n**Authors:** We thank you for pointing out the absence of comparisons with baselines like SGC and SIGN. The decision to exclude these baselines initially was based on their different focus compared to LargeGT. However, aspects of these works are closely related such as \u201coffloading heavy computations away from the training stage\u201d which we also follow in Algorithm 2 in the form of \u201cHop context features\u201d . While we had already included SIGN in Section 2 of our paper, we have now also included SGC in the revised manuscript. This addition falls under the collection of related works that 'perform information propagation prior to the training stage', aiming to reduce the training cost of GCNs.\"\n\n___\n**Authors:** We hope our answers have addressed your concerns. Let us know if you have any other questions or concerns. In light of our answers to your concerns, we hope you consider raising your score. If you have any more concerns, please do not hesitate to ask and we'll be happy to respond."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502289174,
                "cdate": 1700502289174,
                "tmdate": 1700503622773,
                "mdate": 1700503622773,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WD6BUXM3M8",
            "forum": "tGOOP7DGxs",
            "replyto": "tGOOP7DGxs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3578/Reviewer_dgE1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3578/Reviewer_dgE1"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes LargeGT for training graph transformers for large graphs. Neighborhood sampling usually samples at most 2-hop neighbors as in GOAT (Kong et al., 2023). The proposed method stores a matrix storing the sum of node features of 1-hop and 2-hop neighbors before training. Then sample 2-hop neighbors for a specific node and get the sum features from the matrix, which is at most 4-hop information for the node. It also adopts GOAT (Kong et al., 2023) as the global module. Experiments show it trains faster than GOAT."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed neighbor sampling intuitively improves the model accuracy by getting information at most 4-hop away.\n2. Extensive experiments are performed.\n3. The writing of the proposed method is very clear."
                },
                "weaknesses": {
                    "value": "1. The mechanism of why LargeGT runs faster than baselines like GOAT is unclear. Since the proposed neighbor sampling has a bigger input matrix than a simple 2-hop neighbor sampling method, does it run longer than the traditional method?\n2. The runtime highly depends on the hyperparameter $K$, which is the number of nodes for sampling. Authors need to provide a fair and solid comparison with the traditional 2-hop neighbor sampling method.\n3. Experiment performances are not explained well (see questions)."
                },
                "questions": {
                    "value": "1. In Table 2, why does GOAT-local-\u03b4 have better accuracy in ogbn-products?\n2. For snap-patents in Table 2, why does LargeGT have much better model accuracy than all baselines?\n3. For snap-patents in Table 3, why does the model accuracy drop when $K>50$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3578/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3578/Reviewer_dgE1"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865571534,
            "cdate": 1698865571534,
            "tmdate": 1699636312509,
            "mdate": 1699636312509,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zQDFJdSuG4",
                "forum": "tGOOP7DGxs",
                "replyto": "WD6BUXM3M8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dgE1 (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your time reading and evaluating our work, as well as highlighting the strengths of our paper. We would like to take this opportunity to justify some elements of our proposed method and explain the results as asked in your questions.\n___\n>**Reviewer:** (In Summary of the review)  \u201c... Neighborhood sampling usually samples at most 2-hop neighbors as in GOAT (Kong et al., 2023).. \u201d  \n\n**Authors:** We would like to clarify that GOAT\u2019s method DOES NOT usually do 2-hop sampling. In fact, their experiments use a 3-layer or 3-hop sampling, with an equal hop of receptive field. In our method, we perform at most 2-hop sampling and achieve a receptive field of 4-hop by the use of each sampled nodes\u2019 1-hop and 2-hop context features that are pre-computed offline (Figure 1; Algorithm 2).  As such, we have an efficient sampling technique as a contribution  as presented in the paper.\n\n___\n>**Reviewer:** The mechanism of why LargeGT runs faster than baselines like GOAT is unclear. Since the proposed neighbor sampling has a bigger input matrix than a simple 2-hop neighbor sampling method, does it run longer than the traditional method?\n\n**Authors:** There are two reasons for this. (i) Conceptually,  LargeGT runs faster than baselines like GOAT due to the introduction of our local sampling technique that is performed offline and is restricted to 2-hop neighbors, as opposed to GOAT which performs sampling during mini-batching and frequently uses 3+ hop in practice. (ii) Experimentally, in our paper, we keep 2-hops for LargeGT and GOAT-\u03b4. In such a case, the efficiency in LargeGT comes from the fact that these 2-hop neighbors are sampled offline and not during mini-batching. Although we have discussed the characteristics of LargeGT in terms of \u201cScalability\u201d in Section 3 under the subheading \u201cCharacteristics of the framework\u201d, we will elaborate further on these points in the revised manuscript for clarity. Additionally, the computational complexity of global module in both LargeGT and GOAT are the same. However, GOAT\u2019s local module is dependent on the number of nodes in a graph and the sampling hop length, unlike LargeGT\u2019s local module, which enables LargeGT to scale better. We refer to GOAT\u2019s paper where they highlight their local module as a major limitation which is addressed in our work as part of our contribution.\n\n___\n>**Reviewer:** The runtime highly depends on the hyperparameter K, which is the number of nodes for sampling. Authors need to provide a fair and solid comparison with the traditional 2-hop neighbor sampling method.\n\n**Authors:** Since the computational complexity of LargeGT\u2019s local module is dependent on K, the runtime depends on K. We would like to refer to Figure 1 and Eqns. 1-6 in our paper (further in Appendix A.2) which informs that the hyperparameter K determines the size of node sampled \u201cprior to training\u201d as well as the number of tokens for LargeGT\u2019s local module, which is 3K (see \u201cComplexity\u201d in Section 3). As such, the hyperparameter K will influence the size of the data (or input tokens) the local module is operating on, thus affecting the runtime. Our sampling method prior to training is random sampling as outlined in Algorithm 1 \u2013 similar to how traditional sampling method is used. The only difference, say with GOAT\u2019s sampling method or other similar sampling methods, is that Algorithm 1 is executed offline, as illustrated in Section 3 (\u201cOffline Step Prior to Training\u201d); while GOAT\u2019s sampling is done during mini batching. In addition, unlike the sampling performed during mini-batching, Algorithm 1 paves the way for parallelizing the sampling of each graph on distributed computing resources.\n\n___\n>**Reviewer:** In Table 2, why does GOAT-local-\u03b4 have better accuracy in ogbn-products? For snap-patents in Table 2, why does LargeGT have much better model accuracy than all baselines?\n\n**Authors:** Thank you for pointing out these key observations from our experiments. On ogbn-products, GOAT-local-\u03b4 shows better performance due to the dataset's homophilic nature, where local-only information aggregation is often sufficient. Conversely, for snap-patents, a non-homophilic dataset, LargeGT excels by incorporating both local and global neighbor information. This adaptability of LargeGT to different dataset characteristics explains the observed performances. We have included our observation to this regard in Section 4.2 under the heading \u201cOn Performance\u201d, which we will revise for more clarity in our next revision.\n\n___"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501853693,
                "cdate": 1700501853693,
                "tmdate": 1700501853693,
                "mdate": 1700501853693,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]