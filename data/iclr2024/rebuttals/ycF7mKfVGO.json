[
    {
        "title": "Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation"
    },
    {
        "review": {
            "id": "owEI3V3KHf",
            "forum": "ycF7mKfVGO",
            "replyto": "ycF7mKfVGO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4964/Reviewer_n49S"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4964/Reviewer_n49S"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new metric for off-policy evaluation, names SharpeRatio@K. This metric measures the risk-return tradeoff and efficiency of policy portfolios formed by an OPE estimator under varying online evaluation budgets (i.e. top-K policies selected by the estimator). Via examples, the paper demonstrates that existing metrics (MSE, regret@1, rank correlation) fail to differentiate policies with different risk-return tradeoffs while the proposed new metric does. The authors evaluated SharpeRatio@K via benchmark experiments using various existing OPE estimators regarding their risk-return tradeoff. The authors also developed open-source software for using the proposed metric."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Overall the paper has a good clarity and is well-organized. I found the examples on over/under-estimation, conservative / high-stakes estimation to be helpful for understanding the benefit of the proposed metric.\n- Given that in practice, OPE is more often used as a screening process for selecting top-K policies to deploy in A/B tests, the risk-to-return ratio can be a useful and meaningful metric for comparing OPE estimators. The proposed metric based on Sharpe Ratio to be natural and easy-to-evaluate.\n- The benchmark experiments / open-source software provide good evidence that SharpeRatio@K is capable of measuring the risk-to-return efficiency of various OPE estimators, and facilitate the usage of such metric in future research."
                },
                "weaknesses": {
                    "value": "The benchmark results show that SharpeRatio@k can sometime diverge significantly from the conventional metrics in terms of estimator selection. Additional discussion should be added on how practitioners may consolidate the insights given by the different metrics evaluated under certain scenarios."
                },
                "questions": {
                    "value": "- The classic sharpe ratio uses the mean return of the portfolio as the numerator instead of the best policy's return. Is there a particular reason that the best return is used in the proposed metric?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698620465765,
            "cdate": 1698620465765,
            "tmdate": 1699636483248,
            "mdate": 1699636483248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XkjwPiMwK5",
                "forum": "ycF7mKfVGO",
                "replyto": "owEI3V3KHf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for valuable and thoughtful feedback. In particular, we appreciate the supportive comments and acknowledgment of our contributions. We will respond to the key comments and questions below.\n\n> **The classic sharpe ratio uses the mean return of the portfolio as the numerator instead of the best policy's return. Is there a particular reason that the best return is used in the proposed metric?**\n\nThis is a great point. We focus on the best policy\u2019s performance in the numerator because only the best policy in the policy portfolio will be finally chosen via A/B testing as a production policy, as illustrated in Figure 1. To define the numerator, we also subtract the behavior policy\u2019s performance ($J(\\pi_b)$). By doing so, we can avoid an undesired edge case where an estimator that chooses only policies that are worse than the behavior policy but accidentally has extremely small std (denominator) is evaluated as the best method under our metric. We will clarify this in Section 3.\n\n\n> **The benchmark results show that SharpeRatio@k can sometime diverge significantly from the conventional metrics in terms of estimator selection. Additional discussion should be added on how practitioners may consolidate the insights given by the different metrics evaluated under certain scenarios.**\n\nThank you for the valuable feedback. The estimator selection results under SharpeRatio@k sometimes diverge from others because SharpeRatio takes the risk into account when evaluating OPE estimators, while other metrics ignore the risk factor. In general, it would be ideal to be able to develop an estimator that performs best in terms of both existing metrics and SharpeRatio. If it is not possible, one should prioritize SharpeRatio@k more when they do not want to take any risk of deploying detrimental policies in the A/B test. When an estimator is preferred by only existing metrics, the estimator is likely to be high-return but also high-risk, so it should be avoided in high-stakes scenarios such as medical applications. We will provide an in-depth discussion about this point in the revision."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699876096319,
                "cdate": 1699876096319,
                "tmdate": 1699876096319,
                "mdate": 1699876096319,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nOe63WnaYg",
            "forum": "ycF7mKfVGO",
            "replyto": "ycF7mKfVGO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4964/Reviewer_qCGC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4964/Reviewer_qCGC"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new Off-line policy evaluation technique that account for the risk-return tradeoff as usually done in the financial literature when evaluating a portfolio. \nNamely the papers proposes a new evaluation metric (SharpeRatio@k) that measure the risk/return tradeoff of OPEs.\nThe paper is complemented by empirical analysis to evaluate the effectiveness of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies the important problem of offline policy evaluation and propose a risk-aware method for selecting OPE estimators. The idea of incorporating the variance of the estimators seems novel and worth investigating. Moreover the paper is fairly well structured and clearly written."
                },
                "weaknesses": {
                    "value": "My biggest concern is about the technical contribution of the work. While the idea seems novel it also sound quite natural and simple and bears concern about the actual interest that would spark in the community. Moreover the introduction of Sharpe ratio like measure in OPE it seems poorly motivated by the authors who should try to sell better the motivations for the idea"
                },
                "questions": {
                    "value": "1) Way is it important to consider the std of the estimators in assessing OPEs? In finance, big swings might prompt the activation of risk measures, and thus one usually prefers loosing some performance points in favour of more stable results. Way is this important in OPE?\n2) Is there any provable theoretical advantages of using your method compared to others?\n3) Seems like $\\hat J(\\pi; D)$ was never defined explicitly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4964/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4964/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4964/Reviewer_qCGC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699046550836,
            "cdate": 1699046550836,
            "tmdate": 1699636483161,
            "mdate": 1699636483161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RNxlAyXnGY",
                "forum": "ycF7mKfVGO",
                "replyto": "nOe63WnaYg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for valuable and thoughtful feedback. We will respond to the key comments and questions below.\n\n> **My biggest concern is about the technical contribution of the work. While the idea seems novel it also sound quite natural and simple and bears concern about the actual interest that would spark in the community.**\n\nThank you for the great comment. First, we think that the simplicity of our evaluation metric is actually one of its important advantages. It means that it can be used fairly easily in future research and practice. Reviewer n49S acknowledges that being natural and easy-to-use is one of the main advantages of our metric. Second, a method\u2019s simplicity does not necessarily mean that it is not novel and valuable. The concept of evaluating OPE estimators\u2019 risk-return tradeoff and the use of SharpeRatio as an evaluator of OPE estimators are the ideas that we have never seen in the relevant literature. Besides, the similar types of contributions have been evaluated highly by the relevant community. We can actually list several published papers that share similar types of contributions to ours, i.e., (1) highlighting the problem of existing evaluation protocols and (2) resolving the issues with simple and easy-to-use metrics.\n\n- **(Fu et al., ICLR2021)** (1) This paper identified that an OPE estimator that has a lower estimation error (MSE) does not always align candidate policies better than those with a higher estimation error. (2) To address this gap in evaluating policy alignment in OPE, the paper proposed simple rank correlation and regret to see if OPE estimators align candidate policies well. These metrics are now recognized as important metrics of OPE and are used in e.g., (Tang and Wiens, 2021), (Qin et al, 2022), and (Chang et al., 2022).\n\n- **(Agarwal et al., NeurIPS2021)** (1) This paper identified that taking the naive average of the performance of online RL can be sensitive to variance, particularly when only a handful of trials are allowed due to high computational costs. (2) To enable a more statistically robust evaluation of online RL in such situations, the paper proposed to use the simple average of the middle 50% of trials. This paper was awarded the *Outstanding Paper Award for NeurIPS 2021*.\n\n- **(Chan et al., ICLR2020)** This paper (1) identified that the naive average of the performance of online RL may overlook a critical safety concern. (2) To see if the RL policy is risk-averse or not, the paper proposed to report the simple average of the worst $\\alpha$% of trials (referred to as conditional value at risk; CVaR). CVaR is widely used in safe-RL. \n\n**It would thus be great if the reviewer could evaluate our contributions based on their quality, significance, soundness, impact, rather than what the proposed method looks like.** It would also be great if the reviewer could pay attention to another key contribution we have provided, i.e., the development of the open-source software. \n\n====\n\n(Fu et al., ICLR2021) Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, Tom Le Paine. \u201cBenchmarks for Deep Off-Policy Evaluation.\u201d \n\n(Agarwal et al., NeurIPS2021) Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare. \u201cDeep Reinforcement Learning at the Edge of the Statistical Precipice.\u201d\n\n(Chan et al., ICLR2020) Stephanie C.Y. Chan, Samuel Fishman, John Canny, Anoop Korattikara, Sergio Guadarrama. \u201cMeasuring the Reliability of Reinforcement Learning Algorithms.\u201d\n\n(Tang and Weins, 2021) Shengpu Tang, Jenna Wiens. \"Model selection for offline reinforcement learning: Practical considerations for healthcare settings.\" Machine Learning for Healthcare Conference, 2021.\n\n(Qin et al., 2022) Rongjun Qin, Songyi Gao, Xingyuan Zhang, Zhen Xu, Shengkai Huang, Zewen Li, Weinan Zhang, and Yang Yu. \u201cNeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning.\u201d NeurIPS Datasets & Benchmarks, 2022.\n\n(Chang et al., 2022) Jonathan D. Chang, Kaiwen Wang, Nathan Kallus, Wen Sun. \u201cNeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning\u201d ICML, 2022."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699875763995,
                "cdate": 1699875763995,
                "tmdate": 1699875763995,
                "mdate": 1699875763995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "anNIgx81Xa",
            "forum": "ycF7mKfVGO",
            "replyto": "ycF7mKfVGO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4964/Reviewer_S1vJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4964/Reviewer_S1vJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the risk-return tradeoff between different OPE evaluations. Existing methods evaluate the superiority of an OPE estimator via various \u201caccuracy\u201d measures. However, the paper argues that merely looking at the accuracy may not be sufficient as two OPEs with similar accuracies may have different risk implications in practice. \n\nTo fix this issue, the paper proposes to use concepts from portfolio evaluation in finance and develops a new metric called SharpeRatio@k. This metric helps distinguish between conservative and high-stakes OPE estimators. The key idea behind this is to regard the set of top-k candidate policies selected by an OPE estimator as its \u201cpolicy portfolio\u201d. The paper constructs a policy portfolio that is \u201cefficient\u201d, i,e, it contains policies that improve the performance of behavior policy without including poorly performing policies. Finally, the paper evaluates typical OPE estimators using the proposed metric using a number of continuous control benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "\u2013 I think the main strength of the paper lies in identifying how the portfolio evaluation concepts in finance can be applied to evaluation of OPEs. I\u2019m not an expert in finance, and can\u2019t speak to the novelty of the idea, but assuming it is novel, this certainly sounds interesting to me.\n\n\u2013 The paper is well-organized, is a pleasure to read and explains difficult concepts well enough. \n\n\u2013 I also really liked the use of toy examples throughout the paper to drive home the key concepts. \n\n\u2013 Experiments section is thorough; it compares several of commonly used OPE estimators and also identifies several directions for future OPE research."
                },
                "weaknesses": {
                    "value": "\u2013 I felt the figures and toy examples could use more explanation. For example, it wasn\u2019t clear in Fig 2 what the red dots are, black dots are and how to interpret the axes. \n\n\u2013 Related work: In the section on \u201cRisk Measures and risk-Return Tradeoff in Statistics and Finance\u201d, the paper discusses the Sharpe,1998 paper in length (relevant for the Sharpe ratio used in current paper). However, there are only two other related works identified in this entire section. I\u2019m not an expert in this domain but I suspect there has to be more prior research done in this space \u2013 and if that is true, the existing related work section seems a little thin. \n\n\u2013 Other minor suggestions: \n\n1.  In the abstract, the sentence \u201cWe first demonstrate, \u2026\u201d is a bit too long and unwieldy and hard to understand easily. \n\n2.  Contributions paragraph talks about \u201ctop-k policy deployment\u201d without any prior context on what that means. Difficult to follow."
                },
                "questions": {
                    "value": "\u2013 In description of Fig 2, I found the sentence \u201cX underestimates the performance\u2026 Y overestimates \u201d confusing \u2013 I was wondering if it should be the other way around. If the current sentence has to be true, then my understanding of the interpretation is that black policies suffer an underestiation (i.e. even though x-axis value is high, y-axis value is lower than ground truth) and so the top-k ends up picking next best policies. Is this correct or am I missing something? \n\n\u2013 top-k policies are considered in several resource allocation setups where there are limited resources and some index is computed so that resource can be allocated to the top k indexes \n(For e.g. [1] \u201cRestless Bandits: Activity Allocation in Changing World\u201d, P Whittle; [2] https://arxiv.org/abs/2110.02128). Can the method be using for designing robust top-k selection/resource allocation policies in this context? I also wonder how it compares to existing robustness in bandits work such as [3] https://arxiv.org/abs/2107.01689 and whether those ideas can be applied to the OPE evaluation setting discussed in the current paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699113132250,
            "cdate": 1699113132250,
            "tmdate": 1699636483078,
            "mdate": 1699636483078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2du8CfMCUF",
                "forum": "ycF7mKfVGO",
                "replyto": "anNIgx81Xa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for valuable and thoughtful feedback. We will improve the presentation of the paper following the reviewer's suggestions. We will respond to the key comments and questions below.\n\n> **it wasn\u2019t clear in Fig 2 what the red dots are, black dots are and how to interpret the axes.**\n\n> **I found the sentence \u201cX underestimates the performance\u2026 Y overestimates \u201d confusing \u2013 I was wondering if it should be the other way around. If the current sentence has to be true, then my understanding of the interpretation is that black policies suffer an underestiation (i.e. even though x-axis value is high, y-axis value is lower than ground truth) and so the top-k ends up picking next best policies. Is this correct or am I missing something?**\n\nThank you for catching the ambiguity. The red dots indicate the policies whose values are estimated correctly by the estimator, while the black dots indicate the policies whose values are not correctly estimated.\n\nYour understanding about estimator X (\u201cmy understanding of .. next best policies\u201d) is correct indeed, and the three points in the shaded region are chosen as the top-$k$ policies under estimator X (these three policies have higher estimated policy value (y-axis) than others).\n\nOn the other hand, estimator Y substantially overestimates the value of black policies. As a result, one red policy (the rightmost one) and two black policies are chosen as the top-$k$ policies (policy portfolio) under this estimator, which is considered risky because the true values of these black policies are quite low. \n\n\n> **Related work: In .. \u201cRisk Measures and risk-Return Tradeoff in Statistics and Finance\u201d, the paper discusses the Sharpe,1998 paper in length .. However, there are only two other related works .. I\u2019m not an expert in this domain but I suspect there has to be more prior research done in this space \u2013 and if that is true, the existing related work section seems a little thin.**\n\nThank you for the valuable comment. Indeed, there are several variants of risk measures used in finance such as the information ratio, value at risk (VaR), marginal VaR, and entropic VaR (some of them have been used in bandits and RL). We will add comments about these relevant metrics in the section and keep improving the writing of the paper.\n\n\n>  **top-k policies are considered in several resource allocation setups where there are limited resources and some index is computed so that resource can be allocated to the top k indexes .. Can the method be using for designing robust top-k selection/resource allocation policies in this context? I also wonder how it compares to existing robustness in bandits work such as .. and whether those ideas can be applied to the OPE evaluation setting discussed in the current paper.**\n\nWe agree that the high-level concept of the risk-return tradeoff can be applied to the problem of resource allocation and restless bandit as suggested by the reviewer. A notable difference is that we used the concept to define a new metric to evaluate OPE estimators while the same concept can be useful to define a new performance measure of a bandit policy in the field of resource allocation and restless bandit. We can actually find some similar attempts such as the risk-aversion bandit (Sani et al. 2012). Even though this is an independent interest, we thank the reviewer for sharing this interesting thought.\n\n(Sani et al. 2012) Amir Sani, Alessandro Lazaric, and R\u00e9mi Munos. Risk-Aversion in Multi-armed Bandits. NuerIPS2012."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699875387646,
                "cdate": 1699875387646,
                "tmdate": 1699875387646,
                "mdate": 1699875387646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4xa41YpNTR",
            "forum": "ycF7mKfVGO",
            "replyto": "ycF7mKfVGO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4964/Reviewer_wVqE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4964/Reviewer_wVqE"
            ],
            "content": {
                "summary": {
                    "value": "This paper is concerned with defining an appropriate measure for off-policy evaluation. The authors argue that commonly used metrics such as MSE and rank correlation are insufficient because they fail to properly account for risk/return tradeoffs. To remedy this it\u2019s proposed that ideas from portfolio optimization be used. In particular, the authors propose the use of the Sharpe ratio to measure efficiency. A number of empirical results are provided which demonstrate the properties of the Sharpe ratio @ k metric, and compare it to other metrics such as rank correlation and regret."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper tackles an important problem\u2013off policy evaluation is a critical aspect of deployment of RL systems in many real world contexts. The authors' proposal to use ideas from portfolio optimization is an interesting one, and the proposal to use Sharpe @ k is both intuitive and simple. The authors do a nice job of evaluating their work empirically and demonstrating the properties of the proposal."
                },
                "weaknesses": {
                    "value": "My biggest issue with this paper is that the work is very limited in scope which limits the benefit to the larger community. While the proposal to use ideas from portfolio theory is interesting, the authors focus on a fairly simple definition and don't describe the properties and behavior of the proposed approach theoretically. It would be useful if the authors described the proposal in slightly more generality. For example, are there rank-based analogs of the current approach? It would also be useful if there was a full discussion of the necessary assumptions/conditions for Sharpe@k to be applicable in a real world setting. It would also be useful if the authors highlighted cases where current metrics could be preferable. In general, it would seem that the appropriateness of a given evaluation metric is entirely task dependent, a discussion of this could be useful."
                },
                "questions": {
                    "value": "It would be useful if the authors could give a description of the settings in which one would prefer Sharpe@k generally. Also, would be useful if the authors could characterize the necessary assumptions on the reward distributions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699476400753,
            "cdate": 1699476400753,
            "tmdate": 1699636482987,
            "mdate": 1699636482987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GKQJpmdD5B",
                "forum": "ycF7mKfVGO",
                "replyto": "4xa41YpNTR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for valuable and thoughtful feedback. We will respond to the key comments and questions below.\n\n> **My biggest issue with this paper is that the work is very limited in scope which limits the benefit to the larger community.**\n\nThank you for the great point. First, **off-policy evaluation (OPE) of bandits and RL is an established and broad research area and is widely recognized at top-tier conferences like ICLR, ICML, and NeurIPS** (more than 15 relevant papers were accepted to these venues in each of 2021-2023). Second, our work highlighted the issue of the existing evaluation protocol of OPE estimators and proposed a new framework to evaluate their risk-return tradeoff. In Section 4, we have indeed demonstrated that estimators that were considered pretty basic can be the most effective method under our metric. This would have a potential to change the way we evaluate the effectiveness of estimators and develop new estimators in the future of the entire OPE research community as we discussed in Section 5. It would also be possible that our concept of evaluating the risk-return tradeoff will lead to a development of new evaluation metrics for other fields such as fair machine learning, recommender systems, and treatment effect estimation. Thus, we believe that our work is not limited in scope but provides a whole new perspective regarding the evaluation of OPE and potentially many other fields. We will add this discussion in the revision. \n\n\n> **While the proposal to use ideas from portfolio theory is interesting, the authors focus on a fairly simple definition**\n\nThank you for the great comment. First, the simplicity of our evaluation metric is actually one of its important advantages. It means that it can be used fairly easily in future research and practice. Reviewer n49S also acknowledges that being natural and easy-to-use is one of the main advantages of our metric. Second, a method\u2019s simplicity does not necessarily mean that it is not novel and valuable. The concept of evaluating OPE estimators\u2019 risk-return tradeoff and the use of SharpeRatio as an evaluator of OPE estimators are the ideas that we have never seen in the relevant literature. Besides, the similar types of contributions have been evaluated highly by the relevant community. For example, (Fu et al., ICLR2021) proposed the rank correlation and regret as simple evaluation metrics for OPE. (Agarwal et al., NeurIPS2021) proposed the average of the middle 50% of trials as the online RL performance. The values of these papers have been recognized widely because they highlight the issues of existing evaluation protocols and develop a simple metric to deal with them. So is our work, and we believe that the reviewer\u2019s comment actually highlights one of the most important pros of our metric. **It would thus be great if the reviewer could evaluate our contributions based on their quality, significance, soundness, impact, rather than what the proposed method looks like.**\n\n===\n\n(Fu et al., ICLR2021) Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, Tom Le Paine. \u201cBenchmarks for Deep Off-Policy Evaluation.\u201d ICLR, 2021.\n\n(Agarwal et al., NeurIPS2021) Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare. \u201cDeep Reinforcement Learning at the Edge of the Statistical Precipice.\u201d NeurIPS, 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699874929297,
                "cdate": 1699874929297,
                "tmdate": 1699874929297,
                "mdate": 1699874929297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]