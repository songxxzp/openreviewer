[
    {
        "title": "Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning"
    },
    {
        "review": {
            "id": "IMUlQohMBm",
            "forum": "zSxpnKh1yS",
            "replyto": "zSxpnKh1yS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2686/Reviewer_LRDL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2686/Reviewer_LRDL"
            ],
            "content": {
                "summary": {
                    "value": "This work targets on the Information Geometry in unsupervised Reinforcement Learning, especially Mutual Information Skill Learning (MISL). On the basis of previous work, this work first considers the diversity and separability of learned skills. As MISL can not guarantee these properties, this work proposes LSEPIN to measure the disentanglement, and then shows the connection between LSEPIN and downstream task adaptation cost. Moreover, this work investigates the information geometry of Wasserstein distance based skill learning methods. Finally, this work proposes PWSEP and theoretically shows that it can discover all optimal initial policies."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Definitely an important problem to be tackling! Previous work[1] has established the connection between skill-based unsupervised RL and information geometry. [1] has shown that KL-based metric can only find skills with the ''largest radius'' and how to find all vertices of the skill polytope is still an open challenge. This work has elegantly solved this challenge and I believe it will be of interest to researchers of Unsupervised RL.\n\n- The description of the article is very clear, and the introduction to related work is very specific.\n\n- I have basically read all the proofs of the theorems, which are well-written.\n\nThanks to the authors for putting in the effort in doing this work!\n\n[1] Eysenbach, Benjamin, Ruslan Salakhutdinov, and Sergey Levine. \"The information geometry of unsupervised reinforcement learning.\" arXiv preprint arXiv:2110.02719 (2021)."
                },
                "weaknesses": {
                    "value": "Overall, I think this paper is well written and its contribution is solid. I do not find clear weaknesses but I still have some questions (see Questions). I will adjust my score accordingly based on the author's response and other reviewers' comments."
                },
                "questions": {
                    "value": "- This paper claims that \"it is possible for WSEP to discover all vertices of the feasible polytope\". As the theoretical results can only show that all learned skills satisfy $p(z) > 0$ rather than WSEP can learn all $p(z) > 0$ skills, there is no direct evidence to suggest that WSEP can indeed find more skills than MISL (i.e., skills without maximum \u201cdistances\u201d). Can authors show that WSEP can find more skills or even all skills? I think empirical results or theoretical results even in simple cases will be really helpful.\n\n- All experiments are provided in the Appendix. It seems that providing the main experiments and analyses in the main text can help the readers to better understand this work.\n\n- What will happen if we change the W-distance to other distance metrics in PWSEP(i)? In my opinion, it seems that the proof of Thm 3.5 holds for any distance metric (owns Non-negativity, Symmetry, and Triangular Inequalities). Is it right? Or are there some special properties of W-distance necessary for proving Thm 3.5?\n\n- It's better to provide some proof sketch of theorems in the main text, like Thm 3.5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2686/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2686/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2686/Reviewer_LRDL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2686/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697793164999,
            "cdate": 1697793164999,
            "tmdate": 1699636209806,
            "mdate": 1699636209806,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "17XQ4SjY4s",
                "forum": "zSxpnKh1yS",
                "replyto": "IMUlQohMBm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "First response"
                    },
                    "comment": {
                        "value": "We sincerely thank you for your thoughtful review and recognition of our work. We are delighted to address your questions. Please see our response below:\n\n## **Q1:** About whether \"WSEP can find more skills or even all skills\"\n\n**A1:** We consider that, by 'skills' you are referring to those at the vertices. \n\nBy *\"it is possible for WSEP to discover all vertices of the feasible polytope\"* we mean that WSEP is not restricted by maximum \"distances\", so it might discover the vertices that can not be discovered by MISL in certain cases.  For example, in the situation shown by the figure from [this anonymous link](https://anonymous.4open.science/r/iclrrebfig-4B58), MISL can only learn $z_1,z_2$. Even when the skill number is set to be $|Z|=3$, the third skill learned by MISL will be duplicated at $z_1$ or $z_2$ but not lie at $z_3$. Instead, WSEP can discover all 3 vertices in this case.\n\nHowever, as mentioned in Section 3.3.4 and appendix F.6, unlike PWSEP, optimizing WSEP is not guaranteed to **always** discover all vertices. Among the vertices that can not be discovered by WSEP, there can also be ones with maximum \"distance\" that can be discovered by MISL. This motivates us to look into PWSEP, which is guaranteed to always discover all vertices with enough skills. \n\n\n## Q3: What will happen if we change the W-distance to other distance metrics in PWSEP(i)?\n\nThm 3.5 holds for any distance metric that owns \"Non-negativity, Symmetry, and Triangular Inequalities\" and strict convexity. \n\nWe briefly discussed this topic in appendix F.4 after the proof of theorem 3.5. Although total variation distance and Hellinger distance are also true distance metrics, they're less used in RL compared to KL divergence and Wasserstein distance, resulting in limited research on their efficient approximations for state distributions. In addition, we haven't found valid literature to support the strict convexity of total variation distance and Heilinger distance. Additionally, Wasserstein distance has the potential to take advantage of the choice of transportation cost to provide smooth and stable measurements with meaningful information as discussed in appendix G.4 \n\n## Q2&Q4: About experimental results in the main text and proof sketch\n\nThank you for your valuable suggestion on enhancing our paper's presentation. We will certainly take your feedback into account.\n\n---\nThanks again for your time and attention!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086334945,
                "cdate": 1700086334945,
                "tmdate": 1700086334945,
                "mdate": 1700086334945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1sImaO0mUK",
                "forum": "zSxpnKh1yS",
                "replyto": "17XQ4SjY4s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_LRDL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_LRDL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply"
                    },
                    "comment": {
                        "value": "I have read your reply and my Q1 still holds. I fully understand that MISL can only find skills restricted by maximum \"distances\" where WSEP **might** find other skills. My question is: **Is there a situation in which WSEP indeed finds more skills than MISL?** More specifically, can we find an MDP, maybe very simple, where WSEP can find more skills than MISL? The proposed figure in the rebuttal is not a strict example, right?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107348531,
                "cdate": 1700107348531,
                "tmdate": 1700107348531,
                "mdate": 1700107348531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "I40MoO1bg9",
                "forum": "zSxpnKh1yS",
                "replyto": "IMUlQohMBm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Second response"
                    },
                    "comment": {
                        "value": "Thanks for the fast reply! \n\nWe hope the [figure](https://anonymous.4open.science/r/iclrrebfig-4B58) provides the intuition that WSEP can learn the vertices that are not with maximum \"distances\".\n\nHere is a simple MDP that produces the feasible state distribution polytope in the [figure](https://anonymous.4open.science/r/iclrrebfig-4B58), and the quantitative results below shows why in this case, WSEP learns more vertices than MISL.\n\n---\n## An MDP example that produces the polytope in the figure\n\nAn MDP with $|\\mathcal{S}|=3$ for states and $|\\mathcal{A}|=3$ for actions.\nThe transition matrix for action $a_1$ is:\n\n$\\begin{pmatrix}\n0.2 & 0.7 & 0.1\\\\\\\\\n0.2 & 0.7 & 0.1\\\\\\\\\n0.2 & 0.7 & 0.1\n\\end{pmatrix}$\n\nThe transition matrix for action $a_2$ is:\n\n$\\begin{pmatrix}\n0.2& 0.1& 0.7\\\\\\\\\n0.2& 0.1& 0.7 \\\\\\\\\n0.2& 0.1& 0.7\n\\end{pmatrix}$\n\nThe transition matrix for action $a_3$ is:\n\n$\\begin{pmatrix}\n0.4 & 0.3 & 0.3\\\\\\\\\n0.4 & 0.3 & 0.3 \\\\\\\\\n0.4 & 0.3 & 0.3\n\\end{pmatrix}$\n\nBecause the transition probabilities only depend on actions, state distribution $p(S)$ is determined by distribution $p(A)$:\n$p(S) = p(a_1)[0.2,0.7,0.1]+p(a_2)[0.2,0.1,0.7]+p(a_3)[0.4,0.3,0.3]$\n\nWe can see $p(A)$, in this case, is the convex coefficient of three probabilities, so the feasible state distribution is in the convex polytope $\\mathcal{C}$ with these three probabilities being the vertices of $\\mathcal{C}$.\n\nThis MDP accords with the figure, where we have labeled the vertices with $z_1,z_2,z_3$,\n\n$p(S|z_1)= [0.2, 0.7, 0.1]$\n\n$p(S|z_2)= [0.2, 0.1, 0.7]$\n\n$p(S|z_1)= [0.4, 0.3, 0.3]$\n\n\n\n## Quantitative comparision of I(S;Z) and WSEP\n\n>### MISL only learns skills at $z_1,z_2$:\n\nFor this MDP, the unique center of the \"circle\" with \"maximum radius\" (We showed that it's uniquely determined by the MDP in appendix D) would be $[0.2,0.4,0.4]$.\n\nMISL would learn only $z_1,z_2$ with $p(z_1)=p(z_2)=0.5,p(z_3)=0$ to have the average state distribution $p(S)=\\sum_z(S|z)$ at the unique center $[0.2,0.4,0.4]$.\n\nBy this solution, $I(S;Z)$ is maximized by $I(S;Z)=\\sum_z p(z)D_{KL}(p(S|z)\\|p(S))\\approx 0.253$.\n\nPutting weight on the other vertex $z_3$ would lower $I(S; Z)$. For example, if $p(z_1)=p(z_2)=0.45,p(z_3)=0.1$, the average $p(S)$ would be $[0.22, 0.39, 0.39]$ which is not the center of the maximum \"circle\" any more, and their $I(S; Z)\\approx 0.237$.\n\n>### WSEP can learns skills at all vertices $z_1,z_2,z_3$:\n\nFor WSEP, when the skill number is set to $|Z|=3$, when two skills are at $z_1,z_2$ like MISL, putting the last skill at $z_1$ or $z_2$ would have lower WSEP than putting the last skill at $z_3$, due to the triangle inequality.\n\nSuppose the transportation cost between every two states is 1, then\n\n$W(p(S|z_1),p(S|z_2))=0.6$\n\n$W(p(S|z_1),p(S|z_3))=W(p(S|z_2),p(S|z_3))=0.4$\n\nWSEP for $2$ skills at $z_1$ and $1$ skill at $z_2$ would be $2.4$, \n\nWSEP for $2$ skills at $z_2$ and $1$ skill at $z_1$ would also be $2.4$. \n\nWSEP for $3$ skills at $z_1,z_2,z_3$ respectively would be $2.8$, and this is the solution for maximizing WSEP for this case (By convexity, moving any of the three vertices would lower its distance to the others).\n\nTherefore, WSEP would favor learning all $3$ vertices instead of only $2$ skills at $z_1,z_2$ like MISL. In this case, MISL can not discover $z_3$ that is not with maximum \"distances\" but WSEP can.\n\n---\nThanks again and we hope this addressed the question!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136679327,
                "cdate": 1700136679327,
                "tmdate": 1700138725023,
                "mdate": 1700138725023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qxC8EZLRth",
                "forum": "zSxpnKh1yS",
                "replyto": "I40MoO1bg9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_LRDL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_LRDL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply"
                    },
                    "comment": {
                        "value": "I have roughly checked the example and believe it is reasonable. Also, I believe that adding a more detailed version of this example (for example, it is better to strictly show that [0.2, 0.4, 0.4] is exactly the center of the \"circle\") to the paper will make the paper more solid. I would like to keep my score and believe that this work will be a valuable contribution to the community."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700195768667,
                "cdate": 1700195768667,
                "tmdate": 1700195768667,
                "mdate": 1700195768667,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HT5XuqM0Bv",
            "forum": "zSxpnKh1yS",
            "replyto": "zSxpnKh1yS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2686/Reviewer_j7aV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2686/Reviewer_j7aV"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the geometry of state distributions learned with mutual information skill learning for the purpose of theoretical task adaptation analysis. The authors propose Least SEParability and INformativeness (LSEPIN) to measure the diversity and separability and show a relationship with worst-case adaptation cost (WAC). The authors theoretically prove the relationship between the optimization of LSEPIN and Similarly, the authors also propose Wasserstein based distance metric WSEP which is more suitable in symmetric polytope to measure the separability of skill. Similar to LSEPIN and WAC, the relationship between WSEP and Mean Adaptation Cost is studied."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The geometry perspective of task adaptation is interesting.\n* Studies on the geometry is promising and can motivate readers.\n* The theoretical results are well justified."
                },
                "weaknesses": {
                    "value": "* Some Definitions of words are not defined well (diversity and separability).\n* The flow of the paper is weakly organized and hurts readability (the first topic is LSEPIN and the second WSEP.) The paper have no discuss on other perspectives.\n* The findings from the theoretical derivation are not surprising. WAC and adaptation cost.\n* Most contents have high dependency with appendix. Although the authors studied various components, they are not well organized."
                },
                "questions": {
                    "value": "* Here are questions that we want to discuss with the authors.\n* We might incorrectly understand the contribution of this work. Why the theoretical derivation of Theorem 3.1 is important for a task adaptation?\n    * What can we additionally learn from the theorem 3.1 or how can we use the theorem for other task adaptation works? For my understanding, the relationship: increasing LSEPIN results in lower WAC.\n    * Why measuring $\\min_z I(S;z)$ can be used to measure diversity and separability? I guess two features should be computed among skills, but the $\\min_z I(S;z)$ is just a mutual information for a single code.\n    * In LSEPIN, the metric measures the least skill code. I guess an abundant skill code may hurt the calculation. Isn't it problematic? How could you ensure that all the codes are meaningful in the computation of LSEPIN?\n    * WAC is measured with the worst state distribution. how this could be meaningful and practical? That is, why we need to measure the worst case adaptation? To the best of my knowledge,  skill adaptation is applied to the state distributions which are similar to the state distribution for the target skill. Therefore, the importance of $\\max_p$ part in WAC is not persuasive.\n    * Could you please additionally describe the necessity of symmetry and triangle inequality?\n    * What is the limitation of WSEP?\n    * Could this study can be combined with in-distribution and out-distribution perspective of task adaptations?\n\n[Overall]\nAlthough the authors conducted several theoretical derivations and analysis, the choice of metrics and the relationship between them have little meaning. Mostly because the task adaptation is might assume close state distribution $p$ for a skill $p(s|z)$. However, the authors study the worst-case adaptation. Additionally, the organization of the paper is not well constructed and hard to follow.*"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2686/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2686/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2686/Reviewer_j7aV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2686/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698634443466,
            "cdate": 1698634443466,
            "tmdate": 1700638601334,
            "mdate": 1700638601334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KVSKvmUHfQ",
                "forum": "zSxpnKh1yS",
                "replyto": "HT5XuqM0Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "First response (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your valuable feedback and your willingness to engage in discussion. We would like to address your concerns and answer your questions. Please see the following for our response.\n\n---\n## 1. Regarding the concerns related to the \"WAC\" cost and Theorem 3.1\nAbout:\n>- *[Overall] Although the authors conducted several .. the authors study the worst-case adaptation.*\n>- *WAC is measured with the worst  ... the importance of $max_p$ part in WAC is not persuasive.*\n>- *What can we .. For my understanding, the relationship: increasing LSEPIN results in lower WAC.*\n>- The findings from the theoretical derivation are not surprising. WAC and adaptation cost.*\n\n### 1.1 Clarification of the definition of WAC \nTo address these concerns, we would like to first clarify the definition of the WAC cost and mitigate potential misunderstandings. The WAC is defined as:\n$$\\text{WAC} = \\max_r \\min_{z\\in \\mathcal{Z}^*}D_{KL}({p(S|z)}\\|{p^r})$$\n$p^r$ is the optimal state distribution for downstream task $r$, and $\\mathcal{Z}^*$ is the set of learned skills. \n\nWAC is indeed considering adaptation from **closest** skills in the set of learned skills because of $\\min_{z\\in \\mathcal{Z}^*}D_{KL}({p(S|z)}\\|{p^r})$. The $\\max_r$ in WAC means to choose a downstream task $r$ such that its optimal state distribution $p^r$ is the far from all learned skills, even for the skill $z^*=argmin_{z\\in \\mathcal{Z}^*}D_{KL}({p(S|z)}\\|{p^r})$ that is **closest** to $p^r$. \n\n\n### 1.2 Practicality and meaningness\nIn practice, although we can adapt from the closest skill after knowing the downstream task $r$, we do not have prior knowledge about $r$ during unsupervised pre-training. Therefore, we can only prepare for the worst downstream task (the one far from even the closest learned skill). \n\nTheorem 3.1 and the theoretical results in Section 3.2 shed light on what kind of skills are favored for downstream task adaptation and how to quantitatively measure them. \n\n### 1.3 About the contribution of our findings\n\nFollowing the above clarification of the WAC definition, we hope it is clear now that our analysis is dealing with the fundamental question for URL: \"How the learned skills can be used for downstream task adaptation, and what properties of the learned skills are desired for better downstream task adaption?\"\n\n> The findings ... not surprising.\n\nThe findings may appear \"not surprising\", given that promoting diversity and separability of learned skills has been an intuitive heuristic in prior practical algorithms [2][3]. However, our unique contribution lies in offering a theoretical justification for this heuristic.\n\n---\n## 2. Regarding concerns related to \"diversity and separability\"\nAbout:\n>- *Why measuring $min_z I(S;z)$ can be used to measure diversity and separability? ..., but the $min_z I(S;z)$ is just a mutual information for a single code.*\n>- *Some Definitions of words are not defined well (diversity and separability).*\n\n\nFirst of all, LSEPIN is defined as $\\min_z I(S;\\mathbf{1}_{z})$, its mean is not $I(S,Z)$ (details in \"formal difference\" of appendix B.2).\n\nAs we have mentioned at the beginning of Section 3:\n*\"Separability means the discriminability between states inferred by different skills.\"* \nIn inequality (61) of appendix E, we show that increasing $I(S;\\mathbf{1}_{z})$ promotes the KL divergence between $p(S|z)$ and $p(S|Z\\neq z)$(average state distribution of non-$z$ skills). Also discussed in appendix E, higher KL divergence between $p(S|z)$ and $p(S|Z\\neq z)$ means skill $z$ is more distinctive and has less overlap with other skills, so this means better separability.\n\nFor a set of skills, they are diverse because all skills in the set are distinctive and have little overlap with each other (they are \"far\" from each other in terms of KL divergence). The standard MISL objective $I(S;Z)$ can not guarantee the seperability of each skill, as discussed in Section 3 after the list of informal results, so MISL without LSEPIN can not guarantee diversity.\n\n---\n## 3. Regarding why focus on single skill code \n>In LSEPIN, the metric measures the least skill code. ... How could you ensure that all the codes are meaningful in the computation of LSEPIN?\n\nIt is important to ensure every learned skill is distinctive and separable from others. High $I(S;\\mathbf{1}_{z})$ means that this skill covers a specific region of the state space that is less covered by other skills. If it is not separable from other skills, its state coverage could have a huge overlap with other states, so deleting this skill would make no difference for exploration or downstream task adaptation.\n\n\nIn [1], they implement many existing algorithms with a low number of skills, eg. only 4 skills for SMM. If one of its skills exhibits a low $I(S;\\mathbf{1}_{z})$, it may have a huge overlap with other skills, leading to a situation where 25% of the skills are underutilized, offering no meaningful contribution to exploration and downstream task adaptation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088386795,
                "cdate": 1700088386795,
                "tmdate": 1700088716428,
                "mdate": 1700088716428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IGI8BRWxaw",
                "forum": "zSxpnKh1yS",
                "replyto": "HT5XuqM0Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for your valuable feedback on our paper. Our rebuttal addresses the concerns, especially the ones related to the WAC cost definition. By WAC, we actually consider the practical adaptation procedure you mentioned, which is to adapt from the 'closest' skill. We hope this clarifies our main contribution. \n\nWe wonder whether you have any remaining concerns, we are looking forward to addressing any additional concerns you may have."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414112554,
                "cdate": 1700414112554,
                "tmdate": 1700414148945,
                "mdate": 1700414148945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3h8AEQWyZn",
                "forum": "zSxpnKh1yS",
                "replyto": "IGI8BRWxaw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_j7aV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_j7aV"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed explanations on specific questions.\n1. Regarding the concerns related to the \"WAC\" cost and Theorem 3.1\nThank you for clarifying the definition. The definition based on the unknown optimal distribution is persuasive.\n2. Thank you for clarifying the definition of LSEPIN and separability. Please refer B.2 in the paper. The definition of $I(S,1_z)$ helps me understanding the definition of separability.\n3. Thank you additional explanation on the meaning of separable skill learning.\n4. I understand the impact of the lack of triangle inequality. Thank you for the explanation.\nI appreciate your effort to provide additional comments on the questions, especially in sections 5.6 and 5.7. Thank you\n----\nHere are additional comments to clarify the problem tackled in this work.\nTwo terms **\"diversity\" and \"separability\"** are used jointly in this work. In my understanding diversity is about $I(S;Z)$ [1] which measures the coverage of skills, while the separability is defined with $D_{KL}(P(S|Z=z)||P(S|Z\\ne z))$.\nTo the best of my understanding, the main contribution is on separability. Is the contribution of this work is on both properties?\n(related to question 6, organization of the paper)\nI also checked the reviews from other reviewers.  I also agree that the inclusion of the main experimental results to improve readability.\n[1] Eysenbach, Benjamin, et al. \"Diversity is All You Need: Learning Skills without a Reward Function.\" International Conference on Learning Representations. 2018."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700575136239,
                "cdate": 1700575136239,
                "tmdate": 1700575136239,
                "mdate": 1700575136239,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2HHk0fCcdx",
                "forum": "zSxpnKh1yS",
                "replyto": "HT5XuqM0Bv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_j7aV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_j7aV"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors, \n\nThank you for the kind response. \n\nAlthough the paper still exhibits weaknesses in its experimental support and the organization of the flow, the responses provided have clarified the major concerns. Therefore, I would raise my score to 'weak accept (6).\n\nSincerely,\n\nReviewer j7aV"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638543631,
                "cdate": 1700638543631,
                "tmdate": 1700638584187,
                "mdate": 1700638584187,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gNzZtHOrvt",
            "forum": "zSxpnKh1yS",
            "replyto": "zSxpnKh1yS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2686/Reviewer_ZXDZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2686/Reviewer_ZXDZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a theoretical analysis of learning skills using unsupervised reinforcement learning (URL), which serves as an initialization for learning a policy for a downstream task. The paper shows that Mutual Information Skill Learning (MISL) does not guarantee diversity and separability of learned skills, and proposes to replace the uniform distribution of skills objective with the Least SEParability and INformativeness (LSEPIN) metric to promote informativeness and separability. Moreover, the paper proposes to replace the KL divergence in MISL with Wasserstein distance that exploits better geometric properties. Finally, it proposes another Wasserstein distance-based algorithm (PWSEP) that can theoretically discover all optimal initial policies.\n\nThe authors show theoretically that LSEPIN bounds the Worst-case Adaptation Cost (WAC) and show that the Wasserstein distance has better geometrical properties (such as symmetry and triangle inequality) that leads to better skill separation. In addition, the authors provide experiments to validate the proposed theoretical results in the appendix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper investigates an important topic and provides a rigorous analysis of the proposed ideas. In addition, it proposes a practical algorithm that was tested empirically and demonstrates superior results compared to existing MISL methods."
                },
                "weaknesses": {
                    "value": "The paper is hard to read and follow, with lots of details.\n\nSince most of the contribution of the paper is placed in the appendix, including all experimental results, it is hard to understand and assess its contribution without reading carefully the appendix."
                },
                "questions": {
                    "value": "I would like to ask the following questions:\n\n1. Is it possible to rigorously prove that adding a loss that promotes uniform p(Z_input) to objective (1) does not promote any diversity? Or promotes less diversity than the LSEPIN loss in all cases?\n\n2. Is there a measure for adaptation other than Worst-case Adaptation Cost?\n\n3. Is there any advantage to the KL divergence over the Wasserstein distance? theoretically or computationally?\n\n4. What are the limitations of the PWSEP algorithms (specifically the WSEP objective)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2686/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2686/Reviewer_ZXDZ",
                        "ICLR.cc/2024/Conference/Submission2686/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2686/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841649385,
            "cdate": 1698841649385,
            "tmdate": 1700732870349,
            "mdate": 1700732870349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M0TgLvq3bk",
                "forum": "zSxpnKh1yS",
                "replyto": "gNzZtHOrvt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "First response (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate your time and attention. We thank you for the review and comments and please see our response to your questions below.\n\n\n## **Q1:** Is it possible to rigorously prove that adding a loss that promotes uniform $p(Z_{input})$ to objective (1) does not promote any diversity? Or promotes less diversity than the LSEPIN loss in all cases?\n\n**A1:** It might be hard to derive a quantitative bound to show adding a loss like $H(Z_{input})$ to objective (1) promotes less than LSEPIN in all cases, because the diversity of state distributions $p(S|z)$ depend on not only $z_{input}$ but also parameter $\\theta$ and the function class for policy $\\pi_\\theta$. However, we can illustrate why such loss is not guaranteed to promote diversity like LSEPIN by the following example where maximizing $I(S; Z)$ with uniform $p(Z_{input})$ results in limited diversity:\n\nFor a case of with $|S|=3$, the feasible polytope $\\mathcal{C}$ allows a maximum \"circle\" centered at $[0.2,0.4,0.4]$ with a maximum \"radius\" of $0.253$ as this fig in [the anonymous link](https://anonymous.4open.science/r/iclrreb2024-62C5/) shows. There are 5 vertices $v_{1:5}$ of $\\mathcal{C}$, of which $v_1, v_2, v_4, v_5$ lie on the maximum \"circle\". Vertex $v_1$ and vertex $v_4$ are $[0.2,0.7,0.1]$ and $[0.2,0.1,0.7]$.\n\nBecause $\\theta$ is shared by all skills and $z=\\\\{\\theta,Z_{input}\\\\}$, we can assume $p(Z)=p(Z_{input})$\n\nWhen there are 4 skills to learn, uniform $p(Z_{input})$ should have $p(z_{input})=p(z)=0.25$ for all $z$. In order to achieve both uniform $p(Z)$ and maximization of $I(S; Z)$, the optimal skill set $\\\\{z_1,z_2,z_3,z_4\\\\}$ should be the one containing two skills $z_1,z_2$ at $v_1$ and the other two skills $z_3,z_4$ at $v_4$, as shown in the [figure](https://anonymous.4open.science/r/iclrreb2024-62C5/reb_fig.png). Only with this skill set, the uniform average of skills $p(S)=\\sum_z p(z)p(S|z)$ with $p(z)=0.25$ could be the center of the maximum \"circle\" $[0.2,0.4,0.4]$.\n\nWe can see from this example that although skill set $\\\\{z_1,z_2,z_3,z_4\\\\}$ maximizes both $I(S; Z)$ and $H(Z_{input})$, there are two pairs of skills being not separable thus resulting in limited diversity.  \n\n\nIn appendix E, we have shown higher $I(S;1_z)$ explicitly increases $D_{KL}(p(S|Z\\neq z) \\| p(S|z))$ thus promoting $p(S|z)$ to be separable from $p(S|Z\\neq z)$ (average state distribution of skills other than $z$). Therefore, LSEPIN promotes diverse skills explicitly. Compared to skills only at $v_1$ and $v_4$, MISL with LSEPIN would favor skills at $v_1, v_2, v_4, v_5$ respectively, and for $p(S)$ to be the center of maximum \"circle\" $[0.2,0.4,0.4]$, distribution $p(Z)$ is not necessarily unform.\n\n\n\n## **Q2:** Is there a measure for adaptation other than Worst-case Adaptation Cost?\n\n**A2:** This question is inspiring and we have been also considering this question recently. One idea is to consider adapting from a convex combination of learned skills instead of from one of the learned skills because a good convex combination of learned skills can be \"closer\" to the optimal state distribution and practically feasible to obtain. \n\nFor example in Figure 2 of our main paper, a convex combination of $p(S|z_1),p(S| z_5)$ could be closer to $p^r$ than $p(S|z_1)$. In practice, the convex combined state distribution $p_\\lambda(S)=\\sum_z \\lambda_z p(S|z)$ can be obtained by sampling $z$ from the distribution $p_\\lambda(z)=\\frac{\\lambda_z}{\\sum_{z'}\\lambda_{z'}}$. \n\nFor the practical adaptation procedure, we can first find the $\\lambda$ resulting in a $p_\\lambda(S)$ with the best accumulated reward. Because we can not directly update the parameters for $p_\\lambda(S)$, we could use a new parametric model to perform offline RL with data collected by $p_\\lambda(S)$ and relabeled by downstream task reward function $r$. Through this approach, we concurrently distill knowledge from pretraining and execute adaptation for the downstream task.\n\nTheoretical analysis of this adaptation procedure can be an idea for future works.\n\n## **Q3:** Is there any advantage to the KL divergence over the Wasserstein distance? theoretically or computationally?\n\n**A3:** Computationally, for non-parametric estimation with collected data, KL divergence may be preferred over Wasserstein distance. This is attributed to the fact that KL divergence can be directly estimated using samples, whereas the estimation of Wasserstein distance requires solving a linear program, introducing additional computational complexity. Theoretically, KL divergence is always strictly convex, while the strict convexity of Wasserstein distance depends on the choice of transportation cost as mentioned in our proofs in appendix F."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085155688,
                "cdate": 1700085155688,
                "tmdate": 1700085786273,
                "mdate": 1700085786273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FrGx4SWXbc",
                "forum": "zSxpnKh1yS",
                "replyto": "gNzZtHOrvt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "First response (2/2)"
                    },
                    "comment": {
                        "value": "## **Q4:** What are the limitations of the PWSEP algorithms (specifically the WSEP objective)?\n\n**A4:** As mentioned in Section 3.3.4, Although lemma 3.3 shows that maximizing WSEP also discovers vertices, the discovered vertices could be duplicated (shown in appendix F.6), so it might not be able to discover all $|V|$ vertices with only $|V|$ skills. This motivates us to propose PWSEP algorithm that iteratively optimizes a projected Wasserstein distance to make sure every new iteration learns a new skill.\n\nAnother limitation of WSEP is discussed in appendix F.5, showing that although a higher WSEP lowers an upper bound of the adaptation cost as our theoretical analysis shows, because of the gap between the upper bound and actual adaptation cost, sometimes high WSEP could not result in lower adaptation cost. \n\nAs for PWSEP, despite its favorable theoretical property of discovering all vertices, in practice, each iteration could only learn a locally optimal skill, as mentioned in the empirical results of appendix H.\n\n---\nThanks again for the constructive feedback!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700085280784,
                "cdate": 1700085280784,
                "tmdate": 1700086817224,
                "mdate": 1700086817224,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i35d4vN72g",
                "forum": "zSxpnKh1yS",
                "replyto": "FrGx4SWXbc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_ZXDZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_ZXDZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your dedication in answering my questions.\n\n1. I appreciate your effort to provide this illustrative example. It clarifies the difference between promoting uniform distribution $p(Z_{input})$ of skills to promoting diversity.\nIn my opinion, including this example in the paper/appendix would be valuable for the reader to clarify this point.\n\n2. The idea of performing adaptation from a convex combination of learned skills instead of an adaptation from one of the learned skills sounds promising and has the potential to work better in practice. A theoretical analysis of this idea will most probably lead to the same conclusion as with adapting from one of the learned skills.\n\n3. If I understand correctly, since the KL divergence is always strictly convex, for complex RL environments the KL divergence could be more computationally practical - whereas the Wasserstein distance will provide an optimal solution, but with additional computational effort. Am I correct?   \n\n4. Could you elaborate on the impact of PWSEP learning only a **locally** optimal skill in each iteration? In your answer, please relate to the empirical results (section H) and in general.\n\nI have another small question about a detail that I probably missed while reading the paper - Why is your approach free from the \u201cnon-concyclic\u201d assumption, while the previous work of Eysenbach et al. (2022) takes this assumption into account? \n(The assumption that limits the number of vertices on the same \u201ccircle\u201d to be |S|)\n\nIn general, I think that the paper provides a worthy contribution to the community and should be accepted. The authors answered most of my concerns and I\u2019m willing to increase my score.\n\nThat being said, the readability of the paper can be improved. I think that the paper would benefit from a rigorous definition of separability and diversity of skills at the beginning, accompanied by a few sentences dedicated to motivation and examples (see point #1). \nIn addition, including the important empirical results in the main paper will motivate the applied RL community to make use of and build upon the proposed algorithms. \n\nThanks again for your detailed answers."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599452719,
                "cdate": 1700599452719,
                "tmdate": 1700599452719,
                "mdate": 1700599452719,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xQtsJHRLk9",
                "forum": "zSxpnKh1yS",
                "replyto": "FSGwyV4fBh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_ZXDZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Reviewer_ZXDZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed response!\n\nIt helps me very much to understand the details in the paper further. I raised my score to 8."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732833654,
                "cdate": 1700732833654,
                "tmdate": 1700732833654,
                "mdate": 1700732833654,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8JzhCCMTSC",
            "forum": "zSxpnKh1yS",
            "replyto": "zSxpnKh1yS",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2686/Reviewer_xY4U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2686/Reviewer_xY4U"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes unsupervised skill-learning through a rigorous mathematical lens, focusing on the properties of the learned skills and their usefulness for downstream tasks. Most existing works use mutual information as the skill-learning objective and Eysenbach et al. (2022) provides a mathematical analysis of the same. This work analyzes the mutual information-based skill learning paradigm with a focus on downstream task adaptability via worst-case adaptation cost. This work also introduces a complementary metric called LSEPIN to measure diversity of learned skills. The authors show that maximizing MI or LSEPIN is essentially optimizing the KL divergence between state distributions. As an alternative, they suggest using the Wasserstein metric owing to it being a proper metric, and propose a new skill learning objectives built upon Wasserstein distance.\n\n**References:**\n\nBenjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. The information geometry\nof unsupervised reinforcement learning. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. [OpenReview.net](http://openreview.net/), 2022. URL\nhttps://openreview.net/forum?id=3wU2UX0voE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "There is a long line of work on unsupervised skill learning based on mutual information maximization between states and skills, most of the work being motivated by intuition and empirical performance. This work complements Eysenbach et al. (2022) by providing a rigorous understanding of the properties of the learned skills and provides useful insights. The analysis presented in this work is novel, to the best of my knowledge and comprises a fairly significant advancement of our understanding of this sub-area of RL. The quality of analysis and writing is satisfactory, with sufficient background and context provided before explaining the main results of the paper.\n\n**References:**\n\nBenjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. The information geometry\nof unsupervised reinforcement learning. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. [OpenReview.net](http://openreview.net/), 2022. URL\nhttps://openreview.net/forum?id=3wU2UX0voE."
                },
                "weaknesses": {
                    "value": "This is a fairly strong submission which checks all the boxes. The only minor complaint is that the empirical results in Appendix H should ideally be a part of the main paper, since including them makes the submission more well-rounded and gives empirical validation for the results presented in Section 3."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2686/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699100605629,
            "cdate": 1699100605629,
            "tmdate": 1699636209550,
            "mdate": 1699636209550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "huDoNXJsNL",
                "forum": "zSxpnKh1yS",
                "replyto": "8JzhCCMTSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2686/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A response"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your time and attention and thank you for your valuable feedback! We will consider your suggestion for the presentation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2686/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700084673586,
                "cdate": 1700084673586,
                "tmdate": 1700084673586,
                "mdate": 1700084673586,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]