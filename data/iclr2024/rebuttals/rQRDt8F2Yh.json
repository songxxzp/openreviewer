[
    {
        "title": "A Discrete and Variational Approach to Speech Representation Learning"
    },
    {
        "review": {
            "id": "6nLxtiHGD1",
            "forum": "rQRDt8F2Yh",
            "replyto": "rQRDt8F2Yh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_G17g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_G17g"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a variational approximation of the speech representation learning problem that looks to generalize several previous works in the field and also provides advantages on the quality of the representation by imposing a direct relationship between the distribution of the latent representation given a known context (observed frames) and a variational distribution of the latent representation given the unknown context that must be reconstructed (masked or future frames).\n\nThe paper presents results on three standard downstream application tasks: Phone Classification, Speaker Verification, and Automatic Speech Recognition, and also evaluates the behavior of each component of the proposed ELBO, comparing it to the \"equivalent\" terms of HuBERT model. For the experimental phase, the paper uses simplified versions of previously proposed models used for comparison: wav2vec2.0 and HuBERT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed variational approximation tries to provide a better and principled formulation of the learning representation task, which contributes to a better understanding of the problem and presents a way to understand the relationships among the learning objectives of several of the existing solutions in the state-of-the-art. Moreover, it provides results that outperform simplified versions of two widely used models in the context of speech representation."
                },
                "weaknesses": {
                    "value": "The formulation presented in the paper is not well-described; Figure 1b does not contribute to the understanding of the proposed approach and should be rebuilt entirely, and the learning process should also be explained in more detail. The use of simplified versions of benchmarking models limits the evidence of performance improvements presented in the paper."
                },
                "questions": {
                    "value": "- How the proposed model guarantees the identifiability of distributions q(z_i|x_i) and p(z_i|x_\\m) is unclear. Is the model train in fully unsupervised or (self-supervised) learning strategy or do the authors uses a force alligment to get the one-hot encoding vectors for all the experiments?\n\n- The authors did not clarify their process to update the codebook; the whole learning process should be explained better. \n\n- The authors did not perform any experiment to evaluate the effect of the codebook size, which was arbitrarily set to 100. According to previous results using VQ strategies for speech representation, that is a too-small value.  \n\n- Is there any reason that explains why the future prediction model outperforms the masked prediction training on speaker verification?\n\nMinor things:\n\n- The PER acronym is used before definition\n-  There is an error in equiation (11) second row, las term should be p(z|x_A)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1449/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739427241,
            "cdate": 1698739427241,
            "tmdate": 1699636073851,
            "mdate": 1699636073851,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dLq6nYBP9A",
                "forum": "rQRDt8F2Yh",
                "replyto": "6nLxtiHGD1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback.\n\n1. There is a concern about the presentation of the submission. The\noriginal figure 1(b) is closer to the graphical rather than the\nfeed-forward process that is more familiar in deep learning. We update\na first draft of figure 1(b) to show the loss and the feed-forward process.\n\n> The formulation presented in the paper is not well-described; Figure 1b does not contribute to the understanding of the \nproposed approach and should be rebuilt entirely, and the learning process should also be explained in more detail.\n\nWe have a draft version of [an updated figure 1(b)](https://i.imgur.com/XBfgSO4.png).\n\n> The authors did not clarify their process to update the codebook; the whole learning process should be explained better.\n\nIn terms of the training process, the codebook V in equation in (5) and\n(6) are jointly optimized with gradient descent when minimizing equation\n(3).\n\n2. There is a concern about our experimental methodology. \n\n> The use of simplified versions of benchmarking models limits the evidence of performance improvements presented in the p\naper.\n\nOur focus is on the proposed loss function, and the benchmarking is\nexactly controlled under the same model architecture. The goal is more\nto show the connection among approaches, and less about getting the\nbest numbers.\n\n3. Miscellaneous questions\n\n> How the proposed model guarantees the identifiability of distributions q(z_i|x_i) and p(z_i|x_\\m) is unclear.\n\nWe are not sure about the identifiability mentioned here. We certainly\nmake no guarantee that the auxiliary distribution q will be close to the\nposterior. We also want to caution about degeneracy. When the two sets\nA and B in equation (1) have overlaps, p and q can collaborate and lead\nto degeneracy. We will revise the paragraph before 2.1 to make this clear.\n\n> Is the model train in fully unsupervised or (self-supervised) learning strategy or do the authors uses a force alligment to get the one-hot encoding vectors for all the experiments?\n\nOur variational approach is unsupervised and self-supervised, and does\nnot rely on forced alignments.\n\n> The authors did not perform any experiment to evaluate the effect of the codebook size, which was arbitrarily set to 100. According to previous results using VQ strategies for speech representation, that is a too-small value. \n\nWe follow the setting in HuBERT. See Section IV. B. in Hsu et al. (2021) [1].\n\n> Is there any reason that explains why the future prediction model outperforms the masked prediction training on speaker verification?\n\nOur submission is the few to study speaker verification with causal\nself-attention, it is unclear at this point what the potential cause is.\n\nThe key hyperparameters that decide what's learned in the representations\nare the time shift for future prediction and the masking probability\nand masking span in masked prediction. More research is needed to draw\nthe connection between these hyperparameters and the learned representations.\n\n> Minor things:\n\nWe thank the reviewer for catching the errors and will revise accordingly.\n\n\n[1] Hsu et al., HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973668951,
                "cdate": 1699973668951,
                "tmdate": 1699977447670,
                "mdate": 1699977447670,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q3hTHxHpWV",
            "forum": "rQRDt8F2Yh",
            "replyto": "rQRDt8F2Yh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_9zbV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_9zbV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new interpretation of self-supervised learning algorithms such as wav2vec 2.0 and quantized CPC in which a transformation of one part of the data is used to predict a quantized version of another part of the data.  The new formulation focuses on the quantizer, rather than focusing on the predictor: it uses a variational lower bound in which the log probability of the masked data given the unmasked data is bounded by the log probability of reconstruction from the codebook, minus the KL divergence between the quantizer distribution and the predictor distribution."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The theoretical argument is quite interesting.  The original wav2vec 1.0 paper included all of the components of the proposed approach, but in that paper, the codebook entropy was presented as sort of an ad-hoc method of avoiding mode collapse.  The KL divergence (information rate) suggested in this paper is a more principled way of understanding what wav2vec 2.0 is really calculating."
                },
                "weaknesses": {
                    "value": "The theoretical argument is interesting, but the experiments are quite weak.  HuBERT and wav2vec 2.0 are crippled, and then the new representation is shown to outperform them.  Crippling the baselines might be forgivable if the crippling was irrelevant to the theoretical claims, but it is not.  HuBERT is crippled by not retraining the K-means codebook every few epochs, and wav2vec by removing the codebook entropy loss; these are directly relevant to the theoretical claims, and cause the experimental tests to be insufficient proof of the theoretical claims.\n\nAgainst recommendations in the original HuBERT paper, this paper does not re-train HuBERT's codebook between epochs of transformer training. Figure 2 then shows that the proposed method achieves superior performance because it adapts the quantizer representation in a series of modes, which HuBERT cannot do because the authors chose not to allow it.  Indeed, re-training the K-means codebook in the manner recommended in the original HuBERT article would probably lead to a similar learning curve to VLB.\n\n\"As opposed to previous work in advocating codebook usage\" -- The wording of this paragraph suggests that wav2vec increases a quantity while you decrease the same quantity, which is not true.  Your formulation measures D(q||p); diversity loss measures H(q).  Indeed, this is where the choice to remove H(q) from your wav2vec implementation is particularly troubling.  Wav2vec minimizes -H(q)-Eq[logp(z)], which is exactly D(q||p).  In other words, if you add back the entropy loss, wav2vec is already minimizing exactly the quantity proposed in this paper, and there should be no difference in performance between wav2vec and VLB.\n\nCompared to those, this is a relatively minor point: One of the differences between future prediction and masked prediction is that, using future prediction, it's possible for each frame to serve two roles: to be predicted by its predecessor frames, while it is also a predictor of future frames.  Eq. (2) trivializes this by saying that the sum of all prediction log probabilities is less than or equal to the log probability of predicting the rest of the sequence from the first k frames.\n\nThere are a large number of grammar mistakes that.  Some of them, slowed my understanding of the paper somewhat: notable among these include the strange wording in the second line of the abstract, and the notational error in the second line of the equation in Appendix A.\n\np. 1\n\nit is plausible if -> it is plausible that?  But why are you assuming that it is plausible?  I think, rather, you are proposing that this exists.\n\na information theoretic -> an information theoretic\n\nand have a model -> and requiring a model\n\np. 4\n\n$u_j$ is the j-th row of U -- I think you mean the j-th column.  Similarly v_j.\n\ncloset -> closest\n\nself-supervise learning -> self-supervised learning\n\nDeepCluter -> DeepCluster\n\np. 7\n\nTable 1: The parameter count column for the BASE model contains the\nstring \"LS960\" rather than a parameter count.\n\np. 8\n\nleanred -> learned\n\n\"representations achieve better downstream performance when fewer bits\nare needed\" -- I think this sentence belongs in the next paragraph; it\nis not justified by any facts presented in this paragraph.\n\nWERs degrades -> WER degrades\n\nthe model obtain -> the model obtains\n\nAppendix A\n\nSecond line of Eq. (11): log p(z|XB) should be log p(z|XA)."
                },
                "questions": {
                    "value": "1. If you permit HuBERT to re-train its K-means codebook once every few epochs, does the resulting rate/distortion curve resemble the rate distortion curve of VLB?  What are the similarities and differences, and why?\n\n2. If you permit wav2vec 2.0 to have its codebook entropy term, then is the resulting training criterion identical to VLB?  If not, why not?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1449/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1449/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1449/Reviewer_9zbV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1449/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759913907,
            "cdate": 1698759913907,
            "tmdate": 1699636073763,
            "mdate": 1699636073763,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XHWEGMW2C5",
                "forum": "rQRDt8F2Yh",
                "replyto": "q3hTHxHpWV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable and detailed feedback.\n\nThere is a concern about our experimental methodology (quoted below)\nabout the submission intentionally crippling the baselines. However, the\nreviewer seems to have misunderstood HuBERT and wav2vec. We will pinpoint\nwhere the confusion comes from and propose revision to our submission.\n\n> The theoretical argument is interesting, but the experiments are quite weak. HuBERT and wav2vec 2.0 are crippled, and then the new representation is shown to outperform them. Crippling the baselines might be forgivable if the crippling was irrelevant to the theoretical claims, but it is not.\n\n\n1. Based on the quote below, the reviewer claims that HuBERT updates\nthe k-means codebook during training.\n\n> HuBERT is crippled by not retraining the K-means codebook every few epochs ...\n\n> Against recommendations in the original HuBERT paper, this paper does not re-train HuBERT's codebook between epochs of transformer training. Figure 2 then shows that the proposed method achieves superior performance because it adapts the quantizer representation in a series of modes, which HuBERT cannot do because the authors chose not to allow it. Indeed, re-training the K-means codebook in the manner recommended in the original HuBERT article would probably lead to a similar learning curve to VLB.\n\nThe above claim is technically incorrect, in the sense that the codebook\nin HuBERT is updated every gradient step, not every few epochs (See Hsu\net al. (2021) for the term e_c in equation (3) [1] .). The targets (i.e.,\nthat cluster assignment) to frames, however, are updated only after every\niteration (See Hsu et al. Section IV. B.). The targets are updated with a\nnew round of k-means, so a new codebook is produced every iteration, not\nevery few epochs. K-means centroids across iterations are not compatible\nwith each other. For example, each centroid in the first iteration has 39\ndimensions (for MFCCs), while each centroid in the second iteration has\n768 dimensions (for HuBERT hidden vectors).\n\nBesides, iteration two and onwards in HuBERT are technically not\nunsupervised or self-supervised, because the selection of a HuBERT\nlayer to produce targets relies on phone and cluster purity.\n\nWe have stated explicitly in 5.1 that we do not do multiple iterations\nin our submission, because our approach can instantiate the HuBERT loss\nfunction for a particular iteration. In principle, it is possible to\ninstantiate loss functions for multiple iterations, but that is not the\ngoal of the paper. The goal of the paper is to show that it is possible\nto instantiate the HuBERT loss function with our proposed approach.\n\n\n2. In terms of wav2vec 2.0, there is one concern about the diversity\nloss and about our experimental methodology.\n\n> \"As opposed to previous work in advocating codebook usage\" -- The wording of this paragraph suggests that wav2vec increases a quantity while you decrease the same quantity, which is not true. Your formulation measures D(q||p); diversity loss measures H(q). Indeed, this is where the choice to remove H(q) from your wav2vec implementation is particularly troubling. Wav2vec minimizes -H(q)-Eq[logp(z)], which is exactly D(q||p). \n\nWe admit the wording issue for sending the wrong message. We agree\nthat our entropy term serves the same purpose as the diversity loss in\nwav2vec 2.0.\n\nThe point is that the diversity of codebook usage is encouraged and\nis part of the natural consequence of our variational bound, while the\ndiversity loss in wav2vec 2.0 is given without any justification. (See\nSection 3.2 in Baevski et al. (2020) [2].) Another difference is that\nthe diversity loss in wav2vec 2.0 is the entropy of an average within a mini-batch of utterances,\nwhile ours is computed per frame in Eq (3).\n\n> In other words, if you add back the entropy loss, wav2vec is already minimizing exactly the quantity proposed in this paper, and there should be no difference in performance between wav2vec and VLB.\n\nWhen instantiating wav2vec 2.0 in our approach, the entropy term has to\nbe 0 because of q being one-hot. There is also no reconstruction term\nin wav2vec 2.0, which our approach has. We will revise section 4.3 to\nreflect the nuances.\n\nFor the experimental methodology, the reviewer solely focuses on the\ncross entropy loss.\n> The original wav2vec 1.0 paper included all of the components of the proposed approach, but in that paper, the codebook entropy was presented as sort of an ad-hoc method of avoiding mode collapse.\n> Crippling the baselines might be forgivable if the crippling was irrelevant to the theoretical claims, but it is not. [...] and wav2vec by removing the codebook entropy loss ...\n\nThe above claim ignores the fact that our loss function also includes\na reconstruction term that is missing in wav2vec 2.0. Again, the entropy term has to be 0 because of the one-hot q.\n\nTo avoid the confusion, we will revise section 4.3 and detail the\ndifference between our approach and wav2vec 2.0."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969341901,
                "cdate": 1699969341901,
                "tmdate": 1700157606570,
                "mdate": 1700157606570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TwpBtsE96y",
                "forum": "rQRDt8F2Yh",
                "replyto": "q3hTHxHpWV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response Part 2"
                    },
                    "comment": {
                        "value": "3. There is a minor concern about how future prediction is formulated.\n\n> Eq. (2) trivializes this by saying that the sum of all prediction log probabilities is less than or equal to the log probability of predicting the rest of the sequence from the first k frames.\n\nWe will rewrite the log probability of equation (2) as below to make clear the factorization of the autoregressive process:\n$L_{\\text{AR}} \\geq -\\sum_{t=1}^{T-k}\\log p(x_{t+k}|x_{\\leq t})$\n\n4. We thank the reviewer for the detailed feedback. We will revise the\nsubmission accordingly.\n\n> There are a large number of grammar mistakes that. Some of them, slowed my understanding of the paper somewhat: notable among these include the strange wording in the second line of the abstract, and the notational error in the second line of the equation in Appendix A. \n\n[1] Hsu et al., HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\n\n[2] Baevski et al., wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969559545,
                "cdate": 1699969559545,
                "tmdate": 1699969936646,
                "mdate": 1699969936646,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iyS71xLpMe",
            "forum": "rQRDt8F2Yh",
            "replyto": "rQRDt8F2Yh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_cFd9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_cFd9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a variational learning framework for self-supervised learning. The authors studied the links between their framework and a few popular self-supervised learning approaches. More specifically, the authors show VQ-APC and HuBERT are all instances of the general framework they proposed. \n\nThe authors conduct experiments to demonstrate the advantage of their variational lower bound objective in terms of optimization. They observed sizable improvement in their experiments in phone classification, speaker verification and ASR. The authors also conduct analysis on the connection between learning dynamics and downstream ASR performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is a very interesting work which may motivate a new angle for self-supervised learning. There are a couple of advantages:\n\n1. This is the first work I\u2019m aware of that tries to connect a few different self-supervised learning objectives and try to unify them under the same umbrella. Better understanding the connections of existing approaches, their connections, Pros and Cons are important.\u00a0\n\n2. The proposed VLB has benefits in terms of optimization.\u00a0\n\n3. The proposed approach provides an information theoretic len for analysis. Specifically, the authors analyzed the learning dynamics vs ASR performance which is motivated by the theoretical foundations laid out in Alemi et al. (2018) and Prokhorov et al. (2019).\n\n4. The proposed approach achieves, if not state of the art, but sizable improvement on the baselines they have set up, which supports their claim on the optimization benefits."
                },
                "weaknesses": {
                    "value": "I would not say these are really weak points, but may be bullet points the authors may pay attention to.\n\n1. I think this is a very nice work, but maybe it is only 95% done presumably due to the ICLR submission deadline. I saw small typos at places. To name a few, in table 1, params should not be LS960, Sometimes, VLB was written as VLM, and some very minor writing typos.\n\n2. The authors demonstrated the connection between their approach and VQ-APC and many more methods, but they only compared tow wav2vec2 and HuBERT. Also, the authors mostly only test one WSJ. To make the claim stronger, does it make sense to compare to more methods you have mentioned and tested on more downstream datasets?\n\n3. Compared with wav2vec-2 and HuBERT, does the proposed framework have advantages or disadvantages in terms of GPU hours? This analysis could be interesting as the authors are proposing a general framework."
                },
                "questions": {
                    "value": "1. In Table one and two, VLB-base archives even more significant improvement. Does this sound reasonable? My understanding is that, Table one and two are strong evidences on the optimization benefits of the variational framework; However, the baseline can be stronger with more tuning, better initialization, optimizer scheduler, and even more data; That is, the gap between the Hubert/wav2vec-2 and VLB in could much smaller than what is shown in this draft. \n\n2. In Table three, are the rate and distortion calculated on dev93, eval92 or training data? Similar question to figure 2, is the PER curve on dev93 or eval92, or train?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1449/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699129230738,
            "cdate": 1699129230738,
            "tmdate": 1699636073703,
            "mdate": 1699636073703,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jaIxozcfCp",
                "forum": "rQRDt8F2Yh",
                "replyto": "iyS71xLpMe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and valuable feedback. There are a few\nquestions that we briefly answer below.\n\n> The authors demonstrated the connection between their approach and VQ-APC and many more methods, but they only compared to wav2vec2 and HuBERT.\n\nIt is indeed interesting to compare to VQ-APC. Given the short timeline,\nwe cannot guarantee to include VQ-APC during the rebuttal period, but\nwill aim for the camera-ready version.\n\n> Compared with wav2vec-2 and HuBERT, does the proposed framework have advantages or disadvantages in terms of GPU hours? \n\nWe do not observe a significant difference in GPU hours between HuBERT\nand the proposed method. We do find wav2vec-2 more computationally\nexpensive due to the negative sampling step.\n\n> In Table one and two, VLB-base archives even more significant improvement. Does this sound reasonable? My understanding is that, Table one and two are strong evidences on the optimization benefits of the variational framework; However, the baseline can be stronger with more tuning, better initialization, optimizer scheduler, and even more data; That is, the gap between the Hubert/wav2vec-2 and VLB in could much smaller than what is shown in this draft.\n\nThe gap might be smaller if we fine-tune on the downstream tasks instead\nof freezing the pre-trained models. However, all comparisons share almost the\nsame hyperparameters, and we expect any improvement to the baseline to\ntransfer to the proposed approach.\n\n> In Table three, are the rate and distortion calculated on dev93, eval92 or training data? Similar question to figure 2, is the PER curve on dev93 or eval92, or train?\n\nThe rate and distortion are calculated on the training data (LibriSpeech\n960 hr). The PER curve is evaluated on dev93. We will add more details\nto Table 3 and Figure 2."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974139775,
                "cdate": 1699974139775,
                "tmdate": 1699981961774,
                "mdate": 1699981961774,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M1xkDPKvZM",
            "forum": "rQRDt8F2Yh",
            "replyto": "rQRDt8F2Yh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_jFUR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_jFUR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new framework to unify causal and non-causal objective under a variational framework. Experimental results shows it's outperform Hubert and ablation also compared k-mean and on-the-fly learned codebook for the proposed VLB."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Unify causal and non-causal objective is a fundamental and important problem for audio representation."
                },
                "weaknesses": {
                    "value": "(1) There are some analogy and connection to other model make no sense. For example, \"The loss function becomes cross entropy if D contains all possible codes in V , and each code is uniquely sample\", this is simply the difference of softmax and contrastive learning, I don't know what this rephrase means. I cannot see the proposed loss generalize anything to contrastive based approach.\n\n(2) Based on (1), the proposed method is more like a unified version of w2v-bert [1] and best-rq [2], both of them using a mlm loss and learn the code on-the-fly without k-means.\n\n(3) Experimental results are weak. No causal baseline been compared. \n\n(4) The paper is unify causal (predictive) and non-causal (mask based), but none of such unification work been mentioned in the paper. Can the author survey and add it?\n\n[1] W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training\n[2] Self-Supervised Learning with Random-Projection Quantizer for Speech Recognition"
                },
                "questions": {
                    "value": "Can the author explain more on the difference of proposed approach versus VQ-CPC?  Am I right conceptually it's replacing contrastive predictive coding with mlm loss?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1449/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699169202531,
            "cdate": 1699169202531,
            "tmdate": 1699636073630,
            "mdate": 1699636073630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NlDaVMnSzb",
                "forum": "rQRDt8F2Yh",
                "replyto": "M1xkDPKvZM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback.\n\n1. There is a confusion about how our approach links to contrastive\nmethods, such as VQ-CPC. Our approach can instantiate VQ-CPC and wav2vec\n2.0, but with an additional reconstruction term. We will make this clear\nin section 4.3.\n\n> (1) There are some analogy and connection to other model make no sense. For example, \"The loss function becomes cross entropy if D contains all possible codes in V , and each code is uniquely sample\", this is simply the difference of softmax and contrastive learning, I don't know what this rephrase means. I cannot see the proposed loss generalize anything to contrastive based approach.\n\nThe contrastive loss is related to the cross-entropy term in the\nvariational bound. Maximizing InfoNCE is analogous to maximizing the\nstandard cross-entropy loss if negative samples always contain all\npossible values of codes (See Eq. 2 in Kong et al. [1]).\n\n> Can the author explain more on the difference of proposed approach versus VQ-CPC? Am I right conceptually it's replacing contrastive predictive coding with mlm loss?\n\nThere is no reconstruction term in VQ-CPC, while there is one in our approach.\n\n2. There is a concern about missing related work that does something\nsimilar to us.\n\n> (2) Based on (1), the proposed method is more like a unified version of w2v-bert [2] and best-rq [3], both of them using a mlm loss and learn the code on-the-fly without k-means.\n\nIndeed, w2v-BERT has both an MLM branch and a contrastive branch.\n(See Fig 2. in Chung et al. [2].) Our approach can instantiate MLM and\na contrastive loss, but not both. The goal of our submission is to show\nthat the two are special cases, not to have both at the same time.\n\nBEST-RQ does not update the codebook during training (See the abstract\nin Chiu et al. [3]), and can be seen as a simplified HuBERT. We will\nrevise 4.2 to include a short discussion on this.\n\n3. As quoted below, there is a concern about experimental methodology.\n\n> (3) Experimental results are weak. No causal baseline been compared.\n\nWe do have results for a HuBERT trained with causal self-attention,\nand will update Table 2 accordingly. Note that our approach is already\nbetter than the causal baseline Baevski et al. (2020) [4] in Table 5, so the results are by no\nmeans weak.\n\n4. We thank the reviewer for the detailed feedback. We will revise the\nsubmission accordingly.\n\n> The paper is unify causal (predictive) and non-causal (mask based), but none of such unification work been mentioned in the paper. Can the author survey and add it?\n\nThe closest attempt is Kong et al. [1], and we will revise and point\nreaders to related work therein.\n\n[1] Kong et al., A Mutual Information Maximization Perspective of Language Representation Learning\\\n[2] Chung et al., W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training\\\n[3] Chiu et al., Self-Supervised Learning with Random-Projection Quantizer for Speech Recognition\\\n[4] Baevski et al., vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699971603475,
                "cdate": 1699971603475,
                "tmdate": 1699980864153,
                "mdate": 1699980864153,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3XpoM5u0BK",
                "forum": "rQRDt8F2Yh",
                "replyto": "NlDaVMnSzb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Reviewer_jFUR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Reviewer_jFUR"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the detailed reply."
                    },
                    "comment": {
                        "value": "Thank you for the explanation, that make the paper a lot clear to me.\n\nI want to clarify when I say weak experiments, I'm refer to WSJ is a barely used benchmark nowadays for ASR, and the reported results are weak. I understand the author argue that it's better than wav2vec 2 as Table 5, but please check some very simple baseline for WSJ: https://github.com/kaldi-asr/kaldi/blob/master/egs/wsj/s5/RESULTS\n\nI would keep my original score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645539658,
                "cdate": 1700645539658,
                "tmdate": 1700645539658,
                "mdate": 1700645539658,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O1GiFxuPsA",
                "forum": "rQRDt8F2Yh",
                "replyto": "M1xkDPKvZM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Re: Thank you for the detailed reply."
                    },
                    "comment": {
                        "value": "Thanks for your reply. We agree with that the absolute numbers are not better than Kaldi (and many other systems), but we respectfully disagree with the argument. We assume the differences among settings are clear, for example, that we are freezing the model to evaluate the representations, that we are using a relatively small end-to-end system, that we do not use a language model. Comparing to those absolute numbers misses the point of the paper and does not provide any evidence to support or to disprove any scientific argument."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650298948,
                "cdate": 1700650298948,
                "tmdate": 1700650602951,
                "mdate": 1700650602951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xQc1VFT7C6",
            "forum": "rQRDt8F2Yh",
            "replyto": "rQRDt8F2Yh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_sRTF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1449/Reviewer_sRTF"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an innovative approach to self-supervised speech representation learning by adopting a variational perspective that unifies existing disparate methods under a predictive coding framework. By using a speech encoder that predicts certain data partitions from others, the system is able to learn predictive knowledge from the signal's context. This includes elements like phonetic details or speaker identity. The novelty lies in their proposition of a variational lower bound (VLB) on the log-likelihood for predicting context from input partitions, framing this process as a generative model with discrete latent variables.\n\nThis variational approach eliminates the need for an additional clustering step found in previous methods and provides a more efficient optimization strategy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The strength of paper is that the method is not only aligned with but also extends the reach of other self supervised representation methods. Importantly, their VLB can draw parallels with contrastive objectives that aim to maximize mutual information.\nAdditionally, the authors explore the learning process through an information-theoretic lens, examining the interplay between KL loss (rate) and reconstruction loss (distortion) during training. They find that effective learning occurs in stages where these terms are balanced to achieve a stable latent distribution, leading to improved performance in downstream tasks when the KL divergence between disjoint contexts is minimized."
                },
                "weaknesses": {
                    "value": "The authors should have more discussion and conclusion around those speaker verification downstream task, and discuss about why MLM-VLB performs better for phone recognition while causal-VLB performs better for speaker verification. More simulation and visualization of learned feature representations for an example sentence and compare it with other VQ based method would be beneficial and add more values to the work."
                },
                "questions": {
                    "value": "The written English can be improved. There are few typos in different part of paper,  e.g. variey instead of variety in 2nd page.\nPlease revise and fix the problems."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1449/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1449/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1449/Reviewer_sRTF"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1449/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699401373951,
            "cdate": 1699401373951,
            "tmdate": 1699636073570,
            "mdate": 1699636073570,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "EVWEsj71Ir",
                "forum": "rQRDt8F2Yh",
                "replyto": "xQc1VFT7C6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and valuable feedback.\nThere is a question about the result on speaker verification.\n\n> More discussion and conclusion around those speaker verification downstream task, why does MLM-VLB perform better for phone recognition while causal-VLB performs better for speaker verification?\n\nOur submission is the few to study speaker verification with causal self-attention, it is unclear at this point what the potential cause is.\n\nThe key hyperparameters that decide what's learned in the representations\nare the time shift for future prediction and the masking probability\nand masking span in masked prediction. More research is needed to draw\nthe connection between these hyperparameters and the learned representations.\n\n\n> More simulation and visualization of learned feature representations for an example sentence and compare it with other VQ based method would be beneficial and add more values to the work.\n\nWe thank the reviewer for the suggestion on adding more values to the work. We will add more visualization of the learned representations in the revision."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699982214797,
                "cdate": 1699982214797,
                "tmdate": 1699982214797,
                "mdate": 1699982214797,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YyPjhDSjyV",
                "forum": "rQRDt8F2Yh",
                "replyto": "EVWEsj71Ir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1449/Reviewer_sRTF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1449/Reviewer_sRTF"
                ],
                "content": {
                    "title": {
                        "value": "Adding figures of learned representation"
                    },
                    "comment": {
                        "value": "Thanks! It would add more insight to the paper, and it would be great to do the same with the other approaches which your formulation could instantiate. \nPlease provide context information (phonetic boundaries) along your learned representations."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1449/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647130407,
                "cdate": 1700647130407,
                "tmdate": 1700647130407,
                "mdate": 1700647130407,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]