[
    {
        "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes"
    },
    {
        "review": {
            "id": "8ZjD5znK1N",
            "forum": "oMLQB4EZE1",
            "replyto": "oMLQB4EZE1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3857/Reviewer_ZSAj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3857/Reviewer_ZSAj"
            ],
            "content": {
                "summary": {
                    "value": "The author proposes a tokenizer for DNA language model, namely, using BPE, in contrast to the k-mer approaches as used before. The authors also propose a large-scale benchmark called GUE to compare DNA language models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Clear definition of motivations, challenges, and solutions\n- The use of BPE makes intuitive sense\n- Experiments look solid and extensive\n- Solving an important problem of DNA LM \n- A new large-scale benchmark"
                },
                "weaknesses": {
                    "value": "- Novelty is questionable since BPE is a well-known technique. The use of FlashAttention, LoRA, and AliBi are also not new. So methodologically, it is hard to gauge its novelty."
                },
                "questions": {
                    "value": "- Is there potential for cross-species information leakage? For instance, given the substantial overlap in genomes between humans and primates, the model might easily predict the masked token.\n\n- How does this compare to HyenaDNA?\n\n- On page 7, the authors note that they utilize LoRA for NT but opt for full fine-tuning for DNABERT/DNABERT-2. However, in the methods section, LoRA is described as an integral part of the approach. This is somewhat perplexing.\n\n- While the authors suggest further pre-training on GUE sequences, this might raise concerns regarding its ability to generalize to datasets with novel sequences. For a balanced comparison, it might be best if the authors refrain from additional pre-training on GUE sequences.\n\n- Did the authors evaluate the sequence statistics of the GUE sequences in relation to the sequences from the pre-training corpus?\n\n- The authors claim the method requires significantly less computational power and memory. Did they test the performance with a larger model size? If there wasn't a notable performance enhancement, it would be noteworthy to highlight this.\n\n- Have the authors assessed how the model's performance varies with different dataset sizes?\n\n- Have the authors conducted ablation on FlashAttention, AliBi, and LoRA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3857/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3857/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3857/Reviewer_ZSAj"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3857/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698033859427,
            "cdate": 1698033859427,
            "tmdate": 1700633681720,
            "mdate": 1700633681720,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nn2hksP9vB",
                "forum": "oMLQB4EZE1",
                "replyto": "8ZjD5znK1N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ZSAj - Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your detailed assessment of our work! We understand your concerns. We have provided much more empirical results to support our claims and provide more detailed explanation to solve the confusion.\n\n**W1: Novelty is questionable since BPE is a well-known technique. The use of FlashAttention, LoRA, and AliBi are also not new. So methodologically, it is hard to gauge its novelty.**\n\nThank you for your comments regarding the novelty of our work. We understand your concerns about the application of well-known techniques in our study. However, we would like to emphasize that the novelty of our work lies not in the invention of new techniques, but in their innovative application and combination to address specific challenges in genome language modeling, as well as provide solid and well-tailored foundation model and benchmark to this area.\n\n1. While BPE is established in NLP, our work is among the first to explore the use of BPE for genome sequences. Instead of trivially applying it, we provide a comprehensive theoretical foundation (Section 2) and empirical evidence (Section 1 of our common response) to demonstrate the advantages of BPE in this context. We also conduct a comprehensive empirical analysis of the model\u2019s performance and efficiency with different vocab sizes. We believe our work provides key insights and guidance for future research in this domain.\n2. The novelty also lies in our unique combination of advanced techniques (FlashAttention, LoRA, and ALiBi) to overcome specific barriers in large genome foundational models. This combination, tailored to the unique challenges, demonstrates how existing tools can be synergistically applied to create a more efficient and effective model for genome analysis. And the pre-trained model itself can serve as an efficient yet powerful tool for countless downstream tasks.\n3. Additionally, our work introduces the Genome Understanding Evaluation (GUE) benchmark, the largest and most comprehensive benchmark to date for genome foundational models that address the lack of standardized and comprehensive benchmarks in this field. \n\nIn summary, while the individual techniques we employed are known, their application in the context of genome language modeling, the way they are combined to address specific challenges, the pre-trained DNABERT-2 model, and the introduction of a comprehensive new benchmark constitute significant novel contributions to our work. We believe these aspects collectively advance the field of genome foundational models and provide valuable insights and resources for future research.\n\n**Q1: Is there potential for cross-species information leakage? For instance, given the substantial overlap in genomes between humans and primates, the model might easily predict the masked token.**\n\nThank you for indicating this! We agree with the concern of cross-species information leakage. In an extreme case, if all the species have the same genome, than training on the multi-species genome is identical to training the model on a single species genome for lots of epochs, which leads to severe overfitting problems. This is one of the main reasons we chose to train DNABERT-2 on multi-species genomes. \n\nWe can also find some evidence in existing works, for example, if we compare the Nucleotide Transformers variants trained on 1000G and multi-species genome, the one trained on the multi-species genome actually performs much better despite the size of the 1000G dataset being larger than the multi-species one. We think the diversity and comprehensiveness of pre-training data is essential to the model\u2019s performance. There may exist more strategical ways of selecting different species to achieve better diversity and comprehensiveness and we will investigate more in this direction in the future.\n\n**Q2: How does this compare to HyenaDNA?**\n\nThanks for asking. HyenaDNA is a concurrent work of us. We have added it as a baseline and reported its results on the common results. In short, we think DNABERT-2 and HyenaDNA have different focuses. Based on our current evaluation, DNABERT-2 is more effective on different types of tasks, while HyenaDNA, as a convolution-based model, has the potential to handle much longer sequences given the same GPU memory. We will compare with it rigorously and add our comparison results and conclusion in the camera-ready version."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110102493,
                "cdate": 1700110102493,
                "tmdate": 1700110102493,
                "mdate": 1700110102493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rJ0W9yNiia",
                "forum": "oMLQB4EZE1",
                "replyto": "8ZjD5znK1N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer ZSAj - Part 2"
                    },
                    "comment": {
                        "value": "**Q3: On page 7, the authors note that they utilize LoRA for NT but opt for full fine-tuning for DNABERT/DNABERT-2. However, in the methods section, LoRA is described as an integral part of the approach. This is somewhat perplexing.**\n\nSorry for causing the confusion. We did make LoRA available in our DNABERT-2 implementation but we did not use it in the final results since it leads to little efficiency improvements given the small size and efficiency of DNABERT-2 and does not affect the performances. We will re-structure the method section in the camera-ready version to avoid confusion. Thanks for pointing out this!\n\n**Q4: While the authors suggest further pre-training on GUE sequences, this might raise concerns regarding its ability to generalize to datasets with novel sequences. For a balanced comparison, it might be best if the authors refrain from additional pre-training on GUE sequences.**\n\nSorry for causing the confusion! We will explain it better in the camera-ready version. We evaluate two variants of the DNABERT-2 model, **DNABERT-2** and **DNABERT-2\u2662**, as shown in Tables 2, 3, and 4 in the paper. The first one is the model purely trained on the pre-training corpus (never seen GUE before), while the second one is achieved by further training the first one with MLM loss on GUE's training set. As shown in Table 2, without further pre-training on GUE, **DNABERT-2** already performs on par with **NT-2.5B-multi**. The fact that **DNABERT-2\u2662** outperforms **DNABERT-2** by 1 on the average score on GUE shows the benefit of further pre-training.\n\n**Q5: Did the authors evaluate the sequence statistics of the GUE sequences in relation to the sequences from the pre-training corpus?**\n\nThanks for asking! The average sequence length during pre-training is about 700 bp. After adding the 8 datasets (as described in the common response), the sequence length in GUE ranges from 70bp to 10000bp. Based on our evaluations, DNABERT-2 performs consistently well across tasks with different sequence lengths, either the input is 10 times shorter or 15 times longer than the sequences it saw during pre-training. We will add this discussion in the camera-ready version, too.\n\n**Q6: The authors claim the method requires significantly less computational power and memory. Did they test the performance with a larger model size? If there wasn't a notable performance enhancement, it would be noteworthy to highlight this.**\n\nUnfortunately, we do not have enough resources yet to train a much larger model. But we agree with your point on this. The phenomenon you described can imply the existence of some sort of \"ceiling\" in DNA language models. This is something we are interested in diving into in the future. However, given the fact that a small-scale in-domain pre-training can lead to big performance improvement, we think we are still far from the ceiling of DNA language models.\n\n**Q7: Have the authors assessed how the model's performance varies with different dataset sizes?**\n\nThanks for pointing out this! We agree it is important, so we design experiments to assess the model on this. We show the results in section 4 of our common response and will add it to the camera-ready version. Thanks!\n\n**Q8: Have the authors conducted ablation on FlashAttention, AliBi, and LoRA?**\n\nThanks for asking! As shown in Section 1 of the common response, we have done ablation studies on the effect of BPE.\n\nThe FlashAttention module, in theory, does not affect the final result at all, since it only changes the process of attention calculation. So we did not perform an ablation study on it. We are performing the ablation study on ALiBi and it may take a few days. We will share the results here is we manage to get it before the end of the discussion period, and we will add it to the camera-ready version, too. As for LoRA, we only perform an ablation study on its effects on Nucleotide Transformers, since NT are the only models we use LoRA. In Appendix A.4 we compare the results of our implementation of NT with LoRA with the results they reported in the paper to show our comparison with them is fair. We do not compare NT on LoRA vs full finetuning, since we do not have enough resources to fully finetune NTs."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110129374,
                "cdate": 1700110129374,
                "tmdate": 1700110129374,
                "mdate": 1700110129374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KaO7AOqwEd",
                "forum": "oMLQB4EZE1",
                "replyto": "8ZjD5znK1N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind remind"
                    },
                    "comment": {
                        "value": "Dear reviewer ZSAj,\n\nThank you for dedicating your time to reviewing our manuscript. Your insights have been invaluable, and we've crafted a detailed response to address the points you raised. We're confident that our responses have effectively addressed all your concerns. With these changes in mind, we hope you might reconsider and adjust your evaluation of our work. \n\nShould there be any remaining issues, we're more than ready to provide further clarifications. As the rebuttal phase is nearing its deadline, we're looking forward to engaging in a timely discussion. Thank you again for your expertise and guidance!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515543392,
                "cdate": 1700515543392,
                "tmdate": 1700515543392,
                "mdate": 1700515543392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ykWHvhnIyL",
                "forum": "oMLQB4EZE1",
                "replyto": "KaO7AOqwEd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_ZSAj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_ZSAj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response! I am raising my scores to 6."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633701497,
                "cdate": 1700633701497,
                "tmdate": 1700633701497,
                "mdate": 1700633701497,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XFjTuzdeG7",
            "forum": "oMLQB4EZE1",
            "replyto": "oMLQB4EZE1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3857/Reviewer_cdaf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3857/Reviewer_cdaf"
            ],
            "content": {
                "summary": {
                    "value": "The authors describe a foundation model for DNA sequences, improving on existing models (of which there are relatively few) in terms of computational requirements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "-Incorporation of more recent language model techniques into the modeling approach.\n\n- Foundation models have the potential to be a highly useful resource for the computational biology community.  There are very few options at the moment, and the significantly reduced computational requirements of DNABERT2 compared to the Nucleotide Transformer on the one hand, and improved accuracy over DNABERT, make it a welcome addition.\n\n- DNABERT2 is appropriately benchmarked against DNABERT and the Nucleotide Transformer.\n\n- The authors have curated a collection of datasets for benchmarking DNA language models.  The benchmark datasets are sufficiently challenging to provide good discrimination between the performance of the various methods, and indicate that there is still plenty of room for improvement."
                },
                "weaknesses": {
                    "value": "- Deep learning models applied to one-hot encoded genomic sequences appear to have a much higher level of interpretability than those that utilize k-mer tokenization, and I expect this to be even worse for the BPE encoding used in this work.  Unlike other areas of application, in computational biology applications, interpretability is a key factor in choosing a model."
                },
                "questions": {
                    "value": "- \"Despite having 30% more parameters than DNABERT, DNABERT-2 requires only one-third the number of FLOPs. This indicates the superiority of the Byte Pair Encoding (BPE)-based tokenization method over overlapping k-mer tokenization in terms of modeling efficiency.\"\nNot sure I agree with this statement - the increased efficiency might be the result of other differences between the models.\n\"This underscores the importance of providing the model with adequate data, particularly when the model size is scaled up, and further highlights the inefficiency of overlapping k-mer tokenization. The comparison between DNABERT and NT-2500M-1000g exposes the sample inefficiency of non- overlapping k-mer tokenization. Despite being trained on 2.5 times more tokens, NT-2500M-1000g achieves a performance similar to that of DNABERT.\"\nAgain, there are other differences between the models, so ascribing this to the difference in tokenization method is a stretch.  If you want to demonstrate the advantage of BPE tokenization, you will need to perform an experiment on two different versions of DNABERT2 - one with k-mer tokenization, and one with BPE tokenization.  **The authors have addressed this point with a thorough ablation study**.\n\n- Please compare your benchmark datasets with the recently published \"Genomic benchmarks\":\nGre\u0161ov\u00e1, K., Martinek, V., \u010cech\u00e1k, D. et al. Genomic benchmarks: a collection of datasets for genomic sequence classification. BMC Genom Data 24, 25 (2023). https://doi.org/10.1186/s12863-023-01123-8\n\ntypos / grammar:\n\nBENCKMARK: GENOME UNDERSTANDING EVALUATION (GUE)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3857/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3857/Reviewer_cdaf",
                        "ICLR.cc/2024/Conference/Submission3857/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3857/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722936544,
            "cdate": 1698722936544,
            "tmdate": 1700582601730,
            "mdate": 1700582601730,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CPiHfXk83s",
                "forum": "oMLQB4EZE1",
                "replyto": "XFjTuzdeG7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cdaf"
                    },
                    "comment": {
                        "value": "Thank you for your encouraging evaluation of our work on DNABERT2. We are especially grateful for your recognition of the model's potential as a valuable resource in the computational biology community, and your acknowledgment of the significant improvements introduced by our model. Following your suggestions, we have added multiple experiments.\n\n**W1 : Deep learning models applied to one-hot encoded genomic sequences appear ot have a much higher level of interpretability than those that utilize k-mer tokenization, and I expect this to be even worse for the BPE encoding used in this work. Unlike other areas of application, in computational biology applications, interpretability is a key factor in choosing a model.**\n\nThank you for highlighting the critical aspect of interpretability in computational biology applications. We acknowledge that models trained on one-hot encoded sequences offer a high level of interpretability, especially when it comes to understanding the interactions at the level of individual nucleotide bases. In fact, there does exist an inevitable trade-off between fine-grant interpretability and the model\u2019s computational efficiency. \n\nWe think BPE tokenization also offers a unique perspective on genomic sequence analysis. BPE tokenization groups frequently co-occurring nucleotide sequences, providing insights into how these 'groups'  play a role in the genome's function. For instance, consider a scenario where a mutation occurs. We may be able to understand how impactful a mutation is by looking at both the difference between the tokenized sequences before and after the mutation, and the model\u2019s predictions/attentions among different tokens.\n\n**Q1: If you want to demonstrate the advantage of BPE tokenization, you will need to perform an experiment on two different versions of DNABERT2 - one with k-mer tokenization, and one with BPE tokenization.**\n\nThank you for highlighting the importance of directly comparing the effectiveness of different tokenization methods in DNABERT2. We concur that an ablation study focusing on tokenization techniques is crucial for substantiating the superiority of BPE (Byte Pair Encoding) over k-mer tokenization. In response to your suggestion, we have conducted additional experiments with two distinct versions of DNABERT2 - one utilizing k-mer tokenization and the other employing BPE tokenization. As detailed in Section 1 of our common response, these experiments have yielded compelling results. The version of DNABERT2 with BPE tokenization demonstrates a marked improvement in performance over its k-mer counterpart. This outcome not only confirms the efficacy of BPE tokenization in the context of DNA language modeling but also provides robust empirical evidence to support our claim. We believe that these findings significantly bolster the validity of our approach and contribute valuable insights into the optimization of language models for genomic data analysis\n\n**Q2: Please compare your benchmark datasets with the recently published \"Genomic benchmarks\"**\n\nThanks for indicating this! In short, GenomicsBenchmark is a concurrent work with an aim similar to us, to create a comprehensive and discriminative space to compare different genome analysis models. It contains 9 datasets from 3 species with input sizes ranging from 200 to ~4000, while GUE contains 36 datasets from multiple species with input sizes ranging from 70 to 1000. Compared to GUE, GenomicsBenchmark also provides a Python package for data loading and processing, which is very helpful and we plan to do this in the future. We will include and discuss this paper in the camera-ready version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108549955,
                "cdate": 1700108549955,
                "tmdate": 1700108549955,
                "mdate": 1700108549955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zzqd9bJZVO",
                "forum": "oMLQB4EZE1",
                "replyto": "XFjTuzdeG7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind remind"
                    },
                    "comment": {
                        "value": "Dear reviewer cdaf,\n\nThank you for dedicating your time to reviewing our manuscript, and thanks a lot for your recognition of our novelty and contribution to this field. Your insights have been invaluable, and we've crafted a detailed response to address the points you raised. We're confident that our responses have effectively addressed all your concerns. Should there be any remaining issues, we're more than ready to provide further clarifications. As the rebuttal phase is nearing its deadline, we're looking forward to engaging in a timely discussion. Thank you again for your expertise and guidance!"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515625861,
                "cdate": 1700515625861,
                "tmdate": 1700515625861,
                "mdate": 1700515625861,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g8Z5Dpd7p6",
                "forum": "oMLQB4EZE1",
                "replyto": "CPiHfXk83s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_cdaf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_cdaf"
                ],
                "content": {
                    "comment": {
                        "value": "The authors' rebuttal has addressed my concerns.  Overall the authors were very responsive to reviewer comments, and performed additional experiments that further demonstrate the validity of their approach, and should improve the paper once incorporated.  I expect this will be an impactful paper, and strongly support it being published.  My score already reflected that assessment."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582978578,
                "cdate": 1700582978578,
                "tmdate": 1700582978578,
                "mdate": 1700582978578,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K7LGsqTbp8",
            "forum": "oMLQB4EZE1",
            "replyto": "oMLQB4EZE1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3857/Reviewer_1Kmp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3857/Reviewer_1Kmp"
            ],
            "content": {
                "summary": {
                    "value": "DNABERT2 is an update of the DNABERT, which is an application of the BERT structure to DNA data. My guess is that it first performs tokenisation of input DNA sequence, then pre-trains on DNA dataset to get the token embeddings, after that it adds a few layers to utilise the token embeddings for classification tasks such as promoter detection and transcription factor prediction. The manuscript made the following improvements: (1) use Byte Pair Encoding (BPE) for tokenisation (2) use attention with linear biases (ABiLi) for position encoding and (3) use flash attention and low-rank adaptation (LoRA) for acceleration. It also compiles a larger benchmark dataset for comparing different methods. The manuscript demonstrated that DNABERT2 improved over DNABERT and had a similar performance as Nucleotide transformer. \n\nI think the authors have done a decent amount of work and the work could be more useful for the community if the authors could\n(1) perform an ablation study to quantify the contribution of BPE and ALiBi independently.\n(2) explain why the code, data and pre-trained model could not be made public now\n(3) explain why mcc and f1 are used as the comparison metric for different tasks\n(4) explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "DNABERT2 is an update of the DNABERT, which is an application of the BERT structure to DNA data. My guess is that it first performs tokenisation of input DNA sequence, then pre-trains on DNA dataset to get the token embeddings, after that it adds a few layers to utilise the token embeddings for classification tasks such as promoter detection and transcription factor prediction. The manuscript made the following improvements: (1) use Byte Pair Encoding (BPE) for tokenisation (2) use attention with linear biases (ABiLi) for position encoding and (3) use flash attention and low-rank adaptation (LoRA) for acceleration. It also compiles a larger benchmark dataset for comparing different methods. The manuscript demonstrated that DNABERT2 improved over DNABERT and had a similar performance as Nucleotide transformer. \n\nI think the authors have done a decent amount of work and the work could be more useful for the community if the authors could\n(1) perform an ablation study to quantify the contribution of BPE and ALiBi independently.\n(2) explain why the code, data and pre-trained model could not be made public now\n(3) explain why mcc and f1 are used as the comparison metric for different tasks\n(4) explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3."
                },
                "weaknesses": {
                    "value": "DNABERT2 is an update of the DNABERT, which is an application of the BERT structure to DNA data. My guess is that it first performs tokenisation of input DNA sequence, then pre-trains on DNA dataset to get the token embeddings, after that it adds a few layers to utilise the token embeddings for classification tasks such as promoter detection and transcription factor prediction. The manuscript made the following improvements: (1) use Byte Pair Encoding (BPE) for tokenisation (2) use attention with linear biases (ABiLi) for position encoding and (3) use flash attention and low-rank adaptation (LoRA) for acceleration. It also compiles a larger benchmark dataset for comparing different methods. The manuscript demonstrated that DNABERT2 improved over DNABERT and had a similar performance as Nucleotide transformer. \n\nI think the authors have done a decent amount of work and the work could be more useful for the community if the authors could\n(1) perform an ablation study to quantify the contribution of BPE and ALiBi independently.\n(2) explain why the code, data and pre-trained model could not be made public now\n(3) explain why mcc and f1 are used as the comparison metric for different tasks\n(4) explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3."
                },
                "questions": {
                    "value": "DNABERT2 is an update of the DNABERT, which is an application of the BERT structure to DNA data. My guess is that it first performs tokenisation of input DNA sequence, then pre-trains on DNA dataset to get the token embeddings, after that it adds a few layers to utilise the token embeddings for classification tasks such as promoter detection and transcription factor prediction. The manuscript made the following improvements: (1) use Byte Pair Encoding (BPE) for tokenisation (2) use attention with linear biases (ABiLi) for position encoding and (3) use flash attention and low-rank adaptation (LoRA) for acceleration. It also compiles a larger benchmark dataset for comparing different methods. The manuscript demonstrated that DNABERT2 improved over DNABERT and had a similar performance as Nucleotide transformer. \n\nI think the authors have done a decent amount of work and the work could be more useful for the community if the authors could\n(1) perform an ablation study to quantify the contribution of BPE and ALiBi independently.\n(2) explain why the code, data and pre-trained model could not be made public now\n(3) explain why mcc and f1 are used as the comparison metric for different tasks\n(4) explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3857/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794184380,
            "cdate": 1698794184380,
            "tmdate": 1699636344053,
            "mdate": 1699636344053,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9xHianqN8N",
                "forum": "oMLQB4EZE1",
                "replyto": "K7LGsqTbp8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 1Kmp"
                    },
                    "comment": {
                        "value": "We are grateful for your positive assessment of DNABERT2 and your acknowledgment of the key improvements we have implemented over the original DNABERT model. We have done more experiments and will polish the paper accordingly to account for them.\n\n**W1: perform an ablation study to quantify the contribution of BPE and ALiBi independently.**\n\nThanks for indicating this! We totally agree that the ablation study is important for the audience to understand the model. As shown in the common response, we have done an ablation study to compare BPE vs k-mer with the same DNABERT-2 architecture and shown that BPE is indeed more data-efficient. Thank you for this suggestion! We are performing the ablation study on ALiBi and it may take a few days. We will share the results here if we manage to get it before the end of the discussion period, and we will add it to the camera-ready version, too.\n\n**W2 explain why the code, data and pre-trained model could not be made public now**\n\nSorry for the confusion. To avoid violating the double-blind reviewing process, we follow the common practice to describe our open-sourcing plan in this way. In fact, our code has been publicly available in the supplementary file since the paper submission time. The link to the model and data is also shared in the README file. We will add links to the non-anonymous DNABERT-2 resources in the camera-ready version since we are not allowed to add them now. Thanks for your support to the open-source community.\n\n**W3 explain why mcc and f1 are used as the comparison metric for different tasks**\n\nAs suggested by [1], MCC is more reliable than F1 in binary classification and in scenarios when the test set is unbalanced. Since most of our datasets are binary classification and are unbalanced, we use MCC in most cases. The only exception is the virus classification dataset, which is multi-task and perfectly balanced. Thus, we used F1, which we considered to be more common in this scenario. Thanks for indicating the confusion! Since MCC also works well in the multi-class classification case, we plan to always use it as the metric to avoid confusion and will explain it in the camera-ready version.\n\n**W4 explain the benefit of further pre-training. I get lost in understanding the sentence \"This results in 0.41B training tokens...\" right above section 5.3.**\n\nSorry for causing the confusion! We will explain it better in the camera-ready version. In general, if we consider there exists a distributional difference between the pre-training corpus and downstream task, further pre-training on the downstream task datasets helps the model to adapt tot the downstream domain. Here we also investigate the effect of further pre-training in the DNA domain.\n\nWe evaluate two variants of the DNABERT-2 model, **DNABERT-2** and **DNABERT-2\u2662**, as shown in Tables 2, 3, and 4 in the paper. The first one is the model purely trained on the pre-training corpus, while the second one is achieved by further training the first one with MLM loss on GUE's training set. The sentence \"This results in 0.41B training tokens...\" refers to the process of training DNABERT-2 on GUE's training set. Please kindly let me know if this is clearly explained.\n\nAs shown in Table 2, **DNABERT-2\u2662** outperforms **DNABERT-2** by 1 on the average score on GUE, showing the benefit of further pre-training on genome sequence modeling.\n\n[1] Chicco, Davide, and Giuseppe Jurman. \"The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation.\" *BMC genomics* 21.1 (2020): 1-13"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108180493,
                "cdate": 1700108180493,
                "tmdate": 1700108180493,
                "mdate": 1700108180493,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vmBQODR2jC",
                "forum": "oMLQB4EZE1",
                "replyto": "9xHianqN8N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_1Kmp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_1Kmp"
                ],
                "content": {
                    "title": {
                        "value": "The authors have addressed all my concerns."
                    },
                    "comment": {
                        "value": "I keep my original rating."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392433872,
                "cdate": 1700392433872,
                "tmdate": 1700392433872,
                "mdate": 1700392433872,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hiEwjLi49j",
            "forum": "oMLQB4EZE1",
            "replyto": "oMLQB4EZE1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces DNABERT-2, an advancement in genome foundation modeling, which aims to decode the linguistic intricacies of genomes. The authors assert that the computational and sample inefficiencies of k-mer tokenization, predominantly used in earlier models, act as barriers in the development of foundational models for large genomes. To address this, the paper introduces Byte Pair Encoding (BPE) as a replacement for k-mer tokenization. BPE is more efficient and overcomes the limitations of the k-mer approach. The authors also emphasize the need for a standardized benchmark for genome understanding and consequently introduce the Genome Understanding Evaluation (GUE) dataset. Experimental results reveal that DNABERT-2 performs on par with state-of-the-art models but with fewer parameters and less GPU time during pre-training. The model also shows significant improvements over the original DNABERT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. DNABERT-2 incorporates ALiBi and Flashattention mechanisms, enhancing speed and context length.\n2. The model successfully borrows several techniques from LLM (Large Language Models) and integrates them into DNABERT.\n3. The authors have collected a comprehensive dataset tailored for short sequence prediction.\n4. The research is detailed, with a focus on the nuances of the biology setting and the existing benchmarks, showcasing a holistic approach."
                },
                "weaknesses": {
                    "value": "1. The input size for the proposed benchmark seems to be on the shorter side for genomics, potentially limiting its applicability to broader genomics problems.\n2. The benchmark's design appears constrained, lacking baseline models like CNNs and omits language model training from scratch, which could provide comparative insights.\n3. While the paper is apt for an ML conference, there is a discernible deficiency in the depth of biological insights. Better downstream tasks, such as CAGE-seq prediction and so on.... (longer sequence context)"
                },
                "questions": {
                    "value": "1. In the introduction, can you clarify what you specifically mean by \"genome language modeling\"?\n2. Following up on the theme, why was there no citation or reference to models like deepbind/deepSEA? For instance, the TF-DNA binding prediction from Wang et al., 2022, seems not a great citation? Not a genomics language modeling. \n3. Given the unique structure and function of DNA, why is there a continued emphasis on tokenization in DNA language modeling?\n4. I recommend adding the count of sequences for each dataset in Table 1 to provide a clearer understanding of the dataset sizes.\n5. Why weren't tasks involving longer sequences incorporated after introducing DNABERT?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3857/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3857/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3857/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814624012,
            "cdate": 1698814624012,
            "tmdate": 1700517891936,
            "mdate": 1700517891936,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V0MV84XVfZ",
                "forum": "oMLQB4EZE1",
                "replyto": "hiEwjLi49j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cVdN (Part 1)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your thorough evaluation of our work on DNABERT-2 and your insightful comments and questions. Your recognition of the strengths of our research is highly valued. We have added 8 new datasets and 3 baselines to extend GUE\u2019s applicability and provide comparative insights.\n\n**W1: The input size for the proposed benchmark seems to be on the shorter side for genomics, potentially limiting its applicability to broader genomics problems.**\n\nWe appreciate your observation regarding the input size limitation in the original version of our Genome Understanding Evaluation (GUE) benchmark. You are correct in noting that the initial input sizes, ranging up to 1000 base pairs (bp), might not fully encompass the broader spectrum of genomics problems that involve longer genomic sequences.\n\nThe decision to initially focus on shorter input sizes was driven by practical considerations related to the capabilities of existing models and the computational resources available to us. Specifically, DNABERT's architecture inherently limits it to handling sequences no longer than 512 bp. Similarly, the Nucleotide Transformer is limited for inputs up to 6000 bp, and processing sequences of this length requires substantial memory (45-50GB) on each single GPU, which exceeded our resource capabilities.\n\nHowever, recognizing the importance of including longer sequences to enhance the comprehensiveness and applicability of GUE, we have now incorporated 8 additional datasets with input sizes ranging from 5000 to 10000 bp. This expansion allows us to better represent the diversity of genomic sequence lengths encountered in real-world genomics problems.\n\nOur updated results with these additional datasets demonstrate DNABERT-2's remarkable capability in handling longer sequences. Despite being trained on sequences up to 700 bp, DNABERT-2 shows impressive performance on sequences between 5k and 10k bp, outperforming the strongest baseline by a significant margin on all 8 of these datasets. This underscores DNABERT-2's robustness and versatility in modeling genomic sequences of varying lengths.\n\nWe agree that extra-long input sequences, such as those around 1 million bp, are also very meaningful to study. However, they present unique challenges that might require specifically designed architectures or different approaches. Our current benchmark focuses on sequence lengths that can be effectively handled by existing pre-trained models without necessitating specialized architectural modifications.\n\n**W2: The benchmark's design appears constrained, lacking baseline models like CNNs and omits language model training from scratch, which could provide comparative insights.**\n\nThank you for your insightful feedback regarding the design of our benchmark and the selection of baseline models. We understand your concern that the initial version of our benchmark may have appeared constrained due to the focus on state-of-the-art (SOTA) models, potentially overlooking the insights that could be gained from including a broader range of baselines.\n\nRecognizing the importance of a more comprehensive comparative analysis, we have expanded our set of baseline models in the revised manuscript. Specifically, we have added two recent CNN models from referenced papers [1] and [2]. These models represent a different approach to genomic sequence analysis and provide a valuable point of comparison to highlight the strengths and potential limitations of our approach.\n\nFurthermore, to address the aspect of language model training from scratch, we have included an additional baseline: DNABERT-2 without pre-training. This allows us to directly assess the impact and effectiveness of pre-training in our model. By comparing DNABERT-2 without pre-training to the pre-trained version, especially when finetuned on the entire dataset versus finetuned on only 5% of the training data, we offer a clearer picture of how pre-training contributes to the model's performance. This comparison is detailed in Section 4 of our common response.\n\n[1] Nguyen, Eric, et al. \"Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution.\" *arXiv preprint arXiv:2306.15794* (2023).\n[2] Gre\u0161ov\u00e1, Katar\u00edna, et al. \"Genomic benchmarks: a collection of datasets for genomic sequence classification.\" *BMC Genomic Data* 24.1 (2023): 25"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700110951218,
                "cdate": 1700110951218,
                "tmdate": 1700111304077,
                "mdate": 1700111304077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1JGnWWpo7X",
                "forum": "oMLQB4EZE1",
                "replyto": "hiEwjLi49j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cVdN (Part 2)"
                    },
                    "comment": {
                        "value": "**W3: While the paper is apt for an ML conference, there is a discernible deficiency in the depth of biological insights. Better downstream tasks, such as CAGE-seq prediction and so on.... (longer sequence context)**\n\nWe sincerely appreciate your constructive feedback regarding the depth of biological insights in our paper. We acknowledge the importance of incorporating biologically relevant downstream tasks, such as CAGE-seq prediction for transcriptional regulation and activity, to enhance the biological applicability and insights of our model. \n\nHowever, it is crucial to highlight that the primary goal of this paper was to tackle specific computational challenges in genome foundation models. Our focus was on enhancing the efficiency and effectiveness of these models in handling large and diverse genomic datasets. This approach was driven by a need for more computationally practical yet powerful tools that can manage the increasing scale and complexity of genomic data, a challenge that is pivotal for advancing genome research.\n\nIn developing the Genome Understanding Evaluation (GUE) benchmark, our emphasis was slightly different from providing in-depth biological insights. Our aim was to construct a benchmark that, for evaluation purposes, could reveal a clear distinction between the computational capabilities of various methods in genome modeling. To achieve this, we considered several metrics, including the difficulty of tasks, comprehensiveness of datasets, standardization of benchmarks, and applicability to existing genome foundational models. With this focus in mind, tasks and datasets that could best examine and showcase such computational strengths of these models were prioritized.\n\nWe recognize the importance of bridging the gap between computational methodology and biological application. In future works, we plan to incorporate more biologically oriented downstream tasks, such as CAGE-seq prediction and other tasks requiring longer sequence contexts and are actively working on it. We believe that incorporating these downstream tasks will not only enrich the biological insights provided by our model but also enhance its practical relevance and utility in real-world genomic research. \n\n**Q1: In the introduction, can you clarify what you specifically mean by \"genome language modeling\"?**\n\nIn our study, the terms 'DNA language modeling' and 'genome language modeling' are used synonymously. By 'genome language modeling', we specifically mean the development of a language model tailored to genomic sequences. This involves leveraging language model techniques to enhance our understanding of the genome, such as predicting genome functionality and interpreting genomic data.\n\n**Q2: Following up on the theme, why was there no citation or reference to models like deepbind/deepSEA? For instance, the TF-DNA binding prediction from Wang et al., 2022, seems not a great citation? Not a genomics language modeling.**\n\nWe apologize for the oversight in not citing seminal works such as DeepBind and DeepSEA, which indeed represent significant advancements in the field. To rectify this, we will revise the statement 'Recent advancements in genome language modeling have demonstrated their superiority \u2026' to 'Recent advancements in deep learning techniques for genomic data analysis have demonstrated their superiority ...' in the camera-ready version. This change more accurately reflects the scope of the referenced literature. We will also ensure to include and discuss DeepBind and DeepSEA appropriately, recognizing their critical contributions to the field."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700111222717,
                "cdate": 1700111222717,
                "tmdate": 1700111222717,
                "mdate": 1700111222717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y6Mk9dAFKw",
                "forum": "oMLQB4EZE1",
                "replyto": "hiEwjLi49j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer cVdN (Part 3)"
                    },
                    "comment": {
                        "value": "**Q3: Given the unique structure and function of DNA, why is there a continued emphasis on tokenization in DNA language modeling?**\n\nThe concept of tokenization is pivotal in DNA language modeling, mirroring its importance in natural language processing (NLP). In NLP, tokenization involves converting text into smaller units (tokens) for language models to process. Similarly, in DNA language modeling, we tokenize long, continuous sequences of nucleotides into small chunks. This step is crucial as it transforms genomic sequences into a form amenable to language model analysis. The choice of tokenization method significantly influences the model's perception and interpretation of the genomic data. As such, our work delves into various DNA tokenization strategies, assessing their impact and providing valuable insights for future research in this domain. \n\nWe have now also demonstrated BPE leads to better model performance than k-mer tokenizations (Ablation study 1 in common response), which may be attributed to the fact that BPE iteratively merges the most frequent co-occurring genome segments that may be biologically relevant. Such observation emphasizes the critical role of proper tokenization on more effectively applying language model techniques to genomic sequences.\n\n**Q4: I recommend adding the count of sequences for each dataset in Table 1 to provide a clearer understanding of the dataset sizes.**\n\nWe sincerely appreciate the reviewer\u2019s suggestion. We completely agree and will add the counts to the camera-ready version.\n\n**Q5: Why weren't tasks involving longer sequences incorporated after introducing DNABERT?**\n\nWe appreciate the reviewer for raising this important question. We have addressed this in our responses to the weaknesses and have now added 8 additional datasets involving longer sequences."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112138433,
                "cdate": 1700112138433,
                "tmdate": 1700112138433,
                "mdate": 1700112138433,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VjRTQaiKXl",
                "forum": "oMLQB4EZE1",
                "replyto": "hiEwjLi49j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind remind"
                    },
                    "comment": {
                        "value": "Dear reviewer cVdN,\n\nThank you for dedicating your time to reviewing our manuscript. Your insights have been invaluable, and we've crafted a detailed response to address the points you raised. We're confident that our responses have effectively addressed all your concerns. With these changes in mind, we hope you might reconsider and adjust your evaluation of our work. \n\nShould there be any remaining issues, we're more than ready to provide further clarifications. As the rebuttal phase is nearing its deadline, we're looking forward to engaging in a timely discussion. Thank you again for your expertise and guidance!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515518989,
                "cdate": 1700515518989,
                "tmdate": 1700515518989,
                "mdate": 1700515518989,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ikLNTHCTW2",
                "forum": "oMLQB4EZE1",
                "replyto": "VjRTQaiKXl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I'm very happy and satisfied with the response to Weakness 2 and Questions 1, 2, 4.\n\nHowever, for Weakness 1, I believe HyenaDNA also includes species classification tasks in their paper(and even much longer input size). Why not include them for comparison? And for Q3, I believe the base-pair is a more nature option. Regarding other tasks such as CAGE-seq prediction, I am still not convinced about why they should not be incorporated. I understand that most papers on DNA language models, like NT transformer and HyenaDNA, don't cover this, but I think it's a very important issue in this field.\n\nOverall, I really appreciate the response and the additional experiments, so I have raised my score from 5 to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517860436,
                "cdate": 1700517860436,
                "tmdate": 1700517860436,
                "mdate": 1700517860436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lBr84FZrX7",
                "forum": "oMLQB4EZE1",
                "replyto": "6rERV1g8EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3857/Reviewer_cVdN"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Considering the benchmark for long-length tasks and tokenization, I will maintain the score at 6"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3857/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720799713,
                "cdate": 1700720799713,
                "tmdate": 1700720799713,
                "mdate": 1700720799713,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]