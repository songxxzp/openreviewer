[
    {
        "title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers"
    },
    {
        "review": {
            "id": "bS40PzdxX2",
            "forum": "nqlymMx42E",
            "replyto": "nqlymMx42E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6203/Reviewer_feem"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6203/Reviewer_feem"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a study of different design choices (problem representation, neural architecture, pretraining, etc) in the space of string-based RL for molecular discovery. The paper presents extensive experiments, including more than 25 molecule design tasks, to compare different design choices. The best combination of design choices is then used to propose a new RL-based molecular design algorithm (ChemRLformer). The paper also provides a thorough discussion of the results and provides valuable insights and recommendations for practitioners in the field of molecular discovery.\n\nThe paper is well written and organized. It is easy to follow and understand. The paper is also well motivated and provides a good introduction to the field of string-based RL for molecular discovery. I recommend the paper for acceptance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Technical Soundness: The paper demonstrates a high level of technical rigor. It conducts extensive experiments that compare various design choices (such as problem representation, neural architecture, pretraining, etc.) within the realm of string-based reinforcement learning for molecular discovery. Moreover, the paper offers a comprehensive discussion of the results and imparts valuable insights and recommendations for future research.\n\n- Clarity and Organization: The paper is well-written and organized, making it easy to understand and follow. It effectively motivates the reader and provides a solid introduction to the field of string-based reinforcement learning for molecular discovery.\n\n- Significance: The paper makes a substantial contribution to the field of molecular discovery. It offers a comprehensive comparative study of diverse design options (problem representation, neural architecture, pretraining, etc.) within the domain of string-based reinforcement learning for molecular discovery.\n\n- Table 1 provides a nice summary of the different algorithms for text-based molecule design."
                },
                "weaknesses": {
                    "value": "- Originality: While the paper offers a thorough numerical comparison of different design choices (problem representation, neural architecture, pretraining, etc.) in the context of string-based reinforcement learning for molecular discovery, its originality is somewhat constrained."
                },
                "questions": {
                    "value": "- (Introduction) \"...our own algorithm (MoLRL)\" - what is MoLRL?\n- (Table 1) What is the difference between \"MoLRL\" and \"ChemRLformer (Ours)\"? Is MolRL the general framework and ChemRLformer the best combination of design choices based on the ablation study? Please clarify.\n- (Section 4) \"The molecular design space is complex but the benefit from finding improved options is great.\" This sentence reads a bit too colloquially. Please rephrase.\n- (Section 4) \"However, the corresponding transition function induced in the graph representation of molecules is more complex as it is determined by the encoding/decoding rules of the chosen text representation.\" Are these constraints hard-coded in situ on the transitions dynamics or are they learned? Please clarify.\n- (Figure 2) \"(a) Performance on SMILES-based molecular design with pertaining (left) and with pretrianing and RL (right).\" Please correct the typos \"pertaining\" and \"pretrianing\".\n- (Figure 2) \"b) performance on SMILES-based molecular docking with pertaining (right) and with pretrainig and RL (right).\" First (right) should be (left). Please correct.\n- (Figure 2) How does the \"pretraining only\" results (left column) are computed? Is this a zero-shot evaluation? I can't find the answer in the text. Please clarify."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697920303697,
            "cdate": 1697920303697,
            "tmdate": 1699636676096,
            "mdate": 1699636676096,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mytHCtSliN",
                "forum": "nqlymMx42E",
                "replyto": "bS40PzdxX2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to feem"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe thank the reviewer for their feedback and summary of our work. We appreciate the reviewer\u2019s assessment that the paper makes a significant contribution to molecular discovery and is technically sound and well-written. It seems that the main concern of the reviewer is the originality of the proposed algorithm. We agree that the basic components algorithms and design choices that we experimentally study have been used separately in prior work, ours is the first fair and extensive comparison at a large scale for string-based RL. To highlight prior work, we have included Table 1 in our paper, which contains relevant prior works that have used some of the methods that we incorporate.\n\nBelow, we will answer your questions in more detail, and incorporate your feedback by making updates to the paper. \n\n> (Introduction) \"...our own algorithm (MoLRL)\" - what is MoLRL?\n\nThis is a typo. We have substitute MoLRL -> ChemRLformer. Thank you for pointing this out.\n\n> Is MolRL the general framework and ChemRLformer \n\nThis is a typo as well, they are the same method. We have fixed this in our updated paper.\n\n> This sentence reads a bit too colloquially.\n\nWe have changed this sentence to : \u201cThe space of drug-like molecules is vast, and reinforcement learning methods hold great promise in improving the speed and reducing the cost of drug discovery.\u201d\n\n> However, the corresponding transition function induced in the graph representation of molecules is more complex as it is determined by the encoding/decoding rules of the chosen text representation\n\nThese transition dynamics are determined by the encoding / decoding rules of the chosen text representation. But the underlying constraints occur due to valencies and other chemical constraints that come into play when combining atoms and bonds to form molecules.\n\n> Please correct the typos \"pertaining\" and \"pretrianing\".\n\nWe have corrected these typos in the updates paper.\n\n>  Please correct.\n\nWe have corrected this in the updated paper, thank you.\n\n> How does the \"pretraining only\" results (left column) are computed? Is this a zero-shot evaluation? \n\nThe pretraining only scores are calculated by sampling molecules from the pretrained model, without any RL training. We have added a sentence to clarify this in the updated paper (section 5.2, 1st para) : \u201cWe compare the scores of the molecules generated by the policy after pretraining and after RL training.\u201d"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191725295,
                "cdate": 1700191725295,
                "tmdate": 1700191725295,
                "mdate": 1700191725295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ubhYYIiPiz",
                "forum": "nqlymMx42E",
                "replyto": "VCLpsbKYLt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_feem"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_feem"
                ],
                "content": {
                    "comment": {
                        "value": "I acknowledge that I have read the answers from the authors and the other reviewers. I thank the authors for their answers. I maintain that the paper is a valuable contribution that I recommend for acceptance."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516545518,
                "cdate": 1700516545518,
                "tmdate": 1700516545518,
                "mdate": 1700516545518,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f1EDHV3cdr",
            "forum": "nqlymMx42E",
            "replyto": "nqlymMx42E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6203/Reviewer_AgdE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6203/Reviewer_AgdE"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose Transformer-based approach with Reinforcement Learning tuning for molecular generation. Approach is tested using different datasets and wide ablation study is conducted."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Promising approach for the molecular generation. Experiments are conducted using three different datasets and impact of each of them is shown. \n\nGood ablation study includes the design choices, different models architectures and even other RL algorithms beside REINFORCE are tested."
                },
                "weaknesses": {
                    "value": "**Relevant work**\n\nSome relevant works seem to be missing: \n\n* MolGPT (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10232454/) also uses Transformer for molecular generation but without RL.\n\n* Taiga (https://pubs.acs.org/doi/10.1021/acs.jcim.1c00600) uses Transformer and REINFORCE algorithm for the molecular generation with different properties optimization.\n\n* Please also consider citing the relevant parallel work which applies offline RL for the similar problem: https://openreview.net/forum?id=olMBz7gxls (once it is public)\n\nThe second work seems to be very similar in terms of approach and I'm not sure about the novelty of the approach itself.\n\n** Results presentation **\n\n* I haven't noticed the comparison against any other methods.\n\n* From the plots I couldn't understand what \"Div.\" and \"Red.\" columns stand for.\n\n* I couldn't find reported scores for different tasks (e.g., similarity, QED or SA). It would be nice to see them.\n\n* There are no samples of generated molecules. RL could hack reward functions and generate molecules which make no sense. Reward hacking is mentioned by the authors but including resulting molecules is essential."
                },
                "questions": {
                    "value": "Please include relevant work which is mentioned in the **Weaknesses**\n\n* What are the principal differences between you approach and Taiga?\n\n* Can you compare against MolGPT, Taiga and REINVENT (or similar approach)? \n\n* What are \"Div.\" and \"Red.\" columns?\n\n* What are the scores for different reward components (e.g. QED)? \n\n* Can you please provide examples of generated molecules? The picture of molecular graphs and real drugs for given targets is fine.\n\n* How did you make run docking process so fast? From my experience even GPU-accelerated programs for docking are quite slow. Is acceleration achieved because the docking is performed to the fixed target protein?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Reviewer_AgdE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698166228544,
            "cdate": 1698166228544,
            "tmdate": 1700609671110,
            "mdate": 1700609671110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZXomyntmxe",
                "forum": "nqlymMx42E",
                "replyto": "f1EDHV3cdr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to AgdE (1  / 2)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe thank the reviewer for their feedback and summary of our work. We appreciate the reviewer\u2019s assessment that the paper proposes a promising approach for molecular generation with extensive, relevant ablations. It seems like the reviewer\u2019s main concerns are missing relevant work, presentation of the results, and examples of generated and hacked molecules. We have modified our paper to incorporate these concerns:\n\n1) In Table 1 we add the relevant citations and a conceptual comparison with our work.\n2) In Table 6,7,8,9 we add individual scores of the pretrained and RL trained agent on all PyTDC and docking tasks. \n3) In Figure 14, we include molecules that are sampled as a result of reward hacking.\n4) In Figure 18 - 40, we add the top 5 molecules corresponding to all 21 PyTDC tasks.\n\n **Do these new updates and experiments, with our answers to specific questions, address all the reviewer's concerns?**\n\n> Some relevant works seem to be missing:\n\nWe have added the first two papers (MolGPT, Taiga) in Table 1 of our updated paper, and can cite the third one as concurrent work after it is public.\n\n> The second work seems to be very similar in terms of approach \n \nIndeed, the second work is similar to ours in that both papers use a pre-trained transformer and train it using policy gradients, and we have added both papers in Table 1 of our updated paper. The difference is in the scope of our experiments and the large number of other design choices combined in a novel manner. For example, \n1) Both the papers mentioned by you only show results for 2 or 3 different tasks, which include QED or SA scores. These tasks have been categorized as substantially easier to optimize in the literature compared to the tasks in our experiments [1], which span 25 + tasks including docking scores. \n2) In previous papers, the size of the datasets used were around 1 million molecules. We conduct experiments with sizes up to 100 million molecules.\n3) Moreover, we also study algorithmic components like regularization and the use of replay buffers which seem to have a huge impact on the results, but are not studied in the papers you mention.\n4) Our experiments also include more architectural diversity, including fully-connected MLPs, RNNs and Transformer backbones.\n\n> I haven't noticed the comparison against any other methods.\n\nThere are not many prior works that have been applied to text-based representations of molecules. Hence our goal was to extensively compare various design and algorithmic choices. However, many of our ablations can be recombined in different ways to recover prior algorithms. In Table 1, we show how many prior algorithms can be categorized by the design choices that we compare in our experiments. \n\n> What are \"Div.\" and \"Red.\" columns?\n\nThese mean diversity and redundancy respectively. We have added the clarification in Section 5.1, evaluation metrics paragraph and also in the caption of Figure 2.\n\n> I couldn't find reported scores for different tasks (e.g., similarity, QED or SA). \n\nIn Table 6,7,8,9, we have added the exact individual scores achieved by just the pretrained model and the benefits achieved from reinforcement learning on these pretrained models across all 26 docking and PyTDC tasks. These tables show that the RL algorithm improves the pretrained model by 69% and 35% on docking and PyTDC tasks respectively.\n\n> Reward hacking is mentioned by the authors but including resulting molecules is essential.\n\nIn Figure 14 (Page 25) we have added examples of molecules sampled due to reward hacking. We can see that ChemRLformer agents are able to obtain unusually high docking scores by stacking together long chains and rings of sulfur, phosphorus or carbon atoms.\n\nBelow, we will answer the specific reviewer questions and weaknesses. **Do these revisions and answers address all the reviewer's concerns?**\n\n>  What are the principal differences between your approach and Taiga?\n\nThe largest difference is the scale of our experiments. Our experiments are much more extensive when compared to Taiga:\n1) Num of tasks \u2013  ours : 25, Taiga : 2.\n2) Docking scores \u2013 ours : Yes, Taiga : No.\n3) Largest datasets used \u2013 ours : 100 million, Taiga : 1 million.\n4) Architectures compared \u2013 ours : FC, Transformer, RNN. Taiga : Transformers.\n5) Important Algorithmic choices \u2014 ours : hill climb replay buffers, log p regularization. Taiga : None.\n\nWe also have other contributions that differentiate us from prior work:\n\n1) We show that current docking functions are not perfect, and can be easily hacked by ChemRLformer.\n2) We show that when choosing a pre-training datasets, the alignment matters more than its size.\n3) We show that transformers aren\u2019t superior to RNNs for current string-based RL algorithms, at the scale of data currently available."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191608514,
                "cdate": 1700191608514,
                "tmdate": 1700191608514,
                "mdate": 1700191608514,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SQByHMKRyW",
                "forum": "nqlymMx42E",
                "replyto": "gs65G5U7nr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_AgdE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_AgdE"
                ],
                "content": {
                    "comment": {
                        "value": "I thank authors for their rebuttal work. My concerns are resolved. I've increased the score and confidence."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609734977,
                "cdate": 1700609734977,
                "tmdate": 1700609734977,
                "mdate": 1700609734977,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7Kyx94iXss",
            "forum": "nqlymMx42E",
            "replyto": "nqlymMx42E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method for molecule design using language models and RL. First, a GPT-style model is pre-trained in an autoregressive fashion. The model is then finetuned using REINFORCE algorithm to generate high-value molecules. The authors discuss several design choices and perform ablation studies to confirm their hypotheses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem is of high importance\n1. The paper is well-written\n1. The numerical results are well documented and contain ablation studies"
                },
                "weaknesses": {
                    "value": "1. Some of the design choices seem to be dated. For example, \n    * it is not clear why REINFORCE is used as the policy learning algorithm, as opposed for instance to PPO. \n    * there is no automatic entropy tuning for exploration.\n    * Reinforcement Learning with Human Feedback can be also an interesting approach to try\n1. The details for the backbone language model as scarce. It would be also interesting to discuss the design choices for language modeling, e.g., pretraining architecture (BERT vs GPT),  tokenizer etc\n1. It would be good to see if increasing the transformer size would lead to performance improvement. If I am not mistaken increasing the RNN size could lead to issues while training and therefore this could potentially become the strength of the paper."
                },
                "questions": {
                    "value": "1. A most popular approach nowadays to finetuning transformers is reinforcement learning with human feedback. In RLHF, the reward preference model is trained to choose between several samples of the transformer and then the fine-tuning is performed using PPO algorithm. Have the authors considered this approach? \n1. It is not clear why the authors use Reinforce instead of more popular PPO for example. I appreciate that the authors did an ablation study on the KL constraint, but PPO has other benefits in comparison with Reinforce \n1. Details of the backbone model are not given. Is it a standard GPT model? Was the tokenizer standard or also trained? \n1. It is not clear why the authors chose GPT-style model as opposed to BERT style model, where the whole sequences can be predicted directly. This approach was used, e.g  by Cowen-Rivers et al albeit for a different problem and with a different RL algorithm. A discussion on the subject would be interesting.\n1. Haarnoja et al 2018 proposed an automatic tuning procedure of entropy that can explicitly state the target level of entropy. It would be interesting to have this as ablation as well. \n1. I am confused about referencing Mnih et al 2013 regarding the use of replay buffer for on-policy algorithms. Don\u2019t Mnih et al use an off-policy algorithm? \n1. Furthermore, I am not quite sure what a replay buffer for on-policy algorithm means. Does it mean that we use samples for several previous iterations not just the most recent one?\n1. The results could be displayed better, with this scale the improvement of the model does not seem too large, but could be deceptive. \n1. In Figure 2 CheMBL offers high diversity and high redundancy results, which seems confusing at first. Please comment in the figure caption\n\nUPD:\nReferences :\n* Cowen-Rivers, Alexander I., et al. \"Structured Q-learning For Antibody Design.\" arXiv preprint arXiv:2209.04698 (2022).   \n* Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689107373,
            "cdate": 1698689107373,
            "tmdate": 1700515345647,
            "mdate": 1700515345647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ylji3jRyYq",
                "forum": "nqlymMx42E",
                "replyto": "7Kyx94iXss",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to ANoo (1 / 2)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe thank the reviewer for their feedback. We appreciate the reviewer\u2019s assessment that the paper is of high importance and well-written with extensive experiments and ablation studies. It appears that the reviewer's main concerns are the choice of using the REINFORCE algorithm, additional experiments, details of the model and presentation of results. We address the concerns using new experiments and updates to the paper:\n\n1) Figure 13 compares REINFORCE with PPO across 21 tasks. This result supports our choice of using REINFORCE which achieves superior performance while being simpler. \n\n2) We add Table 6,7,8,9 which contain individual scores of the pretrained and RL trained agent on all PyTDC and docking tasks.\n\n3) We update appendix A to contain all details related to the experiments, including details of the size and hyperparameters of all pretraining models in appendix A.3, para \u201cpre training models\u201d.\n\n**Do these new experiments, with our answers to specific questions, address all the reviewer's concerns?**\n\n> It is not clear why the authors use Reinforce instead of more popular PPO for example\n\nAlthough PPO has been shown to generally be a better algorithm for control problems, the comparison is not clear in the case of molecular optimization ([1,2]) . We perform an experiment across 21 PyTDC objectives (see Figure 13, page 22) to compare REINFORCE with PPO. Our results indicate that vanilla policy gradient algorithms achieve higher performance on all metrics and are more stable when compared to actor critic algorithms like PPO. This experiment guided the choice of using REINFORCE over PPO.\n\n>  finetuning transformers is reinforcement learning with human feedback.\n\nWhile we agree that incorporating feedback from humans is a promising future direction, one reason why this will be more difficult in the space of molecular optimisation is that the humans that would be needed to give feedback are expert chemists. Given the scarcity of experts, the RLHF framework may not scale well for this use case. Indirectly, those experts are performing this already by creating simulations (e.g. Autodock) which we use in our paper.\n\n> The details for the backbone language model are scarce.\n\nIn appendix A.3, in paragraph \u2018pretraining models\u2019 we have included the number of parameters and architecture specification of all the models used.\n\n> automatic tuning procedure of entropy that can explicitly state the target level of entropy\n\nThe logp regularization we applied is similar to entropy regularization in that both regularize the log probability of the policy. Entropy regularization reduces log(prob), while logp regularisation increases 1 / log(prob). We add this clarification in the last paragraph of section 5.4\n\n> The details for the backbone language model \n\nIn appendix A, we have included Table 4 and Table 5, which contains all the details on the pretraining models.  \n\n> It would be good to see if increasing the transformer size helps\n\nWe agree that this would be promising. But more importantly, our work uncovers an immediate problem of reward hacking of the docking functions that needs to be solved by domain experts. This is a problem that cannot be solved by increasing the transformer size alone. In addition, to make good use of a larger tranformer, we would also need larger, more diverse data, which is currently limited in the community.\n\n> In RLHF, the reward preference model \n\nLearning a reward preference model needs a large amount of expert human feedback. Rewards are limited in real-world chemistry, unlike general language chatbots. We need methods that use minimal rewards. Hence, we focus on more sample efficiency [1]. \n\n> It is not clear why the authors use Reinforce instead of more popular PPO for example.\n\nWe add a new Figure 13 (page 22) to compare REINFORCE with PPO across 21 molecular optimization tasks.  We found that for these tasks, more complicated approaches were more unstable than REINFORCE, which seems to perform better. These results resonate with other work in the molecular optimisation community [1,2]\n\n> I am confused about referencing Mnih et al 2013 regarding the use of replay buffer for on-policy algorithms\n\nYou are correct in that Mnih et al 2013 use an off-policy algorithm, while we use REINFORCE which is on policy. We use replay buffers [3] to store high-scoring molecules and re-sample them to make REINFORCE updates, making the algorithm off-policy.  In appendix A.5, we add details in our paper about how this is done in practice.\n\n> Does it mean that we use samples for several previous iterations, not just the most recent one?\n\nYes. We store a small number of previously seen high-scoring molecules and apply REINFORCE updates on them. Although this makes the algorithm slightly off policy, it vastly improves the performance (Figure 5). In appendix A.5, we add details in our paper about how this is done in practice."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191318048,
                "cdate": 1700191318048,
                "tmdate": 1700191318048,
                "mdate": 1700191318048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DXt4iT6s4B",
                "forum": "nqlymMx42E",
                "replyto": "ylji3jRyYq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
                ],
                "content": {
                    "title": {
                        "value": "follow-up clarifications"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response and the new detailed experiments. You addressed many of my concerns, but before finalizing the assessment, I want to follow up with a few more clarifications:\n1. `Re: Reinforce vs PPO`. Could the authors provide the details for the Reinforce and PPO? Specifically, I am curious if the KL penalty, the replay buffer, and the entropy regularization were used in both algorithms. I am interested if these additions made a difference or if there's something inherent in this problem that makes Reinforce a better option. \n1. `Re: entropy tuning`. Perhaps there is a misunderstanding regarding automatic tuning. One could have a weight parameter $-\\alpha \\log(\\pi)$ instead of unweighted entropy $-\\log(\\pi)$. The parameter $\\alpha$ can be chosen through hyperparameter search or tuned automatically. If I understood correctly the current approach sets $\\alpha = 1$, and the question is if you have considered automatic tuning or reweighting. \n2.  `Re: replay buffer`. I see your point, and I think my confusion came from this sentence:\n`Although text-based  RL algorithms are trained on-policy, prior work has proposed using a replay buffer to improve performance [Mnih et al., 2013].` This seems to imply that Mnih et al proposed using a replay buffer for on-policy algorithms, which is not correct. I recommend rephrasing it.\n3. I recommend adding a quick discussion (perhaps in the appendix) regarding RLHF and why it's preferable to use other RL methods. Also, note that the reward model can potentially be trained using simulators, as well.\n\nPS. In the future, I recommend highlighting the changes in the PDF with a different color to make the evaluations a bit easier."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331969765,
                "cdate": 1700331969765,
                "tmdate": 1700331969765,
                "mdate": 1700331969765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1Toq11FmWW",
                "forum": "nqlymMx42E",
                "replyto": "XPn5MEInrf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the clarifications!"
                    },
                    "comment": {
                        "value": "Thank you for the quick response! \n\nA few final recommendations:\n\n1. Please add the details for the PPO vs Reinforce comparison to the appendix (editing the figure caption would suffice). I think this discussion could be interesting. \n\n2. Regarding the entropy for the discrete SAC this is the reference that introduces it:\n\n```\n@article{christodoulou2019soft,\n  title={Soft actor-critic for discrete action settings},\n  author={Christodoulou, Petros},\n  journal={arXiv preprint arXiv:1910.07207},\n  year={2019}\n}\n```\n\nOn a personal note, I never had any luck with the target entropy of $-0.98 \\log(1/ |A|)$ in the discrete case, which seems to promote a purely random policy."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700412131746,
                "cdate": 1700412131746,
                "tmdate": 1700412131746,
                "mdate": 1700412131746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DgiBCQfirU",
                "forum": "nqlymMx42E",
                "replyto": "ALrlEpyLX0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Reviewer_ANoo"
                ],
                "content": {
                    "title": {
                        "value": "Apologies for a late reply!"
                    },
                    "comment": {
                        "value": "I lowered my confidence in the score but raised the overall score. \n\nI realized that I didn't add the references in my review. I updated the review to reflect these references. Feel free to have a look!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515452151,
                "cdate": 1700515452151,
                "tmdate": 1700515452151,
                "mdate": 1700515452151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6ERUYhkHAV",
            "forum": "nqlymMx42E",
            "replyto": "nqlymMx42E",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6203/Reviewer_QKJh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6203/Reviewer_QKJh"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the potential of reinforcement learning (RL) methods to discover new, high-value molecules and presents a new RL-based molecular design algorithm called ChemRLformer. The authors conduct extensive experiments and analysis to show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work. The paper provides unique insights into the application of RL to molecular design and highlights the importance of careful search space structuring and algorithm design."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper addresses an important problem in the field of molecular design, which has significant implications for society. The potential of RL methods to discover new, high-value molecules could have a major impact on drug discovery, materials science, and other fields.\n\n2. The paper presents a new RL-based molecular design algorithm called ChemRLformer, which achieves state-of-the-art performance while being more straightforward than prior work. The authors provide unique insights into the application of RL to molecular design and highlight the importance of careful search space structuring and algorithm design.\n\n3. The paper is well-written and easy to understand, even for readers who may not be familiar with RL or molecular design. The authors provide clear explanations of their methods and results, and use visual aids to help illustrate their points.\n\n4. The authors conduct extensive experiments and analysis using 25 molecule design tasks, including computationally complex protein docking simulations. They explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy\u2019s ability to generate molecules with desired properties."
                },
                "weaknesses": {
                    "value": "1. It is confusing why the authors assume a discount rate of 1 in Section 4.1. Even with a finite trajectory, a discount rate smaller than 1 is not obligatory, it can also help. A discount rate of 1 can prevent the agent from learning and executing long-term tasks, as it won't appropriately discount long-term returns, leading it to prioritize immediate rewards and neglect long-term benefits. It is suggested to justify this setting.\n\n2. REINFORCE is a very classic yet old algorithm. It is highly recommend to try more recent algorithms, e.g., TROP or PPO. I am not an exert in RL for molecular optimization, but I doubt whether it is the state-of-the-art RL algorithm in this field. For example, (i) due to its reliance on stochastic policies and simple optimization methods, REINFORCE can be more susceptible to getting stuck in local optima, making it less effective in complex and high-dimensional environments; (ii) due to its on-policy mechanism, REINFORCE requires a large number of samples to estimate gradients accurately. This can make it computationally expensive and slow for complex tasks and environments. \n\n3. It would be more convincing if domain experts can help to judge the effectiveness of ChemRLformer from molecular's perspective."
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6203/Reviewer_QKJh"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6203/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699316658590,
            "cdate": 1699316658590,
            "tmdate": 1700787023366,
            "mdate": 1700787023366,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TGftpOMzM4",
                "forum": "nqlymMx42E",
                "replyto": "6ERUYhkHAV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6203/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6203/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to QKJh"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nWe thank the reviewer for their feedback. We appreciate the reviewer\u2019s assessment that the paper addresses an important problem with a new algorithm, is well-written and provides an extensive analysis. It appears that the reviewer's main concerns are regarding the discount factor, the choice of REINFORCE and a shortage of domain experts' perspective on the effectiveness of ChemRLformer. We have incorporated new updates Section 4 (footnote on page 5 explaining the use of discount factor 1) and experiments :\n\n1) Figure 13 compares REINFORCE with PPO across 21 tasks PyTDC. This result supports our choice of using REINFORCE which achieves superior performance while being simpler. \n\n2) In Appendix D, we added remarks by domain experts on ChemRLformer.\n\n **Do these new experiments, with our answers to specific questions, address all the reviewer's concerns?** We address all of these concerns in detail below:\n\n> It is confusing why the authors assume a discount rate of 1 in Section 4.1\n\nThe primary reason why we set the discount factor as 1 is related to the problem setting. During search, it is common to set a maximum length of the molecule (100 in our experiments) that the agent can sample. Hence, the MDP is a finite-length MDP. Rewards are only obtained once in an episode, when the agent samples the stop action or the length of the molecule hits the maximum length. Having a discount factor of 1 is an intentional choice because discount factors less than one bias the agent towards molecules with shorter lengths. This bias is undesirable in practice.\n\n> REINFORCE is a very classic yet old algorithm.\n\nREINFORCE is a very classic yet old algorithm. It is highly recommended to try more recent algorithms, e.g., TROP or PPO. Although PPO has generally been shown to be a better algorithm for control problems, the comparison is not clear in the case of molecular optimization ([1,2]) . We perform an experiment across 21 PyTDC objectives (see Figure 13) to compare REINFORCE with PPO. Our results indicate that vanilla policy gradient algorithms achieve higher performance on all metrics and are more stable when compared to actor-critic algorithms like PPO. This experiment guided the choice of using REINFORCE over PPO.\n\n> It would be more convincing if domain experts can help to judge the effectiveness of ChemRLformer from molecular's perspective.\n\nWe include comments of a Domain expert who has read the paper: They find the versatility of ChemRLformer in solving multiple tasks, especially expensive tasks like docking, useful for early-stage drug discovery. The reward-hacking insights are particularly useful as they show the shortcomings of docking score evaluations and the need for better evaluation, both in simulation and real-world experiments.\n\nWe will add a deeper discussion on domain expert and application perspectives in the camera-ready version.\n\n\n[1] Cieplin\u0301ski, Tobiasz, et al. \"Generative Models Should at Least Be Able to Design Molecules That Dock Well: A New Benchmark.\" Journal of Chemical Information and Modeling (2023).\n\n[2] Gao, Wenhao, et al. \"Sample efficiency matters: a benchmark for practical molecular optimization.\" Advances in Neural Information Processing Systems 35 (2022): 21342-21357."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6203/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191180459,
                "cdate": 1700191180459,
                "tmdate": 1700191180459,
                "mdate": 1700191180459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]