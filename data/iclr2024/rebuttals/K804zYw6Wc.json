[
    {
        "title": "NIR-Assisted Image Denoising: A Selective Fusion Approach and A Real-World Benchmark Dataset"
    },
    {
        "review": {
            "id": "14KavfcYJv",
            "forum": "K804zYw6Wc",
            "replyto": "K804zYw6Wc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2435/Reviewer_KiWB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2435/Reviewer_KiWB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an NIR-assist RGB image denoising method, which mainly includes two parts: First, an effective SFM module is proposed to help the selective fusion of NIR and RGB images and solve the content inconsistency between NIR-RGB images. And the module is plug-and-play. Secondly, a real-world NIR-Assisted Image Denoising (NAID) dataset is proposed."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors conduct experiments to verify the proposed method for denoising images on both synthetic and proposed real-world datasets. Ablation studies are conducted to validate the effectiveness of the two contributions. The paper looks technically sound and describes the algorithm clearly."
                },
                "weaknesses": {
                    "value": "I am concerned that the overall contributions are trivial. Especially, the proposed SFM module should be provided with more design reasons.\n\nWeaknesses:\n\n1. For GMM and LMM, NIR and RGB are concatenated along channel dimensions to input the same subsequent modules, and then split to obtain the two estimated NIR weights and RGB weights to apply to the corresponding branches, respectively. It seems to me that two estimated weights are the same, how to achieve selective fusion? Or are manually setting parameters involved? It would be better to provide some justification or motivation for the design choices.\n2. Table 7 is not mentioned in the paper."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2435/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2435/Reviewer_KiWB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698627061445,
            "cdate": 1698627061445,
            "tmdate": 1699636179223,
            "mdate": 1699636179223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eT2lTMjYb1",
                "forum": "K804zYw6Wc",
                "replyto": "14KavfcYJv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KiWB (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions. We appreciate the reviewer's questions and hope our responses can address the concerns.  \n\n`1.` **Overall contributions.**\n\nPrevious related works perform NIR-assist image denoising on synthetic noisy images. The most important contribution of this work is to introduce this into the real world. We will revise the paper to make the contribution clear. \n \nMoreover, other main contributions can be summarized as follows:  \n\n(1) Towards the content inconsistency issue, we propose a simple yet efficient selective fusion module (SFM) that can be seamlessly integrated into existing denoising networks with few increasing computation costs.\n  \n(2) Towards the real-world dataset scarcity issue, we construct a real-world NIR-assisted image denoising (NAID) dataset that covers diverse scenarios and various noise levels, which is expected to serve as a benchmark for future works.  \n\n(3) Extensive experiments demonstrate that our method achieves better results than state-of-the-art ones.  \n\nIn addition, we promise the dataset, codes, and pre-trained models will be publicly available, which may be advantageous for future studies.\n\n`2.` **Motivation of the SFM module.**\n\nColor and structure inconsistency issues between NIR-RGB images are the main challenges in NIR-assisted image denoising. First, the NIR image is captured under additional NIR light and is monochromatic, which leads to color discrepancy between NIR-RGB images. Second, the NIR image may 'more-see' or 'less-see' the objects than the visible light ones, primarily due to inherent differences in the optical properties within each spectral domain, which leads to structure discrepancy between NIR-RGB images. \n \nOn the one hand, we hope to design a module to address two inconsistency issues in this work. Thus, we present a global modulation module (GMM) and a local modulation module (LMM) to handle the color and structure inconsistencies, respectively. The combination of the two modules forms SFM. On the other hand, we hope the module can be simple, efficient, and plug-and-play. Thus, we intentionally avoid some complex operations (e.g., self-attention and cross-attention) during design in order to maintain efficiency as much as possible.  \n\nIn addition, in the second response of '**Response to Reviewer YVkk**', we further elaborate on the advantages and characteristics of the proposed SFM.  \n\nWe will revise the relevant descriptions to make SFM clearer in the revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679603444,
                "cdate": 1700679603444,
                "tmdate": 1700679983929,
                "mdate": 1700679983929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uRLBGngfYU",
                "forum": "K804zYw6Wc",
                "replyto": "14KavfcYJv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KiWB (2/2)"
                    },
                    "comment": {
                        "value": "`3.` **How to achieve selective fusion?**\n\nWe apologize for the misunderstanding caused by the unclear description of the proposed SFM. Here we provide a more detailed and clear description. \n \nSFM consists of a GMM, an LMM, and a fusion operation, where GMM mainly handles the color discrepancy and LMM handles the structure one. It is worth noting that SFM automatically predicts the weights for RGB and NIR features, without manually setting parameters. And the selective fusion is mainly inspired by \u2018Selective Kernel Networks\u2019[a].  \n\nSpecifically, for GMM, it first concatenates NIR features $\\mathbf{F_{N}} \\in \\mathbb{R}^{C \\times H \\times W}$ and RGB ones $\\mathbf{F_{R}} \\in \\mathbb{R}^{C \\times H \\times W}$ as inputs, generating initial weights $\\mathbf{W_{init}^{g}} \\in \\mathbb{R}^{2C \\times H \\times W}$ by a $1\\times1$ convolutional layer, two $1\\times1$ blocks for feature enhancement and another $1\\times1$ convolutional layer. It can be written as, \n$$\\mathbf{W_{init}^{g}} = \\mathrm{Conv_{1\\times1}}(\\mathrm{Blocks_{1\\times1}}(\\mathrm{Conv_{1\\times1}}([\\mathbf{F_{N}},\\mathbf{F_{R}}]))),$$\n\nwhere $\\mathrm{Conv_{1\\times1}}$ denotes the $1\\times1$ convolutional layer and $\\mathrm{Blocks_{1\\times1}}$ denotes two $1\\times1$ blocks. Next, $\\mathbf{W_{init}^{g}}$ is split along channel dimension to get NIR weight maps $\\mathbf{\\tilde{W} _{N}^{g}} \\in \\mathbb{R}^{C \\times H \\times W}$ and RGB ones $\\mathbf{\\tilde{W} _{R}^{g}} \\in \\mathbb{R}^{C \\times H \\times W}$. Then, a \nchannel-wise softmax operation is applied to  $\\mathbf{\\tilde{W} _{N}^{g}}$ and  $\\mathbf{\\tilde{W} _{R}^{g}}$, getting the final NIR modulation weights $\\mathbf{W}^g _\\mathbf{N} \\in \\mathbb{R}^{C \\times H \\times W}$ and RGB ones $\\mathbf{W}^g _\\mathbf{R} \\in \\mathbb{R}^{C \\times H \\times W}$, which can be written as,\n$$ [\\mathbf{W}^g _\\mathbf{N}] _{i} = \\frac{\\exp([\\mathbf{\\tilde{W}}^g _\\mathbf{N}] _{i})}{\\exp([\\mathbf{\\tilde{W}}^g _\\mathbf{N}] _{i}) + \\exp([\\mathbf{\\tilde{W}}^g _\\mathbf{R}] _{i})},  \\qquad[\\mathbf{W}^g _\\mathbf{R}] _{i} = \\frac{\\exp([\\mathbf{\\tilde{W}}^g _\\mathbf{R}] _{i})}{\\exp([\\mathbf{\\tilde{W}}^g _\\mathbf{N}] _{i}) + \\exp([\\mathbf{\\tilde{W}}^g _\\mathbf{R}] _{i})}.$$\n\nwhere $[\\ \\cdot\\ ] _{i}$ denotes the selection of the $i$-th channel. Finally, we modulate the NIR features $\\mathbf{F} _\\mathbf{N}$ and the RGB ones $\\mathbf{F} _\\mathbf{R}$ with $\\mathbf{W}^g _\\mathbf{N}$ and $\\mathbf{W}^g _\\mathbf{R}$, respectively, i.e.,\n$$\n\\mathbf{F}^g _\\mathbf{N} = \\mathbf{W}^g _\\mathbf{N} \\odot \\mathbf{F _{N}},\n\\quad\n\\mathbf{F}^g _\\mathbf{R} = \\mathbf{W}^g _\\mathbf{R} \\odot \\mathbf{F _{R}}, \n$$\n\nwhere $\\mathbf{F}^g _\\mathbf{N} \\in \\mathbb{R}^{C \\times H \\times W}$ and $\\mathbf{F}^g _\\mathbf{R} \\in \\mathbb{R}^{C \\times H \\times W}$ are the globally modulated NIR features and the RGB ones, respectively. \n\nIn order to keep simplicity and capture more local information, LMM is built upon GMM by replacing the $1\\times1$ convolutional layer in $1\\times1$ blocks with a $5\\times5$ depth-wise convolutional layer. As its processing is similar to GMM, we do not elaborate on it cumbersomely. \n\nWe will revise the relevant descriptions to make selective fusion clearer in the revision.\n\n`4.` **Table 7 is not mentioned in the paper.**\n\nThanks for pointing it out, we will rectify it in the revision by illustrating Table 7 in Sec. 6.2.\n\nReference \n\n[a] Li, Xiang, et al. \"Selective kernel networks.\" CVPR. 2019."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681788414,
                "cdate": 1700681788414,
                "tmdate": 1700740070760,
                "mdate": 1700740070760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m1wYZ4iqLc",
            "forum": "K804zYw6Wc",
            "replyto": "K804zYw6Wc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2435/Reviewer_D7EG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2435/Reviewer_D7EG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an efficient Selective Fusion Module (SFM) for NIR-Assisted Image Denoising (NAID) and presents a real-world dataset covering diverse scenarios and noise levels. The proposed method achieves better results than state-of-the-art techniques, and the dataset serves as a benchmark for future research. The SFM decouples color and structure discrepancy issues and addresses them with Gaussian Mixture Model (GMM) and Laplacian Mixture Model (LMM), respectively. The compact and lightweight network design adds few parameters and computation costs and can be integrated into existing advanced denoising networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed SFM achieves significant performance improvements while maintaining interpretability.\n2. The NAID dataset covers diverse scenarios and noise levels, making it a valuable benchmark for future research.\n3. The compact and lightweight network design adds few parameters and computation costs and can be integrated into existing advanced denoising networks."
                },
                "weaknesses": {
                    "value": "1. Relative to other NIR-assist denoising approaches, the innovation quotient of the proposed method appears constrained, potentially limiting its adaptability across diverse imaging conditions and noise levels.\n2. Concerns arise regarding the efficiency of the methodology, particularly when juxtaposed with alternative techniques, as evidenced by its prolonged training durations.\n3. The introduced SFM module lacks explicit clarity on addressing pivotal challenges inherent to NIR-assist operations."
                },
                "questions": {
                    "value": "1.How does the proposed SFM compare to other denoising techniques in terms of computational efficiency?\n2.Can the proposed method be applied to other types of images, such as medical images or satellite images?\n3.How does the NAID dataset compare to other benchmark datasets in terms of diversity and size?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732256230,
            "cdate": 1698732256230,
            "tmdate": 1699636179139,
            "mdate": 1699636179139,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iVQAzW2TsF",
                "forum": "K804zYw6Wc",
                "replyto": "m1wYZ4iqLc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D7EG (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions. We appreciate the reviewer's questions and hope our responses can address the concerns.\n\n`1.` **Overall contributions and innovation quotient.**\n\nPrevious related works perform NIR-assist image denoising on synthetic noisy images. We kindly remind the reviewer that the most important contribution of this work is to introduce this into the real world. We will revise the paper to make the contribution clear. \n\nMoreover, other main contributions can be summarized as follows:  \n\n(1) Towards the content inconsistency issue, we propose a simple yet efficient selective fusion module (SFM) that can be seamlessly integrated into existing denoising networks with few increasing computation costs.  \n\n(2) Towards the real-world dataset scarcity issue, we construct a real-world NIR-assisted image denoising (NAID) dataset that covers diverse scenarios and various noise levels, which is expected to serve as a benchmark for future works.  \n\n(3) Extensive experiments demonstrate that our method achieves better results than state-of-the-art ones.  \n\nIn addition, we promise the dataset, codes, and pre-trained models will be publicly available, which may be advantageous for future studies.\n\n`2.` **The introduced SFM module lacks explicit clarity on addressing pivotal challenges inherent to NIR-assist operations.**\n\nHere we elaborate on the motivations of SFM.  \n\nColor and structure inconsistency issues between NIR-RGB images are the main challenges in NIR-assisted image denoising. First, the NIR image is captured under additional NIR light and is monochromatic, which leads to color discrepancy between NIR-RGB images. Second, the NIR image may 'more-see' or 'less-see' the objects than the visible light ones, primarily due to inherent differences in the optical properties within each spectral domain, which leads to structure discrepancy between NIR-RGB images.  \n\nOn the one hand, we hope to design a module to address two inconsistency issues in this work. Thus, we present a global modulation module (GMM) and a local modulation module (LMM) to handle the color and structure inconsistencies, respectively. The combination of the two modules forms SFM. Moreover, the visualization results in Figure A of the appendix show that GMM indeed enables better color and LMM indeed enables better structure.  \n\nOn the other hand, we hope the module can be simple, efficient, plug-and-play. Thus, we intentionally avoid some complex operations (e.g., self-attention and cross-attention) during design in order to maintain efficiency as much as possible.  \n\nIn addition, in the second response of '**Response to Reviewer YVkk**', we further elaborate on the advantages and characteristics of the proposed SFM.  \n\nWe will revise the relevant descriptions to make SFM clearer in the revision.\n\n`3.` **Training and inference efficiency.**\n\nWe compare the proposed method with the most competitive one DVN [a] as well as the naive feature fusion manner (i.e., feature summation). It is worth noting that we retrain all compared methods with the same configurations, thus the comparisons are fair. As shown in the following table, our method achieves higher performance with lower training time than the compared ones, which illustrates the training efficiency of the proposed method.\n| Methods | Training Time | Inference TIme | PSNR / SSIM / LPIPS |\n|:---:|:---:|:---:|:---:|\n| DVN | 44h 50min | 761ms | 24.93 / 0.7578 / 0.332 |\n| Summation | 30h 6min | 461ms | 25.00 / 0.7628 / 0.304 |\n| Ours | 30h 13min | 462ms | 25.26 / 0.7676 / 0.303 |\n\nMoreover, we also report the inference time and \\#Flops when generating a $1792 \\times 1008$ output in the following table. It shows that our method outperforms state-of-the-art ones in inference computation cost, which further illustrates the inference efficiency of our proposed method.\n| Methods | #FLOPs | Inference Time |\n|:---:|:---:|:---:|\n| FGDNet [b] | 38.67G | 476ms |\n| SANet [c] | 161.06G | 2761ms |\n| CUNet [d] | 14.48G | 542ms |\n| MNNet [e] | 23.68G | 1360ms |\n| DVN [a] | 104.05G | 761ms |\n| Ours | 13.17G | 462ms |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681251353,
                "cdate": 1700681251353,
                "tmdate": 1700681251353,
                "mdate": 1700681251353,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DugGyuv40t",
                "forum": "K804zYw6Wc",
                "replyto": "m1wYZ4iqLc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer D7EG (2/2)"
                    },
                    "comment": {
                        "value": "`4.` **Adaptability across diverse imaging conditions and noise levels.**\n\nIt is worth noting that the proposed method is tailored to address the color and structure discrepancies between NIR-RGB images, which is an inherent issue of NIR-assisted image denoising. In other words, the issue does not disappear as imaging conditions and noise levels change. Thus, our method has the potential to be adapted to different imaging conditions and noise levels.\n\nOn the one hand, the proposed NAID dataset covers three different lighting environments, i.e., 2 lux, 5 lux, and 10 lux respectively. For these lighting conditions, our extensive experiments have validated the effectiveness of our method. For lighting conditions beyond the range, it may be more appropriate to collect new data with our data acquisition pipeline and retrain our models on it.\n\nOn the other hand, we have conducted extensive experiments on both synthetic DVD [a] and real-world NAID datasets, where the former covers two noise levels and the latter covers three ones. The results show that the proposed method enables performance improvement for all noise levels. And the noise level the higher, the improvement is more significant. It demonstrates that the proposed method can be effectively applied to different noise levels.\n\n`5.` **Application to other types of images.**\n\nThe proposed method is elaborately designed to overcome the color and structure discrepancies between NIR-RGB images. It is simple yet efficient for real-world NIR-assisted image denoising. Simultaneously, we indeed do not know so much about other image types (e.g., medical images and satellite images) and tasks. However, when a similar content inconsistency issue is presented in other images or other tasks, we believe that our method will be also valuable.\n\n`6.` **Comparison between NAID and other benchmark datasets in diversity and size.**\n\nAs far as we know, for real-world NIR-assisted denoising, Dark Flash Photography [f] is the only publicly available dataset. However, it only consists of five image pairs. It is hard to satisfy data-driven deep learning methods. \n\nIn contrast, the proposed NAID dataset includes 300 NIR-RGB paired images, and covers diverse real-world scenarios and various noise levels, which has the potential to serve as a benchmark for further studies. Moreover, we promise the proposed dataset will be publicly available, which may be advantageous for future studies.\n\nReference \n\n[a] Jin, Shuangping, et al. \"DarkVisionNet: Low-light imaging via RGB-NIR fusion with deep inconsistency prior.\" AAAI. 2022.\n\n[b] Zehua Sheng, et al. \"Frequency-domain deep guided image denoising.\" TMM. 2022.\n\n[c] Zehua Sheng, et al. \"Structure aggregation for cross-spectral stereo image guided denoising.\" CVPR. 2023.\n\n[d] Xin Deng, et al. \"Deep convolutional neural network for multi-modal image restoration and fusion.\" TPAMI. 2020.\n\n[e] Shuang Xu, et al. \"A model-driven network for guided image denoising.\" Information Fusion. 2022.\n\n[f] Zhi, Tiancheng, et al. \"Deep material-aware cross-spectral stereo matching.\" CVPR. 2018."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681736860,
                "cdate": 1700681736860,
                "tmdate": 1700740106727,
                "mdate": 1700740106727,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rJ70BVzwjF",
            "forum": "K804zYw6Wc",
            "replyto": "K804zYw6Wc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2435/Reviewer_HWD6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2435/Reviewer_HWD6"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new NIR-Assisted Image Denoising dataset which covers diverse scenarios as well as various noise levels. They also propose an efficient selection fusion module in order to address the inconsistency between NIR-RGB images. This module can be plug-and-played into the existing denoising networks to merge the NIR-RGB features. The experimental results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The new dataset is valuable.\n2. The proposed selective fusion module is simple yet effective.\n3. The overall writing quality is good."
                },
                "weaknesses": {
                    "value": "There is one place that is not clear.\nThe authors claim that \"Therefore, we deploy a GMM to handle color discrepancy first followed by an LMM dealing with structure discrepancy\". Is there any supporting evidence that the GMM handles the color discrepancy while the LMM handles the structure discrepancy?"
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813249827,
            "cdate": 1698813249827,
            "tmdate": 1699636179035,
            "mdate": 1699636179035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BJXTy3sTrh",
                "forum": "K804zYw6Wc",
                "replyto": "rJ70BVzwjF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HWD6"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions. We appreciate the reviewer's questions and hope our responses can address the concerns. \n\n`1.` **Evidence to support that GMM handles the color discrepancy while LMM handles the structure one.**\n\nWe have provided the visualization comparison results in Figure A of the appendix by removing GMM and LMM separately. It shows that GMM results in better color recovery and LMM results in better structure recovery, which supports that GMM handles the color discrepancy and the LMM handles the structure one."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679441024,
                "cdate": 1700679441024,
                "tmdate": 1700679441024,
                "mdate": 1700679441024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GlwWf4zNk4",
            "forum": "K804zYw6Wc",
            "replyto": "K804zYw6Wc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2435/Reviewer_YVkk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2435/Reviewer_YVkk"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose an efficient lightweight selective fusion module that can be plug-and-played into any denoising network to accomplish the NIR-assisted denoising. In addition, this paper constructs a corresponding real-world NIR-assisted image denoising dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Plug-and-play lightweight fusion modules can be embedded in any single-image denoising network.\nReal-world dataset is a promising solution to the current lack of data for this problem and are expected to be the baseline benchmark dataset for future research."
                },
                "weaknesses": {
                    "value": "1. The ablation experiment lacked a probe to see if the loss of multiple scales would affect the final output picture.\n1. The lightweight fusion module in this paper does not contribute enough to the technology and simply fuses the features.\n1. This paper does not demonstrate whether the improvement in denoising performance is due to the superiority of the method or only to the inclusion of NIR information."
                },
                "questions": {
                    "value": "Are the methods compared in the paper all image restoration assisted by NIR images? If not, is retraining performed for the comparison?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2435/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2435/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2435/Reviewer_YVkk"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2435/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698860016857,
            "cdate": 1698860016857,
            "tmdate": 1699636178941,
            "mdate": 1699636178941,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qpyppYr3yd",
                "forum": "K804zYw6Wc",
                "replyto": "GlwWf4zNk4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review YVkk (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggestions. We appreciate the reviewer's questions and hope our responses can address the concerns.\n\n`1.` **Effect of multi-scale loss.**\n\nWe apologize for the lack of explanations and effect of multi-scale loss. The multi-scale loss calculates the difference between output and supervision at different scales, and it can slightly improve performance with few increasing training costs. Taking NAFNet [a] as an example, we evaluate its effect by comparing it with naive $\\ell_2$ loss. The following table shows that it improves both single-image and NIR-assisted denoising performance. And the improvement is more obvious for the latter. It is worth noting that we conduct all ablation studies with the multi-scale loss, thus the comparisons between them are fair. \n| Denoising Methods | Input Images | Loss Function | PSNR / SSIM / LPIPS |\n|:---:|:---:|:---:|:---:|\n| Single-Image | RGB | Naive $\\ell_2$ | 24.72 / 0.7476 / 0.343 |\n| Singel-Image | RGB | Multi-Scale  | 24.73 / 0.7483 / 0.336 |\n| NIR-Assisted | RGB and NIR | Naive $\\ell_2$ | 25.20 / 0.7691 / 0.299 |\n| NIR-Assisted | RGB and NIR | Multi-Scale | 25.26 / 0.7695 / 0.299 |\n\nWe will add this in the revision.\n\n`2.` **The lightweight fusion module does not contribute enough and simply fuses the features.**\n\nHere we elaborate on the proposed selective fusion module (SFM) to further show its contributions. \n\n(1) The proposed SFM is a problem-oriented design module.\n\nColor and structure discrepancies between RGB and NIR images are the main challenges of NIR-assisted denoising. SFM is elaborately designed towards the issue. Specifically, it consists of a global modulation module (GMM), a local modulation module (LMM) and a fusion operation, where GMM modulates features globally to overcome the color discrepancy and LMM does that locally to overcome the structure one. \n\n(2) The proposed SFM is a simple yet efficient module.\n\nExtensive quantitative and qualitative experiment results show that the proposed method outperforms state-of-the-art ones. Besides, we have tried some prevailing methods like feature summation, channel attention [b],  self-attention [c], and cross-attention [c] in experiments. However, as shown in the following table, they do not achieve competitive performance while introducing higher computation costs.\n| Fusion Methods | PSNR / SSIM / LPIPS | Inference Time |\n|:---:|:---:|:---:|\n| Summatioin | 25.03 / 0.7647 / 0.304 | 461ms |\n| Channel Attention | 25.12 / 0.7664 / 0.309 | 461ms |\n| Self-Attentnion | 25.12 / 0.7659 / 0.307 | 1614ms |\n| Cross-Attention | 25.18 / 0.7692 / 0.304 | 1712ms |\n| Ours | 25.26 / 0.7695 / 0.299 | 462ms |\n\n(3) The proposed SFM is a plug-and-play module. \n\nIt can be seamlessly integrated into existing denoising networks with few increasing computation costs.\n\nCombining the above reasons, we finally choose the simple yet effective, plug-and-play SFM as our solution. Besides, we propose a real-world NIR-assisted image denoising (NAID) dataset that covers diverse scenarios as well as various noise levels and is expected to serve as a benchmark for future studies. More importantly, we introduce NIR-assisted image denoising from a synthetic manner into the real world.\n\nWe will revise the relevant descriptions to make SFM clearer in the revision.\n\n`3.` **Is the performance improvement due to the method's superiority or only to the inclusion of NIR information?**\n\nHere we provide a detailed and clearer description of that, and we will add it in the revision.  \n\nWe take NAFNet as a denoising backbone and compare our method with a single-image denoising manner (only using RGB images) and a naive fusion manner (i.e., simple summation of RGB features and NIR ones). The results on NAID dataset are shown in the following table. We employ PSNR, SSIM and LPIPS as evaluation metrics. \n\nOn the one hand, the inclusion of NIR information improves RGB image denoising, even simply fusing their features by summation. On the other hand, simple summation is not the optimal solution as it ignores color and structure discrepancies between NIR-RGB images. Taking this into account, the proposed SFM further improves performance, and the improvement is more significant when the noise level is higher. It illustrates the superiority of the proposed method.\n| Input Images | Fusion Methods |  Low-Level Noise | Middle-Level Noise | High-Level Noise |\n|:---:|:---:|:---:|:---:|:---:|\n| RGB | None |  25.71 / 0.7780 / 0.294 | 24.76 / 0.7482 / 0.335 | 23.71 / 0.7186 / 0.378 |\n| RGB and NIR | Summation |  25.92 / 0.7883 / 0.277 | 25.00 / 0.7682 / 0.304 | 24.16 / 0.7429 / 0.329 |\n| RGB and NIR | Our SFM | 26.06 / 0.7906 / 0.274 | 25.26 / 0.7676 / 0.303 | 24.48 / 0.7503 / 0.321 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682410373,
                "cdate": 1700682410373,
                "tmdate": 1700682410373,
                "mdate": 1700682410373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2EPRJoeSP9",
                "forum": "K804zYw6Wc",
                "replyto": "GlwWf4zNk4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2435/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review YVkk (2/2)"
                    },
                    "comment": {
                        "value": "`4.` **Whether all compared methods are assisted by NIR images? Whether they are retrained for the comparison?**\n\nIn Tables 2 and 3, the methods marked as 'Single-Image Denoising' in the top three rows do not use NIR images, and the bottom eight ones marked as 'NIR-Assisted Denoising' remove noise assisted by NIR images. \n\nAll compared methods are retrained with the same configurations on synthetic DVD [d] and our real-world NAID datasets respectively. Thus, the comparisons are fair.\n\nCompared to the most competitive one DVN [d],  our method achieves 0.39dB PSNR gain when $\\sigma = 4$ and 0.43dB PSNR gain when $\\sigma = 8$ on DVD dataset. It also achieves 0.1dB, 0.33dB, and 0.53dB improvement on PSNR in dealing with low-level, middle-level, and high-level noise on the NAID dataset, respectively.\n\nReference \n\n[a] Chen, Liangyu, et al. \"Simple baselines for image restoration.\" ECCV. 2022.\n \n[b] Zhang, Yulun, et al. \"Image super-resolution using very deep residual channel attention networks.\" ECCV. 2018.\n\n[c] Liu, Ze, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" ICCV. 2021.\n\n[d] Jin, Shuangping, et al. \"DarkVisionNet: Low-light imaging via RGB-NIR fusion with deep inconsistency prior.\" AAAI. 2022."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2435/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682531168,
                "cdate": 1700682531168,
                "tmdate": 1700740132151,
                "mdate": 1700740132151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]