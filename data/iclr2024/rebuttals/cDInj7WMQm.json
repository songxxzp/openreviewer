[
    {
        "title": "UGC: UNIVERSAL GRAPH COARSENING"
    },
    {
        "review": {
            "id": "XNigRr6nrw",
            "forum": "cDInj7WMQm",
            "replyto": "cDInj7WMQm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6082/Reviewer_Nt9N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6082/Reviewer_Nt9N"
            ],
            "content": {
                "summary": {
                    "value": "This paper gives a graph coarsening algorithm based on random projections. The nodes are repeatedly hashed using a random projections, and then assigned to vertices in the smaller graphs via a majority scheme.\n\nGuarantees of this reduction scheme are given via properties of random projections. The effectiveness of this coarsening scheme are then experimentally measured, demonstrating good efficiency, preservation of spectral properties, and in training of graph neural networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The random projection based scheme is natural, and is well known to be among the most efficient possible. The experiments are quite extensive, and demonstrate a lot of useful and intriguing properties about this coarsening scheme."
                },
                "weaknesses": {
                    "value": "Some of the formal derivations were a bit difficult to parse, e.g. in equation (1) on page 2, what is the <C_l, C_l> term? I also mistook the \\forall i \\neq j before it to be for this term too because it included a d_i.\n\nAlso, in theorem 4.2, it's not clear what the role of x is.\n\nAfter equation (5), it's not clear what the `proof in Appendix A.7' is for (is it missing a theorem statement here?)"
                },
                "questions": {
                    "value": "I had difficulties finding a concise summary of the theoretical guarantees (in terms of the graph Laplacian) proven about this coarsening scheme. Would it be possible to point to a 'main theorem' that's proven?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507367860,
            "cdate": 1698507367860,
            "tmdate": 1699636655570,
            "mdate": 1699636655570,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PXyouPIE0L",
                "forum": "cDInj7WMQm",
                "replyto": "XNigRr6nrw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Eagerly waiting for your feedback"
                    },
                    "comment": {
                        "value": "Thank you for reviewing the paper and providing feedback! We have responded below to individual quotes from your review.\n\n**Ques 1)** Some of the formal derivations were a bit difficult to parse, e.g. in equation (1) on page 2, what is the <C_l, C_l> term? I also mistook the \\forall i \\neq j before it to be for this term too because it included a d_i.\n\n**Ans 1)** We thank the reviewer for bringing this to our attention. We have utilized the condition $C_l, C_l = d_l$ to denote the degree of super-nodes. We appreciate the reviewer's feedback and have addressed the concern by removing the comma before $\\forall i \\neq j$ to enhance clarity and avoid any potential confusion in the main manuscript.\n\n**Ques 2)** In theorem 4.2, it's not clear what the role of x is.\nAfter equation (5), it's not clear what the 'proof in Appendix A.7' is for (is it missing a theorem statement here?). I had difficulties finding a concise summary of the theoretical guarantees (in terms of the graph Laplacian) proven by this coarsening scheme. Would it be possible to point to a 'main theorem' that's proven?\n\n**Ans 2)**\n\nIn theorem 4.2 $x \\in R^d$ represents the feature vector of the node, while $X \\in R^{N * d}$ represents a node features matrix of all the nodes. \n\n**Theorem 4.2 :($\\epsilon-similarity$)**\nGiven a Graph $G(L,X)$ and a coarsened graph $G_c(L_c,X_c)$, they are said to be $\\epsilon$ similar if there exists some $\\epsilon \\geq 0$ such that:\n\n\n\n$(1-\\epsilon)\\lVert X\\rVert_{L}\\leq\\lVert X_c\\rVert_{L_c}  \\leq (1 + \\epsilon) \\lVert X\\rVert_L$\n\nwhere $L$ and $L_c$ are the Laplacian matrices of $G$ and $G_c$ respectively. The coarsened graph$G_c({L_c,X_c}$) and features learned as an outcome of UGC satisfy this $\\epsilon$ similarity. \n\nIn the section **Bounded $\\epsilon$-similarity**, we have further improved this $\\epsilon$ similarity result by relearning the coarsened features via imposing the smoothness condition mentioned in Equation 5. As a consequence of it we are able to obtain a better bound on $\\epsilon$, i.e., $\\epsilon\\sim(0,1]$ which is summarized in the below Theorem\n\n\n**Updated Theorem:**\nGiven a Graph **$G(L,X)$** we learn a coarsened Graph via UGC **$G_c(L_c, X_c)$**, and then we relearn enhanced features as, $\\tilde{X}$ by enforcing the smoothness condition. The original graph $G(L,X)$ and coarsened graph $G_c(L_c,\\tilde{X})$ are $\\epsilon$ similar with $0 < \\epsilon \\leq 1$ \n\n$(1-\\epsilon)\\lVert X\\rVert_L\\leq \\lVert\\tilde{X}\\rVert_{L_c}\\leq (1+\\epsilon)\\lVert X\\rVert_L$\n\nThe proof regarding this Theorem is deferred in Appendix A.7.\n\nIf the reviewer deems it beneficial, we are open to putting this discussion in the main paper or Appendix.\n\nFor theoretical guarantees(in terms of the graph Laplacian) we refer reviewers to this updated version of Theorem 4.2."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215811427,
                "cdate": 1700215811427,
                "tmdate": 1700221038874,
                "mdate": 1700221038874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GO0MkfiqQu",
                "forum": "cDInj7WMQm",
                "replyto": "PXyouPIE0L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6082/Reviewer_Nt9N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6082/Reviewer_Nt9N"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing these issues, and the additional information / pointers."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700322638792,
                "cdate": 1700322638792,
                "tmdate": 1700322638792,
                "mdate": 1700322638792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "711cCFgmtJ",
            "forum": "cDInj7WMQm",
            "replyto": "cDInj7WMQm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6082/Reviewer_rN8j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6082/Reviewer_rN8j"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a strategy called UGC to coarsen an attributed graph to a smaller graph, while preserving certain desirable traits (e.g., certain spectral properties). The algorithm uses locality sensitive hashing to operate in a fast manner, and is empirically tested in a number of tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem is certainly relevant and the idea of employing locality sensitive hashing appears novel.\nI believe there are several interesting ideas encapsulated in this paper, which appear however to be not fully developed"
                },
                "weaknesses": {
                    "value": "* The paper is written in a somewhat haphazard way: the notation and introduced concepts remain often unclear and the paper lacks a clear structure and organization in my opinion.\n\n* Problem formulation -- what precisely is the mathematical objective that UGC tries to achieve? This is not clearly stated. \nThe set S does not define the set of indicator matrices the authors seem to have in mind (there are matrices that fulfill those constraints that are not indicator matrices). Why introduce it this way? Also C is supposed to be N times n, yet the example given is n times N.\n\n* The discussion on heterophily appears out of the blue -- it is not clear what relevance it has to the paper and the technique used.\n\n* The discussion on related work is pretty mixed, but it is not always clear on what these relations are build. There is a whole literature on network summarization, which appears to be largely ignored that is much closer to the type of problems discussed in this paper.\n\n* The notation and decsription in section 3.1 is unclear to me. What are the asterisks as opposed to \\cdot denote? Figure 3 does not really help much either as it is rather cryptic.\n\n* Section 3.2. and 3.3 are not well written either. There is an algorithm but the idea of it is hardly explained and the intuition remains completely absent in my opinion.\n\n* Section 4 appears to be a long list of quality criteria, whose relative merits and selection is never discussed. Symbols appear that have not been introduced before etc.\n\n* It is not really clear what questions the experimental session tries to answer and why. For instance, yes, run-time can be important, but only in conjunction with some other assessment of the quality of the coarsening -- what does 50% coarsening even mean for the other methods? How would you coarsen a graph to 50% by kron reduction -- there are many options. It is not clear what we are comparing here..\n\nOverall the paper appears to have been put together in a rushed manner. There are many typos and grammatical mistakes throughout, the organization is not clear, and the key messages get lost in details."
                },
                "questions": {
                    "value": "see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757474543,
            "cdate": 1698757474543,
            "tmdate": 1699636655459,
            "mdate": 1699636655459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CYg8WIMZhD",
                "forum": "cDInj7WMQm",
                "replyto": "711cCFgmtJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Eagerly waiting for your feedback"
                    },
                    "comment": {
                        "value": "Thank you for reviewing the paper and providing feedback! We have responded below to individual quotes from your review.\n\n**Ques 1)** Problem formulation -- what precisely is the mathematical objective that UGC tries to achieve? This is not clearly stated. The set S does not define the set of indicator matrices the authors seem to have in mind (there are matrices that fulfill those constraints that are not indicator matrices). Why introduce it this way? Also C is supposed to be N times n, yet the example given is n times N.\n\n**Ans 1)** \nWe have tried to elucidate the notation. To clarify, S is not a set of indicator matrices. It is the set of coarsening matrices. A coarsening matrix maps each node of the original graph to a node in the coarsened graph. Each node in the coarsened graph may be thought of as a supernode, that subsumes many nodes of the original graph. Hence, entries in a coarsening matrix are {1,0} indicating node numbers; each entry ($C_{ij}$) indicates the $i^{th}$ node number in the original graph paired with the $j^{th}$ node number in the coarsened graph.\n\n***Problem formulation*** This manuscript presents a novel approach to coarsening all types of graphs, including those that largely follow a homophily assumption, and others that depart substantially. The coarsening task attempts to to reduce a given graph $G(V,E,A,X)$ with N nodes, into a new graph $G_c(\\tilde{V},\\tilde{E},\\tilde{A}, \\tilde{X})$, with $n$-nodes and $\\tilde{X} \\in \\mathbb{R}^{n \\times d}$ where $n<< N$ nodes. The coarsened graph  $G_c$ is summarized from the original graph $G$ by using a coarsening matrix, i.e. the Graph Coarsening problem is that of learning a coarsening matrix $C \\in R^{N\\times n}$. Every non-zero entry $C_{ij}$ in $C$ denotes the merger of the $i^{th}$ node of $G$ to the $j^{th}$ supernode. This $C$ matrix belongs to the following set S:\n\n$$  S =  \\{ C \\in \\mathbb{R}^{N \\times n}, \\langle C_i,C_j \\rangle = 0 \\quad \\forall i \\neq j, \\langle C_l,C_l \\rangle = d_i, \\|C_i\\|_0 \\geq 1  \\} $$\nThis equation becomes important to impose the next two discussed properties on the *\u201cC\u201d* matrix; the condition $\\langle C_i, C_j \\rangle = 0$ ensures that each node of $G$ is mapped to a unique super-node. The constraint $\\|C_i\\|_0 \\geq 1$ requires that each super-node in $G_c$ contains at least one node of $G$. \n\nWe thank reviewer for pointing out we have modified $C$ to $C^T$ in the example.\n\n\n**Ques 2)** The discussion on heterophily appears out of the blue -- it is not clear what relevance it has to the paper and the technique used.\n\n**Ans 2)** We clarify the point. One novelty of our work is its applicability to a wide range of datasets; this includes those satisfying homophily assumptions, as well as those that violate it, such as graphs characterized by heterophily. Since the literature on graph coarsening largely assumes homophily, this point is relevant and important in the context of our work. Hence, the concept of heterophily is mentioned in the abstract, introduction, and explicitly discussed in Section 2.2.\n\n**Ques 3)** The discussion on related work is pretty mixed, but it is not always clear on what these relations are build. There is a whole literature on network summarization, which appears to be largely ignored that is much closer to the type of problems discussed in this paper.\n\n**Ans 3)**  The manuscript cites recent literature on Graph Coarsening [1,2,3]. References were chosen from S-O-T-A methods to provide a baseline for UGC on two criteria. These include speed, and the ability to handle datasets that do not exhibit homophily. We might have missed some relevant references, it would be great if reviewers can point out those network summarization references relevant to this work. Certainly, more references could be included. However, with regard to speed, UGC is compared with the fastest methods published.\n\n[1] Loukas, Andreas. \"Graph Reduction with Spectral and Cut Guarantees.\" J. Mach. Learn. Res. 20.116 (2019): 1-42.\n\n[2] Huang, Zengfeng, et al. \"Scaling up graph neural networks via graph coarsening.\" Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining. 2021.\n\n[3] Kumar, Manoj, Anurag Sharma, and Sandeep Kumar. \"A Unified Framework for Optimization-Based Graph Coarsening.\" Journal of Machine Learning Research 24.118 (2023): 1-50.\n\n\n\n**Ques 4)** The notation and description in section 3.1 is unclear to me. What are the asterisks as opposed to \\cdot denote? Figure 3 does not really help much either as it is rather cryptic.\n\n**Ans 4)**  We thank the reviewer for pointing out the confusion, we have modified the equation in section 3.1 to $$ F_i = \\{(1 - \\alpha) \\cdot  X_i\\oplus \\alpha \\cdot A_i \\}$$ where $\\oplus$ represents the concatenation. And $\\cdot$ is the multiplication. Figure 3 is used to visualize this augmented feature."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212761141,
                "cdate": 1700212761141,
                "tmdate": 1700288818928,
                "mdate": 1700288818928,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7ftI3n2Saf",
                "forum": "cDInj7WMQm",
                "replyto": "711cCFgmtJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6082/Reviewer_rN8j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6082/Reviewer_rN8j"
                ],
                "content": {
                    "comment": {
                        "value": "re A1) This is exactly what I would have referred to as indicator matrices. To say it once more in different terms: It appears the authors have a set of 0/1 matrices in mind; however, the characterization of the set is such that it contains many matrices that are not 0/1 -- for instance we may simply multiply the matrix C in the example by -1 and it would still be in the set...\nI believe the correct definition of the matrices C could be given much more succinctly in other terms.\n\nThe current problem formulation the authors point out just states what the approach does (coarsen a graph) but it does not say what it aims to do in quantitative terms; e.g., is the spectrum supposed to be (approximately) conserved? some cut metric? the eigenvectors? There is still no mathematical problem statement here, in my opinion.\n\nre A2) homophily is not a property of a graph, but of a generative process that creates a graph (it is a causal model for the formation of links and it cannot be inferred from observational data alone -- see [1,2]) it is thus still not clear to me what the authors really mean. My guess is that they want to say that prior methods for graph coarsening do not show good performance according to some measure (but which one?) if the graph considered is created according to a process that is not homophilic.\n\n[1] McPherson, Miller, Lynn Smith-Lovin, and James M. Cook. \"Birds of a feather: Homophily in social networks.\" Annual review of sociology 27.1 (2001): 415-444.\n[2] Shalizi, Cosma Rohilla, and Andrew C. Thomas. \"Homophily and contagion are generically confounded in observational social network studies.\" Sociological methods & research 40.2 (2011): 211-239.\n\nThere are also no citations given in this paragraph, so it is unclear what methods the author refer to when they say, those do not work for non-homophilic graphs. \n\nre A3) the mentioned papers all have a clear problem formulation and objective, which the current paper is still lacking in my opinion. Again graph coarsening is an ill-defined term and as long as the authors do not clearly specify what properties the coarsened graph is supposed to preserve precisely\nThere are even surveys on graph summarization available already like Liu, Yike, et al. \"Graph summarization methods and applications: A survey.\" ACM computing surveys (CSUR) 51.3 (2018): 1-34\n\nre A4) Thanks for the clarification\n\nre A5) What is not fully clear here is why the authors believe that their method would perform well in terms of these metrics (in terms of the algorithm design). That these are possibly quality criteria is quite clear, but how do they relate to the proposed approach?\n\nre A6) What does \"resemble\" and \"quality\" mean precisely here -- this is the question. In mathematical, quantifyable terms.\nLoukas work has a clear objective, already stated in the title of his work!\n\n\nLet me iterate my point here from my initial review: I think the problem area and potentially the algorithm the authors propose is interesting; however, the exposition and the discussion of the idea, motivation and algorithmic guarantees appears too vague to me to recommend acceptance here."
                    },
                    "title": {
                        "value": "reponse"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498575589,
                "cdate": 1700498575589,
                "tmdate": 1700499394625,
                "mdate": 1700499394625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j8yNQfOoh8",
                "forum": "cDInj7WMQm",
                "replyto": "ptnitXaRxI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6082/Reviewer_rN8j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6082/Reviewer_rN8j"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their continued responses and for correcting equation (1) to now reflect the actual set of \"coarsening\" matrices. I will take those responses into account during the reviewer discussion period and will then decide to change my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641416213,
                "cdate": 1700641416213,
                "tmdate": 1700641416213,
                "mdate": 1700641416213,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LaslAZEtDy",
            "forum": "cDInj7WMQm",
            "replyto": "cDInj7WMQm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6082/Reviewer_aXHZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6082/Reviewer_aXHZ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a graph coarsening algorithm that works in heterophilic scenarios by borrowing from locality sensitive hashing (LSH) literature. To do so, they consider the node feature vector in addition to the adjacency matrix. The core idea is to map nodes using this augmented matrix to the same node using an appropriately instantiated LSH. To validate the quality of the coarsening, the authors consider relative eigen error, hyperbolic error, and, to account for the feature vector, bounded $\\epsilon$-similarity. They show computational gains over previous coarsening state-of-the-art in terms of memory and compute time. To demonstrate the applicability of their approach, the authors train a single hidden layer GCN, and show that training on the coarsened graph has negligible impact on accuracy while benefiting from computational gains when tested on the original graph for predictions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The approach seems intuitive and there is a simplicity appeal to an adjacency augmented node feature vector.\n- The results seem promising (even if the downstream task is more illustratory than extensive)"
                },
                "weaknesses": {
                    "value": "- Section 3.4 is not a time complexity analysis, and no appropriate appendix is present. Crucially, it is unclear how the hidden loop on Line 14 in Algorithm 1 is maintained as O(N) rather than O(N^2). A time complexity analysis section should not leave finding all hidden loops in mixed pseudo-code and mathematical notation algorithms as an exercise for the reader. The empirical results indeed hint that the time cost benefits exist, but as the linear time claim is made in the abstract and introduction, this section requires considerable improvement.\n- The choice of downstream GNN architecture feels unjustified. Why specifically convolution instead of GIN, or GAT? More concretely, how should a reader know that the coarsening benefits are not specific to GCN but rather more universally exploitable? Additionally, low eigen error is shown as a benefit, but no investigation was made to show if, for example, the coarsening maps to community detection when performed using spectral methods."
                },
                "questions": {
                    "value": "[Repeated from Weaknesses]\n- Why specifically convolution instead of GIN, or GAT? More concretely, how should a reader know that the coarsening benefits are not specific to GCN but rather more universally exploitable? \n\n- Additionally, low eigen error is shown as a benefit, but no investigation was made to show if, for example, the coarsening maps to community detection when performed using spectral methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6082/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6082/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6082/Reviewer_aXHZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6082/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699431746806,
            "cdate": 1699431746806,
            "tmdate": 1699636655357,
            "mdate": 1699636655357,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rS1COOUhba",
                "forum": "cDInj7WMQm",
                "replyto": "LaslAZEtDy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6082/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6082/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Eagerly waiting for your feedback"
                    },
                    "comment": {
                        "value": "Thank you for reviewing the paper and providing feedback! We have responded below to individual quotes from your review.\n\n**Ques 1)** Section 3.4 is not a time complexity analysis, and no appropriate appendix is present. Crucially, it is unclear how the hidden loop on Line 14 in Algorithm 1 is maintained as O(N) rather than O(N^2). A time complexity analysis section should not leave finding all hidden loops in mixed pseudo-code and mathematical notation algorithms as an exercise for the reader. The empirical results indeed hint that the time cost benefits exist, but as the linear time claim is made in the abstract and introduction, this section requires considerable improvement\n\n**Ans 1)**  We have three phases for our framework. In the first phase(Algo 1 Line 1-7), we can see Line 7 is driving the complexity of the algorithm where we multiply two $F \\in \\mathbb{R}^{N \\times d}$ and $W \\in \\mathbb{R}^{L \\times d}$ matrices which results to $O(NLd)$. In the second pass, the supernodes for the coarsened graphs are constructed with the help of the accumulation of nodes in the bins. The main contribution of UGC is up to these two phases i.e., Line 1-10. Till now, time-complexity is $O(NLd) \\equiv O(NC)$ where $C$ is a constant. Hence, the time complexity for getting the partition matrix is $O(N)$. \n\nIn the third phase, Line 14-15, we calculate the adjacency and features of the supernodes of the coarsened graph $G_{c}$. For this we iterate over the edges of the original graph and use the edge nodes along with the surjective mapping $\\pi : V \\rightarrow V_c$ to increment the weight of the corresponding edge between the supernodes in $G_{c}$. The computational cost of this operation is $O(m)$, where $m$ is the number of edges in the original graph, and this is a one time step. Indeed, the overall time complexity of all three phases combined is O(N+m) where m is the number of edges. However, it's important to note that the primary contribution of UGC lies in the process of finding the partition matrix whose time complexity is O(N). We have  compared the partition matrix computational of all other methods with ours.\n\nWe have added this Detailed Time-complexity analysis in Appendix also.\n\n**Ques 2)** The choice of downstream GNN architecture feels unjustified. Why specifically convolution instead of GIN, or GAT? More concretely, how should a reader know that the coarsening benefits are not specific to GCN but rather more universally exploitable?\n\n**Ans 2)** We thank the reviewer for their constructive suggestions. While we initially employed GCN for validating the quality of our coarsened graph, it's essential to note that our model is not constrained to any specific architecture. Responding to the suggestion, we have tested our framework on additional models and results from three models (GraphSage, GIN, and GAT) are incorporated here. These results validate that our framework is not model dependent. If the reviewer deems it beneficial, we are open to add these results into Appendix.\n\nModels are trained on 50% coarsened graph and accuracies are quoted on original data.\n| Model\\Data | Cora     | Pubmed   | Physics  | Squirrel |\n| --------   | -------- | -------- | -------- | -------- |\n| GCN        | 89.30    | 84.77    | 96.12    | 31.62    |\n| GraphSage  | 69.39    | 85.72    | 94.49    | 61.23    |\n| GIN        | 67.23    | 84.12    | 85.15    | 44.72    |\n| GAT        | 74.21    | 84.37    | 92.60    | 48.75    |\n\n\n**Ques 3)**\n\nAdditionally, low Eigen error is shown as a benefit, but no investigation was made to show if, for example, the coarsening maps to community detection when performed using spectral methods.\n\n**Ans 3)** We thank the reviewer for their feedback. To elucidate the correlation between Eigen Error(REE) and the quality of the coarsened graph, we draw the reviewer's attention to Figure 5(a). In this figure, it is evident that the Relative Eigen Error (REE) increases as we reduce the graph. This observed trend is consistent with the relationship observed in node classification accuracy using GNNs, where the accuracy decreases as the graph gets reduced. These findings are corroborated in Table 6, presented in Appendix A.5. The presented results collectively illustrate the intrinsic connection between Relative Eigen Error, GNNs accuracy, and the quality of the coarsened graphs."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212043584,
                "cdate": 1700212043584,
                "tmdate": 1700212043584,
                "mdate": 1700212043584,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "whA8rklxIx",
                "forum": "cDInj7WMQm",
                "replyto": "rS1COOUhba",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6082/Reviewer_aXHZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6082/Reviewer_aXHZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and clarification.\n\nRegarding Ans3, thank you for the clarifications.\n\nRegarding Ans1, thank you for including this in the appendix, I do think this is needed within the paper.\n\nRegarding, Ans2, I did suspect that the results should be model-agnostic, but I did want to see at least empirical evidence towards this as well. I think the paper would be improved if these results were included in the Appendix as well."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6082/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268659657,
                "cdate": 1700268659657,
                "tmdate": 1700268659657,
                "mdate": 1700268659657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]