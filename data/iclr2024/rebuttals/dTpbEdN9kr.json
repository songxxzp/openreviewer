[
    {
        "title": "Human Motion Diffusion as a Generative Prior"
    },
    {
        "review": {
            "id": "2HoZlr3aDx",
            "forum": "dTpbEdN9kr",
            "replyto": "dTpbEdN9kr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2385/Reviewer_1uD6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2385/Reviewer_1uD6"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackle the problem of motion generation. Given a pretrained diffusion based text-to-motion model i.e., MDM, this paper introduce three different way of adaptation to account for long sequence generation, multi-person motion generation and joint trajectory control.  For long sequence generation, two sequences are first generated individually and then blended with a fixed length overlapping. For two-person motion generation, a communication module (ComMDM) is trained to modify the intermediate features from two individual pre-trained MDM so as to coordinate the interaction between them. For trajectory control, several MDMs are frist finetuned to be given the trajectory of different joints. To achieve the control of multiple joints, the paper proposes to interpolate between two MDMs trained beforehand."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation of limited long sequence data, multi-person interaction and detailed control is valid and convincing.\n- The proposed method is sound and interesting. \n- The experiments are comprehensive and the results on all three tasks are very good especially for the qualitative results."
                },
                "weaknesses": {
                    "value": "- Despite the good results, technically, there are not much contributions. The main contribution is the way of using/finetuning a pre-trained motion diffusion model for three different tasks. However, in each of those tasks the adaptation is straightforward. They are either engineering ticks such as soft or hard masking or  the use of existing techniques such as the interpolation mechanism from Ho & Salimans, (2022).\n\n- The proposed ComMDM is constrained to two persons only. It is hard to scale to multiple persons.\n\n- It is also unclear how the DiffusionBlending can be used to blend more than 2 joints."
                },
                "questions": {
                    "value": "For the DiffusionBlending, is the interpolation applied to the final motion or the intermediate feature as the CommMDM?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2385/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2385/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2385/Reviewer_1uD6"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731450157,
            "cdate": 1698731450157,
            "tmdate": 1699636173864,
            "mdate": 1699636173864,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KIxmotJVYQ",
                "forum": "dTpbEdN9kr",
                "replyto": "2HoZlr3aDx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2385/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "* Regarding a generalization of ComMDM for more than two people - that\u2019s an interesting point. Although presented for the two-person case, ComMDM can get as input the activations of more than two people and output the correction terms for each. In the absence of such a dataset with meaningful interactions, we were not able to validate the performance of ComMDM in such a setting.\n* Regarding DiffusionBlending with more than two joints - following your comment, we added a generalized equation for N joints (in blue). Our initial experiments show that it works well for N=3 and we are willing to add it to the next revision of the paper upon request.\n* Regarding your question about DiffusionBlending - the interpolation is applied to the final output of the model (X0 prediction), similarly to the Classifier-Free-Guidance scheme."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640942250,
                "cdate": 1700640942250,
                "tmdate": 1700640942250,
                "mdate": 1700640942250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AOk6IHCm2j",
                "forum": "dTpbEdN9kr",
                "replyto": "KIxmotJVYQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2385/Reviewer_1uD6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2385/Reviewer_1uD6"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal."
                    },
                    "comment": {
                        "value": "The rebuttal clarifies some of the details but the concern of lack of technical contribution still remains. I have no further questions and will keep my original rating."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699151080,
                "cdate": 1700699151080,
                "tmdate": 1700699151080,
                "mdate": 1700699151080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HaPCuyfj8C",
            "forum": "dTpbEdN9kr",
            "replyto": "dTpbEdN9kr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2385/Reviewer_xoQq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2385/Reviewer_xoQq"
            ],
            "content": {
                "summary": {
                    "value": "This paper leverages a pretrained Motion Diffusion Model (MDM) as a generative prior and introduces three distinct techniques to enhance generation quality and enable new tasks: 1) DoubleTake is employed to reduce artifacts in long-range motion generation, enhancing the smoothness of transitions. 2) The introduction of ComMDM, a compact model integrated after the transformer layer in MDM, allows the model to handle two-person generation by freezing the original MDM and training this new module on a smaller data collection. 3) The authors also propose a fine-tuning method and DiffusionBlending to enhance controllability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper presents compelling quantitative and qualitative results, setting a new state-of-the-art benchmark with a significant lead.\n\n2. This article broadens the scope of existing text-driven motion generation from three perspectives. The conclusions and experiments associated with these extensions are valuable contributions to the research community.\n\n3. The paper is well written, ensuring that its content is readily comprehensible to its readers."
                },
                "weaknesses": {
                    "value": "1. The authors should conduct a user study to quantitatively compare the visual results of TEACH with the proposed method for long sequence generation. Model parameters and inference speed should also be provided for a more comprehensive performance comparison between the two.\n\n2. Dual-person motion generation lacks comparative experiments, for example, with InterGen \\[1\\]. This paper introduces the InterHuman benchmark, a large-scale dataset for dual-person motion, and provides more comparative references in the article\n\n\\[1\\] Han Liang, et al. InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions"
                },
                "questions": {
                    "value": "Please kindly refer to the weaknesses mentioned above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834130790,
            "cdate": 1698834130790,
            "tmdate": 1699636173770,
            "mdate": 1699636173770,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tdvQcWkWgJ",
                "forum": "dTpbEdN9kr",
                "replyto": "HaPCuyfj8C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2385/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "* Regarding the comparison with TEACH - Following your comment we have conducted a user study as requested and added the results to the appendix (Figures 11, 12). We also added a paragraph regarding the user study in section 4.1 (in blue). As for the user study, we compared our DoubleTake method to TEACH by sampling randomly out of the validation set of BABEL 20 sequences of long motion such that each is ~10 seconds long in total and generated out of 3 different text prompts and durations. Both (DoubleTake vs TEACH) generated the exact same duration of motion when the transitions were marked with purple color and motions with orange. We asked 3 different question per motion sequence: \n(1)  Which animation aligns better with the text?\n(2) [Purple Frames only]: Which animation shows a smoother and natural motion transition?\n(3) Which animation, overall, demonstrates higher quality?\nTesting Overall Quality, Transition quality and Text To Motion alignment. Most of the users (82 to 85%) preferred our method over TEACH. Each survey had 10 different questions and each question was answered by 10 different users; In total, 20 people answered the survey.  \n* Following your request we added the number of parameters and runtime of both TEACH and DoubleTake Appendix, section D (in blue).\n* InterGen follows and extends the two instances\u2019 communication principle presented in ComMDM. As you mentioned, they introduced a new benchmark and used it to evaluate ComMDM as well as their model. Following your comment, we acknowledge their contribution in our related work section (in blue), yet, we will not duplicate their experiment which was already made public."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640891940,
                "cdate": 1700640891940,
                "tmdate": 1700640891940,
                "mdate": 1700640891940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oULgYxuA05",
            "forum": "dTpbEdN9kr",
            "replyto": "dTpbEdN9kr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2385/Reviewer_ijG5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2385/Reviewer_ijG5"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors address the challenges in human motion generation by introducing three new composition methods that use diffusion based generative models. The authors align these methods to 3 main challenges: long sequence generation, multi-person interactions, and controllable generation. The authors highlight that much of these problems arise from the lack of available data. The 3 proposed methods are sequential composition for long sequence generation, parallel composition for two-person motion generation, and model composition for fine-grained control and editing. The models involved in these methodologies are respectively DoubleTake for generating long motion sequences in a zero-shot manner, ComMDM to combine two frozen priors to enable two-person motion generation, and DiffusionBlending for flexible control of generated motion. The authors present several qualitative and quantitative results demonstrating the positive effects of their method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I want to highlight the following strengths:\n- The main strength I see is that the methods presented work well without the need of generating more data (or consuming large amounts of unavailable data). This is a strong benefit, since the field of human motion generation is still lagging in terms of data availability. The authors demonstrate in all 3 cases that they can satisfy the task at hand requiring small amounts of extra data/training.\n- I see major novelty in the methods developed for long sequence generation and 2 person generation. Both methods have interesting new ways to combine different generations from diffusion models (one over time and one in space). Adding to it that the method doesn't require a lot of extra training, these proposed methods seem solid and novel to me.\n- The paper is well written, with good experiments on all fronts. The author's explanation of each method is easy to follow. I want to particularly highlight figures 3 and 4, where the choice of colors and graphics makes it very intuitive to understand."
                },
                "weaknesses": {
                    "value": "My only concern is on the fine-tuned motion control part. The task seems very similar to controlled motion generation. In that case, there is a body of literature in this subject, many of which uses diffusion models for controlled motion generation. The authors failed to include these methods and compare against them. Of course these methods have different data requirements, but they seem to achieve the same goal. I put a list of these methods below. I ask the authors to explain why they did not include these methods in their comparisons? I'm still happy with the paper and I think the authors could make a case while still including these works, but I would like to hear from the authors on these choices.\n\n- Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian Holz. Avatarposer: Articulated full-body pose tracking from sparse motion sensing. ECCV, 2022.\n- Du, Y., Kips, R., Pumarola, A., Starke, S., Thabet, A., & Sanakoyeu, A. (2023). Avatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 481-490).\n- Castillo, Angela, et al. \"BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion Synthesis.\""
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698854188814,
            "cdate": 1698854188814,
            "tmdate": 1699636173682,
            "mdate": 1699636173682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q0do07TmCh",
                "forum": "dTpbEdN9kr",
                "replyto": "oULgYxuA05",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2385/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "* Regarding motion control prior work. This is a valid point, the works you mentioned are closely related to our fine-tuning task, hence, we added a paragraph presenting and discussing them in the related work (in blue). While related, we find an important difference between the two lines of work that leads to two different evaluation settings: Jiang et al., Du et al. and Castillo et al. are solving the task of reconstructing full body motion given the motion of a VR headset and two hand controllers. As such, they are measured with reconstruction metrics (i.e. MPJPE and jitter). On the other hand, our fine-tuned and DiffusionBlending models are aimed at modeling the distribution of motion given one or more control signals - for example, given the left hand\u2019s motion we aim to model the distribution of the right hand (and the rest of the body), not to reconstruct the original motion. Another example is given a trajectory, a person can walk/dance/run, etc. through it. In addition, we enable combining textual conditions to vary this distribution. Hence our evaluation is distribution-based (i.e. FID and diversity). As a result, we think that comparing the reconstruction works in the distribution setting (and vice versa) will be unfair."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640716756,
                "cdate": 1700640716756,
                "tmdate": 1700640716756,
                "mdate": 1700640716756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ywNt5HHxXt",
            "forum": "dTpbEdN9kr",
            "replyto": "dTpbEdN9kr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2385/Reviewer_w1k5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2385/Reviewer_w1k5"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents off-the-shelf motion diffusion models as a priori for realizing three forms of synthesis tasks. doubleTake is used to generate long-term human motion. comMDM is used to generate two-person motion. DiffusionBlending is used to enable flexible and efficient fine-grained joint and trajectory level control and editing. Experimental results show that these low-cost composite methods generalize well-trained motion prior to different tasks and outperform previous specialized techniques in the respective tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall, the paper is well-written with a clear and well-motivated introduction.\n\nThe proposed method outperforms previous specialized techniques in the respective task. The experimental designs are comprehensive and show visually appealing results."
                },
                "weaknesses": {
                    "value": "1. For the generation of long sequences, I don't think it makes sense to essentially generate each interval completely independently. A better option would be to use an autoregressive generation method similar to TEACH, but with the smarter option of combining each subsequence. Would it be possible to compare this with a scheme similar to EDGE [1]? Also, the paper admits that a comparison with DiffCollage is not possible due to a lack of publicly available code resources. Their implementation is simple. This may be optional, but would further support the paper.\n \n2. Regarding multiplayer motion generation, considering that MRT is an old paper, it could be considered to include a discussion and comparison with [1,2,3]. MRT focuses on deterministic prediction, and for the evaluation, is the prediction from ComMDM sampled once? A better comparison would be to use [2] as a baseline, to check the performance of diverse prediction and to measure diversity.\n\n[1] Tseng et al. EDGE. Edge: Editable dance generation from music. CVPR 2023\n\n[2] Xu et al. Stochastic Multi-Person 3D Motion Forecasting. ICLR 2023\n\n[3] Peng et al. Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting. CVPR 2023\n\n[4] Liang et al. InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions. Arxiv 2023"
                },
                "questions": {
                    "value": "1. The comparison with TEACH is not very fair either. Is it possible to have the same setting with TEACH but using DoubleMDM, e.g. noising the start and end intervals as diffusion inversions and then performing DoubleMDM with additional transition frames? In this case, you can use accuracy measurements instead of current metrics such as FID. This is because current metrics are more of a test of the quality of the movement and whether the movement is consistent with the text. It is also important to determine if the transition is good by measuring smoothness and accuracy.\n\nOverall, the author's response to the concerns is needed to make the final decision. I am also happy to increase the rating if my concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2385/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2385/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2385/Reviewer_w1k5"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2385/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698900515512,
            "cdate": 1698900515512,
            "tmdate": 1699636173595,
            "mdate": 1699636173595,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LpTuM5B3MP",
                "forum": "dTpbEdN9kr",
                "replyto": "ywNt5HHxXt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2385/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2385/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "* Regarding the long sequence generation - DoubleTake **does not** generate each interval completely independently. At the first take, the transition parts of the motions are shared between the neighboring intervals as demonstrated in Figure 3. The first take is similar to the EDGE scheme, with a major difference - while EDGE suggests complete overlap, in our case, the overlap is only for the transition part, which is more appropriate to our application.\n* Regarding ComMDM - Thank you for referring us to \u201cStochastic Multi-Person 3D Motion Forecasting\u201d. We added it to the related work (in blue) and will evaluate ComMDM using the metrics presented in the paper. Since the evaluation code seems missing from their published code base, we contacted the authors and will include this evaluation in our next revision.\n* Regarding the method suggested to compare with TEACH - DoubleTake generates long sequences by generating all the intervals simultaneously with their shared transition. We are afraid that generating transitions in a stand-alone manner will divert too much from our suggested method. Yet, we do suggest an evaluation setting that measures the quality of the motion and the quality of transitions independently. Additionally, in our ablation studies, we evaluate the second take by itself in a setting that quite resembles your suggestion. Since the transitions are generated simultaneously with the motion, and not stand-alone given input motions, we cannot measure its accuracy. Instead, we evaluate the quality and playability of the motion using FID and diversity. To address your concern regarding the fairness of the evaluation we conducted an additional user study comparing to TEACH (in blue) which shows a clear advantage to our DoubleTake considering the quality of motion, transition, and alignment with the input text."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2385/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640661969,
                "cdate": 1700640661969,
                "tmdate": 1700640661969,
                "mdate": 1700640661969,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]