[
    {
        "title": "TCD: TEXT IMAGE CHANGE DETECTION FOR MULTILINGUAL DOCUMENT COMPARISON"
    },
    {
        "review": {
            "id": "s3UM16D8X2",
            "forum": "zKgrmMOQjg",
            "replyto": "zKgrmMOQjg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3003/Reviewer_RtiC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3003/Reviewer_RtiC"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on multilingual document comparison. OCR-based methods highly rely on the performance of recognition, restricting the potential toward untrained languages. Hence, the authors propose an image-based method called TCD to get rid of OCR process. TCD only compares the embedding of source and target image bidirectionally to detect inconsistency where the correlation marginalization process is the key. A new text image change detection test dataset is presented. Experiments demonstrate the effectiveness of different modules in TCD and the full setting achieves the SoTA performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe structure of the article is well organized.\n2.\tThe proposed Correlation Marginalization sounds good to find inconsistency. And the two-way segmentation approach is a targeted design and effectively improves the performance. The proposed method is independent to OCR tools and can be extended to unseen multilingual data.\n3.\tA data synthesis approach for multilingual document is proposed. A new test dataset for text image change detection is created.\n4.\tThe proposed method achieves the SoTA overall performance on segmentation and OCR benchmark. The experiments on Russian texts shows the potential of TCD to handle untrained language."
                },
                "weaknesses": {
                    "value": "1.\tI wonder how the source image is obtained in actual situations. The downstream applications of should be highlighted.\n2.\tThe analysis of experiments needs to be enriched. For example, why the precision of U-Net is better among the benchmark methods?\n3.\tThe Correlation Marginalization is designed to save computation time. The ablation study only compares with Conv layer while ignoring the straightforward comparison to 4-D Correlation Map.\n4.\tIt would be better to provide some cases compared to OCR methods. For example, the false recognition leads to poor performance.\n5.\tThe writing needs to be improved. For example, in Sec 3.1 line 8, the conjunction \u2018However\u2019 is confusing. I kindly suggest the authors to polish the language."
                },
                "questions": {
                    "value": "In the second column of Fig 4, the character \u2018s\u2019 is annotated as changed, but it is aligned with the \u2018s\u2019 in source image. What\u2019s the criteria of annotating such cases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3003/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3003/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3003/Reviewer_RtiC"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3003/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746686466,
            "cdate": 1698746686466,
            "tmdate": 1699636244772,
            "mdate": 1699636244772,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "w4xdmKnSA6",
            "forum": "zKgrmMOQjg",
            "replyto": "zKgrmMOQjg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3003/Reviewer_6nRM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3003/Reviewer_6nRM"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new framework and a data synthesis method for a new research topic: Image Document Comparison. This topic, according to the explanation in the paper, aims to detect changed text locations such as changed, added, or deleted in document text images at character level. \nThe framework is built based on an Encoder-Decoder architecture, which consists of a share-weight encoder, a correlation marginalization module and a parallel decoder, for predicting the segmentation map of different characters between the source text image and target text image. And the image pair is collected and synthesized by the proposed data synthesis method, which includes using a synthetic image generator for training data and cropping-and-sticking character image manually from real world document image for testing data.\nAccording to the authors, this paper first proposed a linguistic-free model with a unique correlation marginalization process for text image comparison. Besides, they present a new text image change detection test dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "With respect to originality, the paper put forward a new research topic of text image change detection. This topic seems to be similar to another emerging topic in text-image research area, named tamper detection, for both predict the segmentation map between paired images, while the former focuses on the semantic difference at linguistic level and the later aims to detect the tamper area at visual level. The proposed topic appears to be novelty for seeking a linguistic-free method for text image comparison, but lack of necessity and practicability in reality. \nWith respect to quality, the paper adopts a complex and manually-designed encoder-decoder architecture which mainly concludes FPN and attention block. It is lack of interpretability for the necessity and effectiveness of such complicated design and inconvincible for the robustness.    \nAs to the clarity, the paper is of poor readability with a lot of grammatical errors. \nWith respect to significance, there is hardly any word about the application or reality necessity of the proposed \u201cImage Document Comparison\u201d, nor any example from reality is shown in the paper since both the training and testing dataset are synthesized or manually concatenated by authors."
                },
                "weaknesses": {
                    "value": "1.\tThe amount of the grammatical mistakes in the paper is unbearable. \n2.\tThere isn\u2019t enough word about the application or necessity of text image comparison through the passage, which makes it doubtful for the necessity of the research.\n3.\tThe works reviewed in Section \u201cRelated Work\u201d is quite different from the topic proposed in this paper. It will be better to discuss about and compare with the works relevant to universal image comparison if this paper is the first work in text image comparison. \n4.\tAccording to Section 3, a component called \u201ccross-self attention\u201d is frequently used in several module in proposed framework. The authors claim that it benefits the ability of feature matching between feature maps. This naturally aroused a question: why a common transformer block is not used. The paper is lack of experiment for proving the effectiveness of proposed component."
                },
                "questions": {
                    "value": "1.\tWhere does the necessity of text image comparison task lie in?\n2.\tCan the proposed dataset well represent the scenarios that will actually be encountered?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3003/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3003/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3003/Reviewer_6nRM"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3003/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824756235,
            "cdate": 1698824756235,
            "tmdate": 1699636244696,
            "mdate": 1699636244696,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "gWKA0fSDZL",
            "forum": "zKgrmMOQjg",
            "replyto": "zKgrmMOQjg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3003/Reviewer_qbQL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3003/Reviewer_qbQL"
            ],
            "content": {
                "summary": {
                    "value": "This article presents a new method for detecting changes in document images. Unlike conventional OCR-based methods, this method directly compares thumbnail images of lines of text to identify areas of difference. The method is based on a deep neural network combining feature extraction with a ResNet and multi-scale attention modules. The proposed method is compared to image-based methods and different OCRs on a dataset newly created by the authors. This comparison shows that the proposed method outperforms image-based methods and gives performances similar to those obtained with OCR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- an original change detection method, precisely described\n- a comparison with semantic segmentation, CD and OCR methods\n- an ablation study\n- an honest analysis of the results"
                },
                "weaknesses": {
                    "value": "## Why  Multilingual documents ?\n\nThe need for a specific method for analysing multi-lingual documents is not really argued. \n\n## The references in the introduction and related work section are mostly irrelevant.\n\n Very few references to previous work in document change detection are given. In particular :\n\nFirst paragraph:\n- 3 references to the history of OCR are not needed; TAUSCHECK is badly formatted.\n- idem, 3 general references to DL are not needed\n- DL OCR references are a bit outdated, most recent is 2016, 7 years old\n- give a reference of a transformer applied to document processing, not a generic transformer\n\n- I don't understand why Taafti2016 is relevant for image comparison.\n\nAt the end of the introduction, the context is not clear: are we talking about forgery, detecting changes in different versions of a document that should be identical? are we talking about scanned documents or image PDFs? \n\nRelated work: This section should focus on document comparison methods, not on OCR subsystems such as text detection and recognition and generic semantic segmentation of images. The references to text detection and text recognition methods cannot be exhaustive, and the reader does not know whether all these methods have been used in the context of document comparison.\n\nMissing references; \n- Rajiv Jain and David Doermann. VisualDiff: Document Image Verifica- tion and Change Detection. In 2013 12th International Conference on Document Analysis and Recognition, pages 40\u201344, August 2013. ISSN: 2379-2140. \u2028\n- Rajiv Jain and David Doermann. Localized document image change detection. In 2015 13th International Conference on Document Analysis and Recognition (ICDAR), pages 786\u2013790, August 2015. \u2028\n- Noo-ri Kim, YunSeok Choi, HyunSoo Lee, Jae-Young Choi, Suntae Kim, Jeong-Ah Kim, Youngwha Cho, and Jee-Hyong Lee. Detection of document modification based on deep neural networks. Journal of Ambient Intelligence and Humanized Computing, 9(4):1089\u20131096, August 2018. \u2028\n- Comparison of scanned administrative document images, 2020, https://arxiv.org/abs/2001.10785 \n\n## The method is not applicable to a full document\n\nIt is not clear how the method is applied to a complete document. It seems that lines of text need to be detected first, as the method is applied to lines of text. One limitation that seems very important is that you also have to compare the lines two by two, but how do you do this on an entire document? What if the documents don't have the same number of lines? What if the text is offset and overflows onto the next line? All the lines will be detected as different just because of an offset.\n\n## Training and testing with synthetic data only, on a single database, produced by the authors.\n\nAll the data used appears to be synthetic. There may be no datasets with actual documents for detecting changes in documents, but it would be useful to be more precise on this point. \n\n## No open-source code and the database is not distributed."
                },
                "questions": {
                    "value": "- why are there two independent decoders? why not only one output? It seems that the 2 outputs are identical.\n- regarding the data, the 2 categories, same and diff, are balanced in training and testing. Is it realistic? We expect the diff sequences to be more rare than the identical sequences; is this experimental setup realistic? \n- the data seems to consist of lines of text of just a few letters or words, according to the examples presented. What are the statistics for the data in terms of size (pixel) and number of characters? It is realistic ?\n- Table 6: same question with the metric: if the modifications are rare, precision/recall/F1 are not suitable for an evaluation, unless only the modified pixels are considered in the metric.\n- Table 7: PPOcr V3 is as good as TCD, except for Russian, so that the average is in favour of TCD. But for all the other languages, there is no advantage in using TCD over a standard OCR. This very important result is only presented in the appendix, which is misleading. \n- how did you train the semantic segmentation models with 2 inputs and 2 outputs ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3003/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875164085,
            "cdate": 1698875164085,
            "tmdate": 1699636244623,
            "mdate": 1699636244623,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "QkdEJJF91a",
            "forum": "zKgrmMOQjg",
            "replyto": "zKgrmMOQjg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3003/Reviewer_ygsh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3003/Reviewer_ygsh"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a technique to detect changes in multi-lingual documents without performing OCR explicitly. It utilizes a unit-level text image-to-image comparison by building a correlation marginalization process on each feature surroundings to detect text-change between source and target documents. \n\nThe proposed network uses an encoder-decoder model. In the encoder, a Siamese network takes in source and target images. Using ResNet as an FPN backbone network, it produces 3 mutli-scale feature pyramid map pairs (of sizes N/2, N/4, N/8). Using ideas from transformers, positional encoding is applied on N/8 feature vector, followed by cross attention and self-cross attention (source and target images). Assuming changes are local, a cross-correlation and marginalization map is constructed from cosine similarity. In the decoder, cross attention and self-cross attention is applied independently on source and target marginalized correlation maps. Dice and binary cross-entropy loss are used as loss functions. \n\nTraining is done with synthetic data. Evaluation is performed across various scripts like English, Korean Russian and Chinese. For segmentation maps, the proposed TCD approach outperforms the others in F1 and IoU scores. TCD also gets the best OCR average F1 results."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The primary contribution of the paper is the proposed Change Detection Model Architecture that combines the best of Transformer model and Siamese network to compare changes across two documents. Assuming the changes are local, the addition of correlation marginalization module on cross-attention and self-cross attention feature maps help introduce a similarity concept."
                },
                "weaknesses": {
                    "value": "There are several weaknesses of the paper:\n1. Clarity in writing: It is highly recommended to get the paper edited through a native English speaker. The paper gets progressively harder to read. More importantly, Section 4, arguably one of the most important sections, is not coherent due to lack in sentence formulation. E.g. the sentences like below are extremely hard to make sense of:\n- We set the height of training data is fixed and set as 32.\n- Our model output two way segmentation maps are merged into one map and then the input text image pair classified as same if the number of change pixels all are 0 otherwise different.\n- With vigorous research in document analysis (Huang et al., 2022; Fang et al., 2021), this served as a momentum for document comparison technology to be released as off-the-shelf software in the field of research\n- it can be observed that the segmentation map is a pseudo segmentation map of rectangular shape based on the width and height of the character in the image which is set to 1 if the particular position changed or 0.\n\nApart from these, characters are incorrectly capitalized, used of 'however' and 'in general' is excessive and incorrect\n\n2. Scientific vocabulary: The paper lacks scientific vocabulary. E.g. Abstract states that the proposed model outperforms 'by a good margin'. It is always recommended to be quantitative in such claims. 'Input' and 'Queried' vocabulary is mixed with 'source' and 'target' vocabulary in section 3.5. Section 4.4 states accuracy improvements in whole/integer numbers. Please be specific in how much improvements there were. \n\n3. Section 4 is the weakest. Please provide clear instructions on how the data was generated. There aren't any details on OCR model and how OCR is performed across a source and target images, which fundamentally differ from each other. Lastly, the ablation study does not stress on the need for such a complex system. The improvements are minimal."
                },
                "questions": {
                    "value": "1. How was data generated? Section 4.1 states that 'merged randomly to make concatenated text images'. Does this mean there are random words that don't make sense? \n\n2. Are there only 80k image pairs across 4 languages? They seem too low to train and test a transformer based model. How was training done exactly? Was it per language basis?\n\n3. Was every SOTA model stopped after 200 epochs or they were stopped right before overfitting?\n\n4. No details are provided on how OCR classification is done! Does this mean OCR is applied to only that subset where image pair is classified as same?\n\n Once the paper is rewritten, perhaps more questions would emerge due to a better understanding of the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3003/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699254360049,
            "cdate": 1699254360049,
            "tmdate": 1699636244525,
            "mdate": 1699636244525,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]