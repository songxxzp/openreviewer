[
    {
        "title": "ExcelFormer: Making Neural Network Excel in Small Tabular Data Prediction"
    },
    {
        "review": {
            "id": "xFQ8AgZM8r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4888/Reviewer_Wsjx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4888/Reviewer_Wsjx"
            ],
            "forum": "sYv3OMboTF",
            "replyto": "sYv3OMboTF",
            "content": {
                "summary": {
                    "value": "**The paper studies** machine learning problems on small (<1K objects) tabular datasets (e.g. classification, regression, etc.) with additional experiments on larger (up to 500K objects) datasets.\n\n**The main contribution of the paper** is ExcelFormer -- a deep learning scheme (Transformer-like architecture + custom training recipe) with the following new elements compared to the vanilla Transformer:\n- (architecture) custom attention\n- (architecture) custom feed-forward block\n- (architecture) custom feature embeddings\n- (architecture) custom prediction head\n- (training) custom initialization\n- (training) two custom augmentations\n\n**The main claim:** *\"EXCELFORMER consistently and substantially outperforms previous works\"*"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The story is mostly easy to follow (also, I like the main illustration!).\n- The research direction (designing better tabular deep learning architectures and augmentations) is important.\n- The experimental part includes many datasets.\n- I like the idea of using feature importances (1) to guide the attention between features and (2) to guide one of the two proposed data augmentations."
                },
                "weaknesses": {
                    "value": "(1) (major) **Many *orthogonal* changes (listed in the summary above) are proposed *at once*.** It makes it difficuilt to attribute the observed results to any single element, which I believe to be important in the research context, especially for this genre of papers. I believe that the elements should be introduced either in isolation or step-by-step, but not at once (unfortunately, ablating each of the elements using the *final* architecture does not addresses the issue). Also, in my opinion, each of the elements should be compared against existing alternatives (i.e. the proposed augmentations VS existing augmentations, the proposed embeddings VS the existing embeddings, etc.).\n\nOverall, modifying the well-established Transformer architecture in six(!) different aspects (listed in the summary), most of which has dedicated research subfields looks like an extremely ambitious goal to me. And I respect that, however, it makes it extremely hard to properly introduce and analyse each of the elements.\n\n(2) (major) In my opinion, **the storyline around rotation invariance should be extended with specific analysis/experiments/results. Purely intuitive guidance may not be enough to drive the design decisions.** There are multiple places where the *formal* term \"rotation invariance\" is used in *informal* ways. For example, the paper uses terms like \"more/nearly non-rotationally invariant\". Overall, there is nothing wrong with relying on intuition, but after a certain threshold, there is a risk of coming to wrong conclusions.\n\nA potential solution is to design a dedicated experiment that will quantify rotation invariance of any ML model. Then, some of the proposed elements can be motivated as a way to reduce the invariance according to the designed experiment. Again, this should be done *when introducing the elements*, not with the final architecture (as in Figure 5).\n\n(3) Unfortunately, in my opinion, **the novelty is limited.** Some of the proposed modifications (listed in the summary above) are technically new, however, from the same technical perspective, they remain similar to the existing alternatives.\n\n(4) In my opinion, sharing code, starting from the review stage, is important for this kind of studies. I wish I had an opportunity to have a look at the code to review the experimental setup and implementation details.\n\n(5) Instead of Paragraph 2 of Section 1, I recommend writing only ~2-3 high level sentences and then referring to Section 5.4 of \"Why do tree-based models still outperform deep learning on tabular data?\" by Grinsztajn et al.\n\n(6) I recommend proof-reading the paper for English style, vocabulary and grammar issues."
                },
                "questions": {
                    "value": "How exactly is the mutual information computed for the continuous features and for regression labels?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Reviewer_Wsjx"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4888/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697402500962,
            "cdate": 1697402500962,
            "tmdate": 1699636473327,
            "mdate": 1699636473327,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A57dgeWHSM",
                "forum": "sYv3OMboTF",
                "replyto": "xFQ8AgZM8r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer Wsjx (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for helpful comments. We trust that our responses will effectively address all your concerns.\n\n---\n\n> **Q1**: Many *orthogonal* changes are proposed at once? Need more ablation studyies.\n\n**A1**: Thank you for your concerns. We would like to clarify it from 2 aspects:\n\n**(1) More detailed studies**. We have included more ablation studies and addictive studies to inspect the performances of proposed components.\n\n1. Addictive study: on a vanilla Transformer (Tra), we show that **all of our proposed modules**, SPA, IAI, and GLU projection/embedding (GLU projection and embedding share the idea, and both of them process features through the GLU) **make positive contributions to tabular data prediction tasks**, and the contributions are cumulative.\n\n| | Tra | Tra + IAI | Tra + SPA | Tra + SPA + IAI |Tra + SPA + IAI + GLU|\n|-|-|-|-|-|-|\n| binclass (Ave Norm AUC, \u2191) | 0.232\u00b10.36 | 0.683\u00b10.37| 0.289\u00b10.37 | 0.753\u00b10.33  | 0.775\u00b10.30|\n| regression (Ave Norm nRMSE, \u2191) | 0.138\u00b10.30  | 0.738\u00b10.25  | 0.751\u00b10.26|0.766\u00b10.35 | 0.911\u00b10.16|\n\n2. Compare our data augmentation with Mixup and CutMix. Ablation study in Figure 4 show CutMix and Mixup perform worse than our proposed data augmentation. To analyze the difference of cutmix and Feat-Mix carefully, in Table 6, we present the results of cutmix and Feat-Mix on several datasets. Theoretically, the difference between cutmix and feat-mix lies in the use of feature importance. Empirically, we find that:\n\n* In general, **Feat-Mix outperforms Cutmix** due to its ability to provide more accurate labels by considering feature importance.\n\n* When noise exists in the data, **Feat-Mix is less affected by the noise while cutmix is more heavily impacted**. Since tabular datasets have been shown to contain many uninformative features`[1]`, we believe Feat-Mix is a superior choice.\n\nThe detailed results:\n\n| | Breast | Diabetes | Campus | cpu | fruitfly | yacht |  \n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n| CutMix (\u2191) | 0.702 | 0.822 | 0.972 | -102.06 | -16.19 | -3.59 |   \n| Feat-Mix (\u2191) | **0.713** | **0.837** | **0.980** | **-79.10** | **-15.86** | **-0.83** |   \n| CutMix (on noisy data) (\u2191) | 0.688 | 0.809 | 0.938 | -115.10 | -17.09 | -4.40 |\n| Feat-Mix (on noisy data) (\u2191) | **0.700** | **0.834** | **0.969** | **-74.56** | **-16.60** | **-0.89** |  \n| $\\Delta$ CutMix (\u2193) | 0.014 | 0.013 | 0.034 | 13.04 | 0.90 | 0.81 |   \n| $\\Delta$ Feat-Mix (\u2193) | **0.013** | **0.003** | **0.011** | **-4.54** | **0.74** | **0.06** |\n\n3. Compare Hid-Mix, Feat-Mix with Feature resample (an augmentation for tabular data) on MLP: \n\nThe results are presented below. We observe that Hid-Mix outperforms Feature Resample and all these approach dominate different parts of datasets.\n\n|   | Feature Resample | Feat-Mix | Hid-Mix | No data augmentation |\n|-|--|--|-|--|\n| Binclass (Ave Norm AUC, \u2191) | 0.576\u00b10.42 | 0.530\u00b10.41 | 0.614\u00b10.41 | 0.436\u00b10.42 |\n| regression (Ave Norm nRMSE, \u2191)|0.661\u00b10.39|0.629\u00b10.40 | 0.681\u00b10.39 | 0.334\u00b10.42 |\n\n4. An ablation study is shown in Figure 4 that the performances of Excelformer drops when removing one of those components.\n\n**(2) Why propose those components?**\n\n1. We would like to point out a specific issue of the tabular data prediction domain: **Tabular datasets are highly diverse and heterogeneous. Unlike other fields (such as computer vision) where a single model can dominate most datasets, different previous tabular data prediction models only excel on a part of datasets (see Table 2: excluding Excelformer, the best models vary in different groups). Excelformer is the first work that realizes such dominance.**\n\n2. Given the diversity of datasets in this field and the no-free-lunch principle, finding a single component that is universally applicable to all types of datasets is challenging (previous works have faltered in this aspect). **In fact, the orthogonality of methods is indeed the cornerstone of our contribution. When addressing the diversity of tabular datasets and tasks, the proposal and integration of orthogonal methods augment the robustness and efficacy of Excelformer**. Then, we achieve:\n\n*  **Using default hyperparameters (it is very convenient in application!)**, ExcelFormer outperforms previous hyperparameter-tuned models;\n\n* ExcelFormer achieves robust performance **across various types of datasets / tasks** (see Table 2)\n\n**We argue that AI development aims to facilitate practical applications.** The combination of versatility and the convenience of not requiring hyperparameter tuning positions Excelformer as an excellent practical tool for tabular data prediction tasks, especially for users with limited computer science proficiency or computational resources. **Please note that no previous work attains performances as dominating as Excelformer, making this a key contribution.**\n\n---\n\nReferences:\n\n`[1]` Why do tree-based models still outperform deep learning on typical tabular data? NeurIPS, 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624823764,
                "cdate": 1700624823764,
                "tmdate": 1700638699168,
                "mdate": 1700638699168,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PCd7e0yiMb",
                "forum": "sYv3OMboTF",
                "replyto": "xFQ8AgZM8r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer Wsjx (part 2)"
                    },
                    "comment": {
                        "value": "> **Q2**: Novelty?\n\n**A2**: Thank you for this concern. I believe the evaluation of novelty is quite objective. I suggest considering the changes an approach brings as a reference for novelty creation. Let's examine what changes we made by introducing Excelformer:\n\n* BEFORE EXCELFORMER:\n\n  * The overall performance of deep learning approaches is **inferior** to GBDTs.\n  * Different previous models excels in specific types of datasets / tasks (see Table 2).  It is **challenging to determine a \"good\" model** before conducting an evaluation.\n  * **Fine-tuning hyperparameters is crucial** for attaining state-of-the-art results across all previous models. This poses challenges for users with limited proficiency in computer science or access to computational resources.\n\n* AFTER EXCELFORMER IS PROPOSED:\n  * Excelformer (a DL approach) beats GBDTs and various DL competitors;\n  * Excelformer outperforms previous models on various types of datasets / tasks (see Table 2). It is easier for users to choose a model (Excelformer) now.\n   * Although fine-tuning hyperparameters is beneficial for Excelformer, using its default hyperparameters can achieve top results.\n\n* To achieve this, THE COST WE INCUR IS MINIMAL:\n  * Obviously, the computational efficiency of SPA is the same as vanilla self-attention module.\n  * Interaction Attenuated Initialization is almost computational free.\n  * As analyzed in Sec. 2.2, the computational complexity of GLU projection is equivalent to that of vanilla FFN.\n  * Feat-Mix and Hid-Mix are nearly computational free.\n\nWe will highlight the contribution of Excelformer in the final version.\n\nConsidering the changes that Excelformer has brought to the field of tabular data prediction, we would appreciate it if you could reconsider your evaluation of novelty. Thank you!\n\n---\n\n> **Q3**: In my opinion, the storyline around rotation invariance should be extended with specific analysis/experiments/results. \n\n**A3**: Thank you for your constructive suggestion! In the initial version, we conducted an analysis of the rotation invariance property, i.e. the subfig (a) in Figure 5. Following your suggestion, we have taken a step further by providing an additive study and an ablation study to analyze the effects of SPA and IAI on breaking the rotation invariance of deep learning models. \n\n* Additive study setting: We use SPA and IAI on the backbone of FTT `[1]`\n\n* Ablation study setting: We exclude SPA and IAI from the architecture of Excelformer\n\nWe find that:\n\n* With SPA and IAI, our Excelformer shows better non-rotationally invariance, compared with previous works.\n\n* On the FTT backbone, the addition of SPA and IAI makes positive contributions to break the rotational invariance property.\n\n* On the Excelformer architecture, removing SPA and IAI weakens Excelformer's rotational invariance property.\n\n* **The effects of adding or removing SPA and IAI are cumulative.**\n\n---\n\n> **Q4**: Sharing the codes?\n\n**A4**: We would like to make all codes publicly available, including detailed implementations, model training and experiment examples, along with user-friendly APIs. Given the substantial volume of implementation and experiment codes, we chose not to include them in the initial submission to allow more time for the improved organization and presentation of this project. **The code is now included in the supplementary materials**, and we will continue to refine the project to offer more comprehensive documentation and unified APIs.\n\n---\n\n> **Q6**: How exactly is the mutual information computed for the continuous features and for regression labels?\n\n**A6**: In the implementation, we utilize the scikit-learn package to calculate mutual information. Specifically, for classification tasks, we employ the \"feature_selection.mutual_info_classif\" function, and for regression tasks, we use the \"feature_selection.mutual_info_regression\" function. Thank you for your reminder! We have depitcted the implementation in Appendix F.\n\n---\n\n> **Q7**: Paper polishment.\n\n**A7**: Thank you for your careful suggestions. We will do careful proof-reading for the final version.\n\nThanks again for your time and feedback. We've incorporated your suggestions into the manuscript, which is very helpful to improve the quality of our paper. We kindly request you to reconsider the ranking, considering the added ablation studies, additive studies and the practical benefits of ExcelFormer.\n\n---\n\nSincerely,\n\nAuthors of paper 4888\n\nReferences:\n\n`[1]`  Revisiting deep learning models for tabular data. NeurIPS. 2021."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627387326,
                "cdate": 1700627387326,
                "tmdate": 1700647280776,
                "mdate": 1700647280776,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lzYSuDS0u2",
            "forum": "sYv3OMboTF",
            "replyto": "sYv3OMboTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4888/Reviewer_J7k8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4888/Reviewer_J7k8"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel transformer architecture with two main suggestions:\n- SEA: A semi-permeable attention block that masks the similarity scores from less informative features to the more informative ones, effectively blocking the transfer of information/representation from the less informative one.\n- Interaction Attenuated Initialization: A rescaling of the variance of the weight initialization that in turn reduces the impact of SEA, which restricts feature interaction in the initial stages, making the proposed transformer architecture more non-rotationally invariant.\n\nThe authors then additionally propose two augmentation methods:\nHidden-Mix and Feat-Mix. One works by augmenting the data on the embedding space, while the other one works on the feature space by using the feature importance.\n\nThe authors combine the different components and one of the suggested augmentation methods at a time to yield an architecture that surpasses the baselines in 96 small-scale datasets and 21 large-scale datasets. The method outperforms the baselines without hyperparameter tuning and with hyperparameter tuning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper has a good structure.\n- The authors propose quite a few interesting additions. The additions are ablated individually and the authors additionally show that the algorithm is more non-rotational invariant compared to the other transformer baseline.\n- Experiments are extensive, a large number of datasets is considered and all the major baselines are included."
                },
                "weaknesses": {
                    "value": "- The paper can be written better, typos exist here and there throughout the manuscript. (I will list a few of them in the questions section)\n- The work should be self-contained and the \"mutual information\" should be described.\n- In table 1, an interesting investigation would be how ExcelFormer would behave without any data augmentation (compared to the rest, not the ablation that is given) or how FTT would perform with the proposed augmentation approaches. I am additionally surprised that CatBoost performs worse compared to XGBoost consistently.\n- No multi-class classification problems in the 96 datasets for the small-scale tabular datasets and only 4 datasets in the 21 large-scale datasets. 4 datasets in 117 datasets is an underrepresentation. \n- Regarding the evaluation metrics, why would the authors use AUC for binary classification and ACC for multi-class classification? The latter would not be a good metric for imbalanced datasets.\n- An ablation is given when mixup is used as data augmentation, however, I would also prefer to see cutmix usage as an ablation.\n- Without code release, I find it difficult to trust the results, as unfortunately there exist a plethora of recent DL architectures that claim state-of-the-art performance (TabNet, Node, Saint, etc) [1][2][3] only to be debunked later on [4]. It is necessary to validate the proper setup of the baseline algorithms and to verify the results of the method.\n\n[1] Arik, Sercan \u00d6., and Tomas Pfister. \"Tabnet: Attentive interpretable tabular learning.\" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 8. 2021.\n\n[2] Popov, Sergei, Stanislav Morozov, and Artem Babenko. \"Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data.\" International Conference on Learning Representations. 2019.\n\n[3] Somepalli, Gowthami, et al. \"SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training.\" NeurIPS 2022 First Table Representation Workshop. 2022.\n\n[4] Shwartz-Ziv, Ravid, and Amitai Armon. \"Tabular data: Deep learning is not all you need.\" Information Fusion 81 (2022): 84-90."
                },
                "questions": {
                    "value": "- This presents a significant challenge and bottleneck in the broader adoption of neural networks on tasks involve tables. -> that involve tables*\n- DNNs\u2019 leanring procedure -> Learning *\n- Section 2.1, the mask is defined as M, however, Equation 2 and the follow-up text continue with W\n- Section 5.1, TabFPN -> TapPFN \n- Section 5.2, indicating that applies hyperparameter finetuning onto EXCELFORMER can yield -> applying*\n\n- Can you present the results where no augmentation is performed for ExcelFormer to analyze how it would perform against the other baselines? Can you provide the results where the proposed augmentation is applied to FTT?\n- Can you include more multi-class classification problems in the used benchmarks and provide results?\n- It would be interesting to see an ablation of the proposed augmentation methods against maybe cutmix or cutout to observe the overall improvement.\n- **Is the EXCELFORMER more non-rotationally invariant and more noise insensitive?** In my perspective, an interesting addition would be to include the plain architecture of ExcelFormer in the investigation, then with every suggestion included one at a time (SEA, IAI), then both. This would show how the architecture gets more non-rotationally invariant as the different components are added compared to the beginning. Comparing against FTT is interesting, but it does not separate the impact of the overall differences in the architecture vs (SEA, IAI).\n- I would urge the authors to provide the code to reproduce the results.\n\nI am open to increasing my score if my concerns are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Reviewer_J7k8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4888/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698055538665,
            "cdate": 1698055538665,
            "tmdate": 1699636473139,
            "mdate": 1699636473139,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5lXLELR01s",
                "forum": "sYv3OMboTF",
                "replyto": "lzYSuDS0u2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer J7k8 (part 1)"
                    },
                    "comment": {
                        "value": "Thank you for recognizing the good structure, comprehensive experiments, and the interesting aspects of our components. We trust that our responses will effectively address all your concerns.\n\n----\n\n> **Q1**: \n\n> (i) Release codes?\n\n> (ii) Why previous works are debunked to be not a real sota?\n\n> (ii) Why propose several components?\n\n> (iii) Why XGboost outperforms Catboost?\n\n**A1**: Thank you for your thoughtful considerations. We would like to address the aforementioned questions together as they are closely related.\n\n(i) Certainly, we intend to make all codes publicly available, including detailed implementations, model training and experiment examples, along with some user-friendly APIs. We are confident that Excelformer can serve as a convenient and versatile solution for tabular data prediction. Given the substantial volume of implementation and experiment codes, we chose not to include them in the initial submission to allow more time for the improved organization and presentation of this project. **The code is now included in the supplementary materials**, and we will continue to refine the project to offer more comprehensive documentation and unified APIs.\n\n(ii) **Why previous SOTA be debunked?** Without intending any offense, we would like to explain why you find that recent approaches (e.g., TabNet, Node, SAINT) initially claimed state-of-the-art performance but were later \"debunked\". Actually, tabular datasets are highly diverse and heterogeneous. Unlike other fields (such as computer vision) where a single model can dominate most datasets, **different previous tabular data prediction models only excel on a part of datasets** (see Table 2: excluding Excelformer, the best models vary in different groups). In this paper, Excelformer tries to realize such dominance. Previous works ignored such dataset diversity, and most of them (e.g., TabNet, Node, SAINT) only validated their claims on limited datasets. Similarly, the conclusions in the paper titled *Tabular data: Deep learning is not all you need* may also not be entirely convincing, as it relied on only 11 datasets. Hence, in our paper, **we extensively utilized 130+ diverse datasets, and also presented results categorized by characteristics in Table 2**. See Table 2, one may observe that different previous methods exhibit preferences for distinct datasets, but Excelformer obtains top performances in all the groups.\n\n(iii) **Why propose several components?** Given the diversity of datasets in this field and the no-free-lunch principle, it is challenging to find a component widely applicable to all the dataset types (under default hyper-parameter setting). Following your suggestion, we compare the Excelformer (without data augmentation) against previous models, and found Excelformer (without data augmentation) achieves the best performance (see `Q4 & A4`). However, as an application-oriented research, we prioritize both model performance and user convenience. Thus, in this paper, we introduce those useful components, and we find they can make Excelformer:\n\n* exempt from hyper-parameter tuning (using default hyperparameters is very convenient in application!)\n\n* capable of robust performance across various types of datasets and tasks (see Table 2)\n\nWe believe that the purpose of AI research is to facilitate practical applications. The combination of versatility and the convenience of not requiring hyperparameter tuning positions Excelformer as an excellent practical tool on tabular data prediction tasks, especially for users with limited computer science proficiency or computational resources.\n\n(iv) **Why XGBoost outperforms CatBoost?**\n\n* Firstly, in our experiments, **XGBoost does not consistently outperform CatBoost**. See Table 3, under default parameters, CatBoost outperforms XGBoost. Additionally, see Table 2, CatBoost performs better than XGBoost in classification and the case of #. Features $\\ge$ 16. Besides, CatBoost typically outperforms XGboost in multi-class classification (see Table 7 of the revised version). **Such distinction is due to the diversity of datasets and tasks.**\n\n* A key contribution of CatBoost is reducing the workload for users in terms of feature engineering, visualization, and hyper-parameter tuning. This enables CatBoost to outperform XGBoost when using default parameters. However, after conducting extensive hyperparameter search for GBDTs (see Tables 6 and 7 - we provide more estimator candidates (4096) than previous works), we find that XGboost can outperform CatBoost on over 50% of datasets.\n\n**In summary, Excelformer is an application-oriented research. It achieves robust performance across diverse datasets using default hyperparameters. The proposed components work together and make Excelformer superior on the diversity of tabular data with almost no added cost.**"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605733694,
                "cdate": 1700605733694,
                "tmdate": 1700636475106,
                "mdate": 1700636475106,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ck8QC7JXzi",
                "forum": "sYv3OMboTF",
                "replyto": "lzYSuDS0u2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To reviewer J7k8 (part 3)"
                    },
                    "comment": {
                        "value": "> **Q7**: It would be interesting to see an ablation of the proposed augmentation methods against maybe cutmix or cutout to observe the overall improvement.\n\n**A7**: Thank you for your suggestion. We compare the our data augmentation approaches with cutmix in two aspects:\n\n(1) We have added results of Excelformer architecture + cutmix into Figure 4, which shows that cutmix performs better than vanilla mixup but is still not as well as our proposed data augmentation approaches. To further analyze the difference between cutmix and Feat-Mix carefully, in Table 6 of revised version, we present the results of cutmix and Feat-Mix on several datasets. Theoretically, the difference between cutmix and feat-mix lies in the use of feature importance. Empirically, we find that:\n\n* In general, **Feat-Mix outperforms Cutmix** due to its ability to provide more accurate labels by considering feature importance.\n\n* When noise exists in the data, **Feat-Mix is less affected by the noise while cutmix is more heavily impacted**. This is clearly due to our use of feature importance when synthesizing new samples. Since tabular datasets have been shown to contain many uninformative features`[1]`, we believe Feat-Mix is a superior choice.\n\nThe detailed results are as follow:\n| | Breast | Diabetes | Campus | cpu | fruitfly | yacht |  \n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n| CutMix (\u2191) | 0.702 | 0.822 | 0.972 | -102.06 | -16.19 | -3.59 |   \n| Feat-Mix (\u2191) | **0.713** | **0.837** | **0.980** | **-79.10** | **-15.86** | **-0.83** |   \n| CutMix (on noisy data) (\u2191) | 0.688 | 0.809 | 0.938 | -115.10 | -17.09 | -4.40 |\n| Feat-Mix (on noisy data) (\u2191) | **0.700** | **0.834** | **0.969** | **-74.56** | **-16.60** | **-0.89** |  \n| $\\Delta$ CutMix (\u2193) | 0.014 | 0.013 | 0.034 | 13.04 | 0.90 | 0.81 |   \n| $\\Delta$ Feat-Mix (\u2193) | **0.013** | **0.003** | **0.011** | **-4.54** | **0.74** | **0.06** |\n\n> **Q8**: An interesting addition would be to include the plain architecture of ExcelFormer in the investigation, then with every suggestion included one at a time (SEA, IAI), then both. This would show how the architecture gets more non-rotationally invariant as the different components are added compared to the beginning. Comparing against FTT is interesting, but it does not separate the impact of the overall differences in the architecture vs (SEA, IAI).\n\n**A8**: Thank you for your constructive suggestion. We conduct an additive study and an ablation study (see Fig. 5 of the revised version) to inspect the benefits introduced by SPA and IAI on breaking the rotational invariance property of DL models. We find that:\n\n* With SPA and IAI, our Excelformer shows better non-rotationally invariance, compared with previous works.\n\n* On the FTT backbone, the addition of SPA and IAI makes positive contributions to break the rotational invariance property.\n\n* On the Excelformer architecture, removing SPA and IAI weakens Excelformer's non-rotational invariance property.\n\n* **The effects of adding or removing SPA and IAI are cumulative.**\n\nThank you once again for your valuable time and constructive feedback. We have taken your feedback into careful consideration and have made revisions to the manuscript accordingly. We kindly request you to reconsider the ranking, considering the improvements (especially the detailed ablation and additive studies)  made and the significant practical value that ExcelFormer offers.\n\nSincerely,\n\nAuthors of paper 4888\n\n---\n\nReferences:\n\n`[1]` Why do tree-based models still outperform deep learning on typical tabular data? NeurIPS, 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615894369,
                "cdate": 1700615894369,
                "tmdate": 1700647263181,
                "mdate": 1700647263181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1sw1XpFimU",
            "forum": "sYv3OMboTF",
            "replyto": "sYv3OMboTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4888/Reviewer_w8mp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4888/Reviewer_w8mp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a few modifications to the transformer\n  architecture for small tabular data problems. The modifications are\n  motivated by (1) the lack of rotational invariance in GBDTs and by (2) the\n  efficacy of data augmentations in mainstream DL domains.\n\n  To address (1), the authors propose:\n  - semi-permeable attention (regular self-attention is masked such that more important features do not interact with less important features in self-attention)\n  - interaction-attenuated initialization (initializing weights in semi-permeable attention with small values)\n\n  To address (2) authors propose two variations of mixup tailored for tabular data problems:\n  - Feat-Mix: swapping a random subset of features in two samples and mixing the labels taking feature's MI with the target into account\n  - Hid-Mix: mixing channels after feature embedding and mixing labels proportionally, as in mixup\n\n  With those changes, the proposed ExcelFormer outperforms deep-learning\n  baselines (both traditional and more recent transformer-based models)\n  and GBDTs in terms of average rank on 96 small (< 10000 samples) tabular problems."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly and nicely written (both in overall structure and details in the technical details regarding proposed methods).\n- It builds upon previous observations in its domain (tabular data) and proposes interesting \"domain-specific\" solutions to previously stated challenges/points for potential growth. For example, a large portion of the paper concerns with rotational invariance or the lack thereof as an inductive bias and.\n- It obtains decent empirical results by integrating said solutions to the transformer architecture."
                },
                "weaknesses": {
                    "value": "My concerns boil down to two things:\n\n(1) **Using reduced rank as the main and only metric of model performance**. There are multiple problems I see with this approach to reporting the results, which keep me from agreeing with the ExcelFormer performance claims:\n   - On this particular set of datasets DL models already perform on-par with GBDTs in terms of average rank (see FT-Transformer avg. rank), thus win over GBDT\n   - The degree of improvement (in terms of the task metrics) is not quantifiable from the average rank. Did the ExcelFormer improved upon vanilla FT-Transformer by 10%, 50% in terms of AUC, neg. RMSE, ACC? The magnitude of the improvement is also important.\n\nSee also `[1]` regarding issues with comparing average ranks of multiple algorithms across multiple datasets.\n\nI see that you provide all the results (albeit without standard deviations) for all models from Table 1, but this full table from the appendix is on the other side of the spectrum \u2013 too large to make generalizable conclusions. A more \"zoomed in\" view on performance would be very helpful. For example, you could provide metrics for DL baselines, GBDTs and ExcelFormer variants on datasets which were initially \"won\" by GBDT, but the changes introduced in ExcelFormer turned this around (I assume here that ExcelFormer is in essence a Transformer with potentially important domain-specific tweaks, comparison with MLP and FT-Transformer should be enough for a conclusion).\n\n(2) **Limited ablations and comparisons to baselines**. The paper proposes a few architectural tweaks for a base transformer model: SPA instead of MHSA, IAI initialization in attention, new FFN block, new nonlinearity. With SPA and IAI highlighted as the more important ones. But the section with the ablation is rather short and lacking details regarding the setup, reporting only average rank performance. Could you provide a more detailed ablation and comparison to the vanilla transformer. For example:\n- Transformer (no SPA, IAI, fancy embeddings and GLUs in FFN)\n- Transformer + SPA\n- Transformer + IAI\n- Transformer + SPA + IAI\n- ExcelFormer\n\nA subset of datasets with metrics instead of ranks would be enough (see point 1).\n\nFor a second contribution - novel data augmentations, I believe they could be compared with baselines from pertaining on tabular data `[2,3,4]`, where resampling from marginal distributions for a set of columns was shown to be a decent augmentation. The results for the simplest possible setup (like MLP with all features linearly embedded \u2013 MLP-LR from `[5]`) with different augmentation strategies:\n- Resample Augmentation\n- Feat-Mix\n- Hid-Mix\n- Feat-Mix + Hid-Mix\n\nwould greatly improve the understanding of the efficacy of the proposed augmentations for tabular data.\n\nIn SPA and Feat-Mix, ExcelFormer uses mutual information. Could you discuss how different ways of estimating mutual information compare? It seems like a significant detail, but there are no mentions of this in the ablations or the experimental setup.\n\n**References**:\n- `[1]` Benavoli, Alessio, Giorgio Corani, and Francesca Mangili. \"Should we really use post-hoc tests based on mean-ranks?.\" The Journal of Machine Learning Research 17.1 (2016): 152-161.\n- `[2]` Bahri, Dara, et al. \"Scarf: Self-supervised contrastive learning using random feature corruption.\" arXiv preprint arXiv:2106.15147 (2021).\n- `[3]` Yoon, Jinsung, et al. \"Vime: Extending the success of self-and semi-supervised learning to tabular domain.\" Advances in Neural Information Processing Systems 33 (2020): 11033-11043.\n- `[4]` Rubachev, Ivan, et al. \"Revisiting pretraining objectives for tabular deep learning.\" arXiv preprint arXiv:2207.03208 (2022).\n- `[5]` Gorishniy, Yury, Ivan Rubachev, and Artem Babenko. \"On embeddings for numerical features in tabular deep learning.\" Advances in Neural Information Processing Systems 35 (2022): 24991-25004."
                },
                "questions": {
                    "value": "Technical details I'd like to clarify:\n  - Could you provide details on how you compute mutual information, used in proposed augmentation and the attention module?\n  - Could you provide more info on how ablations were run? You compare ablated variants to the fully tuned baseline, are the ablated variations also tuned?\n  - How long were the models trained for? Was early stopping used during training? How the number of steps compare across deep models?\n  - How ranks were calculated?\n\nOther remarks:\n- In the figure 3 hid-mix is called hidden-mix (only in the figure and nowhere else)\n- The table with various datasets aggregations looks redundant in its current form. Not much interesting there besides TabPFN comparison. Not sure why grouping by classification vs regression and the number of continuous/categorical features should in differentiate general purpose methods. The results on the aggregated benchmark tell basically the same story: ExcelFormer is better than the baseline in terms of average rank. This space could be used to expand and address weaknesses (more ablations, more metrics).\n\nOverall, I like the paper, and find the proposed architectural tweaks very interesting and important for the field.\n\nI'm open to raise the score if my two concerns are addressed:\n\n1. Results on multiple **challenging for DL datasets** where ExcelFormer significantly outperforms the DL competitors are demonstrated (not in ranks, but in raw metrics improved)\n2. Comparisons for augmentations and ablations for SPA and IAI are presented (preferably on the datasets from point 1).\n\nLooking forward to the discussion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4888/Reviewer_w8mp"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4888/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782367945,
            "cdate": 1698782367945,
            "tmdate": 1699636472983,
            "mdate": 1699636472983,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AArvjn0ACk",
                "forum": "sYv3OMboTF",
                "replyto": "1sw1XpFimU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging the clarity and significance of our work in this field and expressing your appreciation for our paper. We trust that our responses will effectively address all your concerns.\n\n---\n\n> **Q1**: Using reduced rank as the main and only metric of model performance.\n\n**A1**: Thank you for your suggestions, and we acknowledge the limitations of mean-rank. We utilized the performance rank following previous works (e.g., `[1]`). Additionally, we compute **the average normalized scores** (introduced in `[2]`) to aggregate the model performances across datasets. The results are presented in Table 2, Table 7, Table 8 in the revised version. The average normalized scores linearly scale the model's performance, providing insights into the performance differences among models beyond just ranking. The definitions are presented in Appendix F.\nInterestingly, we observe that average normalized score and performance rank tend to offer consistent evaluations of the models. This consistency may be attributed to the substantial volume of datasets we employed. **Both performance rank and average normalized scores consistently demonstrate that our model outperforms previous works, on various datasets.**\n\nThe detailed results are summarized below:\n\n**(1)** The performances on 96 small-scale datasets:\n\n* Binary-class Classification (51%; metric: Rank{$\\downarrow$} (Average Normalized AUC {$\\uparrow$}); best are in bold):\n\n| Model  | Excelformer | FTT (t)   | XGb (t)  | Cat (t)| MLP (t) | DCNv2 (t)     | AutoInt (t)    | SAINT (t)    | TransTab (d) | XTab (d)               | TabPFN (t)         |\n|-|-|--|-|-|-|--|--|--|--|--|--|\n| Setting: Hid-Mix (d) | **3.88 (0.79)**  | 4.88 (0.74)   | 5.97 (0.63)  | 5.77 (0.65)   | 6.61 (0.60)    | 6.38 (0.57)       | 6.63 (0.56)            | 6.07 (0.64)  | 6.31 (0.64)   | 9.50 (0.24)    | 4.01 (0.78) |\n| Setting: Mix Tuned   | **3.78 (0.80)**      | 4.91 (0.73)     | 5.95 (0.62)  | 5.79 (0.64)   | 6.60 (0.59)    | 6.39 (0.56)           | 6.71 (0.56)            | 6.10 (0.63)            | 6.37 (0.63)   | 9.46 (0.24)             | 3.95 (0.78)   |\n\n* Regression (49\\%; metric: Rank{$\\downarrow$} (Average Normalized nRMSE {$\\uparrow$}); best are in bold):\n\n| Model | Excelformer  | FTT (t)  | XGb (t)  | Cat (t)   | MLP (t) | DCNv2 (t)  | AutoInt (t) | SAINT (t) | TransTab (d)  | XTab (d)  |\n|-|-|--|--|--|--|-|--|--|-|--|\n| Setting: Hid-Mix (d) | **3.81 (0.81)**  | 4.45 (0.78)  | 3.43 (0.83)      | 4.26 (0.80)  | 4.64 (0.74)   | 6.26 (0.52)  | 5.53 (0.65)  | 5.64 (0.66)  | 8.21 (0.33)   | 8.79 (0.20)   |\n| Setting: Mix Tuned   | **3.17 (0.86)**   | 4.49 (0.78)    | 3.53 (0.82)       | 4.28 (0.81)     | 4.74 (0.74)   | 6.32 (0.52)    | 5.68 (0.65)   | 5.72 (0.65)   | 8.23 (0.33)  | 8.83 (0.19)    |\n\n**(2)** On 21 larger-scale datasets, we compute the **normalized average scores of ACC, AUC, and nRMSE ($\\uparrow$)** on multi-class classification datasets, binary-class classification datasets, and regression datasets, respectively. **It is evident that Excelformer performs best.**\n\n| Model (default hyperparameters)  | binclass    | regression  | multiclass|\n|-|-|-|-|\n| XGboost (d)  | 0 $\\pm$ 0    | 0.470$\\pm$0.400  | 0.218$\\pm$ 0.400 |\n| Catboost (d) | 0.977$\\pm$0.034  | 0.286$\\pm$0.291  | 0.280$\\pm$0.408  |\n| FTT (d)   | 0.807$\\pm$0.397  | 0.613$\\pm$0.313  | 0.763$\\pm$0.163  |\n| ExcelFormer w/ Feat-Mix  (d) | 0.982$\\pm$0.022  | 0.513$\\pm$0.422  | 0.791$\\pm$0.143  |\n| ExcelFormer w/ Hid-Mix (d) | **0.976**$\\pm$0.030 | **0.825**$\\pm$0.279 | **0.990**$\\pm$0.020 |\n\n| Model (hyperparameter tuned) | binclass | regression| multiclass |\n|-|-|-|-|\n| XGboost (t) | 0.409$\\pm$0.424  | 0.756$\\pm$0.285  | 0.258$\\pm$0.241  |\n| Catboost (t) | 0.281$\\pm$0.365  | 0.276$\\pm$0.385  | 0.095$\\pm$0.135  |\n| FT-T (t)   | 0.316$\\pm$0.387  | 0.428$\\pm$0.398  | 0.596$\\pm$0.425  |\n| ExcelFormer (Mix Tuned)| 0.526$\\pm$0.413  | **0.865**$\\pm$0.150 | 0.735$\\pm$0.286  |\n| ExcelFormer (Fully Tuned) | **0.777**$\\pm$0.393 | **0.865**$\\pm$0.203 |**0.957**$\\pm$0.085 |\n\n**(3)** Following the suggestions of the reviewer `J7k8`, we also provide the results on 16 **additional multi-class classification datasets** (refer to Table 7 in the revised version), and **both performance rank and average normalized ACC indicate that Excelformer performs best**:\n\n| | XGb (t)       | Cat (t)  | FTT(t)  | MLP(t) | DCNv2(t)   | AutoInt(t)  | SAINT(t) | Excelformer(t)  |\n|-|-|-|--|-|-|-|--|--|\n| Rank($\\pm$std) {$\\downarrow$}    | 3.91$\\pm$2.28     | 2.53$\\pm$1.70     | 5.53$\\pm$1.85    | 6.09$\\pm$1.88    | 5.97$\\pm$1.61    | 4.66$\\pm$1.65    | 5.22$\\pm$1.41    | **2.09$\\pm$1.23** |\n| average normalized ACC ($\\pm$std) {$\\uparrow$} | 0.65$\\pm$0.35 | 0.82$\\pm$0.27    | 0.39$\\pm$0.33   | 0.25$\\pm$0.31   | 0.33$\\pm$0.35   | 0.50$\\pm$0.31   | 0.42$\\pm$0.29   | **0.85$\\pm$0.24** |\n\n---\n\nReferences:\n\n`[1]`XTab: Cross-table Pretraining for Tabular Transformers. ICML, 2023\n\n`[2]`Learning hyperparameter optimization initializations. DSAA, 2015"
                    },
                    "title": {
                        "value": "To Reviewer w8mp (part 1)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587932705,
                "cdate": 1700587932705,
                "tmdate": 1700633001899,
                "mdate": 1700633001899,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kh4xSj2aRW",
                "forum": "sYv3OMboTF",
                "replyto": "1sw1XpFimU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer w8mp (part 2)"
                    },
                    "comment": {
                        "value": "> **Q2**: Results on multiple challenging-for-DL datasets where ExcelFormer significantly outperforms the DL competitors should be demonstrated.\n\n**A2**: Thank you for your insightful suggestion. We agree that the tabular datasets are diverse, making parts of them specifically challenging for DL (i.e., GBDTs achieve relatively better performances while DL models obtain lower performances). In Table 2, we inspected the model performances on diverse types of tabular datasets / tasks. Additionally, in response to your suggestion, we introduced a subset known as \"GBDT-best\" datasets. The datasets in the \"GBDT-best\" group are selected based on: ignoring the results of Excelformer, XGBoost or CatBoost achieves the best performance. These results are included in the revised version (Table 2) with detailed analysis, and are also presented below.\n\nThough these datasets pose challenges for deep learning approaches, **we observed a reversal in performance with the changes introduced in ExcelFormer**:\n\n* Excelformer still outperforms GBDTs on \"GBDT-best\" classification datasets.\n\n* Excelformer surpasses all DL competitors and achieves competitive performances with CatBoost. Importantly, this doesn't imply that Excelformer lags behind XGboost in regression, since these datasets are GBDT-friendly. In fact, Excelformer outperforms GBDTs on regression tasks (please refer to `Q1 & A1`).\n\n* On those datasets initially \"won\" by GBDTs, Excelformer successfully outperforms GBDTs on 11 out of 15 GBDT-best classification datasets and outperforms GBDTs on 8 out of 22 GBDT-best regression datasets.\n\nHere we present the results on GBDT-best datasets (top 3, including GBDTs, are marked in bold):\n\n(1) Binclass Classification Tasks on GBDT-best datasets, measured by: Rank {$\\downarrow$} (Ave Normalized AUC {$\\uparrow$})\n\n| Model                  | Excelformer               | FTT (t)                | XGb (t)                | Cat (t)                | MLP (t)                | DCNv2 (t)              | AutoInt (t)            | SAINT (t)              | TransTab (d)           | XTab (d)               | TabPFN (t)             |\n|--|--|---|---|----|--|-|--|-|------------------------|------------------------|------------------------|\n| Setting: Hid-Mix (d)   | **4.63 (0.82)**  | 4.70 (0.79) | **3.50 (0.84)** | **3.73 (0.83)**  | 6.50 (0.65)            | 6.70 (0.64)            | 7.27 (0.53)            | 7.43 (0.63)            | 6.27 (0.69)            | 10.03 (0.16)           | 5.23 (0.71)            |\n| Setting: Mix Tuned    | **3.28 (0.88)**   | 4.63 (0.77) | **3.94 (0.82)**    | **3.81 (0.81)**  | 6.69 (0.64)            | 6.97 (0.63)            | 7.66 (0.53)            | 7.41 (0.63)            | 6.44 (0.68)            | 10.09 (0.15)           | 5.09 (0.72)            |\n\n(2) Regression Tasks on GBDT-best datasets, measured by: Rank {$\\downarrow$} (Ave Normalized nRMSE {$\\uparrow$})\n\n| Model                  | Excelformer               | FTT (t)                | XGb (t)                | Cat (t)                | MLP (t)                | DCNv2 (t)              | AutoInt (t)            | SAINT (t)              | TransTab (d)           | XTab (d)               |\n|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|------------------------|\n| Setting: Hid-Mix (d)   | **4.66 (0.70)**  | 4.95 **(0.70)** | **1.95 (0.89)**    | **3.11 (0.85)**    | 5.41 (0.64)| 6.50 (0.47)            | 5.86 (0.58)            | 6.00 (0.55)            | 8.14 (0.28)            | 8.41 (0.21)            |\n| Setting: Mix Tuned    | **3.18 (0.82)** | 5.09 (0.69) | **2.18 (0.87)** | **3.23 (0.85)**    | 5.55 (0.63)            | 6.64 (0.47)            | 6.18 (0.57)            | 6.18 (0.54)            | 8.27 (0.27)            | 8.50 (0.18)            |\n\nThank you again for your constructive suggestion, which helps further validate the contributions of Excelformer. All the results with discussions are included in the revised version."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589747214,
                "cdate": 1700589747214,
                "tmdate": 1700633700549,
                "mdate": 1700633700549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hTwlIfQwHS",
                "forum": "sYv3OMboTF",
                "replyto": "1sw1XpFimU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer w8mp (part 3)"
                    },
                    "comment": {
                        "value": "> **Q3**: The table with various datasets aggregations looks redundant...Not sure why grouping by classification vs regression and the number of continuous/categorical features ...\n\n**A3**: Thank you for your concern. We present performances on these subgroups separately because **tabular datasets exhibit significant diversity**, and thus different previous models only excel on specific portions of them (see Table 2: excluding Excelformer, the best models vary in different groups). This contrasts with domains like computer vision, where a single model can dominate most datasets. Excelformer aims to achieve a similar impact in tabular data prediction, but no previous works achieved this. **We have made some modifications and try to clarify this point in the introduction**.\n\nThis diversity sparks ongoing debates within the community about the state-of-the-art (SOTA) model. Users often find model selection challenging, and reviewers `J7k8` also raises questions about the inconsistency in previous works being initially stated as SOTA and later \"debunked\". **Similar to drug studies analyzing various characteristics (such as race, age, pregnancy) to determine applicable scenarios, our Table 2 experiments aim to investigate the universality of compared models.** The proved universal applicability is a key advantage of Excelformer, not achieved by previous works, including GBDTs (as evident in Table 2, where GBDTs perform worse than FTT when the feature count is lower than 16). In essence, we argue that AI development aims to facilitate practical applications: the amalgamation of versatility and the convenience of not requiring hyperparameter tuning positions Excelformer as an excellent practical tool for tabular data prediction tasks, and Table 2 is helpful to users (especially those with limited computer science proficiency or computational resources) in model selection.\n\nHonestly, we are open to discussions and welcome further exploration of these considerations.\n\n---\n\n> **Q4**: Limited ablations and comparisons to baselines. Comparisons for augmentations and ablations for SPA and IAI should be presented.\n\n**A4**: Thank you for your thoughtful question. We would like to clarify this points from two aspects: the selection of ablation study datasets; presenting more ablation results.\n\n* The selection of tabular datasets. **While some literature claims that GBDTs \"dominate\" tabular data prediction tasks, it's essential to clarify that this is not an absolute truth.** See Table 2, TabPFN often outperforms GBDTs in classification on small-scale datasets, and GBDTs may perform worse than other DL models when the feature count is lower than 16. Thus, **surpassing GBDTs is not the sole objective in building Excelformer**; beyond defeating GBDTs, we also aim to safeguard the advantageous niche of DL models (we have made some changes to clarify this point in the introduction section; thank you for bringing it to our attention). To simulate real-world applications, our ablation studies are based on all datasets (including those GBDT-best datasets).\n\n* More ablation study results.\n\n**(I)** Key Structure Components\n\n1. Following your suggestion, we conduct additive study to examinate the performances of the key structure components on a vanilla Transformer (Tra). We find that all the components make positive contributions to tabular data prediction tasks, and the contributions are cumulative.\n\n| | Tra | Tra + IAI | Tra + SPA |Tra + SPA + IAI| Excelformer (no data augmentation) |\n|-|-|-|-|-|-|\n| binclass (Ave Norm AUC, \u2191)| 0.232\u00b10.36 | 0.683\u00b10.37|0.289\u00b10.37|0.753\u00b10.33|0.775\u00b10.30|\n| regression (Ave Norm nRMSE, \u2191) |0.138\u00b10.30| 0.738\u00b10.25| 0.751\u00b10.26| 0.766\u00b10.35 |0.911\u00b10.16|\n\n2. Additionally, we perform an additive study and an ablation study (refer to Fig. 5) to examine the advantages introduced by SPA and IAI in disrupting the rotational invariance property of DL models. We find:\n\n* On the FTT backbone, the addition of SPA and IAI individually to some extent breaks the rotational invariance. \n* On the Excelformer architecture, removing SPA and IAI weakens Excelformer's rotational invariance property.\n* **The effects of adding or removing SPA and IAI are cumulative.**\n\n**(II)** Compare Hid-Mix, Feat-Mix, and Resample on MLP:\n\nThe results are presented below (in Table 5 of the revised version). We observe that the performances of feature resampling fall between Feat-Mix and Hid-Mix. We notice that each approach excels on different datasets. We will provide more discussions on this aspect in the final version.\n\n|| Resample | Feat-Mix | Hid-Mix | No data augmentation |\n|-|-|-|-|-|\n| Binclass (Ave Norm AUC, \u2191) | 0.576\u00b10.42 | 0.530\u00b10.41 | 0.614\u00b10.41 | 0.436\u00b10.42 |\n| regression (Ave Norm nRMSE, \u2191)|0.661\u00b10.39|0.629\u00b10.40 | 0.681\u00b10.39 | 0.334\u00b10.42 |\n\nAll the additional results have been added into the revised version. Thank you for your constructive suggestion!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700591757443,
                "cdate": 1700591757443,
                "tmdate": 1700647299575,
                "mdate": 1700647299575,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p4Ax2djlBr",
                "forum": "sYv3OMboTF",
                "replyto": "1sw1XpFimU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4888/Authors"
                ],
                "content": {
                    "title": {
                        "value": "To Reviewer w8mp (part 4)"
                    },
                    "comment": {
                        "value": "> **Q5**: How to compute mutual information?\n\n**A5**: In the implementation, we utilize the scikit-learn package to calculate mutual information. Specifically, for classification tasks, we employ the \"feature_selection.mutual_info_classif\" function, and for regression tasks, we use the \"feature_selection.mutual_info_regression\" function. Thank you for your reminder! We have depitcted the implementation in Appendix F.\n\n---\n\n> **Q6**: You compare ablated variants to the fully tuned baseline, are the ablated variations also tuned?\n\n**A6**: Yes. For fair comparison, the ablated variations are tuned under the same setting.\n\n---\n\n> **Q7**: How long were the models trained for? Was early stopping used during training? How the number of steps compare across deep models?\n\n**A7**: We implemented early stopping with a patience of 32, and the maximum epoch for Excelformer was set to 500. To ensure a comprehensive tuning of XGBoost and CatBoost for optimal performance, we conducted 500 tuning iterations and increased the number of estimators to 4096 (most previous works used less than 2000). Excelformer underwent fine-tuning for 50 iterations across all datasets. For other deep learning approaches, on large-scale datasets, we followed the settings outlined in `[1]` or `[2]`. On small-scale datasets, all deep learning approaches were tuned for 50 iterations, as we found this to be sufficient for small-scale datasets and is fair among DL approaches. We did not set a time limit because fine-tuning on small-scale datasets does not consume an excessive amount of time. We have included these information in the revised version, thank you!\n\n> **Q8**: How ranks are computed?\n\n**A8**: We performed 5 runs with different random seeds and calculated the average results for each dataset. Additionally, we computed the overall rank across datasets for comparison. Average rank is given to tied values. These details were included in Appendix F of the revised version. Thank you!\n\n> **Q9**: In the figure 3 hid-mix is called hidden-mix (only in the figure and nowhere else)\n\n**A9**: Thank you for bringing this to our attention. We have revised Figure 3 in the updated version.\n\n**Thank you again for your time and your constructive feedback, which we've carefully considered and incorporated into the revised manuscript.** We kindly request a reconsideration of the ranking, taking into account the improvements made and the substantial practical value of ExcelFormer.\n\n\nSincerely,\n\nAuthors of paper 4888\n\n---\n\nReferences:\n\n`[1]`Revisiting deep learning models for tabular data. NeurIPS, 2021.\n\n`[2]`Why do tree-based models still outperform deep learning on typical tabular data? NeurIPS, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4888/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592378677,
                "cdate": 1700592378677,
                "tmdate": 1700634920741,
                "mdate": 1700634920741,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]