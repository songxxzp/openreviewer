[
    {
        "title": "Deepfake Detection with Contrastive Learning in Curved Spaces"
    },
    {
        "review": {
            "id": "LcrAJBuZxH",
            "forum": "G4D6jClNFl",
            "replyto": "G4D6jClNFl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2798/Reviewer_GhyQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2798/Reviewer_GhyQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of face deepfakes detection. It uses a supervised contrastive learning where a prior set of possible \nmodifications/alteration  of faces is used as data augmentation. The main novelty of the paper comes from the use of a mixed curvature space\nfor the embedding, designed as a product of hyperspherical and hyperbolic geometries. Within this geometries, prototypes are defined \nas corresponding to the different classes of possible alterations of pristine faces. Then a dissimilarity measure to those mixed-space prototypes \nis defined as a combination of a distance over the sphere and a measure of alignment with a hyperbolic prototype thanks to the Busemann \nfunction. A detection score is crafted as a product of similarity in the hyperspherical embedding and a confidence score in the hyperbolical space\ndefined as the distance to the origin. Thorough experiments are conducted on the FaceForensics++ dataset, and comparisons with SOTA approaches \nreveal added value of the mixed-space representation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Empirical evidences thorough experiments of enhanced detection performances with the proposed method ;\n - although I am not an expert in deepfake detection, the considered SOTA seems relevant and complete"
                },
                "weaknesses": {
                    "value": "- the paper combines two well-known strategies (contrastive learning on an hypersphere embedding and Busemann prototypes\nfrom the Ghadimi et al. Neurips paper). The amount of novelties with this respect is low, and one could expect from such a paper\na better justification of the choice of this mixed-curvature space besides \u2018the manifold of faces is complicated and non-Euclidean\u2019.\nNotably, it is not clear which aspects necessary to deepfake detection is captured by the two geometries \n- some details are missing from the experimental part (see my questions below). The ablation study is not fully convincing to me \n\nAll in all, and though the proposed approach seems novel and has merits, it seems to me that the paper would be more suited and  impactful in the computer vision community, as far as the novel insight wrt. representation learning are rather limitated."
                },
                "questions": {
                    "value": "- in the experimental section, I did not see the dimensions used fo both embeddings (I may have overlooked). Are they comparable to what is used in  other supervised contrastive learning strategies ? What is the impact of those dimensions on performances ? \n- in the ablation study, do you keep the total number of dimensions constant (e.g. if S^100 + H^100 is used, do you compare with a hyperspherical embedding with dimension S^200 ?) I really believe that this question (effectiveness of combination of spherical and hyperbolic geometry) is unsufficiently detailed in the paper)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2798/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2798/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2798/Reviewer_GhyQ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697619505395,
            "cdate": 1697619505395,
            "tmdate": 1700673792404,
            "mdate": 1700673792404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4KGflgHPbh",
                "forum": "G4D6jClNFl",
                "replyto": "LcrAJBuZxH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer GhyQ (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for providing constructive reviews. We have addressed each of your questions individually in our response and hope these clarifications contribute to an improved rating. We have revised the paper according to your feedback, marking the changes in magenta. Additionally, we've highlighted in brown the text that was either part of the initial submission or present in the Appendix, which may have been overlooked.\n\n**Question: the paper combines two well-known strategies ... All in all, and though the proposed approach seems novel and has merits, it seems to me that the paper would be more suited and impactful in the computer vision community, as far as the novel insight wrt. representation learning are rather limited.**\n\n**Reply:**\n\nThank you for your remark and recognition.\n\n\n1. First, we believe that our approach goes beyond a simple combination of two known learning methods. In relation to existing works, different novelties (new loss terms, new ideal prototypes with improved properties) were considered in each space. Additionally, a new method for generating predefined/ideal prototypes was proposed. \n\n- In hypersphere space (Sec. 3.3), novel terms (the first terms in Eq.3) were incorporated instead of relying solely on the well-known supervised contrastive loss (sCL). This results in improved performance, as demonstrated in Tab.4.  In essence, $L_S$ differs from and outperforms $L_{sCL}$.\n\n- In hyperbolic space (Sec. 3.4), we already clarified the distinctions between our method and that of Atigh et al. (2021) in Appendix (we moved it to the main paper). The main contribution of Atigh et al. (2021) is to integrate a penalized term into the Busemann function to avoid the overconfidence issue. In contrast, we just employ the Busemann function but **without** the penalized term. In our experiments, we find that when using two considered complementary losses, penalizing the distance to the ideal prototype is unnecessary. Moreover, unlike Atigh et al. (2021) that **randomly** samples points from the unit sphere, we use **the predefined evenly distributed points** as ideal prototypes.\n\n- We introduce a novel method to generate evenly distributed vectors (at the end of Sec 3.1) that can be used as both predefined prototypes in sphere space and as ideal prototypes in hyperbolic space. \n\n2. Second, we have already conducted the experiments to demonstrate the hierarchical relations in deepfake detection datasets. Originally located in the Appendix, we have now integrated them into the main paper. Please refer to Figure 2.\n\u201cLeft: Inherent hierarchical structure of different manipulations in FF++ represented as a tree. Right: t-SNE projection of embeddings for fake images from the FF++ in the Poincar\u00e9 ball model. Dotted line circles indicate leaf nodes. CTru effectively captures coarse semantic differences closer to the origin in hyperbolic space (e.g., Reenactment vs. Replacement), with finer-grained distinctions positioned farther away (e.g., Face2Face vs. NeuralTextures).\u201d\n\nWe also developed the explanation of using spaces with different curvatures in page 1.\n\n3. Third, we also think that the representation learning aspect may have been overlooked. In Fig. 1, given an image I, we compute $z=f_e(I)$ while $f^S_p$ and $f^H_p$ are only two non-parameter projection operators: $z^S=f^S_p(z) = z/norm(z)$; $z^H=f^H_p(z)=exp_0(z)$ (defined in Sec 3.3 and 3.4). We only have a single vector $z$ which conveys the rich information extracted from image I, **not two**. Our approach involves pushing $z^S$ and $z^H$ towards the predefined/ideal prototypes as part of the learning objective for $z$.\n\n- We further evaluate the performance of feature representations learned with different losses using CIFAR10/100 with ResNet-18 backbone in **Tab. 6 in the revised paper** (considering larger backbones/datasets for future work).  $L_H$ with our predefined prototypes outperforms Atigh et al. (2021). $L_S$ surpasses $L_{sCL}$, and ``Ours'' performs the best, proving the strength of our learned features in image classification tasks.\n\nDataset  | $L_{CE}$  |  Atigh et al. |  $L_{sCL}$  |  $L_{H}$ + our prototypes | $L_{S}$  | Ours \n---|:---:|:---:|---:|:---:|:---:|:---:\nCIFAR10 | 94.9 | 92.3 | 95.0  | 94.1 | 95.2 | 95.4 \nCIFAR100 | 76.1 | 65.8 | 76.3  | 68.0 | 76.7 | 77.8"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700218581922,
                "cdate": 1700218581922,
                "tmdate": 1700218581922,
                "mdate": 1700218581922,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PePrPGT7U7",
                "forum": "G4D6jClNFl",
                "replyto": "LcrAJBuZxH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer GhyQ (2/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThe initial submission may have contained some ambiguities in certain details. We have thoroughly revised the paper in accordance with your reviews and hope that this prompts you to reconsider your rating.\n\n1. **Question**. In the experimental section, I did not see the dimensions used for **both** embeddings (I may have overlooked). Are they comparable to what is used in other supervised contrastive learning strategies ? What is the impact of those dimensions on performances?\n\n**Reply:**\nThank you for your questions.\n\nAs mentioned above, $f^S_p$ and $f^H_p$ are only two **non-parameter** projection operators, and we only have **one embedding** $z$ which conveys the rich information extracted from image I, **not two**. \n\nAs discussed in Sec 3.3, *supervised contrastive learnings are two-stage approaches*. During inference, they discard the projection layer obtained during training and require training additional linear layer(s) (using cross entropy in the original paper, cross-entropy). In contrast, **our method is one-stage**. Once trained either with $L_H$, $L_S$ or both (using predefined ideal prototypes), we can use the network as is (without removing any layer or training additional layer). Classification is **directly** done by comparing the network\u2019s output with predefined prototypes. In our experiments, we achieved similar performance with dimensions of 100 and 200. Similar findings have been observed in previous representation learning works when the output dimension of the projector is relatively low like 128 or 256. We plan to evaluate the performance of our method with higher output dimensions on larger datasets/backbones. However, it is worth noting that the most challenging deepfake datasets are relatively small compared to large-scale datasets comprising of millions samples.\n\n2. **Question**. In the ablation study, do you keep the total number of dimensions constant (e.g. if S^100 + H^100 is used, do you compare with a hyperspherical embedding with dimension S^200 ?)...\n\n**Reply:**\nThank you for your questions.\n\nThe dimension of $z^S$ and $z^H$ is identical to that of $z$, **yet at no point during training/inference do we combine the two vectors** $z^S$ and $z^H$. Our approach involves pushing $z^S$ and $z^H$ towards the predefined/ideal prototypes as part of the learning objective for $z$. As mentioned above, in our experiments, we achieved similar performance with relatively low dimensions of 100 and 200. We further demonstrated the effectiveness of our method in image classification tasks in Tab. 6.\n\nWe are eager to continue the discussion. Have all your concerns been addressed in our rebuttal, or are there any remaining comments you would like us to consider?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700222359765,
                "cdate": 1700222359765,
                "tmdate": 1700222359765,
                "mdate": 1700222359765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "En7O9r8gUI",
                "forum": "G4D6jClNFl",
                "replyto": "PePrPGT7U7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Reviewer_GhyQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Reviewer_GhyQ"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors\n\nThank you for adressing my concerns and your detailed review. I appreciate the efforts put into this process. \nNow I understand better why there are no two separate geometric spaces, but only one embedding. I might have been misled by Figure 1. \nI think one issue remain: you highlight a difference with Atigh et al. (2021) by saying that you have a uniform distribution over the hyperpshere, contrary to a random one. After checking the Atigh et al. (2021) paper, it seems not entirely true, since they use an initialization similar to the one used in the Hyperspherical Prototype Learning paper (from what I understood)\n \nI have updated my socre positively, but I still believe that the amount of novlety in the paper is rather moderated."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673880598,
                "cdate": 1700673880598,
                "tmdate": 1700673880598,
                "mdate": 1700673880598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ygQknmAwco",
                "forum": "G4D6jClNFl",
                "replyto": "LcrAJBuZxH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for your thoughtful and constructive feedback. We appreciate your acknowledgment of the efforts we've invested in addressing your concerns and providing a detailed review.\n\nUpon reevaluation, we acknowledge the importance of clarity in understanding the differences between our approach and Hyperspherical Prototype Networks (HPN) [1]. In our method, denoting the set of $K$ class labels as $C = \\\\{1,..,K\\\\}$ and the input dimensionality as $L$, we, like HPN, incorporate K prototypes $P = \\\\{p_{1},...,p_{K}\\\\}$ within the hypersphere. However, it is crucial to emphasize that our definition of uniformity and the methodology employed for prototype generation diverge from those presented in HPN.\n\n* First, the definition of prototype optimality in [1] differs from ours. They describes their points as *approximately* uniform.\n  We specifically opt for our points to be *evenly distributed*. Formally, in a Euclidean space $R^L$, a set $P$ of $K$ prototypes is said to be evenly distributed only if: $ \\\\forall\\\\, i \\\\neq j, \\\\;  p_{i} \\\\cdot p_{j} = -1/(K-1) $.\n> the optimal set of prototypes, $P^*$, is the one where the largest cosine similarity between two class prototypes.$p_i$, $p_j$ from the set is minimized:\n\\label{eq:hpn_proto}\n$$\n P^{*}  = min_{{P}' \\\\in P} \\\\bigg( \\\\max_{(k, l, k\\\\not=l) \\\\in C} \\\\cos(\\theta_{(\\mathbf{p}'_k,\\mathbf{p}'_l)}) \\bigg)\n$$ \n\n* Second, in [1], prototypes are derived through the gradient descent of a priori points before training. In contrast, our approach generates the set of points through a *closed form solution* that employs the vertices of a $L$-simplex, as elaborated in the supplementary material.\n> To position hyperspherical prototypes prior to network training, we rely on a gradient descent optimization for the loss function of~\\\\cref{hpn_proto}.\n   \n* Finally, our points exhibit the exact property of being *evenly distributed*, which is *contrastive-optimal*. It is noteworthy that contrastive losses attain their minimum once the representations of each class collapse to the vertices of a regular simplex, as demonstrated in [2]. \nAdditionally, it's important to mention that the vertices of a simplex form an ETF frame and are evenly distributed. We also note that these prototypes are \"Busemann\" ideal and can therefore be utilized in the Poincar\u00e9 ball model as prototypes.\n\nPlease note that relying solely on the closed form of the simplex results in the same identical set of points, which may be suboptimal due to numerous zeros in the upper triangular part of matrix prototypes. To address this limitation, our paper introduces a three-step generation process. We start with the closed form, add zero padding, apply the Gram-Schmidt process to compute a random basis, and then project the intermediate prototypes onto this basis. This process ensures a *unique* set that adheres to the *evenly distributed* property.\n\nWe appreciate your meticulous review of these aspects, and we will ensure that the manuscript accurately reflects these nuances. If you have further specific points or queries related to this comparison, we would be grateful for the opportunity to address them.\n\nThank you again for your valuable feedback.\n\n[1] Pascal Mettes, Elise van der Pol, and Cees G. M. Snoek. Hyperspherical prototype networks. In\nNeurIPS, 2019.\n\n[2] Florian Graf, Christoph D. Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised\ncontrastive learning. In ICML, 2021."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695036344,
                "cdate": 1700695036344,
                "tmdate": 1700695280563,
                "mdate": 1700695280563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WUoFJs8NvC",
            "forum": "G4D6jClNFl",
            "replyto": "G4D6jClNFl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2798/Reviewer_L6YJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2798/Reviewer_L6YJ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose building facial features by incorporating principles of hyperbolic geometry and using a contrastive loss on a hypersphere to aggregate similar faces, thereby achieving the goal of detecting forged faces. In general, this paper presents a promising approach that contributes to the advancement of deepfake detection."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. An interesting method for constructing facial features.\n2. An effective attempt for using contrastive loss to detect deepfakes."
                },
                "weaknesses": {
                    "value": "1. The authors should provide more case studies in the main manuscript, including new features in Section 3 and facial features after clustering using contrastive loss.\n2. More analysis on efficiency should be added, such as overall training time, parameters, and convergence steps.\n3. It is suggested that the authors consider applying this method to other well-known backbones, such as Xception."
                },
                "questions": {
                    "value": "na"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698640405741,
            "cdate": 1698640405741,
            "tmdate": 1699636222483,
            "mdate": 1699636222483,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0qNF2b87ji",
                "forum": "G4D6jClNFl",
                "replyto": "WUoFJs8NvC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer L6YJ"
                    },
                    "comment": {
                        "value": "Thank you for providing constructive reviews. We have addressed each of your questions individually in our response and hope these clarifications contribute to an improved rating. We have revised the paper according to your feedback, marking the changes in magenta. Additionally, we've highlighted in brown the text that was either part of the initial submission or present in the Appendix, which may have been overlooked.\n\n1. **Question. The authors should provide more case studies in the main manuscript, including new features in Section 3 and facial features after clustering using contrastive loss.**\n\n**Reply:**\nThank you for your suggestion. Indeed, we have already conducted the experiments to visualize the features after clustering using contrastive loss. **Originally located in the Appendix, we have now integrated them into the main paper.** Please refer to Figure 2.\n``Left: Inherent hierarchical structure of different manipulations in FF++ represented as a tree. Right: t-SNE projection of embeddings for fake images from the FF++ in the Poincar\u00e9 ball model. Dotted line circles indicate leaf nodes. CTru effectively captures coarse semantic differences closer to the origin in hyperbolic space (e.g., Reenactment vs. Replacement), with finer-grained distinctions positioned farther away (e.g., Face2Face vs. NeuralTextures).''\n\n\n2. **Question. More analysis on efficiency should be added, such as overall training time, parameters, and convergence steps.**\n\n**Reply:**\nThank you for your remarks. Some implementation details (number of training epochs, scheduler, inference time) were already provided but some were in Appendix. We synthesized them in the main paper.\n\nWe employ the AdamW optimizer with an initial learning rate of $0.002$ and no weight decay.\nThe model is trained for 200 epochs using the cosine scheduler without restart, preceded by a linear warmup of 20 epochs. We use a batch size of 64. Images are resized to $299 \\times 299$ before being fed into the model.\nAs many other recent detectors like SBI, SLADD, we use EffNetb4 with *19M parameters* as backbone by default. Training is conducted on two NVIDIA V100 GPUs and requires approximately *16 hours* while during inference, CTru achieves a frame rate of 74fps on a PC with RTX3060.\n\n3. **Question: It is suggested that the authors consider applying this method to other well-known backbones, such as Xception.**\n\n**Reply:**\nThank you for your suggestion. Xception has been widely utilized in deepfake detection. However, more recent studies, such as SBI, SLADD, have indicated that among CNN backbones, EffNet-b4 has demonstrated superior performance to Xception. In response to your recommendation, we conducted an evaluation of our algorithm using ResNet-50 and Xception. **As shown in Table 5 in the revised version**, our findings align with prior research. Currently, we are exploring Transformer backbones, but please note that they come with a higher parameter count and require pretraining on larger datasets than deepfake datasets."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217892163,
                "cdate": 1700217892163,
                "tmdate": 1700217892163,
                "mdate": 1700217892163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0x1n9ds12u",
                "forum": "G4D6jClNFl",
                "replyto": "0qNF2b87ji",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Reviewer_L6YJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Reviewer_L6YJ"
                ],
                "content": {
                    "title": {
                        "value": "Concerns are addressed."
                    },
                    "comment": {
                        "value": "Thanks for the response. My concerns are addressed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553147987,
                "cdate": 1700553147987,
                "tmdate": 1700553147987,
                "mdate": 1700553147987,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ckz1DVk95l",
            "forum": "G4D6jClNFl",
            "replyto": "G4D6jClNFl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2798/Reviewer_gKak"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2798/Reviewer_gKak"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to learn deepfake detection representations across multiple-curvature spaces in a self-supervised manner. The detection results combine advantages of both positive and negative curvature spaces. Experimental results validate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed model is the first attempt to learn representations across multiple-curvature spaces for deepfake detection. \n2. The proposed abnormal face generation method can generate fake faces of many different types.\n3. The experimental results show that the proposed model has satisfactory deepfake detection performances."
                },
                "weaknesses": {
                    "value": "The reason to combine both negative and positive curvature representation spaces in deepfake detection is not insightful. This makes the paper merely a combination of existing techniques.\n\n(1) The authors only emphasize that the Euclidean-based distances appear sub-optimal for faces as the complexity and nature of human faces go beyond a basic Euclidean manifold. This explanation is vague and general, thus not convincing. \n(2) As I know, using hyperbolic space representations always work well for the tasks with hierachical relation nature. However, the authors fail to explain the inherent hierachical relations in deepfake detection tasks."
                },
                "questions": {
                    "value": "I suggest the authors give more detailed and insightful analysis to explain the motivation of using curved spaces."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698659650873,
            "cdate": 1698659650873,
            "tmdate": 1699636222401,
            "mdate": 1699636222401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kjYoTaSlbv",
                "forum": "G4D6jClNFl",
                "replyto": "Ckz1DVk95l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer gKak (1/2)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for providing constructive reviews. We have addressed each of your questions individually in our response and hope these clarifications contribute to an improved rating. We have revised the paper according to your feedback, marking the changes in magenta. Additionally, we've highlighted in brown the text that was either part of the initial submission or present in the Appendix, which may have been overlooked.\n\n**Question: The reason to combine both negative and positive curvature representation spaces in deepfake detection is not insightful. This makes the paper merely a combination of existing techniques.**\n\n**Reply:**\n\nThank you for your remarks.\n\n1. First, we believe that our approach goes beyond a simple combination of two known learning methods. In relation to existing works, different novelties (new loss terms, new ideal prototypes with improved properties) were considered in each space. Additionally, a new method for generating predefined/ideal prototypes was proposed. \n\n- In hypersphere space (Sec. 3.3), novel terms (the first terms in Eq.3) were incorporated instead of relying solely on the well-known supervised contrastive loss (sCL). This results in improved performance, as demonstrated in Tab.4.  In essence, $L_S$ differs from and outperforms $L_{sCL}$.\n\n- In hyperbolic space (Sec. 3.4), we already clarified the distinctions between our method and that of Atigh et al. (2021) in Appendix (we moved it to the main paper). The main contribution of Atigh et al. (2021) is to integrate a penalized term into the Busemann function to avoid the overconfidence issue. In contrast, we just employ the Busemann function but **without** the penalized term. In our experiments, we find that when using two considered complementary losses, penalizing the distance to the ideal prototype is unnecessary. Moreover, unlike Atigh et al. (2021) that **randomly** samples points from the unit sphere, we use **the predefined evenly distributed points** as ideal prototypes.\n\n- We introduce a novel method to generate evenly distributed vectors (at the end of Sec 3.1) that can be used as both predefined prototypes in sphere space and as ideal prototypes in hyperbolic space. \n\n2. Second, we think that the representation learning aspect may have been overlooked. In Fig. 1, given an image I, we compute $z=f_e(I)$ while $f^S_p$ and $f^H_p$ are only two non-parameter projection operators: $z^S=f^S_p(z) = z/norm(z)$; $z^H=f^H_p(z)=exp_0(z)$ (defined in Sec 3.3 and 3.4). We only have a single vector $z$ which conveys the rich information extracted from image I, not two. Our approach involves pushing $z^S$ and $z^H$ towards the predefined/ideal prototypes as part of the learning objective for $z$.\n\n\n- We further evaluate the performance of feature representations learned with different losses using CIFAR10/100 with ResNet-18 backbone in **Tab. 6 in the revised paper** (considering larger backbones/datasets for future work).  with our predefined prototypes outperforms Atigh et al. (2021).  surpasses , and ``Ours'' performs the best, proving the strength of our learned features in image classification tasks.\n\nDataset  | $L_{CE}$  |  Atigh et al. |  $L_{sCL}$  |  $L_{H}$ + our prototypes | $L_{S}$  | Ours \n---|:---:|:---:|---:|:---:|:---:|:---:\nCIFAR10 | 94.9 | 92.3 | 95.0  | 94.1 | 95.2 | 95.4 \nCIFAR100 | 76.1 | 65.8 | 76.3  | 68.0 | 76.7 | 77.8 \n \n- It is worth noting that supervised contrastive learnings are two-stage approaches. During inference, they discard the projection layer obtained during training and require training additional linear layer(s) (using cross entropy in the original paper, cross-entropy). In contrast, our method is one-stage. Once trained either with ,  or both (using predefined ideal prototypes), we can use the network as such (without removing any layer or training additional layer). Classification is directly done by comparing the output of network with predefined prototypes."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217331048,
                "cdate": 1700217331048,
                "tmdate": 1700217331048,
                "mdate": 1700217331048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kf60eLzBHP",
                "forum": "G4D6jClNFl",
                "replyto": "Ckz1DVk95l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer gKak (2/2)"
                    },
                    "comment": {
                        "value": "**Question. The authors only emphasize that the Euclidean-based distances appear sub-optimal for faces as the complexity and nature of human faces go beyond a basic Euclidean manifold. This explanation is vague and general, thus not convincing.** \n\n**Reply:**\n\nThank you for your suggestion. We developed the explanation in page 1. ``Unlike the assumptions of a Euclidean manifold, which presumes a flat and linear space, the intricate shapes and curvatures of facial features such as the nose, eyes, and mouth defy adequate description through Euclidean geometry alone. The human face is not a rigid structure; facial expressions are non-linear deformations of facial features (e.g., when a person smiles, the shape of the face undergoes complex transformations). In other words, human faces challenge Euclidean geometry with their dynamic and non-linear features.''\n\n\n**Question. As I know, using hyperbolic space representations always work well for the tasks with hierarchical relation nature. However, the authors fail to explain the inherent hierachical relations in deepfake detection tasks.**\n\n**Reply:**\n\nWe have already conducted the experiments to demonstrate the hierarchical relations in deepfake detection datasets. **Originally located in the Appendix, we have now integrated them into the main paper**. Please refer to Figure 2. Left: Inherent hierarchical structure of different manipulations in FF++ represented as a tree. Right: t-SNE projection of embeddings for fake images from the FF++ in the Poincar\u00e9 ball model. Dotted line circles indicate leaf nodes. CTru effectively captures coarse semantic differences closer to the origin in hyperbolic space (e.g., Reenactment vs. Replacement), with finer-grained distinctions positioned farther away (e.g., Face2Face vs. NeuralTextures)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217523023,
                "cdate": 1700217523023,
                "tmdate": 1700217523023,
                "mdate": 1700217523023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RK6QcH9oBB",
                "forum": "G4D6jClNFl",
                "replyto": "kjYoTaSlbv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Reviewer_gKak"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Reviewer_gKak"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed reply. \n\nI agree that you have made some improvements beyond a simple combination of known methods. However, these improments, including extra or removed loss terms, or prototype generation method, are not significant enough. \n\nIn your reply 2/2, The explanation to my first question is still not specific enough. The key point should be \"why curved spaces are better for deepfake detection\" rather than \"why curved spaces are better for human face representation\".  The hierarchical relation may answer the first question and figure 2 in the revised version is useful. From this perspective, your logical chain of \"deepfake detection is hierarchical --> curved space representation works well in hierarchical relation learning --> we employ curved space representation\" is indeed novel. But the novelty is still not enough for ICLR since the first two steps in this logical chain are purely borrowed from previous works and other contributions are not significant. \n\nThis work is indeed valuable and I suggest the authors to delve deeper into this topic. By now, I have to keep my original rating."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618810126,
                "cdate": 1700618810126,
                "tmdate": 1700618810126,
                "mdate": 1700618810126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AcRBiJZZIf",
            "forum": "G4D6jClNFl",
            "replyto": "G4D6jClNFl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2798/Reviewer_V3Kg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2798/Reviewer_V3Kg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a framework, CTru, for fakeface detection. The idea is to project face features into different geometric spaces, and combine the projections into a loss function to learn the encoder with contrastive learning. Some experimental results have been shown for demonstration."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Slightly better results."
                },
                "weaknesses": {
                    "value": "1. Novelty: The paper integrates several existing techniques widely used in the computer vision community for the application of fake face detection, with no theoretical justification. Why does such an integration work? Why not other ways? This is one of my major concerns as a publication in ICLR, as to me I feel learning nothing from the paper.\n\n2. Writing: I am not clear how Eqs. 2 and 7 are implemented. Eq. 2 is for generating \u201chigh\u201d quality fake images, but why is \u201chigh\u201d quality? Eq. 7 is for making decisions, but \u201chow\u201d?\n\n3. Experimental results are slightly higher than the approaches that all are before 2023. Not sure if they are state-of-the-art."
                },
                "questions": {
                    "value": "see my comments"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2798/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698852240806,
            "cdate": 1698852240806,
            "tmdate": 1699636222334,
            "mdate": 1699636222334,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ghctTLVg9o",
                "forum": "G4D6jClNFl",
                "replyto": "AcRBiJZZIf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer V3Kg (1/3)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for providing constructive reviews. We have addressed each of your questions individually in our response and hope these clarifications contribute to an improved rating. We have revised the paper according to your feedback, marking the changes in magenta. Additionally, we've highlighted in brown the text that was either part of the initial submission or present in the Appendix, which may have been overlooked.\n\n**Question. Novelty: The paper integrates several existing techniques widely used in the computer vision community for the application of fake face detection, with no theoretical justification. Why does such an integration work? Why not other ways?**\n\n**Reply:**\n\n1. First, we believe that our approach goes beyond a simple combination of two known learning methods. In relation to existing works, different novelties (new loss terms, new ideal prototypes with improved properties) were considered in each space. Additionally, a new method for generating predefined/ideal prototypes was proposed. \n\n- In hypersphere space (Sec. 3.3), novel terms (the first terms in Eq.3) were incorporated instead of relying solely on the well-known supervised contrastive loss (sCL). This results in improved performance, as demonstrated in Tab.4.  In essence, $L_S$ differs from and outperforms $L_{sCL}$.\n\n- In hyperbolic space (Sec. 3.4), we already clarified the distinctions between our method and that of Atigh et al. (2021) in Appendix (we moved it to the main paper). The main contribution of Atigh et al. (2021) is to integrate a penalized term into the Busemann function to avoid the overconfidence issue. \nIn contrast, we just employ the Busemann function but **without** the penalized term. In our experiments, we find that when using two considered complementary losses, penalizing the distance to the ideal prototype is unnecessary. Moreover, unlike Atigh et al. (2021) that **randomly** samples points from the unit sphere, we use **predefined evenly distributed points** as ideal prototypes.\n\n- We introduce a novel method to generate evenly distributed vectors (at the end of Sec 3.1) that can be used as both predefined prototypes in sphere space and as ideal prototypes in hyperbolic space. \n\n2. Second, we think that the representation learning aspect may have been overlooked. In Fig. 1, given an image I, we compute $z=f_e(I)$ while $f^S_p$ and $f^H_p$ are only two non-parameter projection operators: $z^S=f^S_p(z) = z/norm(z); z^H=f^H_p(z)=exp_0(z)$ (defined in Sec 3.3 and 3.4). We only have a single vector $z$ which conveys the rich information extracted from image I, not two. Our approach involves pushing $z^S$ and $z^H$ towards the predefined/ideal prototypes as part of the learning objective for $z$.\n\n- We further evaluate the performance of feature representations learned with different losses using CIFAR10/100 with ResNet-18 backbone in **Tab. 6 in the revised paper** (considering larger backbones/datasets for future work). $L_H$ with our predefined prototypes outperforms Atigh et al. (2021). $L_S$ surpasses $L_{sCL}$, and ``Ours'' performs the best, proving the strength of our learned features in image classification tasks.\n\nDataset  | $L_{CE}$  |  Atigh et al. | $L_{sCL}$   | $L_{H}$ + our prototypes | $L_{S}$ | Ours \n---|:---:|---:|---:|---:|---:|---:\nCIFAR10 | 94.9 | 92.3 | 95.0  | 94.1 | 95.2 | 95.4 \nCIFAR100 | 76.1 | 65.8 | 76.3  | 68.0 | 76.7 | 77.8 \n \n- It is worth noting that supervised contrastive learnings are two-stage approaches. During inference, they discard the projection layer obtained during training and require training additional linear layer(s) (using cross entropy in the original paper, cross-entropy). In contrast, our method is one-stage. Once trained either with $L_H$, $L_S$ or both (using predefined ideal prototypes), we can use the network as such (without removing any layer or training additional layer). Classification is directly done by comparing the output of network with predefined prototypes."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700214899193,
                "cdate": 1700214899193,
                "tmdate": 1700214899193,
                "mdate": 1700214899193,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9aSQnPlXVR",
                "forum": "G4D6jClNFl",
                "replyto": "AcRBiJZZIf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer V3Kg (2/3)"
                    },
                    "comment": {
                        "value": "**Question. Writing: I am not clear how Eqs. 2 and 7 are implemented. Eq. 2 is for generating \u201chigh\u201d quality fake images, but why is \u201chigh\u201d quality? Eq. 7 is for making decisions, but \u201chow\u201d?**\n\n**Reply:**\n\nThank you for your questions.\n- Concerning Eq. 2, we generated our synthetic images using a variety of augmentation techniques, akin to those utilized in contemporary deepfake detectors like [SBI] but our approach introduces only localized artifacts to the images. Due to the space limit, the specific augmentations were detailed in the Appendix and cross-referenced in the main paper. By employing an array of sophisticated augmentation techniques\u2014spanning spatial and frequency transformations, scaling, contrast adjustments, down-sampling, sharpening filters, blending, and more\u2014we effectively simulate diverse fake artifacts.\n\n**In Appendix**. \u201cThe following hyperparameter values were selected in a way that resulted in visually subtle artifacts, similarly to Shiohara & Yamasaki (2022). Specifically, we applied to \u201csource\u201d: (i) random scaling (followed by center cropping) by up to 5%, (ii) random shifting of HSV channel values by up to 0.1, and (iii) random translations of up to 3% and 1.5% of the image width and height, respectively. We also used more fine augmentations to \u201ctarget\u201d: (i) random shifting of HSV channel values by up to 0.3, (ii) shifting of RGB channel values by up to 20, (iii) random scaling of the brightness and contrast by a factor of up to 0.1, or (iv) down-sampling by a factor of 2 or 4, (v) a sharpening filter and blending with the original with an \u03b1 value in the range [0.2, 0.5], (vi) JPEG compression with a quality factor between 30 and 70.\u201d\n\n- Concerning the use of fakeness score for detecting fake during inference, we follow the standard evaluation protocol for deepfake detection and report the Area Under the Curve (AUC) as the performance metric. We consider the Receiver Operating Characteristic (ROC) curve of the validation split of the FF++ dataset and select the threshold that yields the highest AUC. This threshold remains fixed when evaluating the testing dataset."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215341000,
                "cdate": 1700215341000,
                "tmdate": 1700215341000,
                "mdate": 1700215341000,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3uzBO5k0GH",
                "forum": "G4D6jClNFl",
                "replyto": "AcRBiJZZIf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer V3Kg (3/3)"
                    },
                    "comment": {
                        "value": "**Question. 3. Experimental results are slightly higher than the approaches that all are before 2023. Not sure if they are state-of-the-art.**\n\n**Reply:**\n\nThank you for your questions.\n\n- At the time of submitting the CTru paper, **we had already carefully verified** the SOTA results on the GitHub\nhttps://github.com/Daisy-Zhang/Awesome-Deepfakes-Detection which gathers and updates almost all the deepfake papers from CVPR, ICCV, ICLR, NeuRIPS, and so on. In our initial submission, we already included the best known results so far coming from SeeABLE (at that time, the paper was just on arXiv and we now update its official publisher ICCV\u201923).\n\n- For a clearer comparison, we have now  incorporated in the revised version the results of other recent methods from CVPR\u201923, ICCV\u201923 papers in Tab. 1 (please note that those ICCV\u201923 papers were published after the submission deadline of ICLR 2024).\n\n\nMethod     |       Celeb-DF-v2   |       DFDC\t|      DFDC-p      |    Note       \n---|:---:|:---:|:---:|:---: \nIIL - CVPR\u201923 [1]'s Tab.3            |      \t\t\t     -*\t         |           -\t   |      73.8 | -\nIIL + SBI (CVPR\u201923 + CVPR\u201922 Oral) [1]'s Tab.6       |    \t     -*        |                      -\t |           79.6     |    Combine 2 methods\nIID (CVPR\u201923) [2]'s Tab.2                      |              \t\t    82.0      |                      -\t |           81.2       |  -\nTALL + Swin (ICCV\u201923) [3]'s Tab.2       |                                  90.7      |                    76.8    |            -     |     x5 paras + 14M images\nTALL + ViT-B (ICCV\u201923) [3]'s Tab.2  \t |  \t\t    86.6\t |  \t     74.1\t    |          -\t |       x5 paras + 14M images\nTALL + EffNet (ICCV\u201923) [3]'s Tab.2        |                              83.4 \t |  \t     67.1\t      |        -    |    + 1M images \nSeeABLE EffNet (ICCV\u201923) \t\t      |               87.3\t |  \t     75.9  \t  |         86.3   |  \nCTru   (EffNet)    \t\t\t\t       |              89.4\t |  \t     77.0\t   |        87.9  |  \n\n*: IIL ([1]-Tab. 3) was evaluated on Celeb-DF-v1 which is older and easier than v2.\n                                       \n- Note that TALL [3] requires an additional large-scale dataset to pre-train its backbones (1M and 14M images when using CNN EffNet and Transformer Swin backbones, respectively), while CTru is trained from scratch with only 720 images. It is clear that CTru\u2019s results are SOTA. We believe that our improvement is **not slight**. In comparison with SeeABLE, CTru consistently outperforms by a margin of nearly **2 points**, while maintaining a **faster and more generic** approach (without the requirement of a specific training scheduler and prior knowledge of the face, as mentioned in the paper).\n- When comparing to TALL (ICCV\u201923), using the same EffNet backbone, CTru demonstrates significant superiority with improvements of **+6 and +10 points** on the CDF-v2 and DFDC datasets, respectively. Notably, CTru even surpasses TALL when *using Vit-B pretrained on Image-Net21k, comprising 14 million images and five times* as many parameters as CTru.\n\n[1] Dong et al., \u201cImplicit Identity Leakage: The Stumbling Block to Improving Deepfake Detection Generalization, CVPR 2023.\n\n[2] Huang et al., \u201cImplicit Identity Driven Deepfake Face Swapping Detection\u201d, CVPR 2023 \n\n[3] Xu et al., \u201cTALL: Thumbnail Layout for Deepfake Video Detection\u201d, ICCV 2023"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700216343085,
                "cdate": 1700216343085,
                "tmdate": 1700216343085,
                "mdate": 1700216343085,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FevbohnxNS",
                "forum": "G4D6jClNFl",
                "replyto": "AcRBiJZZIf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2798/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer V3Kg"
                    },
                    "comment": {
                        "value": "We would also like to emphasize that the two losses in two spaces are connected through the utilization of the same predefined/ideal prototypes.\n\nWe are eager to continue the discussion. Have all your concerns been addressed in our rebuttal, or are there any remaining comments you would like us to consider?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2798/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700236564520,
                "cdate": 1700236564520,
                "tmdate": 1700236564520,
                "mdate": 1700236564520,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]